{"Question": "openmp parallel for loop with two or more reductions\r\n                \r\nHi just wondering if this is the right way to go going about having a regular for loop but with two reductions , is this the right approach below? Would this work with more then two reductions as well. Is there a better way to do this?\nalso is there any chance to integrate this with an MPI_ALLREDUCE command?\n\n```\nheres the psuedo code\n\n      #pragma omp parallel for \\\n      default(shared) private(i) \\\n      //todo first  reduction(+:sum)\n      //todo second reduction(+:result)\n\n      for loop i < n; i ++; {\n        y = fun(x,z,i)\n        sum += fun2(y,x)\n        result += fun3(y,z)\n      }\n```\n\n    ", "Answer": "\r\nYou can do reduction by specifying more than one variable separated by a comma, i.e. a list:\n\n```\n#pragma omp parallel for default(shared) reduction(+:sum,result) ...```\n\n\nPrivate thread variables will be created for ```\nsum```\n and ```\nresult```\n that will be combined using ```\n+```\n and assigned to the original global variables at the end of the thread block.\n\nAlso, variable ```\ny```\n should be marked private.\n\nSee https://computing.llnl.gov/tutorials/openMP/#REDUCTION\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda Calculus Reductions\r\n                \r\nI am able to do simple Lambda Calculus reductions however, I can not figure out how to do the ones that obtain \"currying\".\n\nThese are the two examples that I cannot figure out:\n\n\n```\n( ( ( lambda x . ( lambda y . ( j y ) ) ) j ) m )```\n\n```\n( ( lambda p . ( p j ) ) ( lambda x . ( q x )))```\n\n\n    ", "Answer": "\r\nRemark There's a mistake with the evaluation, it reduces to ```\nj m```\n instead, so the part about self-application isn't relevant.\n\nCurrying is the observation that you can view a series of lambda abstractions in different ways:\n\nIn mathematical terms, ```\n( ( lambda x . ( lambda y . ( j y ) ) )```\n can be given a name and then written as: ```\nf(x,y) = j(y)```\n. In your example, you would evaluate ```\nf(m,j) = j(j)```\n. So what happens if we don't have both arguments for f? We can't evaluate it completely, but we can define a new function g(y) = f(j,y), where we just insert the first argument. This step-wise function evaluation is called partial evaluation or currying.\n\nIn lambda calculus, these two aspects look absolutely the same. If you want to apply both aruments to your term, you start with the first argument:\n\nYour initial function f(m,j): ```\n( ( ( lambda x . ( lambda y . ( j y ) ) ) j ) m )```\n reduces to g(j): ```\n( ( lambda y . ( j y ) ) ) j```\n. When we continue our evaluation (we can still apply our function to j), we reach j(j): ```\nj j```\n. Now we cannot apply any reduction rules anymore, so we could view ```\nj j```\n as the result of our computation. That our result is an application is fine, but that it is applied to itself is something special.\n\n(The rest is not related to currying anymore, but to self-application, which bridges to what @Matt was writing)\n\nPerhaps one should explain what this means: the function j gets itself as argument. With this you can implement recursion. The famous Y combinator Y: ```\n(lambda x . f x x)```\n does exactly that: if you evaluate ```\nY Y```\n, that is ```\n(lambda x . f x x) Y```\n, you compute ```\nf (Y Y)```\n. When you evaluate the inner ```\nY Y```\n again, you compute ```\nf f (Y Y)```\n and so on. This is exactly a recursive application of the function ```\nf```\n. A side-effect is that for some ```\nf```\n, the evaluation will never terminate (already if you use the identity function ```\n(lambda x.x)```\n).\n\nLogicians in the mid-20th century wanted to use lambda-calculus as a data-structure, where infinite evaluation sequences should be forbidden. A possibility to restrict lambda calculus is that you give a type (quite similar to types in programming languages) to each variable. If you want to apply a variable to another one, the types need to fit.\n\nE.g. suppose that ```\nx```\n is of type ```\nint```\n, then in the application ```\nf x```\n, ```\nf```\n needs to be of a type which takes a variable of type ```\nint```\n and computes a result, let's say of type ```\nstring```\n. Then we can write the type of f as ```\nint -> string```\n. The type of ```\nf x```\n is ```\nstring```\n, since that's what we get when we evaluate f on x. \n\nAbstractions create a new function. For instance ```\n(lambda x . x)```\n needs an argument of type ```\nint```\n and produces a term of type ```\nint```\n, i.e. it is of type ```\nint -> int```\n.\n\nBut now self-application like ```\nj j```\n does not work anymore: say the inner j is of type ```\nt```\n. Then the outer j must be of type ```\nt -> t```\n. The only way to make this work is that your type is an infinite nesting of ```\nt```\n, which is usually forbidden.\n\nEven though this approach seems a little limited, you can add recursion on top of typed lambda calculus to build programming languages like Haskell or OCaml.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Clojure: reduce, reductions and infinite lists\r\n                \r\nReduce and reductions let you accumulate state over a sequence.\nEach element in the sequence will modify the accumulated state until\nthe end of the sequence is reached. \n\nWhat are implications of calling reduce or reductions on an infinite list?\n\n```\n(def c (cycle [0]))\n(reduce + c)\n```\n\n\nThis will quickly throw an OutOfMemoryError. By the way, ```\n(reduce + (cycle [0]))```\n does not throw an OutOfMemoryError (at least not for the time I waited). It never returns. Not sure why. \n\nIs there any way to call reduce or reductions on an infinite list in a way that makes sense?  The problem I see in the above example, is that eventually the evaluated part of the list becomes large enough to overflow the heap. Maybe an infinite list is not the right paradigm. Reducing over a generator, IO stream, or an event stream would make more sense. The value should not be kept after it's evaluated and used to modify the state.      \n    ", "Answer": "\r\nIt will never return because reduce takes a sequence and a function and applies the function until the input sequence is empty, only then can it know it has the final value.\n\nReduce on a truly infinite seq would not make a lot of sense unless it is producing a side effect like logging its progress.  \n\nIn your first example you are first creating a var referencing an infinite sequence.\n\n```\n(def c (cycle [0]))\n```\n\n\nThen you are passing the contents of the var c to reduce which starts reading elements to update its state. \n\n```\n(reduce + c)\n```\n\n\nThese elements can't be garbage collected because the var c holds a reference to the first of them, which in turn holds a reference to the second and so on. Eventually it reads as many as there is space in the heap and then OOM.\n\nTo keep from blowing the heap in your second example you are not keeping a reference to the data you have already used so the items on the seq returned by cycle are GCd as fast as they are produced and the accumulated result continues to get bigger. Eventually it would overflow a long and crash (clojure 1.3) or promote itself to a BigInteger and grow to the size of all the heap (clojure 1.2) \n\n```\n(reduce + (cycle [0]))\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP: multiple reductions in parallel\r\n                \r\nI have some code that looks like this:\n```\ndouble r1 = 0.0, r2 = 0.0;\n\nfor (size_t i = 0; i < k; ++i) {\n    r1 += reduction1(data1[i]);\n}\n\nfor (size_t i = 0; i < k; ++i) {\n    r2 += reduction2(data2[i]);\n}\n```\n\nThe two reductions, while they run over the same number of iterations, otherwise run on different paths and on different sets of data. I was wondering whether there is a way to run both reductions in parallel.\nBonus: what if the two loops ran for different number of iterations?\nEdit: in my case ```\nk```\n is fairly small, and most of the work is done inside the individual ```\nreduction```\n functions. So my goal is to parallelize as many executions of reduction functions as possible.\n    ", "Answer": "\r\nIf you can use one loop it is very easy:\n```\n#pragma omp parallel for reduction(+:r1,r2)\nfor (size_t i = 0; i < k; ++i) {\n    r1 += reduction1(data1[i]);\n    r2 += reduction2(data2[i]);\n}\n\n```\n\nif not, use tasks/taskloop:\n```\n#pragma omp parallel\n#pragma omp single\n{\n    #pragma omp taskloop reduction(+:r1)\n    for (size_t i = 0; i < k; ++i) {\n        r1 += reduction1(data1[i]);\n    }\n\n    #pragma omp taskloop reduction(+:r2)\n    for (size_t i = 0; i < k; ++i) {\n        r2 += reduction2(data1[i]);\n    }\n\n}\n```\n\nEDIT: You mentioned in the comments that ```\nk```\n is small, so probably the best is to use a separate task for each ```\nreduction1/reduction2```\n calculation. Also @JeromeRichard pointed out that in the case of using ```\ntaskloop```\n the second loop will wait for all the previous tasks of the first loop to be completed. So, based on these new information, a better alternative may be something like this:\n```\n#pragma omp parallel\n#pragma omp single\n{\n#pragma omp taskgroup task_reduction(+:r1,r2)\n{\n    for (size_t i = 0; i < k; ++i) {\n        #pragma omp task in_reduction(+:r1)\n        {\n            r1 += reduction1(data1[i]);\n        }\n    }\n\n    for (size_t i = 0; i < k; ++i) {\n        #pragma omp task in_reduction(+:r2)\n        {\n            r2 += reduction2(data2[i]);\n        }   \n    }\n}\n\n}\n```\n\nYou also mentioned in the comments that the parallel code is only 25% faster than the serial one, which suggests a major problem, which should be investigated by profiling your program.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Clojure - how to do reductions function but drop state?\r\n                \r\nIf I use the reductions function like so:\n\n```\n(reductions + [1 2 3 4 5])\n```\n\n\nThen I get \n\n```\n(1 3 6 10 15)\n```\n\n\nWhich is great - but I'd like to apply a binary function in the same way without the state being carried forward - something like\n\n```\n(magic-hof + [1 2 3 4 5])\n```\n\n\nleads to\n\n```\n(1 3 5 7 9)\n```\n\n\nie it returns the operation applied to the first pair, then steps 1 to the next pair.\n\nCan someone tell me the higher-order function I'm looking for? (Something like reductions)\n\nThis is my (non-working) go at it:\n\n```\n(defn thisfunc [a b] [(+ a b) b])\n\n(reduce thisfunc [1 2 3 4 5])\n```\n\n    ", "Answer": "\r\nYou can do it with map:\n\n```\n(map f coll (rest coll))\n```\n\n\nAnd if you want a function:\n\n```\n(defn map-pairwise [f coll]\n  (map f coll (rest coll)))\n```\n\n\nAnd if you really need the first element to remain untouched (thanx to juan.facorro's comment):\n\n```\n(defn magic-hof [f [x & xs :as s]]\n  (cons x (map f s xs)))\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Efficient boolean reductions `any`, `all` for PySpark RDD?\r\n                \r\nPySpark supports common reductions like ```\nsum```\n, ```\nmin```\n, ```\ncount```\n, ...  Does it support boolean reductions like ```\nall```\n and ```\nany```\n?  \n\nI can always ```\nfold```\n over ```\nor_```\n and ```\nand_```\n  but this seems inefficient.\n    ", "Answer": "\r\nthis is very late, but ```\nall```\n on a set of ```\nboolean```\n values ```\nz```\n is the same as ```\nmin(z) == True```\n and ```\nany```\n is the same as ```\nmax(z) == True```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Faster Parallel Reductions on Kepler\r\n                \r\nI'm just a CUDA beginner and  trying to use Faster Parallel Reductions on Kepler on my program, but I didn't get the result, below is a function of what I'm doing, the output is 0, I would be appreciated to know what is my mistake?\n\n```\n#ifndef __CUDACC__  \n#define __CUDACC__\n#endif\n\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include \"device_launch_parameters.h\"\n#include <iostream>\n#include <cuda_runtime_api.h>\n#include <device_functions.h>\n#include <stdio.h>\n#include <math.h>\n\n__inline__ __device__\nfloat warpReduceSum(float val) {\n  for (int offset = warpSize/2; offset > 0; offset /= 2) \n    val += __shfl_down(val, offset);\n  return val;\n}\n\n__inline__ __device__\nfloat blockReduceSum(float val) {\n\n  static __shared__ int shared[32]; // Shared mem for 32 partial sums\n  int lane = threadIdx.x % warpSize;\n  int wid = threadIdx.x / warpSize;\n\n  val = warpReduceSum(val);     // Each warp performs partial reduction\n\n  if (lane==0) shared[wid]=val; // Write reduced value to shared memory\n\n  __syncthreads();              // Wait for all partial reductions\n\n  //read from shared memory only if that warp existed\n  val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;\n\n  if (wid==0) val = warpReduceSum(val); //Final reduce within first warp\n\n  return val;\n}\n\n__global__ void deviceReduceKernel(float *in, float* out, size_t N)\n{\n  float sum = 0;\n  //reduce multiple elements per thread\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) \n  {\n    sum += in[i];\n  }\n  sum = blockReduceSum(sum);\n  if (threadIdx.x==0)\n    out[blockIdx.x]=sum;\n}\n\nint main()\n{\n    int n = 1000000;\n    float *b = new float[1]();\n    float *d = new float[1]();\n    float *a ;\n\n\n    int blocks = (n/512)+1;\n    float *d_intermediate;\n\n    cudaMalloc((void**)&d_intermediate, n*sizeof(float));\n    cudaMalloc((void**)&a, n*sizeof(float));\n\n    cudaMemset(a, 1, n*sizeof(float));\n\n    deviceReduceKernel<<<blocks, 512>>>(a, d_intermediate, n);\n    deviceReduceKernel<<<1, 1024>>>(d_intermediate, &b[0], blocks);\n    cudaMemcpy(d, b, sizeof(float), cudaMemcpyDeviceToHost);\n    cudaFree(d_intermediate);\n    std::cout << d[0];\n    return 0;\n\n}\n```\n\n    ", "Answer": "\r\nThere are various problems with your code:\n\n\nAny time you are having trouble with a CUDA code, you should use proper cuda error checking and run your code with ```\ncuda-memcheck```\n, before asking others for help. Even if you don't understand the error output, it will be useful to others trying to help you. If you had done that with this code, you would be advised of various errors/problems\nAny pointer passed to a CUDA kernel should be a valid CUDA device pointer.  Your ```\nb```\n pointer is a host pointer:\n\n```\nfloat *b = new float[1]();\n```\n\n\nso you cannot use it here:\n\n```\ndeviceReduceKernel<<<1, 1024>>>(d_intermediate, &b[0], blocks);\n                                                 ^\n```\n\n\nsince you evidently want to use it for storage of a single ```\nfloat```\n quantity on the device, we can easily re-use the ```\na```\n pointer for that.\nFor a similar reason, this isn't sensible:\n\n```\ncudaMemcpy(d, b, sizeof(float), cudaMemcpyDeviceToHost);\n```\n\n\nin this case both ```\nb```\n and ```\nd```\n are host pointers.  That will not copy data from the device to the host.\nThis probably doesn't do what you think:\n\n```\ncudaMemset(a, 1, n*sizeof(float));\n```\n\n\nI imagine you think this will fill a ```\nfloat```\n array with the quantity 1, but it won't.  ```\ncudaMemset```\n, like ```\nmemset```\n, fills bytes and takes a byte quantity.  If you use it to fill a ```\nfloat```\n array, you are effectively creating an array filled with ```\n0x01010101```\n.  I don't know what value that translates into when you convert the bit pattern to a ```\nfloat```\n quantity, but it will not give you a ```\nfloat```\n value of 1.  We'll fix this by filling an ordinary host array with a loop, and then transferring that data to the device to be reduced.\n\n\nHere's a modified code that has the above issues addressed, and runs correctly for me:\n\n```\n$ cat t1290.cu\n#include <iostream>\n#include <stdio.h>\n#include <math.h>\n\n__inline__ __device__\nfloat warpReduceSum(float val) {\n  for (int offset = warpSize/2; offset > 0; offset /= 2)\n    val += __shfl_down(val, offset);\n  return val;\n}\n\n__inline__ __device__\nfloat blockReduceSum(float val) {\n\n  static __shared__ int shared[32]; // Shared mem for 32 partial sums\n  int lane = threadIdx.x % warpSize;\n  int wid = threadIdx.x / warpSize;\n\n  val = warpReduceSum(val);     // Each warp performs partial reduction\n\n  if (lane==0) shared[wid]=val; // Write reduced value to shared memory\n\n  __syncthreads();              // Wait for all partial reductions\n\n  //read from shared memory only if that warp existed\n  val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;\n\n  if (wid==0) val = warpReduceSum(val); //Final reduce within first warp\n\n  return val;\n}\n\n__global__ void deviceReduceKernel(float *in, float* out, size_t N)\n{\n  float sum = 0;\n  //reduce multiple elements per thread\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x)\n  {\n    sum += in[i];\n  }\n  sum = blockReduceSum(sum);\n  if (threadIdx.x==0)\n    out[blockIdx.x]=sum;\n}\n\nint main()\n{\n        int n = 1000000;\n        float b;\n        float *a, *a_host;\n        a_host = new float[n];\n\n        int blocks = (n/512)+1;\n        float *d_intermediate;\n\n        cudaMalloc((void**)&d_intermediate, blocks*sizeof(float));\n        cudaMalloc((void**)&a, n*sizeof(float));\n        for (int i = 0; i < n; i++) a_host[i] = 1;\n        cudaMemcpy(a, a_host, n*sizeof(float), cudaMemcpyHostToDevice);\n\n        deviceReduceKernel<<<blocks, 512>>>(a, d_intermediate, n);\n        deviceReduceKernel<<<1, 1024>>>(d_intermediate, a, blocks);\n        cudaMemcpy(&b, a, sizeof(float), cudaMemcpyDeviceToHost);\n        cudaFree(d_intermediate);\n        std::cout << b << std::endl;\n        return 0;\n}\n$ nvcc -arch=sm_35 -o t1290 t1290.cu\n$ cuda-memcheck ./t1290\n========= CUDA-MEMCHECK\n1e+06\n========= ERROR SUMMARY: 0 errors\n$\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Using NP Reductions\r\n                \r\nI have been having some difficulty understanding reductions using NP problems and would like clarification. Consider the following problem:\n\n```\nShow that the following problem is NP-Complete by designing\na polynomial-time reduction algorithm from an already known\nNP-Complete problem.\n\nProblem: Given an undirected graph G=(V,E) and integer k,\n         test whether G has a cycle of length k.\n```\n\n\nI know there are other topics regarding this subject, but I am still not sure I understand how reductions like this would be done.\n\nIt is my understanding that this is how you would approach a problem such as this.\n\n\nAssume the given problem can be solved in polynomial time.\nUse the given problem to solve a problem that we know is NP-Hard in polynomial time\nThis creates a contradiction, so the assumption must be incorrect\nThus, the given problem mustn't be solvable in polynomial time\n\n\nSo, for a problem like this, would this be a proper approach?\n\n\nIf we choose k to be the length of the Hamiltonian cycle in the graph (assuming there is one) that means that this problem could be used to find the Hamiltonian cycle in the graph. \nBecause we can only find the Hamiltonian cycle in NP time, this problem must also only be solvable in NP time.\n\n    ", "Answer": "\r\nThis looks rather like homework so I'll only give you a hint, but try consider a unweighted graph ```\nV```\n, with ```\nk```\n nodes. What is equivalent to finding a cycle with length ```\nk```\n, which is solvable with the algorithm you assumed that is polynomial? Try to proceed from this.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reductions in the Erlang BEAM machine\r\n                \r\nErlang is a well-known programming language that is famous (among other things)\nfor it's lightweight threading. Erlang is usually implemented with the BEAM machine.\nThe description (H'97) of the Erlang BEAM machine says\n\n\n  To guarantee a fair scheduling a process is suspended after a fixed\n   number of reductions and then the first process from the queue is\n   resumed.\n\n\nI'm interested in this notion of reduction. According to (H'97) only the following BEAM commands count as a reduction:\n\n\nC/CO/ResC: calls local/resident Erlang function\nCL: Discard the current stack frame. Call a local Erlang function.\nCEx/TrCEx: Call an external Erlang function (traced or otherwise).\nCExL/TrCExL: Discard the current stack frame and call an external Erlang fuction (traced or otherwise).\nM_C_r: Load the argument register x(0). Call a resident Erlang function.\nM_CL_r: Load the argument register x(0). Discard the current stack  frame. Call a local Erlang function.\n\n\nAll of those involve a function call. \nIn contrast, calls to C-functions (e.g. TrC/TrCO) and calls to  built-in functions (e.g. called by Bif_0_)  don't count as\nreductions.\n\nQuestions. After this preamble, here is what I would like to know.\n\n\nWhy are reductions used for scheduling between threads, and not time-slices?\nWhy do only the above commands advance the reduction counter?\nThe description in (H'97) is a bit dated, how does contemporary Erlang handle scheduling?\n\n\n\n\n(H'97) B. Hausman, The Erlang BEAM Virtual Machine Specification.\n    ", "Answer": "\r\nI'll try to answer you questions.\n\n1) The main reason for not using time slices is performance and portability. It is quite expensive to read a monotonic time value from the operating system, and if we have to do it for every function call, the overhead becomes quite large. The cost also varies quite a lot on different OS's. The reduction counting mechanic however only requires the machine to be good at decrementing integers, which most machines are.\n\n2) They don't. That list, as you say, is very outdated. Much of the way the VM works has been rewritten since. As a general rule of thumb; a function call (not the return) or anything that may take an unknown amount of time counts reductions. This includes bifs, nifs, gc, sending/receiving messages and probably more that I cannot think of right now.\n\n3) Scheduling and pre-emption are very different things. You may want to see my webinar I did a couple a years ago about how scheduling is done: https://www.youtube.com/watch?v=tBAM_N9qPno\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "is it possible to print all reductions in Haskell - using WinHugs?\r\n                \r\nI have written the following function.. and executed using WinHugs\n\n```\nteneven =  [x | x <- [1..10], even x]\n```\n\n\nMy output:\n\n```\nMain> teneven\n[2,4,6,8,10] :: [Integer] \n(63 reductions, 102 cells)\n```\n\n\nis there anyway to print all the reductions.. so I can learn the core evaluation happening inside WinHugs?\n    ", "Answer": "\r\nSome ideas:\n\n\nThe debug command-line option (which you can set with ```\n:set +d```\n in Hugs) is informative, but is very verbose and does not show you the reductions in Haskell syntax.\nTry Hat - the Haskell Tracer.  I just tried it on a simple program and it's pretty cool.  I'm not on Windows, though, and I don't know how difficult it would be to get it running.  It's likely fairly difficult, which is a shame since it's cool and essentially what you want.  If you do get it running, you can get something like this information from Hat:\n\n```\nmain = {IO}\nteneven = [2,4,6,8,10]\n_foldr (\\..) [1,2,3,4,5,6,7,8, ...] [] = [2,4,6,8,10]\n(\\..) 1 [2,4,6,8,10] = [2,4,6,8,10]\n(\\..) 2 [4,6,8,10] = [2,4,6,8,10]\n(\\..) 3 [4,6,8,10] = [4,6,8,10]\n(\\..) 4 [6,8,10] = [4,6,8,10]\n(\\..) 5 [6,8,10] = [6,8,10]\n(\\..) 6 [8,10] = [6,8,10]\n(\\..) 7 [8,10] = [8,10]\n(\\..) 8 [10] = [8,10]\n(\\..) 9 [10] = [10]\n(\\..) 10 [] = [10]\n```\n\n\nThe lambda there is ```\neven```\n.  Also, if you want, Hat can trace into calls of ```\nfoldr```\n and other internal calls; by default, it doesn't do that.\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Printing all reductions in Bison\r\n                \r\nIs there a way to have Yacc/Bison print out all the reductions it does to the input it processes? This would be a great debugging aid. I already tried\n\n```\n| Item1 { printf(\"Item1: %s\\n\", yytext); }\n```\n\n\nwhich only prints the last character and\n\n```\n| Item1 { printf(\"Item1: %s\\n\", $$); }\n```\n\n\nwhich results in a format argument is not a pointer warning from the compiler. Am I doing it wrong?\n    ", "Answer": "\r\nI was able to enable debug output by\n\n\nPutting ```\n#define YYDEBUG 1```\n in my C declarations\nPutting ```\nint yydebug = 1;```\n in the additional C code section\n\n\nFor additional reading, check Using YYDEBUG to generate debugging information, Debugging Your Parser and this yacc example.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Why is the OpenMP reduction clause needed to make reductions concurrent?\r\n                \r\nThe OpenMP 'parallel' construct and 'SIMD' construct (new in revision 4.0) define the reduction clause which tells the compiler which variable the reduction is performed on and what is the reduction operator.\nBut why does the compiler need the programmer to tell it this information? GCC, for example, has the capability of identifying reductions without getting any help from the programmer (see here and here).\nAre there any examples of loops that cannot be made concurrent without specifying the reduction clause?\n    ", "Answer": "\r\nReductions are a simple mechanism to improve the performance of parallel applications by removing synchronisation points and at the price of relaxed consistence of memory view. The need to explicitly have a reduction clause should be immediately apparent from the following example:\n\nImagine that you have a code that does a search over an unordered collection of ```\nNUM_ITEMS```\n items and the goal is to find all items that match given criteria and to collect them in an array ```\nmatches```\n as well as to compute the sum of some property of these items. It doesn't matter in what order the items are found. The serial code could be something like this:\n\n```\nint num_matches = 0;\nint prop_sum = 0;\n\nfor (i = 0; i < NUM_ITEMS; i++)\n{\n   if (criteria(item[i]))\n   {\n      match[num_matches] = item[i];\n      num_matches++;\n      prop_sum += item[i]->some_property;\n   }\n}\n```\n\n\nBoth ```\nnum_matches```\n and ```\nprop_sum```\n are variables, whose values are accumulated as the loop goes. But both variables have completely different semantics. While it is possible that ```\nprop_sum```\n can be computed as a sum of partial sums, ```\nnum_matches```\n cannot because of its use as an index in the output array. ```\nprop_sum```\n is a typical candidate for reduction while ```\nnum_matches```\n cannot only be reduced, but also one has to utilise an explicit synchronisation construct in order to prevent data races and different threads overwriting the same element of ```\nmatch```\n:\n\n```\nint num_matches = 0;\nint prop_sum = 0;\n\n#pragma omp parallel for reduction(+:prop_sum)\nfor (i = 0; i < NUM_ITEMS; i++)\n{\n   if (criteria(item[i]))\n   {\n      #pragma omp critical(update_matches)\n      {\n          match[num_matches] = item[i];\n          num_matches++;\n      }\n      prop_sum += item[i]->some_property;\n   }\n}\n```\n\n\nAlthough you might argue that the compiler might be smart enough to notice the way ```\nnum_matches```\n is used and automatically generate atomic increments, the goal of OpenMP is to be portable among platforms and compiler vendors. That means that if you write an OpenMP program that conforms to the standard and compiles and works correctly with one compiler, then it should compile and work correctly with other compilers too. The standard is written by many different vendors and not everyone of them has this super smart data dependency discovery mechanism. Besides it becomes very hard to have reliable detection when data, external to the compilation unit, is involved.\n\n```\nreduction```\n is not a necessity - it is merely a convenience. You can implement your own reduction e.g. using atomic increments and this might be optimal for your platform, but it might be very far from optimal on other platforms that do not provide efficient atomic increments. On the other side, it is expected that each compiler will generate code that implements the ```\nreduction```\n clause in the best possible way for the given target platform.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Javascript equivalent to Clojure's \"reductions\" or python's itertools.accumulate\r\n                \r\nIs there a JavaScript equivalent to Clojure's \"reductions\" function or Python's ```\nitertools.accumulate```\n? In other words, given an array ```\n[x_0, x_1, x_2 ... x_n-1]```\n and a function ```\nf(prev, next)```\n, it would return an array of length ```\nn```\n with values:\n\n```\n[x_0, f(x_0, x_1), f(f(x_0, x_1), x_2)... f(f(f(...)), x_n)]```\n\n\nI'm simulating the desired behavior below:\n\n```\nfunction accumsum(prev, next) {\n    last = prev[prev.length - 1] || 0;\n    prev.push(last + next);\n    return prev;\n}\n\nvar x = [1, 1, 1, 1];\nvar y = x.reduce(accumsum, []);\nvar z = y.reduce(accumsum, []);\n\nconsole.log(x);\nconsole.log(y);\nconsole.log(z);\n```\n\n\nwhich displays:\n\n```\n[ 1, 1, 1, 1 ]\n[ 1, 2, 3, 4 ]\n[ 1, 3, 6, 10 ]\n```\n\n\nBut I'm wondering if there is a way to write something simpler like\n\n```\n[1, 1, 1, 1].reductions(function(prev, next) {return prev + next;});\n```\n\n\nIf not, is there a more idiomatic way to do this in JavaScript than what I wrote?\n    ", "Answer": "\r\n```\nvar a = [1, 1, 1, 1];\nvar c = 0;\na.map(function(x) { return c += x; })\n// => [1, 2, 3, 4]\n\na.reduce(function(c, a) {\n  c.push(c[c.length - 1] + a);\n  return c;\n}, [0]).slice(1);\n// => [1, 2, 3, 4]\n```\n\n\nI'd use the first one, personally.\n\nEDIT:\n\n\n  Is there a way of doing your first suggestion that doesn't require me to have a random global variable (c in this case) floating around? If I forgot to re-initialize c back to 0, the second time I wrote a.map(...) it would give the wrong answer.\n\n\nSure - you can encapsulate it.\n\n```\nfunction cumulativeReduce(fn, start, array) {\n  var c = start;\n  return array.map(function(x) {\n    return (c = fn(c, x));\n  });\n}\ncumulativeReduce(function(c, a) { return c + a; }, 0, [1, 1, 1, 1]);\n// => [1, 2, 3, 4]\nc\n// => ReferenceError - no dangling global variables\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Perform multiple reductions in a single pass in Clojure\r\n                \r\nIn Clojure I want to find the result of multiple reductions while only consuming the sequence once. In Java I would do something like the following:\n\n```\ndouble min = Double.MIN_VALUE;\ndouble max = Double.MAX_VALUE;\nfor (Item item : items) {\n    double price = item.getPrice();\n    if (price > min) {\n        min = price;\n    }\n\n    if (price < max) {\n        max = price;\n    }\n}\n```\n\n\nIn Clojure I could do much the same thing by using loop and recur, but it's not very composable - I'd like to do something that lets you add in other aggregation functions as needed.\n\nI've written the following function to do this:\n\n```\n(defn reduce-multi\n  \"Given a sequence of fns and a coll, returns a vector of the result of each fn\n  when reduced over the coll.\"\n  [fns coll]\n  (let [n (count fns)\n        r (rest coll)\n        initial-v (transient (into [] (repeat n (first coll))))\n        fns (into [] fns)\n        reduction-fn\n        (fn [v x]\n          (loop [v-current v, i 0]\n            (let [y (nth v-current i)\n                  f (nth fns i)\n                  v-new (assoc! v-current i (f y x))]\n              (if (= i (- n 1))\n                v-new\n                (recur v-new (inc i))))))]\n    (persistent! (reduce reduction-fn initial-v r))))\n```\n\n\nThis can be used in the following way:\n\n```\n(reduce-multi [max min] [4 3 6 7 0 1 8 2 5 9])\n=> [9 0]\n```\n\n\nI appreciate that it's not implemented in the most idiomatic way, but the main problem is that it's about 10x as slow as doing the reductions one at at time. This might be useful for lots performing lots of reductions where the seq is doing heavy IO, but surely this could be better.\n\nIs there something in an existing Clojure library that would do what I want? If not, where am I going wrong in my function?\n    ", "Answer": "\r\nthat's what i would do: simply delegate this task to a core ```\nreduce```\n function, like this:\n\n```\n(defn multi-reduce\n  ([fs accs xs] (reduce (fn [accs x] (doall (map #(%1 %2 x) fs accs)))\n                        accs xs))\n  ([fs xs] (when (seq xs)\n             (multi-reduce fs (repeat (count fs) (first xs))\n                           (rest xs)))))\n```\n\n\nin repl:\n\n```\nuser> (multi-reduce [+ * min max] (range 1 10))\n(45 362880 1 9)\n\nuser> (multi-reduce [+ * min max] [10])\n(10 10 10 10)\n\nuser> (multi-reduce [+ * min max] [])\nnil\n\nuser> (multi-reduce [+ * min max] [1 1 1000 0] [])\n[1 1 1000 0]\n\nuser> (multi-reduce [+ * min max] [1 1 1000 0] [1])\n(2 1 1 1)\n\nuser> (multi-reduce [+ * min max] [1 1 1000 0] (range 1 10))\n(46 362880 1 9)\n\nuser> (multi-reduce [max min] (range 1000000))\n(999999 0)\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "clojure: reductions: applying function n times, keeping intermediate results\r\n                \r\nProblem Statement\n\nGiven n, x, f:\n    I want output of the form:\n\n```\n[x, f(x), f(f(x)), f(f(f(x))), ..., f^{n-1}(x)]\n```\n\n\nExisting solution\n\nThis can be done via reductions\n\n```\n(reductions\n  (fn [state _] (f state))\n  state\n  (range n))\n```\n\n\nQuestion\n\nIs there a primitive that provides a shorter solution?\n    ", "Answer": "\r\nWhat you want is clojure.core/iterate, which provides ```\nf -> x -> [x, f(x), f^2(x), f^3(x), ...]```\n and clojure.core/take which provides a way to slice the first ```\nn```\n elements off of a sequence. ```\ntake```\n is lazy, as is ```\niterate```\n so there are no guarantees about side-effects.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Julia parallel for loop with two reductions\r\n                \r\nI would like to perform two reductions in a parallel for loop in Julia. I am trying to compute the error in a random forest inside the parallel for loop as each tree is built. Any ideas?\n\nCurrent:\n\n```\nforest = @parallel (vcat) for i in 1:ntrees\n    inds = rand(1:Nlabels, Nsamples)\n    build_tree(labels[inds], features[inds,:], nsubfeatures)\nend\n```\n\n\nWhat I want, intuitively is to do an addition inside this for loop as well to get the out of bag error. This is how I would wish for it to work:\n\n```\nforest, ooberror = @parallel (vcat, +) for i in 1:ntrees\n    inds = rand(1:Nlabels, Nsamples)\n    tree = build_tree(labels[inds], features[inds,:], nsubfeatures)\n    error = geterror(ids, features, tree)\n    (tree, error)\nend\n```\n\n    ", "Answer": "\r\nUsing a type might be best in terms of simplicity and clarity, e.g.\n\n```\ntype Forest\n  trees :: Vector\n  error\nend\njoin(a::Forest, b::Forest) = Forest(vcat(a.trees,b.trees), a.error+b.error)\n\n#...\n\nforest = @parallel (join) for i in 1:ntrees\n    inds  = rand(1:Nlabels, Nsamples)\n    tree  = build_tree(labels[inds], features[inds,:], nsubfeatures)\n    error = geterror(ids, features, tree)\n    Forest(tree, error)\nend\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "polynomial-time mapping reductions for NP\r\n                \r\nI have already proven the transitive property for polynomial-time mapping reductions, that is:\n\n\n  if A is reducible to B and B is reducible to C, then A is reducible to C.\n\n\nSo from this point, knowing that B is NP and A is polynomial-time mapping reducible to B, how should I prove A is also NP?\n    ", "Answer": "\r\nB is in NP, so there is some Turing machine, let's call it M(B), which decides B in polynomial time. Furthermore, because A is polynomial-time reducible to B, there are TMs, let's call them M(R) and M(R'), which transform input instances of A into input instances of B, and outputs of B into outputs of A, both in polynomial time. Consider a TM constructed as follows:\n\n\nExecute M(R) on the input tape and then reset the tape head\nExecute M(B) on the input tape and then reset the tape head\nExecute M(R') on the input tape and then reset the tape head\n\n\nEach of these steps takes polynomial time and so the whole process takes polynomial time. Because nondeterministic Turing machines are closed under concatenation (by replacing halt_accept in the LHS with the initial state of the RHS), the computation can be done by a single nondeterministic Turing machine combining these steps. Thus, A can be decided by a nondeterministic Turing machine in polynomial time - the criterion for inclusion in NP.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Mutation reductions for parallelStream in Java 8\r\n                \r\nJoshua Bloch in ```\n<Effective Java> (Third Edition)```\n mentions that \n\n\n  The operations performed by Stream’s collect method, which are known as   mutable reductions, are not good candidates for parallelism because the overhead of combining collections is costly.\n\n\nI read the docs on Mutable reduction, but I am still not quite sure why reduction is not a good candidate for parallelism. Is it the ```\nsynchronization```\n? \n\nAs @Ravindra Ranwala points out (I also saw this on the Reduction, concurrency, and ordering docs):\n\n\n  It may actually be counterproductive to perform the operation in parallel. This is because the combining step (merging one Map into another by key) can be expensive for some Map implementations.\n\n\nIf so, then are there other major factors we need to care about that might result in low performance?\n    ", "Answer": "\r\nNo it's nothing to do with the ```\nsynchronization```\n. Consider you have a 1 million ```\nPerson```\n objects and need to find out all ```\npeople```\n who live in New York. So a typical stream pipeline would be,\n\n```\npeople.parallelStream()\n    .filter(p -> p.getState().equals(\"NY\"))\n    .collect(Collectors.toList());\n```\n\n\nConsider a parallel execution of this query. Let's say we have 10 threads executing it in parallel. Each thread will accumulate it's own data set into a separate local container. Finally the 10 result containers are merged to form one large container. This merge will be costly and is an additional step introduced by the parallel execution. Hence parallel execution may not always be faster. Some times sequential execution may be faster than it's parallel counter part.\n\nSo always start with a sequential execution. If that makes sense only, you may fall back to it's parallel counterpart at some later point in time.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Haskell algorithm to find all possible Beta reductions\r\n                \r\nI'm trying to come up with an algorithm that would print all available beta reductions for a given expression.\nI know I will need a matching pattern cases to see if item is reducible and if not then the three cases cases for variable, lambda and application. I have custom type for these defined as follows:\n```\ndata Term =\n    Variable Var\n  | Lambda   Var  Term\n  | Apply    Term Term\n  deriving Show\n```\n\nPreviously I've implemented following methods:\nthe functionality of the given methods is as follows:\n\nMerge\nRename renames to avoid variable capturing\nSubstitution reducing the expression\nfresh gives a new variable to which was not previously used\nused returns all used variables for given expression\n\nSo far everything works well, however, the function for returning all possible beta reductions is where I'm lost. I've tried to define the four case matching pattern as shown below, however I'm struggling with the definition of the redex case (especially looking into redex for other redex) and the operations which need to take place in the beta method.:\n```\nbeta :: Term -> [Term]\nbeta (Variable var) = \nbeta (Lambda var term) = \nbeta (Apply term1 term2)\n```\n\nI now don't know how to proceed here in order to get all available reductions. The outcome should be:\n```\n*Main> Apply (x y)\n(\\a. \\x. (\\y. a) x b) (\\f. \\x. f x)\n*Main> beta it\n[\\c. (\\b. \\f. \\x. f x) c b,(\\a. \\x. a b) (\\f. \\x. f x)]\n```\n\n    ", "Answer": "\r\nHere's an informal description, as a hint.\n```\nbeta :: Term -> [Term]\nbeta (Variable var) = ...\n```\n\nVariables don't beta-reduce, so this case should be easy.\n```\nbeta (Lambda var term) = ...\n```\n\nLambdas only reduce if their body reduces. Start with recursing on ```\nterm```\n.\n```\nbeta (Apply term1 term2) = ...\n```\n\nThis is the complex part: we have three subcases here.\n\nIf ```\nterm1```\n reduces, the whole application reduces accordingly.\nIf ```\nterm2```\n reduces, the whole application reduces accordingly.\nIf ```\nterm1```\n happens to be a lambda, you have found a redex. You can substitute the abstracted variables in the lambda body with ```\nterm2```\n.\n\nThe whole code could follow this shape:\n```\nbeta :: Term -> [Term]\nbeta (Variable var) = ...\nbeta (Lambda var term) = ...\nbeta (Apply term1 term2) = term1Moves ++ term2Moves ++ redexMoves\n   where\n   term1Moves = ...\n   term2Moves = ...\n   redexMoves = case term1 of\n      Lambda var term -> ...\n      _               -> []        -- no further moves\n```\n\nList comprehensions could be convenient to use.\n\nTaking a part of the code the OP posted, and adding a few more hints, we get:\n```\nbeta :: Term -> [Term]\nbeta (Variable var) = ...\nbeta (Lambda var term) = ...\nbeta (Apply term1 term2) = term1Moves ++ term2Moves ++ redexMoves\n   where\n   term1Moves = [ Apply term1' term2 | term1' <- beta term1 ]\n   term2Moves = ...\n   redexMoves = case term1 of\n      Lambda var body -> [substitute var term2 body]  -- don't recurse here\n      _               -> []        -- no further moves\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Kolmogorov complexity is uncomputable using reductions\r\n                \r\nCan anyone please give me a proof of K-Complexity is unsolvable using reductions.\n\neg:\nPCP(2) <= PCP(3)\n\nI can prove that PCP(3) is unsolvable by reducing to PCP(2) (by mapping every instance).\nI am not sure how to reduce K-Complexity to another known undecidable problem (like halting problem). i.e., X <= K-Complexity\nCan you please provide me proof for that? At least provide me some idea (X ).\nThanks in Advance\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Regarding OpenMP Parallel SIMD Reductions\r\n                \r\nI have a rather simple for loop summing a very large array of double values ```\nx```\n (100 mio data points) in C. I want to do this in parallel with SIMD reductions, using a specified number of threads. The OpenMP instruction in my reading should be:\n```\nint nthreads = 4, l = 1e8;\ndouble sum = 0.0;\n\n#pragma omp parallel for simd num_threads(nthreads) reduction(+:sum)\nfor (int i = 0; i < l; ++i) sum += x[i];\n\n```\n\nThis however gives a compiler warning\n```\nloop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\n```\n\nand running it with multiple threads is slower than single threaded. I'm using the Apple M1 Mac with ```\nclang```\n (```\nXclang```\n) v13.0.0 compiler. What I would like to know is: is this an issue with my system or is there actually something wrong / infeasible with this OpenMP instruction?\n    ", "Answer": "\r\nThis compiles without warning on clang >= 15, but performance depends on the system. With the Apple M1 it seems that multithreading does not add much to the SIMD vectorization and single threaded execution with a ```\n#pragma omp simd reduction(+:sum)```\n instruction is about as good as it gets.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "C++ Eigen reductions are slower than simple loop\r\n                \r\nGiven two 3xK matrices I wish to compute the squared average of column by column dot products.\nThis can be accomplished with a simple loop:\n\n```\nEigen::Matrix<float,3,Eigen::Dynamic> L(3,K);\nEigen::Matrix<float,3,Eigen::Dynamic> P(3,K);\n\nfloat distance = 0;\nfor (int q = 0; q < K; q++){\n    const Eigen::Vector3f& line = L.col(q);\n    const Eigen::Vector3f& point = P.col(q);\n    const float d = line.dot(point);\n    distance += d*d;\n }\n const float residual2 = distance / K;\n```\n\n\nwhich outperforms (```\ng++ -O3 -DNDEBUG```\n) the fancier reduction techniques, e.g.:\n\n```\n const float residual2 = (L.array() * P.array()).colwise().sum().square().mean();\n const float residual2 = L.cwiseProduct(P).array().colwise().sum().array().square().mean();\n const float residual2 = (L.transpose() * P).diagonal().array().square().mean();\n```\n\n\nPerhaps there is something I am missing here. Shouldn't the reductions be faster?\n\nEdit: Using K = 20.\nI perform each of the above 100*632*631 times with the loop version taking about 1200 msec while the others would take around 2000 msec. 3.2 GHz Intel Core i5, MacOS, clang++ -O3\n\nEdit2: Created a small test program. Adding ```\n-NDEBUG```\n for compiling made a huge difference (I thought you got this for free with ```\n-O3```\n). The loop version is significantly faster than\nthe reductions:\n\n```\n./eigentest\nCASE 1: 12 milliseconds\nsolution = 1482.5\nCASE 2: 835 milliseconds\nsolution = 1482.5\nCASE 3: 849 milliseconds\nsolution = 1482.5\nCASE 4: 843 milliseconds\nsolution = 1482.5\n```\n\n\nEdit3: I think the test above is crap since the compiler unrolled the loop.... sigh... I'll get back to this soon...\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Not returning all possible one-step reductions for combinatory expressions\r\n                \r\nI'm trying to implement combinatory logic in Haskell, and am currently writing a function step, which returns a list of all possible one step reductions using the standard reduction rules. When running the step function on the variable S (S I) (K I) (S I K) I, it returns the correct expression for the first reduction, but when I reduce it again with step (head it) it returns only one of the possible reductions.\nThis is my code\n```\nmodule Main where\n\n data Combinator\n  = I\n  | K\n  | S\n  | V String\n  | Combinator :@ Combinator\n  deriving (Eq, Ord)\n\ninstance Show Combinator where\n  show = f 0\n    where\n      f _ I = \"I\"\n      f _ K = \"K\"\n      f _ S = \"S\"\n      f _ (V s) = s\n      f i (c :@ d) = if i == 1 then \"(\" ++ s ++ \")\" else s where s = f 0 c ++ \" \" ++ f 1 d\n\nstep :: Combinator -> [Combinator]\nstep (I :@ x) = [x]\nstep (K :@ x :@ y) = [x]\nstep (S :@ x :@ y :@ z) = [x :@ z :@ (y :@ z)]\nstep (c :@ d) = [c' :@ d | c' <- step c] ++ [c :@ d' | d' <- step d] ++ \n[c' :@ d' | c' <- step            c, d' <- step d]\nstep _ = []\n\nparse :: String -> Combinator\nparse s = down [] s\n  where\n    down [] (' ' : str) = down [] str\n    down cs ('(' : str) = down (Nothing : cs) str\n    down cs ('I' : str) = up cs I str\n    down cs ('K' : str) = up cs K str\n    down cs ('S' : str) = up cs S str\n    down cs (c : str) = up cs (V [c]) str\n    up [] c [] = c\n    up (Just c : cs) d str = up cs (c :@ d) str\n    up (Nothing : cs) d (')' : str) = up cs d str\n    up cs d str = down (Just d : cs) str\n\nmain :: IO ()\nmain = do\n  putStrLn $ \"Combinatory Logic in Haskell\"\n```\n\nI am aiming for this when running in GHCi-\n```\n*Main> parse \"S(SI)(KI)(SIK)I\"\nS (S I) (K I) (S I K) I\n*Main> step it\n[S I (S I K) (K I (S I K)) I]\n*Main> step (head it)\n[ I (K I (S I K)) (S I K (K I (S I K))) I, S I (S I K) I I ]\n\nI am getting -\n\nghci> parse \"S(SI)(KI)(SIK)I\"\nS (S I) (K I) (S I K) I\nghci> step it\n[S I (S I K) (K I (S I K)) I]\nghci> step (head it)\n[I (K I (S I K)) (S I K (K I (S I K))) I]\n```\n\n    ", "Answer": "\r\n\n```\n[ I (K I (S I K)) (S I K (K I (S I K))) I, S I (S I K) I I ]```\n\n\nThis implies ```\nstep ((S :@ x :@ y :@ z) :@ I)```\n should produce ```\n[s :@ I | s <- step (S :@ x :@ y :@ z)]```\n, which it does, by the last equation of ```\nstep```\n. However, if you want to explore only one step at a time, you should skip the ```\n[c' :@ d' | …]```\n part of this equation, since it makes two steps in parallel.\nIt also implies ```\nstep (S :@ x :@ y :@ z)```\n should produce ```\n[S :@ x :@ y :@ z' | z' <- step z]```\n, yet it doesn’t, because this is already matched by the third (S) equation of ```\nstep```\n.\nIn other words, your evaluator is slightly too lazy: it omits some of the steps that would be done by eager evaluation of arguments, since it only evaluates arguments if a call isn’t fully saturated.\nOne possible solution is to always visit each subexpression, even if reducing the whole application is possible; and separately consider whether the whole expression is reducible. This is perhaps easier to get right if we make ```\nstep```\n always step exactly once, returning an empty list if there is no single-step reduction.\n```\nstep (f :@ z) = outer ++ left ++ right\n  where\n\n    --       I   z ~> z\n    -- (  K   y) z ~> y\n    -- ((S x) y) z ~> xz(yz)\n    outer = case f of\n      I -> [z]\n      K :@ y -> [y]\n      S :@ x :@ y -> [(x :@ z) :@ (y :@ z)]\n      _ -> []\n\n    --   f ~> f'\n    -- -----------\n    -- f z ~> f' z\n    left = [f' :@ z | f' <- step f]\n\n    --   z ~> z'\n    -- -----------\n    -- f z ~> f z'\n    right = [f :@ z' | z' <- step z]    \n\nstep _ = []\n```\n\nNow ```\nstep```\n returns the result you expected, and you can compute the sequence of reductions using ```\niterate```\n and ```\nconcatMap```\n (or ```\n=<<```\n).\n```\nλ import Data.List (nub)\n\nλ reductions input = iterate (nub . concatMap step) [parse input]\n\nλ import Data.Foldable (traverse_)\n\nλ traverse_ print $ takeWhile (not . null) $ reductions \"K(I(K(Ia)))bc\"\n\n[K (I (K (I a))) b c]\n[I (K (I a)) c,K (K (I a)) b c,K (I (K a)) b c]\n[K (I a) c,I (K a) c,K (K a) b c]\n[I a,K a c]\n[a]\n```\n\nI use ```\nnub```\n here for illustration, to filter out duplicate solutions (confluent reductions), but I should point out that it’s very inefficient—not only because of ```\nnub```\n itself, but also because of generating terms that will be discarded.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Need help understanding OpenCL reductions\r\n                \r\nI have been reading the following note:\nhttp://developer.amd.com/resources/documentation-articles/articles-whitepapers/opencl-optimization-case-study-simple-reductions/\n\nThe following kernel should reduce a chunk of data and theres one part of it i simply dont understand.\n\n```\nwhile (global_index < length) ....  global_index += get_global_size(0)\n```\n\n\nI belived that it was smarter to read data from global storage that was sequential layed out. Meaning reading data at k, k+1, k+2 is faster then reading k+1000, k+2000, k+3000. Is this not what they are doing when saying global_index += get_global_size(0) ?\n\n```\n__kernel\nvoid reduce(__global float* buffer,\n            __local float* scratch,\n            __const int length,\n            __global float* result) {\n\n  int global_index = get_global_id(0);\n  float accumulator = INFINITY;\n  // Loop sequentially over chunks of input vector\n  while (global_index < length) {\n    float element = buffer[global_index];\n    accumulator = (accumulator < element) ? accumulator : element;\n    global_index += get_global_size(0);\n  }\n\n  // Perform parallel reduction\n  int local_index = get_local_id(0);\n  scratch[local_index] = accumulator;\n  barrier(CLK_LOCAL_MEM_FENCE);\n  for(int offset = get_local_size(0) / 2;\n      offset > 0;\n      offset = offset / 2) {\n    if (local_index < offset) {\n      float other = scratch[local_index + offset];\n      float mine = scratch[local_index];\n      scratch[local_index] = (mine < other) ? mine : other;\n    }\n    barrier(CLK_LOCAL_MEM_FENCE);\n  }\n  if (local_index == 0) {\n    result[get_group_id(0)] = scratch[0];\n  }\n}\n```\n\n    ", "Answer": "\r\nWork items 0,1,2,3,... will first read buffer indices 0,1,2,3,... in parallel (this is generally the best case for memory access), and then 1000,1001,1002,1003,... in parallel, etc.\n\nRemember that each instruction in the kernel code will be executed \"in parallel\" by all work-items.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "A language that is RE complete with respect to polynomial-time reductions?\r\n                \r\nIs there any language in RE that is complete with regard to polynomial-time reductions?\n\nI think that A_TM will be a good example,but I'm not sure...\n    ", "Answer": "\r\nYes, ATM is RE-complete with respect to polynomial-time reductions. Given any RE language L, let M be a recognizer for it. Then the function f(w) =  can be computed in polynomial time (for some reasonable representation of tuples) because M is a fixed machine and the length of w in the encoded version should certainly be at most polynomially larger than the original input w. We also have that w &in; L if and only if M accepts w if and only if  &in; ATM, so f is a polynomial-time reduction from an arbitrary RE language L to ATM, making ATM RE-complete with respect to polynomial-time reductions.\n\nI'm not sure why you'd be interested in this particular notion of RE-completeness, since RE is mostly useful for notions of computability (can you solve this problem at all?) while polynomial-time reductions are usually for complexity (can you solve this problem efficiently?) If you do have an interesting use case for them, though, I'd love to hear about it!\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reductions in parallel in logarithmic time\r\n                \r\nGiven ```\nn```\n partial sums it's possible to sum all the partial sums in log2 parallel steps.  For example assume there are eight threads with eight partial sums: ```\ns0, s1, s2, s3, s4, s5, s6, s7```\n.  This could be reduced in ```\nlog2(8) = 3```\n sequential steps like this;\n\n```\nthread0     thread1    thread2    thread4\ns0 += s1    s2 += s3   s4 += s5   s6 +=s7\ns0 += s2    s4 += s6\ns0 += s4\n```\n\n\nI would like to do this with OpenMP but I don't want to use OpenMP's ```\nreduction```\n clause.  I have come up with a solution but I think a better solution can be found maybe using OpenMP's ```\ntask```\n clause.\n\nThis is more general than scalar addition. Let me choose a more useful case: an array reduction (see here, here, and here for more about array reductions).  \n\nLet's say I want to do an array reduction on an array ```\na```\n. Here is some code which fills private arrays in parallel for each thread.\n\n```\nint bins = 20;\nint a[bins];\nint **at;  // array of pointers to arrays\nfor(int i = 0; i<bins; i++) a[i] = 0;\n#pragma omp parallel\n{\n    #pragma omp single   \n    at = (int**)malloc(sizeof *at * omp_get_num_threads());        \n    at[omp_get_thread_num()] = (int*)malloc(sizeof **at * bins);\n    int a_private[bins];\n    //arbitrary function to fill the arrays for each thread\n    for(int i = 0; i<bins; i++) at[omp_get_thread_num()][i] = i + omp_get_thread_num();\n}\n```\n\n\nAt this point I have have an array of pointers to arrays for each thread. Now I want to add all these arrays together and write the final sum to ```\na```\n.  Here is the solution I came up with.\n\n```\n#pragma omp parallel\n{\n    int n = omp_get_num_threads();\n    for(int m=1; n>1; m*=2) {\n        int c = n%2;\n        n/=2;\n        #pragma omp for\n        for(int i = 0; i<n; i++) {\n            int *p1 = at[2*i*m], *p2 = at[2*i*m+m];\n            for(int j = 0; j<bins; j++) p1[j] += p2[j];\n        }\n        n+=c;\n    }\n    #pragma omp single\n    memcpy(a, at[0], sizeof *a*bins);\n    free(at[omp_get_thread_num()]);\n    #pragma omp single\n    free(at);\n}\n```\n\n\nLet me try and explain what this code does. Let's assume there are eight threads.  Let's define the ```\n+=```\n operator to mean to sum over the array.  e.g. ```\ns0 += s1```\n is\n\n```\nfor(int i=0; i<bins; i++) s0[i] += s1[i]\n```\n\n\nthen this code would do\n\n```\nn   thread0     thread1    thread2    thread4\n4   s0 += s1    s2 += s3   s4 += s5   s6 +=s7\n2   s0 += s2    s4 += s6\n1   s0 += s4\n```\n\n\nBut this code is not ideal as I would like it. \n\nOne problem is that there are a few implicit barriers which require all the threads to sync.  These barriers should not be necessary. The first barrier is between filling the arrays and doing the reduction.  The second barrier is  in the ```\n#pragma omp for```\n declaration in the reduction.  But I can't use the ```\nnowait```\n clause with this method to remove the barrier.  \n\nAnother problem is that there are several threads that don't need to be used. For example with eight threads. The first step in the reduction only needs four threads, the second step two threads, and the last step only one thread. However, this method would involve all eight threads in the reduction. Although, the other threads don't do much anyway and should go right to the barrier and wait so it's probably not much of an issue.\n\nMy instinct is that a better method can be found using the omp ```\ntask```\n clause. Unfortunately I have little experience with the ```\ntask```\n clause and all my efforts so far with it do a reduction better than what I have now have failed.\n\nCan someone suggest a better solution to do the reduction in logarithmic time using e.g. OpenMP's ```\ntask```\n clause?\n\n\n\nI found a method which solves the barrier problem. This reduces asynchronously. The only remaining problem is that it still puts threads which don't participate in the reduction into a busy loop.  This method uses something like a stack to push pointers to the stack (but never pops them) in critical sections (this was one of the keys as critical sections don't have implicit barriers.  The stack is operated on serially but the reduction in parallel.\n\nHere is a working example.\n\n```\n#include <stdio.h>\n#include <omp.h>\n#include <stdlib.h>\n#include <string.h>\n\nvoid foo6() {\n    int nthreads = 13;\n    omp_set_num_threads(nthreads);\n    int bins= 21;\n    int a[bins];\n    int **at;\n    int m = 0;\n    int nsums = 0;\n    for(int i = 0; i<bins; i++) a[i] = 0;\n    #pragma omp parallel\n    {\n        int n = omp_get_num_threads();\n        int ithread = omp_get_thread_num();\n        #pragma omp single\n        at = (int**)malloc(sizeof *at * n * 2);\n        int* a_private = (int*)malloc(sizeof *a_private * bins);\n\n        //arbitrary fill function\n        for(int i = 0; i<bins; i++) a_private[i] = i + omp_get_thread_num();\n\n        #pragma omp critical (stack_section)\n        at[nsums++] = a_private;\n\n        while(nsums<2*n-2) {\n            int *p1, *p2;\n            char pop = 0;\n            #pragma omp critical (stack_section)\n            if((nsums-m)>1) p1 = at[m], p2 = at[m+1], m +=2, pop = 1;\n            if(pop) {\n                for(int i = 0; i<bins; i++) p1[i] += p2[i];\n                #pragma omp critical (stack_section)\n                at[nsums++] = p1;\n            }\n        }\n\n        #pragma omp barrier\n        #pragma omp single\n        memcpy(a, at[2*n-2], sizeof **at *bins);\n        free(a_private);\n        #pragma omp single\n        free(at);\n    }\n    for(int i = 0; i<bins; i++) printf(\"%d \", a[i]); puts(\"\");\n    for(int i = 0; i<bins; i++) printf(\"%d \", (nthreads-1)*nthreads/2 +nthreads*i); puts(\"\");\n}\n\nint main(void) {\n    foo6();\n}\n```\n\n\nI sill feel a better method may be found using tasks which does not put the threads not being used in a busy loop.\n    ", "Answer": "\r\nActually, it is quite simple to implement that cleanly with tasks using a recursive divide-and-conquer approach. This is almost textbook code.\n\n```\nvoid operation(int* p1, int* p2, size_t bins)\n{\n    for (int i = 0; i < bins; i++)\n        p1[i] += p2[i];\n}\n\nvoid reduce(int** arrs, size_t bins, int begin, int end)\n{\n    assert(begin < end);\n    if (end - begin == 1) {\n        return;\n    }\n    int pivot = (begin + end) / 2;\n    /* Moving the termination condition here will avoid very short tasks,\n     * but make the code less nice. */\n#pragma omp task\n    reduce(arrs, bins, begin, pivot);\n#pragma omp task\n    reduce(arrs, bins, pivot, end);\n#pragma omp taskwait\n    /* now begin and pivot contain the partial sums. */\n    operation(arrs[begin], arrs[pivot], bins);\n}\n\n/* call this within a parallel region */\n#pragma omp single\nreduce(at, bins, 0, n);\n```\n\n\nAs far as i can tell, there are no unnecessary synchronizations and there is no weird polling on critical sections. It also works naturally with a data size different than your number of ranks. I find it very clean and easy to understand. So I do indeed think this is better than both of your solutions.\n\nBut let's look at how it performs in practice*. For that we can use Score-p and Vampir:\n\n*```\nbins=10000```\n so the reduction actually takes a little bit of time. Executed on a 24-core Haswell system w/o turbo. gcc 4.8.4, ```\n-O3```\n. I added some buffer around the actual execution to hide initialization/post-processing\n\n\n\nThe picture reveals what is happening at any thread within the application on a horizontal time-axis. The tree implementations from top to bottom:\n\n\n```\nomp for```\n loop\n```\nomp critical```\n kind of tasking.\n```\nomp task```\n\n\n\nThis shows nicely how the specific implementations actually execute. Now it seems that the for loop is actually the fastest, despite the unnecessary synchronizations. But there are still a number of flaws in this performance analysis. For example, I didn't pin the threads. In practice NUMA (non-uniform memory access) matters a lot: Does the core does have this data in it's own cache / memory of it's own socket? This is where the task solution becomes non-deterministic. The very significant variance among repetitions is not considered in the simple comparison.\n\nIf the reduction operation becomes variable in runtime, then the task solution will become better than thy synchronized for loop.\n\nThe ```\ncritical```\n solution has some interesting aspect, the passive threads are not continuously waiting, so they will more likely consume CPU resources. This can be bad for performance e.g. in case of turbo mode.\n\nRemember that the ```\ntask```\n solution has more optimization potential by avoiding spawning tasks that immediately return. How these solutions perform also highly depends on the specific OpenMP runtime. Intel's runtime seems to do much worse for tasks.\n\nMy recommendation is:\n\n\nImplement the most maintainable solution with optimal algorithmic\ncomplexity\nMeasure which parts of the code actually matter for run-time\nAnalyze based on actual measurements what is the bottleneck. In my experience it is more about NUMA and scheduling rather than some unnecessary barrier.\nPerform the micro-optimization based on your actual measurements\n\n\nLinear solution\n\nHere is the timeline for the linear ```\nproccess_data_v1```\n from this question.\n\n\n\nOpenMP 4 Reduction\n\nSo I thought about OpenMP reduction. The tricky part seems to be getting the data from the ```\nat```\n array inside the loop without a copy. I do initialize the worker array with ```\nNULL```\n and simply move the pointer the first time:\n\n```\nvoid meta_op(int** pp1, int* p2, size_t bins)\n{\n    if (*pp1 == NULL) {\n        *pp1 = p2;\n        return;\n    }\n    operation(*pp1, p2, bins);\n}\n\n// ...\n\n// declare before parallel region as global\nint* awork = NULL;\n\n#pragma omp declare reduction(merge : int* : meta_op(&omp_out, omp_in, 100000)) initializer (omp_priv=NULL)\n\n#pragma omp for reduction(merge : awork)\n        for (int t = 0; t < n; t++) {\n            meta_op(&awork, at[t], bins);\n        }\n```\n\n\nSurprisingly, this doesn't look too good:\n\n\n\ntop is ```\nicc 16.0.2```\n, bottom is ```\ngcc 5.3.0```\n, both with ```\n-O3```\n.\n\nBoth seem to implement the reduction serialized. I tried to look into ```\ngcc```\n / ```\nlibgomp```\n, but it's not immediately apparent to me what is happening. From intermediate code / disassembly, they seem to be wrapping the final merge in a ```\nGOMP_atomic_start```\n/```\nend```\n - and that seems to be a global mutex. Similarly ```\nicc```\n wraps the call to the ```\noperation```\n in a ```\nkmpc_critical```\n. I suppose there wasn't much optimization going into costly custom reduction operations. A traditional reduction can be done with a hardware-supported atomic operation. \n\nNotice how each ```\noperation```\n is faster because the input is cached locally, but due to the serialization it is overall slower. Again this is not a perfect comparison due to high variances, and earlier screenshots were with different ```\ngcc```\n version. But the trend is clear, and I also have data on the cache effects.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "where does OpenSSL perform it's modular reductions on binary finite fields?\r\n                \r\nOpenSSL performs modular reductions when doing RSA but in order to maximize it's efficiency for elliptic curves over finite fields it'd need to do modular reduction for those as well.\n\nI'm curious in what file in the OpenSSL source code does OpenSSL perform these modular reductions. I'm specifically interested in where it does it for binary finite fields but knowing where it does it for prime finite fields might give me insight into where it's doing it for binary finite fields.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Numpy reductions over successive non-contiguous slices\r\n                \r\nSay I have two numpy arrays, ```\nA```\n of shape ```\n(d, f)```\n and ```\nI```\n of shape ```\n(d,)```\n containing indices in ```\n0..n```\n, e.g.\n\n```\nI = np.array([0, 0, 1, 0, 2, 1])\nA = np.arange(12).reshape(6, 2)\n```\n\n\nI am looking for a fast way to make reductions, in particular ```\nsum```\n, ```\nmean```\n and ```\nmax```\n, over all the slices ```\nA[I == i, :]```\n; a slow version would be\n\n```\nresults = np.zeros((I.max() + 1, A.shape[1]))\nfor i in np.unique(I):\n    results[i, :] = np.mean(A[I == i, :], axis=0)\n```\n\n\nwhich gives in this case\n\n```\nresults = [[ 2.66666667,  3.66666667],\n           [ 7.        ,  8.        ],\n           [ 8.        ,  9.        ]])\n```\n\n\n\n\nEDIT: I did some timings based on Divakar's answer and a previous poster's (deleted) ```\npandas```\n-based answer.\n\nTiming code:\n\n```\nfrom __future__ import division, print_function\nimport numpy as np, pandas as pd\nfrom time import time\n\nnp.random.seed(0)\nd = 500000\nf = 500\nn = 500\nI = np.hstack((np.arange(n), np.random.randint(n, size=(d - n,))))\nnp.random.shuffle(I)\nA = np.random.rand(d, f)\n\ndef reduce_naive(A, I, op=\"avg\"):\n    target_dtype = (np.float if op==\"avg\" else A.dtype)\n    results = np.zeros((I.max() + 1, A.shape[1]), dtype=target_dtype)\n    npop = {\"avg\": np.mean, \"sum\": np.sum, \"max\": np.max}.get(op)\n    for i in np.unique(I):\n        results[i, :] = npop(A[I == i, :], axis=0)\n    return results\n\ndef reduce_reduceat(A, I, op=\"avg\"):\n    sidx = I.argsort()\n    sI = I[sidx]\n    sortedA = A[sidx]\n    idx = np.r_[ 0, np.flatnonzero(sI[1:] != sI[:-1])+1 ]\n    if op == \"max\":\n        return np.maximum.reduceat(sortedA, idx, axis=0)\n    sums = np.add.reduceat(sortedA, idx, axis=0)\n    if op == \"sum\":\n        return sums\n    if op == \"avg\":\n        count = np.r_[idx[1:] - idx[:-1], A.shape[0] - idx[-1]]\n        return sums/count.astype(float)[:,None]\n\ndef reduce_bincount(A, I, op=\"avg\"):\n    ids = (I[:,None] + (I.max()+1)*np.arange(A.shape[1])).ravel()\n    sums = np.bincount(ids, A.ravel()).reshape(A.shape[1],-1).T\n    if op == \"sum\":\n        return sums\n    if op == \"avg\":\n        return sums/np.bincount(ids).reshape(A.shape[1],-1).T\n\ndef reduce_pandas(A, I, op=\"avg\"):\n    group = pd.concat([pd.DataFrame(A), pd.DataFrame(I, columns=(\"i\",))\n                     ], axis=1\n                    ).groupby('i')\n    if op == \"sum\":\n        return group.sum().values\n    if op == \"avg\":\n        return group.mean().values\n    if op == \"max\":\n        return group.max().values\n\ndef reduce_hybrid(A, I, op=\"avg\"):\n    sidx = I.argsort()\n    sI = I[sidx]\n    sortedA = A[sidx]\n\n    idx = np.r_[ 0, np.flatnonzero(sI[1:] != sI[:-1])+1 ]\n    unq_sI = sI[idx]    \n\n    m = I.max()+1\n    N = A.shape[1]\n\n    target_dtype = (np.float if op==\"avg\" else A.dtype)\n    out = np.zeros((m,N),dtype=target_dtype)\n    ss_idx = np.r_[idx,A.shape[0]]\n\n    npop = {\"avg\": np.mean, \"sum\": np.sum, \"max\": np.max}.get(op)\n    for i in range(len(idx)):\n        out[unq_sI[i]] = npop(sortedA[ss_idx[i]:ss_idx[i+1]], axis=0)\n    return out\n\nfor op in (\"sum\", \"avg\", \"max\"):\n    for name, method in ((\"naive   \", reduce_naive), \n                         (\"reduceat\", reduce_reduceat),\n                         (\"pandas  \", reduce_pandas),\n                         (\"bincount\", reduce_bincount),\n                         (\"hybrid  \", reduce_hybrid)\n                         (\"numba   \", reduce_numba)\n                        ):    \n        if op == \"max\" and name == \"bincount\":\n            continue\n        # if name is not \"naive\":\n        #      assert np.allclose(method(A, I, op), reduce_naive(A, I, op))\n        times = []\n        for tries in range(3):\n            time0 = time(); method(A, I, op)\n            times.append(time() - time0); \n        print(name, op, \"{:.2f}\".format(np.min(times)))\n    print()\n```\n\n\nTimings:\n\n```\nnaive    sum 1.10\nreduceat sum 4.62\npandas   sum 5.29\nbincount sum 1.54\nhybrid   sum 0.62\nnumba    sum 0.31\n\nnaive    avg 1.12\nreduceat avg 4.45\npandas   avg 5.23\nbincount avg 2.43\nhybrid   avg 0.61\nnumba    avg 0.33\n\nnaive    max 1.19\nreduceat max 3.18\npandas   max 5.24\nhybrid   max 0.72\nnumba    max 0.34\n```\n\n\n(I chose ```\nd```\n and ```\nn```\n as typical values for my use case - I have added the code for numba-versions in my answer).\n    ", "Answer": "\r\nApproach #1 : Using NumPy ufunc reduceat\n\nWe have ```\nufuncs```\n for those three reduction operations and luckily we also have  ```\nufunc.reduceat```\n to perform those reductions limited at particular intervals along an axis. So, using those, we would have those three operations computed like so -\n\n```\n# Gives us sorted array based on input indices I and indices at which the\n# sorted array should be interval-limited for reduceat operations to be\n# applied later on using those results\ndef sorted_array_intervals(A, I):\n    # Compute sort indices for I. To be later used for sorting A based on it.\n    sidx = I.argsort()\n    sI = I[sidx]\n    sortedA = A[sidx]\n\n    # Get indices at which intervals change. Also, get count in each interval\n    idx = np.r_[ 0, np.flatnonzero(sI[1:] != sI[:-1])+1 ]\n    return sortedA, idx\n\n# Groupby sum reduction using the interval indices \n# to perform interval-limited ufunc reductions\ndef groupby_sum(A, I):\n    sortedA, idx = sorted_array_intervals(A,I)\n    return np.add.reduceat(sortedA, idx, axis=0)\n\n# Groupby mean reduction\ndef groupby_mean(A, I):\n    sortedA, idx = sorted_array_intervals(A,I)\n    sums = np.add.reduceat(sortedA, idx, axis=0)\n    count = np.r_[idx[1:] - idx[:-1], A.shape[0] - idx[-1]]\n    return sums/count.astype(float)[:,None]\n\n# Groupby max reduction\ndef groupby_max(A, I):\n    sortedA, idx = sorted_array_intervals(A,I)\n    return np.maximum.reduceat(sortedA, idx, axis=0)\n```\n\n\nThus, if we need all those operations, we could reuse with one instance of ```\nsorted_array_intervals```\n, like so -\n\n```\ndef groupby_sum_mean_max(A, I):\n    sortedA, idx = sorted_array_intervals(A,I)\n    sums = np.add.reduceat(sortedA, idx, axis=0)\n    count = np.r_[idx[1:] - idx[:-1], A.shape[0] - idx[-1]]\n    avgs = sums/count.astype(float)[:,None]\n    maxs = np.maximum.reduceat(sortedA, idx, axis=0)\n    return sums, avgs, maxs\n```\n\n\nApproach #1-B : Hybrid version (sort + slice + reduce)\n\nHere's a hybrid version that does takes help from ```\nsorted_array_intervals```\n to get the sorted array and the indices at which the intervals change into the next group, but at the last stage uses slicing to sum each interval and does this iteratively for each of the groups. The slicing helps here as we are working with ```\nviews```\n.\n\nThe implementation would look something like this -\n\n```\ndef reduce_hybrid(A, I, op=\"avg\"):\n    sidx = I.argsort()\n    sI = I[sidx]\n    sortedA = A[sidx]\n\n    # Get indices at which intervals change. Also, get count in each interval\n    idx = np.r_[ 0, np.flatnonzero(sI[1:] != sI[:-1])+1 ]\n    unq_sI = sI[idx]    \n\n    m = I.max()+1\n    N = A.shape[1]\n\n    target_dtype = (np.float if op==\"avg\" else A.dtype)\n    out = np.zeros((m,N),dtype=target_dtype)\n    ss_idx = np.r_[idx,A.shape[0]]\n\n    npop = {\"avg\": np.mean, \"sum\": np.sum, \"max\": np.max}.get(op)\n    for i in range(len(idx)):\n        out[unq_sI[i]] = npop(sortedA[ss_idx[i]:ss_idx[i+1]], axis=0)\n    return out\n```\n\n\nRuntime test (Using the setup from the benchmarks posted in the question) -\n\n```\nIn [432]: d = 500000\n     ...: f = 500\n     ...: n = 500\n     ...: I = np.hstack((np.arange(n), np.random.randint(n, size=(d - n,))))\n     ...: np.random.shuffle(I)\n     ...: A = np.random.rand(d, f)\n     ...: \n\nIn [433]: %timeit reduce_naive(A, I, op=\"sum\")\n     ...: %timeit reduce_hybrid(A, I, op=\"sum\")\n     ...: \n1 loops, best of 3: 1.03 s per loop\n1 loops, best of 3: 549 ms per loop\n\nIn [434]: %timeit reduce_naive(A, I, op=\"avg\")\n     ...: %timeit reduce_hybrid(A, I, op=\"avg\")\n     ...: \n1 loops, best of 3: 1.04 s per loop\n1 loops, best of 3: 550 ms per loop\n\nIn [435]: %timeit reduce_naive(A, I, op=\"max\")\n     ...: %timeit reduce_hybrid(A, I, op=\"max\")\n     ...: \n1 loops, best of 3: 1.14 s per loop\n1 loops, best of 3: 631 ms per loop\n```\n\n\n\n\nApproach #2 : Using NumPy bincount\n\nHere's another approach using ```\nnp.bincount```\n that does bin-based summing. So, with it, we could compute the sum and average values and also avoid sorting in the process, like so -\n\n```\nids = (I[:,None] + (I.max()+1)*np.arange(A.shape[1])).ravel()\nsums = np.bincount(ids, A.ravel()).reshape(A.shape[1],-1).T\navgs = sums/np.bincount(ids).reshape(A.shape[1],-1).T\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Does gcc-via-nvcc vectorize these sum and max reductions?\r\n                \r\nI have a CUDA application that currently uses the thrust library to do sum and max reductions of a vector on the GPU.  I'm finding that for certain vector lengths it is much faster if I send the vector back to the host and compute the sum and max reductions in C++.\n\nThe sum and max reductions should be vectorizable on the host.  The memory on the host is linear/contiguous and the compiler I'm using (GCC) supports it.  Given the timings I'm seeing, it seems that the compiler is vectorizing the code, but how do I confirm this?  I don't have any experience with forcing compiler optimizations, but I understand there are some pragma statements that could be used.  (You'll find little information by googling it though.)  Also, I would rather not go digging through assembly to confirm, because I won't understand it.  Is there a compiler setting (in GCC or NVCC) that I can use to force vectorization on the host or find confirmation that the code is being vectorized?\n\nThe functions I've written for the sum and max reductions are as follows.  The nvcc compiler is ultimately compiling this, as the functions contain CUDA code.\n\n```\nvoid calc_vector_max_host(double& maxval, double *const vec_h, const double *const vec_d, int len)\n{\n    //copy device vector to host\n    gpuErrchk(cudaMemcpy(vec_h, vec_d, len*sizeof(double), cudaMemcpyDeviceToHost));\n\n    //vectorized? max\n    maxval = *vec_h;\n    double* temp = vec_h;\n    for(int i = 1; i < len; i++, temp++)\n    {\n        if(*temp > maxval)\n        {\n            maxval = *temp;\n        }\n    }\n}\n\nvoid calc_vector_sum_host(double& sum, double *const vec_h, const double *const vec_d, int len)\n{\n    //copy device vector to host\n    gpuErrchk(cudaMemcpy(vec_h, vec_d, len*sizeof(double), cudaMemcpyDeviceToHost));\n\n    //vectorized? sum\n    sum = 0.0;\n    double* temp = vec_h;\n    for(int i = 0; i < len; i++, temp++)\n    {\n        sum += *temp;\n    }\n}\n```\n\n\nEDIT: the following contains the corrections needed for gcc to auto-vectorize.  The compiler options listed in the comments were also needed.\n\n```\nvoid calc_vector_max_host(double& maxval, double *const __restrict__ vec_h, const double *const __restrict__ vec_d, int len)\n{\n    //copy device vector to host\n    gpuErrchk(cudaMemcpy(vec_h, vec_d, len*sizeof(double), cudaMemcpyDeviceToHost));\n\n    //vectorized? max\n    double local_maxval = vec_h[0];\n    for(int i = 1; i < len; i++)\n    {\n        double val = vec_h[i];\n        if(val > local_maxval)\n        {\n            local_maxval = val;\n        }\n    }\n    maxval = local_maxval;\n}\n\nvoid calc_vector_sum_host(double& sum, double *const __restrict__ vec_h, const double *const vec_d, int len)\n{\n    //copy device vector to host\n    gpuErrchk(cudaMemcpy(vec_h, vec_d, len*sizeof(double), cudaMemcpyDeviceToHost));\n\n    //vectorized? sum\n    double local_sum = 0.0;\n    for(int i = 0; i < len; i++)\n    {\n        local_sum += vec_h[i];\n    }\n    sum = local_sum;\n}\n```\n\n    ", "Answer": "\r\nThe first, and most important point to make is that nvcc isn't a compiler. It is a compiler driver -- it just steers compilation using the host compiler in combination with a couple of custom preprocessing tools which break out the actual GPU code and pass it off to the GPU toolchain. Only a small portion of a typical program gets touched by the GPU compiler and assembler. The rest of the build is done directly with the host compiler and linker.\n\nSo all of the code you have posted is compiled by gcc (and could be compiled directly without the use of nvcc). nvcc has an option ```\n-Xcompiler```\n which can be used to pass whatever options you want to the host compilation trajectory. For vectorisation, you can pass any of the options described here that your version of gcc supports. You could also use SSE style intrinsics directly to make the compiler's job easier, if you are that way inclined\n\nTo see whether vectorisation is already happening in your host code, just use something like objdump/otool (depending on whether you are using Linux or OS X, you haven't said). You will be able to look at a disassembly of the code the compiler emitted, and the presence of vectorised instructions will immediately answer your question.\n\nFinally, it is worth nothing that nvcc has pretty good  documentation these days and you can find the answer to this and probably every other question you have ever had regarding nvcc by familiarising yourself with it.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reductions in programming\r\n                \r\nSometimes to make a variable/method/class name descriptive I need to make it longer. But I don't want to, I'd like to have short names that are easy to read. So I thought of a special addin to IDE like Visual Studio to be able to write short names for class, method, field but be able to attach long names. If you need to - you can make it all long or you can make single name long. If you want to reduce it - use reduction, like two views of the same code. I`d like to know what others thinking about it? Do you think it is usefull? Would anybody use the kind of addin?\n    ", "Answer": "\r\nWhy not just use the standard XML commenting system built into Visual Studio.\nIf you type /// above the Class/Method/variable etc, it creates the comment stub.\nThese comments popup through Intelisense/Code Completion with extra info.\n\nThis way you keep your naming conventions short and descriptive whilst commenting your code.\nYou can run a process to then create documentation for your code using these comments.\n\nSee: http://msdn.microsoft.com/en-us/magazine/cc302121.aspx\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "To find the minimum(least) number of Unique numbers in a given list after 'n' reductions\r\n                \r\nProblem Statement: To find the minimum(least) number of Unique numbers in a given list after 'n' reductions\n\nInput:\n\n```\nN and an Array(or list)\n\nWhere 0 < N < len(Array)\n```\n\n\nN is the number of reductions possible  and the input for the array needs to be separated by commas(,)\n\nExample 1:\n\n```\nN = 2\n\nArray = 1, 2, 3, 3, 4, 4\n```\n\n\nOutput: \nTo find the Least or minimum number of unique elements after deleting N number of elements in the Array\n\nIn the above example,\nAfter deleting N = 2 number of elements from the Array\n\nIn the above example 1, 2 should be deleted from the array\n         3, 3, 4, 4 will be remaining\nSo, 2 unique elements remaining after deleting 2 elements from the array\n\nSo, the output should be 2\n\nExample 2: \n\n```\nN = 2 [ number of reductions possible]\n\nInput Array : 1,3,4,1,2,4,2,2 \n\nOutput: 3 [least number of unique elements] \n```\n\n\nExplanation :[1,1,2,2,4,4] will be the resultant array when [2,3] are removed\n\nSupposed to be coded in Python exclusively but solutions in any language will be appreciated.\n    ", "Answer": "\r\nFinding the minimum number of unique elements is equivalent to finding the maximum number of duplicates.\n\nThe driving idea here would be to use your reductions to take out the elements that appear the fewest number of times first. In order to do that, you'd want to count the number of occurrences of each element in the list, sort them by number of occurrences, and remove them from least to most until you run out of deletions. The only tricky part is the first part, and that's only if you have to code it in pure python (@DerekLangley's answer gives a good example of how you might do that).\n\nIf you're allowed to import other parts of the standard library, then ```\ncollections.Counter```\n makes quick work of this problem. Here's a sample implementation that doesn't account for anything that could go wrong (such as an empty list, or ```\nN```\n being larger than ```\nlen(lst)```\n - these are things that the interviewer would expect you to mention and know how to handle, so work on that).\n\n```\nimport collections\n...\ndef min_uniques(N, lst):\n    # use collections.Counter to get a sorted list of unique elements and their frequencies\n    most_common = collections.Counter(lst).most_common()\n    # returns [(most_frequent, num_occurrences), ...], so we pull from the back to get fewest occurrences.\n    # We could reverse the list and pull from the front but that would be less efficient\n    while N >= most_common[-1][1]:\n        # remove the element with lowest count and subtract its count from N, all at once\n        N -= most_common.pop()[1] \n    # return the number of unique elements left, after we can no longer remove enough to decrease that count\n    return len(most_common)\n\nmin_uniques(2, [1, 2, 3, 3, 4, 4])\n# 2\nmin_uniques(2, [1, 3, 4, 1, 2, 4, 2, 2])\n# 3\n```\n\n\nMy comments on that code represent how I would talk through the problem with the interviewer as I was writing it. This is a four-line python function, but I'm pretty sure you could also do it in two - the interviewer might ask for how you can improve this code, and if you can put in that as an example (maybe say \"I think it would use mechanism X or mechanism Y, but I'd have to look at the documentation and do some tinkering first).\n\nI don't especially see how Dynamic Programming is relevant here, though I kind of feel like Dynamic Programming is a bit of a buzzword anyway.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Understanding Reductions to show NP-Completeness\r\n                \r\nI have a homework problem that I am finding difficult to begin. We are working on Karp (single-call) reductions to show intractability. For this assignment, the problem is intentionally vague. I was hoping someone here may know it or can provide an example of a solution to help me get started.\nThe problem is described only in code they provided. It has the following components:\nB = (G, T, k) where B is an instance of the \"Blah\" problem, G = (V,E) is a graph (unweighted, undirected), T is a subset of vertices in V, and k is an integer. The certificate for B returns a subgraph, S = (V', E'), of G. The verifier for a yes instance is also provided:\na yes instance if\n```\nforall [t in T], there exists some edge [e in E'] s.t. t is an endpoint of e\nand, the number of edges of S is at most k (|E'| <= k)\nand, S is a connected graph\n```\n\nI see some similarities between this problem and the Independent-Set or Vertex-Cover problems. I feel these are good candidates to reduce from to show intractability, but do not yet understand the problem well enough. If there is discussion of this problem somewhere, or anyone can provide some examples, I would greatly appreciate it. Thank you!\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Proving A Language Is Undecidable Using Turing Reductions\r\n                \r\nI need to prove that the language ```\nL(EVEN) = { M : |L(M)| is even }```\n is undecidable.  \n\nIn other words, the language ```\nL(EVEN)```\n is the set of all Turing Machines which accept some language of even cardinality.  \n\nHere, ```\nM```\n is the encoding of some Turing Machine which would be passed in as input if there existed a decider for ```\nL(EVEN)```\n.\n\nI've completed other problems similar to this one using Turing Reductions, one example can be seen here: \n\n\n\nMy issue is that I am unable to come up with some previously proved undecidable language that would be useful to show ```\nL <= L(EVEN)```\n.  \n\nThe undecidable languages weve covered so far in class are as follows:  \n\n```\n- L(emptyset) = { M | M is a TM and |L(M)| = emptyset}  \n- L(ACC) = { (M, x) | M is a TM, and M accepts input x}  \n- L(HALT) = { (M, x) | M is a TM, and M halts on input x}  \n- L(EQ) = { (M1, M2) | M1, M2 are TMs, and L(M1) == L(M2) }  \n- L(∈ - HALT) = { M | M is a TM, M halts on input ∈ } \n```\n\n\nI could also possibly use the complements of these languages as decidability is closed under complementation. How could I use one of these undecidable languages to prove that L(EVEN) is also undecidable, using a similar setup to the example problem I included?\n    ", "Answer": "\r\nSuppose we had a decider for L(EVEN). Then, we can decide L(ACC) as follows:\n\nFrom input M to the TM for L(ACC), construct a TM M' which first verifies the input tape is the input x to M, and then runs M on x. M' so constructed either accepts the language {x}, if M accepts x, or the empty language if M does not.\n\nBy using our decider for L(EVEN) on the encoding of M', we can tell if |L(M')| is even (in which case L(M') is empty and M does not accept x) or odd (in which case L(M') = {x} and M accepts x).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Python equivalent to clojure reductions\r\n                \r\nIn Clojure, we have a function like this\n```\n(reductions str [\"foo\" \"bar\" \"quax\"])\n=> [\"foo\" \"foobar\" \"foobarquax\"]\n\n```\n\nor\n```\n(reductions + [1 2 3 4 5])\n=> [1 3 6 10 15]\n```\n\nIt's basically just reduce but it collects the intermediate results.\nI'm having trouble finding an equivalent in Python. Does a base library function exist.\nPython 3\n    ", "Answer": "\r\nYou can use ```\nitertools.accumulate```\n\n```\nfrom itertools import accumulate\n\nl = [1, 2, 3, 4, 5]\n\nprint([*accumulate(l)])\n```\n\nPrints:\n```\n[1, 3, 6, 10, 15]\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Nested reductions -- what is the most idiomatic way to write these in Chapel?\r\n                \r\nChapel reductions currently ignore the initial values of variables. That means this code\n\n```\nvar x: int;\nfor i in 1..3 {\n  forall j in 1..10 with (+ reduce x) {\n    x += 1;\n  }\n}\n\nwriteln(x);\n```\n\n\nreturns 10 and not 30, as this user naively thought. While this behavior is fine (and it is documented in the notes on reduction clauses -- I just didn't think hard about it), it turns out that if I want to get 30 (by accumulating across both loops), I need to actually do the sum by hand. I think it would be quite elegant and symmetric for ```\nfor```\n loops to also have a ```\nreduce```\n intent.... i.e. I'd like to write \n\n```\nvar x: int;\nfor i in 1..3 with (+ reduce x) {\n  forall j in 1..10 with (+ reduce x) {\n    x += 1;\n  }\n}\n\nwriteln(x);\n```\n\n\nNote that even in the case of summing numbers, I need to introduce a temporary variable. For max/min like operations, one needs to be even more careful.\n\nIs there a reason not to support ```\nreduce```\n intents inside for loops? Alternately, is there a more idiomatic (Chapel-rrific) way to do this?\n\nUPDATE: The more I think about this, it's not obvious that my proposed code would work in the case that the outer ```\nfor```\n was replaced by a ```\nforall```\n. I think the issue is that the variables are task-local and not iteration-local, so that the reduction would only occur over tasks. So one would still need a separate internal reduction step. What this would remove is the need for a temporary variable.\n\nI think the more overarching question is what the correct way to do these sorts of nested reductions is...\n    ", "Answer": "\r\nIt seems to me that this is an oversight in the design of Chapel's reduce intent.  Specifically, while I think it is appropriate that each task ignores the original variable's value in initializing its personal copy of the reduction variable to the identity (as you note is currently done), I believe the tasks' contributions should be combined back into the original variable's value at the end of the parallel loop rather than simply overwriting that original value as they are combined with one another.  This would make your original attempt work as you had expected, and would also follow what OpenMP does, as suggested by the following C example which gets 35 as its result:\n\n```\n#include <stdio.h>\n#include <omp.h>\n\nint main(int argc, char* argv[]) {\n  int tot = 5;\n  for (int i=0; i<3; i++) {\n  #pragma omp parallel for reduction(+:tot)\n    for (int j=0; j<10; j++) {\n      tot += 1;\n    }\n  }\n  printf(\"tot is: %d\\n\", tot);\n}\n```\n\n\nI would recommend filing a bug / feature request advocating for this behavior on the Chapel GitHub issues page.\n\nAs of Chapel 1.15.0, one way to work around this would be to do the reduction manually within the serial loop, as follows:\n\n```\nconfig var tot: int = 5;\n\nfor i in 1..3 {\n  var subtot: int;\n  forall j in 1..10 with (+ reduce subtot) do\n    subtot += 1;\n  tot += subtot;\n}\n\nwriteln(\"tot is: \", tot);\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Python File Reduction - part 2\r\n                \r\nThis is a follow up to my previous question (text file reduction with randomization in Python). Was modifying to run through multiple reductions, but only the first output file contains reductions, the following 3 files are zero size. This must be something obvious I am not seeing...\n\n```\n#!/usr/bin/env python\nimport random\nimport sys\nfrom itertools import chain, groupby\n\ndef choose_random(iterator, fraction, random=random.random):\n    \"\"\"Lazy analog of:\n\n        L = list(iterator)\n        k = int(len(L) * fraction + .5) or 1 # get at least one\n        result = random.sample(L, k)\n\n    Note: this function doesn't randomize the order of elements\n          that would require to keep selected elements in memory\n          and number of output elements is not exactly k\n    \"\"\"\n    # always yield at least one item if input is not empty\n    item = next(iterator)\n    it = (x for x in chain([item], iterator) if random() < fraction)\n    for x in chain([next(it, item)], it):\n        yield x\n\ndef getkey(line):\n    return line.split(\"\\t\")[3] # 4th column\n\nreductions = [0.25, 0.50, 0.75, 1]\nfilename = \"foo\"\noutfile = [open(\"-\".join([x, filename]), \"w\") for x in map(str, reductions)]\n\ntry:\n    with open(filename, \"r\") as f:\n        for ln, k in enumerate(map(float, reductions)):\n            for key, group in groupby(f, key=getkey):\n                outfile[ln].writelines(choose_random(group, fraction=k))\n\nfinally:\n    for f in outfile:\n        f.close()\n```\n\n\n\n\noutput shows up like this (file 0.25-foo contains correct reduction, the rest are empty):\n\n```\n-rw-r--r--  1 staff  staff   53326048 Mar 27 03:42 0.25-foo\n-rw-r--r--  1 staff  staff          0 Mar 27 03:42 0.5-foo\n-rw-r--r--  1 staff  staff          0 Mar 27 03:42 0.75-foo\n-rw-r--r--  1 staff  staff          0 Mar 27 03:42 1-foo\n```\n\n    ", "Answer": "\r\nYou open ```\nfoo```\n once, but try to iterate over it four times. By the end of the first reduction, you're at the end of file. Either reopen it:\n\n```\ntry:\n    for ln, k in enumerate(map(float, reductions)):\n        with open(filename, \"r\") as f:\n            for key, group in groupby(f, key=getkey):\n                outfile[ln].writelines(choose_random(group, fraction=k))\n\nfinally:\n    for f in outfile:\n        f.close()\n```\n\n\nor rewind after each reduction:\n\n```\ntry:\n    with open(filename, \"r\") as f:\n        for ln, k in enumerate(map(float, reductions)):\n            for key, group in groupby(f, key=getkey):\n                outfile[ln].writelines(choose_random(group, fraction=k))\n            f.seek(0)\n\nfinally:\n    for f in outfile:\n        f.close()\n```\n\n\nI'd open both files at one point:\n\n```\nreductions = [0.25, 0.50, 0.75, 1.0]\nfilename = \"foo\"\n\nfor fraction in reductions:\n    with open(filename, \"r\") as f, open('%s-%s' % (fraction, filename), 'w') as outfile:\n        for key, group in groupby(f, key=getkey):\n            outfile.writelines(choose_random(group, fraction=fraction))\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Algorithm reductions\r\n                \r\nIf I have a algorithm A that i have proven belongs to P can this algorithm also belong to the NPC class or is it strictly P? What about NP? P Belongs to NP right?\n\nThx for any help!\n\n/Marthin\n    ", "Answer": "\r\nIf P!= NP then P is not a subset of NPC, in fact they don't intersect. If P=NP, then P and NPC are the same.  All P algorithms are part of NP though.  Check the Wikipedia page for more information, and a diagram that explains exactly what you're asking.\n\nIf you can prove that P=NP, you will be very famous.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reductions down a column in Pandas\r\n                \r\nI'm trying to transform a (well, many) column of return data to a column of closing prices. In Clojure, I'd use ```\nreductions```\n, which is like ```\nreduce```\n, but returns a sequence of all the intermediate values. \n\ne.g.\n\n```\n$ c\n\n0.12\n-.13\n0.23\n0.17\n0.29\n-0.11\n\n# something like this\n$ c.reductions(init=1, lambda accumulator, ret: accumulator * (1 + ret)) \n\n1.12\n0.97\n1.20\n1.40\n1.81\n1.61\n```\n\n\nNB: The actual closing price doesn't matter, hence using 1 as the initial value. I just need a \"mock\" closing price.\n\nMy data's actual structure is a DataFrame of named columns of TimeSeries. I guess I'm looking for a function similar ```\napplymap```\n, but I'd rather not do something hacky with that function and reference the DF from within it (which I suppose is one solution to this problem?)\n\nAdditionally, what would I do if I wanted to keep the ```\nreturns```\n data, but have the closing \"price\" with it? Should I return a tuple instead, and have the TimeSeries be of the type ```\n(returns, closing_price)```\n?\n    ", "Answer": "\r\nIt doesn't look like it's a well publicized feature yet, but you can use ```\nexpanding_apply```\n to achieve the returns calculation:\n\n```\nIn [1]: s\nOut[1]:\n0    0.12\n1   -0.13\n2    0.23\n3    0.17\n4    0.29\n5   -0.11\n\nIn [2]: pd.expanding_apply(s ,lambda s: reduce(lambda x, y: x * (1+y), s, 1))\n\nOut[2]:\n0    1.120000\n1    0.974400\n2    1.198512\n3    1.402259\n4    1.808914\n5    1.609934\n```\n\n\nI'm not 100% certain, but I believe ```\nexpanding_apply```\n works on the applied series starting from the first index through the current index.  I use the built-in ```\nreduce```\n function that works exactly like your Clojure function.\n\nDocstring for ```\nexpanding_apply```\n:\n\n```\nGeneric expanding function application\n\nParameters\n----------\narg : Series, DataFrame\nfunc : function\n    Must produce a single value from an ndarray input\nmin_periods : int\n    Minimum number of observations in window required to have a value\nfreq : None or string alias / date offset object, default=None\n    Frequency to conform to before computing statistic\ncenter : boolean, default False\n    Whether the label should correspond with center of window\n\nReturns\n-------\ny : type of input argument\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Clojure: How to reductions with a predicate?\r\n                \r\nIn Clojure, how do I do ```\nreductions```\n with a ```\npredicate```\n?\nFor example:\n```\n(reductions - 1000 [500 600 300 90 180 180 30 80])\n```\n\nI want: (1) subtraction to be executed only if the subtrahend is not greater than the minuend; (2) if the condition in (1) is not met, leave the minuend as it is. Thus, the result should be:\n```\n=> (500 500 200 110 110 80 0)\n```\n\n    ", "Answer": "\r\nPut your logic in a function and use that instead of ```\n-```\n:\n```\nuser> (defn f [a b] (if (>= a b) (- a b) a))\n#'user/f\n\nuser> (reductions f 1000 [500 600 300 90 180 180 30 80])\n(1000 500 500 200 110 110 110 80 0)\n```\n\n```\nreduction```\n includes the initial value at the head of the result, you can use ```\nrest```\n or ```\nnext```\n if you don't want it. Also I get ```\n110```\n three times, which is different than your requested output, but I think it's correct according to your rules.\nOr if you prefer to use an anonymous function:\n```\n(reductions #(if (> %1 %2) (- %1 %2) %1) 1000 [500 600 300 90 180 180 30 80])\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Nonassociative reductions with Java Streams\r\n                \r\nAs you may know, the Java Stream API handles reduction operations that are associative; this allows for the possibility that reduction may be performed in parallel through spliteration.  However, do we have an analogue for reduction operations that are strictly sequential?\n\nConsider this:\n\n```\npublic interface ExpressionFactory {\n    public Expression createExpression(Expression inner);\n}\n\npublic interface Expression {\n    public void evaluate();\n}\n\npublic class SequentialExpressionFactory implements ExpressionFactory {\n    private final List<ExpressionFactory> factories;\n}\n```\n\n\nWhat we want to do is have the implementation of ```\ncreateExpression()```\n in ```\nSequentialExpressionFactory```\n return an ```\nExpression```\n that consists of starting with the base expression and calling ```\ncreateExpression()```\n for each factory, using the output of one as the input for the next.  This is of course, trivially doable with a for-loop, but let's try to consider the use of the Stream API here.\n\nIf we try to use Stream API, then we would have to use ```\nreduce()```\n, but ```\nreduce()```\n takes a combiner.  Given that ```\nSequentialExpressionFactory```\n must evaluate each ```\nExpression```\n in order, semantically the notion of a combiner doesn't really make sense.  Of course, using a throwing combiner is not only a code smell, we cannot guarantee that the throwing combiner will never be used (and of course, it leads to brittleness when the reduction operation gives the correct result \"some of the time\").  Furthermore, the way we have this, this really isn't an associative operation, and the Stream API requires reduction operations to be associative.\n\nAgain, given this operation and a need to do what is a reduction operation, is there a (third-party) \"sequence\" analogue to the Stream API that can do these nonassociative reduction operations?\n    ", "Answer": "\r\nIt's not third-party; it's the traditional Java ```\nIterable```\n API.  If necessary, you can call ```\nStream.iterator()```\n to sequentially iterate over the stream.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to define generic reductions using any binary Operator example OR or AND, custom function?\r\n                \r\nI believe Halide currently supports ```\nsum```\n,```\nminimum```\n,```\nmaximum```\n and ```\nproduct```\n which work with ```\nRDom```\n. I would like to write a function which does reductions over custom binary operations For Eg. 'AND (&&)', 'OR (||)', '(&)', '(|)' etc. How can I do this in Halide ?\n\nHere are my thoughts on this problem:\nSuppose we have a ```\nuint8_t input```\n and to perform a reduction using ```\n(|)```\n, \n\n```\nRDom rw(0,width); rh(0,height);\nFunc f,g;\nf(y) = cast<uint8_t>(0);\nf(y) = f(y) | input(rw,y);\ng(x) = cast<uint8_t>(0);\ng(x) = g(x) | f(rh);\n```\n\n\nIt would be nice to have a Halide function which can perform generic reduction by means of specifying a reduction function (two inputs)\n\nThanks in advance for your reply. \n    ", "Answer": "\r\nThe helpers sum, product, etc are not actually built-ins. They are just helpers written in the Halide front-end language itself, so you can define more if you like, or make a generic that takes a binary operator. I would start by looking at https://github.com/halide/Halide/blob/master/src/InlineReductions.cpp\n\nThe bulk of the magic is just the automatic capturing of free variables, the rest is just like the code you wrote.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reductions inside subgrograms\r\n                \r\nThe following Fortran code fails (random result), but replacing the call to mysum by abc=abc+1\ngives the correct result. How to make OpenMP recognizing the reduction in a subprogram?\n```\n  program reduc\n\n    use omp_lib\n    implicit none\n\n    integer :: abc=0, icount\n    call OMP_set_num_threads(8)\n\n    !$omp parallel private (icount) reduction(+:abc)\n    !$omp do\n\n    do icount = 1,8\n      !abc = abc + 1\n      call mysum(OMP_get_thread_num())\n    end do\n\n    !$omp end do\n    !$omp end parallel\n    print*,\"abc at end: \",abc\n\n    contains\n\n    subroutine mysum(omp_rank)\n\n    integer :: omp_rank\n\n      abc = abc + 1\n      print*,\"OMP rank: \", omp_rank, \" abc: \", abc\n\n    endsubroutine mysum\n\n  end program reduc\n```\n\nI also tried to put !$omp threadprivate (abc) into mysum, which was rejected with\n\"Error: Symbol 'abc' at (1) has no IMPLICIT type.\", which is of course not true.\n    ", "Answer": "\r\nBecause of the reduction, the original variable ```\nabc```\n is privatised in each thread, but ```\nabc```\n in the contained subroutine always refers to the original, non privatised variable.\nThe solution is to pass it as an argument:\n```\nsubroutine mysum(omp_rank,abc)\n    integer, intent(in) :: omp_rank\n    integer, intent(inout) :: abc\n    \n    abc = abc + 1\n    print*,\"OMP rank: \", omp_rank, \" abc: \", abc\nendsubroutine mysum\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "SCIP-Jack gives incorrect Steiner Arborescence solution when reductions are enabled\r\n                \r\nWhen running SCIP-Jack (The STP application) on a particular Steiner Arborescence problem instance, the output logfile does not contain a valid solution when the stp/reduction setting is 1 or 2. \n\nI'm wondering whether my file is not formatted correctly, whether my graph contains errors or whether the reductions are not applied correctly.\n\nMy graph was constructed in python using NetworkX. I check whether each edge is reachable from the root node and always give both the arc and anti-parallel arc a weight. The problem type is also set to \"SAP\".\nMost of the edges are bi-directional, and thus have the same weight in both directions.\n\nMy input file is as follows\n\n```\n33D32945 STP File, STP Format Version 1.0\nSECTION Comment\nProblem \"SAP\"\nEND\n\nSECTION Graph\nNodes 49\nEdges 62\nA 23 47 289.36 10000000000\nA 27 44 297.37 10000000000\nA 29 46 295.36 10000000000\nA 32 48 294.02 10000000000\nA 34 45 295.66 10000000000\nA 1 2 0.0 0.0\nA 1 6 0.0 0.0\nA 2 37 12.24 12.24\nA 2 38 4.82 4.82\nA 3 4 0.0 0.0\nA 3 5 0.0 0.0\nA 3 22 382.56 382.56\nA 3 41 367.16 367.16\nA 3 42 15.51 15.51\nA 5 6 0.0 0.0\nA 5 37 274.53 274.53\nA 5 41 176.09 176.09\nA 5 49 0 0\nA 7 8 338.24 338.24\nA 7 9 65.86 65.86\nA 7 43 97.23 97.23\nA 8 40 46.79 46.79\nA 9 10 573.38 573.38\nA 10 11 162.1 162.1\nA 11 12 729.19 729.19\nA 12 13 3.23 3.23\nA 13 14 136.91 136.91\nA 13 28 818.47 818.47\nA 14 15 184.65 184.65\nA 15 16 2.03 2.03\nA 15 30 818.4 818.4\nA 16 17 20.2 20.2\nA 17 18 2023.02 2023.02\nA 17 31 818.4 818.4\nA 18 19 1465.78 1465.78\nA 18 33 817.96 817.96\nA 19 20 3.54 3.54\nA 20 21 13.11 13.11\nA 20 35 817.71 817.71\nA 21 22 3.55 3.55\nA 22 36 817.7 817.7\nA 23 24 452.58 452.58\nA 23 25 96.21 96.21\nA 25 26 319.98 319.98\nA 25 40 779.27 779.27\nA 26 27 709.62 709.62\nA 26 43 726.02 726.02\nA 27 28 852.76 852.76\nA 28 29 328.06 328.06\nA 29 30 30.42 30.42\nA 30 31 40.42 40.42\nA 31 32 1280.36 1280.36\nA 32 33 742.84 742.84\nA 33 34 482.5 482.5\nA 34 35 986.65 986.65\nA 35 36 16.66 16.66\nA 36 37 398.31 398.31\nA 38 39 3.14 3.14\nA 38 41 450.94 450.94\nA 40 43 342.61 342.61\nA 41 42 366.82 366.82\nEND\n\nSECTION Terminals\nTerminals 6\nRoot 49\nT 44\nT 45\nT 46\nT 47\nT 48\nT 49\nEND\n\nEOF\n```\n\n\nI would expect the solution to have at least 10 edges, to connect everything. However, the reported solution is as follows (with stp/reduction=2).\n\n```\nSECTION Finalsolution\nVertices 2\nV 5\nV 49\nEdges 1\nE 5 49\nEnd\n```\n\n    ", "Answer": "\r\nThe behaviour is caused by a bug in SCIP-Jack; SCIP-Jack is not very well tested for SAP unfortunately (due to a lack of test instances). It will be fixed as part of the next bugfix release. Please contact me (Daniel Rehfeldt) if you want to have a fixed version already.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "remquo: argument reduction?\r\n                \r\nIn the C99 spec it says of remquo:\n\n\n  The remquo functions are intended for implementing argument reductions which can exploit a few low-order bits of the quotient. Note that x may be so large in magnitude relative to y that an exact representation of the quotient is not practical.\n\n\nWhat is an \"argument reduction\" in this context, and what is an example of one that can exploit a few low-order bits of the quotient?\n    ", "Answer": "\r\nArgument reduction means mapping the argument of a periodic function into the canonical period (for example, (-π,π] or similar). If you used π/2 as the divisor, the low bits of the quotient would be sufficient for determining the right sign/etc. for trig functions.\n\nUnfortunately, however, ```\nremquo```\n is useless for implementing standard trigonometric argument reduction, because ```\nπ```\n is irrational; reducing large arguments modulo an approximation of π will give you results with no significant bits, i.e. all error.\n\nIf however you're writing a function ```\nf(x)```\n defined as sin(πx) or similar, the period is now exactly representable in floating point, and ```\nremquo```\n can do exactly what you need, whereas calling ```\nsin(2*M_PI*x)```\n directly will give results with no significant bits (i.e. all error) when ```\nx```\n is large.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda Calculus beta reduction\r\n                \r\nI have the following lambda calculus:\n\n```\n( x ( λyz.xz ) ( λxy.zyx )) (( λyx.xyz ) ( λy.xz ))\n```\n\n\nwhich I already reduced:\n\n```\nalpha => ( x ( λyz.xz ) ( λxy.zyx )) (( λyx1.x1yz )) ( λy.xz ))\nbeta  => ( x ( λyz.xz ) ( λxy.zyx )) ( λx1.x1 ( λy.xz ) z )\n```\n\n\nMy question: Why are the following reductions wrong? They seem to simplify the expression to me:\n\n```\nbeta1 => ( x ( λyz.xz ) ( λxy.zyx )) ( λx1.x1 ( xz ))\nbeta2 => ( x ( λy.x ( λxy.zyx ))) ( λx1.x1 ( xz ))\n```\n\n    ", "Answer": "\r\nTo avoid confusion I would suggest to alpha-rename a bit more.\nSo it would appear you have ```\nx```\n and ```\nz```\n in \"scope\" (they are not bound by any ```\nλ```\n in your term), it is a good idea to make sure if you are referring to them or not.\n\nI would rewrite your term like this:\n\n```\n(x (λy1,z1. x z1) (λx2,y2. z y2 x2)) ((λy3,x3. x3 y3 z) (λy4. x z))\n```\n\n\nI went quite far in renaming (this is unnecessary of course, but like this, we are sure which is which).\n\nThis term beta-reduces to\n\n```\n(x (λy1,z1. x z1) (λx2,y2. z y2 x2)) ((λx3. x3 (λy4. x z) z))\n```\n\n\nAnd then\n\n```\n(x (λy1,z1. x z1) (λx2,y2. z y2 x2)) ((λx3. x3 (λy4. x z) z))\n```\n\n\nThis is indeed alpha-equivalent to what you wrote.\nWhat you want to do then is apply ```\n(λy4. x z)```\n to ```\nz```\n.\nThis is not how you should be reading ```\nx3 (λy4. x z) z```\n which could be also written with more parentheses like this ```\n(x3 (λy4. x z)) z```\n.\nSomehow this term is stuck because its head is a variable.\n\nThere isn't much you can do about it because this term is also involved in an application you can't reduce.\nPerhaps you could also rewrite ```\n(λy1,z1. x z1)```\n into ```\n(λy1. x)```\n by eta-reduction.\n\nSo you cannot get ```\n(x (λyz.xz) (λxy.zyx)) (λx1.x1 (x z))```\n and for the same reason, you can't beta-reduce it to ```\n(x (λy.x (λxy.zyx))) (λx1.x1 (x z))```\n because you got the parenthesizing wrong.\n\n```\nf x y```\n is read from left to right, you take ```\nf```\n and you apply it to ```\nx```\n, then you apply the result to ```\ny```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "erlang elixir observer ui: what does reds/reductions mean in processes tab?\r\n                \r\nI was doing the exercise of making a dictionary module, implemented with Agent, that should output random word on ```\nrandom_word(pid)```\n function call. After implementation was complete the author asked: \"Using the observer (:observer.start), find this process. Double click on it to bring up the process details. Now generate some random words. Do you see any changes in the process display (you may have to wait a few seconds or refresh the process window to see a change)?\" \n\nThe only change I have noticed is in the number of reds/reductions (it increases when I call the function from iex). However there was no explanation of what \"Reds\" is or why it increased.\n\nAfter a quick web surf I could not find anything understandable for a beginner (a lot of elixir/erlang in production articles popup). Could you folks give a nice, simple answer to this.\n\n\n    ", "Answer": "\r\nIn a BEAM application, you can potentially have millions of processes running at the same time. In order to ensure that each process gets (roughly) equal share of the available CPU(s), the VM counts reductions. One reduction is essentially equivalent to one function call. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA shuffle instruction reduction slower than shared memory reduction?\r\n                \r\nShuffle instruction based warp reduction is expected to perform faster reduction than reduction using shared memory or global memory, as mentioned in Faster Parallel Reductions on Kepler and CUDA Pro Tip: Do The Kepler Shuffle\nIn the following code, I tried to validate this:\n```\n#include \"cuda_runtime.h\"\n#include \"device_launch_parameters.h\"\n#include <cuda_profiler_api.h>\n#include <stdio.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n\n__inline__ __device__\nfloat warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down(val, offset);\n    return val;\n}\n\n__inline__ __device__\nfloat blockReduceSum(float val) {\n    static __shared__ int shared[32];\n    int lane = threadIdx.x%32;\n    int wid = threadIdx.x / 32;\n    val = warpReduceSum(val);\n\n    //write reduced value to shared memory\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    //ensure we only grab a value from shared memory if that warp existed\n    val = (threadIdx.x<blockDim.x / 32) ? shared[lane] : int(0);\n    if (wid == 0) val = warpReduceSum(val);\n\n    return val;\n}\n\n__global__ void device_reduce_stable_kernel(float *in, float* out, int N) {\n    float sum = int(0);\n    //printf(\"value = %d \", blockDim.x*gridDim.x);\n    for (int i = blockIdx.x*blockDim.x + threadIdx.x; i<N; i += blockDim.x*gridDim.x) {\n        sum += in[i];\n    }\n    sum = blockReduceSum(sum);\n    if (threadIdx.x == 0)\n        out[blockIdx.x] = sum;\n}\n\nvoid device_reduce_stable(float *in, float* out, int N) {\n    //int threads = 512;\n    //int blocks = min((N + threads - 1) / threads, 1024);\n    const int maxThreadsPerBlock = 1024;\n    int threads = maxThreadsPerBlock;\n    int blocks = N / maxThreadsPerBlock;\n    device_reduce_stable_kernel << <blocks, threads >> >(in, out, N);\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess)\n        printf(\"Error: %s\\n\", cudaGetErrorString(err));\n\n    device_reduce_stable_kernel << <1, 1024 >> >(out, out, blocks);\n    //cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess)\n        printf(\"Error: %s\\n\", cudaGetErrorString(err));\n}\n\n__global__ void global_reduce_kernel(float * d_out, float * d_in)\n{\n    int myId = threadIdx.x + blockDim.x * blockIdx.x;\n    int tid = threadIdx.x;\n\n    // do reduction in global mem\n    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1)\n    {\n        if (tid < s)\n        {\n            d_in[myId] += d_in[myId + s];\n        }\n        __syncthreads();        // make sure all adds at one stage are done!\n    }\n\n    // only thread 0 writes result for this block back to global mem\n    if (tid == 0)\n    {\n        d_out[blockIdx.x] = d_in[myId];\n    }\n}\n\n__global__ void shmem_reduce_kernel(float * d_out, const float * d_in)\n{\n    // sdata is allocated in the kernel call: 3rd arg to <<<b, t, shmem>>>\n    extern __shared__ float sdata[];\n\n    int myId = threadIdx.x + blockDim.x * blockIdx.x;\n    int tid = threadIdx.x;\n\n    // load shared mem from global mem\n    sdata[tid] = d_in[myId];\n    __syncthreads();            // make sure entire block is loaded!\n\n    // do reduction in shared mem\n    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1)\n    {\n        if (tid < s)\n        {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();        // make sure all adds at one stage are done!\n    }\n\n    // only thread 0 writes result for this block back to global mem\n    if (tid == 0)\n    {\n        d_out[blockIdx.x] = sdata[0];\n    }\n}\n\nvoid reduce(float * d_out, float * d_intermediate, float * d_in,\n    int size, bool usesSharedMemory)\n{\n    // assumes that size is not greater than maxThreadsPerBlock^2\n    // and that size is a multiple of maxThreadsPerBlock\n    const int maxThreadsPerBlock = 1024;\n    int threads = maxThreadsPerBlock;\n    int blocks = size / maxThreadsPerBlock;\n    if (usesSharedMemory)\n    {\n        shmem_reduce_kernel << <blocks, threads, threads * sizeof(float) >> >\n            (d_intermediate, d_in);\n        cudaError_t err = cudaGetLastError();\n        if (err != cudaSuccess)\n            printf(\"Error: %s\\n\", cudaGetErrorString(err));\n    }\n    else\n    {\n        global_reduce_kernel << <blocks, threads >> >\n            (d_intermediate, d_in);\n        cudaError_t err = cudaGetLastError();\n        if (err != cudaSuccess)\n            printf(\"Error: %s\\n\", cudaGetErrorString(err));\n    }\n    // now we're down to one block left, so reduce it\n    threads = blocks; // launch one thread for each block in prev step\n    blocks = 1;\n    if (usesSharedMemory)\n    {\n        shmem_reduce_kernel << <blocks, threads, threads * sizeof(float) >> >\n            (d_out, d_intermediate);\n        cudaError_t err = cudaGetLastError();\n        if (err != cudaSuccess)\n            printf(\"Error: %s\\n\", cudaGetErrorString(err));\n    }\n    else\n    {\n        global_reduce_kernel << <blocks, threads >> >\n            (d_out, d_intermediate);\n        cudaError_t err = cudaGetLastError();\n        if (err != cudaSuccess)\n            printf(\"Error: %s\\n\", cudaGetErrorString(err));\n    }\n}\n\nint main()\n{\n    /*int deviceCount;\n    cudaGetDeviceCount(&deviceCount);\n    if (deviceCount == 0) {\n        fprintf(stderr, \"error: no devices supporting CUDA.\\n\");\n        exit(EXIT_FAILURE);\n    }\n    int dev = 0;\n    cudaSetDevice(dev);\n\n    cudaDeviceProp devProps;\n    if (cudaGetDeviceProperties(&devProps, dev) == 0)\n    {\n        printf(\"Using device %d:\\n\", dev);\n        printf(\"%s; global mem: %dB; compute v%d.%d; clock: %d kHz\\n\",\n            devProps.name, (int)devProps.totalGlobalMem,\n            (int)devProps.major, (int)devProps.minor,\n            (int)devProps.clockRate);\n    }\n*/\n    const int ARRAY_SIZE = 2048;\n    const int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n\n    // generate the input array on the host\n    float h_in[ARRAY_SIZE];\n    float sum = 0.0f;\n    for (int i = 0; i < ARRAY_SIZE; i++) {\n        // generate random float in [-1.0f, 1.0f]\n        h_in[i] = i;\n        sum += h_in[i];\n    }\n\n    // declare GPU memory pointers\n    float * d_in, *d_intermediate, *d_out;\n\n    // allocate GPU memory\n    cudaMalloc((void **)&d_in, ARRAY_BYTES);\n    cudaMalloc((void **)&d_intermediate, ARRAY_BYTES); // overallocated\n    cudaMalloc((void **)&d_out, sizeof(float));\n\n    // transfer the input array to the GPU\n    cudaMemcpy(d_in, h_in, ARRAY_BYTES, cudaMemcpyHostToDevice);\n\n    int whichKernel = 2;\n    \n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n    // launch the kernel\n    cudaProfilerStart();\n    switch (whichKernel) {\n    case 0:\n        printf(\"Running global reduce\\n\");\n        cudaEventRecord(start, 0);\n        //for (int i = 0; i < 100; i++)\n        //{\n            reduce(d_out, d_intermediate, d_in, ARRAY_SIZE, false);\n        //}\n        cudaEventRecord(stop, 0);\n        break;\n    case 1:\n        printf(\"Running reduce with shared mem\\n\");\n        cudaEventRecord(start, 0);\n        //for (int i = 0; i < 100; i++)\n        //{\n            reduce(d_out, d_intermediate, d_in, ARRAY_SIZE, true);\n        //}\n        cudaEventRecord(stop, 0);\n        break;\n    case 2:\n        printf(\"Running reduce with shuffle instruction\\n\");\n        cudaEventRecord(start, 0);\n        /*for (int i = 0; i < 100; i++)\n        {*/\n            device_reduce_stable(d_in, d_out, ARRAY_SIZE);\n        //}\n        cudaEventRecord(stop, 0);\n        break;\n    default:\n        fprintf(stderr, \"error: ran no kernel\\n\");\n        exit(EXIT_FAILURE);\n    }\n    cudaProfilerStop();\n    cudaEventSynchronize(stop);\n    float elapsedTime;\n    cudaEventElapsedTime(&elapsedTime, start, stop);\n    elapsedTime /= 100.0f;      // 100 trials\n\n    // copy back the sum from GPU\n    float h_out;\n    cudaMemcpy(&h_out, d_out, sizeof(float), cudaMemcpyDeviceToHost);\n\n    printf(\"average time elapsed: %f\\n\", elapsedTime);\n\n    // free GPU memory allocation\n    cudaFree(d_in);\n    cudaFree(d_intermediate);\n    cudaFree(d_out);\n\n    return 0;\n}\n```\n\nThe results showed that warp based reduction took nearly twice the time of shared memory based reduction. These results contradict the behavior expected.\nThe experiment was performed on Tesla K40c with Compute capability higher than 3.0.\n    ", "Answer": "\r\nI'm comparing the following two reduction kernels, one using only shared memory WITHOUT using warp shuffling for the last warp reduction stage (```\nversion4```\n) and one using shared memory AND warp shuffling for the last warp reduction stage (```\nversion5```\n).\n\n```\nversion4```\n\n\n```\ntemplate <class T>\n__global__ void version4(T *g_idata, T *g_odata, unsigned int N)\n{\n    extern __shared__ T sdata[];\n\n    unsigned int tid = threadIdx.x;                                 // --- Local thread index\n    unsigned int i = blockIdx.x * (blockDim.x * 2) + threadIdx.x;   // --- Global thread index - Fictitiously double the block dimension\n\n    // --- Performs the first level of reduction in registers when reading from global memory. \n    T mySum = (i < N) ? g_idata[i] : 0;\n    if (i + blockDim.x < N) mySum += g_idata[i + blockDim.x];\n    sdata[tid] = mySum;\n\n    // --- Before going further, we have to make sure that all the shared memory loads have been completed\n    __syncthreads();\n\n    // --- Reduction in shared memory. Only half of the threads contribute to reduction.\n    for (unsigned int s = blockDim.x / 2; s > 32; s >>= 1)\n    {\n        if (tid < s) { sdata[tid] = mySum = mySum + sdata[tid + s]; }\n        // --- At the end of each iteration loop, we have to make sure that all memory operations have been completed\n        __syncthreads();\n    }\n\n    // --- Single warp reduction by loop unrolling. Assuming blockDim.x >64\n    if (tid < 32) {\n        sdata[tid] = mySum = mySum + sdata[tid + 32]; __syncthreads();\n        sdata[tid] = mySum = mySum + sdata[tid + 16]; __syncthreads();\n        sdata[tid] = mySum = mySum + sdata[tid + 8]; __syncthreads();\n        sdata[tid] = mySum = mySum + sdata[tid + 4]; __syncthreads();\n        sdata[tid] = mySum = mySum + sdata[tid + 2]; __syncthreads();\n        sdata[tid] = mySum = mySum + sdata[tid + 1]; __syncthreads();\n    }\n\n    // --- Write result for this block to global memory. At the end of the kernel, global memory will contain the results for the summations of\n    //     individual blocks\n    if (tid == 0) g_odata[blockIdx.x] = mySum;\n}\n```\n\n\n```\nversion5```\n\n\n```\ntemplate <class T>\n__global__ void version5(T *g_idata, T *g_odata, unsigned int N)\n{\n    extern __shared__ T sdata[];\n\n    unsigned int tid = threadIdx.x;                                 // --- Local thread index\n    unsigned int i = blockIdx.x * (blockDim.x * 2) + threadIdx.x;   // --- Global thread index - Fictitiously double the block dimension\n\n    // --- Performs the first level of reduction in registers when reading from global memory. \n    T mySum = (i < N) ? g_idata[i] : 0;\n    if (i + blockDim.x < N) mySum += g_idata[i + blockDim.x];\n    sdata[tid] = mySum;\n\n    // --- Before going further, we have to make sure that all the shared memory loads have been completed\n    __syncthreads();\n\n    // --- Reduction in shared memory. Only half of the threads contribute to reduction.\n    for (unsigned int s = blockDim.x / 2; s > 32; s >>= 1)\n    {\n        if (tid < s) { sdata[tid] = mySum = mySum + sdata[tid + s]; }\n        // --- At the end of each iteration loop, we have to make sure that all memory operations have been completed\n        __syncthreads();\n    }\n\n    // --- Single warp reduction by shuffle operations\n    if (tid < 32)\n    {\n        // --- Last iteration removed from the for loop, but needed for shuffle reduction\n        mySum += sdata[tid + 32];\n        // --- Reduce final warp using shuffle\n        //for (int offset = warpSize / 2; offset > 0; offset /= 2) mySum += __shfl_down_sync(0xffffffff, mySum, offset);\n        for (int offset=1; offset < warpSize; offset *= 2) mySum += __shfl_xor_sync(0xffffffff, mySum, i);\n    }\n\n    // --- Write result for this block to global memory. At the end of the kernel, global memory will contain the results for the summations of\n    //     individual blocks\n    if (tid == 0) g_odata[blockIdx.x] = mySum;\n}\n```\n\n\nI confirm that there is no sensitive difference between the two. On my GTX920M card, the timing have been the following:\n\n```\nN = 33554432\nversion4 = 27.5ms\nversion5 = 27.095ms\n```\n\n\nSo, I'm confirming Robert's comment above.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Erlang Processes Reduction count\r\n                \r\nI am trying to find out more about the work distribution between the\nprocesses of my Erlang application. The number of reductions executed by a\nprocess is, among others, one of the things I am interested.\n\nSo I am looking for a means of reading the number of reductions of a\nprocess when it exits. While it is be quite straightforward to get\nthis kind of info while the processes is still executing, getting them\njust before (or rightly after) they exit is another story. I have\nlooked all over the documentation, specially dbg:* functions,\nunfortunately, to no avail. Mostly because whenever I am notified\nabout the exit of a process it is already too late to do anything\nabout it.\n\nChanging the code of the entire application to read these values\nbefore the processes exit is unfeasible. Is there some way to do it\nother than diving into the VM code and instrumenting it?\n\nThanks\n    ", "Answer": "\r\nI don't think you can get ```\nreductions```\n for processes, but you could get a time-based work distribution by doing a ```\nerlang:trace/3```\n with ```\nrunning```\n and ```\ntimestamp```\n options. That would get you what you want I think. Naturally you have to collect the data and do some post-processing or perhaps just-in-time processing.\n\nI would also use the option ```\nprocs```\n to the trace to get the necessary meta-information, i.e. started, terminated etc. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Openmp array reductions with Fortran\r\n                \r\nI'm trying to parallelize a code I've written. I'm having problems performing reducitons on arrays. It all seems to work fine for smallish arrays, however when the array size goes above a certain point I either get a stack overflow error or a crash.\n\nI've tried to increased the stack size using the /F at compile time, I'm using ifort on windows, I've also tried passing set KMP_STACKSIZE=xxx the intel specific stacksize decleration. This sometimes helps and allows the code to progress further through my loop but in the end doesn't resolve the issue, even with a stack size of 1Gb or greater.\n\nBelow is a small self-contained working example of my code. It works in serial, and with one thread. Or with many threads but a small 'N'. A large N (i.e. like 250,000 in the example) causes problems.\n\nI didn't think these arrays were so massive so as to cause major problems, and presumed increasing my stack space would help - are there any other options, or have I missed something important in my coding ?\n\n```\nprogram testreduction\n    use omp_lib\n    implicit none\n    integer :: i, j, nthreads, Nsize\n    integer iseed /3/\n    REAL, allocatable :: A(:,:), B(:), C(:), posi(:,:)\n    REAL :: dx, dy, r, Axi, Ayi, m, F\n    !Set size of matrix, and main loop\n    Nsize = 250000\n    m = 1.0\n    F = 1.0\n    !Allocate posi array\n    allocate(posi(2,Nsize))\n    !Fill with random numbers\n    do i=1,Nsize\n        do j=1,2\n            posi(j,i) = (ran(iseed))\n        end do\n    end do\n    !Allocate other arrays\n    allocate(A(2,Nsize), C(Nsize), B(Nsize))\n\n    print*, sizeof(A)+sizeof(B)+sizeof(C)\n    !$OMP parallel\n    nthreads = omp_get_num_threads()\n    !$OMP end parallel\n\n    print*, \"Number of threads \", nthreads\n    !Go through each array and do some work, calculating a reduction on A, B and C.\n    !$OMP parallel do schedule(static) private(i, j, dx, dy, r, Axi, Ayi) reduction(+:C, B, A)\n    do i=1,Nsize\n        do j=1,Nsize\n            !print*, i\n            dx = posi(1,i) - posi(1,j)\n            dy = posi(2,i) - posi(2,j)\n            r = sqrt(dx**2+dy**2)\n            Axi = -m*(F)*(dx/(r))\n            Ayi = -m*(F)*(dy/(r))\n            A(1,i) = A(1,i) + Axi\n            A(2,i) = A(2,i) + Ayi\n            B(i) = B(i) + (Axi+Ayi)\n            C(i) = C(i) + dx/(r) + dy/(r)\n        end do    \n    END DO\n    !$OMP END parallel do\n\nend program\n```\n\n\nUPDATE\n\nA better example of what I'm talking about ..\n\n```\nprogram testreduction2\n    use omp_lib\n    implicit none\n    integer :: i, j, nthreads, Nsize, q, k, nsize2\n    REAL, allocatable :: A(:,:), B(:), C(:)\n    integer, ALLOCATABLE :: PAIRI(:), PAIRJ(:)\n\n    Nsize = 25\n    Nsize2 = 19\n    q=0\n\n    allocate(A(2,Nsize), C(Nsize), B(Nsize))\n    ALLOCATE(PAIRI(nsize*nsize2), PAIRJ(nsize*nsize2))\n\n    do i=1,nsize\n        do j =1,nsize2\n            q=q+1\n            PAIRI(q) = i\n            PAIRJ(q) = j\n        end do\n    end do\n\n    A = 0\n    B = 0\n    C = 0\n\n    !$OMP parallel do schedule(static) private(i, j, k)\n    do k=1,q\n        i=PAIRI(k)\n        j=PAIRJ(k)\n        A(1,i) = A(1,i) + 1\n        A(2,i) = A(2,i) + 1\n        B(i) = B(i) + 1\n        C(i) = C(i) + 1\n    END DO\n    !$OMP END parallel do\n\n    PRINT*, A\n    PRINT*, B\n    PRINT*, C       \nEND PROGRAM\n```\n\n    ", "Answer": "\r\nThe problem is that you are reducing really large arrays. Note that other languages (C, C++) could not reduce arrays until OpenMP 4.5.\nBut I don't see any reason for the reduction in your case, you update each element only once.\nTry just\n```\n!$OMP parallel do schedule(static) private(i, dx, dy, r, Axi, Ayi)\ndo i=1,Nsize\n  do j=1,Nsize\n    ...\n    A(1,i) = A(1,i) + Axi\n    A(2,i) = A(2,i) + Ayi\n    B(i) = B(i) + (Axi+Ayi)\n    C(i) = C(i) + dx/(r) + dy/(r)\n  end do\nend do\n!$OMP END parallel do\n```\n\nThe point is the threads do not interfare. Every thread uses different set of ```\ni```\ns and therefore different elements of ```\nA```\n, ```\nB```\n and ```\nC```\n.\nEven if you come up with a case where it seems to be necessary, you can always rewrite it to avoid it. You can even allocate some buffers yourself and simulate the reduction. Or use atomic updates.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP: templated custom reduction for container (Eigen type)\r\n                \r\nI want to define custom OpenMP reductions for types from the Eigen library, specifically matrices. I managed to define one for a specific type, i.e. a dynamically sized vector of ```\nlong int```\ns, but I'd much rather define them more generally. Is it possible to define templated reductions? Of course the following does not compile (```\n'#pragma' is not allowed here```\n):\n\n```\ntemplate<typename Derived>\n#pragma omp declare reduction(+ : Eigen::MatrixBase<Derived> : std::transform( \\\n        omp_out.begin(), omp_out.end(), \\\n        omp_in.begin(), omp_out.begin(), \\\n        std::plus<Index>() \\\n    )) \\\n    initializer( omp_priv = decltype(omp_orig)::Zero( \\\n        omp_orig.rows(), omp_orig.cols() \\\n    ) )\n```\n\n\n(code snippet based on this answer to a previous question of mine)\n\nIs there a way to define templated custom reductions or something that achieves the same goal?\n\nI'm aware of this question btw. It's not the same, because it describes using a standard reduction in a templated function.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP using loops and array reductions\r\n                \r\nI have written a program as follows:\n\n```\n#include \"omp.h\"\n#include \"stdio.h\"\n\nint main()\n{\n    int i, j, cnt[] = {0,0,0,0};\n    #pragma omp parallel\n    {\n        int cnt_private[] = {0,0,0,0};\n        #pragma omp for private(j)\n        for(int i = 1 ; i <= 10 ; i++) {\n            for(j = 1 ; j <= 10 ; j++) {\n                int l= omp_get_thread_num();\n                cnt_private[l]++;         \n            }\n            #pragma omp critical\n            {   \n               for(int m=0; m<3; m++){\n                   cnt[m] = cnt_private[m];\n               }\n            }\n           printf(\"%d %d %d %d %d\\n\",i,cnt[0],cnt[1],cnt[2],cnt[3]);\n        }\n     }\n     return 0;\n}\n```\n\n\nIt should print the number of times each thread is executed for each i. As only one thread takes a particular i, the expected output should satisfy the sum of each row as 100. But I am getting the output of the form:\n\n```\n1 10 0 0 0\n2 20 0 0 0\n3 30 0 0 0\n7 0 0 10 0\n8 0 0 20 0\n9 0 0 0 0\n10 0 0 0 0\n4 0 10 0 0\n5 0 20 0 0\n6 0 30 0 0\n```\n\n\nWhere is the problem? Could it be in my fundamental understanding of OpenMP? or is my reduction process wrong?\n(I use a GNU gcc compiler and a 4 core machine)\nCompilation steps:  \n\n```\ng++ -fopenmp BlaBla.cpp\nexport OMP_NUM_THREADS=4\n./a.out  \n```\n\n    ", "Answer": "\r\nI do not see why the sum of each row should be 100.\n\nYou declared ```\ncnt_private```\n to be private:\n\n```\n#pragma omp parallel\n{\n    int cnt_private[] = {0,0,0,0};\n    // ...\n}\n```\n\n\nAs such the summation stored to it is not shared between threads. If thread ```\nl```\n is executed only ```\ncnt_private[l]```\n will be incremented and all others will be left at zero. Then you assing the content of ```\ncnt_private```\n to ```\ncnt```\n, which is not private. You assign every entry that is zero as well!\n\n```\n#pragma omp critical\n{   \n    for(int m=0; m<4; m++){ // I guess you want 'm<4' for the number of threads\n        cnt[m] = cnt_private[m];\n    }\n}\n```\n\n\nWith ```\ni```\n ranging from 0 to 10 and the program using 4 threads, each threads gets 2 to 3 ```\ni```\n's. As such I would expect the sum of each column to be either 30(10+20) or 60(10+20+30).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction of array in cython parallel\r\n                \r\nI have an array that needs to contain sum of different things and therefore I want to perform reduction on each of its elements.\nHere's the code:\n\n```\ncdef int *a=<int *>malloc(sizeof(int) * 3)\nfor i in range(3):\n    a[i]=1*i\ncdef int *b\nfor i in prange(1000,nogil=True,num_threads=10):\n    b=res() #res returns an array initialized to 1s\n    with gil: #if commented this line gives erroneous results \n        for k in range(3):\n            a[k]+=b[k]\nfor i in range(3):\n    print a[i]\n```\n\n\nTill there is with gil the code runs fine else gives wrong results.\nHow to deal with reductions on each element of array without using gil cause gil i think will block other threads\n    ", "Answer": "\r\nThe way reductions usually work in practice is to do the sum individually for each thread, and then add them together at the end. You could do this manually with something like\n\n```\ncdef int *b\ncdef int *a_local # version of a that is duplicated by each thread\ncdef int i,j,k\n\n# set up as before\ncdef int *a=<int *>malloc(sizeof(int) * 3)\nfor i in range(3):\n    a[i]=1*i\n\n# multithreaded from here\nwith nogil, parallel(num_threads=10):\n    # setup and initialise a_local on each thread\n    a_local = <int*>malloc(sizeof(int)*3)\n    for k in range(3):\n        a_local[k] = 0\n\n    for i in prange(1000):\n        b=res() # Note - you never free b\n                # this is likely a memory leak....\n\n        for j in range(3):\n            a_local[j]+=b[j]\n\n    # finally at the end add them all together.\n    # this needs to be done `with gil:` to avoid race conditions \n    # but it isn't a problem\n    # because it's only a small amount of work being done\n    with gil:\n        for k in range(3):\n            a[k] += a_local[k]\n    free(a_local)\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Efficient reduction of 2D array in CUDA?\r\n                \r\nIn the CUDA SDK, there is example code and presentation slides for an efficient one-dimensional reduction.  I have also seen several papers on and implementations of one-dimensional reductions and prefix scans in CUDA.\n\nIs there efficient CUDA code available for a reduction of a dense two-dimensional array?  Pointers to code or pertinent papers would be appreciated.\n    ", "Answer": "\r\nI don't know what exactly the problem you try to solve, but actually you could simply think about 2D array as a long 1D array and use SDK code to reduce operation. Simple arrays in CUDA are just 1D memory blocks with special addressing rules - why wouldn't you take advantage of that opportunity.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "NP-Complete Proofs Reductions (Direction)?\r\n                \r\nWhy do you have to reduce an NP-Complete algorithm to the algorithm you're trying to prove is NP-complete, and not vice versa? I feel like the explanation is simple-and i have searched it online, without success-but my mind isn't wrapping around it very well. \n\nThank you!:)\n    ", "Answer": "\r\nBecause if you can reduce problem A to problem B, then problem B cannot be any easier than A. After all, you now have a new way to solve instances of problem A: turn it into an instance of problem B and solve that. If B is easy, then A is also easy by that process.\n\nThat only works if the process of translating the problem instance is itself not hard, which is why you also have to show that. If the translation was allowed to be hard, it could just solve the problem and let B be trivial.\n\nAnd by itself that only proves problem B NP-hard, in order to show that problem B is NP-complete you also have to prove it's in NP (which is usually easier than the reduction).\n\nThe other way around just shows that your problem is no harder than NP-complete, which is not entirely useless, but generally less interesting. For example you can solve the problem \"is there an integer ```\nx```\n such that ```\nx*3=9```\n\" with a SAT solver using a circuit for binary multiplication and encoding that as a SAT instance, but that problem is much easier.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Precision in Sum reduction kernel with floats\r\n                \r\nI am creating a routine that calls the Sum Reduction kernel of Nvidia (reduction6), but when I compare the results between the CPU and GPU get an error that increases as the vector size increases, so: \n\nBoth CPU and GPU reductions are floats\n\n```\nSize: 1024  (Blocks : 1,  Threads : 512)\nReduction on CPU:  508.1255188 \nReduction on GPU:  508.1254883 \nError:  6.0059137e-06\n\nSize: 16384 (Blocks : 8, Threads : 1024)\nReduction on CPU:  4971.3193359 \nReduction on GPU:  4971.3217773 \nError:  4.9109825e-05\n\nSize: 131072 (Blocks : 64, Threads : 1024)\nReduction on CPU:  49986.6718750 \nReduction on GPU:  49986.8203125 \nError:  2.9695415e-04\n\nSize: 1048576 (Blocks : 512, Threads : 1024)\nReduction on CPU:  500003.7500000 \nReduction on GPU:  500006.8125000 \nError:  6.1249541e-04\n```\n\n\nAny idea about this error?, thanks.\n    ", "Answer": "\r\nFloating point addition is not necessarily associative.\n\nThis means that when you change the order of operations of your floating-point summation, you may get different results. Parallelizing a summation by definition changes the order of operations of the summation.\n\nThere are many ways to sum floating-point numbers, and each has accuracy benefits for different input distributions. Here's a decent survey. \n\nSequential summation in the given order is rarely the most accurate way to sum, so if that is what you are comparing against, don't expect it to compare well to the tree-based summation used in a typical parallel reduction.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Efficient partial reductions given arrays of elements, offsets to and lengths of sublists\r\n                \r\nFor my application I have to handle a bunch of objects (let's say ```\nint```\ns) that gets subsequently divided and sorted into smaller buckets. To this end, I store the elements in a single continuous array\n\n```\narr = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14...}\n```\n\n\nand the information about the buckets (sublists) is given by offsets to the first element in the respective bucket and the lengths of the sublist.\n\nSo, for instance, given\n\n```\noffsets = {0,3,8,..}\nsublist_lengths = {3,5,2,...}\n```\n\n\nwould result in the following splits:\n\n```\n0 1 2 || 3 4 5 6 7 || 8 9 || ...\n```\n\n\nWhat I am looking for is a somewhat general and efficient way to run algorithms, like reductions, on the buckets only using either custom kernels or the ```\nthrust```\n library. Summing the buckets should give:\n\n```\n3 || 25 || 17 || ...\n```\n\n\nWhat I've come up with:\n\n\noption 1: custom kernels require a quite a bit of tinkering, copies into shared memory, proper choice of block and grid sizes and an own implementation of the algorithms, like scan, reduce, etc. Also, every single operation would require an own custom kernel. In general it is clear to me how to do this, but after having used ```\nthrust```\n for the last couple of days I have the impression that there might be a smarter way\noption 2: generate an array of keys from the offsets (```\n{0,0,0,1,1,1,1,1,2,2,3,...}```\n in the above example) and use ```\nthrust::reduce_by_key```\n. I don't like the extra list generation, though.\noption 3: Use ```\nthrust::transform_iterator```\n together with ```\nthrust::counting_iterator```\n to generate the above given key list on the fly. Unfortunately, I can't come up with an implementation that doesn't require increments of indices to the offset list on the device and defeats parallelism.\n\n\nWhat would be the most sane way to implement this?\n    ", "Answer": "\r\nWithin Thrust, I can't think of a better solution than Option 2.  The performance will not be terrible, but it's certainly not optimal.\n\nYour data structure bears similarity to the Compressed Sparse Row (CSR) format for storing sparse matrices, so you could use techniques developed for computing sparse matrix-vector multiplies (SpMV) for such matrices if you want better performance.  Note that the \"offsets\" array of the CSR format has length (N+1) for a matrix with N rows (i.e. buckets in your case) where the last offset value is the length of ```\narr```\n.  The CSR SpMV code in Cusp is a bit convoluted, but it serves as a good starting point for your kernel.  Simply remove any reference to ```\nAj```\n or ```\nx```\n from the code and pass ```\noffsets```\n and ```\narr```\n into the ```\nAp```\n and ```\nAv```\n arguments respectively.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Listen for font reductions on iOS [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has an answer here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        How can I set same font-scale for multiple labels depending on which one is autoshrinked?\r\n                            \r\n                                (1 answer)\r\n                            \r\n                    \r\n                Closed 7 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nWhen you use ```\nUILabel```\n, you can set adjustsFontSizeToFitWidth along with the appropriate scale factor.\n\nHow can I get notified when it scales down? My use-case is that when one label down-sizes, I would like to do it for others around it for uniformity. \n    ", "Answer": "\r\nThere is no observation mechanism, you'd need to calculate the size required to fit the text in the available space and use that size on all.\n\nThe alternative is to use autolayout to allow the text to wrap and then it doesn't need to scale down.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda Expression reduction to NF\r\n                \r\nI need to reduce the following Lambda expressions into Normal Form, using Normal Order. These are my reductions but they don't make sense to me:\n\n```\n//First expression\n(λf.λx.f(fx))(λy.+y1)(+ 2 3)\n(λx.fx)(λy.+y1)(+ 2 3)\nf(λy.+y1)(+ 2 3)\nf(+(+ 2 3) 1)\nf(+ 5 1)\nf 6\n\n//Second expression\nλx. + ((λy.((λx.∗ xy) 2)) x) y)\n+((λy.((λw.* wy) 2) w)\n+((λw.* ww) 2)\n+(* 2 2)\n+ 4\n```\n\n\nBoth NF that I got make no sense. The second one is a + function which requires 2 variables but ends up with only one.\n\nI would appreciate any suggestions and corrections.\n    ", "Answer": "\r\n1.\n\n```\n//First expression\n(λf.λx.f(fx))(λy.+y1)(+ 2 3)\n```\n\n\nI think your mistake here is thinking that the first term can be reduced. I don't think it can, so start applying it to the other terms right away.\n\n2.\n\n```\n//Second expression\nλx. (+ ((λy.((λx.∗ xy) 2)) x) y)  \n```\n\n\nTo reduce this start by applying the innermost term and renaming the last ```\ny```\n so you get:\n\n```\nλx. (+ ((λy.(∗ 2y)) x) w)\n```\n\n\nThen continue :)\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA Thrust reduction with double2 arrays\r\n                \r\nI have the following (compilable and executable) code using CUDA Thrust to perform reductions of ```\nfloat2```\n arrays. It works correctly\n\n```\nusing namespace std;\n\n// includes, system \n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <math.h>\n#include <conio.h>\n\n#include <typeinfo>  \n#include <iostream>\n\n// includes CUDA\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n// includes Thrust\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <thrust/reduce.h>\n\n// float2 + struct\nstruct add_float2 {\n    __device__ float2 operator()(const float2& a, const float2& b) const {\n        float2 r;\n        r.x = a.x + b.x;\n        r.y = a.y + b.y;\n        return r;\n    }\n };\n\n// double2 + struct\nstruct add_double2 {\n    __device__ double2 operator()(const double2& a, const double2& b) const {\n        double2 r;\n        r.x = a.x + b.x;\n        r.y = a.y + b.y;\n        return r;\n    }\n };\n\nvoid main( int argc, char** argv) \n{\n    int N = 20;\n\n    // --- Host\n    float2* ha; ha = (float2*) malloc(N*sizeof(float2));\n    for (unsigned i=0; i<N; ++i) {\n        ha[i].x = 1;\n        ha[i].y = 2;\n    }\n\n    // --- Device\n    float2* da; cudaMalloc((void**)&da,N*sizeof(float2));\n    cudaMemcpy(da,ha,N*sizeof(float2),cudaMemcpyHostToDevice);\n\n    thrust::device_ptr<float2> dev_ptr_1(da);\n    thrust::device_ptr<float2> dev_ptr_2(da+N);\n\n    float2 init; init.x = init.y = 0.0f;\n\n    float2 sum = thrust::reduce(dev_ptr_1,dev_ptr_2,init,add_float2());\n\n    cout << \" Real part = \" << sum.x << \"; Imaginary part = \" << sum.y << endl;\n\n    getch();\n\n }\n```\n\n\nHowever, when I change ```\nfloat2```\n to ```\ndouble2```\n in the ```\nmain```\n program, namely\n\n```\nvoid main( int argc, char** argv) \n{\n    int N = 20;\n\n    // --- Host\n    double2* ha; ha = (double2*) malloc(N*sizeof(double2));\n    for (unsigned i=0; i<N; ++i) {\n        ha[i].x = 1;\n        ha[i].y = 2;\n    }\n\n    // --- Device\n    double2* da; cudaMalloc((void**)&da,N*sizeof(double2));\n    cudaMemcpy(da,ha,N*sizeof(double2),cudaMemcpyHostToDevice);\n\n    thrust::device_ptr<double2> dev_ptr_1(da);\n    thrust::device_ptr<double2> dev_ptr_2(da+N);\n\n    double2 init; init.x = init.y = 0.0;\n\n    double2 sum = thrust::reduce(dev_ptr_1,dev_ptr_2,init,add_double2());\n\n    cout << \" Real part = \" << sum.x << \"; Imaginary part = \" << sum.y << endl;\n\n    getch();\n\n}\n```\n\n\nI receive an ```\nexception```\n at the ```\nreduce```\n line. How can I use CUDA Thrust reduction with ```\ndouble2```\n arrays? Am i doing anything wrong? Thanks in advance.\n\nWORKING SOLUTION FOLLOWING TALONMIES' ANSWER\n\nusing namespace std;\n\n```\n// includes, system\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <math.h>\n#include <conio.h>\n\n#include <typeinfo>\n#include <iostream>\n\n// includes CUDA\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n// includes Thrust\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <thrust/reduce.h>\n\nstruct my_double2 {\n    double x, y;\n};\n\n// double2 + struct\nstruct add_my_double2 {\n    __device__ my_double2 operator()(const my_double2& a, const my_double2& b) const {\n        my_double2 r;\n        r.x = a.x + b.x;\n        r.y = a.y + b.y;\n        return r;\n    }\n};\n\nvoid main( int argc, char** argv) \n{\n    int N = 20;\n\n    // --- Host\n    my_double2* ha; ha = (my_double2*) malloc(N*sizeof(my_double2));\n    for (unsigned i=0; i<N; ++i) {\n        ha[i].x = 1;\n        ha[i].y = 2;\n    }\n\n    // --- Device\n    my_double2* da; cudaMalloc((void**)&da,N*sizeof(my_double2));\n    cudaMemcpy(da,ha,N*sizeof(my_double2),cudaMemcpyHostToDevice);\n\n    thrust::device_ptr<my_double2> dev_ptr_1(da);\n    thrust::device_ptr<my_double2> dev_ptr_2(da+N);\n\n    my_double2 init; init.x = init.y = 0.0;\n\n    cout << \"here3\\n\";\n    my_double2 sum = thrust::reduce(dev_ptr_1,dev_ptr_2,init,add_my_double2());\n\n    cout << \" Real part = \" << sum.x << \"; Imaginary part = \" << sum.y << endl;\n\n    getch();\n\n}\n```\n\n    ", "Answer": "\r\nThis is a known incompatibility with MSVC and nvcc. See here for example. The solution is to define your own version of ```\ndouble2```\n  and use that instead. \n\nJust for reference, I can compile and run your code correctly on a Linux 64 bit box with CUDA 5.5.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Multigraph reduction in O(m+n) time\r\n                \r\nConsider a multigraph G, where the following three reductions need to be made:\n\nVertices with two neighbors are removed from the graph and their neighbors joined to each other via a new edge.\nVertices with one neighbor are removed from the graph.\nDuplicate edges are removed from the graph.\n\nThis is a homework question that I had on a recent assignment, where I am asked to show that these three reductions can be done in O(m+n) time. Any help to better understand how to go about doing this is greatly appreciated. Thanks!\n    ", "Answer": "\r\nThis reduction isn't unique: consider a graph with two vertices and one edge, ```\nv-w```\n, which has two possible reductions. I will explain how to get an arbitrary valid reduction.\nYou'll first want to remove duplicate edges: this can be done using a set or a hash-table to identify duplicates, in ```\nO(n+m)```\n time. I'll assume you're storing the graph as a dictionary from vertices to their adjacency sets.\nAfter this, you'll want to iterate over the vertices, and keep a set (or any container with ```\nO(1)```\n membership testing) to store 'to be deleted' vertices. After this first pass over vertices, this will contain any vertices with degree 1 or 2.\nNow, while your 'to be deleted' set isn't empty, you'll:\n\nPop a vertex ```\nv```\n from the set.\nIf ```\nv```\n has degree 0, ignore it.\nIf ```\nv```\n has degree 1 and its neighbor is ```\nw```\n, delete ```\nv```\n from your graph and remove ```\nv```\n from ```\nw's```\n adjacency set. If ```\nw```\n now has degree ```\n1```\n or ```\n2```\n and isn't in the 'to be deleted' set, add it to the set.\n\nOtherwise, ```\nv```\n has degree ```\n2```\n, and two distinct neighbors ```\nu, w```\n.\n\nIf ```\nu```\n and ```\nw```\n are not adjacent: add an edge from ```\nu```\n to ```\nw```\n, remove ```\nv```\n and its edges from your graph.\n\nIf ```\nu```\n and ```\nw```\n are adjacent: remove ```\nv```\n and its edges from your graph. If ```\nu```\n or ```\nw```\n now have degree ```\n1```\n or ```\n2```\n, add them to the 'to be deleted' set.\n\n\nThis does constant work per vertex and edge, but relies upon a certain graph representation of 'adjacency sets' where edges can be deleted in constant time. Converting to and from this representation, given adjacency lists or a list of edges, can be done in ```\nO(m+n)```\n time.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Are lower-bounds established by reductions tight?\r\n                \r\nReducing problem A to problem B means that problem B is at least as hard as A, if not even more so.\n\nIf I can reduce sorting to some other problem X, I know that X has a lower-bound of Omega(n log n).\nIs that lower-bound guaranteed to be a tight lower-bound? I suspect that it shouldn't be, because X is only known to be at least as hard as A -- implying that it could be harder, and have a different lower-bound as a result.\n\nI mean that in the sense that it's correct to say that because insertion sort has a worst-case tight upper-bound of O(n^2), it's also correct to say it has a worst-case running time of O(n^3). It's correct, but not of much practical value -- because we're interested in tight bounds 99% of the time.\n    ", "Answer": "\r\nYou are absolutely right, the bound does not need to be tight.\n\nFor instance, consider a simple example: finding the smallest integer in an array of ```\nn```\n integers. There is an ```\nO(1)```\n space and ```\nO(n)```\n time algorithm to solve this problem every time. However, this problem reduces to the problem of sorting an array of integers via a reduction in ```\nO(1)```\n both ways:\n\n\nTransforming ```\nMININT```\n input to ```\nSORTINT```\n input: use ```\nMININT```\n's input directly for ```\nSORTINT```\n.\nTransforming ```\nSORTINT```\n output to ```\nMININT```\n output: return the first element of the sorted array (assuming elements are sorted in ascending order).\n\n\nSorting certainly does have a lower bound of Omega(n) on worst-case inputs. This is not a tight bound; Omega(n lg n) is tighter for ```\nSORTINT```\n. But the reduction of ```\nMININT```\n to ```\nSORTINT```\n, by itself, does not tell us that.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "clojure loop see values\r\n                \r\nWhen using ```\nreduce```\n there is the ```\nreductions```\n function to see the list of successive reductions. Is there something similar to debug a ```\nloop```\n in Clojure ?\n    ", "Answer": "\r\n@progo is right, you can always add another accumulator, but if you don't really want to do it every time, you could make some utility macro doing that for you, while being semantically equal to the default ```\nloop/recur```\n (i am thinking of pair of ```\nloop+/recur+```\n (the latter would be used implicitly):\n\n```\n(defmacro recur+ [& args]\n  (let [names (repeatedly (count args) gensym)]\n    `(let ~(vec (interleave names args))\n       (recur ~@names (conj ~'&loop-history [~@names])))))\n\n(defmacro loop+ [bindings & body]\n  (let [val-names (repeatedly (/ (count bindings) 2) gensym)\n        vals (take-nth 2 (rest bindings))\n        binding-lefts (take-nth 2 bindings)]\n    `(let [~@(interleave val-names vals)]\n       (loop [~@(interleave binding-lefts val-names)\n              ~'&loop-history [~(vec val-names)]]\n         ~@(clojure.walk/postwalk-replace\n            {'recur 'recur+\n             'loop 'loop+}\n            body)))))\n```\n\n\nas you can see, ```\nloop+```\n introduces the implicit value ```\n&loop-history```\n, and replaces all the inner ```\nloop```\ns and ```\nrecur```\ns with ```\nloop+```\n and ```\nrecur+```\n, while ```\nrecur+```\n adds this implicit var to the ```\nrecur```\n call (the part with ```\nval-names```\n, ```\nvals```\n and ```\nbinging-lefts```\n is essential to avoid the double evaluation of the forms passed to the ```\nloop+```\n).\n\nso, imagine you have some loop like this:\n\n```\nuser> (loop [a 1 b 2]\n        (if (<= b 10)\n          (recur a (inc b))\n          (str a \" \" b)))\n\"1 11\"\n```\n\n\nto use the new loop just call loop+ instead of loop:\n\n```\nuser> (loop+ [a 1 b 2]\n        (if (<= b 10)\n          (recur a (inc b))\n          (str a \" \" b)))\n\"1 11\"\n```\n\n\nit is expanded into the following:\n\n```\n(let*\n  [G__20054 1 G__20055 2]\n  (loop*\n    [a G__20054 b G__20055 &loop-history [[G__20054 G__20055]]]\n    (if (<= b 10)\n      (let*\n        [G__20056 a G__20057 (inc b)]\n        (recur\n          G__20056\n          G__20057\n          (conj &loop-history [G__20056 G__20057])))\n      (str a \" \" b))))\n```\n\n\nnow ```\n&loop-history```\n is totally accessible anywhere inside loop+:\n\n```\nuser> (loop+ [a 1 b 2]\n        (if (<= b 10)\n          (do\n            (println \"history length: \" (count &loop-history)\n                     \"last item: \" (last &loop-history))\n            (recur a (inc b)))\n          {:result (str a \" \" b)\n           :history &loop-history}))\n\n;; history length:  1 last item:  [1 2]\n;; history length:  2 last item:  [1 3]\n;; history length:  3 last item:  [1 4]\n;; history length:  4 last item:  [1 5]\n;; history length:  5 last item:  [1 6]\n;; history length:  6 last item:  [1 7]\n;; history length:  7 last item:  [1 8]\n;; history length:  8 last item:  [1 9]\n;; history length:  9 last item:  [1 10]\n;; {:result \"1 11\", :history [[1 2] [1 3] [1 4] [1 5] [1 6] [1 7] [1 8] [1 9] [1 10] [1 11]]}\n```\n\n\nnotice, that it also introduces ```\n&loop-history```\n for inner loops, without the need to change the source code:\n\n```\nuser> (loop+ [a 1 b 2]\n        (if (<= b 10)\n          (do (println :outer-hist &loop-history)\n              (recur a (inc b)))\n          (loop [a a]\n            (if (>= a -4)\n              (do (println :inner-hist &loop-history)\n                  (recur (dec a)))\n              (str a b)))))\n\n:outer-hist [[1 2]]\n:outer-hist [[1 2] [1 3]]\n:outer-hist [[1 2] [1 3] [1 4]]\n:outer-hist [[1 2] [1 3] [1 4] [1 5]]\n:outer-hist [[1 2] [1 3] [1 4] [1 5] [1 6]]\n:outer-hist [[1 2] [1 3] [1 4] [1 5] [1 6] [1 7]]\n:outer-hist [[1 2] [1 3] [1 4] [1 5] [1 6] [1 7] [1 8]]\n:outer-hist [[1 2] [1 3] [1 4] [1 5] [1 6] [1 7] [1 8] [1 9]]\n:outer-hist [[1 2] [1 3] [1 4] [1 5] [1 6] [1 7] [1 8] [1 9] [1 10]]\n:inner-hist [[1]]\n:inner-hist [[1] [0]]\n:inner-hist [[1] [0] [-1]]\n:inner-hist [[1] [0] [-1] [-2]]\n:inner-hist [[1] [0] [-1] [-2] [-3]]\n:inner-hist [[1] [0] [-1] [-2] [-3] [-4]]\n\"-511\"\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Principal component analysis and feature reductions\r\n                \r\nI have a matrix composed of 35 features, I need to reduce those\n   feature because I think many variable are dependent. I undertsood PCA\n   could help me to do that, so using matlab, I calculated:\n\n```\n [coeff,score,latent] = pca(list_of_features)\n```\n\n\nI notice \"coeff\" contains matrix which I understood (correct me if I'm wrong) have column with high importance on the left, and second column with less importance and so on. However, it's not clear for me which column on \"coeff\" relate to which column on my original \"list_of_features\" so that I could know which variable is more important.\n    ", "Answer": "\r\nPCA doesn't give you an order relation on your original features (which feature is more 'important' then others), rather it gives you directions in feature space, ordered according to the variance, from high variance (1st direction, or principle component) to low variance. A direction is generally a linear combination of your original features, so you can't expect to get information about a single feature.\n\nWhat you can do is to throw away a direction (one or more), or in other words project you data into the sub-space spanned by a subset of the principle components. Usually you want to throw the directions with low variance, but that's really a choice which depends on what is your application.\n\nLet's say you want to leave only the first k principle components:\n\n```\nx = score(:,1:k) * coeff(:,1:k)';\n```\n\n\nNote however that ```\npca```\n centers the data, so you actually get the projection of the centered version of your data.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Is there good method to profile the process for large reductions?\r\n                \r\nI found the process reduction is large in our product environment, and the messages didn't decrease.\n\nFYI, the reduction is 10831243888178 and then 10838818431635 after 5 minutes. The message_queue_len is 1012 and then 1014 according to the reduction.\n\nI supposed that the ```\nmessages```\n returned from ```\nprocess_info(Pid)```\n should be consumed in the 5 minutes but it didn't. Can I say that the process was blocked by some messages?\n\nI read from the web that one reduction can be looked as one function call, but I don't fully understand it. I'll appreciate if someone can tell me more about the \"reduction\".\n    ", "Answer": "\r\nReductions is a way to measure work done by a process. \n\nEvery scheduled process given a number of reductions to spend before preempting, in other words before it will have to let other processes to execute. Calling a function will spend 1 reduction, that seems right, but it is not the only thing that spends them, a lot of reductions will vanish inside this function call too.\n\nIt seems that numbers you given are accumulated reductions spent by a process. A big number by itself do not mean something at all actually. A big increase, however, means that process is doing some hard work. If this workhorse is not consuming the message queue, a great chance it is stuck inside one very long, or even unending computation.\n\nYou may try to inspect it further with ```\nprocess_info(Pid, current_function)```\n or ```\nprocess_info(Pid, current_stacktrace)```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA - Multiple parallel reductions sometimes fail\r\n                \r\nI have the following problem. I have implemented several different parallel reduction algorithms and all of them work correctly if I'm reducing just one value per kernel. But now I need to reduce several (21) and I've just no idea why it's working sometimes and sometimes not.\n\nThe steps performed are:\n\n\ncalculating the relevant values per thread (in the example I simply set them to 1 since it's showing the same behavior)\nload them into shared memory\nsync my threads within the block\nreduce the values down in shared memory\n\n\nHere is the complete code that you can just cpy&pst and run.\n\n```\n#include <stdio.h>\n#include <cuda_runtime.h>\n\n// switch the compiler flag if you don't have the sdk's helper_cuda.h file\n#if 1\n#include \"helper_cuda.h\"\n#else\n#define checkCudaErrors(val) (val)\n#define getLastCudaError(msg)\n#endif\n\n#ifdef __CDT_PARSER__\n#define __global__\n#define __device__\n#define __shared__\n#define __host__\n#endif\n\n// compute sum of val over num threads\n__device__ float localSum(const float& val, volatile float* reductionSpace, const uint& localId)\n{\n    reductionSpace[localId] = val;  // load data into shared mem\n    __syncthreads();\n\n    // complete loop unroll\n    if (localId < 128) reductionSpace[localId] += reductionSpace[localId + 128];\n    __syncthreads();\n\n    if (localId < 64) reductionSpace[localId] += reductionSpace[localId + 64];\n    __syncthreads();\n\n    // within one warp (=32 threads) instructions are SIMD synchronous\n    // -> __syncthreads() not needed\n    if (localId < 32)\n    {\n        reductionSpace[localId] += reductionSpace[localId + 32];\n        reductionSpace[localId] += reductionSpace[localId + 16];\n        reductionSpace[localId] += reductionSpace[localId + 8];\n        reductionSpace[localId] += reductionSpace[localId + 4];\n        reductionSpace[localId] += reductionSpace[localId + 2];\n        reductionSpace[localId] += reductionSpace[localId + 1];\n    }\n\n    ## Edit: Here we need to sync in order to guarantee that the thread with ID 0 is also done... ##\n    __syncthreads();\n\n    return reductionSpace[0];\n}\n\n__global__ void d_kernel(float* od, int n)\n{\n    extern __shared__ float reductionSpace[];\n    int g_idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const unsigned int linId = threadIdx.x;\n    __shared__ float partialSums[21];\n\n    float tmp[6] =\n    { 0, 0, 0, 0, 0, 0 };\n\n    // for simplification all computations are remove - this version still shows the same behaviour\n    if (g_idx < n)\n    {\n        tmp[0] = 1.0f;\n        tmp[1] = 1.0f;\n        tmp[2] = 1.0f;\n        tmp[3] = 1.0f;\n        tmp[4] = 1.0f;\n        tmp[5] = 1.0f;\n    }\n\n    float res = 0.0f;\n    int c = 0;\n    for (int i = 0; i < 6; ++i)\n    {\n        for (int j = i; j < 6; ++j, ++c)\n        {\n            res = tmp[i] * tmp[j];\n            // compute the sum of the values res for blockDim.x threads. This uses\n            // the shared memory reductionSpace for calculations\n            partialSums[c] = localSum(res, reductionSpace, linId);\n\n        }\n    }\n    __syncthreads();\n\n    // write back the sum values for this block\n    if (linId < 21)\n    {\n        atomicAdd(&od[linId], partialSums[linId]);\n    }\n}\n\nint main()\n{\n    int w = 320;\n    int h = 240;\n    int n = w * h;\n\n    // ------------------------------------------------------------------------------------\n    float *d_out;\n    checkCudaErrors(cudaMalloc(&d_out, 21 * sizeof(float)));\n    float* h_out = new float[21];\n\n    int dimBlock = 256;\n    int dimGrid = (n - 1) / dimBlock + 1;\n    int sharedMemSize = dimBlock * sizeof(float);\n\n    printf(\"w: %d\\n\", w);\n    printf(\"h: %d\\n\", h);\n    printf(\"dimBlock: %d\\n\", dimBlock);\n    printf(\"dimGrid: %d\\n\", dimGrid);\n    printf(\"sharedMemSize: %d\\n\", sharedMemSize);\n\n    int failcounter = 0;\n    float target = (float) n;\n    int c = 0;\n    // ------------------------------------------------------------------------------------\n\n    // run the kernel for 200 times\n    for (int run = 0; run < 200; ++run)\n    {\n        cudaMemset(d_out, 0, 21 * sizeof(float));\n        d_kernel<<<dimGrid, dimBlock, sharedMemSize>>>(d_out, n);;\n        getLastCudaError(\"d_kernel\");\n\n        checkCudaErrors(cudaMemcpy(h_out, d_out, 21 * sizeof(float), cudaMemcpyDeviceToHost));\n\n        // check if the output has target value\n        // since all threads get value 1 the kernel output corresponds to counting the elements which is w*h=n\n        bool failed = false;\n        for (int i = 0; i < 21; ++i)\n        {\n            if (abs(h_out[i] - target) > 0.01f)\n            {\n                ++failcounter;\n                failed = true;\n            }\n        }\n\n        // if failed, print the elements to show which one failed\n        if (failed)\n        {\n            c = 0;\n            for (int i = 0; i < 6; ++i)\n            {\n                for (int j = i; j < 6; ++j, ++c)\n                {\n                    printf(\"%10.7f \", h_out[c]);\n                }\n                printf(\"\\n\");\n            }\n        }\n    }\n\n    printf(\"failcounter: %d\\n\", failcounter);\n\n    // ------------------------------------------------------------------------------------\n    delete[] h_out;\n    checkCudaErrors(cudaFree(d_out));\n    // ------------------------------------------------------------------------------------\n\n    return 0;\n}\n```\n\n\nSome comments:\n\nBlockSize is always 256 - so the unrolled loop in localSum() checks for the right threadIds.\nLike mentioned at the beginning, out of 200 runs it's sometimes completely correct, sometimes only 2 values are wrong and sometimes 150 or so are wrong. \n\nAnd it doesn't have to to anything with floating point precision since only 1x1 is multiplied and stored in the variable res in d_kernel(). I can clearly see that sometimes just some threads or blocks don't get started, but I don't know why. :/\n\nJust from looking at the results it should be obvious that there is some kind of race-condition but I simply can't see the problem.\n\nHas anyone an idea where the problem is?\n\nEdit:\n\nI tested now a lot of things and I saw that it has to do something with the BlockSize. If I reduce it to smth <=64 and change the localSum() accordingly then everything is working always as expected.\n\nBut that simply makes no sense to me?! I still do nothing else here than a normal parallel reduction with shared memory with the only difference that I do it 21 times per thread.\n\nEdit 2:\n\nNow I'm completely confused. The problem is unrolling the loop!! Or better said synchronizing the warp. The following localSum() code works:\n\n```\n// compute sum of val over num threads\n__device__ float localSum(const float& val, volatile float* reductionSpace, const uint& localId)\n{\n    reductionSpace[localId] = val;  // load data into shared mem\n    __syncthreads();\n\n    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1)\n    {\n        if (localId < s)\n        {\n            reductionSpace[localId] += reductionSpace[localId + s];\n        }\n\n        __syncthreads();\n    }\n\n    return reductionSpace[0];\n}\n```\n\n\nBut if I unroll the last warp and do not synchronize between the threads, I get again sometimes like 2 or 3 wrong results out of 2000 runs.\nSo the following code does NOT work:\n\n```\n// compute sum of val over num threads\n__device__ float localSum(const float& val, volatile float* reductionSpace, const uint& localId)\n{\n    reductionSpace[localId] = val;  // load data into shared mem\n    __syncthreads();\n\n    for (unsigned int s = blockDim.x / 2; s > 32; s >>= 1)\n    {\n        if (localId < s)\n        {\n            reductionSpace[localId] += reductionSpace[localId + s];\n        }\n\n        __syncthreads();\n    }\n\n    if (localId < 32)\n    {\n        reductionSpace[localId] += reductionSpace[localId + 32];\n        reductionSpace[localId] += reductionSpace[localId + 16];\n        reductionSpace[localId] += reductionSpace[localId + 8];\n        reductionSpace[localId] += reductionSpace[localId + 4];\n        reductionSpace[localId] += reductionSpace[localId + 2];\n        reductionSpace[localId] += reductionSpace[localId + 1];\n    }\n\n    return reductionSpace[0];\n}\n```\n\n\nBut how does that make sense, since CUDA executes one warp (32 threads) simultaneously and no __syncthreads() is needed?!\n\nI don't need someone to post me working code here but I'm really asking someone with a lot of experience and deep knowledge in CUDA programming to describe me the underlying problem here. Or at least to give me a hint.\n    ", "Answer": "\r\nThe solution is so easy that I'm nearly ashamed to tell it. I was so blinded and looked everywhere but not at the most obvious code. A simple __syncthreads() was missing before the return statement in localSum(). Bc the last warp itself is beeing executed simultaneously but it's not guaranteed that the one with threadID 0 is done... Such a stupid mistake and I just didn't see it.\n\nSorry for all the trouble.. :)\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "reduction of stream of object does not work for me\r\n                \r\nI wrote several reductions, where I had array to begin with. But if I try to read raw data and transform each line into object, I don't have much luck reducing them together\n```\necho -e \"1\\n2\\n\\n\\n3\\n4\\n5\" | jq --raw-input '. | select (. != \"\") | {(.):123} | reduce . as $i ({}; . + $i)'\n```\n\nthe reduction does nothing. Why? How to correct the reduction to produce single object having keys 1,2,3,4,5?\n    ", "Answer": "\r\nFirst, the initial ```\n.|```\n is unnecessary.\nSecond, since your input is a stream, you will either need to use the -s option, or better, use the -n option with ```\ninputs```\n.\nSo you could go with:\n```\necho -e \"1\\n2\\n\\n\\n3\\n4\\n5\" | \n  jq -nR 'reduce (inputs|select(. != \"\")) as $i ({}; . + {($i): 123})'\n```\n\nthough maybe ```\n{($i): null}```\n might be more appropriate.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "reduction with OpenMP with SSE/AVX\r\n                \r\nI want to do a reduction on an array using OpenMP and SIMD.  I read that a reduction in OpenMP is equivalent to:\n\n```\ninline float sum_scalar_openmp2(const float a[], const size_t N) {\n    float sum = 0.0f;\n    #pragma omp parallel\n    {\n        float sum_private = 0.0f;\n        #pragma omp parallel for nowait\n        for(int i=0; i<N; i++) {\n            sum_private += a[i];\n        }\n        #pragma omp atomic\n        sum += sum_private;\n    }\n    return sum;\n}\n```\n\n\nI got this idea from the follow link:\nhttp://bisqwit.iki.fi/story/howto/openmp/#ReductionClause\nBut atomic also does not support complex operators.  What I did was replace atomic with critical  and implemented the reduction with OpenMP and SSE like this:\n\n```\n#define ROUND_DOWN(x, s) ((x) & ~((s)-1))\ninline float sum_vector4_openmp(const float a[], const size_t N) {\n    __m128 sum4 = _mm_set1_ps(0.0f);\n    #pragma omp parallel \n    {\n        __m128 sum4_private = _mm_set1_ps(0.0f);\n        #pragma omp for nowait\n        for(int i=0; i < ROUND_DOWN(N, 4); i+=4) {\n            __m128 a4 = _mm_load_ps(a + i);\n            sum4_private = _mm_add_ps(a4, sum4_private);\n        }\n        #pragma omp critical\n        sum4 = _mm_add_ps(sum4_private, sum4);\n    }\n    __m128 t1 = _mm_hadd_ps(sum4,sum4);\n    __m128 t2 = _mm_hadd_ps(t1,t1);\n    float sum = _mm_cvtss_f32(t2);  \n    for(int i = ROUND_DOWN(N, 4); i < N; i++) {\n        sum += a[i];\n    }\n    return sum;\n} \n```\n\n\nHowever, this function does not perform as well as I hope.  I'm using Visual Studio 2012 Express.  I know I can improve the performance a bit by unrolling the SSE load/add a few times but that still is less than I expect.  \n\nI get much better performance by running over slices of the arrays equal to the number of threads:\n\n```\ninline float sum_slice(const float a[], const size_t N) {\n    int nthreads = 4;\n    const int offset = ROUND_DOWN(N/nthreads, nthreads);\n    float suma[8] = {0};\n    #pragma omp parallel for num_threads(nthreads) \n    for(int i=0; i<nthreads; i++) {\n        suma[i] = sum_vector4(&a[i*offset], offset);\n    }\n    float sum = 0.0f;\n    for(int i=0; i<nthreads; i++) {\n        sum += suma[i]; \n    }\n    for(int i=nthreads*offset; i < N; i++) {\n        sum += a[i];\n    }\n    return sum;    \n}\n\ninline float sum_vector4(const float a[], const size_t N) {\n    __m128 sum4 = _mm_set1_ps(0.0f);\n    int i = 0;\n    for(; i < ROUND_DOWN(N, 4); i+=4) {\n        __m128 a4 = _mm_load_ps(a + i);\n        sum4 = _mm_add_ps(sum4, a4);\n    }\n    __m128 t1 = _mm_hadd_ps(sum4,sum4);\n    __m128 t2 = _mm_hadd_ps(t1,t1);\n    float sum = _mm_cvtss_f32(t2);\n    for(; i < N; i++) {\n        sum += a[i];\n    }\n    return sum;\n```\n\n\n}\n\nDoes someone know if there is a better way of doing reductions with more complicated operators in OpenMP?\n    ", "Answer": "\r\nI guess the answer to your question is No. I don't think there is a better way of doing reduction with more complicated operators in OpenMP.\n\nAssuming that the array is 16 bit aligned, number of openmp threads is 4, one might expect the performance gain to be 12x - 16x by OpenMP + SIMD. In realistic, it might not produce enough performance gain because\n\n\nThere is a overhead in creating the openmp threads.\nThe code is doing 1 addition operation for 1 Load  operation. Hence, the CPU isn't doing enough computation. So, it almost looks like the CPU spends most of the time in loading the data, kind of memory bandwidth bound.\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Terminating unactive threads during reduction\r\n                \r\nMost reductions I've ever seen look like:\n\n\nfor( i = N; i > 0; i /=2 ) {\n    if( tid < i )\n        assign-shared;\n    __syncthreads();\n}\nif( tid == 0 )\n    copy-value-to-global;\n\n\nI've just reversed that to:\n\n\nfor( i = N; i > 0; i /= 2 ) {\n    if( tid >= i )\n        return;\n    assign-shared;\n    __syncthreads();\n}\ncopy-value-to-global;\n\n\nand noticed a substantial performance benefit.  Is there any drawback to having the threads that are no longer involved in the reduction return early? \n    ", "Answer": "\r\nSince you're already performing an ```\nif```\n statement with your original code, I don't see any drawback. \n\nIf the results of your ```\nif```\n statement did not have spatial locality (generally the same result across the block), you may not see any speedup.  Also, the speedup may be dependent on the capabilities of your device: earlier CUDA devices may not give you the performance enhancement.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to do an ordered reduction in OpenMP\r\n                \r\nOpenMP 4.5+ provides the capability to do vector/array reductions in C++ (press release)\n\nUsing said capability allows us to write, e.g.:\n\n```\n#include <vector>\n#include <iostream>\n\nint main(){\n  std::vector<int> vec;\n\n  #pragma omp declare reduction (merge : std::vector<int> : omp_out.insert(omp_out.end(), omp_in.begin(), omp_in.end()))\n\n  #pragma omp parallel for default(none) schedule(static) reduction(merge: vec)\n  for(int i=0;i<100;i++)\n    vec.push_back(i);\n\n  for(const auto x: vec)\n    std::cout<<x<<\"\\n\";\n\n  return 0;\n}\n```\n\n\nThe problem is, upon executing such code, the results of the various threads may be ordered in any which way.\n\nIs there a way to enforce order such that thread 0's results preceed thread 1's, and so on?\n    ", "Answer": "\r\nThe order of a reduction is explicitly not specified. (\"The location in the OpenMP program at which the values are combined and the order in which the\nvalues are combined are unspecified.\", 2.15.3.6 in OpenMP 4.5). Therefore you cannot use a reduction.\n\nOne way would be to use ordered as follows:\n\n```\nstd::vector<int> vec;\n#pragma omp parallel for default(none) schedule(static) shared(vec)\nfor(int i=0;i<100;i++) {\n    // do some computations here\n    #pragma omp ordered\n    vec.push_back(i);\n}\n```\n\n\nNote that ```\nvec```\n is now shared, and ```\nordered```\n implies a serialization of execution and synchronization among threads. This can be very bad for performance except if each of your computations require a significant and uniform amount of time.\n\nYou can make a custom ordered reduction. Split the ```\nparallel```\n region from ```\nfor```\n loop and manually insert the local results in a sequential order.\n\n```\nstd::vector<int> global_vec;\n#pragma omp parallel\n{\n    std::vector<int> local_vec;\n    #pragma omp for schedule(static)\n    for (int i=0; i < 100; i++) {\n        // some computations\n        local_vec.push_back(i);\n    }\n    for (int t = 0; t < omp_get_num_threads(); t++) {\n        #pragma omp barrier\n        if (t == omp_get_thread_num()) {\n            global_vec.insert(local_vec.begin(), local_vec.end())\n        }\n    }\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reductions from Vertex Cover to LP\r\n                \r\nI want to reduce the vertex cover problem to a specific decision problem.  This decision problem is the following: \n\n\n  I have a  nxn matrix A, a vector b in R^n, and a positive integer k.  Does there exists a vector x in R^n with at most k non-zero entries such that A*x is greater than or equal to b?\n\n\nI was thinking that A could be viewed as an adjacency matrix, but I'm not sure how to reduce the vertex cover problem to this problem. \n\nCan anyone give me a hint or two on what I should do next?  \n\nEDIT**I originally thought about using dominating set problem, but after thinking through the problem a little more, I thought I should use vertex cover instead. (so question originally referred to dominating set)\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Accumulate partial reductions into array in numpy\r\n                \r\nProblem description\n\nHow do I accumulate into ```\na```\n the values in ```\nc```\n using ```\nb```\n to index into ```\na```\n? That is, given\n\n```\nimport numpy as np\na = np.zeros(3)\nb = np.array([2, 1, 0, 1])\nc = np.arange(0.1, 0.5, 0.1)\nprint ('a=%s b=%s c=%s'.replace(' ', '\\n') % (str(a), str(b), str(c)))\n```\n\n\nwhich outputs\n\n```\na=[ 0.  0.  0.]\nb=[2 1 0 1]\nc=[ 0.1  0.2  0.3  0.4]\n```\n\n\nhow do I achieve\n\n```\nd = np.array([0.3, 0.2 + 0.4, 0.1])\nprint 'd=%s' % str(d)\n```\n\n\nwhich outputs\n\n```\nd=[ 0.3  0.6  0.1]\n```\n\n\nusing ```\na```\n, ```\nb```\n, and ```\nc```\n without using a for loop?\n\nMy solution attempt\n\nI can sort ```\nb```\n and then sort ```\nc```\n using the indices that sorted ```\nb```\n\n\n```\np = b.argsort()\nprint ('b[p]=%s c[p]=%s'.replace(' ', '\\n') % (str(b[p]), str(c[p])))\n```\n\n\nwhich outputs\n\n```\nb[p]=[0 1 1 2]\nc[p]=[ 0.3  0.2  0.4  0.1]\n```\n\n\nthen reduce ```\nb```\n to occurrence counts\n\n```\nocc = np.bincount(b[p])\nprint 'occ=%s' % str(occ)\n```\n\n\nwhich outputs\n\n```\nocc=[1 2 1]\n```\n\n\nand use this to compute partials sums\n\n```\nprint np.array([np.sum(c[p][0:occ[0]]),\n                np.sum(c[p][occ[0]:occ[0]+occ[1]]),\n                np.sum(c[p][occ[0]+occ[1]:occ[0]+occ[1]+occ[2]])])\n```\n\n\nwhich outputs\n\n```\n[ 0.3  0.6  0.1]\n```\n\n\nHow do I generalize this?\n\nAll code and output\n\n```\nimport numpy as np\na = np.zeros(3)\nb = np.array([2, 1, 0, 1])\nc = np.arange(0.1, 0.5, 0.1)\nprint ('a=%s b=%s c=%s'.replace(' ', '\\n') % (str(a), str(b), str(c)))\nd = np.array([0.3, 0.2 + 0.4, 0.1])\nprint 'd=%s' % str(d)\np = b.argsort()\nprint ('b[p]=%s c[p]=%s'.replace(' ', '\\n') % (str(b[p]), str(c[p])))\nocc = np.bincount(b[p])\nprint 'occ=%s' % str(occ)\nprint np.array([np.sum(c[p][0:occ[0]]),\n                np.sum(c[p][occ[0]:occ[0]+occ[1]]),\n                np.sum(c[p][occ[0]+occ[1]:occ[0]+occ[1]+occ[2]])])\n```\n\n\nwhich outputs\n\n```\na=[ 0.  0.  0.]\nb=[2 1 0 1]\nc=[ 0.1  0.2  0.3  0.4]\nd=[ 0.3  0.6  0.1]\nb[p]=[0 1 1 2]\nc[p]=[ 0.3  0.2  0.4  0.1]\nocc=[1 2 1]\n[ 0.3  0.6  0.1]\n```\n\n    ", "Answer": "\r\n```\nnp.bincount```\n does exactly what you want:\n\n```\n>>> import numpy as np\n>>> \n>>> b = [2, 1, 0, 1]\n>>> c = np.arange(0.1, 0.5, 0.1)\n>>> c\narray([0.1, 0.2, 0.3, 0.4])\n>>> np.bincount(b, c)\narray([0.3, 0.6, 0.1])\n```\n\n\nThere is also ```\nnp.add.at```\n but unless the update is very sparse in ```\na```\n it is much slower.\n\n```\n>>> a = np.zeros(3)\n>>> np.add.at(a, b, c)\n>>> a\narray([0.3, 0.6, 0.1])\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Parallel reduction on CUDA with array in device\r\n                \r\nI need to perform a parallel reduction to find the min or max of an array on a CUDA device. I found a good library for this, called Thrust. It seems that you can only perform a parallel reduction on arrays in host memory. My data is in device memory. Is it possible to perform a reduction on data in device memory?\nI can't figure how to do this. Here is documentation for Thrust: http://code.google.com/p/thrust/wiki/QuickStartGuide#Reductions. Thank all of you.\n    ", "Answer": "\r\nYou can do reductions in thrust on arrays which are already in device memory. All that you need to do is wrap your device pointers inside ```\nthrust::device_pointer```\n containers, and call one of the reduction procedures, just as shown in the wiki you have linked to:\n\n```\n// assume this is a valid device allocation holding N words of data\nint * dmem;\n\n// Wrap raw device pointer \nthrust::device_ptr<int> dptr(dmem);\n\n// use max_element for reduction\nthrust::device_ptr<int> dresptr = thrust::max_element(dptr, dptr+N);\n\n// retrieve result from device (if required)\nint max_value = dresptr[0];\n```\n\n\nNote that the return value is also a ```\ndevice_ptr```\n, so you can use it directly in other kernels using ```\nthrust::raw_pointer_cast```\n:\n\n```\nint * dres = thrust::raw_pointer_cast(dresptr); \n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Determining time complexity of solution by reductions\r\n                \r\nSuppose that you found a solution to the ```\nA```\n problem and are trying to get some idea of its complexity. You solve ```\nA```\n by calling your ```\nB```\n sub-routine a total of n^2 times and also doing a constant amount of additional work.\n\nIf ```\nB```\n is selection sort, what is the time complexity of this solution?\n\nIf ```\nB```\n is merge sort, what is the time complexity of this solution?\n\n\nMy answer to 1st question is ```\nn^2```\n and to 2nd one is ```\nnlogn```\n. Any idea will be appreciated about my answers.\n    ", "Answer": "\r\nI assume that by \"solution\" you mean \"algorithm\", and by \"this solution\", you mean the algorithm that solves problem ```\nA```\n by calling ```\nB```\n ```\nn^2```\n times. Furthermore, I assume that by ```\nn```\n you mean the size of the input.\nThen if ```\nB```\n is selection sort, which is an ```\nO(n^2)```\n algorithm, the algorithm for solving ```\nA```\n would be ```\nO(n^2 * n^2) = O(n^4)```\n.\nIf ```\nB```\n is merge sort, which is ```\nO(n log n)```\n, the algorithm for solving ```\nA```\n would be ```\nO(n^2* n log n) = O(n^3 log n)```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Does reduction on an ordered stream reduce in order?\r\n                \r\nI have a ```\nList```\n of A, B, C.\n\n```\nC reduce A reduce B !=  A reduce B reduce C```\n (however, A reduce (B reduce C) is OK). \n\nIn other words, my reduction operation is associative but not commutative.\n\nDoes java enforce on an ordered sequential stream (such as the default one from a list) that reduction will always happen according to the encounter order?  That is to say, will java reorder reductions (such that B reduce A instead of A reduce B)?\n\n(Hopefully this is clear enough).\n\nedit to add a little demo and maybe helping to clarify\n\n```\nStream.of(\" cats \", \" eat \", \" bats \")\n  .reduce(\"\", (a, b) -> a + b); // cats eat bats\n```\n\n\nWith the above, could the output ever be \"bats cats eat\" or \"eat bats cats\"?  Is that guaranteed somewhere in the spec?\n    ", "Answer": "\r\nAccording to the specification it respects the order of the elements.\nA proof is very simple. The specification claims that a reduction function has to be associative.\nHowever, associativity it self doesn't make any sense if the order is not preserved. According to the mathematical definition of the associative property:\n\nWithin an expression containing two or more occurrences in a row of\nthe same associative operator, the order in which the operations are\nperformed does not matter as long as the sequence of the operands is\nnot changed.\n\nIn other words, associative property doesn't imply that:\n```\n(a + b) + c = (a + c) + b\n```\n\nIt only allows an arbitrary permutation of the order in which operations are applied.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Cormen's Introduction to algorithms - reductions - is the proof correct?\r\n                \r\nThe theorem says: Suppose we got a problem A with no poly-time algorithm. If we can reduce it in polynomial time to problem B, then no poly-time algorithm solving B exists.\n\nThe proof can be found in Cormen's \"Introduction to algorithms\", chapter 34 in 3rd edition.\n\nAs we can see, two assumptions have been made here:\n\na) there exists no polynomial-time algorithm solving A\n\nb) there exists polynomial-time reduction algorithm from A to B\n\nThis theorem has been proved by assuming that there exists a poly-time algorithm for B, which lead to contradiction. However, what if the contradiction results from the fact that our assumption b) was incorrect?\nI'm having doubts here because all the theorem says is: if problem A is hard, and we can quickly transform it to problem B, then B is hard as well, which is not a very intuitive conclusion. Thank you for help.\n\n\n\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to make multiple eta reductions in Haskell\r\n                \r\nI have a task to get a column from a ```\n[[a]]```\n matrix.\n\nA simple solution would be\n\n```\ncolFields :: Int -> [[a]] -> [a]\ncolFields n c = map (!! n) c\n```\n\n\nand when reduced by one level of abstraction it would be\n\n```\ncolFields n = map (!! n)\n```\n\n\nI sense that I could get rid of ```\nn```\n easily, but I can't do it.\n    ", "Answer": "\r\nWhat you're looking for is\n\n```\ncolFields = map . flip (!!)\n```\n\n\nHowever, this is not very clear to read, I'd leave the ```\nn```\n parameter in there.  With the ```\nn```\n as an explicit parameter, I understand immediately what the function does.  Without it, I have to think for a minute in order to understand the definition, even for a simple case like this.\n\nI obtained this answer very simply by using the pointfree tool, although there are methods for deriving this by hand.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Algebraic reductions of signed integer expressions in C/C++\r\n                \r\nI wanted to see if GCC would reduce ```\na - (b - c)```\n to ```\n(a + c) - b```\n with signed and unsigned integers so I created two tests\n\n```\n//test1.c\nunsigned fooau(unsigned a, unsigned b, unsigned c) { return a - (b - c); }\nsigned   fooas(signed   a, signed   b, signed   c) { return a - (b - c); }\nsigned   fooms(signed   a) { return a*a*a*a*a*a; }\nunsigned foomu(unsigned a) { return a*a*a*a*a*a; }  \n\n//test2.c\nunsigned fooau(unsigned a, unsigned b, unsigned c) { return (a + c) - b; }\nsigned   fooas(signed   a, signed   b, signed   c) { return (a + c) - b; }\nsigned   fooms(signed   a) { return (a*a*a)*(a*a*a); }\nunsigned foomu(unsigned a) { return (a*a*a)*(a*a*a); }\n```\n\n\nI compiled first with ```\ngcc -O3 test1.c test2.c -S```\n and looked at the assembly.  For both tests ```\nfooau```\n were identical however ```\nfooas```\n was not.\n\nAs far as I understand unsigned arithmetic can be derived from the following formula\n\n```\n(a%n + b%n)%n = (a+b)%n\n```\n\n\nwhich can be used to show that unsigned arithmetic is associative. But since signed overflow is undefined behavior  this equality does not necessarily hold for signed addition (i.e. signed addition is not associative) which explains why GCC did not reduce ```\na - (b - c)```\n to ```\n(a + c) - b```\n for signed integers. But we can tell GCC to use this formula using ```\n-fwrapv```\n. Using this option ```\nfooas```\n for both tests is identical.\n\nBut what about multiplication?  For both tests ```\nfooms```\n and ```\nfoomu```\n were simplified to three multiplications (```\na*a*a*a*a*a to (a*a*a)*(a*a*a)```\n). But multiplication can be written as repeated addition so using the formula above I think it can be shown that \n\n```\n((a%n)*(b%n))%n = (a*b)%n\n```\n\n\nwhich I think can also show that unsigned modular multiplication is associative as well.  But since GCC used only three multiplications for ```\nfoomu```\n this shows that GCC assumes signed integer multiplication is associative.\n\nThis seems like a contradiction to me. For addition signed arithmetic was not associative but for multiplication it is.\n\nTwo questions:\n\n\nIs it true that addition is not associative with signed integers but multiplication is in C/C++?\nIf signed overflow is used for optimization isn't the fact that GCC not reducing the algebraic expression a failure to optimize? Wouldn't it better better for optimization to use ```\n-fwrapv```\n (I understand that ```\na - (b - c)```\n to ```\n(a + c) - b```\n is not much of a reduction but I'm worried about more complicated cases)? Does this mean for optimization sometimes using ```\n-fwrapv```\n is more efficient and sometimes it's not?\n\n    ", "Answer": "\r\n\nNo, multiplication is not associative in signed integers. Consider ```\n(0 * x) * x```\n vs. ```\n0 * (x * x)```\n - the latter has potentially undefined behavior while the former is always defined.\nThe potential for undefined behavior only ever introduces new optimization opportunities, the classic example being optimizing ```\nx + 1 > x```\n to ```\ntrue```\n for signed ```\nx```\n, an optimization that is not available for unsigned integers.\n\n\nI don't think you can assume that gcc failing to change ```\na - (b - c)```\n to ```\n(a + c) - b```\n represents a missed optimization opportunity; the two calculations compile to the same two instructions on x86-64 (```\nleal```\n and ```\nsubl```\n), just in a different order.\n\nIndeed, the implementation is entitled to assume that arithmetic is associative, and use that for optimizations, since anything can happen on UB including modulo arithmetic or infinite-range arithmetic. However, you as the programmer are not entitled to assume associativity unless you can guarantee that no intermediate result overflows.\n\nAs another example, try ```\n(a + a) - a```\n - gcc will optimize this to ```\na```\n for signed ```\na```\n as well as for unsigned.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "numpy-based spatial reduction\r\n                \r\nI'm looking for a flexible fast method for computing a custom reduction on an np.array using a square non-overlapping window. e.g.,\n```\narray([[4, 7, 2, 0],\n       [4, 9, 4, 2],\n       [2, 8, 8, 8],\n       [6, 3, 5, 8]])\n```\n\nlet's say I want the ```\nnp.max```\n, (on a 2x2 window in this case) I'd like to get:\n```\narray([[9, 4],\n       [8, 8]])\n```\n\nI've built a slow function using for loops, but ultimately I need to apply this to large raster arrays.\nscipy.ndimage.generic_filter is close, but this uses sliding windows (with overlap), giving a result with the same dimensions (no reduction).\nnumpy.lib.stride_tricks.as_strided combined with a reducing function doesn't seem to handle relationships between rows (i.e., 2D spatial awareness).\nrasterio has some nice resampling methods built on GDAL, but these don't allow for custom reductions.\nskimage.transform.downscale_local_mean does not support custom functions on the blocks.\nI'm sure there's something out there for custom spatial anti-aliasing, but I can't seem to find a solution and am feeling dumb.\nAny help is greatly appreciated,\n    ", "Answer": "\r\nWith the max (or other function supporting axis), you can just reshape the array:\n```\na.reshape(a.shape[0]//2, 2, a.shape[1]//2, 2).max(axis=(1,3))\n```\n\nIn general, you can reshape, swap the axis, flatten the 2x2 into a new axis ```\n4```\n, then work on that axis.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "JAX(XLA) vs Numba(LLVM) Reduction\r\n                \r\nIs it possible to make CPU only reductions with JAX comparable to Numba in terms of computation time?\nThe compilers come straight from ```\nconda```\n:\n```\n$ conda install -c conda-forge numba jax\n```\n\nHere is a 1-d NumPy array example\n```\nimport numpy as np\nimport numba as nb\nimport jax as jx\n\n@nb.njit\ndef reduce_1d_njit_serial(x):\n    s = 0\n    for xi in x:\n        s += xi\n    return s\n\n@jx.jit\ndef reduce_1d_jax_serial(x):\n    s = 0\n    for xi in x:\n        s += xi\n    return s\n\nN = 2**10\na = np.random.randn(N)\n```\n\nUsing ```\ntimeit```\n on the following\n\n```\nnp.add.reduce(a)```\n gives ```\n1.99 µs ...```\n\n```\nreduce_1d_njit_serial(a)```\n gives ```\n1.43 µs ...```\n\n```\nreduce_1d_jax_serial(a).item()```\n gives ```\n23.5 µs ...```\n\n\nNote that ```\njx.numpy.sum(a)```\n and using ```\njx.lax.fori_loop```\n gives comparable (marginally slower) comp. times to ```\nreduce_1d_jax_serial```\n.\nIt seems there is a better way to craft the reduction for XLA.\nEDIT: compile times were not included as a print statement proceeded to check results.\n    ", "Answer": "\r\nWhen performing these kinds of microbenchmarks with JAX, you have to be careful to ensure you're measuring what you think you're measuring. There are some tips in the JAX Benchmarking FAQ. Implementing some of these best practices, I find the following for your benchmarks:\n```\nimport jax.numpy as jnp\n\n# Native jit-compiled XLA sum\njit_sum = jx.jit(jnp.sum)\n\n# Avoid including device transfer cost in the benchmarks\na_jax = jnp.array(a)\n\n# Prevent measuring compilation time\n_ = reduce_1d_njit_serial(a)\n_ = reduce_1d_jax_serial(a_jax)\n_ = jit_sum(a_jax)\n\n%timeit np.add.reduce(a)\n# 100000 loops, best of 5: 2.33 µs per loop\n\n%timeit reduce_1d_njit_serial(a)\n# 1000000 loops, best of 5: 1.43 µs per loop\n\n%timeit reduce_1d_jax_serial(a_jax).block_until_ready()\n# 100000 loops, best of 5: 6.24 µs per loop\n\n%timeit jit_sum(a_jax).block_until_ready()\n# 100000 loops, best of 5: 4.37 µs per loop\n```\n\nYou'll see that for these microbenchmarks, JAX is a few milliseconds slower than both numpy and numba. So does this mean JAX is slow? Yes and no; you'll find a more complete answer to that question in JAX FAQ: is JAX faster than numpy?. The short summary is that this computation is so small that the differences are dominated by Python dispatch time rather than time spent operating on the array. The JAX project has not put much effort into optimizing for Python dispatch of microbenchmarks: it's not all that important in practice because the cost is incurred once per program in JAX, as opposed to once per operation in numpy.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Fortran OpenMP with multiple simultaneous reductions results in seg fault\r\n                \r\nI used the intel openMP tutorials a while back. I wrote the pi Program reduction and am working on a fortran code using openMP now. I want to sum 4 quantities at once with the reduction clause. the code looks like so:\n\n```\ncall omp_set_num_threads(num_threads)\nwrite(*,*) \"number of parallel threads\"\nwrite(*,*) num_threads\n\n\nN_init = 1200\nN_t    = 1250\n\nfilename = 'POD_input/POD_avg.dat'\n\nio = 0\nnCell = 0\nopen(UNIT = 10, FILE = filename, STATUS = 'OLD', form = 'formatted')\n    do\n        read(10,*, end=67) nonsense, nonsense, nonsense, nonsense, nonsense, nonsense, nonsense, nonsense, nonsense, nonsense, nonsense\n        nCell = nCell + 1\n    end do\n67 close(10)\n\nallocate(eig(nCell))\nallocate(wr(nCell))\nallocate(wi(nCell))\nallocate(work(4*nCell))\n\nallocate(R_Corr(nCell, nCell))\nallocate(U_Corr(nCell, nCell))\nallocate(V_Corr(nCell, nCell))\nallocate(P_Corr(nCell, nCell))\n\nallocate(R_Tot(nCell, nCell))\nallocate(U_Tot(nCell, nCell))\nallocate(V_Tot(nCell, nCell))\nallocate(P_Tot(nCell, nCell))\n\nallocate(R_Fin(nCell, nCell))\nallocate(U_Fin(nCell, nCell))\nallocate(V_Fin(nCell, nCell))\nallocate(P_Fin(nCell, nCell))\n\nallocate(x(nCell))\nallocate(y(nCell))\nallocate(A(nCell))\n\nallocate(Rho(nCell))\nallocate(U(nCell))\nallocate(V(nCell))\nallocate(P(nCell))\n\nallocate(R_x(nCell))\nallocate(U_x(nCell))\nallocate(V_x(nCell))\nallocate(P_x(nCell))\n\nallocate(R_c(nCell))\nallocate(U_c(nCell))\nallocate(V_c(nCell))\nallocate(P_c(nCell))\n\nallocate(R_av(nCell))\nallocate(U_av(nCell))\nallocate(V_av(nCell))\nallocate(P_av(nCell))\n\n\nopen(UNIT = 10, FILE = filename, STATUS = 'OLD', form = 'formatted')\n    do iCell = 1, nCell\n        read(10,*)  x(iCell), y(iCell), A(iCell), nonsense, nonsense, nonsense, nonsense, R_av(iCell), U_av(iCell), V_av(iCell), P_av(iCell)\n    end do\nclose(10)\n\nfilename = 'POD_output/POD_Mesh.dat'\n\nopen(UNIT = 10, FILE = filename, STATUS = 'unknown', form = 'unformatted', access='stream')\n    write(10) nCell\n    write(10) x(:)\n    write(10) y(:)\nclose(10)\n\nR_Tot = 0.0_dp\nU_Tot = 0.0_dp\nV_Tot = 0.0_dp\nP_Tot = 0.0_dp\n\nwrite(*,*) \"begin correlation\"\n\n!$OMP PARALLEL DO REDUCTION(+:R_Tot, U_Tot, V_Tot, P_Tot) private(i, j, nonsense, filename, num, iCell, iTime, R_x, R_C, R_Corr, U_x, U_C, U_Corr, V_x, V_C, V_Corr, P_x, P_C, P_Corr)\n   do iTime = N_init,N_t\n       write(*,*) \"inside loop\"\n       filename = 'POD_input/POD_input.'\n       write(num,'(I6.6)') iTime\n       filename = trim(adjustl(filename))//trim(adjustl(num))//trim(adjustl('.dat'))\n       ! Read file\n\n       write(*,*) \"read file\"\n       open(UNIT = 10, FILE = filename, STATUS = 'OLD', form = 'formatted')\n           do iCell = 1, nCell\n               read(10,*) nonsense, nonsense, nonsense, R_x(iCell), U_x(iCell), V_x(iCell), P_x(iCell)\n           end do\n       close(10)\n\n       R_x = R_x-R_av\n       U_x = U_x-U_av\n       V_x = V_x-V_av\n       P_x = P_x-P_av\n\n       R_C(:) = R_x(:)*sqrt(A(:))\n       U_C(:) = U_x(:)*sqrt(A(:))\n       V_C(:) = V_x(:)*sqrt(A(:))\n       P_C(:) = P_x(:)*sqrt(A(:))\n\n\n       do i = 1, nCell\n           do j =1, nCell\n               R_Corr(i,j) = R_C(i)*R_C(j)\n               U_Corr(i,j) = U_C(i)*U_C(j)\n               V_Corr(i,j) = V_C(i)*V_C(j)\n               P_Corr(i,j) = P_C(i)*P_C(j)\n           end do\n       end do\n\n\n       R_Tot =  R_Tot + R_Corr\n       U_Tot =  U_Tot + U_Corr\n       V_Tot =  V_Tot + V_Corr\n       P_Tot =  P_Tot + P_Corr\n   end do\n!$OMP END PARALLEL DO \n```\n\n\nI get a segmentation fault error even when running with 1 thread. what can I do to debug this or fix it. thank you.\n\nEDIT: Included more code for ease of assistance\n    ", "Answer": "\r\nI have experienced a similar problem once. The used arrays might be large so the stack size is not enough large to fit these data. I sugest you to increase the stack size. I'm using linux and I generally do it through the cmd ```\nulimit -s unlimited```\n.\n\nIn addition when you use a reduction clause with openMP, each threads create a private copy of the variables targeted by the reduction clause. These data are placed on a private stack that size is generally limited to a few MB. to overwrite this limitation, assuming that you are a Linux user, you can run, for instance, the cmd ```\nexport OMP_STACKSIZE=50m```\n to set the private stack size to 50MB. The value 50m could be changed as function of the hardware limitation and the size of your arrays.\n\nIt is important to mention that reduction clause might be not effecient at all with large arrays.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "lambda calculus, expanded and compressed form have different beta-reductions? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is off-topic. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\r\n                \r\n                    \r\n                        Closed 11 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\ngiven \n\n```\nS=\\x.\\y.\\z.x z (y z)\n```\n\n\nand\n\n```\nK=\\x.\\y.x\n```\n\n\nI cannot understand how two beta equivalent forms of the same expression (S K K) yield different results in untyped lambda calculus if I start from the (S K K) form or the equivalent expanded form:\n\n```\n(S K K) = ((S K) K) -> ((\\y.(\\z.((K z) (y z)))) K) -> (\\z.((K z) (K z))) ->\n(\\z.((\\y.z) (K z))) -> (\\z.z) -> 4 reductions!\n\n(S K K) = \\x.\\y.\\z.x z (y z) \\x.\\y.x \\x.\\y.x -> 0 reductions!\n```\n\n\nIt seems the compressed and the expanded form have different parenthesizations, indeed the first one is parenthsized as:\n\n```\n(S K K) = ((S K) K)\n```\n\n\nwhile the second as:\n\n```\n\\x.\\y.\\z.x z (y z) \\x.\\y.x \\x.\\y.x =\n(\\x.(\\y.(\\z.(((x z) (y z)) (\\x.(\\y.(x (\\x.(\\y.x)))))))))\n```\n\n\nDoes anyone have any insight into this???\nThank you\n    ", "Answer": "\r\nCheck out the formal definition of lambda calculus on Wikipedia. An abstraction and an application always have a set of enclosing parentheses.\nThis means more correct definitions of S and K are:\n\n```\nS = (\\x.\\y.\\z.x z (y z))\n```\n\n\nand\n\n```\nK = (\\x.\\y.x)\n```\n\n\nSubstituting these in ```\n(S K K)```\n gives the correct result.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda reductions prove S K = K I\r\n                \r\nHello I am having trouble proving these combinators S K = K I\n\nThe steps with the brackets [] are just telling you the step i am doing. For example [λxy.x / x] in λyz.x z(y z)  means I am about to substitute (λxy.x) for every x in the expression λyz.x z(y z)\n\nwhat I have tried so far is reducing S K and I got this:\n\n```\nS K\n(λxyz.x z(y z)) (λxy.x)\n[λxy.x / x] in λyz.x z(y z) \n(λyz. (λxy.x) z(y z))\n[z/x] in λy.x\n(λyz. (λy.z) (y z))\n[y/y] in λy.z\n(λyz. z z)\n```\n\n\nand then reducing K I and I got this:\n\n```\nK I\n(λxy.x) (λx.x)\n[λx.x / x] in λy.x\nλy. λx.x\n```\n\n\nthough the two answers do not seem to be equal to me (λyz. z z) and λy. λx.x\ncan someone please explain to me what I did wrong? Thank you.\n    ", "Answer": "\r\n```\n(λy.z) (y z)```\n reduces to just ```\nz```\n, not ```\nz z```\n, so ```\n(λyz. (λy.z) (y z))```\n is ```\nλyz. z```\n, which is the same as ```\nλy. λx. x```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Optimal workgroup size for sum reduction in OpenCL\r\n                \r\nI am using the following kernel for sum reduciton.\n\n```\n__kernel void reduce(__global float* input, __global float* output, __local float* sdata)\n{\n    // load shared mem\n    unsigned int tid = get_local_id(0);\n    unsigned int bid = get_group_id(0);\n    unsigned int gid = get_global_id(0);\n\n    unsigned int localSize = get_local_size(0);\n    unsigned int stride = gid * 2;\n    sdata[tid] = input[stride] + input[stride + 1];\n\n    barrier(CLK_LOCAL_MEM_FENCE);\n    // do reduction in shared mem\n    for(unsigned int s = localSize >> 2; s > 0; s >>= 1) \n    {\n        if(tid < s) \n        {\n            sdata[tid] += sdata[tid + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n\n    // write result for this block to global mem\n    if(tid == 0) output[bid] = sdata[0];\n}\n```\n\n\nIt works fine, but I don't know how to choose the optimal workgroup size or number of workgroups if I need more than one workgroup (for example if I want to calculate the sum of 1048576 elements). As far as I understand, the more workgroups I use, the more subresults I will get, which also means that I will need more global reductions at the end.\n\nI've seen the answers to the general workgroup size question here. Are there any recommendations that concern reduction operations specifically?\n    ", "Answer": "\r\nThis question is a possible duplicate of one I answered a while back:\nWhat is the algorithm to determine optimal work group size and number of workgroup.\n\nExperimentation will be the best way to know for sure for any given device.\n\nUpdate:\nI think you can safely stick to 1-dimensional work groups, as you have done in your sample code. On the host, you can try out the best values.\n\nFor each device:\n\n1) query for CL_KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE.\n\n2) loop over a few multiples and run the kernel with that group size. save the execution time for each test.\n\n3) when you think you have an optimal value, hard code it into a new kernel for use with that specific device. This will give a further boost to performance. You can also eliminate your sdata parameter in the device-specific kernel.\n\n```\n//define your own context, kernel, queue here\n\nint err;\nsize_t global_size; //set this somewhere to match your test data size\nsize_t preferred_size;\nsize_t max_group_size;\n\nerr = clGetKernelWorkGroupInfo(kernel, device_id, CL_KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE, sizeof(size_t), preferred_size, NULL);\n//check err\nerr = clGetKernelWorkGroupInfo(kernel, device_id, CL_KERNEL_WORK_GROUP_SIZE, sizeof(size_t), max_group_size, NULL);\n//check err\n\nsize_t test_size;\n\n//your vars for hi-res timer go here\n\nfor (unsigned int i=preferred_size ; i<=max_group_size ; i+=preferred_size){\n    //reset timer\n    test_size = (size_t)i;\n    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &test_size, 0, NULL, NULL);\n    if(err){\n        fail(\"Unable to enqueue kernel\");  //implement your own fail function somewhere..\n    }else{\n        clfinish(queue);\n        //stop timer, save value\n        //output timer value and test_size\n    }\n}\n```\n\n\nThe device-specific kernel can look like this, except the first line should have your optimal value substituted:\n\n```\n#define LOCAL_SIZE 32\n__kernel void reduce(__global float* input, __global float* output)\n{\n    unsigned int tid = get_local_id(0);\n    unsigned int stride = get_global_id(0) * 2;\n    __local float sdata[LOCAL_SIZE];\n    sdata[tid] = input[stride] + input[stride + 1];\n\n    barrier(CLK_LOCAL_MEM_FENCE);\n\n    for(unsigned int s = LOCAL_SIZE >> 2; s > 0; s >>= 1){\n        if(tid < s){\n            sdata[tid] += sdata[tid + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n    if(tid == 0) output[get_group_id(0)] = sdata[0];\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Why should I use a reduction rather than an atomic variable?\r\n                \r\nAssume we want to count something in an OpenMP loop. Compare the reduction\n\n```\nint counter = 0;\n#pragma omp for reduction( + : counter )\nfor (...) {\n    ...\n    counter++;\n}\n```\n\n\nwith the atomic increment\n\n```\nint counter = 0;\n#pragma omp for\nfor (...) {\n    ...\n    #pragma omp atomic\n    counter++\n}\n```\n\n\nThe atomic access provides the result immediately, while a reduction only assumes its correct value at the end of the loop. For instance, reductions do not allow this:\n\n```\nint t = counter;\nif (t % 1000 == 0) {\n    printf (\"%dk iterations\\n\", t/1000);\n}\n```\n\n\nthus providing less functionality.\n\nWhy would I ever use a reduction instead of atomic access to a counter?\n    ", "Answer": "\r\nShort answer:\nPerformance\nLong Answer:\nBecause an atomic variable comes with a price, and this price is synchronization.\nIn order to ensure that there is no race conditions i.e. two threads modifying the same variable at the same moment, threads must synchronize which effectively means that you lose parallelism, i.e. threads are serialized.\nReduction on the other hand is a general operation that can be carried out in parallel using parallel reduction algorithms.\nRead this and this articles for more info about parallel reduction algorithms.\n\nAddendum: Getting a sense of how a parallel reduction work\nImagine a scenario where you have ```\n4```\n threads and you want to reduce a ```\n8```\n element array A. What you could do this in 3 steps (check the attached image to get a better sense of what I am talking about):\n\nStep 0. Threads with index ```\ni<4```\n take care of the result of summing ```\nA[i]=A[i]+A[i+4]```\n.\nStep 1. Threads with index ```\ni<2```\n take care of the result of summing ```\nA[i]=A[i]+A[i+4/2]```\n.\nStep 2. Threads with index ```\ni<4/4```\n take care of the result of summing ```\nA[i]=A[i]+A[i+4/4]```\n\n\nAt the end of this process you will have the result of your reduction in the first element of ```\nA```\n i.e. ```\nA[0]```\n\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Show that term `cons` works by showing all beta reductions\r\n                \r\nI'm new to functional programming.\nSo the terms ```\ncons```\n appends an element to the front of the list. Where\n```\ncons ≜ λx:λl:λc:λn: c x (l c n)\n```\n\nHow should I go about proving that ```\ncons```\n works correctly using beta reduction for a sample function call? For example reducing ```\ncons 3 [2,1]```\n to ```\n[3,2,1]```\n?\nIs there a formula like for the arithmetic operations in lambda calculus? I'm a bit confused on how to approach this compared to an arithmetic operation (i.e. addition or multiplication).\nThanks.\n    ", "Answer": "\r\n```\ncons ≜ λx:λl:λc:λn: c x (l c n)```\n means that\n```\ncons x l c n =\n   c x (l c n)\n```\n\n(in functional / applicative / combinatory notation). So\n```\ncons 3 [2,1] c n = \n = c 3 ([2,1] c n)\n```\n\nand what is ```\n[2,1]```\n if not just shortcut notation for ```\ncons 2 [1]```\n so that we continue\n```\n = c 3 (cons 2 [1] c n)\n = c 3 (c    2 ([1] c n))\n = c 3 (c    2 (cons 1 [] c n))\n = c 3 (c    2 (c    1 ([] c n)))\n```\n\nSo there's no reduction from ```\ncons 3 [2,1]```\n to ```\n[3,2,1]```\n; ```\n[3,2,1]```\n is ```\ncons 3 [2,1]```\n. And ```\n[2,1]```\n is ```\ncons 2 [1]```\n, and ```\n[1]```\n is ```\ncons 1 []```\n.\nThe list ```\ncons x xs```\n, when supplied with ```\nc```\n and ```\nn```\n arguments, will turn into ```\nc x (xs c n)```\n, and so will ```\nxs```\n, in its turn; so any list's elements are used in the  chain of applications of ```\nc```\n on top one another.\nAnd what should ```\n[] c n```\n turn into? It has nothing in it to put through the ```\nc```\n applications -- those are to be applied to a list's elements, and ```\n[]```\n has none. So the only reasonable thing to do (and I'm sure you're already given that definition) is to turn ```\n[] c n```\n into just ```\nn```\n:\n```\ncons 3 [2,1] c n = \n = c 3 (c    2 (c    1 ([] c n)))\n = c 3 (c    2 (c    1       n ))\n```\n\nwhatever ```\nc```\n and ```\nn```\n are.\nAnd that's that.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda Calculus Reduction steps\r\n                \r\nI am studying Lambda Calculus and I am stuck at Reduction.... Can anyone explain the types of reduction with this example, especially beta reduction in the simplest way possible. Also wouldn't mind an easy to understand tutorial.\n\n```\n(λxyz .xyz )(λx .xx )(λx .x )x\n```\n\n    ", "Answer": "\r\nLambda calculus\n\nLambda calculus has a way of spiraling into a lot of steps, making solving problems tedious, and it can look real hard, but it isn't actually that bad. In lambda calculus, there are only lambdas, and all you can do with them is substitution. Lambdas are like a function or a method - if you are familiar with programming, they are functions that take a function as input, and return a new function as output.\n\nThere are basically two and a half processes in lambda calculus:\n\n1) Alpha Conversion - if you are applying two lambda expressions with the same variable name inside, you change one of them to a new variable name. For example (λx.xx)(λx.x) becomes something like (λx.xx)(λy.y) or (λx.xx)(λx'.x') after reduction. The result is equivalent to what you start out with, just with different variable names. \n\n2) Beta Reduction - Basically just substitution. This is the process of calling the lambda expression with input, and getting the output. A lambda expression is like a function, you call the function by substituting the input throughout the expression. Take (λx.xy)z, the second half of (λx.xy), everything after the period, is output, you keep the output, but substitute the variable (named before the period) with the provided input. ```\nz```\n is the input, ```\nx```\n is the parameter name, ```\nxy```\n is the output.  Find all occurrences of the parameter in the output, and replace them with the input and that is what it reduces to, so ```\n(λx.xy)z```\n => ```\nxy```\n with ```\nz```\n substituted for ```\nx```\n, which is ```\nzy```\n.\n\n2.5) Eta Conversion/Eta Reduction - This is special case reduction, which I only call half a process, because it's kinda Beta Reduction, kinda, as in technichally it's not. You may see it written on wikipedia or in a textbook as \"Eta-conversion converts between λx.(f x) and f whenever x does not appear free in f\", which sounds really confusing. All that really means is λx.(f x) = f if f does not make use of x. if It actually makes complete sense but is better shown through an example. Consider (λx.(λy.yy)x), this is equivalent through eta reduction to (λy.yy), because f = (λy.yy), which does not have an x in it, you could show this by reducing it, as it would solve to (λx.xx), which is observably the same thing. You said to focus on beta reduction, and so I am not going to discuss eta conversion in the detail it deserves, but plenty of people gave their go at it on the cs theory stack exchange \n\nOn the Notation for Beta Reduction:\n\nI'm going to use the following notation for substituting the provided input into the output:\n\n```\n(λ param . output)input```\n => ```\noutput [param := input]```\n => ```\nresult```\n\n\nThis means we substitute occurrences of param in output, and that is what it reduces down to\n\nExample: \n\n```\n(λx.xy)z```\n\n\n= ```\n(xy)[x:=z]```\n\n\n= ```\n(zy)```\n\n\n= ```\nzy```\n\n\nEnough theory, let's solve this. Lambda Calculus is good fun.\n\nThe problem you came up with can be solved with only Alpha Conversion, and Beta Reduction, Don't be daunted by how long the process below is. It's pretty long, no doubt, but no step in solving it is real hard.\n\n(λxyz.xyz)(λx.xx)(λx.x)x\n\n= (((λxyz.xyz)(λx.xx))(λx.x))x  - Let's add the parenthesis in \"Normal Order\", left associativity, abc reduces as ((ab)c), where b is applied to a, and c is applied to the result of that\n\n= (((λxyz.xyz)(λx.xx))(λx.x))x - Select the deepest nested application and reduce that first.\n\nThe bolded section reduces as:\n\n(λxyz.xyz)(λx.xx)\n\n= (λx.λyz.xyz)(λx.xx) - means the same thing, but we pull out the first parameter since we are going to reduce it away and so I want it to be clear\n\n= (λx.λyz.xyz)(λx'.x'x') - Alpha conversion, some people stick to new letters, but I like appending numbers at the end or `s, either way is fine. Because both expressions use the parameter x we have to rename them on one side, because the two Xs are local variables, and so do not have to represent the same thing.\n\n= (λyz.xyz)[x := λx'.x'x'] - Notation for a beta reduction, we remove the first parameter, and replace it's occurrences in the output with what is being applied [a := b] denotes that a is to be replaced with b.\n\n= (λyz.(λx'.x'x')yz) - The actual reduction, we replace the occurrence of x with the provided lambda expression.\n\n= (λyz. ((λx'.x'x')y) z) - Normal order for parenthesis again, and look, another application to reduce, this time y is applied to (λx'.x'x'), so lets reduce that now\n\n= (λyz. ((x'x')[x' := y]) z) - Put this into notation for beta reduction.\n\n= (λyz. (yy) z) - we swap the two occurrences of x'x' for Ys, and this is now fully reduced.\n\nAdd this back into the original expression:\n\n(((λxyz.xyz)(λx.xx))(λx.x))x\n\n= ((λyz.(yy)z)(λx.x))x - This is not new, just putting what we found earlier back in.\n\n= ((λyz.(yy)z)(λx.x))x - Grab the deepest nested application, it is of (λx.x) applied to (λyz.(yy)z)\n\nWe'll solve this out separately again:\n\n(λyz.(yy)z)(λx.x)\n\n= (λy.λz.(yy)z)(λx.x) - Just bringing the first parameter out for clarity again.\n\n= (λz.(yy)z)[y := (λx.x)] - Put into beta reduction notation, we pop out the first parameter, and note that Ys will be switched for (λx.x)\n\n= (λz.((λx.x)(λx.x))z) - The actual reduction/substitution, the bolded section can now be reduced\n\n= (λz.((x)[x := λx.x])z) - Hopefully you get the picture by now, we are beginning to beta reduce (λx.x)(λx.x) by putting it into the form (x)[x := λx.x]\n\n= (λz.((λx.x))z) - And there is the substitution\n\n= (λz.(λx.x)z) - Cleaned off the excessive parenthesis, and what do we find, but another application to deal with\n\n= (λz.(x)[x:=z]) - Pop the x parameter, put into notation\n\n= (λz.(z)) - Perform the substitution\n\n= (λz.z) - Clean off the excessive parenthesis\n\nPut it back into the main expression:\n\n((λyz.(yy)z)(λx.x))x\n\n= ((λz.z))x - Filling in what we proved above\n\n= (λz.z)x - cleaning off excessive parenthesis, this is now reduced down to one final application, x applied to(λz.z)\n\n= (z)[z:=x] - beta reduction, put into notation\n\n= (x) - make the substitution\n\n= x - clean off the excessive parenthesis\n\nSo, yeah. The answer is ```\nx```\n, it reduced down just groovy.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Ill-typed with abstraction while pattern matching on the decideable comparator, union type and dependent pair agda\r\n                \r\nI have an object, which is represented as a function ```\nL → A ⊎ B```\n. I usually denote it as ```\nN```\n.\nIt is some mapping from label ```\nl```\n to either ```\nA```\n or ```\nB```\n.\nI have two types of reductions (in my example ```\nreductionType1```\n and ```\nreductionType2```\n) and a mapping between them, so that we could produce the second reduction from the first one.\nThese reductions work only for the ```\nA```\n type.\nIn my project I want to apply these reductions on particular object, replacing the result for special label with result of reduction.\nReductionType1 only applied in singular manner, while ReductionType2 is applied in \"parallel manner\".\nI have a function ```\nred2FromRed1SingleReductionMap```\n, which constructs parallel reductions of type 2 from a single reduction type 1.\nAnd I have functions ```\nreductionMapsTypeExample```\n ```\nsingleReduct1N```\n which apply these reductions to particular ```\nN```\n.\nThe problem that I face with is somehow related to \"ill-typed with abstractions\", and I do not know how to overcome it :(\nHere is the example, that I tried to construct, which represents my logic (I really tried to make it simple...)\n```\nopen import Relation.Binary.PropositionalEquality using (_≡_)\nopen import Data.Sum.Base using (_⊎_; inj₁; inj₂; [_,_]′)\n\nopen import Agda.Builtin.Sigma using (Σ; _,_; fst; snd)\nopen import Agda.Builtin.Unit using (⊤; tt)\n\nopen import Relation.Nullary\nopen import Relation.Nullary.Decidable\nopen import Relation.Binary.Core\n\ndata L : Set where\n  x : L\n  y : L\ndata A : Set where\n  a1 : A\n  a2 : A\ndata B : Set where\n  b1 : B\n  b2 : B\n\ndata reductionType1 : A → A → Set where\n  reductoinRule11 : reductionType1 a1 a1\n  reductoinRule12 : reductionType1 a2 a2\n\ndata reductionType2 : A → A → Set where\n  reductoinRule21 : reductionType2 a1 a1\n  reductoinRule22 : reductionType2 a2 a2\n\nmapReduction1ToReduction2 : ∀ {a a'} → reductionType1 a a' → reductionType2 a a'\nmapReduction1ToReduction2 reductoinRule11 = reductoinRule21\nmapReduction1ToReduction2 reductoinRule12 = reductoinRule22\n\nreduction2-reflexive : ∀ {a} → reductionType2 a a\nreduction2-reflexive {a1} = reductoinRule21\nreduction2-reflexive {a2} = reductoinRule22\n\nreductionMapsTypeExample : (N : L → A ⊎ B) → (c : L) → Set\nreductionMapsTypeExample N l with N l\n... | (inj₁ a) = Σ A (reductionType2 a)\n... | (inj₂ b) = ⊤\n\nparallelReductNExample : \n  (N : L → A ⊎ B)\n  → (reductions : (l : L) → reductionMapsTypeExample N l)\n  → (L → A ⊎ B)\nparallelReductNExample N reductions l with N l | reductions l\n... | (inj₁ a) | (a' , _) = inj₁ a'\n... | (inj₂ b) | _ = inj₂ b\n\nred2FromRed1SingleReductionMap : (comp : (l : L) → (l' : L) → Dec (l ≡ l'))\n  → (N : L → A ⊎ B) \n  → (specialL : L) \n  → (a a' : A) \n  → (e : (N specialL) ≡ (inj₁ a)) \n  → (red1 : reductionType1 a a') \n  → ((l : L) → reductionMapsTypeExample N l)\nred2FromRed1SingleReductionMap comp N specialL a a' e red1 l with comp l specialL\n... | yes e_l_specialL with l | e_l_specialL\n...   | specialL | _≡_.refl with N specialL | e\n...     | inj₁ a | _≡_.refl = a' , (mapReduction1ToReduction2 red1)\nred2FromRed1SingleReductionMap comp N specialL a a' e red1 l\n    | no _ with N l\n...   | inj₁ aa = aa , reduction2-reflexive\n...   | inj₂ b = tt\n\nsingleReduct1N : ∀ {a a' : A} {comp : (l : L) → (l' : L) → Dec (l ≡ l')}\n  → (specialL : L)\n  → (N : L → A ⊎ B) \n  → (N specialL ≡ inj₁ a) → reductionType1 a a'\n  → (L → A ⊎ B)\nsingleReduct1N {a} {a'} {comp} specialL N _ _ l with comp l specialL\n... | yes _ = inj₁ a'\n... | no _ = N l\n\nequalityOfTheseReductions : ∀ {specialL}\n  {N : L → A ⊎ B}\n  {a a' : A}\n  {e : N specialL ≡ (inj₁ a)}\n  {red1 : reductionType1 a a'}\n  {comp : (l : L) → (l' : L) → Dec (l ≡ l')}\n  {l : L}\n  → (((parallelReductNExample N (red2FromRed1SingleReductionMap comp N specialL a a' e red1)) l)\n    ≡ ((singleReduct1N {a} {a'} {comp} specialL N e red1) l))\nequalityOfTheseReductions {specialL} {N} {a} {a'} {e} {red1} {comp} {l} with comp l specialL\n... | yes e_l_specialL with l | e_l_specialL\n...   | specialL | _≡_.refl with N specialL | red2FromRed1SingleReductionMap comp N specialL a a' e red1 specialL\n...     | inj₁ aa | aaa , _ = ?\n```\n\nAnd the produced error:\n```\nN specialL != w₁ of type A ⊎ B\nwhen checking that the type\n(specialL : L) {comp : (l l' : L) → Dec (l ≡ l')} {N : L → A ⊎ B}\n{a a' : A} {e : N specialL ≡ inj₁ a} {red1 : reductionType1 a a'}\n(w₁ : A ⊎ B) →\nreductionMapsTypeExample N specialL | w₁ →\n{l : L} (e_l_specialL : l ≡ specialL) →\n(parallelReductNExample N\n (λ l₁ →\n    red2FromRed1SingleReductionMap comp N specialL a a' e red1 l₁\n    | comp l₁ specialL)\n specialL\n | w₁\n | (UntypedPhiCalculus.with-1294 specialL specialL _≡_.refl specialL\n    _≡_.refl comp N a a' e red1\n    | w₁ | e))\n≡ inj₁ a'\nof the generated with function is well-formed\n(https://agda.readthedocs.io/en/v2.6.3/language/with-abstraction.html#ill-typed-with-abstractions)\n```\n\n    ", "Answer": "\r\nIndeed, this kind of error is/can be infuriating, because it is just over the horizon of intelligibility for most people (complex machine-generated error message, associated with complex machine-generated expression that the user never themselves wrote), even when your instincts are on the right lines:\nyour goal(s) explicitly quantify over an equation which specifies the value and type of a ```\nwith```\n-abstracted term, which you nevertheless want to correlate 'in-place' with the associated use of ```\nwith```\n...\n... so the 'standard'/'official' remedy is to use the new ```\nwith exp in eqn```\n syntax, which in a given partial construction installs:\n\nnot only the expression ```\nexp```\n which you wish to ```\nwith```\n-abstract;\nbut also an additional equational hypothesis ```\neqn```\n which asserts the equality of ```\nexp```\n and the (various) values of the subsequent pattern scrutinee introduced by the ```\nwith```\n\n\nIn each of the subsequent case branches corresponding to pattern-matching on the scrutinee, the equation may then (sometimes/usually) be used to rewrite the current goal (in a well-typed way) to 'the thing you want'. See if this helps!\nSadly, there are examples in the wild where even this desirable outcome fails to hold, and where other methods seem necessary. If your example turns out to be one of those, the Agda stdlib maintainers would like to know, as we have recently deprecated one of the other ways of dealing with ```\nwith```\n-abstractions in favour of ```\nwith ... in ...```\n syntax, so failing regression tests are (sadly) useful to know about.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "yacc loses values among reduction\r\n                \r\nI'm working on this grammar to build a SDD for type checking, or similar. I spent yesterday working out for data structures and parsing action, but I always reached a segmentation fault. It seems to me that YACC(bison) is loosing values among reduction.\n\nThus I decided to build a simpler grammar with simpler actions. It seems that the values are lost among one reduction and another, or maybe am I doing something wrong? The lexer part is not relevant in this example so I omitted it.\n\nFollowing the grammar with its action and the result vs expected result..\n\n```\nD:   T VAR SEMICOLON D              {\n                                    printf(\"processing D -> T var ; D\\n\");\n                                    printf(\"\\tvalue of T is %f\\n\", $1);\n                                }\n|/*empty*/                      {\n                                    printf(\"processing D -> empty\\n\");\n                                }\n;\n\nT:  B                               {\n                                    printf(\"processing B inside T\\n\");\n                                    printf(\"\\tvalue of B is %f\\n\", $1);\n                                } \n\nC                               {   printf(\"processing C inside T\\n\");\n                                    printf(\"processing T-> B C\\n\");\n                                    printf(\"\\tvalue of B is %f\\n\", $1);\n                                    printf(\"\\tvalue of C is %f\\n\", $<dbl>2);\n                                    $$ = $1 + $<dbl>2;\n\n                                }\n| RECORD '{' D '}'              {   printf(\"processing record { D }\\n\");}\n;\n\nB:   INT                            {   printf(\"processing B -> int\\n\");\n                                    $$ = 1;\n                                }\n| FLOAT                         {   printf(\"processing B -> float\\n\");\n                                    $$ = 1;\n                                }\n;\n\nC:  /*empty*/                       {   printf(\"processing C -> empty\\n\");\n                                    printf(\"\\tsetting C to be equal to 1\\n\");\n                                    $$=1;\n                                }\n| LBRACK NUM RBRACK C           {   int n = $2;\n                                    printf(\"processing C -> [%d] C\\n\", n);\n                                    double d = $4;\n                                    printf(\"\\tprevious C value is %f\\n\", d);\n                                    double f = d+ 1;\n                                    printf(\"\\tnew value of $$ is %f\\n\", f);\n                                    $$ = f;\n                                }\n;\n```\n\n\nthis is the output for an input like ```\nint [12][3] ciao;```\n\n\n```\nprocessing B -> int\nprocessing B inside T\n    value of B is 1.000000\nprocessing C -> empty\n    setting C to be equal to 1\nprocessing C -> [3] C\n    previous C value is 1.000000\n    new value of $$ is 2.000000           \nprocessing C -> [12] C                    \n    previous C value is 2.000000          \n    new value of $$ is 3.000000           \nprocessing C inside T\nprocessing T-> B C\n    value of B is 1.000000\n    value of C is 0.000000                (*)\nprocessing D -> empty\nprocessing D -> T var ; D\n    value of T is 1.000000                (*)\n```\n\n\nAs you can see the value are lost among C reductions marked with ```\n*```\n I expect it to grew up, like the following\n\n```\nprocessing B -> int\nprocessing B inside T\n    value of B is 1.000000\nprocessing C -> empty\n    setting C to be equal to 1\nprocessing C -> [3] C\n    previous C value is 1.000000\n    new value of $$ is 2.000000           \nprocessing C -> [12] C                    \n    previous C value is 2.000000          \n    new value of $$ is 3.000000           \nprocessing C inside T\nprocessing T-> B C\n    value of B is 1.000000\n    value of C is 3.000000                \nprocessing D -> empty\nprocessing D -> T var ; D\n    value of T is 4.000000                \n```\n\n\nANY hint is appreciated as well as explanation and suggestion to reach the scope, is there anything that I am missing?\n    ", "Answer": "\r\nThe production for ```\nT```\n is as follows, simplified considerably.:\n\n```\nT: B { /* Mid Rule Action (MRA) */ } C { $$ = $1 + $2; }\n```\n\n\nIn the final action for ```\nT```\n, the ```\n$2```\n refers to the MRA, because MRAs are counted in the production's terms. (In fact, an MRA is replaced with a non-terminal with an empty RHS.) So ```\nC```\n is ```\n$3```\n.\n\nSince the MRA does not actually set a value, ```\n$2```\n is somewhat unspecified, but ```\n0```\n is not too unlikely.\n\nBison manual references:\n\nUsing Mid-Rule Actions:\n\n\n  The mid-rule action itself counts as one of the components of the rule. This makes a difference when there is another action later in the same rule (and usually there is another at the end): you have to count the actions along with the symbols when working out which number ```\nn```\n to use in ```\n$n```\n.\n\n\nMid-Rule Action Translation: points out that \"mid-rule actions are actually transformed into regular rules and actions\", and then provides a number of examples of the empty rules produced (and their internal names, useful for understanding bison debugging output.)\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "FutureWarning: Dropping of nuisance columns in DataFrame reductions with df.median\r\n                \r\nI have tried lots of things and can't figure out how to do the medians of the columns I need. I dont dont why they are \"nuisance columns\"\nThe warning is:\n```\nFutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.\n```\n\nhere is my code:\n```\ndef readcsv(folder, ICs):\n    result = []\n    for dirname, dirs, files in os.walk(data_dir + folder):\n        \"\"\"dirname = folder under inspection & dirs = folders\"\"\"\n        for filename in files:\n            path = os.path.join(dirname, filename)\n            if 'local-results-' + ICs in filename:\n                df = pd.read_csv(path, nrows=13, skiprows=np.arange(0, 5, 1))\n                mes = pd.read_csv(path, nrows=19, skiprows=np.arange(0, 20, 1))\n                medsT = mes.set_index('Aq').transpose()  # the data frame transposed\n                result.append([df, medsT, filename])\n    return result\n\n\ndef find_results():  # directory where data is (datadir or procdatadir atm),\n    \"\"\"Return results as directories data path as list\"\"\"\n    V50 = readcsv(\"Adorian_Springhill_Lala-Full_Test_V50Beta0.5/\", \"V50Beta0.5\")\n    V10 = readcsv(\"Adorian_Springhill_Lala-Full_Test_V10Beta0.5/\", \"V10Beta0.5\")\n    return V50, V10  # V50 = [[df, medians, filename], ...]\n\n\ndef exclude_outliers(datapoint):\n    timestamp = datapoint[2][10:16]\n    print('plotting datapoint: ', timestamp)\n    med, data, j = datapoint[1], datapoint[0], 0\n    drop_indexs = []\n    while j <= len(data) - 1:\n        point = data.iloc[j, :]\n        threshb = np.abs((med['Beta'] - point['Beta']) / med['Beta'])[0]\n        threshv = np.abs((med['V'] - point['V']) / med['V'])[0]\n        if threshv > 0.3:\n            drop_indexs.append(j)\n        elif threshb > 0.5:\n            drop_indexs.append(j)\n        j += 1\n    data.drop(drop_indexs, inplace=True)\n    # data = data.astype(float, errors='ignore')\n    true_med = data.median(numeric_only=True)  # skipna=True by default\n    print(true_med)\n    return true_med, int(timestamp)\n\n\ndef plotmedians(all_data):\n    \"\"\"Plots all dataframes in the directory in results = find_results(directory)\"\"\"\n    fig = plt.figure()\n    plt.suptitle(\"Velocity, Motility and $\\chi^2$ against Time\", size='xx-large')\n    gs = gridspec.GridSpec(2, 2)\n    axv = fig.add_subplot(gs[0, 0])  # row, column\n    axmot = fig.add_subplot(gs[0, 1])\n    axchi = fig.add_subplot(gs[1, :])\n\n    j = 0\n    for key in legend_dict:\n        dataset = all_data[j]\n        i = 0\n        for datapoint in dataset:\n            med, time = exclude_outliers(datapoint)\n            time = i * 5\n            col, s = legend_dict[key][0], legend_dict[key][1]\n            try:\n                axv.errorbar(time, med['V'], yerr=med['ErrV'], ecolor=col, capsize=3, capthick=2,\n                             elinewidth=1, zorder=2, c=col, fmt='^', ms=s)\n                axmot.errorbar(time, med['Alpha']*100, yerr=med['ErrBeta']*100, ecolor=col, capsize=3,\n                               capthick=2, elinewidth=1, zorder=2, c=col, fmt='^', ms=s)\n                axchi.scatter(time, med['ChiSq'], c=col, marker='^', s=s*10)\n            except:\n                axv.errorbar(time, med['V'], ecolor=col, capsize=3, capthick=2,\n                             elinewidth=1, zorder=2, c=col, fmt='^', ms=s)\n                axmot.errorbar(time, med['Alpha']*100, ecolor=col, capsize=3,\n                               capthick=2, elinewidth=1, zorder=2, c=col, fmt='^', ms=s)\n            i += 1\n        j += 1\n\n\nplotmedians(find_results())\n```\n\nThe median columns that are missing are these (after doing the median):\n```\nErrAq            1.9194410627\nErrBq            0.0176539434\nErrV             0.0649970090\nErrS             6.1035284556\nErrA0            0.0509019872\nErrW0            0.5507963763\nErrBeta          0.0469381723\nErrD             0.1453676077\nErrF0            0.0876619659\nlmfitObj                  NaN\n```\n\nI only really need 5 of the whole data frame but I have tried filtering for the ones I need and it still doesn't work.\n    ", "Answer": "\r\nThe warning is due to deprecated dropping nuisance columns in DataFrame. You can read more about it here. If you want to get rid of it, you should change this code ```\ntrue_med = data.median(numeric_only=True)```\n.\nHave you tried selecting columns first?\n```\ndata[[\"ErrAq\",\"ErrBq\",\"ErrV\",\"ErrS\",\"ErrA0\"]].median() \n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "pandas : reduction of timedelta64 using sum() results in int64?\r\n                \r\nAccording to the pandas 0.13.1 manual, you can reduce a numpy timedelta64 series:\n\nhttp://pandas.pydata.org/pandas-docs/stable/timeseries.html#time-deltas-reductions\n\nThis seems to work fine with, for example, ```\nmean()```\n:\n\n```\nIn[107]:\npd.Series(np.random.randint(0,100000,100).astype(\"timedelta64[ns]\")).mean()\nOut[107]:\n0   00:00:00.000047\ndtype: timedelta64[ns]\n```\n\n\nHowever, using ```\nsum()```\n, this always results in an integer:\n\n```\nIn [108]:\npd.Series(np.random.randint(0,100000,100).astype(\"timedelta64[ns]\")).sum()\nOut[108]:\n5047226\n```\n\n\nIs this a bug, or is there e.g. overflow that is causing this? Is it safe to cast the result into ```\ntimedelta64```\n? How would I work around this?\n\nI am using numpy 1.8.0.\n    ", "Answer": "\r\nLooks like a bug, just filed this: https://github.com/pydata/pandas/issues/6462\n\nThe results are in nanoseconds; as a work-around you can do this:\n\n```\nIn [1]: s = pd.to_timedelta(range(4),unit='d')\n\nIn [2]: s\nOut[2]: \n0   0 days\n1   1 days\n2   2 days\n3   3 days\ndtype: timedelta64[ns]\n\nIn [3]: s.mean()\nOut[3]: \n0   1 days, 12:00:00\ndtype: timedelta64[ns]\n\nIn [4]: s.sum()\nOut[4]: 518400000000000\n\nIn [8]: pd.to_timedelta([s.sum()])\nOut[8]: \n0   6 days\ndtype: timedelta64[ns]\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda calculus operator precedence and reduction strategies\r\n                \r\n1.\nIn lambda calculus applications have higher precedences than abstractions. Now in this example the author shows two reductions in normal and applicative order. The first is: \n\n```\n(λx.x^2 (λx.(x+1) 2))) → (λx.x^2 (2+1)) → (λx.x^2 (3)) → 3^2 → 9\n```\n\n\nMy problem lies within the 1st and 3rd step. Why can he reduce like this\n\n```\n(λx.x^2 (3)) → 3^2\n```\n\n\nif application has a higher precedence than abstraction? Shouldn't this be true:\n\n```\nλx.x^2 (3) = λx.(x^2 (3))\n```\n\n\nand therefore no reduction should be possible? The way he interprets the term is\n\n```\nλx.x^2 (3) = (λx.x^2) (3)\n```\n\n\nwhich is incorrect.\n\n2.\n\nAfaik these are the definitions of 4 reduction strategies.\n\n```\nNormal Order:       Leftmost outermost redex reduced first\nApplicative Order:  Leftmost innermost redex reduced first\nCall by value:      Only outermost redex reduced                Reduction only if right-hand side has been reduced to a value (= variable or abstraction)\nCall by name:       Leftmost outermost redex reduced first      No reductions inside abstractions\n```\n\n\nAccording to this innermost and outermost only refers to abstractions. Does leftmost (and rightmost) similarly only refer to applications?  \n\n3.\n\nSo is this a correct recursive algorithm for applicative order reduction (in pseudo code)?\n\n```\nevaluate(t : Term) {\n    if (t is Abstraction) {\n        evaluate(t.inside)\n    } else if (t is Application) {\n        evaluate(t.first)\n        if (t.first is Abstraction) {\n            t.first.apply(t.second)\n        }\n    } else if (t is Variable) {\n        do nothing\n    }\n}\n```\n\n\nAbstraction, Application and Variable are all child-classes of Term. The function \"apply\" applies the given term to the abstraction. The data structures look something like this (please ignore the missing pointer syntax):\n\n```\nclass Abstraction {\n    var : Variable\n    inside : Term\n}\nclass Variable {\n    name : String\n}\nclass Application {\n    first : Term\n    second : Term\n}\n```\n\n\n4.\nIn the link in 1. the author gives an example on a term that can be reduced to normal form with normal order but not with applicative order, because in the latter case the reduction will not terminate. Is there a term where the reduction terminates but yields different results with both strategies? If so, what would be an example?\n\nSorry for the long question, I didn't want to create 4 different threads for this.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Excel: SUMIF combined with LOOKUP for date between date range\r\n                \r\nI have two sheets - 1) Reduction 2) Calendar.\n\nA sampling of the data in Sheet #1 - Reduction:\n\n```\n     A          B           C           D \n1    Start      End         Resource    Reduction\n2    1/4/2016   1/6/2016    Andrew      -8\n3    3/11/2016  4/1/2016    Andrew      -2\n4    1/5/2016   1/6/2016    Emily       -0.5\n5    1/4/2016   1/7/2016    Andrew      -0.5\n```\n\n\nFormat of Sheet #2 - Calendar:\n\n```\n     A          B           C           D           E           F\n1    Resource   1/4/2016    1/5/2016    1/6/2016    1/7/2016    1/8/2016\n2    Andrew                 \n3    Maria                  \n4    Emily                  \n```\n\n\nWhat I'm trying to accomplish is identify any reductions for the resource identified in Calendar A1:A4 and sum those reductions to identify a daily total reduction. I need to lookup the reductions and sum them from the Reduction sheet. \n\nSo for Cell Calendar C2 (Andrew for 1/5/16) I am looking at the Reductions sheet to find any of Andrew's reductions for the date of 1/5. Which should return then a reduction of -8.5 (rows 2 & 5 from the Reductions sheet = -8 + -.5).\n\nI'm wondering if I can combine SUMIFS and LOOKUP to accomplish this or if there is a better way of going about it. \n\nThank you in advance!\n    ", "Answer": "\r\nUse SUMIFS():\n\n```\n=SUMIFS(Sheet1!$D:$D,Sheet1!$A:$A,\"<=\"&B$1,Sheet1!$B:$B,\">=\"&B$1,Sheet1!$C:$C,$A2)\n```\n\n\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Class Scheduling to Boolean satisfiability [Polynomial-time reduction]\r\n                \r\nI have some theoretical/practical problem and I don't have clue for now on how to manage, Here it is: \n\nI create a SAT solver able to find a model when one is existing and to prove the contradiction when it's not the case on CNF problems in C using genetics algorithms.\n\nA SAT-problem looks basically like this kind of problem : \n\nMy goal is to use this solver to find solutions in a lot of different  NP-completes problems. Basically, I translate different problems into SAT, solve SAT with my solver and then transform the solution into a solution acceptable for the original problem.\n\nI already succeed for the N*N Sudoku and also the N-queens problems, but here is my new goal : to reduce the Class Scheduling Problem to SAT but I have no idea how to formalize a class-scheduling problem in order to easily transform it in SAT after. \n\nThe goal is obviously, in few months, to generate a picture of a schedule like this one : \n\n\n\nI found this source-code who is able to solved the class-scheduling but without any reductions to SAT unfortunately :/ \n\nI also found some articles about Planning in general (http://www.cs.rochester.edu/users/faculty/kautz/papers/kautz-satplan06.pdf for instance). \n\nBut the planning domain definition language used in this article seems quite too general for me to represents a Class-scheduling problem. :/ \n\nDoes someone has an idea on how to formalize efficiently a Class-scheduling in order to reduce it to SAT and after, transform the SAT solution (if it exists ^^) into a Class-schedule ?\n\nI'm basically open to any suggestions, I for now have no idea on how to represents, how to reduce the problem, how to transform the SAT-solution into a schedule... \n\n\n\nFollow up question: Class Scheduling to Boolean satisfiability [Polynomial-time reduction] part 2\n    ", "Answer": "\r\nI am going to try and first formalize the problem, and then attempt to reduce it to SAT.\n\nDefine the class scheduling problem as:\n\n```\nInput = { S1,S2,....,Sn | Si = {(x_i1, y_i1), (x_i2, y_i2) , ... , (x_ik, y_ik) | 0 <= x_ij < y_ij <= M } } \n```\n\n\nInformally: The input is a set of classes, each class is a set of (open) intervals in the form (x,y)\n(M is some constant that describes the \"end of the week\")\n\nOutput: True if and only if there exists some set:\n\n```\nR = { (x_1j1, y_1j1) , ..., (x_njn, y_njn) | for each a,b: (x_aja,y_aja) INTERSECTION (x_bjb,y_bjb) = {} }\n```\n\n\nInformally: true if and only if there is some assignment of intervals such that the intersection between each pair of intervals is empty.\n\n\n\nReduction to SAT:\n\nDefine a boolean variable for each interval, ```\nV_ij```\n\nBased on it, define the formula:\n\n```\nF1 = (V_11 OR V_12 OR ... OR V_1(k_1)) AND .... AND (V_n1 OR V_n2 OR ... OR V_n(k_n))\n```\n\n\nInformally, F1 is satisfied if and only if at least one of the interval for each class is \"satisfied\"\n\nDefine ```\nSmaller(x,y) = true```\n if and only ```\nif x <= y```\n1\nWe will use it to make sure intervals don't overlap.\nNote that if we want to make sure (x1,y1) and (x2,y2) don't overlap, we need:\n\n```\nx1 <= y1 <= x2 <= y2 OR  x2 <= y2 <= x1 <= y1\n```\n\n\nSince the input guarantees ```\nx1<=y1, x2<=y2```\n, it reduces to:\n\n```\ny1<= x2 OR y2 <= x1\n```\n\n\nAnd using our Smaller and boolean clauses:\n\n```\nSmaller(y1,x2) OR Smaller(y2,x1)\n```\n\n\nNow, let's define new clauses to handle with it:\n\nFor each pair of classes a,b and intervals c,d in them (c in a, d in b)\n\n```\nG_{c,d} = (Not(V_ac) OR Not(V_bd) OR Smaller(y_ac,x_bd) OR Smaller(y_bd,x_ac))\n```\n\n\nInformally, if one of the intervals b or d is not used - the clause is satisfied and we are done. Otherwise, both are used, and we must ensure there is no overlap between the two intervals.\nThis guarantees that if both c,d are \"chosen\" - they do not overlap, and this is true for each pair of intervals.\n\nNow, form our final formula:\n\n```\nF = F1 AND {G_{c,d} | for each c,d}\n```\n\n\nThis formula ensures us:\n\n\nFor each class, at least one interval is chosen\nFor each two intervals c,d - if both c and d are chosen, they do not overlap.\n\n\nSmall note: This formula allows to chose more than 1 interval from each class, but if there is a solution with some t>1 intervals, you can easily remove t-1 of them without changing correctness of the solution.\n\nAt the end, the chosen intervals are the boolean variables V_ij we defined.\n\n\n\nExample:\n\n```\nAlebgra = {(1,3),(3,5),(4,6)} Calculus = {(1,4),(2,5)}\n```\n\n\nDefine F:\n\n```\nF1 = (V1,1 OR V1,2 OR V1,3) AND (V2,1 OR V2,2)\n```\n\n\nDefine G's:\n\n```\nG{A1,C1} = Not(V1,1) OR Not(V2,1) OR  4 <= 1 OR 3 <= 1 //clause for A(1,3) C(1,4)\n         = Not(V1,1) OR Not(V2,1) OR false = \n         = Not(V1,1) OR Not(V2,1)\nG{A1,C2} = Not(V1,1) OR Not(V2,2) OR  3 <= 2 OR 5 <= 1 // clause for A(1,3) C(2,5)\n         = Not(V1,1) OR Not(V2,2) OR false = \n         = Not(V1,1) OR Not(V2,2)\nG{A2,C1} = Not(V1,2) OR Not(V2,1) OR  5 <= 1 OR 4 <= 3 //clause for A(3,5) C(1,4)\n         = Not(V1,2) OR Not(V2,1) OR false = \n         = Not(V1,2) OR Not(V2,1)\nG{A2,C2} = Not(V1,2) OR Not(V2,2) OR  5 <= 2 OR 5 <= 3 // clause for A(3,5) C(2,5)\n         = Not(V1,2) OR Not(V2,2) OR false = \n         = Not(V1,2) OR Not(V2,2)\nG{A3,C1} = Not(V1,3) OR Not(V2,1) OR  4 <= 4 OR 6 <= 1 //clause for A(4,6) C(1,4)\n         = Not(V1,3) OR Not(V2,1) OR true= \n         = true\nG{A3,C2} = Not(V1,3) OR Not(V2,2) OR  6 <= 2 OR 5 <= 4 // clause for A(4,6) C(2,5)\n         = Not(V1,3) OR Not(V2,2) OR false = \n         = Not(V1,3) OR Not(V2,2)\n```\n\n\nNow we can show our final formula:\n\n```\n    F = (V1,1 OR V1,2 OR V1,3) AND (V2,1 OR V2,2) \n        AND  Not(V1,1) OR Not(V2,1) AND Not(V1,1) OR Not(V2,2)\n        AND  Not(V1,2) OR Not(V2,1) AND Not(V1,2) OR Not(V2,2)\n        AND  true AND Not(V1,3) OR Not(V2,2)\n```\n\n\nThe above is satisfied only when:\n\n```\nV1,1 = false\nV1,2 = false\nV1,3 = true\nV2,1 = true\nV2,2 = false\n```\n\n\nAnd that stands for the schedule: Algebra=(4,6); Calculus=(1,4), as desired.\n\n\n\n(1) can be computed as a constant to the formula pretty easily, there are polynomial number of such constants.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Is there a good implementation of reduction algorithm callable from kernel with dynamic parallelism?\r\n                \r\nI see reductions algorithms in CUDA (such as summation and maximization over a range of elements) discussed in previous posts, but with dynamic parallelism, they could potentially be implemented in a different way. Is there a more efficient implementation which is callable from inside the kernels?\n    ", "Answer": "\r\n\n  Is there a more efficient implementation which is callable from inside the kernels?\n\n\nCUB provides a CUDA reduction primitive compatible with dynamic parallelism, namely, that can be called within kernels.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "\"FutureWarning: Dropping of nuisance columns in DataFrame reductions\" warning when using df.mean()\r\n                \r\nI have a dataframe that looks something like this:\n```\n   col1   col2 col3\n0     1   True  abc\n1     2  False  def\n2     3   True  ghi\n```\n\nWhen I run ```\ndf.mean()```\n, it shows a warning:\n```\n>>> df.mean()\n<stdin>:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\ncol1    2.000000\ncol2    0.666667\ndtype: float64\n```\n\nHow do I solve this warning?\n    ", "Answer": "\r\nNumeric functions such as ```\nmean```\n, ```\nmedian```\n, ```\nsem```\n, ```\nskew```\n, etc., only support dealing with numeric values. If you look at the data types of your columns...\n```\n>>> df.dtypes\ncol1     int64\ncol2      bool\ncol3    object\ndtype: object\n```\n\n...you can see that the dtype of ```\ncol1```\n is ```\nint64```\n, which ```\nmean```\n can handle, because it's numeric. Likewise, the dtype of ```\ncol2```\n is ```\nbool```\n, which Python, pandas, and numpy essentially treat as ints, so ```\nmean```\n treats ```\ncol2```\n as if it only contains ```\n1```\n (for ```\nTrue```\n) and ```\n0```\n for ```\nFalse```\n.\nThe dtype of ```\ncol3```\n, however, is ```\nobject```\n, the default dtype for strings, which is basically a generic type to encapsulate any type of data that pandas can't understand. Since it's not numeric, ```\nmean```\n has no idea how to deal with it. (After all, how would you compute the mean of ```\nabc```\n and ```\ndef```\n?)\nThere are a few ways to solve this problem, but \"ignoring it\" isn't one of them, because, as the warning indicates, in a future version of pandas, this warning will become an error that will stop your code from running.\n\nUse ```\nnumeric_only=True```\n. This will cause ```\nmean```\n to skip columns that aren't numeric — ```\ncol3```\n in this case:\n```\n>>> df.mean(numeric_only=True)\ncol1    2.000000\ncol2    0.666667\ndtype: float64\n```\n\n(Notice how ```\ncol3```\n is omitted).\n\nSelect only the columns you need to operate on:\n```\n>>> df[['col1', 'col2']].mean()\ncol1    2.000000\ncol2    0.666667\ndtype: float64\n```\n\n\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Attribute Reduction Vs Dimensional Reduction\r\n                \r\nWhat is the difference between attribute reduction and dimensional reduction?\n\nWhat methods are considered attribute reduction techniques as opposed to dimensional reduction? \n    ", "Answer": "\r\nI am not aware of the term 'attribute reduction' in machine learning. Can you provide a link to what this refers to?\n\nOn the other hand, some papers use the term 'attribute selection' to refer to feature selection.\n\nFeature selection is a special type of dimensionality reduction in which the set of features produced must be a subset of the original features. Importantly, it means the features have not been transformed or altered in any way other than inclusion or exclusion.\n\nGeneral dimensionality reduction usually works by first transforming the input features into a new representation, such as by using the coordinate transformation that corresponds to the PCA dimensions, or by first projecting the data into a new space (of possibly higher dimensionality) via a kernel function, and then using some measure of informativeness to prune away features in that new space. \n\nDimensionality reduction could also involve simpler transformation, such as averaging together multiple components of the input feature vector due to observed collinearity. Even though the transformation is simplistic (taking an average), it still differs from feature selection in that the new feature is not a subset of the original.\n\nIn summary, the main difference is that feature selection does not change anything apart from dropping some of the less informative features of the original input. It keeps the remaining features without change. Dimensionality reduction transforms the data, and the final representation may be quite different (even apart from dimensionality) from the original input features.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "The difference between defining a step in pipline or as a step in param_grid, e.g. dimensionality reduction\r\n                \r\nIn this scikit-learn documentation:\nhttps://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html#sphx-glr-auto-examples-compose-plot-compare-reduction-py\nI can not figure out the purpose of having dimensionality reduction in both the pipe and param_grid; In other words what would happen if all the dimensionality reductions where defined in either pipe or param_grid ? Here is the code:\n\n```\npipe = Pipeline(  \n    [\n        # the reduce_dim stage is populated by the param_grid\n        (\"reduce_dim\", \"passthrough\"),\n        (\"classify\", LinearSVC(dual=False, max_iter=10000)),\n    ]\n)  \n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        \"reduce_dim\": [PCA(iterated_power=7), NMF()],\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n    {\n        \"reduce_dim\": [SelectKBest(chi2)],\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n]\nreducer_labels = [\"PCA\", \"NMF\", \"KBest(chi2)\"]\n\ngrid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid)\nX, y = load_digits(return_X_y=True)\ngrid.fit(X, y)  \n```\n\n    ", "Answer": "\r\nIt needs to be in the ```\nparam_grid```\n because you want to try different options.\nAnd it also needs to be in the ```\npipeline```\n otherwise the option that you try at a given point in time won't be used at all.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "transitive reduction algorithm: pseudocode?\r\n                \r\nI have been looking for an algorithm to perform a transitive reduction on a graph, but without success. There's nothing in my algorithms bible (Introduction To Algorithms by Cormen et al) and whilst I've seen plenty of transitive closure pseudocode, I haven't been able to track down anything for a reduction. The closest I've got is that there is one in \"Algorithmische Graphentheorie\" by Volker Turau (ISBN:978-3-486-59057-9), but unfortunately I don't have access to this book! Wikipedia is unhelpful and Google is yet to turn up anything. :^(\n\nDoes anyone know of an algorithm for performing a transitive reduction?\n    ", "Answer": "\r\nSee Harry Hsu. \"An algorithm for finding a minimal equivalent graph of a digraph.\", Journal of the ACM, 22(1):11-16, January 1975.  The simple cubic algorithm below (using an N x N path matrix) suffices for DAGs, but Hsu generalizes it to cyclic graphs.\n\n```\n// reflexive reduction\nfor (int i = 0; i < N; ++i)\n  m[i][i] = false;\n\n// transitive reduction\nfor (int j = 0; j < N; ++j)\n  for (int i = 0; i < N; ++i)\n    if (m[i][j])\n      for (int k = 0; k < N; ++k)\n        if (m[j][k])\n          m[i][k] = false;\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "iOS Multitasking: Reductions performed by system upon entering background\r\n                \r\nI've watched the WWDC 2010 talks about adopting multitasking and I have started enabling our app, however I have a question regarding releasing non-visible views.\n\nAccording to the talk (Session 105 - Adopting Multitasking on iPhone OS, Part 1 - 00:34:50) it is stated that the system releases \"non-visible UIViewController views\" when entering the background. However during my testing (in the simulator), I'm not seeing this happen. \n\nThe ```\n-viewDidUnload```\n methods aren't being called, and after entering the foreground and viewing a previously hidden VC, the ```\n-loadView```\n method is not being called either. So the hidden VC views in my UITabBarController aren't being released at all, contrary to what they said in the talk. Any ideas?\n\nThere's no mention of this in the \"Moving to the Background\" documentation, only in the talk.\n\nAny clarification would be greatly appreciated.\n    ", "Answer": "\r\nHave you tried simulating a memory warning while your app is in the background?  I believe that the views are only released when other apps start requesting memory.  If memory consumption stays the same, your views are not released so that you can switch back quicker.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction with OpenMP\r\n                \r\nI am trying to compute mean of a 2d matrix using openmp. This 2d matrix is actually an image.\n\nI am doing the thread-wise division of data. For example, if I have ```\nN```\n threads than I process Rows/```\nN```\n number of rows with ```\nthread0```\n, and so on. \n\nMy question is: Can I use the openmp reduction clause with \"```\n#pragma omp parallel```\n\"?\n\n```\n#pragma omp parallel reduction( + : sum )\n{\n    if( thread == 0 )\n       bla bla code \n       sum = sum + val;\n\n    else if( thread == 1 )\n       bla bla code\n       sum = sum + val;\n}\n```\n\n    ", "Answer": "\r\nYes, you can - the reduction clause is applicable to the whole parallel region as well as to individual ```\nfor```\n worksharing constructs. This allows for e.g. reduction over computations done in different parallel sections (the preferred way to restructure the code):\n\n\n\n```\n#pragma omp parallel sections private(val) reduction(+:sum)\n{\n   #pragma omp section\n   {\n      bla bla code\n      sum += val;\n   }\n   #pragma omp section\n   {\n      bla bla code\n      sum += val;\n   }\n}\n```\n\n\nYou can also use the OpenMP ```\nfor```\n worksharing construct to automatically distribute the loop iterations among the threads in the team instead of reimplementing it using sections:\n\n```\n#pragma omp parallel for private(val) reduction(+:sum)\nfor (row = 0; row < Rows; row++)\n{\n   bla bla code\n   sum += val;\n}\n```\n\n\nNote that reduction variables are private and their intermediate values (i.e. the value they hold before the reduction at the end of the ```\nparallel```\n region) are only partial and not very useful. For example the following serial loop cannot be (easily?) transformed to a parallel one with reduction operation:\n\n```\nfor (row = 0; row < Rows; row++)\n{\n   bla bla code\n   sum += val;\n   if (sum > threshold)\n      yada yada code\n}\n```\n\n\nHere the ```\nyada yada code```\n should be executed in each iteration once the accumulated value of ```\nsum```\n has passed the value of ```\nthreshold```\n. When the loop is run in parallel, the private values of ```\nsum```\n might never reach ```\nthreshold```\n, even if their sum does.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "From Angular 2 to Angular 4 - File size reduction and speed enhancement\r\n                \r\nI'm using Webpack and upgraded from Angular 2 to Angular 4 successfully. However I'm not seeing the expected 60% file size reductions neither my compiled vendors.js nor app.js.\n\nvendor.js\nbefore: 913kb\nnow: 975kb\n\napp.js\nbefore: 308kb\nnow: 307kb\n\nWhat am I missing? Do we need to use Angular CLI to take advantage of the file size reduction?\n\nmy package.json:\n\n```\n  \"dependencies\": {\n    \"@angular/common\": \"~4.0.1\",\n    \"@angular/compiler\": \"~4.0.1\",\n    \"@angular/core\": \"~4.0.1\",\n    \"@angular/forms\": \"~4.0.1\",\n    \"@angular/http\": \"~4.0.1\",\n    \"@angular/platform-browser\": \"~4.0.1\",\n    \"@angular/platform-browser-dynamic\": \"~4.0.1\",\n    \"@angular/router\": \"~4.0.1\",\n    \"core-js\": \"^2.4.1\",\n    \"rxjs\": \"5.0.1\",\n    \"zone.js\": \"^0.8.4\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^6.0.45\",\n    \"angular2-template-loader\": \"^0.6.0\",\n    \"awesome-typescript-loader\": \"^3.0.4\",\n    \"copy-webpack-plugin\": \"^4.0.0\",\n    \"css-loader\": \"^0.25.0\",\n    \"css-to-string-loader\": \"^0.1.2\",\n    \"extract-text-webpack-plugin\": \"^1.0.1\",\n    \"file-loader\": \"^0.8.5\",\n    \"html-loader\": \"^0.4.3\",\n    \"html-webpack-plugin\": \"^2.15.0\",\n    \"ng2-facebook-sdk\": \"^1.1.0\",\n    \"null-loader\": \"^0.1.1\",\n    \"raw-loader\": \"^0.5.1\",\n    \"resolve-url\": \"^0.2.1\",\n    \"rimraf\": \"^2.5.2\",\n    \"style-loader\": \"^0.13.1\",\n    \"to-string-loader\": \"^1.1.5\",\n    \"typescript\": \"~2.1.6\",\n    \"webpack\": \"^1.13.0\",\n    \"webpack-dev-server\": \"^1.14.1\",\n    \"webpack-merge\": \"^0.14.0\"\n  }\n}\n```\n\n    ", "Answer": "\r\nBased on Estus comment (and thank you). We need to use AoT (Ahead of Time Compilation) not the usual JiT (Just in Time Compilation) in order to take advantage of Angular 4 size reduction and speed enhancement. Using AoT with Angular Cli with create AoT for client browsers (platform-browser). \n\nSee Angular ahead-of-time compilation cookbook\n\nIt is not to be confused with running Angular on the server for search engines indexing and SEO benefits using Universal (another form of AoT using platform-server).\n\nAngular Universal Doc\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Non Atomic Parallel Reduction with Metal\r\n                \r\nI'm just getting into the world of parallel reductions. I'm trying to implement this with Metal. I have been able to successfully write a simple version using atomic types and using the atomic_fetch_* functions. \n\nI am now trying trying to do something similar with non-atomic variables, a simple struct. \n\nDefined like this:\n\n```\nstruct Point2\n{\n    int x;\n    int y;\n};\n```\n\n\nWith a kernel function like this: \n\n```\nkernel void compareX(const device Point2 *array [[ buffer(0) ]],\n                 device Point2 *result [[ buffer(1) ]],\n                 uint id [[ thread_position_in_grid ]],\n                 uint tid [[ thread_index_in_threadgroup ]],\n                 uint bid [[ threadgroup_position_in_grid ]],\n                 uint blockDim [[ threads_per_threadgroup ]]) {\n\n    threadgroup Point2 shared_memory[THREADGROUP_SIZE];\n\n    uint i = bid * blockDim + tid;\n    shared_memory[tid] = array[i];\n\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n\n    // reduction in shared memory\n    for (uint s = 1; s < blockDim; s *= 2) {\n        if (tid % (2 * s) == 0 && shared_memory[tid + s].x < shared_memory[tid].x) {\n\n            shared_memory[tid] = shared_memory[tid + s];\n        }\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n    }\n\n    if (0 == tid ) {\n///THIS IS NOT CORRECT\n        result[0] = shared_memory[0];\n    }\n\n}\n```\n\n\nI first thought something was going wrong with the memory copying to/from the buffers but I have verified the to/from CPU/GPU is working correctly with the struct. I then realized it has to do with cross thread group synchronizations. \n\nThere are a lot of examples/doc for CUDA but very little for anything else and CUDA doesn't always translate very well to Metal.\n\nWhat is the way to get cross thread group synchronization without atomic types?\n\nThe kernel is attempting to get the min Point in the input array. Right now the result changes across executions because of the write orders. \n    ", "Answer": "\r\nThis may not be the most correct, or best solution. But it is one I came up with after struggling with this for a while. If others find a better solution please post! This also may go out of date with different versions of Metal.\n\nI first tried to use the ```\n_atomic<T>```\n that is included in the Metal language on my struct. This should work. After struggling a ton with that I finally checked the doc and realized that template is currently restricted by apple to bool's, int's and uint's.\n\nI then tried to use an atomic int to \"lock\" critical compare sections but wasn't successful in actually protecting the critical section. I was likely doing something wrong with this implementation and could see it working. \n\nI then simplified to return an index instead of the point, which allows me to use an atomic_int on the result again. Kinda cheating, and still uses an atomic for the reduction. But it works so I can keep going. \n\nHere is how the kernel now looks:\n\n```\n\nkernel void compareX(const device Point2 *array [[ buffer(0) ]],\n                     device atomic_int *result [[ buffer(1) ]],\n                     uint id [[ thread_position_in_grid ]],\n                     uint tid [[ thread_index_in_threadgroup ]],\n                     uint bid [[ threadgroup_position_in_grid ]],\n                     uint blockDim [[ threads_per_threadgroup ]]) {\n\n    threadgroup int shared_memory[THREADGROUP_SIZE];\n    uint i = bid * blockDim + tid;\n    shared_memory[tid] = i;\n\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n\n    for (uint s = 1; s < blockDim; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            // aggregate the index to our smallest value in shared_memory\n            if ( array[shared_memory[tid + s]].x < array[shared_memory[tid]].x) {\n                shared_memory[tid] = shared_memory[tid + s];\n            }\n        }\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n    }\n    if (0 == tid ) {\n        // get the current index so we can test against that\n        int current = atomic_load_explicit(result, memory_order_relaxed);\n\n        if( array[shared_memory[0]].x < array[current].x) {\n            while(!atomic_compare_exchange_weak_explicit(result, &current, shared_memory[0], memory_order_relaxed, memory_order_relaxed)) {\n                // another thread won. Check if we still need to set it.\n                if (array[shared_memory[0]].x > array[current].x) {\n                    // they won, and have a smaller value, ignore our best result\n                    break;\n                }\n            }\n        }\n    }\n}\n\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Red eye reduction algorithm\r\n                \r\nI need to implement red eye reduction for an application I am working on.\n\nGoogling mostly provides links to commercial end-user products.\n\nDo you know a good red eye reduction algorithm, which could be used in a GPL application?\n    ", "Answer": "\r\nI'm way late to the party here, but for future searchers I've used the following algorithm for a personal app I wrote.\n\nFirst of all, the region to reduce is selected by the user and passed to the red eye reducing method as a center Point and radius.  The method loops through each pixel within the radius and does the following calculation:\n\n```\n//Value of red divided by average of blue and green:\nPixel pixel = image.getPixel(x,y);\nfloat redIntensity = ((float)pixel.R / ((pixel.G + pixel.B) / 2));\nif (redIntensity > 1.5f)  // 1.5 because it gives the best results\n{\n    // reduce red to the average of blue and green\n    bm.SetPixel(i, j, Color.FromArgb((pixel.G + pixel.B) / 2, pixel.G, pixel.B));\n}\n```\n\n\nI really like the results of this because they keep the color intensity, which means the light reflection of the eye is not reduced.  (This means eyes keep their \"alive\" look.)\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "C++ Armadillo and OpenMp: Parallelization of summation of outer products - define reduction for Armadillo matrix\r\n                \r\nI am trying to parallelize a for loop using OpenMP which sums over Armadillo matrices. I have the following code:\n\n```\n#include <armadillo>\n#include <omp.h>\n\nint main()\n{\n\n        arma::mat A = arma::randu<arma::mat>(1000,700);\n        arma::mat X = arma::zeros(700,700);\n        arma::rowvec point = A.row(0);\n\n        # pragma omp parallel for shared(A) reduction(+:X)\n        for(unsigned int i = 0; i < A.n_rows; i++){\n                arma::rowvec diff = point - A.row(i);\n                X += diff.t() * diff; // Adding the matrices to X here\n        }\n\n}\n```\n\n\nI am getting this error:\n\n```\n[Legendre@localhost ~]$ g++ test2.cpp -o test2 -O2 -larmadillo -fopenmp\ntest2.cpp: In function ‘int main()’:\ntest2.cpp:11:52: error: user defined reduction not found for ‘X’\n```\n\n\nI read up on defining reductions, but I haven't found examples for working with Armadillo matrices. What is the best way to define a reduction for Armadillo matrices in my case?\n    ", "Answer": "\r\nThose reductions are only available for built-in types (```\ndouble```\n, ```\nint```\n, etc.).  Thus you have to do the reduction yourself, which is simple.  Just accumulate the results for each thread in a thread-local variable and add this to the global result within a critical section.\n\n```\n#include <armadillo>\n#include <omp.h>\n\nint main()\n{\n\n  arma::mat A = arma::randu<arma::mat>(1000,700);\n  arma::mat X = arma::zeros(700,700);\n  arma::rowvec point = A.row(0);\n\n  #pragma omp parallel shared(A)\n  {\n    arma::mat X_local = arma::zeros(700,700);\n\n    #pragma omp for\n    for(unsigned int i = 0; i < A.n_rows; i++)\n    {\n      arma::rowvec diff = point - A.row(i);\n      X_local += diff.t() * diff; // Adding the matrices to X here\n    }\n\n    #pragma omp critical\n    X += X_local;\n  }\n}\n```\n\n\nWith more recent OpenMP (4.5 I think?) you can also declare a user-defined reduction for your type.\n\n```\n#include <armadillo>\n#include <omp.h>\n\n#pragma omp declare reduction( + : arma::mat : omp_out += omp_in ) \\\n  initializer( omp_priv = omp_orig )\n\nint main()\n{\n\n  arma::mat A = arma::randu<arma::mat>(1000,700);\n  arma::mat X = arma::zeros(700,700);\n  arma::rowvec point = A.row(0);\n\n  #pragma omp parallel shared(A) reduction(+:X)\n  for(unsigned int i = 0; i < A.n_rows; i++)\n  {\n    arma::rowvec diff = point - A.row(i);\n    X += diff.t() * diff; // Adding the matrices to X here\n  }\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Dask how to scatter data when doing a reduction\r\n                \r\nI am using Dask for a complicated operation. First I do a reduction which produces a moderately sized df (a few MBs) which I then need to pass to each worker to calculate the final result so my code looks a bit like this\n```\nintermediate_result = ddf.reduction().compute()\n\nfinal_result = ddf.reduction(\n    chunk=function, chunk_kwargs={\"intermediate_result\": intermediate_result}\n)\n```\n\nHowever I am getting the warning message that looks like this\n```\nConsider scattering large objects ahead of time\nwith client.scatter to reduce scheduler burden and\nkeep data on workers\n\n    future = client.submit(func, big_data)    # bad\n\n    big_future = client.scatter(big_data)     # good\n    future = client.submit(func, big_future)  # good\n  % (format_bytes(len(b)), s)\n```\n\nI have tried doing this\n```\nintermediate_result = client.scatter(intermediate_result, broadcast=True)\n```\n\nBut this isn't working as the function now sees this as a Future object and not the datatype it is supposed to be.\nI can't seem to find any documentation on how to use scatter with reductions, does anyone know how to do this? Or should I just ignore the warning message and pass the moderately sized df as I am?\n    ", "Answer": "\r\nActually, the best solution probably is not to scatter your materialised result, but to avoid computing it in the first place. You can simply remove the ```\n.compute()```\n, which will mean all the calculation gets done in one stage, with the results automatically moved where you need them.\nAlternatively, if you want to have a clear boundary between the stages, you can use\n```\nintermediate_result = ddf.reduction().persist()\n```\n\nwhich will kick off the reduction and store it on workers without pulling it to the client. You can choose to wait on this to finish before the next step or not.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction on user defined Fortran type containing allocatable array\r\n                \r\nI want to do an OpenMP reduction on a user defined Fortran type. I know OpenMP\ndoes not support Fortran types in reduction clauses but it is possible to define\nown reductions. This is done in the following example. This also works and does what it is\nexpected to\n\n```\n module types \n  !!! your type this can contain scalars and arrays\n  type my_type\n    Real*8,allocatable,dimension( : )  ::x\n  end type\n\n  !!! makes it possible to use the plus symbol for the reduction staement\n  !!! replaces my_add by plus symbol\n  interface operator(+)\n     module procedure :: my_add\n  end interface\n\n !$omp declare reduction (+ : my_type : omp_out = omp_out + omp_in) initializer (omp_priv = my_type ([0,0,0,0,0,0,0,0,0,0]))\n\n contains\n\n\n  function my_add( a1 , a2 )\n    type( my_type ),intent( in ) :: a1, a2\n    type( my_type )              :: my_add\n    my_add%x          =   a1%x + a2%x\n    return\n  end function my_add\n end module types \n\n\n\n\n\n\nprogram main\n  use types\n  use omp_lib\n  type(my_type) :: my_var\n\n  ! Initialize the reduction variable before entering the OpenMP region\n  Allocate( my_var%x( 1:10 ) )  \n  my_var%x = 0d0\n\n  !$omp parallel reduction (+ : my_var) num_threads(4)\n    my_var%x = omp_get_thread_num() + 6\n    print*,omp_get_thread_num()\n  !$omp end parallel\n\n  print *, \"sum of x is \", my_var%x\nend program\n```\n\n\nMy problem is now the allocatable array.\n\nBecause I hard coded the array initializer for the OpenMP reduction statement as ```\ninitializer (omp_priv = my_type ([0,0,0,0,0,0,0,0,0,0]))```\n\nI have to put 10 zeros there since the array is allocated with a length of 10.\nIs it possible to this with a variable name N (length of array)??\n    ", "Answer": "\r\nInside the reduction initializer clause we have limited access to variables, making an array constructor of variable length difficult.  However, we have available to us the Fortran version of the C++ approach.\n\nWe can use the variable ```\nomp_orig```\n to refer to the \"storage of the original variable to be reduced\":\n\n```\n!$omp declare reduction (+ : my_type : omp_out = omp_out + omp_in) &\n!$omp&   initializer (omp_priv=omp_orig)\n```\n\n\nThe assignment statement here successfully allocates the array component of each private copy.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Improving my Image Resize Function for Better Quality Images... loop reductions?\r\n                \r\nSome while ago I was trying to create an image uploading facility for the back-end of my website. I managed to achieve this, but I have still got poor image quality on the smaller images.\n\nI need to create 4 images:\n\n\nZoom image 1800 x 1800 px max\nDisplay image 180 x 275px max\nSearch Image 120 x 100px max\nTiny thumbnail 50 x 50px max\n\n\nI generally manually resize and image to 1800 x 1800 with Photoshop or something before uploading it, then upload and resize using the code below (images are all jpgs)\n\nVariables are:\n\n\nFileName = initially uploaded fine\nNewFileName = file name to save resized image as \nmaxWidth / maxHeight - self explanatory\nuploadDir = the directory to save to\nresolution = the quality jpg resolution 0-100, I'm using 80 for these examples\n\n```\n Public Shared Sub ResizeImages(FileName, NewFileName, maxWidth, maxHeight, uploadDir, resolution)\n Try\n    Dim originalImg As System.Drawing.Image = System.Drawing.Image.FromFile(uploadDir & FileName)\n    Dim aspectRatio As Double\n    Dim newHeight As Integer\n    Dim newWidth As Integer\n   ' Calculate Size '\n        If originalImg.Width > maxWidth Or originalImg.Height > maxHeight Then\n            If originalImg.Width >= originalImg.Height Then ' image is wider than tall\n                newWidth = maxWidth\n                aspectRatio = originalImg.Width / maxWidth\n                newHeight = originalImg.Height / aspectRatio\n            Else ' image is taller than wide\n                newHeight = maxHeight\n                aspectRatio = originalImg.Height / maxHeight\n                newWidth = originalImg.Width / aspectRatio\n            End If\n        Else ' if image is not larger than max then keep original size\n            newWidth = originalImg.Width\n            newHeight = originalImg.Height\n        End If\n\n        Dim newImg As New Bitmap(originalImg, CInt(newWidth), CInt(newHeight)) '' blank canvas\n        Dim canvas As Graphics = Graphics.FromImage(newImg) 'graphics element\n\n        '*** compress ***'\n        Dim myEncoderParameters As EncoderParameters\n        myEncoderParameters = New EncoderParameters(1)\n        ' set quality level based on \"resolution\" variable\n        Dim myEncoderParameter = New EncoderParameter(System.Drawing.Imaging.Encoder.Quality, CType(resolution, Int32))\n        myEncoderParameters.Param(0) = myEncoderParameter\n\n        canvas.CompositingQuality = System.Drawing.Drawing2D.CompositingQuality.HighQuality\n        canvas.InterpolationMode = System.Drawing.Drawing2D.InterpolationMode.HighQualityBicubic\n        canvas.SmoothingMode = System.Drawing.Drawing2D.SmoothingMode.HighQuality\n\n        canvas.DrawImage(newImg, New Rectangle(0, 0, newWidth, newHeight))\n        newImg.Save(uploadDir & (NewFileName), getCodec(\"image/jpeg\"), myEncoderParameters)\n\n        '*** Close ***'\n        canvas.Dispose()\n        originalImg.Dispose()\n        newImg.Dispose()\n        '*** Nothing ***'\n        canvas = Nothing\n        newImg = Nothing\n        originalImg = Nothing\n\n    Catch ex As Exception\n        HttpContext.Current.Response.Write(ex.ToString & \" \" & uploadDir & \" \" & FileName & \" _ \" & NewFileName)\n    End Try\n\nEnd Sub\n```\n\n\n\nTo achieve all four images, I pass the sizes required as a list, and then loop that list, in descending order of intended file size, so, the largest one first, I then pass the most recently uploaded image into the function as the ```\nFileName```\n parameter so that each time, the function is receiving a smaller image, so not trying to resize a 2000x2000px image to 50x50px as I realise from reading various posts, that this much reduction will result in poor quality.\n\nHaving run the loops through in this method, my tiny thumbnails are quite good quality, but my middle images are still poor.\n\nHere they are in descending size order:\n\nhttp://www.hartnollguitars.co.uk/Products2/0/0/0/9/6/57/1347831580.jpg\nhttp://www.hartnollguitars.co.uk/Products2/0/0/0/9/6/57/1347831580-dis.jpg\nhttp://www.hartnollguitars.co.uk/Products2/0/0/0/9/6/57/1347831580-se.jpg\nhttp://www.hartnollguitars.co.uk/Products2/0/0/0/9/6/57/1347831580-tb.jpg\n\nas you can see, both the \"search\" and \"display\" images are still blocky around the edge of the guitar.\n\nWhat am I doing wrong?!\n\nIf my reduction is to much, how would I go about running an in-memory gradual reduction.\n\nWhat I mean by this, is, it strikes me, that the function above is saving the file to disc each time, that must take up some time, so if I were to loop a reduction function, reducing the image in size, in small increments (say 10% at a time) in memory, and then save the final image to disc when the reduction reaches the correct size. I'm not sure how to do this though.\n\nI'm using ASP.NET 2.0 and am relatively new to it, so I am not fully aware of all the methods available to me.\n\nAny code examples would help greatly!\n\nThanks\n    ", "Answer": "\r\nWhat you are doing wrong is that you are creating a downsized image using the ```\nBitmap```\n constructor, then you draw that image onto itself. The ```\nBitmap```\n constructor can naturally not use the quality settings that you set in the ```\nGraphics```\n object later on to resize the image, so the quality will be poor.\n\nInstead you should create a blank ```\nBitmap```\n object with the constructor that takes only the size:\n\n```\nDim newImg As New Bitmap(newWidth, newHeight)\n```\n\n\nThen you should draw the original image on the canvas:\n\n```\ncanvas.DrawImage(originalImg, New Rectangle(0, 0, newWidth, newHeight))\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction of large arrays in Fortran\r\n                \r\nI know that similar questions to this have been asked sometimes: Openmp array reductions with Fortran, Reducing on array in OpenMP, even in Intel forums (https://software.intel.com/en-us/forums/intel-moderncode-for-parallel-architectures/topic/345415) but I would like to know your opinion because the scalability that I get is not the one that I expect.\n\nSo I need to fill a really large array of complex numbers, which I would like to parallelize with OpenMP. Our first approach is this one:\n\n```\nCOMPLEX(KIND=DBL), ALLOCATABLE :: huge_array(:)\nCOMPLEX(KIND=DBL), ALLOCATABLE :: thread_huge_array(:)\nINTEGER :: huge_number, index1, index2, index3, index4, index5, bignumber1, bignumber2, smallnumber, depending_index\n\nALLOCATE(huge_array(huge_number))\n\n!$OMP PARALLEL FIRSTPRIVATE(thread_huge_array)\n      ALLOCATE(thread_huge_array(SIZE(huge_array)))\n      thread_huge_array = ZERO\n!$OMP DO\n      DO index1=1,bignumber1\n         ! Some calculations\n         DO index2=1,bignumber2\n            ! Some calculations\n            DO index3=1,6\n               DO index4=1,6\n                  DO index5=1,smallnumber\n                     depending_index = function(index1, index2, index3, index4, index5)\n                     thread_huge_array(depending_index) = thread_huge_array(depending_index)\n                  ENDDO\n               ENDDO \n            ENDDO\n         ENDDO \n      ENDDO \n!$OMP END DO\n!$OMP BARRIER\n!$OMP MASTER\n      huge_array = ZERO\n!$OMP END MASTER\n!$OMP CRITICAL\n      huge_array = huge_array + thread_huge_array\n!$OMP END CRITICAL\n      DEALLOCATE(thread_huge_array)\n!$OMP END PARALLEL\n```\n\n\nSo, with that approach, we get good scalability until 8 cores, reasonable scalability until 32 cores and from 40 cores, it is slower than with 16 cores (we have a machine with 80 physical cores). Of course, we cannot use REDUCTION clause because the size of the array is so big that it doesn't fit in the stack (even increasing ulimit to the maximum allowed in the machine).\n\nWe have tried a different approach with this one:\n\n```\nCOMPLEX(KIND=DBL), ALLOCATABLE :: huge_array(:)\nCOMPLEX(KIND=DBL), POINTER:: thread_huge_array(:)\nINTEGER :: huge_number\n\nALLOCATE(huge_array(huge_number))\n\nALLOCATE(thread_huge_array(SIZE(huge_array),omp_get_max_threads()))\nthread_huge_array = ZERO\n\n!$OMP PARALLEL PRIVATE (num_thread)\n\n      num_thread = omp_get_thread_num()+1\n!$OMP DO\n      DO index1=1,bignumber1\n         ! Some calculations\n         DO index2=1,bignumber2\n            ! Some calculations\n            DO index3=1,6\n               DO index4=1,num_weights_sp\n                  DO index5=1,smallnumber\n                     depending_index = function(index1, index2, index3, index4, index5)\n                     thread_huge_array(depending_index, omp_get_thread_num()) = thread_huge_array(depending_index, omp_get_thread_num())\n                  ENDDO\n               ENDDO \n            ENDDO\n         ENDDO \n      ENDDO \n!$OMP END DO\n!$OMP END PARALLEL\n\nhuge_array = ZERO\n\nDO index_ii = 1,omp_get_max_threads()\n   huge_array = huge_array + thread_huge_array(:,index_ii)\nENDDO\n\nDEALLOCATE(thread_huge_array)\n\nDEALLOCATE(huge_array)\n```\n\n\nAnd in this last case, we obtain longer times for the method (due to the allocation of the memory, which is much bigger) and worse relative acceleration.\n\nCan you provide some hints to achieve a better acceleration? Or is it impossible with these huge arrays with OpenMP? \n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Benchmark in GHCi, how can I enable to show number of reductions for expression?\r\n                \r\nI am playing with GHCi and I wondering how can I enable in GHCi to raise the number of reduction in expression to compare different solutions?\n    ", "Answer": "\r\nHugs had (has?) such an option. \n\nIn GHCi, you can enter ```\nPrelude> :set +s```\n to print rough timings after each evaluation. Then apply empirical orders of growth analysis as needed. \n\nFor proper testing compile with the -O2 flag, and run standalone executables at your shell prompt with \"+RTS -s\" option to get the stats. Also, there's Criterion package.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Linear time reduction of languages in class P, complexity implications\r\n                \r\nI am having problem understanding this topic of P and NP reductions. I understand that if language L1 can be reduced to Language L2 in linear time and L2 is in P, this implies that L1 is in P. But if we know L2 has a time complexity of lets say theta(n log n), can we say that L1 runs in O(n log n)? since the reduction from L1 to L2 is in linear time and L2 runs in theta(n log n) and so it will be O(n) + theta(n log n). And also lets say L2 can be also linearly reduced to L3, we can say L3 runs in omega(n log n)?\n    ", "Answer": "\r\ntl;dr: Yes. And yes in case you mean big Omega.\n\n\n\nThe first part is correct: If you can decide L2 in Theta((n*log(n))) which implies it can be done in O(n*log(n)) and you can reduce L1 to L2 in O(n), then you can also decide for L1 in O(n*log(n)) with exactly the argument you made. (Note: this does not mean, that you can't possibly decide L1 in less than this - there might be an algorithm to solve L1 in O(n). It's only an upper bound...) \n\nHowever, the second part is not correct. If you can reduce L2 to L3, then you can say nothing about L3s running time not matter what the running time of the reduction from L2 to L3 is. (Update: this only shows that L3 might be harder, not more) L3 might be a very hard problem, like SAT for instance. It then is very likely that you can reduce L2 to it, i.e. that you can solve L2 with 'rephrasing' (a reduction) the problem + a SAT-solver - still SAT is NP-complete.\n\n\n\nDISCLAIMER: as noted in the comments by DavidRicherby the second part of my answer is wrong as it stands - @ uchman21 you were right, L3 has to be in Omega(n*log(n)) (note the upper case!):\n\nIf we know the complexity of L2 is Theata(n*log(n)) (upper and lower bounds, O(n*log(n)) and Omega(n*log(n))) and we can reduce L2 to L3 in time O(n), then L3 is at least as hard as L2 - because we know there is no algorithm which can solve the problem L2 faster than Omega(n*log(n)). However, if L3 was faster, that is in o(n*log(n)), then the algorithm 'reduction+solve_L3' runs in O(n)+o(n*log(n)) which still is in o(n*log(n)) and it solves L2 - contradiction. Hence, L3 has to be in Omega(n*log(n)).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "GCP - GPU staging time reduction\r\n                \r\nI have an application that requires the smallest boot-time/TTL possible with GPUs attached to a VM in GCP CE. To keep cost down, my infrastructure is dependent on starting and stopping instances as demand increases/decreases.\nI have achieved sub-5second start times with custom images without GPUs, but as soon as I attach a GPU, the time to \"RUNNING\" is always past 20-30s.\nI have tried multiple different distros, clear linux, prepackaged Nvidia driver images, minimal installs of Fedora, minimalised Debian, reductions to kernel and userspace - systemd-analyze says my boot-time is 3s, but starting the VM with a GPU takes 20-30s in \"STAGING\" before running.\nThis only occurs when the gpu is attached to the VM and when removed the VM starts within the time mentioned by systemd-analyze. It is consistent across all distros and bootimages.\nIs there any packages or documentation I am missing to speed up this staging-time with a GPU attached or is this a limitation with GCP's internal staging of GPU instances?\nI'd much appreciate any help or advice.\nIf you're also experiencing this issue and would like to track its progress, I created a issue report:\nhttps://issuetracker.google.com/issues/200575905\n    ", "Answer": "\r\nThis is an internal limitation in GCE and GKE, there's not a lot that can be currently done to remediate this.\nHowever, I noticed that startup times have dropped over time, so there is some improvement in this matter.\nYou can report this via Public Issue Tracker to follow development.\nYou can also consider using Committed Use Discount or Sustained Use Discounts. It may be beneficial in the long run to keep the instances running and therefore avoid the startup problem altogether.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Does Church-Rosser theorem apply to call-by-value reduction?\r\n                \r\nI've been studying the lambda calculus and recently saw the Church-Rosser theorem. The theorem states that when applying reduction rules to terms in the lambda calculus, the ordering in which the reductions are chosen does not make a difference to the eventual result (from wiki). But I find this inconsistent with call-by-value reduction and normal order reduction. For example, a lambda term λz.(λx.x) y can be reduced to λz.z when following the normal order reduction rules. But it cannot be further reduced when using call-by-value reduction because call-by-value reduction forbids reduction inside a λ-abstraction. So the term term λz.(λx.x) y cannot be evaluated to the same result using different rules, which seems to contradict to the Church-Rosser theorem. What's the problem here? Please help me out.Thanks a lot!\n    ", "Answer": "\r\nI think you are being imprecise about the Church-Rosser theorem, and that's what is causing confusion. \n\nAs far as I have understood it (and I am writing this with \"The Implementation of Functional Programming Languages\" at hand), the theorem says the following:\n\n\n  If two lambda expressions E and F are interconvertible by any sequence of reductions, then there exists an expression G, such that both E and F can be reduced to G.\n\n\nFrom this follows only that one expression cannot have two distinct normal forms (although there might be no normal form at all). However, given two reductions orders, one might lead to an normal form, while the other one might diverge -- and some might even stop before, such as call-by-name.\n\nNow, in your case, ```\nE = \\z.z```\n is in normal form, and ```\nF = \\z.((\\x.x) z)```\n* can be reduced to it; the only thing that can be said here is that ```\nF```\n cannot be reduced to anything else than ```\nE```\n, while nothing is said about how much it has to be reduced. \n\nThere is also another part of the theorem, which says that if a normal form exists, normal order will find it. Again, no contradiction to your observation, since call-by-value and call-by-name might end up differently than normal order.\n\n*I assume you meant this insted of ```\n\\z.((\\x.x) y)```\n, since the latter would make no sense.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "User defined reduction not returning expected result with each run\r\n                \r\nI'm trying to find, parallelly using OpenMP, the minimum and maximum values in a 2d array as well as the indexes of the minimum and maximum. In my attempt, I use user-defined reductions, however I get unexpected results for each run.\n\nI've tried inspecting the values of min and max in the for loops and it seems that inside the parallelized for loops, the min and max values are as  expected. However, at the end of the run, min and max contain completely wacko values.\n\nMy reduction definition\n\n```\ntypedef struct {\n    int value;\n    int index_i;\n    int index_j;\n} Point;\n\n#pragma omp declare reduction(minimum : Point : \\\n    omp_out = omp_in.value < omp_out.value ? omp_in : omp_out) \\\n    initializer(omp_priv = {INT_MAX, 0, 0})\n#pragma omp declare reduction(maximum : Point : \\\n    omp_out = omp_in.value > omp_out.value ? omp_in : omp_out) \\\n    initializer(omp_priv = {0, 0, 0})\n```\n\n\nInitialization of the 2d array where ```\nsize```\n is ```\n10000```\n\n\n```\nfor (int i = 0; i < size; i++) {\n    for (int j = 0; j < size; j++) {\n        matrix[i][j] = rand()%99;\n    }\n}\n```\n\n\nThe parallelized loops:\n\n```\nint i, j, total=0;\nPoint min, max;\n\n#pragma omp parallel for reduction (+:total) reduction(minimum : min) reduction(maximum : max) private(j)\n    for (i = 0; i < size; i++) {\n        for (j = 0; j < size; j++) {\n            total += matrix[i][j];\n\n            if (matrix[i][j] < min.value) {\n                min.value = matrix[i][j];\n                min.index_i = i;\n                min.index_j = j;\n            }\n\n\n            if (matrix[i][j] > max.value) {\n                max.value = matrix[i][j];\n                max.index_i = i;\n                max.index_j = j;\n            }\n        }\n    }\n```\n\n\nThe expected result is that ```\nmin = 0```\n at index ```\n(0, 70)```\n and ```\nmax = 98```\n at index ```\n(0, 20)```\n.\n\nThe actual results are different each time but an example output:\n\n```\nThe min is -290390323 at index (21850, -9176672)\nThe max is 32595 at index (0, 0)\n```\n\n    ", "Answer": "\r\nPart of the idea of OpenMP is that it enables parallelization of existing, correct serial code.  Generally speaking, removing or ignoring all the omp pragmas from correct OpenMP code -- so that it runs strictly serially -- should not change the computed result.  Your code does not satisfy that requirement because you do not initialize the ```\nmin```\n and ```\nmax```\n accumulation variables.\n\nI guess you expected the initialization clause of your reduction definition to be applied to the shared variables, but you have misunderstood.  The initialization clause is used to initialize the per-thread local copies, not the shared variables. The local copies are at some point combined with the shared variables as part of the reduction, otherwise the code would not produce the same result when run serially.\n\nAdditionally, note that for C, OpenMP reduction initializer clauses in fact provide initializers, in the C standard's sense of the term.  These are not the same thing as assignment statements, and the difference is especially clear in your case, where the list item has a structure type.  Your initializers are fine as initializers, but they are not valid assignment expressions.  Therefore, they cannot be used to assign initial values to the shared variables inside the parallel region, for initializers appear only as part of variable declarations.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How can I utilize the 'red' and 'atom' PTX instructions in CUDA C++ code?\r\n                \r\nThe CUDA PTX Guide describes the instructions 'atom' and 'red', which perform atomic and non-atomic reductions. This is news to me (at least with respect to non-atomic reductions)... I remember learning how to do reductions with SHFL a while back. Are these instructions reflected or wrapped somehow in CUDA runtime APIs? Or some other way accessible with C++ code without actually writing PTX code?\n    ", "Answer": "\r\n\n  Are these instructions reflected or wrapped somehow in CUDA runtime APIs? Or some other way accessible with C++ code without actually writing PTX code?\n\n\nMost of these instructions are reflected in atomic operations (built-in intrinsics) described in the programming guide.  If you compile any of those atomic intrinsics, you will find ```\natom```\n or ```\nred```\n instructions emitted by the compiler at the PTX or SASS level in your generated code. \n\nThe ```\nred```\n instruction type will generally be used when you don't explicitly use the return value from from one of the atomic intrinsics.  If you use the return value explicitly, then the compiler usually emits the ```\natom```\n instruction.\n\nThus, it should be clear that this instruction by itself does not perform a complete classical parallel reduction, but certainly could be used to implement one if you wanted to depend on atomic hardware (and associated limitations) for your reduction operations.  This is generally not the fastest possible implementation for parallel reductions.\n\nIf you want direct access to these instructions, the usual advice would be to use inline PTX where desired.\n\nAs requested, to elaborate using ```\natomicAdd()```\n as an example:  \n\nIf I perform the following:\n\n```\natomicAdd(&x, data);\n```\n\n\nperhaps because I am using it for a typical atomic-based reduction into the device variable ```\nx```\n, then the compiler would emit a ```\nred```\n (PTX) or ```\nRED```\n (SASS) instruction taking the necessary arguments (the pointer to ```\nx```\n and the variable ```\ndata```\n, i.e. 2 logical registers).\n\nIf I perform the following:\n\n```\nint offset = atomicAdd(&buffer_ptr, buffer_size);\n```\n\n\nperhaps because I am using it not for a typical reduction but instead to reserve a space (```\nbuffer_size```\n) in a buffer shared amongst various threads in the grid, which has an offset index (```\nbuffer_ptr```\n) to the next available space in the shared buffer, then the compiler would emit a ```\natom```\n (PTX) or ```\nATOM```\n (SASS) instruction, including 3 arguments (```\noffset```\n, ```\n&buffer_ptr```\n, and ```\nbuffer_size```\n, in registers).\n\nThe ```\nred```\n form can be issued by the thread/warp which may then continue and not normally stall due to this instruction issue which will normally have no dependencies for subsequent instructions.  The ```\natom```\n form OTOH will imply modification of one of its 3 arguments (one of 3 logical registers).  Therefore subsequent use of the data in that register (i.e. the return value of the intrinsic, i.e. ```\noffset```\n in this case) can result in a thread/warp stall, until the return value is actually returned by the atomic hardware.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenCL 2.x - Sum Reduction function\r\n                \r\nFrom this previous post: strategy-for-doing-final-reduction, I would like to know the last functionalities offered by OpenCL 2.x (not 1.x which is the subject of this previous post above), especially about the atomic functions which allow to perform reductions of a array (in my case a sum reduction).\nOne told me that performances of OpenCL 1.x atomic functions (```\natom_add```\n) were bad and I could check it, so I am looking for a way to get the best performances for a ```\nfinal reduction function```\n (i.e the sum of each computed sum corresponding to each work-group).\nI recall the typical kind of kernel code that I am using for the moment :\n```\n__kernel void sumGPU ( __global const double *input, \n                       __global double *partialSums,\n               __local double *localSums)\n {\n  uint local_id = get_local_id(0);\n  uint group_size = get_local_size(0);\n\n  // Copy from global memory to local memory\n  localSums[local_id] = input[get_global_id(0)];\n\n  // Loop for computing localSums\n  for (uint stride = group_size/2; stride>0; stride /=2)\n     {\n      // Waiting for each 2x2 addition into given workgroup\n      barrier(CLK_LOCAL_MEM_FENCE);\n\n      // Divide WorkGroup into 2 parts and add elements 2 by 2\n      // between local_id and local_id + stride\n      if (local_id < stride)\n        localSums[local_id] += localSums[local_id + stride];\n     }\n\n  // Write result into partialSums[nWorkGroups]\n  if (local_id == 0)\n    partialSums[get_group_id(0)] = localSums[0];\n }             \n```\n\nAs you can see, at the end of kernel code execution, I get the array ```\npartialSums[number_of_workgroups]```\n containing all partial sums.\nCould you tell me please how to perform a second and final reduction of this array, with the best performances possibles of functions availables with OpenCL 2.x . A classic solution is to perform this final reduction with CPU but ideally, I would like to do it directly with kernel code.\nA suggestion of code snippet is welcome.\nA last point, I am working on MacOS High Sierra 10.13.5 with the following model :\n\nCan OpenCL 2.x be installed on my hardware MacOS model ?\n    ", "Answer": "\r\nAtomic functions should be avoided because they do harm performance compared to a parallel reduction kernel.  Your kernel looks to be on the right track, but you need to remember that you'll have to invoke it multiple times; do not perform the final sum on the host (unless you have a very small amount of data from the previous reduction).  That is, you need to keep invoking it until your local size equals your global size.  There's no way to do a single invocation for large amounts of data as there is no way to synchronize between work groups.\n\nAdditionally, you want to be careful to set an appropriate work group size (i.e. local size), which depends on local & global memory throughput & latency.  Unfortunately, as far as I'm aware there is no way to determine this through OpenCL, outside of self-profiling code, though that's not too difficult to write as OCL provides you with JIT compilation.  Through empirical testing I've found you should find a sweet spot between suffering too many bank conflicts (too large a local size) vs. global memory latency penalties (too small a local size).  It's best to do a benchmark first to determine optimal local size for your reduction, and then use that local size for future reductions.\n\nEdit: It's also worth noting that the best way to chain your kernel invocation together is through OpenCL events.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Parallel Reduction\r\n                \r\nI have read the article Optimizing Parallel Reduction in CUDA by Mark Harris, and I found it really very useful, but still I am sometimes unable to understand 1 or 2 concepts.\nIt is written on pg 18:\n\n```\n//First add during load\n\n// each thread loads one element from global to shared mem\n\nunsigned int tid = threadIdx.x;\n\nunsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n\nsdata[tid] = g_idata[i];\n__syncthreads();\n```\n\n\nOptimized Code: With 2 loads and 1st add of the reduction:\n\n```\n// perform first level of reduction,\n\n// reading from global memory, writing to shared memory\nunsigned int tid = threadIdx.x;                                    ...1\n\nunsigned int i = blockIdx.x*(blockDim.x*2) + threadIdx.x;          ...2\n\nsdata[tid] = g_idata[i] + g_idata[i+blockDim.x];                   ...3\n\n__syncthreads();                                                   ...4\n```\n\n\nI am unable to understand line 2; if I have 256 elements, and if I choose 128 as my blocksize, then why I am multiplying it with 2? Please explain how to determine the blocksize?\n    ", "Answer": "\r\nBasically, it is performing the operation shown in the picture below:\n\nThis code is basically saying that half of the threads will performance the reading from global memory and writing to shared memory, as shown in the picture.\nYou execute a Kernel, and now you want to reduce some values, you limit the access to the code above to only half of the total of threads running. Imagining you have 4 blocks, each one with 512 threads, you limit the code above to only be executed by the first two blocks, and you have a ```\ng_idate[4*512]```\n:\n```\nunsigned int i = blockIdx.x*(blockDim.x*2) + threadIdx.x;  \n\nsdata[tid] = g_idata[i] + g_idata[i+blockDim.x];\n```\n\nSo:\n```\nthread 0 of block = 0  will copy the position 0 and 512,  \nthread 1 of block = 0 position 1 and 513;\nthread 511 of block = 0 position 511 and 1023;\nthread 0 of block 1 position 1024 and 1536\nthread 511 of block = 1 position 1535 and 2047\n```\n\nThe ```\nblockDim.x*2```\n is used because each thread will access to position ```\ni```\n and ```\ni+blockDim.x```\n so you need to multiple by ```\n2```\n to guarantee that the threads on next ```\nid```\n block do not compute the position of ```\ng_idata```\n already computed.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Sequelizer console.log output not printing\r\n                \r\nI'm learning how to use Sequelize to persist data to a MySql database from a NodeJS application. The following script is my attempt to insert data into a table which has some records in it.\nThe issue is I am attempting to debug the script in various locations using console.log but I get no output.\n```\nconst db = require('../engage/db');\nconn = db.conn\n\nfunction init() {\n    const Reductions = conn.sequelize.define('Reductions', {\n        pid : {\n            type: conn.Sequelize.DataTypes.INTEGER,\n            allowNull: false,\n            primaryKey: true,\n        },\n        code : {\n            type: conn.Sequelize.STRING\n        },\n        duration : {\n            type: conn.Sequelize.DataTypes.INTEGER\n        }\n    });\n    return Reductions\n}\n\nasync function insert(p) {\n    console.log('Testing @ insert() entrance : ', p)\n    \n    let table = await init(conn.sequelize, conn.Sequelize)\n    \n    // conn.sequelize.sync({force:true}).then( async () => {\n    conn.sequelize.sync().then( async () => {\n        conn.sequelize.query('SELECT * FROM Reductions WHERE pid = :pid',{\n            replacements: {pid : p.id},\n            type: conn.Sequelize.QueryTypes.SELECT,\n        }).then((data) => {\n            console.log('TESTING DATA: ', data)\n            return data\n        }).then((results) => {\n            console.log('Checking results: ', results)\n        })\n    }).finally(() => {\n        console.log('Reductions update complete')\n    })\n}\n\nmodule.exports = { init, insert }\n```\n\nI an performing an SQL search and chaining .then() functions to debug the output.\nBut my problem is non of the console.log calls in the insert function are responding except for the first one.\nI am very certain the required data is in the Reductions table of the database but even if it is not, the console.log in the finally section does not output either.\nHow can I understand and resolve why the console.logs are not printing?\nUpdate\nInsert function call code, as requested.\n```\n.\n.\n.\nproperties.forEach(async(property) => {\n    if( property.reduced !== false ) {\n        await reductions.insert(property)\n    } else {\n        uninvestibles += 1\n    }\n})\n.\n.\n.\n```\n\n    ", "Answer": "\r\nSpecial thanks to Rinkesh P for helping me find the answer to my problem.\nApparently the foreach loop does not wait for promises to resolve. So my await related code wouldn't execute as intended. Not so sure why and curiously my foreach solution had been working.\nBut apparently, foreach loops should not be used with asynchronous code. So I refactored my foreach into the following for loop:\n```\nfor (let index = 0; index < properties.length; index++) {\n    if( properties[index].reduced !== false ) {\n        await reductions.insert(properties[index], location)\n    } else {\n        uninvestibles += 1\n    }\n}\n```\n\nThat got my code working again.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Matrix reduction, custom reduction operators in OpenMP\r\n                \r\nI have to assemble a matrix using two for-loops, based on matrices from different libraries, using the following algorithm (here based on armadillo):\n\n```\ninline void loop_over_matrix_serial(const size_t &size, arma::mat &matrix)\n{\n    for (size_t i = 0; i < size; ++i)\n        for (size_t j = 0; j < size; ++j)\n            matrix(i, j) += position_function(i, j);\n}\n```\n\n\nIn order to speed that operation up I would like to use a reduction with OpenMP. There are already custom reduction operators in other questions, such as here: C++ Armadillo and OpenMp: Parallelization of summation of outer products - define reduction for Armadillo matrix, but it is targeted on full matrices, not on single elements. How could I define a custom operator for all matrices which are accessed in the shown way, but from different libraries, for the reduction? It will always be a double-value which is added from the right side.\n    ", "Answer": "\r\nIf I were to optimize this code, I'll first get to know the size of matrix.\n\nif it's big, I would consider writing a cache-friendly code.\n\nin your case, if your several matrices can't be held in the cache, you also need do that. refer to this link, What is \"cache-friendly\" code? \n\nomp reduction may be less important after cache optimizing.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "XSLT How to call other node by substring of current node name?\r\n                \r\nI have XML document with belowe structure:\n```\n<Data>\n   <FieldBook>\n     <PointRecord>\n       <Name>12_REF1</Name>\n       (...)\n     </PointRecord>\n     <PointRecord>\n       <Name>12_REF2</Name>\n       (...)\n     </PointRecord>\n     <PointRecord>\n       <Name>12</Name>\n       (...)\n     </PointRecord>\n   </FieldBook>\n   <Reductions>\n    <Point>\n        <Name>12_REF1</Name>\n        <MyNecessaryValue>9999</MyNecessaryValue>\n    </Point>\n    <Point>\n        <Name>12_REF2</Name>\n        <MyNecessaryValue>8888</MyNecessaryValue>\n    </Point>\n    <Point>\n        <Name>12</Name>\n        <MyNecessaryValue>1000</MyNecessaryValue>\n    </Point>\n   </Reductions>\n</Data>\n```\n\nMost of my data is located in FieldBook, but i have to also call some of Reductions nodes to get other data, generally looping through records using its IDs.\nBut, I have to also get specific values for calculations: eg. I have to compare  MyNecessaryValue of  Reductions/Point 12_REF1 and 12_REF2 both with Reductions/Point 12, so I have to call the same node for two records in loop.\nAccording to this topic I prepared a key ```\n<xsl:key name=\"red-pointName\" match=\"Reductions/Point\" use=\"Name\"/>```\n and tried to use it this way, with substring:\n```\n<xsl:for-each select=\"/JOBFile/FieldBook/PointRecord\">\n   (...)    \n     <td>\n        <xsl:choose>\n            <xsl:when test=\"Method='GpsContinuousOffsetPointOne'\">\n                <xsl:call-template name=\"my-format\">\n                <xsl:with-param name=\"Val\" select=\"key('red-pointName', substring-before-last(current(Name),'_')')/MyNecessaryValue\"/>\n                    <xsl:with-param name=\"format\" select=\"$DecPl2\"/>\n                </xsl:call-template>\n                </xsl:when>\n            <xsl:otherwise>---</xsl:otherwise>\n        </xsl:choose>\n    </td>\n  (...) \n</xsl:for-each>\n```\n\nBut it not works.\nHow can I call more than one time the same node?\n    ", "Answer": "\r\nIf you want e.g. ```\nkey('red-pointName', '12')```\n to find all ```\nPoint```\n elements where the ```\nName```\n starts with ```\n12```\n before ```\n_```\n then you need to change the key declaration to e.g. ```\n<xsl:key name=\"red-pointName\" match=\"Reductions/Point\" use=\"replace(Name, '_.*$', '')\"/>```\n. and of course make sure anywhere you call the ```\nkey```\n function you then ensure you only pass in the right value, i.e. the string before the ```\n_```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenCL reduction - 2D matrix to 1-D array\r\n                \r\nI have a 2-D array of size MxN, where N is a power of 2 greater than or equal to 16 and M is an arbitrary integer which is not a power of 2. For example the size of array A could be 200x32.\n\nI would like to reduce the array A to size 1x32 by doing a reduce (add) operation across the rows of the array. Most of the reductions I have come across reduce the array to a single value by adding successive elements using a Blelloch/Hillis scan algorithm. In my case though, the successive elements are unrelated and cannot be added. However, I need to add elements [1, 33, 65...] and elements [2,34,66..] and so on.\n\nSince this is not a coalesced access, what would be be the best way to go about solving this problem?\n    ", "Answer": "\r\nIt is coalesced:\n\n```\nWorkItem1 -> 1 + 33 + 65 + ...\nWorkItem2 -> 2 + 34 + 66 + ...\nWorkItem3 -> 3 + 35 + 67 + ...\n...\n```\n\n\nAs you can see all the memory operations in a group of threads are to contiguous memory locations.\n\nAlso for further optimization you can try doing vector operations, but my guess is that the compiler will already optimize it by itself.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Method to do final sum with reduction\r\n                \r\nI take up the continuation of my first issue explained on this link.\n\nI remind you that I would like to apply a method which is able to do multiple sum reductions with OpenCL (my GPU device only supports OpenCL 1.2). I need to compute the sum reduction of an array to check the convergence criterion for each iteration of the main loop, \n\nCurrently, I did a version for only one sum reduction (i.e one iteration\n). In this version, and for simplicity, I have used a sequential CPU loop to compute the sum of each partial sum and get the final value of sum.\n\nFrom your advices in my precedent, my issue is that I don't know how to perform the final sum by calling a second time the ```\nNDRangeKernel```\n function (i.e executing a second time the kernel code). \n\nIndeed, with a second call, I will always face to the same problem for getting the sum of partial sums (itself computed from first call of ```\nNDRangeKernel```\n) : it seems to be a recursive issue.\n\nLet's take an example from the above figure : if input array size is ```\n10240000```\n and ```\nWorkGroup size```\n is ```\n16```\n, we get ```\n10000*2^10/2^4 = 10000*2^6 = 640000 WorkGroups```\n.  \n\nSo after the first call, I get ```\n640000 partial sums```\n : how to deal with the final sumation of all these partial sums ? If I call another time the kernel code with, for example, ```\nWorkGroup size = 16```\n and global ```\nsize = 640000```\n, I will get ```\nnWorkGroups = 640000/16 = 40000 partial sums```\n, so I have to call kernel code one more time and repeat this process till ```\nnWorkGroups < WorkGroup size```\n.\n\nMaybe I didn't understand very well the second stage, mostly this part of kernel code from \"two-stage reduction\" ( on this link, I think this is the case of searching for minimum into input array )\n\n```\n__kernel\nvoid reduce(__global float* buffer,\n            __local float* scratch,\n            __const int length,\n            __global float* result) {\n\n  int global_index = get_global_id(0);\n  float accumulator = INFINITY;\n  // Loop sequentially over chunks of input vector\n  while (global_index < length) {\n    float element = buffer[global_index];\n    accumulator = (accumulator < element) ? accumulator : element;\n    global_index += get_global_size(0);\n  }\n\n  // Perform parallel reduction\n  ...\n```\n\n\nIf someone could explain what this above code snippet of kernel code does.\n\nIs there a relation with the second stage of reduction, i.e the final sumation ?\n\nFeel free to ask me more details if you have not understood my issue.\n\nThanks\n    ", "Answer": "\r\nAs mentioned in the comment: The statement \n\n\n  if input array size is 10240000 and WorkGroup size is 16, we get 10000*2^10/2^4 = 10000*2^6 = 640000 WorkGroups. \n\n\nis not correct. You can choose an \"arbitrary\" work group size, and an \"arbitrary\" number of work groups. The numbers to choose here may be tailored for the target device. For example, the device may have a certain local memory size. This can be queried with ```\nclDeviceGetInfo```\n:\n\n```\ncl_ulong localMemSize = 0;\nclDeviceGetInfo(device, CL_DEVICE_LOCAL_MEM_SIZE, \n    sizeof(cl_ulong), &localMemSize, nullptr);\n```\n\n\nThis may be used to compute the size of a local work group, considering the fact that each work group will require \n\n```\nsizeof(cl_float) * workGroupSize\n```\n\n\nbytes of local memory.\n\nSimilarly, the number of work groups may be derived from other device specific parameters. \n\n\n\nThe key point regarding the reduction itself is that the work group size does not limit the size of the array that can be processed. I also had some difficulties with understanding the algorithm as a whole, so I tried to explain it here, hoping that a few images may be worth a thousand words:\n\n\n\nAs you can see, the number of work groups and the work group size are fixed and independent of the input array length: Even though I'm using 3 work groups with a size of 8 in the example (giving a global size of 24), an array of length 64 can be processed. This is mainly due to the first loop, which just walks through the input array, with a \"step size\" that is equal to the global work size (24 here). The result will be one accumulated value for each of the 24 threads. These are then reduced in parallel.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Numpy: Faster computation of triple nested loops involving sum reductions\r\n                \r\nMy goal is to compute the following nested loop efficiently,\n\n```\nAb = np.random.randn(1000, 100)    \nTb = np.zeros((100, 100, 100))\n\nfor i in range(d):\n    for j in range(d):\n        for k in range(d):\n            Tb[i, j, k] = np.sum(Ab[:, i] * Ab[:, j] * Ab[:, k])\n```\n\n\nI found a faster approach to do the nested loop by looping over the combinations only:\n\n```\nfor i,j,k in itertools.combinations_with_replacement(np.arange(100), 3):\n    Abijk = np.sum(Ab[:, i] * Ab[:, j] * Ab[:, k])\n\n    Tb[i, j, k] = Abijk\n    Tb[i, k, j] = Abijk\n\n    Tb[j, i, k] = Abijk\n    Tb[j, k, i] = Abijk\n\n    Tb[k, j, i] = Abijk\n    Tb[k, i, j] = Abijk\n```\n\n\nIs there a more efficient way of doing this ? \n\nI am hoping for a way that can take advantage of Numpy's Blas, Numba's JIT, or Pytorch GPU implementations.\n    ", "Answer": "\r\nApproach #1\nWe could directly use the iterators as ```\neinsum```\n string notation with NumPy's built-in  ```\nnp.einsum```\n. Thus, the solution would be with a single ```\neinsum```\n call -\n```\nTb = np.einsum('ai,aj,ak->ijk',Ab,Ab,Ab)\n```\n\nApproach #2\nWe could use a combination of ```\nbroadcasted elementwise-multiplication```\n and then ```\nnp.tensordot```\n or ```\nnp.matmul```\n for the all the ```\nsum-reductions```\n.\nHence, get the broadcasted elementwise-multiplications with again ```\neinsum```\n or explicit dimension-extension and ```\nbroadcasting```\n -\n```\nparte1 = np.einsum('ai,aj->aij',Ab,Ab)\nparte1 = (Ab[:,None,:]*Ab[:,:,None]\n```\n\nThen, ```\ntensordot```\n or ```\nnp.matmul```\n -\n```\nTb = np.tensordot(parte1,Ab,axes=((0),(0)))\nTb = np.matmul(parte1.T, Ab) # Or parte1.T @ Ab on Python 3.x\n```\n\nThus, there are a total of four variants possible with this second approach.\nRuntime test\n```\nIn [140]: d = 100\n     ...: m = 1000\n     ...: Ab = np.random.randn(m,d)\n\nIn [148]: %%timeit  # original faster method\n     ...: d = 100\n     ...: Tb = np.zeros((d,d,d))\n     ...: for i,j,k in itertools.combinations_with_replacement(np.arange(100), 3):\n     ...:     Abijk = np.sum(Ab[:, i] * Ab[:, j] * Ab[:, k])\n     ...: \n     ...:     Tb[i, j, k] = Abijk\n     ...:     Tb[i, k, j] = Abijk\n     ...: \n     ...:     Tb[j, i, k] = Abijk\n     ...:     Tb[j, k, i] = Abijk\n     ...: \n     ...:     Tb[k, j, i] = Abijk\n     ...:     Tb[k, i, j] = Abijk\n1 loop, best of 3: 2.08 s per loop\n\nIn [141]: %timeit np.einsum('ai,aj,ak->ijk',Ab,Ab,Ab)\n1 loop, best of 3: 3.08 s per loop\n\nIn [142]: %timeit np.tensordot(np.einsum('ai,aj->aij',Ab,Ab),Ab,axes=((0),(0)))\n     ...: %timeit np.tensordot(Ab[:,None,:]*Ab[:,:,None],Ab,axes=((0),(0)))\n     ...: %timeit np.matmul(np.einsum('ai,aj->ija',Ab,Ab), Ab)\n     ...: %timeit np.matmul(Ab.T[None,:,:]*Ab.T[:,None,:], Ab)\n\n\n10 loops, best of 3:  56.8 ms per loop\n10 loops, best of 3:  59.2 ms per loop\n 1 loop,  best of 3: 673   ms per loop\n 1 loop,  best of 3: 670   ms per loop\n```\n\nFastest ones seem to be the ```\ntensordot```\n based ones. Thus, getting ```\n35x+```\n speedup over the faster one-loopy ```\nitertools```\n based method.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "In Pandas or Numpy how do you multipaly reduce with XOR a row until all reductions are reduced until a final bottowm row\r\n                \r\nIn Pandas or Numpy how do you multiplely reducea row until all reductions are reduced until a final bottowm row\nok i want to reduce my B column in way keeps xoring down each item until the final row: [1 ,8 ,  6 ,   12 , 1 , 2] so i can save it row C\nif tried using an apply loo but this can get very expensive for large datasets. Does anyone have a shortcutor better method thah using a loop to create row after row with\nnot the easist logic to reduce this to the row?\nHere is the data the second number version how i xor each next row to obtain an answer for a new list and keep continuing down to a final result. this is quite slow and\nwas looking for a better solution\n```\n    A   B  C\n0  12   2  0\n1  10   6  0\n2   2   8  0\n3   9  11  0  \n4   5  12  0\n5   0   5  0\n6   4   4  0\n\nfor example column B looks like this with a .T transform:\n\n    0   1  2   3   4  5  6\nA  12  10  2   9   5  0  4\nB   2   6  8  11  12  5  4\n\nso basically is I do these operations:\n2 ^ 6. which is 4\n6 ^ 8, which is 14\n8 ^ 11 which is 3\n11 ^ 12 which is 7\n5 ^ 4 which is 1. which I store in a separate array because I only want the final result. So now I have\n\nso is have [ 4, 14, 3, 7, 1 ]  and a second array with [1]\n4^14 = 10, 14^3=13 3^7=4, 7^1 =1\n\nno I have to keep reducing\n[10, 3, 4, 14, 8]\n10^3 = 9 3^4 = 7, 4^14=8.  and I manually move the last term to the new array which is now [1, 8]\nI do this so forth so my array has all the final results and end up with \n\nthe final result of each iterative reduction: [1  , 8  , 6  ,  12  ,1  ,2] as shown below:\n\n  A   B\n[[12  2]\n [10  6] 4   \n [ 2  8] 14  10\n [ 9 11] 3   3   9  \n [ 5 12] 7   4   7    14 \n [ 0  5] 9   14  10   13  3\n [ 4  4] 1   8   6    12  1  2\n\n\nso that the final out come is the last row in column C\n\n    A   B  C\n0  12   2  1\n1  10   6  8\n2   2   8  6\n3   9  11  12  \n4   5  12  1\n5   0   5  2\n6   4   4  0\n\n```\n\nok i want to reduce my B column in a that way keeps xoring down each item until the final row: [1 ,8 ,  6 ,   12 , 1 , 2] so i can save it row C\nif tried using an apply function and loop but this can get very expensive for large datasets. Does anyone have a shortcutor better method thah using a loop to create row after row with not the easist logic to reduce this to the row?(as I work with very large data sets, this is just an example )\n    ", "Answer": "\r\nThis can be done with one single loop. For performance you can used ```\nnumba```\n maybe.\n```\narr = np.array([2, 6, 8, 11, 12, 5, 4]) # column B\nn = len(arr)\nresults = []\n\nwhile n > 1:\n    results.append(np.bitwise_xor(arr[:n-1], arr[1:]))\n    arr = results[-1]\n    n = arr.size\n\nholder = np.zeros((len(results), len(results)))\nindices = np.triu_indices(len(holder)) # indices of upper triangle of holder\nholder[indices] = np.concatenate(results, axis=0)\nprint(holder[:,-1])\n>> [ 1.  8.  6. 12. 15.  2.]\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "output operand requires a reduction, but reduction is not enabled Python\r\n                \r\n```\nimport numpy as np\nfrom numpy.linalg import solve,norm,cond,inv,pinv\nimport math\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import toeplitz\nfrom numpy.random import rand\n\nc = np.zeros(512)\nc[0] = 2\nc[1] = -1\na = c\nA = toeplitz(c,a)\n\ncond_A = cond(A,2)\n\n# creating 10 random vectors 512 x 1\nb = rand(10,512)\n\n# making b into unit vector\nfor i in range (10):\n    b[i]= b[i]/norm(b[i],2)\n\n# creating 10 random del_b vectors \ndel_b = [rand(10,512), rand(10,512), rand(10,512), rand(10,512), rand(10,512), rand(10,512), rand(10,512), rand(10,512), rand(10,512), rand(10,512)] \n\n# del_b = 10 sets of 10 vectors (512x1) whose norm is 0.01,0.02 ~0.1\nfor i in range(10):\n    for j in range(10):\n        del_b[i][j] = del_b[i][j]/(norm(del_b[i][j],2)/((float(j+1)/100)))\n\nx_in = [np.zeros(512), np.zeros(512), np.zeros(512), np.zeros(512), np.zeros(512), np.zeros(512), np.zeros(512), np.zeros(512), np.zeros(512), np.zeros(512)]\n\nx2 = np.zeros((10,10,512))\nfor i in range(10):\n    x_in[i] =  A.transpose()*b[i]\n\nfor i in range(10):\n    for j in range(10):\n        x2[i][j] = ((A.transpose()*(b[i]+del_b[i][j]))\n```\n\n\nLAST line is giving me the error. ( output operand requires a reduction, but reduction is not enabled)\nHow do i fix it?\nI'm new to python and please let me know if there is easier way to do this \n\nThanks\n    ", "Answer": "\r\nThe error you're seeing is because of a mismatch in the dimensions of what you have created, but your code is also quite inefficient with all the looping and not making use of Numpy's automatic broadcasting optimally.  I've rewritten the code to do what it seems you want:\n\n```\nimport numpy as np\nfrom numpy.linalg import solve,norm,cond,inv,pinv\nimport math\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import toeplitz\nfrom numpy.random import rand\n\n# These should probably get more sensible names\nNvec = 10 # number of vectors in b\nNlevels = 11 # number of perturbation norm levels\nNd = 512 # dimension of the vector space\n\nc = np.zeros(Nd)\nc[0] = 2\nc[1] = -1\na = c\n\n# NOTE: I'm assuming you want A to be a matrix\nA = np.asmatrix(toeplitz(c, a)) \n\ncond_A = cond(A,2)\n\n# create Nvec random vectors Nd x 1\n# Note: packing the vectors in the columns makes the next step easier\nb = rand(Nd, Nvec)\n\n# normalise each column of b to be a unit vector\nb /= norm(b, axis=0)\n\n# create Nlevels of Nd x Nvec random del_b vectors \ndel_b = rand(Nd, Nvec, Nlevels)\n\n# del_b = 10 sets of 10 vectors (512x1) whose norm is 0.01,0.02 ~0.1\ntargetnorms = np.linspace(0.01, 0.1, Nlevels)\n# cause the norms in the Nlevels dimension to be equal to the target norms\ndel_b /= norm(del_b, axis=0)[None, :, :]/targetnorms[None, None, :]\n\n# Straight linear transformation - make sure you actually want the transpose\nx_in = A.T*b\n\n# same linear transformation on perturbed versions of b\nx2 = np.zeros((Nd, Nvec, Nlevels))\nfor i in range(Nlevels):\n    x2[:, :, i] = A.T*(b + del_b[:, :, i])\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "lambda calculus reduction with α reduction\r\n                \r\nI am trying to complete a lambda calculus reduction but I can't continue after a point. I have to reduce the value \"two two\" where \"two = λfx.f (f x)\"\n\nI start writing the following: \n\n```\n(λfx.f (f x) two) = λx.two (two x)\n                  = λa.two(λfx.f(f x) a)\n                  = two(λx.a(a x))\n                  = (λfx.f (f x) (λx.a(a x)))\n```\n\n\nAfter that step I'm starting getting really confused and I'm not sure how to continue. Do I have to apply the second lambda term to the ```\nf```\n variable of the first lambda term? I tried it but I got not a result.\n    ", "Answer": "\r\nLet ```\n2 := λfx.f (f x)```\n, note that\n\n```\n 2 a    = λx.a (a x)     (i)\n(2 a) b = a (a b)        (ii)\n```\n\n\nthus\n\n```\n2 2 \n= λx.2 (2 x)             by (i), a = 2\n= λxy.(2 x) ((2 x) y)    by (i), a = (2 x)\n= λxy.(2 x) (x (x y))    by (ii)\n= λxy.x (x (x (x y)))    by (ii)\n= λfx.f (f (f (f x)))\n=: 4\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to revert beta-reductions to named functions in a lambda calculus-based system?\r\n                \r\nWell, suppose that I have a set of functional definitions (with a syntax tree) in church encoding :\n```\ntrue : λx -> λy -> x\nfalse : λx -> λy -> y\n```\n\nGiving the definition λx -> λy -> y, it is very clear how to return the named definition, applying a matching with alpha-equivalence will be enough.\n```\nα true λx -> λy -> y = false\nα false λx -> λy -> y = true\n```\n\nBut consider the example below:\n```\n0 : λf λz -> x\nsucc : λn λf λx -> f (n f x)\n3 : succ (succ (succ 0)))\n```\n\nSo, when 3 suffers from beta-reduction it will unfold to some definition like :\n```\n3_unfolded : (λf -> (λx -> (f (f (f x))))) : (A -> A) -> A -> A\n```\n\nYou can see the term can get bigger easily, of course, it is not a good way to represent pure data, because of the size of the term. So, I want to know if there is an algorithm able efficiently to rename again every definition after suffering evaluation. Them 3_unfolded will become (succ (succ (succ 0))) again by giving the set of definitions of natural church encoding (0, and succ only).\nI know there are some side effects, like ambiguous representations, but let's ignore that (if you expand the same definition of succ and rename to succ_2, for example).\n    ", "Answer": "\r\nThis is essentially the problem of beta-equivalence, and it’s undecidable in general; it also won’t necessarily produce usable output even when it could produce something, e.g. with some restrictions including strong normalisation. Therefore I think your best strategies here will be heuristic, because by default, reductions destroy information. The solutions are to retain information that you care about, or avoid needing information that’s gone. For instance:\n\nDecouple the memory representation of terms from their LC representations, in particular cases where you care about efficiency and usability. For example, you can store and print a Church numeral as a ```\nNatural```\n, while still allowing it to be converted to a function as needed. I think this is the most practical technical angle.\n\nRetain information about the provenance of each term, and use that as a hint to reconstruct named terms. For example, if you know that a term arose by a given shape of beta-reduction, you can beta-expand/alpha-match to potentially rediscover an application of a function like ```\nsucc```\n. This may help in simple cases but I expect it will fall down in nontrivial programs.\n\nInstead of considering this an algorithmic problem, consider it a usability design problem, and focus on methods of identifying useful information and presenting it clearly. For example, search for the largest matching function body that is also the most specific, e.g. a term might match both ```\nλx. x```\n (identity) and ```\nλf. λx. f x```\n (function application), but the latter is more specific, and even more specifically it can be a numeral (```\nλs. λz. s z```\n = 1); if there are multiple possibilities, present the most likely few.\n\n\nWhenever you encounter a problem that’s undecidable for arbitrary programs, it’s worth remembering that humans write extremely non-arbitrary programs. So heuristic solutions can work remarkably well in practice.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Dimensionality Reduction\r\n                \r\nI am trying to understand the different methods for dimensionality reduction in data analysis. In particular I am interested in Singular Value Decomposition (SVD) and Principle Component Analysis (PCA).\n\nCan anyone please explain there terms to a layperson? - I understand the general premis of dimensionality reduction as bringing data to a lower dimension - But\n\na) how do SVD and PCA do this, and\nb) how do they differ in their approach\n\nOR maybe if you can explain what the results of each technique is telling me, so for\na) SVD - what are singular values\nb) PCA - \"proportion of variance\"\n\nAny example would be brilliant. I am not very good at maths!!\n\nThanks\n    ", "Answer": "\r\nYou probably already figured this out, but I'll post a short description anyway.\n\nFirst, let me describe the two techniques speaking generally.\n\nPCA basically takes a dataset and figures out how to \"transform\" it (i.e. project it into a new space, usually of lower dimension). It essentially gives you a new representation of the same data. This new representation has some useful properties. For instance, each dimension of the new space is associated with the amount of variance it explains, i.e. you can essentially order the variables output by PCA by how important they are in terms of the original representation. Another property is the fact that linear correlation is removed from the PCA representation. \n\nSVD is a way to factorize a matrix. Given a matrix ```\nM```\n (e.g. for data, it could be an ```\nn```\n by ```\nm```\n matrix, for ```\nn```\n datapoints, each of dimension ```\nm```\n), you get ```\nU,S,V = SVD(M)```\n where:```\nM=USV^T```\n, ```\nS```\n is a diagonal matrix, and both ```\nU```\n and ```\nV```\n are orthogonal matrices (meaning the columns & rows are orthonormal; or equivalently ```\nUU^T=I```\n & ```\nVV^T=I```\n).\nThe entries of ```\nS```\n are called the singular values of ```\nM```\n. You can think of SVD as dimensionality reduction for matrices, since you can cut off the lower singular values (i.e. set them to zero), destroying the \"lower parts\" of the matrices upon multiplying them, and get an approximation to ```\nM```\n. In other words, just keep the top ```\nk```\n singular values (and the top ```\nk```\n vectors in ```\nU```\n and ```\nV```\n), and you have a \"dimensionally reduced\" version (representation) of the matrix.\nMathematically, this gives you the best rank ```\nk```\n approximation to ```\nM```\n, essentially like a reduction to ```\nk```\n dimensions. (see this answer for more).\n\n\n\nSo Question 1\n\n\n  I understand the general premis of dimensionality reduction as bringing data to a lower dimension - But\n  a) how do SVD and PCA do this, and b) how do they differ in their approach\n\n\nThe answer is that they are the same.\n\nTo see this, I suggest reading the following posts on the CV and math stack exchange sites:\n\n\nWhat is the intuitive relationship between SVD and PCA?\nRelationship between SVD and PCA. How to use SVD to perform PCA?\nHow to use SVD for dimensionality reduction to reduce the number of columns (features) of the data matrix?\nHow to use SVD for dimensionality reduction (in R)\n\n\nLet me summarize the answer:\nessentially, SVD can be used to compute PCA. \nPCA is closely related to the eigenvectors and eigenvalues of the covariance matrix of the data. Essentially, by taking the data matrix, computing its SVD, and then squaring the singular values (and doing a little scaling), you end up getting the eigendecomposition of the covariance matrix of the data. \n\n\n\nQuestion 2\n\n\n  maybe if you can explain what the results of each technique is telling me, so for a) SVD - what are singular values b) PCA - \"proportion of variance\"\n\n\nThese eigenvectors (the singular vectors of the SVD, or the principal components of the PCA) form the axes of the news space into which one transforms the data.\nThe eigenvalues (closely related to the squares of the data matrix SVD singular values) hold the variance explained by each component. Often, people want to retain say 95% of the variance of the original data, so if they originally had ```\nn```\n-dimensional data, they reduce it to ```\nd```\n-dimensional data that keeps that much of the original variance, by choosing the largest ```\nd```\n-eigenvalues such that 95% of the variance is kept. This keeps as much information as possible, while retaining as few useless dimensions as possible.\n\nIn other words, these values (variance explained) essentially tell us the importance of each principal component (PC), in terms of their usefulness reconstructing the original (high-dimensional) data. Since each PC forms an axis in the new space (constructed via linear combinations of the old axes in the original space), it tells us the relative importance of each of the new dimensions. \n\n\n\nFor bonus, note that SVD can also be used to compute eigendecompositions, so it can also be used to compute PCA in a different way, namely by decomposing the covariance matrix directly. See this post for details.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to revert beta-reductions to named functions in a lambda calculus-based system?\r\n                \r\nWell, suppose that I have a set of functional definitions (with a syntax tree) in church encoding :\n```\ntrue : λx -> λy -> x\nfalse : λx -> λy -> y\n```\n\nGiving the definition λx -> λy -> y, it is very clear how to return the named definition, applying a matching with alpha-equivalence will be enough.\n```\nα true λx -> λy -> y = false\nα false λx -> λy -> y = true\n```\n\nBut consider the example below:\n```\n0 : λf λz -> x\nsucc : λn λf λx -> f (n f x)\n3 : succ (succ (succ 0)))\n```\n\nSo, when 3 suffers from beta-reduction it will unfold to some definition like :\n```\n3_unfolded : (λf -> (λx -> (f (f (f x))))) : (A -> A) -> A -> A\n```\n\nYou can see the term can get bigger easily, of course, it is not a good way to represent pure data, because of the size of the term. So, I want to know if there is an algorithm able efficiently to rename again every definition after suffering evaluation. Them 3_unfolded will become (succ (succ (succ 0))) again by giving the set of definitions of natural church encoding (0, and succ only).\nI know there are some side effects, like ambiguous representations, but let's ignore that (if you expand the same definition of succ and rename to succ_2, for example).\n    ", "Answer": "\r\nThis is essentially the problem of beta-equivalence, and it’s undecidable in general; it also won’t necessarily produce usable output even when it could produce something, e.g. with some restrictions including strong normalisation. Therefore I think your best strategies here will be heuristic, because by default, reductions destroy information. The solutions are to retain information that you care about, or avoid needing information that’s gone. For instance:\n\nDecouple the memory representation of terms from their LC representations, in particular cases where you care about efficiency and usability. For example, you can store and print a Church numeral as a ```\nNatural```\n, while still allowing it to be converted to a function as needed. I think this is the most practical technical angle.\n\nRetain information about the provenance of each term, and use that as a hint to reconstruct named terms. For example, if you know that a term arose by a given shape of beta-reduction, you can beta-expand/alpha-match to potentially rediscover an application of a function like ```\nsucc```\n. This may help in simple cases but I expect it will fall down in nontrivial programs.\n\nInstead of considering this an algorithmic problem, consider it a usability design problem, and focus on methods of identifying useful information and presenting it clearly. For example, search for the largest matching function body that is also the most specific, e.g. a term might match both ```\nλx. x```\n (identity) and ```\nλf. λx. f x```\n (function application), but the latter is more specific, and even more specifically it can be a numeral (```\nλs. λz. s z```\n = 1); if there are multiple possibilities, present the most likely few.\n\n\nWhenever you encounter a problem that’s undecidable for arbitrary programs, it’s worth remembering that humans write extremely non-arbitrary programs. So heuristic solutions can work remarkably well in practice.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Polynomial reduction of Hamiltonian path to cycle and cycle to path\r\n                \r\nI need to show that given undirected graph G, Hamiltonian path and Hamiltonian cycle are polynomial time reducible to each other. Here is my reduction, is this correct?\n\nFor Cycle to Path\nFor a vertex v belonging to V, add a vertex v’ and for all e(v,u) add edge e(v’ ,u). Now if there is a Hamiltonian Path from v to v’, then there is a Hamiltonian cycle for v.\n\nFor Path to cycle\nFor a vertices s and t, for all edges e(t,u) add an edge e(s,u) (if this edge did not existed) and for all edges e(s,u) add an endge (t,u) (if this edge did not existed). Finally add an edge e(s,t). Now if there is a Hamiltonian cycle for s or t, then there is a Hamiltonian path from s to t.\n\nAre there reductions correct ? also is this enough to show that these two problems are polynomial time reducible to each other ?\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Yacc parse won't finish reduction of a production\r\n                \r\nI am working on building a small interpreter through lax and yacc that works on a basic programming language that performs addition and multiplication as well as printing lists of ints.\nFor example the instruction:\n```\nPrint(2,3,4);\n```\n\nShould output: 2 3 4\nand the instruction:\n```\nPrint(+(2,3));\n```\n\nshould output: 5\nThe first print instruction works perfectly fine. However any addition instruction (+ followed by a list) works itself out and the action returns the correct answer (found out through printf) but it seems yacc stops before executing the outer print instruction occurs after the addition instruction.\nHere is my .l file:\n```\n%{\n#include \"y.tab.h\"\n%}\n\ndigit [0-9]\n\n%%\n{digit}{digit}*     {yylval.str = strdup(yytext); return IntLit;}\nPrint               {return Print;}\n\\+                  {yylval.str = strdup(yytext); return '+';}\n\\*                  {yylval.str = strdup(yytext); return '*';}\n\\(                  {return '(';}\n\\)                  {return ')';}\n\\,                  {return ',';}\n\\;                  {return ';';}\n\\t                  {}\n\\r                  {}\n\\n                  {}\n\n%%\n\n    \nint yywrap () {\n    return 1;\n}\n```\n\nAnd here is my .y file:\n```\n%{\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n\nextern int yylex();\nextern int yyparse();\nextern int yyerror(char *s);\nextern char *yytext;\n\nvoid doPrint(char *s);\nint evaluate(char *c, char *s);\nchar* append(char *s, char *s2);\nchar* makeSingle(char *s);\n\n%}\n\n%union {\n  char character;\n  char *str;\n}\n\n%type <str> Item\n%type <str> IntLit\n%type <str> List\n%type <str> Func\n\n%token Print\n%token IntLit\n\n%%\n\nProg        :   StmtSeq                 { };\nStmtSeq     :   Stmt StmtSeq            { };\nStmtSeq     :                           { };\nStmt        : Print '(' List ')' ';'    { doPrint($3); };   \nList        : List ',' Item             { $$ = append($1, $3); };\nList        : Item                      { $$ = makeSingle($1); };\nItem        : Func '(' List ')'         { $$ = evaluate($1, $3); };\nItem        : IntLit                    { $$ = $1;};\nFunc        : '+'                       {$$ = yylval.str; };\nFunc        : '*'                       {$$ = yylval.str; };\n\n%%\n\nint main(int argc, char *argv[]){\n    //yydebug = 1;\n    return yyparse();\n}\n\nvoid doPrint(char *s){\n    char * token = strtok(s, \",\");\n    while(token != NULL){\n        printf(\"%s\", token);\n        printf(\" \");\n        token = strtok(NULL, \",\");\n    }\n    printf(\"\\n\");\n}\n\nint evaluate(char *c, char *s){\n    char * result;\n    int res;\n    int x;\n    char * token;\n    int cmp = strcmp(c, \"+\");\n    if(cmp == 0){\n        token = strtok(s, \",\");\n        res = 0;\n        while(token != NULL){\n            x = atoi(token);\n            res = res + x;\n            token = strtok(NULL, \",\");\n        }\n        sprintf(result,\"%d\",res);\n    } else {\n        token = strtok(s, \",\");\n        res = 1;\n        while(token != NULL){\n            x = atoi(token);\n            res = res * x;\n            token = strtok(NULL, \",\");\n        }\n        sprintf(result,\"%d\",res);\n    }\n    printf(\"resultstring is: '%s'\\n\", result);\n    return result;\n}\n\nchar* append(char *s, char *s2){\n    char * result = s;\n    strcat(result, \",\");\n    strcat(result, s2);\n    return result;\n}\n\nchar* makeSingle(char *s){\n    //char * result;\n    //sprintf(result, \"%d\", c);\n    //return result;\n    return s;\n}\n\nextern int yyerror(char *s)  {\n  printf(s);\n  return 1;\n}\n```\n\nIf I input the instructions:\n```\nPrint(2,3,4);\nPrint(+(2,3));\n```\n\nThe first print instruction works as expected, but the second instruction stops before printing the result of the addition, but after the evaluation is done.\nI'm new to yacc/lex and I'm not sure why yacc is stopping without printing the result of the addition. Shouldn't the result of the addition be an \"Item\" which can be rewritten as a \"List\" and then printed properly? Any help or recommendations would be greatly appreciated, thanks!\nEdit\nAfter looking more into the output when ```\nyydebug = 1;```\n I found that the parse abruptly ends (I believe) before a full reduce is done. The last section of the debug process reads:\n```\nReducing stack by rule 7 (line 40):\n    $1 = nterm Func ()\n    $2 = token '(' ()\n    $3 = nterm List ()\n    $4 = token ')' ()\n```\n\nThere is no ```\n$$```\n in this reduction as there is in all the other reductions done before\n    ", "Answer": "\r\nWhat do you see when you do: ```\ndoPrint(1);```\n ?\nYou have a line in doPrint: ```\nchar * token = strtok(s, \",\");```\n\nWhen you go through the evaluate function you consume all of the commas, leaving a single number. When you call doPrint, there is no comma in the string so it returns NULL.\nYou could have a second if, for when it returns NULL (but the string has contents), or you can rework how you are parsing the string.\nBy the way, strdup() returns a malloced string which you don't free.\nYou also call strcat() using the strings received from strdup, you're writing to memory and you don't know the size. strcat() will let you overflow buffers.\nA better way, when you are appending a string to another and you don't know their sizes is to do something like this:\n```\nchar *mergestrings(char *str1,char *str2) {\n  char *newstr=(char*)malloc(sizeof(char)*(strlen(str1)+strlen(str2)+1));\n  strcpy(newstr,str1);\n  strcat(newstr,str2);\n  free(str1);\n  free(str2);\n  return newstr;\n}\n```\n\nor use realloc or something else, but you should pay attention to what you do with memory.\nThe big problems were the definition of the evaluate function and usage of pointers.\nscanner.l:\n```\n%{\n#include \"../obj/y.tab.h\"\n%}\n\ndigit 0|(([1-9])[0-9]*)\noperator [+*]\n\n%%\n{digit}             {yylval.str = strdup(yytext); return IntLit;}\n{operator}          {yylval.character=yytext[0]; return Operator;}\n\"Print\"             {return Print;}\n\\(                  {return LParen;}\n\\)                  {return RParen;}\n\\,                  {return Comma;}\n\\;                  {return SemiCln;}\n\\t                  {}\n\\r                  {}\n\\n                  {}\n\n%%\n```\n\nparser.y:\n```\n%{\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n\nextern int yylex();\nextern int yyparse();\nextern int yyerror(char *s);\nextern char *yytext;\n\nvoid doPrint(char *s);\nchar* evaluate(char c, char *s);\nchar* append(char *s, char *s2);\n\n%}\n\n%union {\n  char character;\n  char *str;\n}\n\n%type <str> Item List\n%type <character> Func\n\n%token Print Operator IntLit LParen RParen SemiCln Comma\n\n%%\n\nProg       : Stmt                                { }\n           | Prog Stmt                           { }\n           ;\nStmt       : Print LParen List RParen SemiCln    { doPrint($3); }\n           ;\nList       : List Comma Item                     { $$ = append($1, $3); }\n           | Item                                { $$ = $1; }\n           ;\nItem       : Func LParen List RParen             { $$ = evaluate($1, $3); }\n           | IntLit                              { $$ = yylval.str;}\n           ;\nFunc       : Operator                            { $$ = yylval.character; }\n           ;\n\n%%\n\nint main(int argc, char *argv[]){\n    return yyparse();\n}\n\nvoid doPrint(char *s){\n    char *stop = strchr(s, ','), *start=s;\n    while(stop){\n        *stop='\\0';\n        printf(\"%s \", start);\n        start = stop+1;\n        stop = strchr(start, ',');\n    }\n    printf(\"%s\\n\",start);\n    free(s);\n}\n\nchar *evaluate(char c, char *s){\n    int res=0;\n    if(c == '+'){\n        char *stop = strchr(s, ','), *start=s;\n        while(stop){\n            *stop='\\0';\n            res += atoi(start);\n            start = stop+1;\n            stop = strchr(start, ',');\n        }\n        res += atoi(start);\n    } else {\n        char *stop = strchr(s, ','), *start=s;\n        res = 1;\n        while(stop){\n            *stop = '\\0';\n            res *= atoi(start);\n            start = stop+1;\n            stop = strchr(start, ',');\n        }\n        res *= atoi(start);\n    }\n    free(s);\n    char *result = (char*)malloc(12); // big enough for a 32 bit integer\n    sprintf(result,\"%d\",res);\n    return result;\n}\n\nchar* append(char *s, char *s2){\n    char *result = (char*)realloc(s,strlen(s)+strlen(s2)+2);\n    if (result) {\n      strcat(result, \",\");\n      strcat(result, s2);\n    }\n    else {\n        result=(char*)malloc(strlen(s)+strlen(s2)+2);\n        sprintf(result,\"%s,%s\",s,s2);\n        free(s);\n    }\n    free(s2);\n    return result;\n}\n\nextern int yyerror(char *s)  {\n  printf(s);\n  return 1;\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How can I do segmented reduction using CUDA thrust?\r\n                \r\nI want to store partial reduction results in an array.\nSay I have ```\ndata[8] = {10,20,30,40,50,60,70,80}```\n.\nAnd if I divide the ```\ndata```\n with the ```\nchunk_size```\n of ```\n2```\n, the chunks will be ```\n{10,20}```\n, ```\n{30,40}```\n, ... , ```\n{70,80}```\n.\nIf I target the summation, the reduction in total will be ```\n360```\n but I want to get an array of ```\npartial_sums = {30,70,110,150}```\n which is storing the partial sum of each block.\nSo far, what I have in mind is to construct an iterator ```\nstrided_iterator```\n, that will access 0, 2, ... th index of ```\ndata[8] = {10,20,30,40,50,60,70,80}```\n and something like\n```\nthrust::reduce(stride_iterator, stride_iterator + 2,\n               partial_sums.begin(),\n               thrust::plus<int>());\n```\n\ngiving the desired result, but have no idea how could this be done efficiently.\nFor strided access, ```\nthrust/examples/strided_range.cu```\n has a solution but this seems to be not applicable to store segmented reductions.\nOf course I can brutally do it with a loop like this,\n```\nfor (int i = 0; i<4; i++) {\n  partial_sums[i] = thrust::reduce(data+2*i, data+2*i+2, 0, thrust::plus<int>());\n}\n```\n\nBut this kind of practice is what CUDA thrust is trying to avoid as much as possible, right? Somehow I should be able to put it all in a single Thrust call.\n    ", "Answer": "\r\nBased on the useful answer in\nReduce multiple blocks of equal length that are arranged in a big vector Using CUDA, what I come up with so far is like follows.\nIn fact I wanted to get min or max values in each chunk.\n```\nusing namespace thrust::placeholders;\nint main(int argc, char **argv) {\n\n  int N = atoi(argv[1]);\n  int K = atoi(argv[2]);\n\n  std::cout << \"N \" << N << \" K \" << K << std::endl;\n\n  typedef int mytype;\n\n  thrust::device_vector<mytype> data(N*K);\n  thrust::device_vector<mytype> sums(N);\n\n  thrust::sequence(data.begin(),data.end());\n\n  // method 1\n  thrust::reduce_by_key(thrust::device,\n                        \n  thrust::make_transform_iterator(thrust::counting_iterator<int>(0),  _1/K),\n                        \n  thrust::make_transform_iterator(thrust::counting_iterator<int>(N*K),_1/K),\n                        data.begin(),\n                        thrust::discard_iterator<int>(),\n                        sums.begin(),\n                        thrust::equal_to<int>(),\n                        thrust::minimum<mytype>());\n\n  // method 2 (bad)\n  for (int i=0; i<N; i++) {\n    int res = thrust::reduce(data.begin()+K*i, data.begin()+K*i+K,std::numeric_limits<mytype>::max(),thrust::minimum<mytype>());\n  }\n\n  // just print the first 10 results\n  thrust::copy_n(sums.begin(),10,std::ostream_iterator<mytype>(std::cout, \",\"));\n  std::cout << std::endl;\n\n  return 0;\n}\n```\n\nThe results are shown below with the estimated runtime measured via sdkTimer. As can be seen, method 1 with ```\nreduce_by_key```\n is much~ faster than the second one with for loop.\n```\n[sangjun@newmaster01 05_thrust]$ ./exe 1000 100\nN 1000 K 100\n - Elapsed time: 0.00008 sec \n - Elapsed time: 0.02266 sec \n0,100,200,300,400,500,600,700,800,900,\n[sangjun@newmaster01 05_thrust]$ ./exe 1000 256\nN 1000 K 256\n - Elapsed time: 0.00008 sec \n - Elapsed time: 0.02084 sec \n0,256,512,768,1024,1280,1536,1792,2048,2304,\n[sangjun@newmaster01 05_thrust]$ ./exe 100000 100\nN 100000 K 100\n - Elapsed time: 0.00016 sec \n - Elapsed time: 1.98978 sec \n0,100,200,300,400,500,600,700,800,900,\n[sangjun@newmaster01 05_thrust]$ ./exe 100000 256\nN 100000 K 256\n - Elapsed time: 0.00027 sec \n - Elapsed time: 1.92896 sec \n0,256,512,768,1024,1280,1536,1792,2048,2304,\n\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Index an array with a ragged indexing list and perform sum/mean reductions\r\n                \r\nI have some 2D data where the first axis is time and the second axis is person's ID. Thus the data entries are the persons' property values over time.\nWhat I want to do is to group the persons and average the properties in each group at all time frames. Here is a sample of 6 time points and 5 persons with 2 group\n```\nimport numpy as np\n\ndata = np.arange(30)\ndata.shape = 6, 5\ngroups = [[0, 1, 4], [2, 3]]\nresult = np.empty((6, 2))\n\nfor i, indices in enumerate(groups):\n    result[:, i] = data[:, indices].mean(axis=1)\n```\n\nAnd the ```\nresult```\n is\n```\narray([[ 1.66666667,  2.5       ],\n       [ 6.66666667,  7.5       ],\n       [11.66666667, 12.5       ],\n       [16.66666667, 17.5       ],\n       [21.66666667, 22.5       ],\n       [26.66666667, 27.5       ]])\n```\n\nIs this the best we can do in terms of efficiency? I was wondering if that looping over the ```\ngroups```\n could also be eliminated.\n    ", "Answer": "\r\nApproach #1 : Generic case\nHere's an almost vectorized approach making use of ```\nnp.add.reduceat```\n -\n```\ng = np.concatenate(groups)\nlens = list(map(len, groups))\ncuts = np.r_[0,np.cumsum(lens)[:-1]]\nout = np.add.reduceat(data[:, g], cuts, axis=1)/lens\n```\n\nApproach #2 : Specific case\nIf ```\ngroups```\n is a regular ```\n2D```\n array/list, we can simply do -\n```\ndata[:, groups].mean(2)\n```\n\nApproach #3 : Approach 1 + 2\nMixing approaches 1 and 2  we can come up another generic case method -\n```\nfrom itertools import zip_longest\n\nidx = np.vstack(list(zip_longest(*groups, fillvalue=-1)))\nc = (idx == -1).sum(0)\nsums = data[:, idx].sum(1) - c*(data[:,-1,None])\nlens = list(map(len, groups))\nout = sums/lens\n```\n\nApproach #4 : With matrix-multiplication\nWe will create a mask that when matrix-multiplied with ```\ndata```\n, with its sum-reduction will give us the sum-reduced version and then just divide by the lengths to get our desired average values -\n```\nmask = np.zeros((data.shape[1], len(groups)), dtype=bool)\nfor i, indices in enumerate(groups):\n    mask[indices,i] = 1\nout = data.dot(mask)/list(map(len, groups))\n```\n\nAlso, we might want to use ```\nfloat32```\n for faster matrix-multiplication -\n```\ndata.dot(mask.astype(np.float32))\n```\n\nApproach #5 : Approach 2 + 3\nWe will pad with zeros as an additional column, and with a regular indexing array created off ```\nzip_longest```\n, index and sum and hence get the mean values -\n```\ndata0 = np.pad(data,((0,0),(0,1)))\nidx = np.vstack(list(zip_longest(*groups, fillvalue=-1)))\nout = data0[:, idx].sum(1)/list(map(len, groups))\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lattice Reduction\r\n                \r\nI have two matrices A and B with same number of rows. Consider a Lattice generated by the rows of B. I want to reduce B and during the reduction change A accordingly. That is if i-th row and j-th row of B interchanges, need to sweep i-th row and j-th row of A also, similarly other elementary row operations. How can I do these?\n\nAlso is there very simple C or C++-implementation of the LLL algorithm?\n    ", "Answer": "\r\nThis is the source code to sage, a FOSS symbolic math program. It has an implementation of the triple-L that you could use provided you're willing to GPL the code once it's done. This is another standalone implementation.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP slower reduction\r\n                \r\nThere are two versions of openmp codes with reduction and without.\n\n// with reduction\n\n```\n#pragma omp parallel for reduction(+:sum)\n  for (i=1;i<= num_steps; i++){\n      x = (i-0.5)*step;\n      sum = sum + 4.0/(1.0+x*x);\n  }\n```\n\n\n// without reduction\n\n```\n#pragma omp parallel private(i)\n{\n  int id = omp_get_thread_num();\n  int numthreads = omp_get_num_threads();\n  double x;\n\n  double partial_sum = 0;\n\n  for (i=id;i< num_steps; i+=numthreads){\n      x = (i+0.5)*step;\n      partial_sum += + 4.0/(1.0+x*x);\n  }\n#pragma omp critical\n      sum += partial_sum;\n}\n```\n\n\nI run the codes using 8 cores, the total time double for the reduction version.  What's the reason?  Thanks.\n    ", "Answer": "\r\nScalar reduction in OpenMP is usually quite fast. The observed behaviour in your case is due to two things made wrong in two different ways.\n\nIn your first code you did not make ```\nx```\n private. Therefore it is shared among the threads and besides getting incorrect results, the execution suffers from the data sharing. Whenever one thread writes to ```\nx```\n, the core that it executes on sends a message to all other cores and makes them invalidate their copies of that cache line. When any of them writes to ```\nx```\n later, the whole cache line has to be reloaded and then the cache lines in all other cores get invalidated. And so forth. This slows things down significantly.\n\nIn your second code you have used the OpenMP ```\ncritical```\n construct. This is a relatively heavy-weight in comparison with the atomic adds, usually used to implement the reduction at the end. Atomic adds on x86 are performed using the ```\nLOCK```\n instruction prefix and everything gets implemented in the hardware. On the other side, critical sections are implemented using mutexes and require several instructions and often busy waiting loops. This is far less efficient than the atomic adds.\n\nIn the end, your first code is slowed down due to bad data sharing condition. Your second code is slowed down due to the use of incorrect synchronisation primitive. It just happens that on your particular system the latter effect is less severe than the former and hence your second example runs faster.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Difference between beta reduction and single step beta reduction?\r\n                \r\nI went through numerous online sources on lambda calculus searching for the difference between beta reduction and single step beta reduction. But all that I know till now is that beta reduction is defined as :\n\n```\n(/x.L)M --> {M/x}L\n```\n\n\nand following definition of 1 step beta reduction:\n\n\nCan someone please clear the difference between  these two things with some example.They seem to be equivalent to me. Also then there is n step beta reduction which I understood it as being inductively applied 1 step beta reduction. But as the difference between beta reduction and single step beta reduction is not clear, I feel helpless. Thanks in advance.\n    ", "Answer": "\r\nI would think that beta reduction can designate both single and multi step beta reductions. \n\nI can say that beta reduction can yield   ```\n/z.a```\n  from ```\n(/x./y./z.x) a b```\n, but I cannot say that a single step beta reduction can do that. \n\nThe rest of what you said is correct.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction in OpenACC\r\n                \r\nHere is a Fortran subroutine for matrix-vector multiply.  It is probably old-fashioned and inefficient in a number of ways, but right now I am just trying to get it to work with OpenACC directives, and I'm trying to figure out how reduction works:\n\n```\nsubroutine matrmult(matrix,invec,outvec,n)\n\ninteger:: n\nreal*8, intent(in):: matrix(n,n), invec(n)\nreal*8, intent(out) :: outvec(n)\nreal*8 :: tmpmat(n,n)\nreal*8 :: tmpscl\n\ninteger :: i,j,k\n\n!$acc declare create(matrix, invec, outvec, tmpmat)\n\noutvec = 0.d0\n\n!$acc update device(matrix, invec, tmpmat, outvec)\n\n!$acc parallel\n\n!$acc loop gang\ndo j=1,n\n!$acc loop vector\n  do i=1,n\n    tmpmat(i,j) = matrix(i,j)*invec(j)\n  enddo\nenddo\n\n!$acc loop vector reduction(+:tmpsclr)\ndo j=1,n\n  tmpsclr = 0.d0\n  do i=1,n\n    tmpsclr = tmpsclr+tmpmat(j,i)\n  enddo\n  outvec(j) = tmpsclr\nenddo\n\n!$acc end parallel\n\n!$acc update host(outvec)\n\nend subroutine\n```\n\n\nThis code actually gives correct results.  But when I try a gang/vector combination on the last loops, like so:\n\n```\n!$acc loop gang reduction(+:tmpsclr)\ndo j=1,n\n  tmpsclr = 0.d0\n!$acc loop vector\n  do i=1,n\n    tmpsclr = tmpsclr+tmpmat(j,i)\n  enddo\n  outvec(j) = tmpsclr\nenddo\n```\n\n\nthe results come back all wrong.  It looks like the summation is incomplete for most, but not all, of the elements of ```\noutvec```\n.  This is the case no matter where I put the ```\nreduction```\n clause, whether with the gang or the vector.  Changing the location changes the results, but never gives correct results.\n\nThe results I am getting in a simple test are like the following.  ```\nmatrix```\n is 10x10 and all 1's, and ```\ninvec```\n is 1,2,3,...10.  So the elements of ```\noutvec```\n should each just be the sum of the elements in ```\ninvec```\n, 55.  If I run the gang/vector version of the code, each element of ```\noutvec```\n is 1, not 55.  If I put the reduction with the vector, well, then I get the right answer, 55.  And this continues to work until I get past 90 elements.  When I get to 91, every element of ```\noutvec```\n should be equal to 4186.  But only the last one is, and all the rest are equal to 4095 (the sum of 1 to 90).  As the number of elements get bigger the variation of values and the discrepancy from the correct answer gets worse.\n\nI clearly don't understand how the reduction works.  Can anyone explain?\n    ", "Answer": "\r\nThe reduction clause needs to be on loop where the reduction occurs, i.e. the vector loop.  I'd also recommend using the \"kernels\" directive here since \"parallel\" will create one kernel launch for the two loops, while \"kernels\" will create two kernels, one for each loop.\n\nFor example:\n\n```\nsubroutine foo(n,matrix,invec,outvec)\ninteger n\nreal*8, intent(in) :: matrix(n,n)\nreal*8, intent(in) :: invec(n)\nreal*8, intent(out) :: outvec(n)\nreal*8 :: tmpmat(n,n)\nreal*8 :: tmpscl\n\ninteger :: i,j,k\n\n!$acc declare create(matrix, invec, outvec, tmpmat)\n\noutvec = 0.d0\n\n!$acc update device(matrix, invec, tmpmat, outvec)\n\n!$acc kernels\n\n!$acc loop gang\ndo j=1,n\n!$acc loop vector\n  do i=1,n\n    tmpmat(i,j) = matrix(i,j)*invec(j)\n  enddo\nenddo\n\n!$acc loop gang\ndo j=1,n\n  tmpsclr = 0.d0\n!$acc loop vector reduction(+:tmpsclr)\n  do i=1,n\n    tmpsclr = tmpsclr+tmpmat(j,i)\n  enddo\n  outvec(j) = tmpsclr\nenddo\n\n!$acc end kernels\n\n!$acc update host(outvec)\n\nend subroutine foo\n% pgf90 -c -acc -Minfo=accel test2.f90\nfoo:\n     11, Generating create(matrix(:,:),invec(:),outvec(:))\n     15, Generating update device(outvec(:),tmpmat(:,:),invec(:),matrix(:,:))\n     20, Loop is parallelizable\n     22, Loop is parallelizable\n         Accelerator kernel generated\n         Generating Tesla code\n         20, !$acc loop gang, vector(4) ! blockidx%y threadidx%y\n         22, !$acc loop gang, vector(32) ! blockidx%x threadidx%x\n     28, Loop is parallelizable\n         Accelerator kernel generated\n         Generating Tesla code\n         28, !$acc loop gang ! blockidx%x\n         31, !$acc loop vector(128) ! threadidx%x\n             Sum reduction generated for tmpsclr\n     31, Loop is parallelizable\n     39, Generating update host(outvec(:))\n```\n\n\nHope this helps,\nMat\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to generate a curved price reduction rather than stepped\r\n                \r\nI have a spreadsheet that calculates a price based on the number of units purchased.  The more units purchased; the cheaper the price-per-unit.\n\nThere is an issue in my approach however whereby:\n\n\n  Total price for 100 units = £2,500.00 \n  \n  Total price for 101 units = £2,020.00\n\n\nThe price has reduced as the item quantity increases across the pricing step.  I've tried more pricing steps with lower reductions, but the problem still exists but on a smaller scale.\n\n\n\nThis clearly isn't funding my early retirement plans.  So I was wondering if anyone can recommend a formula for a curved pricing structure, rather than stepped.  I need the price-per-unit to fall as the number of units increases, but not so as for the total price to ever drop as the units rise.\n\nThe spreadsheet is here.\n    ", "Answer": "\r\nYou can convert price into a formula that varies it inversely with quantity.\n\nI have used the following formula in sample_data_set:\nprice = 1/quantity - 1/(quantity ^ 2)\n\nUsing this function, the total value converges to 1.\n\nFinally, you can multiply the entire formula with a constant value, c, to increase the value > 1.\n\nSo, if the desired price for 2 units is $500, set c = $1000.\n\nEDIT:\nIF you would like a smoother (more gentle) gradient, you can add more exponential functions, as follows:\nprice = constant * [ (1/qty) - (1/qty^2) - (1/qty^3) ]\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda Calculus reduction\r\n                \r\nAll,\nBelow is the lambda expression which I am finding difficult to reduce i.e. I am not able to understand how to go about this problem.\n```\n(λm λn λa λb . m (n a b) b) (λ f x. x) (λ f x. f x)\n```\n\nThis is what I tried, but I am stuck:\nConsidering the above expression as : ```\n(λm. E) M```\n equates to\n```\nE = (λn λa λb. m (n a b) b)  \nM = (λf x. x) (λ f x. f x)  \n\n => (λn λa λb. (λ f x. x) (λ f x. f x) (n a b) b)  \n```\n\nConsidering the above expression as ```\n(λn. E) M```\n equates to\n```\nE = (λa λb. (λ f x. x) (λ f x. f x) (n a b) b)  \nM = ??  \n```\n\n.. and I am lost!!\nCan anyone please help me understand that, for ANY lambda calculus expression, what should be the steps to perform reduction?\n    ", "Answer": "\r\nYou can follow the following steps to reduce lambda expressions:\n\n\nFully parenthesize the expression to avoid mistakes and make it more obvious where function application takes place.\nFind a function application, i.e. find an occurrence of the pattern ```\n(λX. e1) e2```\n where ```\nX```\n can be any valid identifier and ```\ne1```\n and ```\ne2```\n can be any valid expressions.\nApply the function by replacing ```\n(λx. e1) e2```\n with ```\ne1'```\n where ```\ne1'```\n is the result of replacing each free occurrence of ```\nx```\n in ```\ne1```\n with ```\ne2```\n.\nRepeat 2 and 3 until the pattern no longer occurs. Note that this can lead to an infinite loop for non-normalizing expressions, so you should stop after 1000 iterations or so ;-)\n\n\nSo for your example we start with the expression\n\n```\n((λm. (λn. (λa. (λb. (m ((n a) b)) b)))) (λf. (λx. x))) (λf. (λx. (f x)))\n```\n\n\nHere the subexpression ```\n(λm. (λn. (λa. (λb. (m ((n a) b)) b)))) (λf. (λx. x))```\n fits our pattern with ```\nX = m```\n, ```\ne1 = (λn. (λa. (λb. (m ((n a) b)) b))))```\n and ```\ne2 = (λf. (λx. x))```\n. So after substitution we get ```\n(λn. (λa. (λb. ((λf. (λx. x)) ((n a) b)) b)))```\n, which makes our whole expression:\n\n```\n(λn. (λa. (λb. ((λf. (λx. x)) ((n a) b)) b))) (λf. (λx. (f x)))\n```\n\n\nNow we can apply the pattern to the whole expression with ```\nX = n```\n, ```\ne1 = (λa. (λb. ((λf. (λx. x)) ((n a) b)) b))```\n and ```\ne2 = (λf. (λx. (f x)))```\n. So after substituting we get:\n\n```\n(λa. (λb. ((λf. (λx. x)) (((λf. (λx. (f x))) a) b)) b))\n```\n\n\nNow ```\n((λf. (λx. (f x))) a)```\n fits our pattern and becomes ```\n(λx. (a x))```\n, which leads to:\n\n```\n(λa. (λb. ((λf. (λx. x)) ((λx. (a x)) b)) b))\n```\n\n\nThis time we can apply the pattern to ```\n((λx. (a x)) b)```\n, which reduces to ```\n(a b)```\n, leading to:\n\n```\n(λa. (λb. ((λf. (λx. x)) (a b)) b))\n```\n\n\nNow apply the pattern to ```\n((λf. (λx. x)) (a b))```\n, which reduces to ```\n(λx. x)```\n and get:\n\n```\n(λa. (λb. b))\n```\n\n\nNow we're done.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Why can't I use reduction with default(shared)?\r\n                \r\nHere's the outline of my code:\n\n```\n#pragma omp parallel default(shared)\n{\nfor(i; i<lim; i++)\n    do_work();\n}\n```\n\n\nAnd a while later:\n\n```\ndo_work(){\n    foo();\n    bar();\n}\n\nfoo(){\n    #pragma omp for //etc\n    for(i;i<l;i++) //your typical loop\n}\n\nbar(){ //here's the interesting part\n    int i;\n    int result;\n\n    #pragma omp for reduction(+:result) private(i)\n    for(i=0; i<lim; i++)\n        result++;\n}\n```\n\n\nWhen compiled I get the following error:\n\n\n  reduction variable ‘result’ is private in outer context\n\n\nwhich shouldn't be happening because according to the IBM compiler documentation the ```\nreduction```\n clause\n\n\n  Performs a reduction on all scalar variables in list using the specified operator. Reduction variables in list are separated by commas.\n  \n  A private copy of each variable in list is created for each thread. At the end of the statement block, the final values of all private copies of the reduction variable are combined in a manner appropriate to the operator, and the result is placed back into the original value of the shared reduction variable.\n  \n  \n    Variables specified in the reduction clause:\n  \n  \n  \n  must be of a type appropriate to the operator.\n  must be shared in the enclosing context.\n  must not be const-qualified.\n  must not have pointer type.\n  \n\n\nEmphasis added. Since the outer parallel region is supposed to handle all variables as ```\nshared```\n, which means that ```\nresult```\n should get converted to ```\nprivate```\n once a reduction on it is found. At the very least though it shouldn't be considered ```\nprivate```\n in the outer scope, because the outer scope has explicitly told every variable to be ```\nshared```\n. That's the source of my confusion.\n\nGranted I'm using GCC instead of the IBM compiler but is there a difference in this case?\n\nSo I guess my question is: Why does OpenMP treat a reduction var. as a private variable when it's previously declared as shared in the outer context?\n\nI only have this problem with reductions, everything else works as it's supposed to (especially when there's an example which does the exact same thing).\n    ", "Answer": "\r\n```\nbar()```\n is being called by each OpenMP thread independently and there is one ```\nresult```\n variable in the stack of each thread, therefore there are as many ```\nresult```\n variables as is the number of threads. No thread knows where the others store their ```\nresult```\ns and it is not possible (and makes no sense) to reduce them together. That is why automatic variables in functions called from inside a parallel region are predetermined to be ```\nprivate```\n and this cannot be changed since threads do not share stack space. The same applies for variables declared inside the parallel region - they do not exist before the region and exist only on the stacks of each executing thread. ```\nreduction```\n only works on shared variables that exist in the other scope, i.e. are either declared before the parallel region, or are ```\nstatic```\n, or are global.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "User defined reduction when target cannot be used in intermediate steps of reduction\r\n                \r\nIn the following code case, I am trying to add many ```\nstd::array```\n to an ```\nstd::span```\n with OpenMP.\nThe problem is that ```\nstd::span```\n cannot be used as an intermediate type in a tree reduction of addition operands. So, for that reason, I choose ```\nstd::array```\n.\nMy problem is in initialization of reduction declaration. Althrough the user-defined reduction is well documented, I did not found many examples of such case of reduction.\nAny help?\n```\n#include <iostream>\n#include <limits>\n#include <array>\n#include <vector>\n#include <span>\n#include <chrono>\n\n\ntemplate<int S = 1, typename TT, typename T1>\nvoid muladd(TT &&a, const T1 &b)\n{\n    for (size_t i = 0; i < b.size(); ++i)\n        std::forward<TT>(a)[i] += S * b[i];\n}\n\nint main()\n{\n    static constexpr size_t SIZE = 10000;\n    static constexpr size_t NUM = 10000;\n    std::vector<long long> buffer;\n    buffer.resize(SIZE);\n    std::fill(buffer.begin(), buffer.end(), 0);\n    // This is a view\n    std::span<long long, SIZE> view(buffer.begin(), SIZE);\n\n    std::vector<std::array<long long, SIZE>> operands;\n    operands.resize(NUM);\n    #pragma omp parallel for\n    for (size_t i = 0; i < NUM; ++i)\n        std::fill(operands[i].begin(), operands[i].end(), i + 1);\n\n    #pragma omp declare reduction ( \\\n        add: \\\n        std::span<long long, SIZE>,     /* This is the final reduction result type (already defined as 'view') */ \\\n        std::array<long long, SIZE>:    /* This is the intermediate reductions result type */ \\\n        muladd(omp_out, omp_in))        /* How the reduction takes place */ \\\n        initializer( /* How the intermediate reduction type initialized */ \\\n            std::array<long long, SIZE> omp_priv; \\\n            std::fill(omp_priv.begin(), omp_priv.end(), 0); \\\n        )\n    #pragma omp parallel for reduction(add:view)\n    for (size_t i = 0; i < NUM; ++i)\n        muladd(view, operands[i]);\n\n    // Print first 10 elements to validate the results (all must be 50005000)\n    for (size_t i = 0; i < 10; ++i)\n        std::cout << view[i] << \" \";\n}\n```\n\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Is eta reduction possible?\r\n                \r\nIs it possible to apply eta reduction in below case?\n\n```\nlet normalise = filter (\\x -> Data.Char.isLetter x || Data.Char.isSpace x )\n```\n\n\nI was expecting something like this to be possible:\n\n```\nlet normalise = filter (Data.Char.isLetter || Data.Char.isSpace)\n```\n\n\n...but it is not\n    ", "Answer": "\r\nYour solution doesn't work, because ```\n(||)```\n works on ```\nBool```\n values, and ```\nData.Char.isLetter```\n and ```\nData.Char.isSpace```\n are of type ```\nChar -> Bool```\n.\n\npl gives you:\n\n```\n$ pl \"f x = a x || b x\"\nf = liftM2 (||) a b\n```\n\n\nExplanation: ```\nliftM2```\n lifts ```\n(||)```\n to the ```\n(->) r```\n monad, so it's new type is ```\n(r -> Bool) -> (r -> Bool) -> (r -> Bool)```\n.\n\nSo in your case we'll get:\n\n```\nimport Control.Monad\nlet normalise = filter (liftM2 (||) Data.Char.isLetter Data.Char.isSpace)\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Openmp Reduction for operator \"-\"\r\n                \r\n```\nint main()\n{\n    int a=0;\n    omp_set_num_threads(2);\n    #pragma omp parallel reduction(+ : a)\n    {\n        a = omp_get_thread_num()+1;\n    }\n    std::cout << \"Output:\" << a;\n    return 1;\n}\n```\n\n\nI am getting wrong output using openmp reduction... For the following code, reduction(+) gives the output of sum of threadnum(), but when i mention reduction (-), it gives the same output... I am getting output as 3 for \"+\" and  \"-\".\n    ", "Answer": "\r\nReduction in OpenMP functions as follows: each thread is given its local copy of the reduction variable and operations such as ```\n+=```\n and ```\n-=```\n affect the local copy. Then at the end the following reduction operation is performed:\n\n```\na = init_value op a_0 op a_1 op a_2 op ... op a_(N-1)\n```\n\n\nwhere ```\nN```\n is the number of threads and ```\ninit_value```\n is the initialisation value for the reduction operation. The initialisation value for both ```\n+```\n and ```\n-```\n reduction operations is 0 (zero). There is a catch with the subtraction though and it is explained in the OpenMP standard as a comment to the way that the final value is formed (§2.9.3.6 ```\nreduction```\n clause):\n\n\n  (The partial results of a subtraction reduction are added to form the final value.)\n\n\nThis means that with ```\nreduction(-:a)```\n you still get all private values added together, i.e. both ```\n+```\n and ```\n-```\n reductions are equivalent. This is the correct way to implement it since it is assumed that the ```\n-```\n reduction will only be coupled with the ```\nvar -= expr```\n or ```\nvar = var - expr```\n expressions. Therefore your program is not conforming to the standard and you get what you deserve for not reading into the intricate details of OpenMP.\n\nThe correct example with ```\nreduction(-:a)```\n would be:\n\n```\n#pragma omp parallel reduction(-:a)\n{\n    a -= omp_get_thread_num() + 1;\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Openmp Reduction for operator \"-\"\r\n                \r\n```\nint main()\n{\n    int a=0;\n    omp_set_num_threads(2);\n    #pragma omp parallel reduction(+ : a)\n    {\n        a = omp_get_thread_num()+1;\n    }\n    std::cout << \"Output:\" << a;\n    return 1;\n}\n```\n\n\nI am getting wrong output using openmp reduction... For the following code, reduction(+) gives the output of sum of threadnum(), but when i mention reduction (-), it gives the same output... I am getting output as 3 for \"+\" and  \"-\".\n    ", "Answer": "\r\nReduction in OpenMP functions as follows: each thread is given its local copy of the reduction variable and operations such as ```\n+=```\n and ```\n-=```\n affect the local copy. Then at the end the following reduction operation is performed:\n\n```\na = init_value op a_0 op a_1 op a_2 op ... op a_(N-1)\n```\n\n\nwhere ```\nN```\n is the number of threads and ```\ninit_value```\n is the initialisation value for the reduction operation. The initialisation value for both ```\n+```\n and ```\n-```\n reduction operations is 0 (zero). There is a catch with the subtraction though and it is explained in the OpenMP standard as a comment to the way that the final value is formed (§2.9.3.6 ```\nreduction```\n clause):\n\n\n  (The partial results of a subtraction reduction are added to form the final value.)\n\n\nThis means that with ```\nreduction(-:a)```\n you still get all private values added together, i.e. both ```\n+```\n and ```\n-```\n reductions are equivalent. This is the correct way to implement it since it is assumed that the ```\n-```\n reduction will only be coupled with the ```\nvar -= expr```\n or ```\nvar = var - expr```\n expressions. Therefore your program is not conforming to the standard and you get what you deserve for not reading into the intricate details of OpenMP.\n\nThe correct example with ```\nreduction(-:a)```\n would be:\n\n```\n#pragma omp parallel reduction(-:a)\n{\n    a -= omp_get_thread_num() + 1;\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "XSLT How to call XML item without ID partameter?\r\n                \r\nI have to create HTML table using XML data file and XSLT style sheet with <```\nxsl:for-each```\n> instruction. I should use names of items included into XML file.\nMy data are organised in one XML file in two ways. Structure is as below:\n```\n<Data>\n   <FieldBook> (...) </FieldBook>\n   <Reductions>  (...) </Reductions>\n</Data>\n\n```\n\n```\n<FieldBook>\n<PointRecord ID=\"00000017\" TimeStamp=\"2020-08-04T14:42:41\">\n            <Name>osn2t</Name>\n            <Code></Code>\n            <Description1 Name=\"Description\">k</Description1>\n            <Method>GpsStaticObservation</Method>\n            <SurveyMethod>NetworkFix</SurveyMethod>\n            <Classification>Stakeout</Classification>\n            <Deleted>false</Deleted>\n            <ECEFDeltas>\n                <DeltaX>-23.63342856942</DeltaX>\n                <DeltaY>-3.16591248824</DeltaY>\n                <DeltaZ>10.47772248089</DeltaZ>\n            </ECEFDeltas>\n            <Precision>\n                <Horizontal>0.01284883061</Horizontal>\n                <Vertical>0.01942246037</Vertical>\n            </Precision>\n            <QualityControl1>\n                <NumberOfSatellites>12</NumberOfSatellites>\n                <RelativeDOPs>false</RelativeDOPs>\n                <PDOP>1.20000000000</PDOP>\n                <HDOP>0.70000000000</HDOP>\n                <VDOP>1.00000000000</VDOP>\n                <RMS>57.01754385965</RMS>\n                <NumberOfPositionsUsed>7</NumberOfPositionsUsed>\n                <HorizontalStandardDeviation></HorizontalStandardDeviation>\n                <VerticalStandardDeviation></VerticalStandardDeviation>\n                <StartTime>\n                    <GPSWeek>2117</GPSWeek>\n                    <Seconds>218550.0000</Seconds>\n                </StartTime>\n                <EndTime>\n                    <GPSWeek>2117</GPSWeek>\n                    <Seconds>218575.0000</Seconds>\n                </EndTime>\n                <MonitorStatus>NotMonitored</MonitorStatus>\n            </QualityControl1>\n            <QualityControl2>\n                <NumberOfSatellites>12</NumberOfSatellites>\n                <ErrorScale>0.010833333</ErrorScale>\n                <VCVxx>0.00003600000</VCVxx>\n                <VCVxy>0.00000339435</VCVxy>\n                <VCVxz>0.00002158332</VCVxz>\n                <VCVyy>0.00000900000</VCVyy>\n                <VCVyz>0.00000853788</VCVyz>\n                <VCVzz>0.00010000000</VCVzz>\n                <UnitVariance>0.0</UnitVariance>\n            </QualityControl2>\n            <AntennaID>00000015</AntennaID>\n            <RTK_Base>VRS1</RTK_Base>\n            <ComputedGrid>\n                <North>5544447.5666</North>\n                <East>7424116.2708</East>\n                <Elevation>213.2381</Elevation>\n            </ComputedGrid>\n            <Stakeout>\n                <PointDesign>\n                    <Name>osn2</Name>\n                    <Code></Code>\n                    <StakeoutMethod>ToThePoint</StakeoutMethod>\n                    <DesignElevation>213.2180</DesignElevation>\n                </PointDesign>\n                <GridDeltas>\n                    <DeltaNorth>0.0104</DeltaNorth>\n                    <DeltaEast>0.0132</DeltaEast>\n                    <DeltaElevation>-0.0201</DeltaElevation>\n                </GridDeltas>\n            </Stakeout>\n        </PointRecord>\n...\n</FieldBook>\n```\n\nand\n```\n<Reductions>\n        <Point>\n            <ID>00000017</ID>\n            <Name>osn2t</Name>\n            <Code></Code>\n            <Description1 Name=\"Description\">k</Description1>\n            <SurveyMethod>NetworkFix</SurveyMethod>\n            <Classification>Stakeout</Classification>\n            <Grid>\n                <North>5544447.5666</North>\n                <East>7424116.2708</East>\n                <Elevation>213.2381</Elevation>\n            </Grid>\n            <WGS84>\n                <Latitude>50.03137528405</Latitude>\n                <Longitude>19.94080538786</Longitude>\n                <Height>253.0977</Height>\n            </WGS84>\n        </Point>\n</Reductions>\n```\n\nI get necessary data from one part of XML tree (FieldBook) using such code for NumberOfSatellites, but I don't know how to get it eg. for second field, from Reductions items:\n```\n<xsl:template match=\"PointRecord\">\n  <xsl:for-each select=\"/Data/FieldBook/PointRecord[Deleted!='true']\"></xsl:for-each>\n    <xsl:if test=\"ECEFDeltas/DeltaX/text()\">\n      <tr>\n        (...)\n        <td>\n          <xsl:call-template name=\"my-out\">\n            <xsl:with-param name=\"Val\" select=\"QualityControl1/NumberOfSatellites\"/>\n          </xsl:call-template>\n        </td>\n          (...)\n        <td>\n          <xsl:call-template name=\"my-format\">\n            <xsl:with-param name=\"Val\" select=\" >>> data from <Reductions item> <<<\"/>\n            <xsl:with-param name=\"format\" select=\"$DecPl2\"/>\n          </xsl:call-template>\n        </td>\n\n      </tr>\n    </xsl:if>\n```\n\n\nHow can I call eg. Point/Grid/North item value from Reductions using ID parameter of PointRecord from FieldBook?\n\nHow can I call PointRecord item from FieldBook using Point/ID item value from Reductions?\n\n\n    ", "Answer": "\r\nI think you want to use a key; declare\n```\n<xsl:key name=\"red-ref\" match=\"Reductions/Point\" use=\"ID\"/>\n```\n\nas a top-level element (i.e. a child of ```\nxsl:stylesheet```\n or ```\nxsl:transform```\n). Then, in any context you process a ```\nPointRecord```\n element you can use ```\nkey('red-ref', @ID)```\n to find the referenced ```\nPoint```\n, i.e. ```\nkey('red-ref', @ID)/Grid/North```\n will give you that element.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "audio noise reduction\r\n                \r\nI have one doubt regarding audio processing for noise reduction. Is there any free ware and share ware DLLs available for noise reduction in .wav audio files or any sample codes using c#, vb.net or vb?\n    ", "Answer": "\r\nSoX is a cross-platform command-line application that does audio manipulation.  A quick check of the man page reveals that it can do noise reduction (see ```\nnoiseprof```\n and ```\nnoisered```\n).\n\nYou can use ```\ntrim```\n in combination with ```\nnoiseprof```\n to choose a small clip of a larger audio file to use as the noise.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP Shared Array in C: Reduction vs Atomic Update Problem\r\n                \r\nI am having trouble determining what is going wrong with this OpenMP task example.  For context, y is a large shared array and rowIndex is not unique for each task.  There may be multiple tasks trying to increment the value y[rowIndex].  \n\nMy question is, does y needed to be guarded by a reduction clause, or is an atomic update enough?  I am currently experiencing a crash with a much larger program and am wondering if I am botching something fundamental with this.  \n\nOf the examples I have seen, most array reductions are for very small arrays due to array copying for each thread, while most atomic updates are not used on array elements.  There doesn't seem to be much content for updating a shared array one element at a time with potential for a race condition (also the context of task-based parallelism is rare).\n\n```\n#pragma omp parallel shared(y) // ??? reduction(+:y) ???\n#pragma omp single\nfor(i = 0; i < n; i++)\n{  \n  sum = DoSmallWork_SingleThread();  \n  rowIndex = getRowIndex_SingleThread();\n\n  #pragma omp task firstprivate(sum, rowIndex)  \n  {    \n    sum += DoLotsOfWork_TaskThread();\n\n    // ??? #pragma omp atomic update ???\n    y[ rowIndex ] += sum;  \n  }\n}\n```\n\n    ", "Answer": "\r\nYou have basically 3 solutions to avoid these types of race conditions, which you all mention. They all work distinctly:\n\n\natomic access, i.e. letting threads/tasks access the same array at the same moment but ensure a proper ordering of operations, this is done using a ```\nshared```\n clause for the array with an ```\natomic```\n clause on the operation:\n\n```\n#pragma omp parallel\n#pragma omp single\nfor(i = 0; i < n; i++)\n{\n    sum = DoSmallWork_SingleThread();\n    rowIndex = getRowIndex_SingleThread();\n\n    #pragma omp task firstprivate(sum, rowIndex) shared(y)\n    {\n        increment = sum + DoLotsOfWork_TaskThread();\n\n        #pragma omp atomic\n        y[rowIndex] += increment;\n    }\n}\n```\n\nprivatisation, i.e. every task/thread has its own copy of the array and they’re then summed together later, which is what a ```\nreduction```\n clause does:\n\n```\n#pragma omp parallel\n#pragma omp single\n#pragma omp taskgroup task_reduction (+:y[0:n-1])\nfor(int i = 0; i < n; i++)\n{\n    int sum = DoSmallWork_SingleThread();\n    int rowIndex = getRowIndex_SingleThread();\n\n    #pragma omp task firstprivate(sum, rowIndex) in_reduction(+:y[0:n-1])\n    {\n        y[rowIndex] += sum + DoLotsOfWork_TaskThread();\n    }\n}\n```\n\nexclusive access to the array, or section of the array, which is what task-dependencies are used for (you could implement using mutexes for a thread-based parallelism model for example):\n\n```\n#pragma omp parallel\n#pragma omp single\nfor(i = 0; i < n; i++)\n{\n    sum = DoSmallWork_SingleThread();\n    rowIndex = getRowIndex_SingleThread();\n\n    #pragma omp task firstprivate(sum, rowIndex) depend(inout:y[rowIndex])\n    {\n        y[rowIndex] += sum + DoLotsOfWork_TaskThread();\n    }\n}\n```\n\n\n\nWhen should you use each of them?\n\n\nAn atomic access is a slower type of memory access that provides consistency guarantees, and can be especially slow in case of conflicts, that is when two (or more) threads try to modify the same value simultaneously.\n\nIt is preferable to use atomics only when updates to ```\ny```\n are few and far between, and the probability of having conflicts is low.\nPrivatisation avoids that conflict issue by making copies of the arrays and joining them (in your case, adding them) together.\n\nThis incurs a memory overhead, and possibly impacts the cache, proportionally to the size of ```\ny```\n.\nFinally, providing task dependencies avoids the problem altogether by using scheduling, that is by only running simultaneously tasks that modify separate parts of the array. In general this is preferable when ```\ny```\n is large and the operations modifying ```\ny```\n are frequent in the task.\n\nYour parallelism is however limited by the number of dependencies that you define, so in the example above, by the number of rows in ```\ny```\n. If for example you only have 8 rows but 32 cores, this may not be the best approach because you will only use 25% of your computing power.\n\n\nNB: This means that in the privatisation (aka reduction) case and especially the dependency case, you benefit from grouping together sections of array ```\ny```\n, typically by having a task operate on a number of contiguous rows. You can then reduce (for reductions, respectively increase for dependencies) the size of the array chunk provided in the task’s clause.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Feature Selection and Reduction for Text Classification\r\n                \r\nI am currently working on a project, a simple sentiment analyzer such that there will be 2 and 3 classes in separate cases. I am using a corpus that is pretty rich in the means of unique words (around 200.000). I used bag-of-words method for feature selection and to reduce the number of unique features, an elimination is done due to a threshold value of frequency of occurrence. The final set of features includes around 20.000 features, which is actually a 90% decrease, but not enough for intended accuracy of test-prediction. I am using LibSVM and SVM-light in turn for training and prediction (both linear and RBF kernel) and also Python and Bash in general.\n\nThe highest accuracy observed so far is around 75% and I need at least 90%. This is the case for binary classification. For multi-class training, the accuracy falls to ~60%. I need at least 90% at both cases and can not figure how to increase it: via optimizing training parameters or via optimizing feature selection?\n\nI have read articles about feature selection in text classification and what I found is that three different methods are used, which have actually a clear correlation among each other. These methods are as follows:\n\n\nFrequency approach of bag-of-words (BOW)\nInformation Gain (IG)\nX^2 Statistic (CHI)\n\n\nThe first method is already the one I use, but I use it very simply and need guidance for a better use of it in order to obtain high enough accuracy. I am also lacking knowledge about practical implementations of IG and CHI and looking for any help to guide me in that way.\n\nThanks a lot, and if you need any additional info for help, just let me know.\n\n\n\n\n@larsmans: Frequency Threshold: I am looking for the occurrences of unique words in examples, such that if a word is occurring in different examples frequently enough, it is included in the feature set as a unique feature.   \n@TheManWithNoName: First of all thanks for your effort in explaining the general concerns of document classification. I examined and experimented all the methods you bring forward and others. I found Proportional Difference (PD) method the best for feature selection, where features are uni-grams and Term Presence (TP) for the weighting (I didn't understand why you tagged Term-Frequency-Inverse-Document-Frequency (TF-IDF) as an indexing method, I rather consider it as a feature weighting approach).  Pre-processing is also an important aspect for this task as you mentioned. I used certain types of string elimination for refining the data as well as morphological parsing and stemming. Also note that I am working on Turkish, which has different characteristics compared to English. Finally, I managed to reach ~88% accuracy (f-measure) for binary classification and ~84% for multi-class. These values are solid proofs of the success of the model I used. This is what I have done so far. Now working on clustering and reduction models, have tried LDA and LSI and moving on to moVMF and maybe spherical models (LDA + moVMF), which seems to work better on corpus those have objective nature, like news corpus. If you have any information and guidance on these issues, I will appreciate. I need info especially to setup an interface (python oriented, open-source) between feature space dimension reduction methods (LDA, LSI, moVMF etc.) and clustering methods (k-means, hierarchical etc.).\n\n    ", "Answer": "\r\nThis is probably a bit late to the table, but...\n\nAs Bee points out and you are already aware, the use of SVM as a classifier is wasted if you have already lost the information in the stages prior to classification. However, the process of text classification requires much more that just a couple of stages and each stage has significant effects on the result. Therefore, before looking into more complicated feature selection measures there are a number of much simpler possibilities that will typically require much lower resource consumption.\n\nDo you pre-process the documents before performing tokensiation/representation into the bag-of-words format? Simply removing stop words or punctuation may improve accuracy considerably.\n\nHave you considered altering your bag-of-words representation to use, for example, word pairs or n-grams instead? You may find that you have more dimensions to begin with but that they condense down a lot further and contain more useful information.\n\nIts also worth noting that dimension reduction is feature selection/feature extraction. The difference is that feature selection reduces the dimensions in a univariate manner, i.e. it removes terms on an individual basis as they currently appear without altering them, whereas feature extraction (which I think Ben Allison is referring to) is multivaritate, combining one or more single terms together to produce higher orthangonal terms that (hopefully) contain more information and reduce the feature space.\n\nRegarding your use of document frequency, are you merely using the probability/percentage of documents that contain a term or are you using the term densities found within the documents? If category one has only 10 douments and they each contain a term once, then category one is indeed associated with the document. However, if category two has only 10 documents that each contain the same term a hundred times each, then obviously category two has a much higher relation to that term than category one. If term densities are not taken into account this information is lost and the fewer categories you have the more impact this loss with have. On a similar note, it is not always prudent to only retain terms that have high frequencies, as they may not actually be providing any useful information. For example if a term appears a hundred times in every document, then it is considered a noise term and, while it looks important, there is no practical value in keeping it in your feature set.\n\nAlso how do you index the data, are you using the Vector Space Model with simple boolean indexing or a more complicated measure such as TF-IDF? Considering the low number of categories in your scenario a more complex measure will be beneficial as they can account for term importance for each category in relation to its importance throughout the entire dataset.\n\nPersonally I would experiment with some of the above possibilities first and then consider tweaking the feature selection/extraction with a (or a combination of) complex equations if you need an additional performance boost.\n\n\n\nAdditional\n\nBased on the new information, it sounds as though you are on the right track and 84%+ accuracy (F1 or BEP - precision and recall based for multi-class problems) is generally considered very good for most datasets. It might be that you have successfully acquired all information rich features from the data already, or that a few are still being pruned.\n\nHaving said that, something that can be used as a predictor of how good aggressive dimension reduction may be for a particular dataset is 'Outlier Count' analysis, which uses the decline of Information Gain in outlying features to determine how likely it is that information will be lost during feature selection. You can use it on the raw and/or processed data to give an estimate of how aggressively you should aim to prune features (or unprune them as the case may be). A paper describing it can be found here:\n\nPaper with Outlier Count information\n\nWith regards to describing TF-IDF as an indexing method, you are correct in it being a feature weighting measure, but I consider it to be used mostly as part of the indexing process (though it can also be used for dimension reduction). The reasoning for this is that some measures are better aimed toward feature selection/extraction, while others are preferable for feature weighting specifically in your document vectors (i.e. the indexed data). This is generally due to dimension reduction measures being determined on a per category basis, whereas index weighting measures tend to be more document orientated to give superior vector representation.\n\nIn respect to LDA, LSI and moVMF, I'm afraid I have too little experience of them to provide any guidance. Unfortunately I've also not worked with Turkish datasets or the python language. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Please explain me the following Clojure code\r\n                \r\nI found the following code (in this blog post that solves the Coin Changer Kata):\n\n```\n(defn change-for [amount]\n  (let [denominations [25 10 5 1]\n        amounts (reductions #(rem %1 %2) amount denominations)\n        coins (map #(int (/ %1 %2)) amounts denominations)]\n    (mapcat #(take %1 (repeat %2)) coins denominations)))\n```\n\n\nThe part I find difficult is: ```\n(reductions #(rem %1 %2) amount denominations)```\n.\n\nAs I found out, reductions just calculates the resulting collection incrementally based on some given function. Example: ```\n(reductions + [1 2 3])```\n gives ```\n[1 3 6]```\n.\n\n```\n1         ; first  element\n1 + 2     ; second element\n1 + 2 + 3 ; third  element\n```\n\n\nThe next function, rem which calculates the remainder is still extremely simple to understand.\n\nTo understand the rest of the code I tried the following:\n\n```\n; first try, to see if this call works\n; outside the original code (the change-for function)\n(reductions #(rem %1 %2) 17 [10 5 1]) ; --> [17 7 2 0]\n\n; tried to use the reductions which takes only one argument\n; notice that 17 is now inside the array\n(reductions #(rem %1 %2) [17 10 5 1]) ; --> [17 7 2 0]\n\n; further simplified the expression\n(reductions rem [17 10 5 1]) ; --> [17 7 2 0]\n```\n\n\nThe last step was to remove the anonymous function as described in this blog post.\n\nHere, things get confusing (at least for me): ```\nrem```\n takes 2 arguments and I don't get it how they are applied when using the array ```\n[17 10 5 1]```\n. I tried the following calls:\n\n```\n(rem [17 10 5 1]) ; --> gives error\n(rem [17 10 5 1] [17 10 5 1]) ; --> also gives error\n(rem 17 10) ; --> works, but how do you use it with collections?\n```\n\n\nCan someone explain me, how this ```\nrem```\n function works with the ```\nreductions```\n function?\n\nAnother thing I don't quite understand is: how are those percent arguments applied (in ```\n#(rem %1 %2)```\n)? I mean where do they come from? I tried calling ```\nrem```\n in the following way, but I get an error: ```\n(#(rem %1 %2) 17 [10 5 1])```\n. There must be something the ```\nreductions```\n function is doing behind the scenes to make this work, right?\n\nAt first I thought that ```\n#(rem %1 %2)```\n was a set. These are declared in a similar way as sets and can be easily misused (by someone just starting out with Clojure):\n\n```\n(type #{1 2 3})   ; --> clojure.lang.PersistentHashSet\n(type #(1 2 3))   ; --> user$eval12687$fn__12688\n```\n\n\nCan someone point me to a site/book/whatever that explains Clojure tricks such as \"The Special Form for an Anonymous Function\"? Most resources just give the simplest constructs (the ones similar to all the other lisp derivatives) without going into the intricacies of Clojure. I found a site which looks pretty good (and also explains the anonymous functions I mentioned above). Any other such resources?\n    ", "Answer": "\r\nThis:\n\n```\n(reductions #(rem %1 %2) amount denominations)\n```\n\n\nis equivalent to this:\n\n```\n(reductions rem amount denominations)\n```\n\n\nand as you noticed\n\n```\n(reductions function start collection) \n```\n\n\nreturns a sequence of intermediate results of reducing the ```\ncollection```\n with ```\nfunction```\n (taking ```\nstart```\n as the first argument of the first step of reduction). ```\nfunction```\n must take two parameters.\n\nSo the result of:\n\n```\n(reductions function start [1 2 3 4 5]) \n```\n\n\nis \n\n```\n((function start 1) (function (function start 1) 2) ...) \n```\n\n\nThe ```\n#(rem %1 %2)```\n syntax is just a shorthand for defining anonymous function that takes two parameters (```\n%1```\n and ```\n%2```\n), calls ```\nrem```\n on them and returns the result.\n\nIt's equivalent to:\n\n```\n(fn [a b] (rem a b))\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction in CUDA\r\n                \r\nI'm just starting to learn CUDA programming, and I have some confusion about reduction. \n\nI know that global memory has much visiting delay as compared to shared memory, but can I use global memory to (at least) simulate a behavior similar to shared memory?\n\nFor example, I want to sum the elements of a large array whose length is exactly ```\nBLOCK_SIZE * THREAD_SIZE```\n (both the dimensions of grid and block are power of ```\n2```\n), and I've tried to use the code below:\n\n```\n    __global__ void parallelSum(unsigned int* array) {\n\n    unsigned int totalThreadsNum = gridDim.x * blockDim.x;\n    unsigned int idx = blockDim.x * blockIdx.x + threadIdx.x;\n\n    int i = totalThreadsNum / 2;\n    while (i != 0) {\n            if (idx < i) {\n                array[idx] += array[idx + i];\n        }\n        __syncthreads();\n        i /= 2;\n    }\n}\n```\n\n\nI compared the result of this code and the result generated serially on the host, and the weird thing is: sometimes the results are the same, but sometimes they are apparently different. Is there any reason related to using global memory here?\n    ", "Answer": "\r\nTom has already answered this question. In his answer, he recommends using Thrust or CUB to perform reductions in CUDA.\n\nHere, I'm providing a fully worked example on how using both libraries to perform reductions.\n\n```\n#define CUB_STDERR\n\n#include <stdio.h>\n\n#include <thrust/device_ptr.h>\n#include <thrust/reduce.h>\n#include <thrust/execution_policy.h>\n\n#include <cub/device/device_reduce.cuh>\n\n#include \"TimingGPU.cuh\"\n#include \"Utilities.cuh\"\n\nusing namespace cub;\n\n/********/\n/* MAIN */\n/********/\nint main() {\n\n    const int N = 8388608;\n\n    gpuErrchk(cudaFree(0));\n\n    float *h_data       = (float *)malloc(N * sizeof(float));\n    float h_result = 0.f;\n\n    for (int i=0; i<N; i++) {\n        h_data[i] = 3.f;\n        h_result = h_result + h_data[i];\n    }\n\n    TimingGPU timerGPU;\n\n    float *d_data;          gpuErrchk(cudaMalloc((void**)&d_data, N * sizeof(float)));\n    gpuErrchk(cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice));\n\n    /**********/\n    /* THRUST */\n    /**********/\n    timerGPU.StartCounter();\n    thrust::device_ptr<float> wrapped_ptr = thrust::device_pointer_cast(d_data);\n    float h_result1 = thrust::reduce(wrapped_ptr, wrapped_ptr + N);\n    printf(\"Timing for Thrust = %f\\n\", timerGPU.GetCounter());\n\n    /*******/\n    /* CUB */\n    /*******/\n    timerGPU.StartCounter();\n    float           *h_result2 = (float *)malloc(sizeof(float));\n    float           *d_result2; gpuErrchk(cudaMalloc((void**)&d_result2, sizeof(float)));\n    void            *d_temp_storage = NULL;\n    size_t          temp_storage_bytes = 0;\n\n    DeviceReduce::Sum(d_temp_storage, temp_storage_bytes, d_data, d_result2, N);\n    gpuErrchk(cudaMalloc((void**)&d_temp_storage, temp_storage_bytes));\n    DeviceReduce::Sum(d_temp_storage, temp_storage_bytes, d_data, d_result2, N);\n\n    gpuErrchk(cudaMemcpy(h_result2, d_result2, sizeof(float), cudaMemcpyDeviceToHost));\n\n    printf(\"Timing for CUB = %f\\n\", timerGPU.GetCounter());\n\n    printf(\"Results:\\n\");\n    printf(\"Exact: %f\\n\", h_result);\n    printf(\"Thrust: %f\\n\", h_result1);\n    printf(\"CUB: %f\\n\", h_result2[0]);\n\n}\n```\n\n\nPlease, note that CUB can be somewhat faster than Thrust due to the different underlying philosophy, since CUB leaves performance-critical details, such as the exact choice of the algorithm and the degree of concurrency unbound and in the hands of the user. In this way, these parameters can be tuned in order to maximize performance for a particular architecture and application.\n\nA comparison for calculating the Euclidean norm of an array is reported at CUB in Action – some simple examples using the CUB template library.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction in Lambda Calculus\r\n                \r\nI have been recently studying about lambda calculation and i have many doubts about reduction and substitution  . What is alpha and beta reduction are? And when and why are they used ?. \n\nIt would be great if someone could tell any good resources on reduction and substitution  in lambda caculus.\n    ", "Answer": "\r\nBeta reduction is just the primary application rule used for computation within the lambda calculus. It is applied through substitution as shown:\n\nIf you had the lambda term: (\\x.x)\nand some value to the right of it: y\n\nThen you would substitute all the bound variables to the right of the (.) in your lambda term.\nThe bound variables are the variables that match the variable left of the (.), so in this case x.\n\n```\nThe reduction would be of the form:\n(\\x.x)y    //y gets bound to all occurences of x to the right of the period\ny\n```\n\n\nWhere y is bound to all occurrences of x in the lambda expression. This is the identity function.\n\nAlpha \"reductions\" are usually called alpha equivalences or alpha rewrite rules. They state that you can change the name of any lambda term and its bound variables without changing the meaning of the expression.\n\nFor example using the identity function from above we could just as easily written the lambda term as (\\j.j). It wouldn't change the outcome of our application as shown:\n\n```\n(\\j.j)y    //y gets bound to all occurrences of j to the right of the period\ny\n```\n\n\nAs to learning resources: the wikipedia page is pretty detailed, but notation heavy and would probably require a few good rereads.\n\nIf you're just looking for better intuition as to how lambda calculus works, most computer science departments have slides laying around.\n\nYou might find these helpful: http://www.classes.cs.uchicago.edu/archive/2002/winter/CS33600/slides/Lesson2.pdf\nhttps://www.utdallas.edu/~gupta/courses/apl/lambda.pdf\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to output the cumulative sum of a variable that undergo multiple % numeric reductions [R]\r\n                \r\nI'm looking to assess the importance of the cleaning frequency per day in the amount of feces that accumulates in the floor of a cattle barn. The problem is that I'm only able to correctly calculate the amount of feces after the first cleaning event (i.e, first reduction in feces), with subsequent calculations failing to consider that a cleaning event already happened.\nFor my attempt, I defined four variables and two constants in R:\nVariables\n\n\"hour\" (numeric; from 1 to 24)\n\"binclean\" (binary; 0=no cleaning event, 1=cleaning event)\n\"noclean\" (numeric; variable storing the cumulative amount of feces in the barn's floor when there is no cleaning)\n\"clean\" (numeric; contains the amount of feces in the barn's floor after cleaning)\n\nConstants\n\n\"totalfeceshour\" (amount of feces produced every hour by 250 cattle = 239.5 kg)\n\"H\" (amount of feces remaining in the environment after the floor is scraped = 0.05% of feces)\n\nI created a matrix to store all the variables and perform calculations. What I've done is use the ```\nReduce```\n function (base R function) to get the feces accumulated without any cleaning and store it in the ```\nnoclean```\n variable. After this, I used a \"for loop\" with the ```\nifelse()```\n function to signal the occurrence of a cleaning event. Information about feces after cleaning is then stored in the \"clean\" variable. So far, I've been able to obtain the correct amount of feces on the floor only after the first cleaning event, but this information is not carried on to subsequent cells. For example, by the 7th hour of the day, there are 1,676 kg of feces in the floor. If a cleaning event occurs at the 8th hour, then the total amount of feces after cleaning is (1,676 + 239.5) * 0.05 = 95.8. If you run the code, you will see that the amount of feces by the 8th hour goes up to 2,155.5 (when it should be 95.8 + 239.5 = 335.3 after a cleaning event already happened). This is my code:\n```\n#Constants\ntotalfeceshour <- 239.5\nH <- 1-0.95\n\n#Variables\nhour<- 1:24\ncleanbin <- c(0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, \n              0, 0, 0, 0, 0, 0, 1) #3 scrapes at hours 8, 16 and \n                                   #24. Each value represent an \n                                   #hour of the day.\nnoclean <- c(Reduce(\"+\", c(totalfeceshour, rep(totalfeceshour, \n             23)), accumulate = TRUE)) #Feces in the environment\nclean <- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\n\n#Matrix\nmtrx <- matrix(data=c(hour, cleanbin, noclean, clean), ncol=4)\ncolnames(mtrx) <- c(\"hour\", \"cleanbin\", \"noclean\", \"clean\")\n\n#Loop\nfor(i in 1:length(mtrx[, 2])){\n  mtrx[, 4][i] <- ifelse(mtrx[, 2][i] == 0, mtrx[, 3][i], mtrx[,3] \n   [i]*H)\n}\n\nmtrx\n```\n\nIf you run the code, you could see that cleaning only reduced the amount of feces in cells in which ```\ncleanbin==1```\n, but this information is not carried on to to subsequent cells. I've tried using nested \"for\" loops, \"shift\" function (\"data.table\" package), and other tools but haven't been able to solve this yet. I was able to create an Excel sheet that does the trick, but I am looking to achieve the same in R. I would deeply appreciate your assistance in coding this.\n    ", "Answer": "\r\nHere's a tidyverse(-ish) solution.  The most difficult part of the solution is that you can't use built-in solutions like ```\ngroup_map```\n or ```\nifelse```\n as they stand because all of these options work atomically - that is, they operate on a column as whole, whereas you need to work iteratively, building up a solution row by row.\nBecause my workflow is different to yours, I've renamed some of your variables and dropped others.\nFirst, create a data frame containing the basic information that defines the problem.\n```\nlibrary(tidyverse)\n\nd <- tibble(\n       Hour=1:24,\n       FecesProduced=239.5,\n       CleaningEvent=Hour %in% c(8, 16, 24),\n       CumulativeFeces=ifelse(Hour==1, FecesProduced, NA)\n     ) \nd\n# A tibble: 24 × 4\n    Hour FecesProduced CleaningEvent CumulativeFeces\n   <int>         <dbl> <lgl>                   <dbl>\n 1     1          240. FALSE                    240.\n 2     2          240. FALSE                     NA \n 3     3          240. FALSE                     NA \n 4     4          240. FALSE                     NA \n 5     5          240. FALSE                     NA \n 6     6          240. FALSE                     NA \n 7     7          240. FALSE                     NA \n 8     8          240. TRUE                      NA \n 9     9          240. FALSE                     NA \n10    10          240. FALSE                     NA \n# … with 14 more rows\n```\n\nNow step through rows 2 to 24, populating ```\nCumulativeFeces```\n as required.\n```\nfor(row in 2:nrow(d)) {\n  d <- d %>% mutate(\n               CumulativeFeces=ifelse(\n                                 row_number() == row,\n                                 ifelse(\n                                   CleaningEvent,\n                                   (FecesProduced + lag(CumulativeFeces)) * 0.05,\n                                   FecesProduced + lag(CumulativeFeces)\n                                  ),\n                                 CumulativeFeces\n                                )\n             )\n}\n```\n\nNow override the default behaviour when printing tibbles to ensure the accuracy of the solution is demonstrated.\n```\nd %>% mutate(CumulativeFeces=num(CumulativeFeces, digits=2))\n# A tibble: 24 × 4\n    Hour FecesProduced CleaningEvent CumulativeFeces\n   <int>         <dbl> <lgl>               <num:.2!>\n 1     1          240. FALSE                  239.50\n 2     2          240. FALSE                  479.00\n 3     3          240. FALSE                  718.50\n 4     4          240. FALSE                  958.00\n 5     5          240. FALSE                 1197.50\n 6     6          240. FALSE                 1437.00\n 7     7          240. FALSE                 1676.50\n 8     8          240. TRUE                    95.80\n 9     9          240. FALSE                  335.30\n10    10          240. FALSE                  574.80\n# … with 14 more rows\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "String reduction\r\n                \r\nMay be it's a well-known problem by now :\n consider any string S , containing only 3 characters (a,b,c). You can do this reduction operation on these strings : \"replace two consecutive distinct characters by the 3rd one like 'ab' can be replace by 'c' and 'ac' cab be by 'b'.\" how much we can reduce by this operation?\n\nthe answer is always either (1,2,string.length) .\n\nstring.length iff all characters are same,\n2 iff count(a) = count(b) = count(c) in S.\n1 otherwise.\nbut i am not able to prove it .\n\nAny suggestion will be really helpful.\n    ", "Answer": "\r\nYou prove this type of claim by induction on the length of S.\n\nBut you have to get the claim right, and you haven't here. Your claim is \n\n\n  the answer is |S| if all characters are same, 2 if count(a) = count(b) = count(c) in S, and 1 otherwise.\n\n\nConsider the string ```\naabb```\n. Your claim says that this can be reduced to length 1, but in fact it can only be reduced to length 2: the possible reduction sequences are ```\naabb```\n → ```\nacb```\n → ```\nbb```\n and ```\naabb```\n → ```\nacb```\n → ```\naa```\n.\n\nGet the claim right and you should be able to complete the proof.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Are there functional programming languages that run on the GPU?\r\n                \r\nUsing the traditional, sequential reduction approach, the following graph is reduced as:\n\n```\n(+ (+ 1 2) (+ 3 4)) ->\n(+ 3 (+ 3 4)) ->\n(+ 3 7) ->\n10\n```\n\n\nGraph reductions are, though, inherently parallel. One could, instead, reduce it as:\n\n```\n(+ (+ 1 2) (+ 3 4)) ->\n(+ 3 7) ->\n10\n```\n\n\nAs far as I know, every functional programming language uses the first approach. I believe this is mostly because, on the CPU, scheduling threads overcompensate the benefits of doing parallel reductions. Recently, though, we've been starting to use the GPU more than the CPU for parallel applications. If a language ran entirely on the GPU, those communication costs would vanish.\n\nAre there functional languages making use of that idea?\n    ", "Answer": "\r\nWhat makes you think on GPU scheduling would not overcomponsate the benefits?\n\nIn fact, the kind of parallelism used in GPUs is far harder to schedule: it's SIMD parallelism, i.e. a whole batch of stream processors do all essentially the same thing at a time, except each one crushes a different bunch of numbers. So, not only would you need to schedule the subtasks, you would also need to keep them synchronised. Doing that automatically for general computations is virtually impossible.\n\nDoing it for specific tasks works out quite well and has been embedded into functional languages; check out the Accelerate project.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Block reduction in CUDA\r\n                \r\nI am trying to do reduction in CUDA and I am really a newbie. I am currently studying a sample code from NVIDIA.\n\nI guess I am really not sure how to set up the block size and grid size, especially when my input array is larger (```\n512 X 512```\n) than a single block size. \n\nHere is the code.\n\n```\ntemplate <unsigned int blockSize>\n__global__ void reduce6(int *g_idata, int *g_odata, unsigned int n)\n{\n    extern __shared__ int sdata[];\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x*(blockSize*2) + tid;\n    unsigned int gridSize = blockSize*2*gridDim.x;\n    sdata[tid] = 0;\n\n    while (i < n) \n    { \n        sdata[tid] += g_idata[i] + g_idata[i+blockSize]; \n        i += gridSize; \n    }\n\n    __syncthreads();\n\n    if (blockSize >= 512) { if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); }\n    if (blockSize >= 256) { if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); }\n    if (blockSize >= 128) { if (tid < 64) { sdata[tid] += sdata[tid + 64]; } __syncthreads(); }\n\n    if (tid < 32) \n    {\n        if (blockSize >= 64) sdata[tid] += sdata[tid + 32];\n        if (blockSize >= 32) sdata[tid] += sdata[tid + 16];\n        if (blockSize >= 16) sdata[tid] += sdata[tid + 8];\n        if (blockSize >= 8) sdata[tid] += sdata[tid + 4];\n        if (blockSize >= 4) sdata[tid] += sdata[tid + 2];\n        if (blockSize >= 2) sdata[tid] += sdata[tid + 1];\n    }\n\n    if (tid == 0) g_odata[blockIdx.x] = sdata[0];\n}\n```\n\n\nHowever, it seems to me the ```\ng_odata[blockIdx.x]```\n saves the partial sums from all blocks, and, if I want to get the final result, I need to sum all the terms within the ```\ng_odata[blockIdx.x]```\n array. \n\nI am wondering: is there a kernel to do the whole summation? or am I misunderstanding things here? I would really appreciate if anyone can educate me with this. Thanks very much.\n    ", "Answer": "\r\nYour understanding is correct. The reductions demonstrated here end up with a sequence of block-sums deposited in global memory.\nTo sum all of these block sums together, requires some form of global synchronization.  You must wait until all the blocks are complete before adding their sums together.  You have a number of options at this point, some of which are:\n\nlaunch a new kernel after the main kernel to sum the block-sums together\nadd the block sums on the host\nuse atomics to add the block sums together, at the end of the main kernel\nuse a method like threadfence reduction to add the block sums together in the main kernel.\nUse CUDA cooperative groups to place a grid-wide sync in the kernel code.  Sum the block sums after the grid-wide sync (perhaps in one block).\n\nIf you search around the CUDA tag you can find examples of all these, and discussions of their pros and cons.  To see how the main kernel you posted is used for a complete reduction, look at the parallel reduction sample code.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Groupby reduction in OpenCL?\r\n                \r\nI want to implement a groupby reduction in OpenCL. For example, the input\n\n```\na1 a2 a3 b1 b2 c3 c4\n```\n\n\nwould produce\n\n```\na6 b3 c7\n```\n\n\nThe C pseudocode looks like this:\n\n```\nint data[n][2], result[n][2], result_count = -1, \n    added = 0, group = data[0][0];\nfor (int i = 0; i < n; i++) { \n  if (group == data[i][0]) {\n    added += data[i][1];\n  } else {\n    result[++result_count][0] = group;\n    result[result_count][1] = added;\n    group = data[i][0];\n    added = 0;\n  }\n} \nreturn result, result_count;\n```\n\n\nThe only standard algorithm I know which goes in this direction is parallel reduction; however, it reduces to one number and not to a buffer of added values by group. I am not sure if parallel reduction could work with a dynamic result buffer (e.g. in local memory) and still be efficient in terms of performance.\n    ", "Answer": "\r\nA solution by Hashing \n\nPhase 1)  A hashing scheme  can be used to hash the group value to a location, then an atomic add can sum the contents of the second value.  \n\nPhase 2) A prefix sum  scan algorithm pass over the hash table to compact it.\n\nPhase 3)  Optionally sort the results\n\nA solution by Sorting \n\nPhase 1) Sort the data on the group value\n\nPhase 2) Use a reduction to sum each group\n\nPhase 3) A prefix sum scan  to compact the sums\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction Expression in Java? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nIs there a module in Java to define a language and make the reduction of expressions as PLT/Redex module of Racket?\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Wikipedia rainbow tables entry\r\n                \r\nWikipedia page for rainbow tables says:\n\n\"this use of multiple reduction functions approximately doubles the speed of lookups.\"\n\nAssuming the \"Average\" position in the chain, we take a hash and run it through a 9 iteration chain...\n\nThe original table runs it through 4 reductions and 4 hashes and finds the end of the chain, then looks it up for another 5 hashes 5 reductions... total 9 hashes 9 reductions\n\nThe rainbow table runs it through Rk-1, Rk-2, Rk-3, and Rk-4 calculations to find the end of the chain, then another 5 hashes 5 reductions to get the plaintext: total 15 hashes 15 reductions...\n\nWhat am I missing here? By my math the only time a rainbow lookup is even the same speed as a normal table is when the hash just happens to be at the very end of the chain... In fact the RT should be incrementally slower the further towards the beginning the hash lies...\n\nA 5k chain with the hash at the beginning should be approx 2500 times slower with rainbow tables than with normal hash tables...\n\nAm I missing something or did Wikipedia make a mistake? (The paper referenced on that page (Page 13) would also be wrong, so I'm leaning towards the former)\n    ", "Answer": "\r\nThe purpose of rainbow tables isn't to necessarily be faster but to reduce space.\nRainbow tables trade speed for size.  \n\nStoring hashes for all possible 10 digit passwords for example would be prohibitively expensive in terms of disk space.  Also you need to consider that since the dictionary space is so large it will require significant paging (very slow operation).\n\nRainbow tables are more CPU intensive but they are much much much smaller requiring less disk space and also allowing more of the potential dictionary space in memory at one time. Keep in mind that means in the real world higher potential performance on large dictionary spaces due to less paging (disk reads are prohibitively slow).\n\nHere is a better illustrated example:\nhttp://kestas.kuliukas.com/RainbowTables/\n\nOf course this is all academic.  Rainbow tables provide no value against well designed security systems.\n1) Use a cryptographically secure algorithm (no \"roll your own\")\n2) Use a key derivation function (with thousands of iterations) to slow attackers hash throughput.\n3) Use large (32 to 64 bit) random salt.  Rainbow tables can no longer be precomputed, nor can that computation be used for any other system (unless they happen to share same salt.\n4) If possible use different salt per record thus making rainbow table completely invalid.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Spam Prevention/Reduction - Contact Form?\r\n                \r\nI want to add a simple Contact form to my web site so that customers can contact me easily.\n\n```\n<form>\n    NAME\n    <input type='text' name='name' />\n    EMAIL\n    <input type='text' name='email' />\n    MESSAGE\n    <textarea name='message' />\n    <input type='submit' />\n</form>\n```\n\n\nThis form would simply email me the customers message.\n\nBut, I also want to reduce (not, I'm not saying eliminate but at least reduce), SPAM.\n\nI've looked into using CAPTCHAs but, ultimately, I don't want to hinder the customer with having to fill out extra information.\n\nAny ideas of a good simple spam prevention/reduction method I could use for my Contact form.\n    ", "Answer": "\r\nA very simple trick I've been using with a surprisingly good success rate is this: Provide a text field that is hidden from human users with ```\nstyle=\"display: none\"```\n, but with an enticing name like ```\nemail```\n. Most bots will fill in something in this field, but humans can't see it so they wont. At the server, just make sure the field is empty, else treat the submission as spam.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction Expression in Java? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nIs there a module in Java to define a language and make the reduction of expressions as PLT/Redex module of Racket?\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Highlight reduction in images\r\n                \r\nI am trying to reduce the highlights in an image caused by a high intensity light source.\nI tried various softwares and I found that \"Highlight Reduction\" works for me.\nBut I am not able to understand the actual processing behind a Highlight Reduction.\nCan anybody please help me regarding that ?\n    ", "Answer": "\r\nPerhaps http://answers.opencv.org/question/92887/specular-highlights-detection/ may give some hints. At least 3 papers are cited there:\n\n\n  Searching around the literature I found the following 3 interesting papers: Real-time\n  highlight removal using intensity ratio, Efficient and Robust Specular Highlight Removal\n  and Fast and High Quality Highlight Removal from A Single Image. The first two that \n  they provide the code as well gave not that good results.\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Java 8: stop reduction operation from examining all Stream elements\r\n                \r\nI am trying to understand if there is a way to terminate reduction operation without examining the whole stream and I cannot figure out a way.\n\nThe use-case is roughly as follows: let there be a long list of ```\nInteger```\ns which needs to be folded into an ```\nAccumulator```\n. Each element examination is potentially expensive, so within the ```\nAccumulator```\n, I perform a check on the incoming ```\nAccumulator```\n to see if we even need to perform expensive operation - if we don't, then I simply return the accumulator.\n\nThis is obviously a fine solution for small(er) lists but huge lists incur unnecessary stream element visiting costs I'd like to avoid.\n\nHere's a code sketch - assume serial reductions only.\n\n```\nclass Accumulator {\n    private final Set<A> setA = new HashSet<>;\n    private final Set<B> setB = new HashSet<>;\n}\n\nclass ResultSupplier implements Supplier<Result> {\n\n    private final List<Integer> ids;\n\n    @Override\n    public Result get() {\n        Accumulator acc = ids.stream().reduce(new Accumulator(), f(), (x, y) -> null);\n\n        return (acc.setA.size > 1) ? Result.invalid() : Result.valid(acc.setB);\n    }\n\n    private static BiFunction<Accumulator, Integer, Accumulator> f() {\n        return (acc, element) -> {\n            if (acc.setA.size() <= 1) {\n                // perform expensive ops and accumulate results\n            }\n            return acc;\n        };\n    }\n}\n```\n\n\nIn addition to having to traverse the whole ```\nStream```\n, there is another fact I dislike - I have to check the same condition twice (namely, ```\nsetA```\n size check).\n\nI have considered ```\nmap()```\n and ```\ncollect()```\n operations but they just seemed like more of the same and didn't find they materially change the fact that I just can't finish the fold operation without examining the entire stream.\n\nAdditionally, my thinking is that imaginary ```\ntakeWhile(p : (A) => boolean)```\n Stream API correspondent would also buy us nothing, as the terminating condition depends on the accumulator, not stream elements per se.\n\nBear in mind I am a relative newcomer to FP so - is there a way to make this work as I expect it? Have I set up the whole problem improperly or is this limitation by design?\n    ", "Answer": "\r\nInstead of starting with ```\nids.stream()```\n you can\n\n\nuse ```\nids.spliterator()```\n\nwrap resulting spliterator into custom spliterator that has a volatile boolean flag\nhave the custom spliterator's ```\ntryAdvance```\n return false if the flag is changed \nturn your custom spliterator into a stream with ```\nStreamSupport.stream(Spliterator<T>, boolean)```\n\ncontinue your stream pipeline as before\nshut down the stream by toggling the boolean when your accumulator is full\n\n\nAdd some static helper methods to keep it functional.\n\nthe resulting API could look about this\n\n```\nAccumulator acc = terminateableStream(ids, (stream, terminator) ->\n   stream.reduce(new Accumulator(terminator), f(), (x, y) -> null));\n```\n\n\n\n  Additionally, my thinking is that imaginary takeWhile(p : (A) => boolean) Stream API correspondent would also buy us nothing\n\n\nIt does work if the condition is dependent on the accumulator state and not on the stream members. That's essentially the approach i've outlined above.\n\nIt probably would be forbidden in a ```\ntakeWhile```\n provided by the JDK but a custom implementation using spliterators is free to take a stateful approach. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Coordinates reduction python\r\n                \r\nHow can I calculate position (ra, dec) of star for observer at some station (longitude, latitude) on specific date and time? I need full coordinates reduction with all elements included in calculation (proper motion of star, atmospheric pressure and temperature... )\n\nI tried with pyephem but I am not sure can I finish.\n\n```\nimport ephem\n\npolaris = ephem.readdb(\"Polaris,f|M|F7,2:31:48.704,89:15:50.72,2.02,1\")\n\npolaris.compute('2016/3/1 16:22:56')\n\nprint polaris.a_ra \nprint polaris.a_dec\n```\n\n\nI also tried with astroplan and I think that I am closer to solution, but still don't know how to get coordinates after reduction and add proper motion. \n\n```\nimport astropy.units as u\nfrom astropy.coordinates import EarthLocation\nfrom astropy.coordinates import SkyCoord\nfrom pytz import timezone\nfrom astroplan import Observer\nfrom astropy.time import Time\nfrom astroplan import FixedTarget\nimport numpy as np\nimport astropy.units as u\nfrom astroplan.plots import plot_sky\nfrom astroplan.plots import plot_parallactic\nfrom astroplan.plots import plot_airmass\nimport matplotlib.pyplot as plt\nfrom astroplan import FixedTarget\n\n\nlongitude = '21d33m20.4s'\nlatitude = '+43d08m24.6s'\nelevation = 1150 * u.m\ntime = Time('2015-06-16 12:00:00')\n\nlocation = EarthLocation.from_geodetic(longitude, latitude, elevation)\n\nobserver = Observer(name='Name',\n               location=location,\n               pressure=0.615 * u.bar,\n               relative_humidity=0.11,\n               temperature=0 * u.deg_C,\n               timezone=timezone('Europe/Belgrade'),\n               description=\"..\")\n\n\n\ncoordinates = SkyCoord('2h31m48.704s', '89d15m50.72s', frame = 'icrs')\npolaris = FixedTarget(name='Polaris', coord=coordinates)\n\nplot_airmass(polaris, observer, time)\n\nax = plt.gca()\nbox = ax.get_position()\nax.set_position([box.x0, box.y0, box.width * 0.8, box.height * 0.8])\n\nplt.legend(loc=1, bbox_to_anchor=(1.35, 1))\nplt.show()\n```\n\n    ", "Answer": "\r\nYou might want to try Skyfield, the successor to PyEphem which has nearly reached 1.0. While PyEphem's underlying library does not store proper motions, but instead has to move all the stars during input if it is going to apply proper motion, Skyfield does it dynamically based on the motion stored in the ```\nStar```\n object itself. The reduction you are considering would look something like this in Skyfield:\n\n```\nfrom skyfield.api import load, Star\nts = load.timescale()\nt = ts.utc(2016, 4, 16, 15, 30)\nplanets = load('de421.bsp')\nearth = planets['earth']\nboston = earth.topos(longitude_degrees=(21, 33, 20.4),\n                     latitude_degrees=(+43, 8, 24.6))\nbarnard = Star(ra_hours=(17, 57, 48.49803),\n               dec_degrees=(4, 41, 36.2072),\n               ra_mas_per_year=-798.71,\n               dec_mas_per_year=+10337.77,\n               parallax_mas=545.4,\n               radial_km_per_s=-110.6)\nastrometric = boston.at(t).observe(barnard)\nra, dec, distance = astrometric.radec()\nprint(ra)\nprint(dec)\napparent = astrometric.apparent()\nra, dec, distance = apparent.radec()\nprint(ra)\nprint(dec)\nalt, az, distance = apparent.altaz(temperature_C=25.0,\n                                   pressure_mbar=1013.25)\nprint(alt)\nprint(az)\n```\n\n\nFurther documentation is at http://rhodesmill.org/skyfield/ if you are curious!\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Sum reduction with CUB\r\n                \r\nAccording to this article, sum reduction with CUB Library should be one of the fastest way to make parallel reduction. As you can see in a code  fragment below, the execution time is measure excluding first ```\ncub::DeviceReduce::Reduce(temp_storage, temp_storage_bytes, in, out, N, cub::Sum());```\n I assume that it's something connected with memory preparation and when we reduce several times the same data it isn't neccesary to call it every time but when I've got many different arrays with the same number of elements and type of data do I have to do it every time? If the answer is yes, it means that usage of CUB Library becomes pointless.\n\n```\n  size_t temp_storage_bytes;\n  int* temp_storage=NULL;\n  cub::DeviceReduce::Reduce(temp_storage, temp_storage_bytes, in, out, N, cub::Sum());\n  cudaMalloc(&temp_storage,temp_storage_bytes);\n\n  cudaDeviceSynchronize();\n  cudaCheckError();\n  cudaEventRecord(start);\n\n  for(int i=0;i<REPEAT;i++) {\n    cub::DeviceReduce::Reduce(temp_storage, temp_storage_bytes, in, out, N, cub::Sum());\n  }\n  cudaEventRecord(stop);\n  cudaDeviceSynchronize();\n```\n\n    ", "Answer": "\r\n\n  I assume that it's something connected with memory preparation and when we reduce several times the same data it isn't neccesary to call it every time\n\n\nThat's correct.\n\n\n  but when I've got many different arrays with the same number of elements and type of data do I have to do it every time? \n\n\nNo, you don't need to do it every time.  The sole purpose of the \"first\" call to  ```\ncub::DeviceReduce::Reduce```\n (i.e. when ```\ntemp_storage=NULL```\n) is to provide the number of bytes required for the temporary storage needed by CUB.  If the type and size of your data does not change, there is no need to re-run either this step or the subsequent ```\ncudaMalloc```\n operation.  You can simply call ```\ncub::DeviceReduce::Reduce```\n again (with ```\ntemp_storage```\n pointing to the previous allocation provided by ```\ncudaMalloc```\n) on your \"new\" data, as long as the size and type of data is the same.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "When is the reduction needed?\r\n                \r\nI've written this code which reads a Matrix and it basically sums the values of the matrix... But my question would be, since I've tried doing the pragma in different ways, I found that the ```\nreduction (+:sum)```\n wouldn't be necessary, but, I just don't know why, I might have missed the actual sense of the reduction system in this case. This would be the alternative: ```\n#pragma omp parallel for private(i, j) reduction (+:sum)```\n\n\nAnd this would be the code:\n\n```\n#include <stdio.h>\n#include <math.h>\n#include <omp.h>\n#include <unistd.h>\n\n\nint main ()\n{\n\n    printf(\"===MATRIX SUM===\\n\");\n    printf(\"N ROWS: \");\n    int i1; scanf(\"%d\",&i1);\n    printf(\"M COLUMNS: \");\n    int j1; scanf(\"%d\",&j1);\n    int matrixA[i1][j1];\n\n    int i, j;\n\n    for(i = 0; i < i1; i++){\n        for (j = 0; j < j1; j++){\n            scanf(\"%d\",&matriuA[i][j]);\n        }\n    }\n\n    printf(\"\\nMATRIX A: \\n\");\n    for (i = 0; i < i1; i++){\n        for (j = 0; j < j1; j++){\n            printf(\"%d \", matrixA[i][j]);\n        }\n        printf(\"\\n\");\n    }\n    int sum = 0;\n    #pragma omp parallel for private(i, j)\n        for (i = 0; i < i1; i++)\n            for (j = 0; j < j1; j++){\n                sum += matrixA[i][j];\n           }\n\n\n    printf(\"\\nTHE RESULT IS: %d\", sum);\n\n    return 0;\n}\n```\n\n\nAnd, I would like to ask, if there would be like, a better solution for the pragma reduction since I read that's the most efficient way.\n    ", "Answer": "\r\nThe code you posted is not correct without the reduction clause.\n\n```\nsum += matrixA[i][j];\n```\n\n\nWill cause a classic race condition when executed by multiple threads in parallel. Sum is a shared variable, but ```\nsum += ...```\n is not an atomic operation.\n\n```\n(sum is initially 0, all matrix elements 1)\nThread 1                     |  Thread 2\n-----------------------------------------------------------\ntmp = sum + matrix[0][0] = 1 |\n                             | tmp = sum + matrix[1][0] = 1\nsum = tmp = 1                |\n                             | sum = tmp = 1 (instead of 2)\n```\n\n\nThe reduction fixes exactly this. With reduction, the loop will work on an implicit thread-local copy of the ```\nsum```\n variable. At the end of the region, the original ```\nsum```\n variable will be set to the sum of all thread-local copies (in a correct way without race-conditions).\n\nAnother solution would be to mark the ```\nsum += ...```\n as atomic operation or critical section. That, however has a significant performance penalty.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reflection reduction in image\r\n                \r\nLighting problems are common and difficult.  How does one detect and reduce light reflection to save more information from an image? I have tried several methods with OpenCV and Python without luck.\n\n(Image with reflection)\n\n\n\n(Image without reflection)\n\n\n\nI tried converting to HSV color space, and apply Histogram Equalization to the V channel, with Clahe equalization:\n\n```\nimport cv2\nimport numpy as np\n\nimage = cv2.imread('glare.png')\n\nhsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\nh, s, v = cv2.split(hsv_image)\n\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\nv = clahe.apply(v)\n\nhsv_image = cv2.merge([h, s, v])\nhsv_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB)\n\ncv2.imwrite('clahe_h.png', hsv_image)\n```\n\n\nresults:\n\n\n\nAs well I tried thresholding to find bright pixels and than use Image Inpainting to replace them with neighbouring pixels.\n\n```\nimport cv2\nimport numpy as np\n\nimage = cv2.imread('glare.png')\n\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nblurred = cv2.GaussianBlur(gray, (3,3), 0)\nthresh = cv2.threshold(blurred, 225, 255, cv2.THRESH_BINARY)[1]\n\ndst_TELEA = cv2.inpaint(image,thresh,3,cv2.INPAINT_TELEA)\ncv2.imwrite('after_INPAINT.png',dst_TELEA)\n```\n\n\nresults:\n(after threshold)\n\n\n\n    ", "Answer": "\r\nThere's no general method for effective glare removal.\n\nHSV + CLAHE is a good and common start, but industry methods \"cheat\" by assuming some information about the subject (human face, fruit on a conveyor belt, retina through an ophthalmoscope), and sometimes information about the lighting (white ceiling light with very sharp edges, for the images in this question).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "View Reduction Steps in Haskell\r\n                \r\nIs there any way to view the reduction steps in haskell, i.e trace the recursive function calls made? For example, chez scheme provides us with trace-lambda. Is there an equivalent form in Haskell?\n    ", "Answer": "\r\nYou could try inserting ```\nDebug.Trace.trace```\n in places you want to trace, but this has the tendency of (a) producing wildly out-of-order output, as your trace statement may belong to a thunk that isn't evaluated until far far away from the original call, and (b) changing the runtime behavior of your program, if tracing requires evaluating things that wouldn't otherwise have been evaluated (yet).\nIs this for debugging?  If so...\n\nHat modifies your source code to output tracing which can be viewed after running.  The output should be pretty close to what you want: the example on their homepage is\n\nFor example, the computation of the faulty program\n```\nmain = let xs :: [Int]\n           xs = [4*2,5 `div` 0,5+6]\n       in  print (head xs,last' xs)\n\nlast' (x:xs) = last' xs\nlast' [x] = x\n```\n\ngives the result\n```\n(8, No match in pattern.\n```\n\nand the Hat viewing tools can be used to explore its behaviour as follows:\n\nHat-stack\n\nFor aborted computations, that is computations that terminated with an error message or were interrupted, hat-stack shows in which function call the computation was aborted. It does so by showing a virtual stack of function calls (redexes). Thus, every function call shown on the stack caused the function call above it. The evaluation of the top stack element caused the error (or during its evaluation the computation was interrupted). The stack shown is virtual, because it does not correspond to the actual runtime stack. The actual runtime stack enables lazy evaluation whereas the virtual stack corresponds to a stack that would be used for eager (strict) evaluation.\nUsing the same example program as above, hat-stack shows\n```\n$ hat-stack Example\nProgram terminated with error:\n        No match in pattern.\nVirtual stack trace:\n(Last.hs:6)     last' []\n(Last.hs:6)     last' [_]\n(Last.hs:6)     last' [_,_]\n(Last.hs:4)     last' [8,_,_]\n(unknown)       main\n$\n```\n\n\n\nThese days, GHCi (≥6.8.1) also comes with a debugger:\n```\n$ ghci -fbreak-on-exception\nGHCi, version 6.10.1: http://www.haskell.org/ghc/  :? for help\nLoading package ghc-prim ... linking ... done.\nLoading package integer ... linking ... done.\nLoading package base ... linking ... done.\nPrelude> :l Example.hs\n[1 of 1] Compiling Main             ( Example.hs, interpreted )\n\nExample.hs:5:0:\n    Warning: Pattern match(es) are overlapped\n             In the definition of `last'': last' [x] = ...\nOk, modules loaded: Main.\n*Main> :trace main\n(8,Stopped at <exception thrown>\n_exception :: e = _\n[<exception thrown>] *Main> :back\nLogged breakpoint at Example.hs:(5,0)-(6,12)\n_result :: t\n[-1: Example.hs:(5,0)-(6,12)] *Main> :hist\n-1  : last' (Example.hs:(5,0)-(6,12))\n-2  : last' (Example.hs:5:15-22)\n-3  : last' (Example.hs:(5,0)-(6,12))\n-4  : last' (Example.hs:5:15-22)\n-5  : last' (Example.hs:(5,0)-(6,12))\n-6  : last' (Example.hs:5:15-22)\n-7  : last' (Example.hs:(5,0)-(6,12))\n-8  : main (Example.hs:3:25-32)\n-9  : main (Example.hs:2:17-19)\n-10 : main (Example.hs:2:16-34)\n-11 : main (Example.hs:3:17-23)\n-12 : main (Example.hs:3:10-33)\n<end of history>\n[-1: Example.hs:(5,0)-(6,12)] *Main> :force _result\n*** Exception: Example.hs:(5,0)-(6,12): Non-exhaustive patterns in function last'\n\n[-1: Example.hs:(5,0)-(6,12)] *Main> :back\nLogged breakpoint at Example.hs:5:15-22\n_result :: t\nxs :: [t]\n[-2: Example.hs:5:15-22] *Main> :force xs\nxs = []\n```\n\nWhile not as nice, it has the benefit of being easily available, and being usable without recompiling your code.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "reduction in the elements of list\r\n                \r\nI have natural number list. After reduction of one element from the list, I want to prove the following relation.\n```\nTheorem reduce_elements:forall (n:nat) (l:list nat),\n (length (n :: l) =? 0) = false->\n (length l =? 0) = false.\n```\n\n    ", "Answer": "\r\nThis statement does not hold:\n```\nRequire Import Coq.Arith.Arith.\n\nTheorem reduce_elements:forall (n:nat) (l:list nat),\n (length (n :: l) =? 0) = false->\n (length l =? 0) = false.\nAdmitted.\n\nGoal False.\npose proof (reduce_elements 0 nil eq_refl).\nsimpl in H.\ncongruence.\nQed.\n```\n\nI've noticed you've come to Stack Overflow a few times asking help to prove false statements. I suggest you try to sketch those proofs on paper before trying to solve them with Coq: it will help you understand your problem better.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Eta reduction in haskell\r\n                \r\nI tried for a long time to reduct this function in haskell, I want to express for example:\n\n```\nmySum x y = x + y\nmySum x y = (+) x y\nmySum x = (+) x\nmySum = (+) -- it's Messi's goal! \n```\n\n\nMy function it a little more complex, but I really can't do it, I was looking out here and there, and I know there are some techniques, like modify the right side, and use ```\nflip```\n. I tried and I got stuck here:\n\n```\nzipWith' :: (a -> b -> c) -> [a] -> [b] -> [c]\nzipWith' f x y  = map  (uncurry f) (zip x y) \n```\n\n\nSteps:\n\n```\nzipWith' f x y  = map  (uncurry f) (zip x y) \nzipWith' f x y  = flip  map  (zip x y) (uncurry f)\nzipWith' f x y  = flip  map  (zip x y) $ uncurry f\n```\n\n\nand then I don't know how to continue...\n\nI'm looking for an answer that could explain step by step how to achieve the \"Messi's goal\", I know is a lot to ask, so I will add as soon as I can a bounty to thank the effort\n    ", "Answer": "\r\n```\nzipWith' f x y = map (uncurry f) (zip x y)\n```\n\n\nRewrite application to composition and eta-reduce:\n\n```\n-- \\y -> let g = map (uncurry f); h = zip x in (g . h) y\n-- let g = map (uncurry f); h = zip x in g . h\n\nzipWith' f x = map (uncurry f) . zip x\n```\n\n\nRewrite infix to prefix:\n\n```\n-- g . h = (.) g h\n\nzipWith' f x = (.) (map (uncurry f)) (zip x)\n```\n\n\nRewrite application to composition and eta-reduce:\n\n```\n-- \\x -> let g = (.) (map (uncurry f)); h = zip in (g . h) x\n-- let g = (.) (map (uncurry f)); h = zip in g . h\n\nzipWith' f = (.) (map (uncurry f)) . zip\n```\n\n\nRewrite infix to prefix:\n\n```\n-- g . h = (.) g h\n\nzipWith' f = (.) ((.) (map (uncurry f))) zip\n```\n\n\nUse ```\nflip```\n to move ```\nf```\n to the right-hand side:\n\n```\n-- flip f x y = f y x\n\nzipWith' f = flip (.) zip ((.) (map (uncurry f)))\n```\n\n\nRewrite application to composition:\n\n```\n-- g (h (i x)) = (g . h . i) x\n\nzipWith' f = flip (.) zip (((.) . map . uncurry) f)\n```\n\n\nRewrite application to composition and eta-reduce:\n\n```\n-- \\f -> let g = flip (.) zip; h = (.) . map . uncurry in (g . h) f\n-- let g = flip (.) zip; h = (.) . map . uncurry in g . h\n\nzipWith' = (flip (.) zip) . ((.) . map . uncurry)\n```\n\n\nRemove redundant parentheses:\n\n```\nzipWith' = flip (.) zip . (.) . map . uncurry\n```\n\n\nAnd simplify to infix if you like:\n\n```\nzipWith' = (. zip) . (.) . map . uncurry\n```\n\n\nThis result isn’t very readable, though.\n\n\n\nOften when writing fully point-free code, you want to take advantage of the ```\n->```\n applicative and arrow combinators from ```\nControl.Arrow```\n. Rather than trying to write a function like ```\n\\ f x y -> ...```\n, you can start by grouping the arguments into tuples to make them easier to rearrange and pipe around. In this case I’ll use ```\n\\ (f, (x, y)) -> ...```\n\n\n```\n\\ (f, (x, y)) -> map (uncurry f) (zip x y)\n```\n\n\nWe can eliminate the unpacking of ```\n(x, y)```\n by applying ```\nuncurry```\n to ```\nzip```\n:\n\n```\n\\ (f, (x, y)) -> map (uncurry f) (uncurry zip (x, y))\n\\ (f, xy) -> map (uncurry f) (uncurry zip xy)\n```\n\n\nNow we have a simple case: applying two functions (```\nuncurry```\n and ```\nuncurry zip```\n) to two arguments (```\nf```\n and ```\nxy```\n), then combining the results (with ```\nmap```\n). For this we can use the ```\n***```\n combinator from ```\nControl.Arrow```\n, of type:\n\n```\n(***) :: Arrow a => a b c -> a b' c' -> a (b, b') (c, c')\n```\n\n\nSpecialised to functions, that’s:\n\n```\n(***) @(->) :: (b -> c) -> (b' -> c') -> (b, b') -> (c, c')\n```\n\n\nThis just lets us apply a function to each element of a pair. Perfect!\n\n```\nuncurry *** uncurry zip\n  :: (a -> b -> c, ([x], [y])) -> ((a, b) -> c, [(x, y)])\n```\n\n\nYou can think of ```\nuncurry f```\n as combining the elements of a pair using the function ```\nf```\n. So here we can combine the results using ```\nuncurry map```\n:\n\n```\nuncurry map . (uncurry *** uncurry zip)\n  :: (a -> b -> c, ([a], [b])) -> [c]\n```\n\n\nAnd you can think of ```\ncurry```\n as turning a function on tuples into a multi-argument function. Here we have two levels of tuples, the outer ```\n(f, xy)```\n and the inner ```\n(x, y)```\n. We can unpack the outer one with ```\ncurry```\n:\n\n```\ncurry $ uncurry map . (uncurry *** uncurry zip)\n  :: (a -> b -> c) -> ([a], [b]) -> [c]\n```\n\n\nNow, you can think of ```\nfmap f```\n in the ```\n->```\n applicative as “skipping over” the first argument:\n\n```\nfmap @((->) _) :: (a -> b) -> (t -> a) -> t -> b\n```\n\n\nSo we can unpack the second tuple using ```\nfmap curry```\n:\n\n```\nfmap curry $ curry $ uncurry map . (uncurry *** uncurry zip)\n  :: (a -> b -> c) -> [a] -> [b] -> [c]\n```\n\n\nAnd we’re done! Or not quite. When writing point-free code, it pays to break things out into many small reusable functions with clearer names, for example:\n\n```\nzipWith' = untuple2 $ combineWith map apply zipped\n  where\n    untuple2 = fmap curry . curry\n    combineWith f g h = uncurry f . (g *** h)\n    apply = uncurry\n    zipped = uncurry zip\n```\n\n\n\n\nHowever, while knowing these techniques is useful, all this is just unproductive trickery that’s easy to get lost in. Most of the time, you should only use point-free style in Haskell when it’s a clear win for readability, and neither of these results is clearer than the simple original version:\n\n```\nzipWith' f x y = map (uncurry f) (zip x y)\n```\n\n\nOr a partially point-free version:\n\n```\nzipWith' f = map (uncurry f) .: zip\n  where (.:) = (.) . (.)\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "What's the difference between the worst case and average case of a problem? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     This question does not appear to be about programming within the scope defined in the help center.\r\n                \r\n                    \r\n                        Closed 2 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI'm studying the reduction about some hard problems on the lattice. What's the meaning of \"worst case to average case reduction\" ? For example, the paper \"Worst-case to Average-case Reductions based on Gaussian Measures\" gives the reduction from the worst case INCGDD problem to average case SIS problem, what does it mean?\n    ", "Answer": "\r\nA problem has average-case time complexity C if there exists an algorithm that solves the problem in C time on average, if the inputs are chosen randomly according to some distribution. Formalizing this is tricky, see here.\nA problem has a worse case to average case reduction if you can show the following: if an algorithm solving the problem with average-case complexity C exists then this algorithm can be applied to also solve the worst-case with the same complexity (modulo a polynomial factor).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP slow reduction\r\n                \r\nI write simple C++ code that compute array reduction sum, but with OpenMP reduction program works slowly. There are two variants of program: one is simplest sum, another - sum of complex math function. In code complex variant is commented.\n\n```\n#include <iostream>\n#include <omp.h>\n#include <math.h>\n\nusing namespace std;\n\n#define N 100000000\n#define NUM_THREADS 4\n\nint main() {\n\n  int *arr = new int[N];\n\n  for (int i = 0; i < N; i++) {\n    arr[i] = i;\n  }\n\n  omp_set_num_threads(NUM_THREADS);\n  cout << NUM_THREADS << endl;\n\n  clock_t start = clock();\n  int sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < N; i++) {\n    // sum += sqrt(sqrt(arr[i] * arr[i])); // complex variant\n    sum += arr[i]; // simple variant\n  }\n\n  double diff = ( clock() - start ) / (double)CLOCKS_PER_SEC;\n  cout << \"Time \" << diff << \"s\" << endl;\n\n  cout << sum << endl;\n\n  delete[] arr;\n\n  return 0;\n}\n```\n\n\nI compile it by ICPC and GCC:\n\n```\nicpc reduction.cpp -openmp -o reduction -O3\ng++ reduction.cpp -fopenmp -o reduction -O3\n```\n\n\nProcessor: Intel Core 2 Duo T5850, OS: Ubuntu 10.10\n\nThere are execution time of simple and complex variants, compiled with and without OpenMP.\n\nSimple variant \"sum += arr[i];\":\n\n```\nicpc\n0.1s without OpenMP\n0.18s with OpenMP\n\ng++\n0.11c without OpenMP\n0.17c with OpenMP\n```\n\n\nComplex variant \"sum += sqrt(sqrt(arr[i] * arr[i]));\":\n\n```\nicpc\n2,92s without OpenMP\n3,37s with OpenMP\n\ng++ \n47,97s without OpenMP\n48,2s with OpenMP\n```\n\n\nIn system monitor I see that 2 cores works in program with OpenMP and 1 core works in program without OpenMP. I'll try several numbers of threads in OpenMP and dont have speedup. I don't understand why reduction is slow.\n    ", "Answer": "\r\nThe function ```\nclock()```\n measures processor time consumed by whole process, so printed time shows sum of time consumed by all threads. If you want to see wall-time (real time elapsed from the begin to the end), use e.g. times() function on the POSIX system\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "parallel reduction technique\r\n                \r\nI have this piece of C++ code and I want to port it to CUDA.\n\n```\nfor (int im = 0; im < numImages; im++)\n{\n    for (p = 0; p < xsize*ysize; p++) \n    {\n        bool ok = false;\n\n        for (f = 0; f < numFeatures; f++)\n        {\n            if (feature[im][f][p] != 0) \n            {\n                ok = true;\n                break;\n            }\n        }\n        if (ok)\n        {         \n            minDist = 1e9;\n            for (i = 0; i < numBins; i++) \n            {\n                dist = 0;\n                for (f = 0; f < numFeatures; f++)\n                {\n                    dist += (float)((feature[im][f][p]-clusterPoint[f][i])*(feature[im][f][p]-clusterPoint[f][i]));\n                }\n\n                if (dist < minDist) \n                {\n                    minDist = dist;\n                    tmp = i;          \n                }\n            }//end for i  \n\n            for (f = 0; f < numFeatures; f++) \n                csum[f][tmp] += feature[im][f][p];\n\n            ccount[tmp]++;\n\n            averageDist[tmp] += sqrt(minDist);\n\n        } // end if (ok)\n    }  //end for p    \n}// end for im\n```\n\n\nI want to calculate ```\ncsum```\n,```\nccount```\n and ```\naverageDist```\n in the GPU. ```\ncsum```\n and ```\naveragedist```\n are floats, ```\nccount```\n is an integer.\n\nIs this a parallel reduction problem?\n    ", "Answer": "\r\nI didn't fully understand what your code should do and I do not know what are approximate values of ```\nnumBins```\n and ```\nnumFeatures```\n as well. Nevertheless, I would make parallel this loop: ```\nfor (p = 0; p < xsize*ysize; p++)```\n, in order to each thread computes its values and stores them in global array. Having these arrays of features and distances you can compute ```\ncsum```\n, ```\nccount```\n and ```\naverageDist```\n using standard parallel reduction.\n\nMain loop over images ```\nfor (int im = 0; im < numImages; im++)```\n is possible to compute by repetitive launching of kernel or it is also possible to make it parallel at once with loop over pixels.\n\nIn a case of ```\nif(ok)```\n is not satisfied enough frequently, warp divergence occurs (see this). Avoiding this you can assign nor one thread to each pixel but one warp and divide remaining computations among the threads within this warp.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Circular Dimensionality Reduction?\r\n                \r\nI want dimensionality reduction such that dimensions it returns are circular. \n\nex) If I reduce 12d data to 2d, normalized between 0 and 1, then I want (0,0) to be as equally close to (.1,.1) as (.9,.9). \n\nWhat is my algorithm? (bonus points for python implementation)\n\nPCA gives me 2d plane of data, whereas I want spherical surface of data.\n\nMake sense? Simple? Inherent problems? Thanks.\n    ", "Answer": "\r\nI think what you ask is all about transformation.\n\nCircular\n\n\n  I want (0,0) to be as equally close to (.1,.1) as (.9,.9).\n\n\nPCA\n\nTaking your approach of normalization what you could do is to\nmap the values in the interval from ```\n[0.5, 1]```\n to ```\n[0.5, 0]```\n\n\nMDS\n\nIf you want to use a distance metric, you could first compute the distances and then do the same. For instance taking the correlation, you could do ```\n1-abs(corr)```\n. Since the correlation is between ```\n[-1, 1]```\n positive and negative correlations will give values close to zero, while non correlated data will give values close to one. Then, having computed the distances you use MDS to get your projection.\n\nSpace\n\n\n  PCA gives me 2d plane of data, whereas I want spherical surface of data.\n\n\nSince you want a spherical surface you can directly transform the 2-d plane to a sphere as I think. A spherical coordinate system with a constant ```\nZ```\n would do that, wouldn't it?\n\nAnother question is then: Is all this a reasonable thing to do?\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "beta reduction on type\r\n                \r\nWhen translate the ```\n(+)```\n operator into lambda calculus, then it would look like:  \n\n```\nλx.λy.x + y\n```\n\n\nthen do beta reduction\n\n```\n(λx.λy.x + y) 1 --substitute x through 1\n(λy.1 + y)      \n(λy.1 + y)    2 --substitute y through 2\n1 + 2\n3               --is the normal form \n```\n\n\nNow, I have a function with following type signature:\n\n```\nf :: (t1 -> t) -> t1 -> t\n```\n\n\nMy questions are:\n\n\nCan I translate the type signature of function ```\nf```\n into lambda\ncalculus(Like I translate the + operator above)?  \nCan I apply beta reduction principle for type too(know as type\nsubstitution)?\n\n\nWith type substitution, I mean as follow:\n\n```\n*Twinplicative> :t id (+)\nid (+) :: Num a => a -> a -> a\n```\n\n\nI did substitute the first parameter of ```\nid```\n, know as ```\na```\n, with the type signature of ```\n(+)```\n operator, then it becomes to ```\nNum a => a -> a -> a```\n\n    ", "Answer": "\r\nYou can use a function the same way as you use a value in lambda calculus. Your example with the ```\n(+)```\n would look like this:\n\n```\n(λf. λx. λy. f x y) (+) 1 2\n~> (λx. λy. (+) x y)  1 2\n~> (λy. (+) 1 y) 2\n~> ((+) 1 2)\n~> 1 + 2\n~> 3\n```\n\n\nYou can substitute your known types, but there to ensure correctness you would have to run the function and see if the types match. \n\nHowever, in pure lambda calculus you can't encode the type signature, that's what System F or Hindley-Milner type system is for. This allows type annotations and you can use a type inference algorithm to check on compile time if everything matches. It narrows the rules for beta reduction by enforcing type substitutions.\n\nA more complete introduction and example code for a type inference algorithm can be found here.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP implementation of reduction\r\n                \r\nI need to implement reduction operation (for each thread the value should be stored in different array entry). However, it runs slower for more threads. Any suggestions?\n\n```\ndouble local_sum[16];.\n//Initializations....\n#pragma omp parallel for shared(h,n,a) private(x, thread_id) \nfor (i = 1; i < n; i++) {\n    thread_id = omp_get_thread_num();\n    x = a  + i* h;\n    local_sum[thread_id] += f(x);\n}\n```\n\n    ", "Answer": "\r\nYou are experiencing the effects of false sharing. On x86 a single cache line is 64 bytes long and therefore holds ```\n64 / sizeof(double)```\n = 8 array elements. When one thread updates its element, the core that it runs on uses the cache coherency protocol to invalidate the same cache line in all other cores. When another thread updates its element, instead or operating directly on the cache, its core has to reload the cache line from an upper-level data cache or from the main memory. This significantly slows down the program execution.\n\nThe simplest solution is to insert padding and thus spread array elements accessed by different threads into distinct cache lines. On x86 that would be 7 ```\ndouble```\n elements. Therefore your code should look like:\n\n```\ndouble local_sum[8*16];\n//Initializations....\n#pragma omp parallel for shared(h,n,a) private(x, thread_id) \nfor (i = 1; i < n; i++) {\n    thread_id = omp_get_thread_num();\n    x = a  + i* h;\n    local_sum[8*thread_id] += f(x);\n```\n\n\n}\n\nDon't forget to take only each 8th element when summing the array at the end (or initialise all array elements to zero).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Controlling token reduction\r\n                \r\nis there a way to controll the reduction operation of a token with ANTLR at runtime.\nFor example, I've an ANTLR grammar that looks like:\n\n```\ns: ( a | b);\na: WORD;\nb: WORD;\nWORD: ('a'..'z')+\n```\n\n\nWhere the exact possible values related to both 'a' and 'b' are known at runtime, i.e. I want to decide at runtime whether to reduce a WORD to 'a' or 'b'.\n    ", "Answer": "\r\nUse a semantic predicate. Unless you plan on adding actions to rules a and b, this example won't be particularly useful.\n\n```\ns:\n   {someBoolFunction();}? a\n | b\n;\na: WORD;\nb: WORD;\nWORD: ('a'..'z')+\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Haskell \"Source reduction\"\r\n                \r\nI'm revising for an upcoming Haskell exam and I don't understand one of the questions on a past paper. Google turns up nothing useful\n\n```\nfst(x, y) = x\nsquare i = i * i\n```\n\n\ni) Source reduce, using Haskells lazy evaluation, the expression:\n\n```\nfst(square(3+4), square 8)\n```\n\n\nii) Source reduce, using strict evaluation, the same expression\n\niii) State one advantage of lazy evaluation and one advantage of strict evaluation\n\nWhat I don't understand is what is source reduction?\n    ", "Answer": "\r\nReduction is a term from the lambda calculus which involves a semantics-preserving transformation that replaces one term with an equivalent term.  For the examples you've given, the most important kind of reductions are\n\n\nReplacement of a name by its definition (an instance of substituting equals for equals).\nBeta-reduction of a function application.\n\n\nBeta-reduction is the fundamental rule in the lambda calculus, and in a pure, lazy language like Haskell, it always preserves semantics.  The beta rule is the one that says:\n\n```\n(\\x. e) m\n```\n\n\ncan be replaced by ```\ne```\n with ```\nm```\n substituted for ```\nx```\n.  (The substitution must avoid \"capturing\" free instances of ```\nx```\n in ```\nm```\n.)\n\nIt's quite possible that your instructor wants you to combine reductions as follows:\n\n\nFind a function application where the function being applied is a name.\nReplace the name with its definition, which will be a lambda abstraction.\nBeta-reduce the application.\nDo this all in one step.\n\n\nNotice you often have a choice about which application to reduce; for example, in the term you give, there are two applications of ```\nsquare```\n and one of ```\nfst```\n that could be reduced in this fashion.  (The application of + can also be reduced, but reduction involving constants requires different rules.)\n\nFrom the questions I see that your instructor wants you to reduce each term repeatedly until it reaches a normal form and that your instructor wants you to demonstrate your understanding of different reduction strategies.  The word \"source\" in \"source reduce\" is superfluous; reduction means manipulation of source terms in some language.  I would have phrased the questions as follows:\n\n\nUsing the reduction strategy that corresponds to Haskell's lazy evaluation, reduce the following expression to weak head normal form.  Show each step in the sequence of reductions.\nUsing the reduction strategy that corresponds to evaluation in a strict functional language, reduce the following expression to head normal form.\n\n\nI probably would have chosen to be less coy and just name the reduction strategies: a call-by-need reduction strategy and a call-by-value reduction strategy.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction in georeferenced tweets\r\n                \r\nWhile streaming twitter data, I found that there has been an obvious reduction in geo-referenced tweets (tweets with lat and lon). Is it because of the Foursquare information integration? Or are there any other issues? \n\nMany thanks!\n    ", "Answer": "\r\nI worked on a Social Analytics by Location application last year. We sampled tweets from twitter with the intension of using the geolocation attributes to determine the sentiment by area. Unfortunately we found that only between 10-15% of tweets (based on our own findings) were actually geo-tagged which was not enough to provide an accurate depiction of sentiment. Instead we opted for using location indicative hashtags.\n\nIn saying that it depends on the sample size. We were trying to determine sentiment in areas such as buildings which had a small amount of active twitter users. If your aim is to find tweets within much larger areas such as Towns/Cities/Countries then 10-15% is probably enough for your needs.\n\nTo answer your original question: users are generally private unless they explicitly intend to checkin somewhere and so my guess is that the 10-15% of tweets that are geo-located are a result of users forgetting to disable geo-location or using a new/infrequently used device where it is not disabled. It can also be attributed to foursquare information integration as I'm sure users just overlook the fact that foursquare provides twitter with the geolocation information.\n\nThis article is an interesting read. It outlines an application developed by the University of SoCal that can help users identify if they are giving away sensitive/private location information with their tweets.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Queue reduction algorithm?\r\n                \r\nI have a queue of activities shared between clients, capturing user activity and executed by a robot on the other site. An example of activites might be like:\n\n```\nCREATE FOLDER  /docs\nCREATE FILE    /docs/journal.txt\nDELETE FILE    /docs/blog.txt\nMOVE   FOLDER  /docs/images /docs/photos\n...\n```\n\n\nOften there are activites that can be reduced to a single one, or none. For instance:\n\n```\nCREATE FOLDER /docs\nRENAME FOLDER /docs /documents\n```\n\n\nCan be simply changed to:\n\n```\nCREATE FOLDER /documents\n```\n\n\nAnd something like:\n\n```\nCREATE FOLDER /docs\nRENAME FOLDER /documents\nDELETE FOLDER /documents\n```\n\n\nCan be removed entirely from the queue.\n\nThis kind of reduction/optimization seems like a very generic problem, and before attacking it I'd like to try some generic solution. It looks like a pathfinding optimization problem.\n\nAny ideas?\n    ", "Answer": "\r\nI don't know of any library or framework that would do this for you. On the other hand, you would have to specify the logic behind it yourself, and as I see it, that would be the bulk of the work anyway.\n\nHere's the approach I would take:\n\n\nDo a topological sort of the actions (rename folder \"depends on\" create folder and so on...)\nEach command with no dependencies represents a \"root\" in a dependency tree.\nCollapse these trees recursively starting from each root.\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Loss reduction in tf.distributed.MirroredStrategy()\r\n                \r\nI'm confused regarding using distributed strategy and the correct way of reduction in loss functions.\nI implemented a U-Net using tf.distribute.MirroredStrategy(). Everything works fine using default loss BinaryCrossentropy as follows:\n```\nwith strategy.scope():\n        model = build_network((size, size, 3), num_classes)\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=args.learning_rate),\n                      loss=tf.keras.losses.BinaryCrossentropy()])\n```\n\nHowever, I want to create custom loss functions. To start with, I wrote a wrapper containing BinaryCrossentropy, to get familiar with the correct way of using the reduction methods. I followed the instructions in https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function\nand used tf.nn.compute_average_loss in order to divide by the global batch_size.\n```\ndef loss_functions(loss_spec):\n    if loss_spec == 'cross_entropy':\n        def c_loss(truth, pred):\n            my_loss = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(truth, pred)\n            my_loss = tf.math.reduce_mean(my_loss, axis=[1, 2])  # to compute average across the two image dimensions\n            my_loss = tf.nn.compute_average_loss(my_loss)   # sums up all items and divides by batch size\n            return my_loss\n    return c_loss\n```\n\nwhich is called in the following way:\n```\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=args.learning_rate),\n                      loss=utils.loss_functions('cross_entropy')])\n```\n\nIt also works, but I realised a difference of factor of number or replica compared to using tf.keras.losses.BinaryCrossentropy(). I.e., when using two kernels, using BinaryCrossentropy() directly yields a loss twice as large as my custom loss. Thus, to geht the same, I would need to divide by the batch size per replica instead of global batch size, i.e., the way it should NOT be done according to the documentation.\nHowever, the documentation refers to building an own training routine, whereas I am using model.compile() and model.fit() methods.\nCan anybody explain this behaviour to me?\nUPDATE:\nThe use of tf.nn.compute_average_loss or the use of any reduction on the batch axis is not needed when using model.compile() and model.fit() at all - the reduction and scaling is done automatically. However, I still do not know how model.fit() does work internally.\nThanks and cheers, everybody\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Android Noise reduction\r\n                \r\njust want to ask on how can I apply a noise reduction in my recorded audio. Well I already created a recorder but one problem is that I used MediaRecorder instead of AudioRecorder. Now I think about adding the noise reduction by altering my recorded audio then replacing the existing record after recording but don't know how to do it. Is theres a way I can do it while recording or is my plan possible? \n\nMaybe someone can give me a steps on how to do it or a tutorial link can do. But I think it would be best if you can recommend some java jar library I can use for this (opensource and free to use).\n\nI used 3gpp output encoded in AMR_NB for the recorded audio by the way and as mentioned I used MediaRecorder for recording of audio source from mic.\n    ", "Answer": "\r\nmaybe you should use WebRTC's NS module. but it's input is PCM, so you need a little transformat. and you should compile the NS module and wrap it into JNI interface yourself, cus' WebRTC has no such jar libraries right now.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "SSE reduction of float vector\r\n                \r\nHow can I get sum elements (reduction) of float vector using sse intrinsics?\n\nSimple serial code:\n\n```\nvoid(float *input, float &result, unsigned int NumElems)\n{\n     result = 0;\n     for(auto i=0; i<NumElems; ++i)\n         result += input[i];\n}\n```\n\n    ", "Answer": "\r\nTypically you generate 4 partial sums in your loop and then just sum horizontally across the 4 elements after the loop, e.g.\n\n```\n#include <cassert>\n#include <cstdint>\n#include <emmintrin.h>\n\nfloat vsum(const float *a, int n)\n{\n    float sum;\n    __m128 vsum = _mm_set1_ps(0.0f);\n    assert((n & 3) == 0);\n    assert(((uintptr_t)a & 15) == 0);\n    for (int i = 0; i < n; i += 4)\n    {\n        __m128 v = _mm_load_ps(&a[i]);\n        vsum = _mm_add_ps(vsum, v);\n    }\n    vsum = _mm_hadd_ps(vsum, vsum);\n    vsum = _mm_hadd_ps(vsum, vsum);\n    _mm_store_ss(&sum, vsum);\n    return sum;\n}\n```\n\n\nNote: for the above example ```\na```\n must be 16 byte aligned and ```\nn```\n must be a multiple of 4. If the alignment of ```\na```\n can not be guaranteed then use ```\n_mm_loadu_ps```\n instead of ```\n_mm_load_ps```\n. If ```\nn```\n is not guaranteed to be a multiple of 4 then add a scalar loop at the end of the function to accumulate any remaining elements.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Extending a reduction relation\r\n                \r\nWhile taking a look at PLT redex, I wanted to play with simplification rules; so  I defined this minimal language for booleans:\n```\n(define-language B0\n  (b T F (not b)))\n```\n\nI wanted to simplify a chain of ```\n(not (not ...))```\n so I extended the language to deal with contexts and defined a reduction relation to simplify the ```\nnot```\n:\n```\n(define-extended-language B1 B0\n  (C (not C) hole)\n  (BV T F))\n\n(define red0\n  (reduction-relation\n   B1\n   (--> (in-hole C (not T)) (in-hole C F))\n   (--> (in-hole C (not F)) (in-hole C T))))\n```\n\nNow I wanted to extend my language to boolean equations and to allow ```\nnot```\n-simplification at each side of the equation, so I defined:\n```\n(define-extended-language B2 B1\n  (E (= C b) (= b C)))\n```\n\nhoping that:\n```\n(define red1\n  (extend-reduction-relation red0 B2))\n```\n\nwill do the thing.\nBut no: ```\nred1```\n can reduce ```\n(not (not (not F)))))```\n but not ```\n(= (not T) F)))```\n\nAm I doing something really silly here?\n    ", "Answer": "\r\nThe problem with ```\nred1```\n is that it only contains the rules of ```\nred0```\n which use the limited context ```\nC```\n. To make it work as expected you could either add the old rules modified to use ```\nE```\n or make somehow the final extended context have the name ```\nC```\n. One not very tedious approach could be:\n```\n(define-language L)\n\n(define R\n  (reduction-relation L\n    (--> (not T) F)\n    (--> (not F) T)))\n\n(define-language LB\n  (b T F (not b))\n  (C (compatible-closure-context b)))\n\n(define RB (context-closure R LB C))\n\n(define-extended-language LBE LB\n  (e (= b b))\n  (C .... (compatible-closure-context e #:wrt b)))\n\n(define RBE (extend-reduction-relation RB LBE))\n```\n\nNote that this doesn't work in some older versions.\nTwo sources of useful information are this tutorial and of course the redex reference.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Openmp reproducible reduction [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has an answer here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Float related numerical stability issues for parallel reduction\r\n                            \r\n                                (1 answer)\r\n                            \r\n                    \r\n                Closed 6 months ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI have the following example code:\n```\n!$omp threadpriavate(var)\n!$omp parallel do reduction(+:var)\ndo\n    var = var + compilated_floating_point_computation()\nend do\n!$omp end parallel do\nprint *,var\n```\n\nAnd I get slightly different results for var per run, even when I use the same number of threads. I tried to add ```\norder(reproducible:concurrent)```\n openmp clause but got the following compile error:\n```\nError: threadprivate variable 'var' used in a region with 'order(concurrent)' clause```\n.\nIs there any way to use reduction and still maintain floating point reproducibility over same number of threads?\n    ", "Answer": "\r\nIf your computation is considerably more expensive than the addition reduction, you could create an array with the computation results, and sum those sequentially.\nOtherwise, differing results are an intrinsic side-effect of parallelism. Accept that for what it is, use a stable algorithm so that it doesn't matter, or use ensembles of sorts to get a statistically meaningful result.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA Array Reduction\r\n                \r\nI'm aware that there are multiple questions similar to this one already answered but I've been unable to piece together anything very helpful from them other than that I'm probably incorrectly indexing something.\n\nI'm trying to preform a sequential addressing reduction on input vector A into output vector B.\n\nThe full  code is available here http://pastebin.com/7UGadgjX, but this is the kernel:\n\n```\n__global__ void vectorSum(int *A, int *B, int numElements) {\n  extern __shared__ int S[];\n  // Each thread loads one element from global to shared memory\n  int tid = threadIdx.x;\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < numElements) {\n    S[tid] = A[i];\n    __syncthreads();\n    // Reduce in shared memory\n    for (int t = blockDim.x/2; t > 0; t>>=1) {\n      if (tid < t) {\n        S[tid] += S[tid + t];\n      }\n      __syncthreads();\n    }\n    if (tid == 0) B[blockIdx.x] = S[0];\n  }\n}\n```\n\n\nand these are the kernel launch statements:\n\n```\n// Launch the Vector Summation CUDA Kernel\n  int threadsPerBlock = 256;\n  int blocksPerGrid =(numElements + threadsPerBlock - 1) / threadsPerBlock;\n  vectorSum<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, numElements);\n```\n\n\nI'm getting a unspecified launch error which I've read is similar to a segfault. I've been following the nvidia reduction documentation closely and tried to keep my kernel within the bounds of numElements but I seem to be missing something key considering how simple the code is.\n    ", "Answer": "\r\nYour problem is that the reduction kernel requires dynamically allocated shared memory to operate correctly, but your kernel launch doesn't specify any. The result is out of bounds/illegal shared memory access which aborts the kernel.\n\nIn CUDA runtime API syntax, the kernel launch statement has four arguments. The first two are the grid and block dimensions for the launch. The latter two are optional with zero default values, but specify the dynamically allocated shared memory size and stream.\n\nTo fix this, change the launch code as follows:\n\n```\n// Launch the Vector Summation CUDA Kernel\n  int threadsPerBlock = 256;\n  int blocksPerGrid =(numElements + threadsPerBlock - 1) / threadsPerBlock;\n  size_t shmsz = (size_t)threadsPerBlock * sizeof(int);\n  vectorSum<<<blocksPerGrid, threadsPerBlock, shmsz>>>(d_A, d_B, numElements);\n```\n\n\n[disclaimer: code written in browser, not compiled or tested, use at own risk]\n\nThis should at least fix the most obvious problem with your code.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Is dimensionality reduction reversible?\r\n                \r\nI have implemented a dimentionality reduction algorithm using ENCOG, that takes a dataset (call it A) with multiple features and reduces it to a dataset (B) with only one feature (I need that for time series analisys). \n\nNow my question is, I have a value from B - predicted by the time series analysis, can I convert it back to two dimensions like in the A dataset?\n    ", "Answer": "\r\nNo, dimensionality reduction is not reversible in general. It loses information.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Graph reduction\r\n                \r\nI have been working on an piece of code to reduce a graph. The problem is that there are some branches that I want to remove. Once I remove a branch I can merge the nodes or not, depending on the number of paths between the nodes the branch joined.\nMaybe the following example illustrates what I want:\n\nThe code I have is the following:\n```\nfrom networkx import DiGraph, all_simple_paths, draw\nfrom matplotlib import pyplot as plt\n\n# data preparation\nbranches = [(2,1), (3,2), (4,3), (4,13), (7,6), (6,5), (5,4), \n            (8,7), (9,8), (9,10), (10,11), (11,12), (12,1), (13,9)]\n\nbranches_to_remove_idx = [11, 10, 9, 8, 6, 5, 3, 2, 0]\nft_dict = dict()\ngraph = DiGraph()\n\nfor i, br in enumerate(branches):\n    graph.add_edge(br[0], br[1])\n    ft_dict[i] = (br[0], br[1])\n\n# Processing -----------------------------------------------------\nfor idx in branches_to_remove_idx:\n\n    # get the nodes that define the edge to remove\n    f, t = ft_dict[idx]\n\n    # get the number of paths from 'f' to 't'\n    n_paths = len(list(all_simple_paths(graph, f, t)))\n\n    if n_paths == 1:\n        # remove branch and merge the nodes 'f' and 't'\n        #\n        #       This is what I have no clue how to do\n        #\n        pass\n\n    else:\n        # remove the branch and that's it\n        graph.remove_edge(f, t)\n        print('Simple removal of', f, t)\n\n# -----------------------------------------------------------------\n\ndraw(graph, with_labels=True)\nplt.show()\n```\n\nI feel that there should be a simpler direct way to obtain the last figure from the first, given the branch indices, but I have no clue.\n    ", "Answer": "\r\nI think this is more or less what you want. I am merging all nodes that are in chains (connected nodes of degree 2) into one hypernode. I return the the new graph and a dictionary mapping the hypernode to the contracted nodes. \n\n```\nimport networkx as nx\n\ndef contract(g):\n    \"\"\"\n    Contract chains of neighbouring vertices with degree 2 into one hypernode.\n\n    Arguments:\n    ----------\n    g -- networkx.Graph instance\n\n    Returns:\n    --------\n    h -- networkx.Graph instance\n        the contracted graph\n\n    hypernode_to_nodes -- dict: int hypernode -> [v1, v2, ..., vn]\n        dictionary mapping hypernodes to nodes\n\n    \"\"\"\n\n    # create subgraph of all nodes with degree 2\n    is_chain = [node for node, degree in g.degree_iter() if degree == 2]\n    chains = g.subgraph(is_chain)\n\n    # contract connected components (which should be chains of variable length) into single node\n    components = list(nx.components.connected_component_subgraphs(chains))\n    hypernode = max(g.nodes()) +1\n    hypernodes = []\n    hyperedges = []\n    hypernode_to_nodes = dict()\n    false_alarms = []\n    for component in components:\n        if component.number_of_nodes() > 1:\n\n            hypernodes.append(hypernode)\n            vs = [node for node in component.nodes()]\n            hypernode_to_nodes[hypernode] = vs\n\n            # create new edges from the neighbours of the chain ends to the hypernode\n            component_edges = [e for e in component.edges()]\n            for v, w in [e for e in g.edges(vs) if not ((e in component_edges) or (e[::-1] in component_edges))]:\n                if v in component:\n                    hyperedges.append([hypernode, w])\n                else:\n                    hyperedges.append([v, hypernode])\n\n            hypernode += 1\n\n        else: # nothing to collapse as there is only a single node in component:\n            false_alarms.extend([node for node in component.nodes()])\n\n    # initialise new graph with all other nodes\n    not_chain = [node for node in g.nodes() if not node in is_chain]\n    h = g.subgraph(not_chain + false_alarms)\n    h.add_nodes_from(hypernodes)\n    h.add_edges_from(hyperedges)\n\n    return h, hypernode_to_nodes\n\n\nedges = [(2, 1),\n         (3, 2),\n         (4, 3),\n         (4, 13),\n         (7, 6),\n         (6, 5),\n         (5, 4),\n         (8, 7),\n         (9, 8),\n         (9, 10),\n         (10, 11),\n         (11, 12),\n         (12, 1),\n         (13, 9)]\n\ng = nx.Graph(edges)\n\nh, hypernode_to_nodes = contract(g)\n\nprint(\"Edges in contracted graph:\")\nprint(h.edges())\nprint('')\nprint(\"Hypernodes:\")\nfor hypernode, nodes in hypernode_to_nodes.items():\n    print(\"{} : {}\".format(hypernode, nodes))\n```\n\n\nThis returns for your example:\n\n```\nEdges in contracted graph:\n[(9, 13), (9, 14), (9, 15), (4, 13), (4, 14), (4, 15)]\n\nHypernodes:\n14 : [1, 2, 3, 10, 11, 12]\n15 : [8, 5, 6, 7]\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Why is context reduction necessary?\r\n                \r\nI've just read this paper (\"Type classes: an exploration of the design space\" by Peyton Jones & Jones), which explains some challenges with the early typeclass system of Haskell, and how to improve it.\n\nMany of the issues that they raise are related to context reduction which is a way to reduce the set of constraints over instance and function declarations by following the \"reverse entailment\" relationship.\n\ne.g. if you have somewhere ```\ninstance (Ord a, Ord b) => Ord (a, b) ...```\n then within contexts, ```\nOrd (a, b)```\n gets reduced to ```\n{Ord a, Ord b}```\n (reduction does not always shrink the number of constrains).\n\nI did not understand from the paper why this reduction was necessary.\n\nWell, I gathered it was used to perform some form of type checking. When you have your reduced set of constraint, you can check that there exist some instance that can satisfy them, otherwise it's an error. I'm not too sure what the added value of that is, since you would notice the problem at the use site, but okay.\n\nBut even if you have to do that check, why use the result of reduction inside inferred types? The paper points out it leads to unintuitive inferred types.\n\nThe paper is quite ancient (1997) but as far as I can tell, context reduction is still an ongoing concern. The Haskell 2010 spec does mention the inference behaviour I explain above (link).\n\nSo, why do it this way?\n    ", "Answer": "\r\nI don't know if this is The Reason, necessarily, but it might be considered A Reason: in early Haskell, type signatures were only permitted to have \"simple\" constraints, namely, a type class name applied to a type variable. Thus, for example, all of these were okay:\n\n```\nOrd a => a -> a -> Bool\nEq a => a -> a -> Bool\nGraph gr => gr n e -> [n]\n```\n\n\nBut none of these:\n\n```\nOrd (Tree a) => Tree a -> Tree a -> Bool\nEq (a -> b) => (a -> b) -> (a -> b) -> Bool\nGraph Gr => Gr n e -> [n]\n```\n\n\nI think there was a feeling then -- and still today, as well -- that allowing the compiler to infer a type which one couldn't write manually would be a bit unfortunate. Context reduction was a way of turning the above signatures either into ones that could be written by hand as well or an informative error. For example, since one might reasonably have\n\n```\ninstance Ord a => Ord (Tree a)\n```\n\n\nin scope, we could turn the illegal signature ```\nOrd (Tree a) => ...```\n into the legal signature ```\nOrd a => ...```\n. On the other hand, if we don't have any instance of ```\nEq```\n for functions in scope, one would report an error about the type which was inferred to require ```\nEq (a -> b)```\n in its context.\n\nThis has a couple of other benefits:\n\n\nIntuitively pleasing. Many of the context reduction rules do not change whether the type is legal, but do reflect things humans would do when writing the type. I'm thinking here of the de-duplication and subsumption rules that let you turn, e.g. ```\n(Eq a, Eq a, Ord a)```\n into just ```\nOrd a```\n -- a transformation one definitely would want to do for readability.\nThis can frequently catch stupid errors; rather than inferring a type like ```\nEq (Integer -> Integer) => Bool```\n which can't be satisfied in a law-abiding way, one can report an error like ```\nPerhaps you did not apply a function to enough arguments?```\n. Much friendlier!\nIt becomes the compiler's job to pinpoint what went wrong. Instead of inferring a complicated context like ```\nEq (Tree (Grizwump a, [Flagle (Gr n e) (Gr n' e') c]))```\n and complaining that the context is not satisfiable, it instead is forced to reduce this to the constituent constraints; it will instead complain that we couldn't determine ```\nEq (Grizwump a)```\n from the existing context -- a much more precise and actionable error.\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Dimension Reduction\r\n                \r\nI'm trying to reduce a high-dimension dataset to 2-D.  However, I don't have access to the whole dataset upfront.  So, I'd like to generate a function that takes an N-dimensional vector and returns a 2-dimensional vector, such that if I give it to vectors that are close in N-dimensional space, the results are close in 2-dimensional space.\n\nI thought SVD was the answer I needed, but I can't make it work.  \n\nFor simplicity, let N=3 and suppose I have 15 datapoints.  If I have all the data upfront in a 15x3 matrix X, then:\n\n```\n[U, S, V] = svd(X);\ns = S; %s is a the reduced version of S, since matlab is case-sensitive.\ns(3:end,3:end)=0;\nY=U*s;\nY=Y(1:2,:);\n```\n\n\ndoes what I want.  But suppose I get a new datapoint, A, a 1x3 vector.  Is there a way to use U, S, or V to turn A into the appropriate 1x2 vector?\n\nIf SVD is a lost cause, can someone tell me what I should be doing instead?\n\nNote:  This is Matlab code, but I don't care if the answer is C, Java, or just math.  If you can't read Matlab, ask and I'll clarify.\n    ", "Answer": "\r\nSVD is a fine approach (probably).  LSA (Latent Semantic Analysis) is based around it, and has basically the same dimensionality approach.  I've talked about that (at length) at:\nlsa-latent-semantic-analysis-how-to-code-it-in-php   or check out the LSA tag here on SO.\n\nI realize it's an incomplete answer.  Holler if you want more help!\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Dimension reduction methods for images\r\n                \r\nI am trying to reduce the dimensions on an a set of images using Matlab Toolbox for Dimensionality Reduction. Problem is: I know very little about dimension reduction. So am trying each one by trial and error, passing the data set to the function. I have tried 6 so far, PCA was returning a matrix with a complex number. And the others was frozen matlab. What Image reduction methods is suitable for images?\n    ", "Answer": "\r\nIt looks like you are trying to implement something like eigenfaces, so first of all take a look at the article. Normally you need to get eigenvectors either with PCA or with SVD. However, if you need really low resources use, take a look at Random Projection method. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction meta-operator inconsistency\r\n                \r\nWhen we examine the reduce function: \n\n```\n    my $result = reduce &reduction_procedure, @array;\n```\n\n\nwe conclude with the following simple rules to the inner workings:\n\n```\n    Reduction Rules\n    ---------------\n    1. For the first pair of objects from our input (@array)\n       we apply the reduction procedure (&reduction_procedure) and we get our first result.\n\n    2. We apply the reduction procedure (&reduction_procedure) to the result (object)\n       of the previous rule and the next input (object) from our input (@array),\n       and we get an intermediate result (object).\n\n    3. We run rule.2 for every of the remaining objects from our input (@array)\n```\n\n\nThis simple rules work also the same for the reduction metaoperator []. For example:\n\n```\n    example.1\n    ---------\n    say [+] [1,2,3,4]; # result : 10\n```\n\n\nFor example.1 the reduction rules apply as is:\n\n```\n    Step.1 use Rule.1 : 1 + 2 = 3     1(first value)     + 2(second value)  = 3\n    Step.2 use Rule.2 : 3 + 3 = 6     3(previous result) + 3(current value) = 6\n    Step.3 use Rule.2 : 6 + 4 = 10    6(previous result) + 4(current value) = 10\n```\n\n\nbut NOT for the following example:\n\n```\n    example.2\n    ----------\n    say [<] 1,2,3,4;   # result : True\n```\n\n\nFor example.2 we observe an inconsistency:\n\n```\n    Step.1 use Rule.1 : 1 < 2 = True    1(first value)         <  2(second value)      = True\n    Step.2 use Rule.2 : 2 < 3 = True    True(previous result) &&  True(current result) = True\n    Step.3 use Rule.2 : 3 < 4 = True    True(previous result) &&  True(current result) = True(result)\n```\n\n\nThe inconsistency is that from Step.2 and onwards we can NOT use the result of the previous step as the first parameter of subsequent reduce operations,\nbut instead we calculate a logical intermediate result, with use of the actual values and finally we add as a final step the use of \"logical AND\" upon \nthe intermediate logical results of each step:\n\n```\n    Reduction Result = True && True && True = True (use of logical AND as \"meta-reduction\" operator)\n```\n\n\nSort of speak we have a \"meta-reduction\" metaoperator!\n\nQuestions:\n\n```\n    1. Is that an inconsistency with a purpose?\n\n    2. Can we exploit this behavior? For instance instead of use of && (logical AND)\n       as \"meta-reduction\" operator can we use || (logical OR) or ?^ (logical XOR) ?\n```\n\n    ", "Answer": "\r\nIt is not an inconsistency but and example of how operator associativity works in Raku.\n\nWriting :\n\n```\n[<] 1,2,3,4\n```\n\n\nIs the same as writing :\n\n```\n1 < 2 < 3 < 4\n```\n\n\nIn most languages this wouldn't work but the < operator has chain associativity so it effectively treats this as :\n\n```\n(1 < 2) and (2 < 3) and (3 < 4)\n```\n\n\nWhich is True. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Is it possible to do a reduction on an array with openmp?\r\n                \r\nDoes OpenMP natively support reduction of a variable that represents an array? \n\nThis would work something like the following...\n\n```\nfloat* a = (float*) calloc(4*sizeof(float));\nomp_set_num_threads(13);\n#pragma omp parallel reduction(+:a)\nfor(i=0;i<4;i++){\n   a[i] += 1;  // Thread-local copy of a incremented by something interesting\n}\n// a now contains [13 13 13 13]\n```\n\n\nIdeally, there would be something similar for an omp parallel for, and if you have a large enough number of threads for it to make sense, the accumulation would happen via binary tree.\n    ", "Answer": "\r\nArray reduction is now possible with OpenMP 4.5 for C and C++. Here's an example:\n\n```\n#include <iostream>\n\nint main()\n{\n\n  int myArray[6] = {};\n\n  #pragma omp parallel for reduction(+:myArray[:6])\n  for (int i=0; i<50; ++i)\n  {\n    double a = 2.0; // Or something non-trivial justifying the parallelism...\n    for (int n = 0; n<6; ++n)\n    {\n      myArray[n] += a;\n    }\n  }\n  // Print the array elements to see them summed   \n  for (int n = 0; n<6; ++n)\n  {\n    std::cout << myArray[n] << \" \" << std::endl;\n  } \n}\n```\n\n\nOutputs:\n\n```\n100\n100\n100\n100\n100\n100\n```\n\n\nI compiled this with GCC 6.2. You can see which common compiler versions support the OpenMP 4.5 features here: https://www.openmp.org/resources/openmp-compilers-tools/\n\nNote from the comments above that while this is convenient syntax, it may invoke a lot of overheads from creating copies of each array section for each thread.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Understanding Bison with no declared type\r\n                \r\nI have the following code below and I am receiving the following error\nparser.y:111.47-48: error: $$ for the midrule at $5 of ‘statement’ has no declared type\n111 |         REDUCE operator reductions ENDREDUCE {$$ = $3;} ';'|\nI know it's generated because I didn't declare a type for something in the statement, I need help understanding line 111.  Also the REAL_LITERAL is a float, that I should add a float to the union and create token like this %token <f_value>REAL_LITERAL.\n```\ninclude <iostream>\n#include <string>\n#include <vector>\n#include <map>\n\nusing namespace std;\n\n#include \"math.h\"\n#include \"values.h\"\n#include \"listing.h\"\n#include \"symbols.h\"\n\nint yylex();\nvoid yyerror(const char* message);\n\nSymbols<int> symbols;\n\nint result;\ndouble *params;\n%}\n\n%define parse.error verbose\n\n%union\n{\n    CharPtr iden;\n    Operators oper;\n    int value;\n}\n\n%token <iden> IDENTIFIER\n%token <value>INT_LITERAL REAL_LITERAL BOOL_LITERAL CASE TRUE FALSE\n\n%token ARROW\n\n%token <oper> ADDOP MULOP RELOP OROP NOTOP REMOP EXPOP\n%token ANDOP\n\n%token BEGIN_ BOOLEAN END ENDREDUCE FUNCTION INTEGER IS REDUCE RETURNS\n%token THEN WHEN\n%token ELSE ENDCASE ENDIF IF OTHERS REAL\n\n%type <value> body statement_ statement reductions expression binary relation term\n    factor primary\n%type <oper> operator\n\n%left OROP\n%left ANDOP\n%left RELOP\n%left ADDOP\n%left MULOP REMOP\n%right EXPOP\n%left NOTOP\n%%\n\nfunction:   \n    function_header optional_variable body {result = $3;} ;\n    \nfunction_header:    \n    FUNCTION IDENTIFIER parameters RETURNS type ';' |\n    FUNCTION IDENTIFIER RETURNS type ';' |\n    FUNCTION IDENTIFIER optional_parameters RETURNS type ';' |\n    error ';' ;\n    \noptional_variable:\n    optional_variable variable |\n    error ';' ;\n    ;\n\nvariable:   \n    IDENTIFIER ':' type IS statement_ {symbols.insert($1, $5);} ;\n\nvariables:\n    variable variables |\n    ;\n\ntype:\n    INTEGER |\n    BOOLEAN ;\n\noptional_parameters:\n    parameters |\n    ;\n    \nparameters:\n    parameter ',' parameters |\n    parameter ;\n    \nparameter:\n    IDENTIFIER ':' type ;\n\ntype:\n    INTEGER |\n    REAL |\n    BOOLEAN ;\n\nbody:\n    BEGIN_ statement_ END ';' {$$ = $2;} ;\n\nstatement_:\n    statement ';' |\n    error ';' {$$ = 0;} ;\n    \nstatement:\n    expression |\n    REDUCE operator reductions ENDREDUCE {$$ = $3;} ';'|\n    IF expression THEN statement_ ELSE statement_ ENDIF\n    {\n        if ($2 == true) {\n                $$ = $4;\n            }\n            else {\n                $$ = $6;\n            }\n    }';' /*|\n    CASE expression IS cases OTHERS ARROW statement_ ENDCASE\n    {$$ = $<value>4 == $1 ? $4 : $7;} ;\n\ncases:\n    cases case\n    {$$ = $<value>1 == $1 ? $1 : $2;} |\n    %empty {$$ = NAN;};\n\ncase:\n    case WHEN INT_LITERAL ARROW statement_ |\n    ;\n*/\noperator:\n    ADDOP |\n    RELOP |\n    EXPOP |\n    MULOP ;\n\nreductions:\n    reductions statement_ {$$ = evaluateReduction($<oper>0, $1, $2);} |\n    {$$ = $<oper>0 == ADD ? 0 : 1;} ;\n\nexpression:\n    expression OROP binary {$$ = $1 || $3;} |\n    binary;\n    \nbinary:\n    binary ANDOP relation {$$ = $1 && $3;} |\n    relation ;\n\nrelation:\n    relation RELOP term {$$ = evaluateRelational($1, $2, $3);} |\n    term ;\n\nterm:\n    term ADDOP factor {$$ = evaluateArithmetic($1, $2, $3);} |\n    factor ;\n      \nfactor:\n    factor MULOP primary {$$ = evaluateArithmetic($1, $2, $3);} |\n    primary ;\n\nprimary:\n    '(' expression ')' {$$ = $2;} |\n    INT_LITERAL |\n    IDENTIFIER {if (!symbols.find($1, $$)) appendError(UNDECLARED, $1);} ;\n\n%%\n\nvoid yyerror(const char* message)\n{\n    appendError(SYNTAX, message);\n}\n\nint main(int argc, char *argv[])    \n{\n    firstLine();\n    yyparse();\n    if (lastLine() == 0)\n        cout << \"Result = \" << result << endl;\n    return 0;\n} \n```\n\n    ", "Answer": "\r\nThe basic problem is that in-rule actions do not get a default type in bison (unlike yacc).  So in your action\n```\nREDUCE operator reductions ENDREDUCE {$$ = $3;} ';'\n```\n\nthere's no ```\n%type```\n for ```\n$$```\n so you need to specify it explicitly -- perhaps something like ```\n{ $<value>$ = $3; }```\n.  That's equivalent to what yacc would do here, as it gives in-rule actions the same type as the lhs, even though there's not really anything connecting them.\nThe bigger issue is that this really makes no sense -- an in-rule action like this does NOT set the value for the symbol being reduced.  That can only happen in the end-of-rule action.  So this is just copying a value to a temp and then throwing it away, never doing anything with it.  The implicit end-rule action just does ```\n{ $$ = $1; }```\n which makes no sense as ```\nstatement```\n and ```\nREDUCE```\n have different types.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "What is the usage of reduction in openmp?\r\n                \r\nI have this piece of code that is parallelized.\n```\nint i,n; double pi,x;\ndouble area=0.0;\n\n#pragma omp parallel for private(x) reduction (+:area)\nfor(i=0; i<n; i++){\n    x= (i+0.5)/n;\n    area+= 4.0/(1.0+x*x);\n}\npi = area/n;\n```\n\nIt is said that the reduction will remove the race condition that could happen if we didn't use a reduction. Still I'm wondering do we need to add lastprivate for area since its used outside the parallel loop and will not be visible outside of it. Else does the reduction cover this as well?\n    ", "Answer": "\r\nReduction takes care of making a private copy of ```\narea```\n for each thread. Once the parallel region ends area is reduced in one atomic operation. In other words the ```\narea```\n that is exposed is an aggregate of all private ```\narea```\ns of each thread.\n\n```\nthread 1 - private area = compute(x)\nthread 2 - private area = compute(y)\nthread 3 - private area = compute(z)\n\nreduction step - public area = area<thread1> + area<thread2> + area<thread3> ...\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Parallel reduction example\r\n                \r\nI found this parallel reduction code from Stanford which uses shared memory.\nThe code is an example of ```\n1<<18```\n number of elements which is equal to 262144 and produces correct results.\nWhy do I get the correct results for certain numbers of elements and for other numbers of elements, like 200000 or 25000 I get different, unexpected results?\nIt looks to me as if it's always appointing the needed thread blocks.\n    ", "Answer": "\r\nThis code causes the bug:\n```\n// launch a single block to compute the sum of the partial sums\nblock_sum<<<1,num_blocks,num_blocks * sizeof(float)>>>\n```\n\nSuppose ```\nnum_blocks```\n is 13, then in the kernel ```\nblockDim.x / 2```\n will be 6, and\n```\nif(threadIdx.x < offset)\n{\n    // add a partial sum upstream to our own\n    sdata[threadIdx.x] += sdata[threadIdx.x + offset]; \n}\n```\n\nwill only add the first 12 elements causing the bug.\nWhen the element count is 200000 or 250000, ```\nnum_blocks```\n will be odd and cause the bug, while for even ```\nnum_blocks```\n it will work fine.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP and reduction()\r\n                \r\nI've got simply 3 functions, one is control function aan the next 2 function are done in a bit different way using OpenMP. But function thread1 gives another score than thread2 and control and I have no idea why?\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nfloat function(float x){\n    return pow(x,pow(x,sin(x)));\n}\n\n\n\n float integrate(float begin, float end, int count){\n    float score = 0 , width = (end-begin)/(1.0*count), i=begin, y1, y2;\n\n\n    for(i = 0; i<count; i++){\n            score += (function(begin+(i*width)) + function(begin+(i+1)*width)) * width/2.0;\n }\n    return score; \n }\n\n\n\n\n float thread1(float begin, float end, int count){\n    float score = 0 , width = (end-begin)/(1.0*count), y1, y2;\n\n    int i;\n    #pragma omp parallel for reduction(+:score) private(y1,i) shared(count)\n    for(i = 0; i<count; i++){\n        y1 = ((function(begin+(i*width)) + function(begin+(i+1)*width)) * width/2.0);\n       score = score + y1;\n    }\n\n    return score;\n }\n\n\n float thread2(float begin, float end, int count){\n    float score = 0 , width = (end-begin)/(1.0*count), y1, y2;\n\n    int i;\n    float * tab = (float*)malloc(count * sizeof(float));\n\n    #pragma omp parallel for\n    for(i = 0; i<count; i++){\n            tab[i] = (function(begin+(i*width)) + function(begin+(i+1)*width)) * width/2.0;\n    }\n\n    for(i=0; i<count; i++)\n            score += tab[i];\n    return score;\n  }\n\n\n  unsigned long long int rdtsc(void){\n     unsigned long long int x;\n     unsigned a, d;\n\n    __asm__ volatile(\"rdtsc\" : \"=a\" (a), \"=d\" (d));\n\n    return ((unsigned long long)a) | (((unsigned long long)d) << 32);\n   }\n\n\n\n\n\n\n   int main(int argc, char** argv){\n        unsigned long long counter = 0;\n\n\n    //test\n       counter = rdtsc();\n       printf(\"control: %f \\n \",integrate (atof(argv[1]), atof(argv[2]), atoi(argv[3])));\n       printf(\"control count: %lld \\n\",rdtsc()-counter);\n        counter = rdtsc();\n       printf(\"thread1: %f \\n \",thread1(atof(argv[1]), atof(argv[2]), atoi(argv[3])));\n       printf(\"thread1 count: %lld \\n\",rdtsc()-counter);\n        counter = rdtsc();\n       printf(\"thread2: %f \\n \",thread2(atof(argv[1]), atof(argv[2]), atoi(argv[3])));\n       printf(\"thread2 count: %lld \\n\",rdtsc()-counter);\n\n       return 0;\n      }\n```\n\n\nHere are simple answears :\n\n```\n gcc -fopenmp zad2.c -o zad -pg -lm\n env OMP_NUM_THREADS=2 ./zad 3 13 100000\n control: 5407308.500000 \n control count: 138308058 \n thread1: 5407494.000000 \n thread1 count: 96525618 \n thread2: 5407308.500000 \n thread2 count: 104770859\n```\n\n\nUpdate:\n\nOk, I tried to do this more quickly, and not count values for periods twice.\n\n```\ndouble thread3(double begin, double end, int count){\n     double score = 0 , width = (end-begin)/(1.0*count), yp, yk;    \n     int i,j, k;\n\n     #pragma omp parallel private (yp,yk) \n     {\n       int thread_num = omp_get_num_threads();\n       k = count / thread_num;\n\n    #pragma omp for private(i) reduction(+:score) \n    for(i=0; i<thread_num; i++){\n        yp = function(begin + i*k*width);\n        yk = function(begin + (i*k+1)*width);\n        score += (yp + yk) * width / 2.0;\n        for(j=i*k +1; j<(i+1)*k; j++){\n            yp = yk;\n            yk = function(begin + (j+1)*width);\n            score  += (yp + yk) * width / 2.0;\n        }\n    }\n\n  #pragma omp for private(i) reduction(+:score) \n  for(i = k*thread_num; i<count; i++)\n    score += (function(begin+(i*width)) + function(begin+(i+1)*width)) * width/2.0;\n }  \n   return score;\n }\n```\n\n\nBut after few tests I found that the scores are near the right value, but not equal. Sometimes one of the threads doesn't start. When I'm not using OpenMp, the value is correct.\n    ", "Answer": "\r\nYou're integrating a very strongly peaked function - x(xsin(x)) - which covers over 7 orders of magnitude in the range you're integrating it.   That's about the limit for a 32-bit floating point number, so there are going to be issues depending on the order you sum the numbers.   This isn't an OpenMP thing -- its just a numerical sensitivity thing.\n\n\n\nSo for instance, consider this completely serial code doing the same integral:\n\n```\n#include <stdio.h>\n#include <math.h>\n\nfloat function(float x){\n    return pow(x,pow(x,sin(x)));\n}\n\nint main(int argc, char **argv) {\n\n    const float begin=3., end=13.;\n    const int count = 100000;\n    const float width=(end-begin)/(1.*count);\n\n    float integral1=0., integral2=0., integral3=0.;\n\n    /* left to right */\n    for (int i=0; i<count; i++) {\n         integral1 += (function(begin+(i*width)) + function(begin+(i+1)*width)) * width/2.0;\n    }\n\n    /* right to left */\n    for (int i=count-1; i>=0; i--) {\n         integral2 += (function(begin+(i*width)) + function(begin+(i+1)*width)) * width/2.0;\n    }\n\n    /* centre outwards, first right-to-left, then left-to-right */\n    for (int i=count/2; i<count; i++) {\n         integral3 += (function(begin+(i*width)) + function(begin+(i+1)*width)) * width/2.0;\n    }\n    for (int i=count/2-1; i>=0; i--) {\n         integral3 += (function(begin+(i*width)) + function(begin+(i+1)*width)) * width/2.0;\n    }\n\n    printf(\"Left to right: %lf\\n\", integral1);\n    printf(\"Right to left: %lf\\n\", integral2);\n    printf(\"Centre outwards: %lf\\n\", integral3);\n\n    return 0;\n}\n```\n\n\nRunning this, we get:\n\n```\n$ ./reduce\nLeft to right: 5407308.500000\nRight to left: 5407430.000000\nCentre outwards: 5407335.500000\n```\n\n\n-- the same sort of differences you see.  Doing the summation with two threads necessarily changes the order of the summation, and so your answer changes.\n\nThere's a few options here.  If this was just a test proble, and this function doesn't actually represent what you'll be integrating, you might be fine already.   Otherwise, using a different numerical method may help.\n\nBut also here, there is a simple solution - the range of the numbers exceeds the range of a ```\nfloat```\n, making the answer very sensitive to summation order, but fits comfortably within the range of a ```\ndouble```\n, making the problem much less severe.  Note that changing to ```\ndouble```\ns is not a magic solution to everything; some cases it just postpones the problem or allows you to paper over a flaw in your numerical method.  But here it actually addresses the underlying problem fairly well.   Changing all the ```\nfloat```\ns above to ```\ndouble```\ns gives:\n\n```\n$ ./reduce\nLeft to right: 5407589.272885\nRight to left: 5407589.272885\nCentre outwards: 5407589.272885\n```\n\n\nOn the other hand, even doubles wouldn't save you if you needed to integrate this function in the range (18,23).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to perform multiple (reusing) reductions/mapping/mixed operations from one Stream object\r\n                \r\nI understand that Java stream object doesn't allow ```\nforked stream```\n. But it's also reasonable to imagine one would perform multiple actions to the same stream. Say I want to map a List of objects but also sum up a measurement across all objects. I'd like to know if it's possible to do so without dump data into a List.\n\n```\nStream<Thing> thingStream = ...;\nint accumulation = thingStream.mapToInt(Thing::measure).sum();\nList<Another> results = thingStream.map(t -> toAnother(t)).collect(toList());\n```\n\n\nI know it's possible to use ```\nStream#peek```\n but it sounds hacky.\n    ", "Answer": "\r\nYou can call only one terminal operation (e.g: sum, count, collect .. ) on a stream, so the answer is no, you can not use the same stream to perform the 2 different terminal operations that you specified.\n\nSmall extract of the documentation says:\n\n\n  After the\n  terminal operation is performed, the stream pipeline is considered\n  consumed, and can no longer be used; if you need to traverse the same\n  data source again, you must return to the data source to get a new\n  stream. In almost all cases, terminal operations are eager, completing\n  their traversal of the data source and processing of the pipeline\n  before returning. Only the terminal operations iterator() and\n  spliterator() are not; these are provided as an \"escape hatch\" to\n  enable arbitrary client-controlled pipeline traversals in the event\n  that the existing operations are not sufficient to the task.\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Is the Reduction function a correspondence?\r\n                \r\nI'm studying Computability and Complexity and i came out with a doubt.\nThe Function that reduce a problem to another one is Turing-Computable. I was wondering if its even a one-to-one function ( a correspondence) since looking,for example, to the Vertex-Cover -> Independent Set reduction i cannot see where an instance of one problem is not in correspondece with another instance of the other one.\n\nThank you\n    ", "Answer": "\r\nNo, there is not a one-to-one correspondence. If you reduce problem A to problem B, for example in polynomial time (A <=_pol B), that means that you can solve problem A with the help of a solution to problem B. But it is possible that there is an input for problem B you cannot solve with a solution to A. Also the reduction function could map multiple inputs for problem A to a single input for problem B.\n\nTake for example the reduction of Clique(G,k) to SubgraphIsomorphism(G,H): Clique <=_pol SubgraphIsomorphism. A clique is only a special subgraph H you can construct in time polynomially in k. But to be able to solve Clique(G,k) wont help you to find arbitrary subgraphs H in G. Thus, not every input for SubgraphIsomorphism corresponds to an input for Clique. This reduction merely shows that SubgraphIsomorphism is at least as hard as Clique.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenACC: reduction operation on arrays\r\n                \r\nIs it possible to use the reduction operation \"or\" on the static array ```\nflag[nx3_tot][nx2_tot][nx1_tot]```\n ?\nhere ```\nFLAG_HLL=4```\n and ```\nFLAG_MINMOD=1```\n. Without the reduction the results of this function running with or without OpenACC are different due to the lines:\n```\nflag[k][j][i+1] |= FLAG_MINMOD;\n```\n\nand I don't understand why: having the \"or\" operator ```\n|```\n I would expect to have correct results even if different threads access the same memory address. But since this is the case I would like to use the reduction clause but I get the message:\nReduction type not supported for this variable datatype - flag flag_shock.c\nSizeof dimensionless array  required flag_shock.c\n```\nint flag[nx3_tot][nx2_tot][nx1_tot];\n#pragma acc enter data create(flag[:nx3_tot][:nx2_tot][:nx1_tot])\n\n#pragma acc parallel loop collapse(3) \\\n  present(d, grid, pt[:nx3_tot][:nx2_tot][:nx1_tot]) reduction(||:flag)\nfor (k = INCLUDE_KDIR; k < nx3_tot-INCLUDE_KDIR; k++){\nfor (j = INCLUDE_JDIR; j < nx2_tot-INCLUDE_JDIR; j++){\nfor (i = INCLUDE_IDIR; i < nx1_tot-INCLUDE_IDIR; i++){\n\ndouble divv, gradp, pt_min;\ndouble dpx1, pt_min1, dvx1;\ndouble dpx2, pt_min2, dvx2;\ndouble dpx3, pt_min3, dvx3;\n\npt_min = pt[k][j][i];\nDIM_EXPAND(pt_min1 = MIN(pt[k][j][i+1], pt[k][j][i-1]); ,\n           pt_min2 = MIN(pt[k][j+1][i], pt[k][j-1][i]);  ,\n           pt_min3 = MIN(pt[k+1][j][i], pt[k-1][j][i]); )\n\nDIM_EXPAND(pt_min = MIN(pt_min, pt_min1);  ,\n           pt_min = MIN(pt_min, pt_min2);  ,\n           pt_min = MIN(pt_min, pt_min3);)\n\nDIM_EXPAND(dpx1 = fabs(pt[k][j][i+1] - pt[k][j][i-1]);  ,\n           dpx2 = fabs(pt[k][j+1][i] - pt[k][j-1][i]);  ,\n           dpx3 = fabs(pt[k+1][j][i] - pt[k-1][j][i]);)\n\ngradp = DIM_EXPAND(dpx1, + dpx2, + dpx3);\n\nif (gradp > EPS_PSHOCK_FLATTEN*pt_min) \n  {\n  flag[k][j][i]   |= FLAG_HLL;\n  flag[k][j][i]   |= FLAG_MINMOD;\n  flag[k][j][i+1] |= FLAG_MINMOD;\n  flag[k][j][i-1] |= FLAG_MINMOD;\n  flag[k][j-1][i] |= FLAG_MINMOD;\n  flag[k][j+1][i] |= FLAG_MINMOD;\n  }\n\n}}}\n```\n\n    ", "Answer": "\r\nI wouldn't recommend using reductions in this case.  Each thread would need it's own private copy of the array which would be quite a bit of memory given it's 3D, plus there's the overhead of performing the final reduction.\nInstead, I'd recommend using atomic operations.  Since there will very few collisions on the elements, the overhead would be very low.\n```\nif (gradp > EPS_PSHOCK_FLATTEN*pt_min) \n  {\n#pragma acc atomic update\n  flag[k][j][i]   |= FLAG_HLL;\n#pragma acc atomic update\n  flag[k][j][i]   |= FLAG_MINMOD;\n#pragma acc atomic update\n  flag[k][j][i+1] |= FLAG_MINMOD;\n.. etc ...\n```\n\nYou don't mention which compiler or compiler version you're using.  If using NVHPC, array reduction support was added recently so if using an older compiler, this is why you'd be getting the message that this data type is not supported.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Hash length reduction?\r\n                \r\nI know that say given a md5/sha1 of a value, that reducing it from X bits (ie 128) to say Y bits (ie 64 bits) increases the possibility of birthday attacks since information has been lost. Is there any easy to use tool/formula/table that will say what the probability of a \"correct\" guess will be when that length reduction occurs (compared to its original guess probability)?\n    ", "Answer": "\r\nCrypto is hard. I would recommend against trying to do this sort of thing. It's like cooking pufferfish: Best left to experts.\n\nSo just use the full length hash. And since MD5 is broken and SHA-1 is starting to show cracks, you shouldn't use either in new applications. SHA-2 is probably your best bet right now.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Best dimensionality reduction algorithms\r\n                \r\nWhat is the best dimensionality reduction algorithm and what are the selection criteria we can use? I know PCA is the best technique when it comes to mean-square and linear reduction. \n    ", "Answer": "\r\nThere is no best technique, period. It applies to all unsupervised learning, where there is no actual aim/criterion different than the one inside the method. This is why for classification you have better and worse methods, but you do not have better/worse clusterers and/or dimensionality reduction. You only have different ones, doing different things, thats all.\n\nEach method is best in what it does. PCA is best for linear reduction leading to highest preserved variance, because it is its definition, not because it is better than other doing the same - there are no others doing the same. \n\nI deliberately omit problems with non-converging methods, there you can obviously say that some optimization technique (algorithm) is better than the other. But it is important to make a distinction between method (such as PCA) and a particular solver/implementation (such as SVD, randomized PCA, etc.)\n\nListing all dimensionality reduction techniques with their definitions (as this is what they are \"best at\") is beyond the scope of SO, in particular because there are dozens (hundreads) of them and you can easily find them by googling.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "TicTacToe strategic reduction\r\n                \r\nI decided to write a small program that solves TicTacToe in order to try out the effect of some pruning techniques on a trivial game.  The full game tree using minimax to solve it only ends up with 549,946 possible games.  With alpha-beta pruning, the number of states required to evaluate was reduced to 18,297.  Then I applied a transposition table that brings the number down to 2,592.  Now I want to see how low that number can go.\n\nThe next enhancement I want to apply is a strategic reduction.  The basic idea is to combine states that have equivalent strategic value.  For instance, on the first move, if X plays first, there is nothing strategically different (assuming your opponent plays optimally) about choosing one corner instead of another.  In the same situation, the same is true of the center of the walls of the board, and the center is also significant.  By reducing to significant states only, you end up with only 3 states for evaluation on the first move instead of 9.  This technique should be very useful since it prunes states near the top of the game tree.  This idea came from the GameShrink method created by a group at CMU, only I am trying to avoid writing the general form, and just doing what is needed to apply the technique to TicTacToe.\n\nIn order to achieve this, I modified my hash function (for the transposition table) to enumerate all strategically equivalent positions (using rotation and flipping functions), and to only return the lowest of the values for each board.  Unfortunately now my program thinks X can force a win in 5 moves from an empty board when going first.  After a long debugging session, it became apparent to me the program was always returning the move for the lowest strategically significant move (I store the last move in the transposition table as part of my state).  Is there a better way I can go about adding this feature, or a simple method for determining the correct move applicable to the current situation with what I have already done? \n    ", "Answer": "\r\nMy gut feeling is that you are using too big of a hammer to attack this problem. Each of the 9 spots can only have one of two labels: X or O or empty. You have then at most 3^9 = 19,683 unique boards. Since there are 3 equivalent reflections for every board, you really only have 3^9 / 4 ~ 5k boards. You can reduce this by throwing out invalid boards (if they have a row of X's AND a row of O's simultaneously).\n\nSo with a compact representation, you would need less than 10kb of memory to enumerate everything. I would evaluate and store the entire game graph in memory.\n\nWe can label every single board with its true minimax value, by computing the minimax values bottom up instead of top down (as in your tree search method). Here's a general outline: We compute the minimax values for all unique boards and label them all first, before the game starts. To make the minimax move, you simply look at the boards succeeding your current state, and pick the move with the best minimax value.\n\nHere's how to perform the initial labeling. Generate all valid unique boards, throwing out reflections. Now we start labeling the boards with the most moves (9), and iterating down to the boards with least moves (0). Label any endgame boards with wins, losses, and draws. For any non-endgame boards where it's X's turn to move: 1) if there exists a successor board that's a win for X, label this board a win; 2) if in successor boards there are no wins but there exists a draw, then label this board a draw; 3) if in successor boards there are no wins and no draws then label this board a loss. The logic is similar when labeling for O's turn.\n\nAs far as implementation goes, because of the small size of the state space I would code the \"if there exists\" logic just as a simple loop over all 5k states. But if you really wanted to tweak this for asymptotic running time, you would construct a directed graph of which board states lead to which other board states, and perform the minimax labeling by traversing in the reverse direction of the edges.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CSS Reduction Tool\r\n                \r\nI was wondering whether anyone knows of any tools available that perform the task of analyzing one or more CSS files, determining the similarity between the various rules within the files and presenting the user with options for the merging and reduction of rulesets.\n\nI ask this because a project I am working on has reached the point where it has so much CSS that Internet Explorer (Still the bottom line I'm afraid) chokes on the CSS after page load, causing a 3-5 second lock-up in interactivity until the choke is processed.\n\nIn case you're wondering: Yes, I am sure it is the CSS causing this issue.\n    ", "Answer": "\r\ntry any of these links, I much prefer css tidy and have used it successfully in the past.\n\ncss optimiser\n\ncleancss\n\ncss tidy\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Normal/Applicative Order Reduction\r\n                \r\nIn terms of evaluating function applications does Haskell only support normal order reduction or does it also support applicative order reduction? Am I correct in thinking that normal order is what gives Haskell its laziness?\n    ", "Answer": "\r\nThe GHC runtime does not use a term reduction strategy, since that would be very inefficient. Indeed, GHC is a proper compiler, targeting the STG-machine as an abstract machine for its runtime.\n\nStill, it achieves the same semantics of normal order reduction -- i.e. non-strict semantics.\n\nOne can still force some arguments to be evaluated, as in applicative order reduction as follows:\n\n```\nfoo x y = x `seq` actualFoo x y\n```\n\n\nAbove, ```\nx```\n is evaluated immediately, while ```\ny```\n is not.\n\nOther common equivalent patterns:\n\n```\nfoo x y | seq x False = undefined\nfoo x y = actualFoo x y\n\n-- or, with a GHC extension\n\nfoo !x y = actualFoo x y\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Parallel reduction example\r\n                \r\nI found this parallel reduction code from Stanford which uses shared memory.\nThe code is an example of ```\n1<<18```\n number of elements which is equal to 262144 and produces correct results.\nWhy do I get the correct results for certain numbers of elements and for other numbers of elements, like 200000 or 25000 I get different, unexpected results?\nIt looks to me as if it's always appointing the needed thread blocks.\n    ", "Answer": "\r\nThis code causes the bug:\n```\n// launch a single block to compute the sum of the partial sums\nblock_sum<<<1,num_blocks,num_blocks * sizeof(float)>>>\n```\n\nSuppose ```\nnum_blocks```\n is 13, then in the kernel ```\nblockDim.x / 2```\n will be 6, and\n```\nif(threadIdx.x < offset)\n{\n    // add a partial sum upstream to our own\n    sdata[threadIdx.x] += sdata[threadIdx.x + offset]; \n}\n```\n\nwill only add the first 12 elements causing the bug.\nWhen the element count is 200000 or 250000, ```\nnum_blocks```\n will be odd and cause the bug, while for even ```\nnum_blocks```\n it will work fine.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How do I send MetaData HTTP POST Request\r\n                \r\nSuppose i want to send these type of Requests which has \"meta\":{} inside the json data, how do I go about it?\nI have this as json data\n```\n{\n  \"amount\": 500,\n  \"narration\": \"Test Int'l bank transfers\",\n  \"currency\": \"USD\",\n \"beneficiary_name\": \"Mark Cuban\",\n  \"meta\":\n    {\n     \"AccountNumber\": \"09182972BH\",\n     \"RoutingNumber\": \"0000000002993\",\n     \"SwiftCode\": \"ABJG190\",\n     \"BankName\": \"BANK OF AMERICA, N.A., SAN FRANCISCO, CA\",\n     \"BeneficiaryName\": \"Mark Cuban\",\n     \"BeneficiaryAddress\": \"San Francisco, 4 Newton\",\n     \"BeneficiaryCountry\": \"US\"\n    }\n}\n```\n\nIf i try to send this in Postman , i get an Error , like this\n\nThen when I send this as a normal request like below, say something like this\n```\n{\n  \"account_bank\":\"044\",\n  \"account_number\":\"09182972BH\",\n  \"RoutingNumber\":\"0000000002993\",\n  \"SwiftCode\": \"ABJG190\",\n  \"amount\":\"450.40\",\n  \"email\":\"john_kross2@yopmail.com\",\n  \"narration\":\"test USD transfer\",\n  \"currency\":\"NGN\",\n  \"reference\":\"akhlm-pstmnpyt-rfxx007_PMCKDU_6876677\",\n  \"debit_currency\":\"USD\"\n}\n```\n\n, it gives me this with HTTP 200, but does not work as I expect it to\n\nThe code in Question that is supposed to work is Looking thus :\n```\nrouter.post(\"/us-transfer\", async (req, res, next) => {\n  try {\n    if (\n      !req.headers.authorization ||\n      !req.headers.authorization.startsWith(\"Bearer \") ||\n      !req.headers.authorization.split(\" \")[1]\n    ) {\n      return res.status(422).json({ message: \"Please Provide Token!\" });\n    }\n\n    var AccountNumber = req.body.AccountNumber;\n    var RoutingNumber = req.body.RoutingNumber;\n    var SwiftCode = req.body.SwiftCode;\n    var amount = req.body.amount;\n    var narration = req.body.narration;\n    var email = req.body.email;\n    var currency = req.body.currency;\n    var reference = req.body.refernce;\n    var debit_currency = req.body.debit_currency;\n    var beneficiary_name = req.body.beneficiary_name;\n\n    url = \"https://api.flutterwave.com/v3/transfers\";\n    await fetch(url, {\n      method: \"POST\",\n      headers: {\n        \"Accept\": \"application/json\",\n        \"Content-type\": \"application/json\",\n        \"Authorization\": \"Bearer FLWSECK_TEST-153740d351951b5f6d5ae8b903e0c467-X\",\n      },\n      body: JSON.stringify({\n        AccountNumber: AccountNumber,\n        RoutingNumber : RoutingNumber,\n        SwiftCode : SwiftCode,\n        amount: amount,\n        narration: narration,\n        currency: currency,\n        reference: reference,\n        debit_currency: debit_currency,\n        beneficiary_name: beneficiary_name,\n      }),\n    })\n      .then((response) => response.json())\n      .then(async (json) => {\n\n        const debit = await User.find({ email: email});\n        const debit_balance = parseInt(debit[0].balance);\n        const debit_amt = debit_balance - amount;\n        await User.findOneAndUpdate({email : email}, {$set: {balance: debit_amt}});\n\n        const transactions = new Trans({\n          email: email,\n          narration: narration,\n          credit: 0.0,\n          debit: amount,\n          amount: amount,\n        });\n        try {\n          transactions.save();\n          //res.send(savedUser);\n          console.log(\"transaction saved\");\n        } catch (err) {\n          //res.status(400).send(err);\n          console.log(err);\n        }\n\n        //sendTransferConfirmation(email);\n        return res.send({ error: false, data: json, message: \"US Bank Transfer Complete\" });\n      })\n      .catch((err) => {\n        throw err;\n      });\n  } catch (err) {\n    next(err);\n  }\n});\n```\n\nI do not seem to know what is going on. Kindly help.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Parentheses Reduction\r\n                \r\nI have written the following:\n\n```\nif ( ( ( weight < 160 && ( age <= 27 && age >= 22 ) )\n    && ( ( height < 72 ) && ( ( !isASmoker ) && ( isMale ) ) )\n    && ( ( isGoodLooking ) && ( isAbleToRelocate) ) ) ) {\n```\n\n\nCan I reduce the parentheses? \n\nThanks\n    ", "Answer": "\r\nAs correctly stated in Turing85's comment, you can actually remove every parenthesis except the outer ones\n\n```\nif  (weight < 160 && age <= 27 && age >= 22 \n        && height < 72 &&   !isASmoker  &&  isMale   \n        &&   isGoodLooking  &&  isAbleToRelocate  )\n```\n\n\nthis is the minimum number of parethesis you can use. The maximum number is... well, virtually infinite (finite number, obviously, but infinite possibilities). You can add as many as you like, as long as it is sintactically correct.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA reduction optimizations\r\n                \r\nI am trying to do all the optimizations seen at Nvidia Reduction. I have implemented the first four parts but I am stuck with part #5 at slide no 22.\n\nI am unable to realize the reason as to why the provided code can work without any syncthreads(). The threads have accesses to same memory locations in the output.\n\nMoreover, the slide suggests that the code won't work if the variables are not set to volatile. How does being volatile help in that aspect? If I don't want to call the kernel, what is best way to program it?\n\nI am also putting that code here for reference.\n\n```\n__device__ void warpReduce(volatile int* sdata, int tid) {\nsdata[tid] += sdata[tid + 32];\nsdata[tid] += sdata[tid + 16];\nsdata[tid] += sdata[tid + 8];\nsdata[tid] += sdata[tid + 4];\nsdata[tid] += sdata[tid + 2];\nsdata[tid] += sdata[tid + 1];\n}\n\nfor (unsigned int s=blockDim.x/2; s>32; s>>=1) {\nif (tid < s)\nsdata[tid] += sdata[tid + s];\n__syncthreads();\n}\n\nif (tid < 32) warpReduce(sdata, tid);\n```\n\n\nThanks in advance for your help. Please comment if further info needed.\n    ", "Answer": "\r\nThe code relies on what is called warp-synchronous programming. It was a common practice to avoid ```\n__syncthreads()```\n within a warp. However, this behaviour is undocumented and actually now NVIDIA strongly discourages writing code which relies on that behaviour.\n\nFrom the Kepler tuning guide:\n\n\n  The absence of an explicit synchronization in a program where different threads communicate via memory constitutes a data race condition or synchronization error. Warp-synchronous programs are unsafe and easily broken by evolutionary improvements to the optimization strategies used by the CUDA compiler toolchain\n\n\nThe examples that you mention are included in the samples which come with the CUDA toolkit. If you look into the recent version you will find that this part of the reduction is now implemented with warp shuffle operations for compute capability >= 3.0 and uses ```\n__syncthreads()```\n for older devices as you would expect. In older samples (e.g. in the CUDA toolkit 6.0) it was still implemented with the warp synchronous techniques.\n\nIf you still want to learn about warp-synchronous programming I recommend this answer.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "noise reduction\r\n                \r\nI have an image with uniform intensity everywhere with gray value = 100 then i added additive zero-mean independent Gaussian noise with standard deviation = 5\n\n```\nI = ones(100,100)*100;\nI_n=I+(5*randn(100,100));\n```\n\n\nI think that the mean and standard deviation of the pixel intensity in the noisy image will be 100 and 5 respectively,\n\nthen i want to reduce the noise in the noisy image by 2x2 averaging mask.\n\nwhat is the effect of the averaging mask on the mean and standard deviation of the pixel intensity in the image?\nis it better to increase the size of the mask?\n    ", "Answer": "\r\nfor a uniform original image, and uniform noise, averaging won't change the mean.  it will reduce the variation between pixels, but also make the noise between adjacent pixels correlated.\n\nif you calculated the standard deviation then you would find that the value is 2.5 (reduced by a factor of 2, 2 = sqrt(4), where you averaged 4 values).\n\nusing a larger mask will reduce the noise further, but correlate it over more pixels.  it will also blur more any structure in the underlying image (not an issue in this case, because it is uniform).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Conditions reduction\r\n                \r\nI'm calculating the displayed payment amount for an order receipt, I want to reduce those conditions to the max:\n\n```\nprivate double CalculateFinalTotal(bool hasPrommoCode, \n                                   double promoCodeValue, \n                                   double finalTotal, \n                                   double? tip)\n{\n    if (!hasPrommoCode) return finalTotal;\n    if (promoCodeValue > finalTotal && tip.HasValue) return tip.Value;\n    else if (promoCodeValue > finalTotal) return 0;\n    else if (tip.HasValue)\n    {\n        var totalWithoutTip = finalTotal - tip.Value;\n        return (totalWithoutTip > promoCodeValue ? totalWithoutTip - promoCodeValue : 0) + tip.Value;\n    }\n    else return finalTotal - promoCodeValue;\n}\n```\n\n    ", "Answer": "\r\nFirst of all, never use ```\ndouble```\n for money-related code, always use ```\ndecimal```\n.\n\nBesides that, you can omit checking whether ```\ntip```\n has a value or not using ```\nGetValueOrDefault()```\n. And, following c# conventions (if you actually care about readability), always use brackets around ```\nif```\n statements.\n\nYou should end up with something like:\n\n```\nprivate decimal CalculateFinalTotal(bool hasPrommoCode, decimal promoCodeValue, \n                                   decimal finalTotal, decimal? tip)\n{\n    if (!hasPrommoCode) \n    {\n        return finalTotal;\n    }\n\n    if (promoCodeValue > finalTotal) \n    {\n        // if .HasValue == true => return .Value; otherwise return 0\n        return tip.GetValueOrDefault();\n    }\n\n    if (tip.HasValue)\n    {\n        var totalWithoutTip = finalTotal - tip.Value;\n        return (totalWithoutTip > promoCodeValue ? totalWithoutTip - promoCodeValue : 0) + tip.Value;\n    }\n\n    return finalTotal - promoCodeValue;\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "About pytorch reduction mean\r\n                \r\nI want use L1loss and BCELoss with reduction='mean' in vae reconstruction loss\nbut it produce same result for all different input i.e. result for landmark\nso i use reduction='sum' it produce correct result that different output for different input.\nhow can i use mean reduction??\n```\nL1Loss = nn.L1Loss(reduction='mean').to(device)   \nBCELoss = nn.BCELoss(reduction='mean').to(device)\nkld_criterion = KLDLoss(reduction='mean').to(device)\n```\n\nin training\n```\nrec_m, (rec_f, mean_f, logvar_f), (rec_l, mean_l, logvar_l) = model(origin)\n\nlm_loss = CELoss(rec_l, lm)\nf_loss = L1Loss(rec_f, f)\nm_loss = CELoss(rec_m, m)\n                \nlm_kld_loss = kld_criterion(mean_l, logvar_l)\nf_kld_loss = kld_criterion(mean_f, logvar_f)\n                \nloss = 4000*(f_loss + m_loss) + 30 * (lm_kld_loss + f_kld_loss) + 2000 * lm_loss\n```\n\nand model code\n```\nclass VAE_NET(nn.Module):\n    def __init__(self, nc=3, ndf=32, nef=32, nz=128, isize=128, device=torch.device(\"cuda:0\"), is_train=True):\n        super(VAE_NET, self).__init__()\n\n        self.nz = nz\n        # Encoder\n        self.l_encoder = Encoder(nc=nc, nef=nef, nz=nz, isize=isize, device=device)\n        self.f_encoder = Encoder(nc=nc, nef=nef, nz=nz, isize=isize, device=device)\n        \n        # Decoder\n        self.l_decoder = Decoder(nc=nc, ndf=ndf, nz=nz, isize=isize)\n        self.m_decoder = Decoder(nc = nc, ndf = ndf, nz = nz * 2, isize = isize)\n        self.f_decoder = Decoder(nc = nc, ndf = ndf, nz = nz * 2, isize = isize)\n\n        if is_train == False:\n            for param in self.encoder.parameters():\n                param.requires_grad = False\n            for param in self.decoder.parameters():\n                param.requires_grad = False\n\n    def forward(self, x):\n        latent_l, mean_l, logvar_l = self.l_encoder(x)\n        latent_f, mean_f, logvar_f = self.f_encoder(x)\n        concat_latent = torch.cat((latent_l, latent_f), 1)\n        rec_l = self.l_decoder(latent_l)\n        rec_m = self.m_decoder(concat_latent)\n        rec_f = self.f_decoder(concat_latent)\n        return rec_m, (rec_f, mean_f, latent_f), (rec_l, mean_l, latent_l)\n```\n\nl is for face landmark\nm is for face mask\nf is for face part\n    ", "Answer": "\r\n```\nreduction='sum'```\n and ```\nreduction='mean'```\n differs only by a scalar multiple. There is nothing wrong with your implementation from what I see. If your model only produces correct results with ```\nreduction='sum'```\n, it is likely that your learning rate is too low (and ```\nsum```\n makes up for that difference by amplifying the gradient).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Scheme: Beta-Reduction Challenge\r\n                \r\nMy teacher has given the class some sample exam questions (the class is basically on Scheme (Racket) and the lambda calculus), and I've hit a wall with the following problem:\n\n\n  Define ```\n(β-reduce e)```\n such that, when a β-reduction is possible (i.e., when ```\ne```\n is of the form ```\n((λ v e1) e2)```\n and there are no free variable conflicts that block the β-reduction and would require some α-renaming first), it returns the result of the β-reduction. Otherwise, it returns ```\n#f```\n.\n\n\nExample:\n\n```\n(β-reduce '((λ x (((λ x (x y)) x) (x b))) z)) ⇒ (((λ x (x y)) z) (z b))\n(β-reduce '((λ x (((λ y (x y)) x) (x b))) y)) ⇒ #f\n```\n\n\nI haven't gotten very far (to say the least). I started thinking that I should look for something in the form ```\n((λ x (x)) y)```\n as that's when I'll need to do a beta reduction, but I'm getting stuck on how to replace the inner expression with ```\ny```\n and then recurse inside of it if there were more sub expressions. Also, I'm really not sure when I would return ```\n#f```\n.\n\nHere's what I've got:\n\n```\n(define beta-reduce \n  (lambda (e)\n    (cond \n      ((and (equal? 'lambda (caar e)) (symbol? (last e))) \n       replace inner bound variable and recurse ))))\n```\n\n\nIf anyone could help me get to the bottom of this, I would be very grateful!\n    ", "Answer": "\r\nThis is not a complete answer but an attempt to get you started.\n\nYou need to consider arbitrary expressions for ```\ne1```\n and ```\ne2```\n in ```\n((λ v e1) e2)```\n.\n\nAnother example:\n\n```\n(β-reduce '((λ x (x x)) (λ y y)) ⇒ ((λ y y) (λ y y))\n```\n\n\nDon't try to write the entire thing as one big function, break the problem down into manageable pieces.\nYou'll probably want the following:\n\n\nA different way of determining that your expression is a function application.\nA function ```\nfree-variables```\n that takes an expression and returns the names that occur free in the expression.\nA function ```\nsubstitute-variable```\n that takes a variable ```\nv```\n, an expression ```\nE1```\n, and an expression ```\nE2```\n, and replaces all free occurences of ```\nv```\n in ```\nE1```\n with ```\nE2```\n.\n\n\nThe substition is possible if the intersection of ```\n(free-variables '(λ v E1))```\n and ```\n(free-variables E2)```\n is empty.\n\nIf your course has a reasonable structure, you've already written these functions, or something very similar to them.\n\nAnd remember that the reduction only takes one \"step\" - you should only reduce the outermost application, as shown by the first example you were given.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction with overloaded operator\r\n                \r\nI'm trying to parallelize the loop in the following function with OpenMP\n\n```\n void CEnergymulti::forcetwobody(vector<CMolecule*>  m_mols,CPnt force0,CPnt torque0)\n{\n const int nmol=m_mols.size();\n vector<CMolecule*> twomols(2);\n CPnt forcetemp,torquetemp;\n twomols.clear();\n force0.zero();\n torque0.zero();\n forcetemp.zero();\n torquetemp.zero();\n #pragma omp parallel for reduction(+:force0,torque0) private(twomols)\n for(int j=1;j<nmol;j++)\n       { twomols.push_back(m_mols[0]);\n         twomols.push_back(m_mols[j]);\n         CMolecule::polarize_mutual(twomols,false, 1000);\n         twomols[0]->computeMol_Force_and_Torque(forcetemp,torquetemp);\n         force0+=forcetemp;\n         torque0+=torquetemp;\n         forcetemp.zero();\n         torquetemp.zero();\n         twomols.clear();\n        }\n     REAL converter=COUL_K*IKbT;\n     force0*=converter;\n     torque0*=converter;\n     return;\n     }\n```\n\n\nWhen I compile the code, it gives the following message:\n\n```\nEnergyD_multi.cpp: In static member function ‘static void\nCEnergymulti::forcetwobody(std::vector<CMolecule*,\nstd::allocator<CMolecule*> >, CPnt, CPnt)’: EnergyD_multi.cpp:226:\nerror: ‘torque0’ has invalid type for ‘reduction’\nEnergyD_multi.cpp:226: error: ‘force0’ has invalid type for\n‘reduction’\n```\n\n\nI understand that variables 'force0' and 'torque0' are neither double or integer type of data, but of type 'CPnt', a class that is defined to represent three-dimensional vectors in space. For class 'CPnt', operator '+' and '-' have already been defined by operator overloading. So my questions is: is it true that reduction in OpenMP cannot handle such overloaded operators? Is there any alternate ways to parallelize this loop with OpenMP without doing reduction on each component of 'force0' and 'torque0'? \n\nThanks a lot.\n    ", "Answer": "\r\nIt's true that OpenMP reduction can't handle such overloaded operators.  However, there is an alternative.  One way to rewrite a reduction in OpenMP is to use the ```\nnowait```\n and ```\natomic```\n paramters.  http://bisqwit.iki.fi/story/howto/openmp/#ReductionClause\n.This is just as fast as the normal way.  \n\nIf you replace ```\natomic```\n with ```\ncritical```\n you can use more complex overloaded operators.  This is not as fast as using ```\natomic```\n but it's still works well in my experience.\n\nI did this so I could use operators that operate on 4 or 8 floats at once (with SEE or AVX).\nreduction with OpenMP with SSE/AVX\n\nEdit: I changed your code to reflect what I think would do what you want.\n\n```\nvoid CEnergymulti::forcetwobody(vector<CMolecule*>  m_mols,CPnt force0,CPnt torque0)\n{\n    const int nmol=m_mols.size();\n    force0.zero();\n    torque0.zero();\n    #pragma omp parallel\n    {\n        CPnt force0_private;\n        CPnt torque0_private; \n        force0_private.clear();\n        torque0_private.clear();\n        #pragma omp for nowait\n        for(int j=1;j<nmol;j++)\n        { \n            CPnt forcetemp,torquetemp;\n            forcetemp.zero();\n            torquetemp.zero();\n            vector<CMolecule*> twomols(2);\n            twomols.clear();\n            twomols.push_back(m_mols[0]);\n            twomols.push_back(m_mols[j]);\n            CMolecule::polarize_mutual(twomols,false, 1000);\n            twomols[0]->computeMol_Force_and_Torque(forcetemp,torquetemp);\n            force0_private+=forcetemp;\n            torque0_private+=torquetemp;\n        }\n        #pragma omp critical \n        {\n           force0 += force0_private;\n           torque0 += torque0_private;\n        }\n\n    }\n    REAL converter=COUL_K*IKbT;\n    force0*=converter;\n    torque0*=converter;\n    return;\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Parallel reduction of an array on CPU\r\n                \r\nIs there a way to do parallel reduction of an array on CPU in C/C++?. I recently learnt that it's not possible using openmp. Any other alternatives?\n    ", "Answer": "\r\nAdded: Note that you can implement \"custom\" reduction with OpenMP, in the way described here.\n\n\n\nFor C++: with ```\nparallel_reduce```\n in Intel's TBB (SO tag: tbb), you can make reduction on complex types such as arrays and structs. Though the amount of required code can be significantly bigger compared to OpenMP's reduction clause.\n\nAs an example, let's parallelize a naive implementation of matrix-to-vector multiplication: ```\ny=Cx```\n. Serial code consists of two loops:\n\n```\ndouble x[N], y[M], C[N][M];\n// assume x and C are initialized, and y consists of zeros\nfor(int i=0; i<N; ++i)\n  for(int j=0; j<M; ++j) \n    y[j] += C[i][j]*x[i];\n```\n\n\nUsually, to parallelize it the loops are exchanged to make the outer loop iterations independent and process them in parallel:\n\n```\n#pragma omp parallel for\nfor(int j=0; j<M; ++j) \n  for(int i=0; i<N; ++i)\n    y[j] += C[i][j]*x[i];\n```\n\n\nHowever it's not always good idea. If M is small and N is large, swapping the loop won't give enough parallelism (for example, think of calculating a weighted centroid of N points in M-dimensional space, with ```\nC```\n being the array of points and ```\nx```\n being the array of weights). So a reduction over an array (i.e. a point) would be helpful. Here is how it can be done with TBB (sorry, the code was not tested, errors are possible):\n\n```\nstruct reduce_body {\n  double y_[M]; // accumulating vector\n  double (& C_)[N][M]; // reference to a matrix\n  double (& x_)[N];    // reference to a vector\n\n  reduce_body( double (&C)[N][M], double (&x)[N] )  : C_(C), x_(x) {\n    for (int j=0; j<M; ++j) y_[j] = 0.0; // prepare for accumulation\n  }\n  // splitting constructor required by TBB\n  reduce_body( reduce_body& rb, tbb::split ) : C_(rb.C_), x_(rb.x_) { \n    for (int j=0; j<M; ++j) y_[j] = 0.0;\n  }\n  // the main computation method\n  void operator()(const tbb::blocked_range<int>& r) {\n    // closely resembles the original serial loop\n    for (int i=r.begin(); i<r.end(); ++i) // iterates over a subrange in [0,N)\n      for (int j=0; j<M; ++j)\n        y_[j] += C_[i][j]*x_[i];\n  }\n  // the method to reduce computations accumulated in two bodies\n  void join( reduce_body& rb ) {\n    for (int j=0; j<M; ++j) y_[j] += rb.y_[j];\n  }\n};\ndouble x[N], y[M], C[N][M];\n...\nreduce_body body(C, x);\ntbb::parallel_reduce(tbb::blocked_range<int>(0,N), body);\nfor (int j=0; j<M; ++j)\n  y[j] = body.y_[j]; // copy to the destination array\n```\n\n\nDisclaimer: I am affiliated with TBB.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "RxJS reduce doesn't continue\r\n                \r\nWhy doesn't the flatMap cause downstream reductions to fire?\n\nI got code like:\n\n```\nhandleFiles.flatMap(files =>\n  Rx.Observable.from(files).\n  flatMap((file, i) => fileReader(file, i)).\n  reduce((form, file, i) => {\n    form.append('file[' + i + ']', result);\n    console.log('reduce step', file);\n    return form;\n  }, new FormData()).\n  tap(console.log.bind(console, 'after reduce'))\n).\nsubscribe(console.log.bind(console, 'response'));\n```\n\n\nAnd the problem is that the 'after reduce' tap is never hit. Why?\n\nThe log is like:\n\n```\nreduce step [data]\nreduce step [data]\n```\n\n\nScreenshot:\n\n\n    ", "Answer": "\r\nThe problem isn't in ```\nflatMap```\n; it's in the way ```\nreduce```\n works.\n\n```\nreduce```\n reads in a whole stream and reduces it to a single value, emitted only when the source stream is closed. If your ```\nfrom(files)```\n stream doesn't end, then ```\nreduce```\n will never output its value.\n\nTry using ```\nscan```\n instead; it emits each intermediate step and seems to be what you're looking for.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Performance slowdown using a pointer to perform a reduction vs. using a normal variable\r\n                \r\nI need to perform an OpenMP reduction in C using a pointer variable (single element). As far as I know, the two alternatives to achieve this are:\n```\ndouble ptr* = ...\n#pragma omp for reduction(max:ptr[0])\nor\n#pragma omp for reduction(max:ptr[:1])\n```\n\nHowever, performing the reduction into a pointer variable is significantly slower than using a normal variable.\nConsider this example:\n```\nint REP = 1000000, ITER = 1000;\n\ndouble red_var() {\n    double foo = 0.0;\n\n    #pragma omp parallel\n    for (int i = 0; i < REP; ++i) {\n        #pragma omp for reduction(+:foo)\n        for (int j = 0; j < ITER; ++j) {\n            foo += 1.0 / (j + 1);\n        }\n    }\n    return foo;\n}\n\ndouble red_ptr() {\n    double foos[1]; double *foo = foos;\n\n    #pragma omp parallel\n    for (int i = 0; i < REP; ++i) {\n        #pragma omp for reduction(+:foo[0])\n        for (int j = 0; j < ITER; ++j) {\n            *foo += 1.0 / (j + 1);\n        }\n    }\n    return *foo;\n}\n```\n\nIn my system, the first function (```\nred_var()```\n) is nearly 3x faster than the second function (```\nred_ptr()```\n).\nMy system:\n\nCPU: Intel Xeon Gold 5120\nOS: Ubuntu Server 22.04\nCompiler: GCC v11.3.0\nCompiler flags: -O3 -fopenmp\n\nAnalyzing the generated assembly code, I see that the compiler uses a ```\nlock cmpxchg```\n instruction to perform the reduction in the first function. In contrast, it uses an OpenMP atomic region for the pointer reduction. Is there any way to tell the compiler it can also use a ```\nlock cmpxchg```\n instruction when using a pointer?\nUpdate\n\n```\nlock cmpxchg```\n is not executed in each iteration of the inner for loop. The compiler uses private accumulators and only performs the reduction into the shared variable at the end of the loop.\nCreating a ```\nconst restrict```\n pointer to tell the compiler that there is no aliasing and that the pointer is never updated does not change the generated assembly.\nMoving the reduction from the ```\nomp for```\n to the ```\nomp parallel```\n improves the performance, but only because the reduction is performed only once. The compiler still creates an atomic region for the pointer reduction.\nCompiling the example in C++ and substituting the pointer with a reference does fix the issue. The generated assembly for the reference version uses ```\nlock cmpxchg```\n.\nCompiling with ```\n-ffast-math```\n or ```\n-march=native```\n does not seem to improve the generated assembly or the performance.\nClang (v14.0.0) and ICX (v2023.1.0) compilers use ```\nlock cmpxchg```\n in the two reductions. The binaries generated by these compilers show similar execution times for both functions.\n\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "dimensionality reduction algorithms\r\n                \r\nI have an  data sheet with almost 2000 input parameters and 4 output parameters.\nI am to optimize the input parameters to define the output\n\nI am not sure whether the input parameters are linearly related so I am looking for a non linear dimensionality reduction algorithm. Will Self organizing maps be a good option for reducing the 2000 input parameters to few  components.\n\nI did try PCA but it is not quite helpful. Please if anyone could suggest any   standard algorithms and codes  that I could try?\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction functions for rainbow tables\r\n                \r\nSince chains of rainbow tables are very long, are there many different reduction functions used to reduce each hash, or are there only a handful, while using indexes to prevent merging? or something else?\n    ", "Answer": "\r\nThe reduction functions of a rainbow table are all different (one per column), but are generally built as an extension of a single reduction function.\n\nFor instance, let r be a reduction function (say, r(x) = x mod N, where N is the size of your input set), then to generate a reduction function family, as one needed in rainbow tables, one could use r_i(x) = r(x+i).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction with overloaded operator\r\n                \r\nI'm trying to parallelize the loop in the following function with OpenMP\n\n```\n void CEnergymulti::forcetwobody(vector<CMolecule*>  m_mols,CPnt force0,CPnt torque0)\n{\n const int nmol=m_mols.size();\n vector<CMolecule*> twomols(2);\n CPnt forcetemp,torquetemp;\n twomols.clear();\n force0.zero();\n torque0.zero();\n forcetemp.zero();\n torquetemp.zero();\n #pragma omp parallel for reduction(+:force0,torque0) private(twomols)\n for(int j=1;j<nmol;j++)\n       { twomols.push_back(m_mols[0]);\n         twomols.push_back(m_mols[j]);\n         CMolecule::polarize_mutual(twomols,false, 1000);\n         twomols[0]->computeMol_Force_and_Torque(forcetemp,torquetemp);\n         force0+=forcetemp;\n         torque0+=torquetemp;\n         forcetemp.zero();\n         torquetemp.zero();\n         twomols.clear();\n        }\n     REAL converter=COUL_K*IKbT;\n     force0*=converter;\n     torque0*=converter;\n     return;\n     }\n```\n\n\nWhen I compile the code, it gives the following message:\n\n```\nEnergyD_multi.cpp: In static member function ‘static void\nCEnergymulti::forcetwobody(std::vector<CMolecule*,\nstd::allocator<CMolecule*> >, CPnt, CPnt)’: EnergyD_multi.cpp:226:\nerror: ‘torque0’ has invalid type for ‘reduction’\nEnergyD_multi.cpp:226: error: ‘force0’ has invalid type for\n‘reduction’\n```\n\n\nI understand that variables 'force0' and 'torque0' are neither double or integer type of data, but of type 'CPnt', a class that is defined to represent three-dimensional vectors in space. For class 'CPnt', operator '+' and '-' have already been defined by operator overloading. So my questions is: is it true that reduction in OpenMP cannot handle such overloaded operators? Is there any alternate ways to parallelize this loop with OpenMP without doing reduction on each component of 'force0' and 'torque0'? \n\nThanks a lot.\n    ", "Answer": "\r\nIt's true that OpenMP reduction can't handle such overloaded operators.  However, there is an alternative.  One way to rewrite a reduction in OpenMP is to use the ```\nnowait```\n and ```\natomic```\n paramters.  http://bisqwit.iki.fi/story/howto/openmp/#ReductionClause\n.This is just as fast as the normal way.  \n\nIf you replace ```\natomic```\n with ```\ncritical```\n you can use more complex overloaded operators.  This is not as fast as using ```\natomic```\n but it's still works well in my experience.\n\nI did this so I could use operators that operate on 4 or 8 floats at once (with SEE or AVX).\nreduction with OpenMP with SSE/AVX\n\nEdit: I changed your code to reflect what I think would do what you want.\n\n```\nvoid CEnergymulti::forcetwobody(vector<CMolecule*>  m_mols,CPnt force0,CPnt torque0)\n{\n    const int nmol=m_mols.size();\n    force0.zero();\n    torque0.zero();\n    #pragma omp parallel\n    {\n        CPnt force0_private;\n        CPnt torque0_private; \n        force0_private.clear();\n        torque0_private.clear();\n        #pragma omp for nowait\n        for(int j=1;j<nmol;j++)\n        { \n            CPnt forcetemp,torquetemp;\n            forcetemp.zero();\n            torquetemp.zero();\n            vector<CMolecule*> twomols(2);\n            twomols.clear();\n            twomols.push_back(m_mols[0]);\n            twomols.push_back(m_mols[j]);\n            CMolecule::polarize_mutual(twomols,false, 1000);\n            twomols[0]->computeMol_Force_and_Torque(forcetemp,torquetemp);\n            force0_private+=forcetemp;\n            torque0_private+=torquetemp;\n        }\n        #pragma omp critical \n        {\n           force0 += force0_private;\n           torque0 += torque0_private;\n        }\n\n    }\n    REAL converter=COUL_K*IKbT;\n    force0*=converter;\n    torque0*=converter;\n    return;\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Feature Reduction\r\n                \r\nHow do I reduce feature dimension ? My feature looks like : \n\n1(Class Number) 10_10_1(File name)  0   0   0   0   0   0   0   0   0.564971751 23.16384181 25.98870056 19.20903955 16.10169492 13.27683616 1.694915254 0   0   0   0   0   0   0   3.95480226  11.5819209  20.33898305 60.4519774  3.672316384 0   0   0   0   0   0   0   0   0   0   0   0   0   0   3.107344633 62.99435028 33.89830508 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1.412429379 66.66666667 31.92090395 0   0   0   0   0   0   0   0   0   0   0   0   0   0.564971751 22.59887006 26.83615819 46.89265537 3.107344633 0   0   0   0   0   0   0   0   0   0   0   0   0   0.564971751 16.38418079 28.53107345 50.84745763 3.672316384 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   90.6779661  9.322033898 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0.847457627 90.11299435 9.039548023 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   17.79661017 81.3559322  0.847457627 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   27.11864407 72.88135593 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0.564971751 37.85310734 61.29943503 0.282485876 0   0   0   0   0   0   0   0   0   0   0   0   0   0   1.412429379 50.84745763 47.45762712 0.282485876 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   24.57627119 75.42372881 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   17.23163842 82.20338983 0.564971751 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   29.37853107 70.62146893 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   55.64971751 44.35028249 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   64.40677966 35.59322034 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   67.79661017 32.20338983 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   66.66666667 33.33333333 0   0   0   0   0   0   0   0   0   0   0   0   1   3   2   6   7   5   4   8   9   10  11  12  13  14  15  16  17  18  14.81834463 3.818489078 3.292123621 2.219541777 2.740791003 1.160544518 2.820053602 1.006906813 0.090413195 2.246638594 0.269778302 2.183126126 2.239168249 0.781498607 2.229795302 0.743329919 1.293839141 0.783068011 1.104421291 0.770312707 0.697659061 1.082266169 0.408339745 1.073922207 0.999148017 0.602195061 1.247286588 0.712143548 0.867327913 0.603063537 0.474115683 0.596387106 0.370847522 0.54900076  0.35930586  0.580272233 0.397060362 0.535337691\n\nAfter filename, feature values are given.\n    ", "Answer": "\r\nIf your feature is unsupervised, you can use PCA.\n\n```\nimport numpy as np\nfrom sklearn.decomposition import PCA\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\npca = PCA(n_components=2)\npca.fit(X)\nPCA(copy=True, n_components=2, whiten=False)\nprint(pca.explained_variance_ratio_) \n```\n\n\nIf it is supervised, you can use LDA\n\n```\nimport numpy as np\nfrom sklearn.lda import LDA\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\ny = np.array([1, 1, 1, 2, 2, 2])\nclf = LDA()\nclf.fit(X, y)\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Mutable Reduction into a StringBuilder object\r\n                \r\nAs per the Java Stream Package Summary,\nA mutable reduction operation accumulates input elements into a mutable result container, such as a Collection or StringBuilder, as it processes the elements in the stream.  \n\nThere are numerous examples out there for performing a mutable reduction into a Collection.  However, I cannot figure out how to realize mutable reduction operation into a StringBuilder, especially given that StringBuilder does not seem to be part of any pertinent interface definition.\n    ", "Answer": "\r\nYou don't see ```\nStringBuilder```\n, because the result of using a ```\nStringBuilder```\n is a ```\nString```\n. The ```\nStringBuilder```\n is used behind the scenes.\n\nExample\n\n```\nString[] input = { \"Hell\", \"oW\", \"orld\" };\nString joined = Stream.of(input).collect(Collectors.joining());\nSystem.out.println(joined); // prints: HelloWorld\n```\n\n\nIf you do this without streaming, you'd do:\n\n```\nStringBuilder buf = new StringBuilder();\nfor (String s : input)\n    buf.append(s);\nString joined = buf.toString();\n```\n\n\nThat is also what the stream is doing. If you look at the source code of ```\njoining()```\n, you'll find:\n\n```\npublic static Collector<CharSequence, ?, String> joining() {\n    return new CollectorImpl<CharSequence, StringBuilder, String>(\n            StringBuilder::new, StringBuilder::append,\n            (r1, r2) -> { r1.append(r2); return r1; },\n            StringBuilder::toString, CH_NOID);\n}\n```\n\n\nAs you can see, it also uses ```\nnew StringBuilder()```\n, ```\nappend(CharSequence s)```\n, and ```\ntoString()```\n.\n\nThe other two overloads, ```\njoining(CharSequence delimiter)```\n and ```\njoining(CharSequence delimiter, CharSequence prefix, CharSequence suffix)```\n, use ```\nStringJoiner```\n instead of ```\nStringBuilder```\n.\n\n```\npublic static Collector<CharSequence, ?, String> joining(CharSequence delimiter) {\n    return joining(delimiter, \"\", \"\");\n}\n\npublic static Collector<CharSequence, ?, String> joining(CharSequence delimiter,\n                                                         CharSequence prefix,\n                                                         CharSequence suffix) {\n    return new CollectorImpl<>(\n            () -> new StringJoiner(delimiter, prefix, suffix),\n            StringJoiner::add, StringJoiner::merge,\n            StringJoiner::toString, CH_NOID);\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Memory reduction\r\n                \r\nI am not a programmer but am facing a programming problem. I need to reduce the memory consumption of a Python code. One option is to reduce the variable precision. In this regard:\n\n\nIdentify Floating points (as I think the data types are assigned on the fly)?\nPython uses 64-bit or 32-bit floating points?\nAny function to reduce floating precision or convert to int?\n\n\nKindly help me. These questions may be stupid, but I have very less knowledge of Programming.\n\nThanks...\n    ", "Answer": "\r\nIf you are already using ```\nnumpy```\n you can set the ```\ndtype```\n field to the type that you want (documentation). You'll get a little more flexibility there for typing, but in general you aren't going to get a lot of control over variable precision in Python.  You might also have some luck if you want to go through the structural overhead of using the static types from Cython, though if you are new to programming that may not be the best route. \n\nBeyond that, you haven't given us a lot to work with regarding your actual problem and why you feel that the variable precision is the best spot for optimization. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda reduction\r\n                \r\nI'm having trouble understanding how to reduce lambda terms to normal form. could someone help me to understand how to reduce this lambda expression? I don't know exactly where to start.\n(λx. ( λa. (λx. x a)) x) 20 (λx. x+3)\nThank you in advance\n    ", "Answer": "\r\nI'll just focus on your expression. For a deeper explanation, you could read the answer to this question.\nFirst of all, let's consider how brackets could be interpreted on these expressions. Consider the following expression:\n```\n(λx. x*x) 5 => 5*5 => 25\n```\n\nSo, evaluation of this expression is done replacing the x by value 5 outside brackets.\nNow, let's put some brackets from left to right on your expression:\n```\n(((λx. ( λa. (λx. x a)) x) 20) (λx. x+3))\n```\n\nLet's call (λa. (λx. x a)) the variable z as a simplification. So, we'll have:\n```\n(((λx. z x) 20) (λx. x+3))\n```\n\nand the expression is now similar to our first example. Replacing x by value 20, we'll have:\n```\n((z 20) (λx. x+3))\n```\n\nwhich is the same as:\n```\n(((λa. (λx. x a)) 20) (λx. x+3))\n```\n\nAgain, we can replace a by 20 resulting\n```\n((λx. x 20) (λx. x+3))\n```\n\nAfterwards, we could replace x at the left expression by the whole expression on the right (λx. x+3).\n```\n((λx. x+3) 20)\n```\n\nFinally, we could replace x by 20:\n```\n((λx. x+3) 20) => 20+3 = 23\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Sparsity reduction\r\n                \r\nI have to factorize a big sparse matrix ( 6.5mln rows representing users* 6.5mln columns representing items) to find users and items latent vectors. I chose the als algorithm in spark framework(pyspark).\nTo boost the quality I have to reduce the sparsity of my matrix till 98%. (current value is 99.99% because I have inly 356mln of filled entries).\nI can do it by dropping rows or columns, but I must find the optimal solution maximizing number of rows(users).\nThe main problem is that I must find some subsets of users and items sets, and dropping some row can drop some columns and vice versa, the second problem is that function that evaluates sparsity is not linear.\nWhich way I can solve this problem? which libraries in python can help me with it? \nThank you.\n    ", "Answer": "\r\nThis is a combinatorial problem. There is no easy way to drop an optimal set of columns to achieve max number of users while reducing sparsity. A formal approach would be to formulate it as a mixed-integer program. Consider the following 0-1 matrix, derived from your original matrix C.\n\n```\nA(i,j) = 1 if C(i,j) is nonzero, \nA(i,j) = 0 if C(i,j) is zero\n```\n\n\nParameters:\n\n```\nM : a sufficiently big numeric value, e.g. number of columns of A (NCOLS)\nN : total number of nonzeros in C (aka in A)\n```\n\n\nDecision vars are\n\n```\nx(j) : 0 or 1  implying whether column j is dropped or not\nnr(i): number of nonzeros covered in row i\ny(i) : 0 or 1 implying whether row i is dropped or not\n```\n\n\nConstraints:\n\n```\nA(i,:) x  = nr(i)         for i = 1..NROWS\nnr(i)    <= y(i) * M      for i = 1..NROWS\n@sum(nr(i)) + e = 0.98 * N  # explicit slack 'e' to be penalized in the objective\ny(i) and x(j) are 0-1 variables (binary variables) for i,j\n```\n\n\nObjective:\n\n```\nmaximize @sum(y(i)) - N.e\n```\n\n\nSuch a model would be extremely difficult to solve as an integer model. However, barrier methods should be able to solve the linear programming relaxations (LP) Possible solvers are Coin/CLP (open-source), Lindo (commercial) etc... It may then be possible to use the LP solution to compute approximate integer solutions by simple rounding. \n\nIn the end, you will definitely require an iterative approach which will require solving MF problem several times each time factoring a different submatrix of C, computed with above approach, until you are satisfied with the solution.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction of matrix rows in OpenCL\r\n                \r\nI have an matrix which is stored as 1D array in the GPU, I'm trying to make an OpenCL kernel which will use reduction in every row of this matrix, for example:\n\nLet's consider my matrix is 2x3 with the elements [1, 2, 3, 4, 5, 6], what I want to do is:\n\n```\n[1, 2, 3] = [ 6]\n[4, 5, 6]   [15]\n```\n\n\nObviously as I'm talking about reduction, the actual return could be of more than one element per row:\n\n```\n[1, 2, 3] = [3, 3]\n[4, 5, 6]   [9, 6]\n```\n\n\nThen the final calculation I can do in another kernel or in the CPU.\n\nWell, so far what I have is a kernel which do the reduction but using all the elements of the array, like so:\n\n```\n[1, 2, 3] = [21]\n[4, 5, 6]\n```\n\n\nThe actual reduction kernel for doing this is that one (which I got from here in stackoverflow actually):\n\n```\n__kernel void\nsum2(__global float *inVector, __global float *outVector,\n     const unsigned int inVectorSize, __local float *resultScratch)\n{\n  const unsigned int localId = get_local_id(0);\n  const unsigned int workGroupSize = get_local_size(0);\n\n  if (get_global_id(0) < inVectorSize)\n    resultScratch[localId] = inVector[get_global_id(0)];\n  else\n    resultScratch[localId] = 0;\n\n  for (unsigned int a = workGroupSize >> 1; a > 0; a >>= 1)\n  {\n    barrier(CLK_LOCAL_MEM_FENCE);\n    if (a > localId)\n      resultScratch[localId] += resultScratch[localId + a];\n  }\n\n  if (localId == 0)\n    outVector[get_group_id(0)] = resultScratch[0];\n  barrier(CLK_LOCAL_MEM_FENCE);\n}\n```\n\n    ", "Answer": "\r\nI suppose one solution is to modify your reduction kernel, so it can make reduction of the part of the array.\n\n```\n__kernel void\nsum2(__global float *inVector,\n     __global float *outVector,\n     unsigned int   inVectorOffset,\n     unsigned int   inVectorSize,\n     __local float  *resultScratch)\n{\n  const unsigned int localId = get_local_id(0);\n  const unsigned int workGroupSize = get_local_size(0);\n\n  if (get_global_id(0) < inVectorSize)\n    resultScratch[localId] = inVector[inVectorOffset + get_global_id(0)];\n  else\n    resultScratch[localId] = 0;\n\n  for (unsigned int a = workGroupSize >> 1; a > 0; a >>= 1)\n  {\n    barrier(CLK_LOCAL_MEM_FENCE);\n    if (a > localId)\n      resultScratch[localId] += resultScratch[localId + a];\n  }\n\n  if (localId == 0)\n    outVector[get_group_id(0)] = resultScratch[0];\n  barrier(CLK_LOCAL_MEM_FENCE);\n}\n```\n\n\nThen you can do reduction of a row of a matrix, providing as inVectorOffset the beginning of your row and as inVectorSize number of elements in the row.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Duplicates Reduction\r\n                \r\nIf I have a string say \"Hello\" n I wants to remove the duplicates from the string. If I uses StringBuilder to remove the character at index 2 \"l\" , will the next \"l\" takes position of the previous one after deleting it??\n    ", "Answer": "\r\nLet's get issue simple. We don't need to delete duplicate character. We just remove the charachter at index 2. The string is \"abcde\".\n\n```\nStringBuilder sb = new StringBuilder(\"abcde\");\nchar one = sb.charAt(2); // comment1, we know it's 'c'\nsb = sb.deleteCharAt(2); // comment2, let's delete 'c'. What would \"abcde\" be?\n// It's so ugly to be \"ab de\", so designer makes it be \"abde\"\nchar two = sb.charAt(2); // comment3, now you will know the result ^v^\n```\n\n\nFor comment2, how does it make \"abcde\" to \"abde\"? Let's look into source of StringBuilder. StringBuilder is a subClass of AbstractStringBuilder. And deleteCharAt(int index) is implemented in AbstractStringBuilder.\n\n```\npublic AbstractStringBuilder deleteCharAt(int index) {\n    if ((index < 0) || (index >= count))\n        throw new StringIndexOutOfBoundsException(index);\n    System.arraycopy(value, index+1, value, index, count-index-1);\n    count--;\n    return this;\n}\n```\n\n\nLet's omite index validation. What does ```\nSystem.arraycopy(value, index+1, value, index, count-index-1);```\n do? You can find it's doc here.\n\n```\npublic static void arraycopy(Object src,\n                             int srcPos,\n                             Object dest,\n                             int destPos,\n                             int length)\n```\n\n\n\n  Copies an array from the specified source array (which is \"abcde\"), beginning at the\n  specified position (```\nindex+1```\n, which is 3), to the specified position (```\nindex```\n, which is 2) of the destination\n  array (which is \"abcde\" too). A subsequence of array components are copied from the source\n  array referenced by ```\nsrc```\n to the destination array referenced by ```\ndest```\n (```\nsrc```\n and ```\ndest```\n are the same here, \"abcde\").\n  The number of components copied is equal to the ```\nlength```\n argument (```\nlength```\n here is 5-2-1, it's 2. So it will copy 2 chars). The\n  components at positions ```\nsrcPos```\n through ```\nsrcPos+length-1```\n in the source\n  array are copied into positions ```\ndestPos```\n through ```\ndestPos+length-1```\n (```\nsrcPos```\n here is 2+1, it's 3. ```\nsrcPos+length-1```\n is 3+2-1, it's 4),\n  respectively, of the destination array.\n\n\nSo ```\nSystem.arraycopy(value, index+1, value, index, count-index-1);```\n means copy 2 chars from index 3, to override index 2, override from index 2 to 3. Copy 'de' to override 'cd'. So after ```\nSystem.arraycopy(value, index+1, value, index, count-index-1);```\n, makes ```\nsb.value```\n to be \"abdee\"(Delete 'l' already.). then count--, means \"abdee\" remove last char, so it becomes \"abde\". \n\n```\nwill the next \"l\" takes position of the previous one after deleting it?```\n Yes, it will.\n\nSee? Coding is not so hard and not so mystic. And the direct way is code yourself and run it.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "cuda Parallel reduction #6 works and reduction #7 fails\r\n                \r\nIm using this code for reduction:\n\nhttp://www.math.nsysu.edu.tw/~lam/MPI/code/cuda/reduction.cu\n\nthat is based on Mark Harris talk as in here\n\nhttp://www.math.nsysu.edu.tw/~lam/MPI/lecture/reduction.pdf\n\nBut for \n\n```\n#define blocksize 1024\n#define gridsize  1024*8\n#define size blocksize*gridsize\n```\n\n\nKernel reduce6 works and reduce7 fails. Is it bcos reduce7 is dependant on amount of shared memory that size has to reach even \"size\" defined above?\n\nCode snippet is here:\n\n```\n#define THR_PER_BLC 1024\n#define BLC_PER_GRD  16\n#define GRID_SIZE THR_PER_BLC * BLC_PER_GRD\n\ntemplate<unsigned int nThreads>\n__global__ void reduce7(int *g_idata, int *g_odata, unsigned int n) {\n     //I added GRID_SIZE myself so it can be volatile\n     __shared__ volatile  int sdata[THR_PER_BLC]; \n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x * (nThreads * 2) + threadIdx.x;\n    unsigned int gridSize = nThreads * 2 * gridDim.x;\n    sdata[tid] = 0;\n    while (i < n) {\n        sdata[tid] += g_idata[i] + g_idata[i + nThreads];\n        i += gridSize;\n    }\n    __syncthreads();\n// reduction in shared memory\n    if (nThreads >= 512) {\n        if (tid < 256) { sdata[tid] += sdata[tid + 256]; }\n        __syncthreads();\n    }\n    if (nThreads >= 256) {\n        if (tid < 128) { sdata[tid] += sdata[tid + 128]; }\n        __syncthreads();\n    }\n    if (nThreads >= 128) {\n        if (tid < 64) { sdata[tid] += sdata[tid + 64]; }\n        __syncthreads();\n    }\n    if (tid < 32) {\n        if (nThreads >= 64) sdata[tid] += sdata[tid + 32];\n        if (nThreads >= 32) sdata[tid] += sdata[tid + 16];\n        if (nThreads >= 16) sdata[tid] += sdata[tid + 8];\n        if (nThreads >= 8) sdata[tid] += sdata[tid + 4];\n        if (nThreads >= 4) sdata[tid] += sdata[tid + 2];\n        if (nThreads >= 2) sdata[tid] += sdata[tid + 1];\n// transfer of the result to global memory\n        if (tid == 0) g_odata[blockIdx.x] = sdata[0];\n    }\n}\n```\n\n\nAnd this kernel is called like this from main:\n\n```\nthreads = THR_PER_BLC /2 ;\n\nint gsize = BLC_PER_GRD /8;\n\nswitch (threads) {\n    case 512:\n        reduce7<512> << < gsize, threads >> > (g_idata, g_odata, GRID_SIZE);\n        break;\n    case 256:\n        reduce7<256> << < gsize, threads >> > (g_idata, g_odata, GRID_SIZE);\n        break;\n    case 128:\n        reduce7<128> << < gsize, threads >> > (g_idata, g_odata, GRID_SIZE);\n        break;\n    case 64:\n        reduce7<64> << < gsize, threads  >> > (g_idata, g_odata, GRID_SIZE);\n        break;\n    case 32:\n        reduce7<32> << < gsize, threads  >> > (g_idata, g_odata, GRID_SIZE);\n        break;\n    case 16:\n        reduce7<16> << < gsize, threads >> > (g_idata, g_odata, GRID_SIZE);\n        break;\n    case 8:\n        reduce7<8> << < gsize, threads >> > (g_idata, g_odata, GRID_SIZE);\n        break;\n    case 4:\n        reduce7<4> << < gsize, threads >> > (g_idata, g_odata, GRID_SIZE);\n        break;\n    case 2:\n        reduce7<2> << < gsize, threads >> > (g_idata, g_odata, GRID_SIZE);\n        break;\n    case 1:\n        reduce7<1> << < gsize, threads >> > (g_idata, g_odata, GRID_SIZE);\n        break;\n}\ncudaThreadSynchronize();\n```\n\n\nIs basically means that reduce7 cant be called with to big GRID_SIZE?\n\nThis are my tests\n\n```\n#################################################################\n6 Unroll the complete loop\nKernal elapsed time =      0.030(ms)\nElapsed time =      0.057(ms)\nSum = 8192, with BLC_PER_GRD 16 THR_PER_BLC 512\n#################################################################\n7 Final\nKernal elapsed time =      0.015(ms), band =\nElapsed time =      0.040(ms)\nSum = 8192, with BLC_PER_GRD 16 THR_PER_BLC 512\n#################################################################\n\n#################################################################\n6 Unroll the complete loop\nKernal elapsed time =      0.031(ms)\nElapsed time =      0.057(ms)\nSum = 8192, with BLC_PER_GRD 8 THR_PER_BLC 1024\n#################################################################\n7 Final\nKernal elapsed time =      0.015(ms), band =\nElapsed time =      0.040(ms)\nSum = 8192, with BLC_PER_GRD 8 THR_PER_BLC 1024\n#################################################################\n\n#################################################################\n6 Unroll the complete loop\nKernal elapsed time =      0.569(ms)\nElapsed time =     12.889(ms)\nSum = 8388608, with BLC_PER_GRD 8192 THR_PER_BLC 1024\n#################################################################\n```\n\n\nAnd my gpu:\n\n```\na@M:/usr/local/cuda/samples/bin/x86_64/linux/release$ ./dev*Drv\n./deviceQueryDrv Starting...\n\nCUDA Device Query (Driver API) statically linked version\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \"GeForce GTX 1060 6GB\"\n  CUDA Driver Version:                           9.2\n  CUDA Capability Major/Minor version number:    6.1\n  Total amount of global memory:                 6078 MBytes (6373572608 bytes)\n  (10) Multiprocessors, (128) CUDA Cores/MP:     1280 CUDA Cores\n  GPU Max Clock rate:                            1709 MHz (1.71 GHz)\n  Memory Clock rate:                             4004 Mhz\n  Memory Bus Width:                              192-bit\n  L2 Cache Size:                                 1572864 bytes\n  Max Texture Dimension Sizes                    1D=(131072) 2D=(131072, 65536) 3D=(16384, 16384, 16384)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  2048\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size (x,y,z):    (2147483647, 65535, 65535)\n  Texture alignment:                             512 bytes\n  Maximum memory pitch:                          2147483647 bytes\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n  Run time limit on kernels:                     No\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Concurrent kernel execution:                   Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Disabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device supports Compute Preemption:            Yes\n  Supports Cooperative Kernel Launch:            Yes\n  Supports MultiDevice Co-op Kernel Launch:      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 3 / 0\n  Compute Mode:\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n```\n\n\nHmm so lets sey that we set 128 threads, grid size as 4:\n\n```\n#define MAX_SHM 49152\n#define GRID_SIZE MAX_SHM / sizeof(int)\n\n#define THR_PER_BLC 128\n#define BLC_PER_GRD GRID_SIZE/THR_PER_BLC\n```\n\n\nThen reduce7 works. So it means that reduce7 depends strictly on max shm?\n\nEdit\n\nSeems that I was confused by this line: ```\nwhile (i < n) {```\n, where n is GRID_SIZE. Then for now I dont know what ```\ni```\n means. Need to digest it some time. But its good to know, that in one block there can only be specific number of threads, that for this case we had to match with SM.\n    ", "Answer": "\r\nFirst of all, the shared memory needed for this reduction is only as large as the needs of the block, not the grid.  So asking for shared memory sized to the grid doesn't make sense.\n\nSecond, this is asking for 64Kbytes of statically allocated shared memory per block:\n\n```\n __shared__ volatile  int sdata[GRID_SIZE]; \n```\n\n\nThat can't work, because:\n\n```\nTotal amount of shared memory per block:       49152 bytes\n```\n\n\nAnd, in addition, this is asking for 64Kbytes of dynamically allocated shared memory per block:\n\n```\n case 128:\n    reduce7<128> << < gsize, threads, GRID_SIZE * sizeof(int) >> > (g_idata, g_odata, GRID_SIZE);\n    break;\n```\n\n\nSo that combo (64K+64K) would never work.\n\nYou seem to be confused about how shared memory is used, and how much is needed per block.  The block only needs one quantity (```\nint```\n in this case) per thread.\n\nYou may also be confused about the syntax and usage of statically allocated shared memory vs. dynamically allocated shared memory.  For this type of problem you would normally use one or the other, not both.\n\nI have no idea what this comment means:\n\n```\n //I added GRID_SIZE myself so it can be volatile\n```\n\n\nUsual suggestion: Any time you are having trouble with a CUDA code, you should be doing proper CUDA error checking and run your code with ```\ncuda-memcheck```\n, before asking others for help.  Even if the example code you start with didn't have proper CUDA error checking, you should add it once you start making modifications and running into trouble.\n\n\n  Then reduce7 works. So it means that reduce7 depends strictly on max shm?\n\n\nIt means that reduce7 needs a certain amount of shared memory per block.  That quantity is one ```\nint```\n per thread.  That is all it needs.  If you give it more, that is OK (sort of) as long as you don't exceed the maximum that can be given.  If you exceed the maximum that can be given, the whole kernel launch fails.\n\nIn other words, all you really need is this:\n\n```\n__shared__ volatile  int sdata[THR_PER_BLC]; \n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "A simple reduction program in CUDA\r\n                \r\nIn the below code, I am trying to implement a simple parallel reduction with blocksize and number of threads per block being 1024. However, after implementing partial reduction, I wish to see whether my implementation is going right or not and in that process I make the program print the first element of the host memory (after data has been copied from device memory to host memory).\nMy host memory is initialize with '1' and is copied to device memory for reduction. And the printf statement after the reduction process still gives me '1' at the first element of the array.\n\nIs there a problem in what I am getting to print or is it something logical in the implementation of reduction?\nIn addition printf statements in the kernel do not print anything. Is there something wrong in my syntax or the call to the printf statement?\nMy code is as below:\n\n```\n    ifndef CUDACC\ndefine CUDACC\nendif\ninclude \"cuda_runtime.h\"\ninclude \"device_launch_parameters.h\"\ninclude\ninclude\nifndef THREADSPERBLOCK\ndefine THREADSPERBLOCK 1024\nendif\nifndef NUMBLOCKS\ndefine NUMBLOCKS 1024\nendif\n\nglobal void reduceKernel(int *c)\n{\nextern shared int sh_arr[];\n\nint index = blockDim.x*blockIdx.x + threadIdx.x;\nint sh_index = threadIdx.x;\n\n// Storing data from Global memory to shared Memory\nsh_arr[sh_index] = c[index];\n__syncthreads();\n\nfor(unsigned int i = blockDim.x/2; i>0 ; i>>=1)\n{\n    if(sh_index < i){\n        sh_arr[sh_index] += sh_arr[i+sh_index];\n    }\n    __syncthreads();\n}\n\nif(sh_index ==0)\n    c[blockIdx.x]=sh_arr[sh_index];\nprintf(\"value stored at %d is %d \\n\", blockIdx.x, c[blockIdx.x]);\nreturn;\n\n}\n\nint main()\n{\nint *h_a;\nint *d_a;\nint share_memSize, h_memSize;\nsize_t d_memSize;\n\nshare_memSize = THREADSPERBLOCK*sizeof(int);\nh_memSize = THREADSPERBLOCK*NUMBLOCKS;\n\nh_a = (int*)malloc(sizeof(int)*h_memSize);\n\nd_memSize=THREADSPERBLOCK*NUMBLOCKS;\ncudaMalloc( (void**)&d_a, h_memSize*sizeof(int));\n\nfor(int i=0; i<h_memSize; i++)\n{\n    h_a[i]=1;    \n};\n\n//printf(\"last element of array %d \\n\", h_a[h_memSize-1]);\n\ncudaMemcpy((void**)&d_a, (void**)&h_a, h_memSize, cudaMemcpyHostToDevice);\nreduceKernel<<<NUMBLOCKS, THREADSPERBLOCK, share_memSize>>>(d_a);\ncudaMemcpy((void**)&h_a, (void**)&d_a, d_memSize, cudaMemcpyDeviceToHost);\n\nprintf(\"sizeof host memory %d \\n\", d_memSize); //sizeof(h_a));\nprintf(\"sum after reduction %d \\n\", h_a[0]);\n\n}\n```\n\n    ", "Answer": "\r\nThere are a number of problems with this code.\n\n\nmuch of what you've posted is not valid code.  As just a few examples, your ```\nglobal```\n and ```\nshared```\n keywords are supposed to have double-underscores before and after, like this:  ```\n__global__```\n  and ```\n__shared__```\n.  I assume this is some sort of copy-paste error or formatting error.  There are problems with your define statements as well.  You should endeavor to post code that doesn't have these sorts of problems.\nAny time you are having trouble with a CUDA code, you should use proper cuda error checking and run your code with ```\ncuda-memcheck```\n before asking for help.  If you had done this , it would have focused your attention on item 3 below.\nYour ```\ncudaMemcpy```\n operations are broken in a couple of ways:\n\n```\ncudaMemcpy((void**)&d_a, (void**)&h_a, h_memSize, cudaMemcpyHostToDevice);\n```\n\n\nFirst, unlike ```\ncudaMalloc```\n, but like ```\nmemcpy```\n, ```\ncudaMemcpy```\n just takes ordinary pointer arguments.  Second, the size of the transfer (like ```\nmemcpy```\n) is in bytes, so your sizes need to be scaled up by ```\nsizeof(int)```\n:\n\n```\ncudaMemcpy(d_a, h_a, h_memSize*sizeof(int), cudaMemcpyHostToDevice);\n```\n\n\nand similarly for the one after the kernel.\n```\nprintf```\n from every thread in a large kernel (like this one which has 1048576 threads) is probably not a good idea.  You won't actually get all the output you expect, and on windows (appears you are running on windows) you may run into a WDDM watchdog timeout due to kernel execution taking too long.  If you need to ```\nprintf```\n from a large kernel, be selective and condition your printf on ```\nthreadIdx.x```\n and ```\nblockIdx.x```\n\nThe above things are probably enough to get some sensible printout, and as you point out you're not finished yet anyway: \"I wish to see whether my implementation is going right or not \".  However, this kernel, as crafted, overwrites its input data with output data:\n\n```\n__global__ void reduceKernel(int *c)\n...\n    c[blockIdx.x]=sh_arr[sh_index];\n```\n\n\nThis will lead to a race condition.  Rather than trying to sort this out for you, I'd suggest separating your output data from your input data.  Even better, you should study the cuda reduction sample code which also has an associated presentation.\n\n\nHere is a modified version of your code which has most of the above issues fixed.  It's still not correct.  It still has defect 5 above in it.  Rather than completely rewrite your code to fix defect 5, I would direct you to the cuda sample code mentioned above.\n\n```\n$ cat t820.cu\n#include <stdio.h>\n\n#ifndef THREADSPERBLOCK\n#define THREADSPERBLOCK 1024\n#endif\n#ifndef NUMBLOCKS\n#define NUMBLOCKS 1024\n#endif\n\n__global__ void reduceKernel(int *c)\n{\nextern __shared__ int sh_arr[];\n\nint index = blockDim.x*blockIdx.x + threadIdx.x;\nint sh_index = threadIdx.x;\n\n// Storing data from Global memory to shared Memory\nsh_arr[sh_index] = c[index];\n__syncthreads();\n\nfor(unsigned int i = blockDim.x/2; i>0 ; i>>=1)\n{\n    if(sh_index < i){\n        sh_arr[sh_index] += sh_arr[i+sh_index];\n    }\n    __syncthreads();\n}\n\nif(sh_index ==0)\n    c[blockIdx.x]=sh_arr[sh_index];\n// printf(\"value stored at %d is %d \\n\", blockIdx.x, c[blockIdx.x]);\nreturn;\n\n}\n\nint main()\n{\nint *h_a;\nint *d_a;\nint share_memSize, h_memSize;\nsize_t d_memSize;\n\nshare_memSize = THREADSPERBLOCK*sizeof(int);\nh_memSize = THREADSPERBLOCK*NUMBLOCKS;\n\nh_a = (int*)malloc(sizeof(int)*h_memSize);\n\nd_memSize=THREADSPERBLOCK*NUMBLOCKS;\ncudaMalloc( (void**)&d_a, h_memSize*sizeof(int));\n\nfor(int i=0; i<h_memSize; i++)\n{\n    h_a[i]=1;\n};\n\n//printf(\"last element of array %d \\n\", h_a[h_memSize-1]);\n\ncudaMemcpy(d_a, h_a, h_memSize*sizeof(int), cudaMemcpyHostToDevice);\nreduceKernel<<<NUMBLOCKS, THREADSPERBLOCK, share_memSize>>>(d_a);\ncudaMemcpy(h_a, d_a, d_memSize*sizeof(int), cudaMemcpyDeviceToHost);\n\nprintf(\"sizeof host memory %d \\n\", d_memSize); //sizeof(h_a));\nprintf(\"first block sum after reduction %d \\n\", h_a[0]);\n}\n$ nvcc -o t820 t820.cu\n$ cuda-memcheck ./t820\n========= CUDA-MEMCHECK\nsizeof host memory 1048576\nfirst block sum after reduction 1024\n========= ERROR SUMMARY: 0 errors\n$\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Trigonometric Functions: Argument reduction\r\n                \r\nI'm trying to implement trigonometric functions for interval arithmetic, preferably with ```\n< ulp```\n error which requires precisely compute trigonometric functions manually. The first step is usually argument reduction, but I'm not really sure how argument reduction is usually implemented. For example, with Python, I get the following results:\n```\n>>> import math\n>>> math.sin(1e100)\n-0.3806377310050287\n```\n\nWhat I expected it to do is apply modulo ```\n2 * math.pi```\n, but this gives me a different result:\n```\n>>> math.sin(1e100 % (2 * math.pi))\n0.6806339877607344\n>>> math.sin(math.remainder(1e100, 2 * math.pi))\n0.6806339877607344\n```\n\nHow is argument reduction done for trigonometric functions?\n    ", "Answer": "\r\nIt seems to be the case that most implementations simply use high precision with ```\npi```\n. There exists other strategies, but they all boil down to using lots of precision somehow. Once you have a highly precise value of ```\npi```\n, you can simply take ```\nx % (2 * pi)```\n and work off of that.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Image palette reduction\r\n                \r\nI am playing with computer graphics programming for the first time. I want to convert RGB (24-bit) images to indexed-palette (8-bit) images (like GIF). My initial thought is to use k-means (with k=256).\n\nHow would one go about picking the optimal palette for a given image? This is a learning experience for me, so I would prefer an overview-type answer to source code.  \n\nEdit: Dithering is currently off-topic. I am only referring to \"simple\" color conversion, psycho-visual/perceptual models aside; color-space is also currently off-topic, though moving between color-spaces is what got me thinking about this in the first place :)\n    ", "Answer": "\r\n\nhttp://en.wikipedia.org/wiki/Color_quantization\nOctree\nMedian-cut\nK-means\nGamut subdivision\nhttp://www.cs.berkeley.edu/~dcoetzee/downloads/scolorq/\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda reduction\r\n                \r\nI'm having trouble understanding how to reduce lambda terms to normal form. could someone help me to understand how to reduce this lambda expression? I don't know exactly where to start.\n(λx. ( λa. (λx. x a)) x) 20 (λx. x+3)\nThank you in advance\n    ", "Answer": "\r\nI'll just focus on your expression. For a deeper explanation, you could read the answer to this question.\nFirst of all, let's consider how brackets could be interpreted on these expressions. Consider the following expression:\n```\n(λx. x*x) 5 => 5*5 => 25\n```\n\nSo, evaluation of this expression is done replacing the x by value 5 outside brackets.\nNow, let's put some brackets from left to right on your expression:\n```\n(((λx. ( λa. (λx. x a)) x) 20) (λx. x+3))\n```\n\nLet's call (λa. (λx. x a)) the variable z as a simplification. So, we'll have:\n```\n(((λx. z x) 20) (λx. x+3))\n```\n\nand the expression is now similar to our first example. Replacing x by value 20, we'll have:\n```\n((z 20) (λx. x+3))\n```\n\nwhich is the same as:\n```\n(((λa. (λx. x a)) 20) (λx. x+3))\n```\n\nAgain, we can replace a by 20 resulting\n```\n((λx. x 20) (λx. x+3))\n```\n\nAfterwards, we could replace x at the left expression by the whole expression on the right (λx. x+3).\n```\n((λx. x+3) 20)\n```\n\nFinally, we could replace x by 20:\n```\n((λx. x+3) 20) => 20+3 = 23\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Sparsity reduction\r\n                \r\nI have to factorize a big sparse matrix ( 6.5mln rows representing users* 6.5mln columns representing items) to find users and items latent vectors. I chose the als algorithm in spark framework(pyspark).\nTo boost the quality I have to reduce the sparsity of my matrix till 98%. (current value is 99.99% because I have inly 356mln of filled entries).\nI can do it by dropping rows or columns, but I must find the optimal solution maximizing number of rows(users).\nThe main problem is that I must find some subsets of users and items sets, and dropping some row can drop some columns and vice versa, the second problem is that function that evaluates sparsity is not linear.\nWhich way I can solve this problem? which libraries in python can help me with it? \nThank you.\n    ", "Answer": "\r\nThis is a combinatorial problem. There is no easy way to drop an optimal set of columns to achieve max number of users while reducing sparsity. A formal approach would be to formulate it as a mixed-integer program. Consider the following 0-1 matrix, derived from your original matrix C.\n\n```\nA(i,j) = 1 if C(i,j) is nonzero, \nA(i,j) = 0 if C(i,j) is zero\n```\n\n\nParameters:\n\n```\nM : a sufficiently big numeric value, e.g. number of columns of A (NCOLS)\nN : total number of nonzeros in C (aka in A)\n```\n\n\nDecision vars are\n\n```\nx(j) : 0 or 1  implying whether column j is dropped or not\nnr(i): number of nonzeros covered in row i\ny(i) : 0 or 1 implying whether row i is dropped or not\n```\n\n\nConstraints:\n\n```\nA(i,:) x  = nr(i)         for i = 1..NROWS\nnr(i)    <= y(i) * M      for i = 1..NROWS\n@sum(nr(i)) + e = 0.98 * N  # explicit slack 'e' to be penalized in the objective\ny(i) and x(j) are 0-1 variables (binary variables) for i,j\n```\n\n\nObjective:\n\n```\nmaximize @sum(y(i)) - N.e\n```\n\n\nSuch a model would be extremely difficult to solve as an integer model. However, barrier methods should be able to solve the linear programming relaxations (LP) Possible solvers are Coin/CLP (open-source), Lindo (commercial) etc... It may then be possible to use the LP solution to compute approximate integer solutions by simple rounding. \n\nIn the end, you will definitely require an iterative approach which will require solving MF problem several times each time factoring a different submatrix of C, computed with above approach, until you are satisfied with the solution.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Leftmost outermost Reduction (Haskell)\r\n                \r\ni got the task to use the leftmost outermost reduction on following expression:\n\n```\nf inc expo 9 (f (*2) expo 3 1)\n```\n\n\ninc is defined as:\n\n```\n inc :: Int -> Int\n inc x = x+1\n```\n\n\nexpo is defined as:\n\n```\nexpo :: Int -> Int\nexpo x = expo (x*2)\n```\n\n\nand f as:\n\n```\nf :: (Int->Int) -> (Int-> Int) -> Int -> Int -> Int\nf g h a b = g(a-b)\n```\n\n\nI have absolutely no clue where to start the reduction with more functions. I read the hint that the redex is not contained in any other redexes, but i dont get it ;(.\n\nI would appreciate every tip/help.\n    ", "Answer": "\r\nThe first (leftmost, outermost) reduction for\n\n```\nf inc expo 9 (f (*2) expo 3 1)\n```\n\n\nis to apply the definition of ```\nf```\n, once, where ```\nf g h a b = g(a-b)```\n so we use ```\ng```\n as ```\ninc```\n, ```\na```\n as ```\n9```\n and ```\nb```\n as ```\n(f (*2) expo 3 1)```\n, giving\n\n```\ninc(9 - (f (*2) expo 3 1))\n```\n\n\nNow we're actually done with the question we were asked. That's the leftmost, outermost reduction made. Notice that we didn't need to use any facts about the other functions, which is probably what the hint was getting at.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "open mp reduction confusing\r\n                \r\nSo I was trying to do simple multiplication between two arrays then add up the result of each multiplication and I am really confused by the reduction, here is my code:\n\n```\n#include <omp.h>\n#include <stdio.h>\n#define SizeOfVector 8\n#define NumberOfThreads 4\nint main(){\n    const int X[SizeOfVector] = {0,2,3,4,5,6,7,8};\n    const int Y[SizeOfVector] = {1,2,4,8,16,32,64,128};\n    int Result[SizeOfVector] = {0};\n    int Sum = 0;\n    unsigned short id;\n\n    omp_set_num_threads(NumberOfThreads);\n\n    #pragma omp parallel private(id)\n    {\n        id = omp_get_thread_num();\n\n        #pragma omp for reduction(+:Sum)\n        for(unsigned short i = 0; i < SizeOfVector; i++)\n        {\n            Result[i] = X[i] * Y[i];\n            Sum = Result[i];    //Problem Here\n            printf(\"Partial result by thread[%d]= %d\\n\", id, Result[i]);\n        }\n    }\n    printf(\"Final result= %d\\n\", Sum);\n    return 0;\n}\n```\n\n\nThe thing is, if i change the \"Sum = Result[i]\" to \"Sum += Result[i]\" I get the correct result.\nWhy does this happen?\nIsn't a local variable of Sum made and initialized to each thread, then the reduction adds it all up when all the threads are done?\n\nHere is the result with Sum += Result[i]:\n\n```\nPartial result by thread[2]= 80\nPartial result by thread[2]= 192\nPartial result by thread[0]= 0\nPartial result by thread[0]= 4\nPartial result by thread[1]= 12\nPartial result by thread[1]= 32\nPartial result by thread[3]= 448\nPartial result by thread[3]= 1024\nFinal result= 1792\n```\n\n\nAnd Here is the result with Sum = Result[i]:\n\n```\nPartial result by thread[2]= 80\nPartial result by thread[2]= 192\nPartial result by thread[0]= 0\nPartial result by thread[0]= 4\nPartial result by thread[3]= 448\nPartial result by thread[3]= 1024\nPartial result by thread[1]= 12\nPartial result by thread[1]= 32\nFinal result= 1252\n```\n\n    ", "Answer": "\r\nEach thread is running through two iterations before coming to a final result for ```\nSum```\n. Because you are not adding to Sum each iteration, but rather assigning it, the final result will simply be ```\nResult[i]```\n for whatever ```\ni```\n was the last run on that thread. That is the value that is ultimately summed up with the results of all the other threads. You need ```\nSum += Result[i]```\n so that each thread keeps its own running ```\nSum```\n until they meet back up and add the different ```\nSum```\ns together.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Cuda Reduction in 2d Array\r\n                \r\nI want to calculate the average of the values over the whole image in Cuda. To test how reduction in 2D array work, I write this kernel below. The final output o should be the sum of all the image values. The input g is a 2D array with value 1 in every pixel. But the result of this program is 0 as the sum. A bit weird to me. \n\nI imitate the reduction in 1D array in this tutorial http://developer.download.nvidia.com/compute/cuda/1.1-Beta/x86_website/projects/reduction/doc/reduction.pdf I write this 2D form. I am new to Cuda. And suggestions to potential bugs and improvement are welcomed!\n\nJust add one comment. I know it makes sense just to calculate the average in 1D array. But I want to exploit more and test more complicated reduction behaviours. It might not be right. But just a test. Hope anyone can give me suggestions more about reduction common practices.\n\n```\n#include <iostream>\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n\ncudaEvent_t start, stop;\nfloat elapsedTime;\n\n__global__ void \nreduce(float *g, float *o, const int dimx, const int dimy)\n{\nextern __shared__ float sdata[];\n\nunsigned int tid_x = threadIdx.x;\nunsigned int tid_y = threadIdx.y;\n\nunsigned int i = blockDim.x * blockIdx.x + threadIdx.x;\nunsigned int j = blockDim.y * blockIdx.y + threadIdx.y; \n\nif (i >= dimx || j >= dimy)\n    return;\n\nsdata[tid_x*blockDim.y + tid_y] = g[i*dimy + j];\n\n__syncthreads();\n\nfor(unsigned int s_y = blockDim.y/2; s_y > 0; s_y >>= 1)\n{\n    if (tid_y < s_y)\n    {\n        sdata[tid_x * dimy + tid_y] += sdata[tid_x * dimy + tid_y + s_y];\n    }\n    __syncthreads();\n}\n\nfor(unsigned int s_x = blockDim.x/2; s_x > 0; s_x >>= 1 )\n{\n\n    if(tid_x < s_x)\n    {\n        sdata[tid_x * dimy] += sdata[(tid_x + s_x) * dimy];\n    }\n    __syncthreads();\n}\n\nfloat sum;\n\nif( tid_x == 0 && tid_y == 0)\n{ \n    sum = sdata[0];\n    atomicAdd (o, sum);   // The result should be the sum of all pixel values. But the program produce 0\n}\n\n//if(tid_x==0 && tid__y == 0 ) \n    //o[blockIdx.x] = sdata[0];\n}\n\nint\nmain()\n{   \nint dimx = 320;\nint dimy = 160;\nint num_bytes = dimx*dimy*sizeof(float);\n\nfloat *d_a, *h_a, // device and host pointers\n            *d_o=0, *h_o=0;\n\nh_a = (float*)malloc(num_bytes);\nh_o = (float*)malloc(sizeof(float));\n\nsrand(time(NULL));\n\n\nfor (int i=0; i < dimx; i++)\n{   \n    for (int j=0; j < dimy; j++)\n    {\n        h_a[i*dimy + j] = 1;\n    }\n}\n\ncudaMalloc( (void**)&d_a, num_bytes );\ncudaMalloc( (void**)&d_o, sizeof(int) );\n\ncudaMemcpy( d_a, h_a, num_bytes, cudaMemcpyHostToDevice);\ncudaMemcpy( d_o, h_o, sizeof(int), cudaMemcpyHostToDevice); \n\ndim3 grid, block;\nblock.x = 4;\nblock.y = 4;\ngrid.x = dimx / block.x;\ngrid.y = dimy / block.y;\n\ncudaEventCreate(&start);\ncudaEventRecord(start, 0);\n\nint sizeofSharedMemory = dimx*dimy*sizeof(float);\n\nreduce<<<grid, block, sizeofSharedMemory>>> (d_a, d_o, block.x, block.y);\n\ncudaEventCreate(&stop);\ncudaEventRecord(stop, 0);\ncudaEventSynchronize(stop);\n\ncudaEventElapsedTime(&elapsedTime, start, stop);\nstd::cout << \"This kernel runs: \" << elapsedTime << \"ms\" << std::endl; \n\nstd::cout << block.x << \" \" << block.y << std::endl;\nstd::cout << grid.x << \" \" << grid.y << std::endl;\nstd::cout << dimx <<  \" \" << dimy << \" \" << dimx*dimy << std::endl;\n\ncudaMemcpy( h_a, d_a, num_bytes, cudaMemcpyDeviceToHost );\ncudaMemcpy( h_o, d_o, sizeof(int), cudaMemcpyDeviceToHost );\n\nstd::cout << \"The sum is:\" << *h_o << std::endl;\n\nfree(h_a);\nfree(h_o);\ncudaFree(d_a);\ncudaFree(d_o);\n\n}\n```\n\n    ", "Answer": "\r\nIf you do basic cuda error checking you will discover that your reduce kernel is not even running.  The reason is as follows:\n\n```\nint dimx = 320;\nint dimy = 160;\n...\nint sizeofSharedMemory = dimx*dimy*sizeof(float); // = 204800\n\nreduce<<<grid, block, sizeofSharedMemory>>> (d_a, d_o, block.x, block.y);\n                          ^\n                          |\n                         204800 is illegal here\n```\n\n\nYou cannot request 204800 bytes of shared memory dynamically (or any other way).  The maximum is slightly less than 48K bytes. \n\nIf you had done proper cuda error checking, you would discover your kernel is not running and would have gotten an instructive error message which suggests the launch configuration (the numbers between the <<< ... >>> ) is invalid.  Shared memory is requested on a per-block basis, and it's probably not sensible that you need to request enough shared memory to cover your entire 2D data set, when each block only consists of a 4x4 thread array.  You probably just need enough data for what will be accessed by each 4x4 thread array.\n\nAfter you have properly instrumented your code with cuda error checking, and detected and corrected all the errors, then run your code with ```\ncuda-memcheck```\n.  This will do an additional level of error checking to point out any kernel access errors. You may also use ```\ncuda-memcheck```\n if you are getting an unspecified launch failure, and it may help pinpoint the issue.\n\nAfter you have done these basic trouble shooting steps, then it might make sense to ask others for help.  But use the power of the tools you have been given first.\n\nI also want to point out one other error before you come back and post this code again, asking for help.\n\nThis will not be useful:\n\n```\nstd::cout << \"The sum is:\" << *h_o << std::endl;\n\ncudaMemcpy( h_a, d_a, num_bytes, cudaMemcpyDeviceToHost );\ncudaMemcpy( h_o, d_o, sizeof(int), cudaMemcpyDeviceToHost );\n```\n\n\nYou are printing out the sum before you have copied the sum from the device to the host.\nReverse the order of these steps:\n\n```\ncudaMemcpy( h_a, d_a, num_bytes, cudaMemcpyDeviceToHost );\ncudaMemcpy( h_o, d_o, sizeof(int), cudaMemcpyDeviceToHost );\n\nstd::cout << \"The sum is:\" << *h_o << std::endl;\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "colour space reduction calculation\r\n                \r\nMy question is based on this link.\nhttps://docs.opencv.org/4.6.0/db/da5/tutorial_how_to_scan_images.html\nThe tutorial introduces colour space reduction. But I don't understand its equation. I believe that if you want to do colour reduction you can simply do (i/divideWith) without multiplying divideWith.\n    ", "Answer": "\r\nThe expression is ```\ntable[i] = (uchar)(divideWith * (i/divideWith))```\n\nThe whole point of that is to exploit integer division (innermost parenthesis, ```\ni/divideWith```\n) to cause discretized colors.\nMathematically, this (over-)\"simplifies\" to ```\ntable[i] = i```\n, which means the range is approximately maintained.\nIf you didn't multiply \"back\" after dividing, you'd get smaller values, rather than values stretching mostly the whole 0 .. 255 range, and that would make your picture very much darker. In fact, the integer division is rounding down, so that already makes your picture darker.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP - critical section + reduction\r\n                \r\nI'm currently learning Parallel Programming using C and OpenMP.\nI wanted to write simple code where two shared values are beeing incremented by multiple threads.\nFirstly I used reduction directive and it worked as it was meant to. Then I switched to using the critical directive to initiate critical section - it also worked.\nOut of curiosity I've tried to merge these two solution and check the behaviour. I expected two valid, equal values.code:\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include \"omp.h\"\n\n#define ITER 50000\n\nint main( void )\n{\n    int x, y;\n    #pragma omp parallel reduction(+:x,y)\n    {\n       #pragma omp for\n       for (int i = 0; i < ITER; i++ )  \n       {\n            x++;\n            #pragma omp critical\n            y++;\n       }\n    }\n\n    printf(\"non critical = %d\\ncritical = %d\\n\", x, y);\n    return 0;\n}\n```\n\n\noutput:\n\n\n  non critical = 50000\n  critical = 4246432\n\n\nOf course output is random when it comes to 'critical' (variable y), the other behaves as expected and is always 50000.  \n\nThe behaviour of x is understandable - reduction makes it private in scope of single thread. After the incrementation values from threads are summed up and passed to the non-local x.  \n\nWhat I don't understand is the behaviour of y. It's private just like x but it's also inside the critical section so it 'has more than one reason' to be inaccessible from other threads. Yet what, I think, happens is the race condition. Did the critical somehow made y public (shared)?  \n\nI know that this code makes no sense since it's enough to use only one of reduction / critical. I'd like just to know what's behind such behaviour. \n    ", "Answer": "\r\nYour code simply exhibits undefined behaviour and the presence of ```\ncritical```\n has nothing to do with you getting wrong results.\n\n\n  Did the critical somehow made y public (shared)?\n\n\nNo, it did not. It only slows down the loop by preventing the concurrent execution of the threads.\n\nWhat you are missing is that the result of the reduction operation is combined with the initial value of the reduction variable, i.e. with the value the variable had before the parallel region. In your case, both ```\nx```\n and ```\ny```\n have random initial values and therefore you are getting random results. That the initial value ```\nx```\n happens to be 0 in your case and that's why you are getting the correct result for it is simply UB. Initialising both ```\nx```\n and ```\ny```\n makes your code behave as expected.\n\nThe OpenMP specification states:\n\n\n  The ```\nreduction```\n clause specifies a reduction-identifier and one or more list items. For each list item, a private copy is created in each implicit task or SIMD lane, and is initialized with the initializer value of the reduction-identifier. After the end of the region, the original list item is updated with the values of the private copies using the combiner associated with the reduction-identifier.\n\n\nHere is the execution of your original code with 4 threads:\n\n\n\n```\n$ icc -O3 -openmp -std=c99 -o cnc cnc.c\n$ OMP_NUM_THREADS=1 ./cnc\nnon critical = 82765\ncritical = 50000\n$ OMP_NUM_THREADS=4 ./cnc\nnon critical = 82765\ncritical = 50000\n$ OMP_NUM_THREADS=4 ./cnc\nnon critical = 50000\ncritical = 50000\n$ OMP_NUM_THREADS=4 ./cnc\nnon critical = 82765\ncritical = 50194\n$ OMP_NUM_THREADS=4 ./cnc\nnon critical = 82767\ncritical = 2112072800\n```\n\n\nThe first run with one thread demonstrates that it is not due to a data race.\n\nWith ```\nint x=0, y=0;```\n:\n\n```\n$ icc -O3 -openmp -std=c99 -o cnc cnc.c\n$ OMP_NUM_THREADS=4 ./cnc\nnon critical = 50000\ncritical = 50000\n$ OMP_NUM_THREADS=4 ./cnc\nnon critical = 50000\ncritical = 50000\n$ OMP_NUM_THREADS=4 ./cnc\nnon critical = 50000\ncritical = 50000\n$ OMP_NUM_THREADS=4 ./cnc\nnon critical = 50000\ncritical = 50000\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "omp reduction and lambda functions\r\n                \r\nThe following simple code doesn't give the expected result with gcc 4.7.0. Is this correct or a bug?\n\n```\n  unsigned count_err(std::vector<unsigned> const&num, unsigned mask)\n  {\n    unsigned c=0;\n    // enables to reuse the lambda later (not in this simple example)\n    auto f = [&] (unsigned i) { if(i&mask) ++c; };\n#pragma omp parallel for reduction(+:c)\n    for(unsigned i=0; i<num.size(); ++i)\n      f(num[i]);\n    return c;\n  }\n```\n\n\nthis returns zero: the reduction of ```\nc```\n from the lambda function is not done. Btw, I expected the result to be that returned by the serial function\n\n```\n  unsigned count_ser(std::vector<unsigned> const&num, unsigned mask)\n  {\n    unsigned c=0;\n    auto f = [&] (unsigned i) { if(i&mask) ++c; };\n    std::for_each(num.begin(),num.end(),f);\n    return c;\n  }\n```\n\n\nThe following implementations give the expected result (in both cases, the code definitions doing the increment of the reduction variable is moved into the parallel region)\n\n```\n  unsigned count_ok1(std::vector<unsigned> const&num, unsigned mask)\n  {\n    unsigned c=0;\n    auto f = [&] (unsigned i) -> bool { return i&mask; };\n#pragma omp parallel for reduction(+:c)\n    for(unsigned i=0; i<num.size(); ++i)\n      if(f(num[i])) ++c;\n    return c;\n  }\n\n  unsigned count_ok2(std::vector<unsigned> const&num, unsigned mask)\n  {\n    unsigned c=0;\n#pragma omp parallel reduction(+:c)\n    {\n      auto f = [&] (unsigned i) { if(i&mask) ++c; };\n#pragma omp for\n      for(unsigned i=0; i<num.size(); ++i)\n        f(num[i]);\n    }\n    return c;\n  }\n```\n\n\nIs the fact that ```\ncount_err()```\n gives a different result a compiler bug or correct?\n    ", "Answer": "\r\nI think it's not a compiler bug. Here is my explaination. I think in your first example the lambdas were holding a reference to the global ```\nc```\n variable. The thread local copies of ```\nc```\n were created when we entered the for-loop. So the threads were incrementing the same global variable (without any synchronization). When we exit the loop the thread-local copies of ```\nc```\n (all equal to zero, because the lambdas don't know about them) are summed up to give you 0. The ```\ncount_ok2```\n version works because lambdas are holding references to the local ```\nc```\n copies.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "TDD as a defect-reduction strategy\r\n                \r\nCan TDD be successful as a defect-reduction strategy without incorporating guidance on test case construction and evaluation?\n    ", "Answer": "\r\nIMO, my answer would be no.  For TDD to be effective, there has to be guidelines around what is a test and what it means to have something be reasonably tested.  Without a guideline, there may be some developers that end up with tons of defects because their initial tests cover a very small set of inputs,e.g. only the valid ones, which can cause the idea of using TDD to become worthless.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Combinator Reduction Wolfram Mathematica\r\n                \r\nHow can I implement the normal reduction strategy for Combinators {S,K,I} in Mathematica?\nHere are the rules: \nS[x][y][z]->x[y][z[y]]\nK[x][y][z]->x\n\nAlso we have an Y combinator ( fixed point)  thus Y[a]=a[Ya]].\n\nAnd we must \"evaluate\" the expression like ,for instance, S[K][K][a] = K[a][K[a]]=a\n\nIt is highly important to have a ONE step of reduction. Thus, that the result will be in application one step many times..\n\nThanks in anticipation for any suggestions!!!\n    ", "Answer": "\r\nLet's define a function called ```\neval```\n that will perform one step of combinator evaluation. First, we need to consider what exactly constitutes a single step. Arbitrarily, we will evaluate the left-outermost expression first and work inwards from there.\n\n```\nClearAll[eval]\neval[e_] := Module[{r = eval1[e]}, r /; r =!= e]\neval[e:f_[g_]] := Module[{r = eval[f][g]}, r /; r =!= e]\neval[e:f_[g_]] := Module[{r = f[eval[g]]}, r /; r =!= e]\neval[e_] := e\n```\n\n\nNote that the rules are only applied if they actually change the expression. The awkwardness of these definitions is due to Mathematica's lack of a pattern expression that will match arbitrarily long curried argument lists.\n\nNow we can define the recognized combinators, S, K and I:\n\n```\nClearAll[eval1]\neval1[s[f_][g_][h_]] := f[h][g[h]]\neval1[k[f_][_]] := f\neval1[i[f_]] := f\n```\n\n\nThe symbols are in lower case here mainly to avoid the way that Mathematica renders the built-in symbol ```\nI```\n, the imaginary unit.\n\nAny other combinators will be taken as variables and left unevaluated:\n\n```\neval1[e_] := e\n```\n\n\nNow we can try out the basic cases:\n\n```\ni[f] // eval\n\n(* f *)\n\nk[f][g] // eval\n\n(* f *)\n\ns[f][g][h] // eval\n\n(* f[h][g[h]] *)\n```\n\n\nFor more interesting cases, let's define ```\nevalAll```\n that shows all steps in an evaluation chain:\n\n```\nClearAll[evalAll]\nevalAll[e_, n_:Infinity] := FixedPointList[eval, e, n+1] // Most // Column\n\ns[k][k][a] // evalAll \n\n(*\ns[k][k][a]\nk[a][k[a]]\na\n*)\n\ns[s[m][n][r]][k[a][b]][c] // evalAll\n\n(*\ns[s[m][n][r]][k[a][b]][c]\ns[m][n][r][c][k[a][b][c]]\nm[r][n[r]][c][k[a][b][c]]\nm[r][n[r]][c][a[c]]\n*)\n\nf[s[i[k]][k][g][i[f]]] // evalAll\n(*\nf[s[i[k]][k][g][i[f]]]\nf[i[k][g][k[g]][i[f]]]\nf[k[g][k[g]][i[f]]]\nf[g[i[f]]]\nf[g[f]]\n*)\n```\n\n\n```\nevalAll```\n takes an optional \"count\" argument to limit the number of steps shown.  This is useful for divergent expressions -- like a direct expression of the Y combinator:\n\n```\neval1[y[f_]] := f[y[f]]\n\ny[f] // evalAll[#, 4]&\n\n(*\ny[f]\nf[y[f]]\nf[f[y[f]]]\nf[f[f[y[f]]]]\nf[f[f[f[y[f]]]]]\n*)\n```\n\n\n... but it is more fun to express such sequences in terms of SKI:\n\n```\nModule[{y = s[k[s[i][i]]][s[s[k[s]][k]][k[s[i][i]]]]}\n, evalAll[y[f], 28]\n]\n\n(*\ns[k[s[i][i]]][s[s[k[s]][k]][k[s[i][i]]]][f]\nk[s[i][i]][f][s[s[k[s]][k]][k[s[i][i]]][f]]\ns[i][i][s[s[k[s]][k]][k[s[i][i]]][f]]\ni[s[s[k[s]][k]][k[s[i][i]]][f]][i[s[s[k[s]][k]][k[s[i][i]]][f]]]\ns[s[k[s]][k]][k[s[i][i]]][f][i[s[s[k[s]][k]][k[s[i][i]]][f]]]\ns[k[s]][k][f][k[s[i][i]][f]][i[s[s[k[s]][k]][k[s[i][i]]][f]]]\nk[s][f][k[f]][k[s[i][i]][f]][i[s[s[k[s]][k]][k[s[i][i]]][f]]]\ns[k[f]][k[s[i][i]][f]][i[s[s[k[s]][k]][k[s[i][i]]][f]]]\nk[f][i[s[s[k[s]][k]][k[s[i][i]]][f]]][k[s[i][i]][f][i[s[s[k[s]][k]][k[s[i][i]]][f]]]]\nf[k[s[i][i]][f][i[s[s[k[s]][k]][k[s[i][i]]][f]]]]              <-- f[...]\nf[s[i][i][i[s[s[k[s]][k]][k[s[i][i]]][f]]]]\nf[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]][i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]\nf[i[s[s[k[s]][k]][k[s[i][i]]][f]][i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]\nf[s[s[k[s]][k]][k[s[i][i]]][f][i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]\nf[s[k[s]][k][f][k[s[i][i]][f]][i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]\nf[k[s][f][k[f]][k[s[i][i]][f]][i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]\nf[s[k[f]][k[s[i][i]][f]][i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]\nf[k[f][i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]][k[s[i][i]][f][i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]]\nf[f[k[s[i][i]][f][i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]]       <-- f[f[...]]\nf[f[s[i][i][i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]]\nf[f[i[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]][i[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]]]\nf[f[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]][i[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]]]\nf[f[i[s[s[k[s]][k]][k[s[i][i]]][f]][i[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]]]\nf[f[s[s[k[s]][k]][k[s[i][i]]][f][i[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]]]\nf[f[s[k[s]][k][f][k[s[i][i]][f]][i[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]]]\nf[f[k[s][f][k[f]][k[s[i][i]][f]][i[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]]]\nf[f[s[k[f]][k[s[i][i]][f]][i[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]]]\nf[f[k[f][i[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]][k[s[i][i]][f][i[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]]]]\nf[f[f[k[s[i][i]][f][i[i[i[s[s[k[s]][k]][k[s[i][i]]][f]]]]]]]] <-- f[f[f[...]]]\n*)\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Explain reduction in throughput at 802.11\r\n                \r\nActual throughput is lower than physical layer transmission rate\nDecrease. \n\nWhat is the relevance of the 11a MAC to this speed reduction?\n    ", "Answer": "\r\nAre you comparing the Transmission rate of 802.11 and Application throughput (For example, Iperf result)?\n\nIf yes, \nFirst of all, the transmission rate is counting with the MAC layer of the packet including  802.11 headers. The length counting for a single data packet size is already different. \n\nMore important, 802.11 is a CSMA/CA protocol that depends on the airtime for transmitting the packets. Exact management packet (block ACK, RTS/CTS, ... )or time waiting (for example, SIFS / Contention Window, ...) are needed for this mechanism. This used up the airtime so that the actual throughput will be slightly lower than the transmission rate.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Unfolding a definition without reduction\r\n                \r\nThe Coq manual states that the ```\nunfold qualid```\n tactic unfolds each occurrence of ```\nqualid```\n in the goal and replaces it with its beta-iota-normal form.\n\nIs there a simple way to unfold a definition without triggering beta/iota reduction?\n    ", "Answer": "\r\nYou might want to try e.g. the ```\ncbv```\n tactic like so:\n\n```\nDefinition foo := (fun (x : nat) => x) 42.\nDefinition bar := (fun (x : nat) => x) 42.\n\nGoal foo = bar.\n  cbv delta [foo].\n```\n\n\nThis results in the following proof state:\n\n```\n(fun x : nat => x) 42 = bar\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction on array in FORTRAN\r\n                \r\nI'm trying to parallelize a module in my FORTRAN code using OpenMP and I'm running into some issues with threads overwriting updated values in the array. Obviously my first instinct was to do a reduction, but I'm not really sure how to go about it in this context, as I've only done it in a simple x = x + update kind of situation, where-as this is similar, but does so in a normally out-of-order fashion, and also in an array.\n\n```\nsubroutine chargeInterp(q,x,xmin,xmax,dg,np,ng)\nreal(kind = 8) :: charge, dg, xmin, weight, xmax,wp\ninteger :: g1,g2,g1temp,g2temp,i,np,ng\nreal(kind = 8), dimension(np) :: q,x\n\n\n!$OMP PARALLEL DO PRIVATE(g1,g2) REDUCTION(+:q)\ndo i=1,np\n\ng1 = floor((x(i)-xmin)/dg)\ng2 = g1 + 1\nwp=((x(i)-xmin)/dg-g1)\nweight=1-wp\nq(g1+1) = q(g1+1) - weight \nq(g2+1) = q(g2+1) - wp      \nenddo   \n!$OMP END PARALLEL DO\n```\n\n\nJust to give a rundown of what it's doing, essentially it's taking the position of a particle and weighting its charge onto adjacent grid points on the mesh.\n\nThanks for the help!\n\nP.S. The omp statements wrapped around the loop don't work. Just throwing that one out there. Have also tried !$OMP ATOMIC before updating q. Compiles and runs, but my results don't match my un-parallelized results, so it's a no-go.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP Handmade reduction directive\r\n                \r\nI'm working on factorial function. I have to write its parallel version using OpenMP.\n\n```\ndouble sequentialFactorial(const int N) {\n    double result = 1;\n    for(int i = 1; i <= N; i++) {\n        result *= i;\n    }\n    return result;\n}\n```\n\n\nIt is well known that this algorithm can be efficiently parallelized using reduction tecnique. \n\nI'm aware of the existence of ```\nreduction```\n clause (standard §§ 2.15.3.6).\n\n```\ndouble parallelAutomaticFactorial(const int N) {\n    double result = 1;\n    #pragma omp parallel for reduction(*:result)\n    for (int i=1; i <= N; i++) \n        result *= i;\n    return result;\n}\n```\n\n\nHowever, I want to try to implement reduction tecnique \"handmade\".\n\n```\ndouble parallelHandmadeFactorial(const int N) {\n\n    // maximum number of threads\n    const int N_THREADS = omp_get_max_threads();\n\n    // table of partial results\n    double* partial = new double[N_THREADS];\n    for(int i = 0; i < N_THREADS; i++) {\n        partial[i] = 1;\n    }\n\n    // reduction tecnique\n    #pragma omp parallel for\n    for(int i = 1; i <= N; i++) {\n        int thread_index = omp_get_thread_num();\n        partial[thread_index] *= i;\n    }\n\n    // fold results\n    double result = 1;\n    for(int i = 0; i < N_THREADS; i++) {\n        result *= partial[i];\n    }\n\n    delete partial;\n\n    return result;\n}\n```\n\n\nI expect the performance of the last two snippet to be very similar, and better than the first one. However, the average performance is:\n\n```\nSequential Factorial          3500 ms\nParallel Handmade Factorial   6100 ms\nParallel Automatic Factorial   600 ms\n```\n\n\nAm I missing something?\n\n\n\nThanks to @Gilles and @P.W, this code works as expected \n\n```\ndouble parallelNoWaitFactorial(const int N) {\n\n    double result = 1;\n\n    #pragma omp parallel\n    {\n        double my_local_result = 1;\n\n        // removing nowait does not change the performance\n        #pragma omp for nowait\n        for(int i = 1; i <= N; i++)\n            my_local_result *= i;\n\n        #pragma omp atomic\n        result *= my_local_result;\n    }\n\n    return result;\n}\n```\n\n    ", "Answer": "\r\nIf array elements happen to share a cache line, this leads to false sharing which further leads to performance degradation. \n\nTo avoid this:      \n\n\nUse a private variable ```\ndouble partial```\n instead of the ```\ndouble```\n array\n```\npartial```\n.   \nUse the ```\npartial```\n result of each thread to compute the final ```\nresult```\n in a critical region \nThis final ```\nresult```\n should a variable that is not private to the parallel region.\n\n\nThe critical region will look like this: \n\n```\n#pragma omp critical\n    result *= partial;\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Understanding downstream reduction implementation\r\n                \r\nI'm trying to understand the implementation of downstream reduction in JDK. Here is it:\n\n```\n   public static <T, K, D, A, M extends Map<K, D>>\n    Collector<T, ?, M> groupingBy(Function<? super T, ? extends K> classifier,\n                                  Supplier<M> mapFactory,\n                                  Collector<? super T, A, D> downstream) {\n        Supplier<A> downstreamSupplier = downstream.supplier();\n        BiConsumer<A, ? super T> downstreamAccumulator = downstream.accumulator();\n        BiConsumer<Map<K, A>, T> accumulator = (m, t) -> {\n            K key = Objects.requireNonNull(classifier.apply(t), \"element cannot be mapped to a null key\");\n            A container = m.computeIfAbsent(key, k -> downstreamSupplier.get());\n            downstreamAccumulator.accept(container, t);\n        };\n        BinaryOperator<Map<K, A>> merger = Collectors.<K, A, Map<K, A>>mapMerger(downstream.combiner());\n        @SuppressWarnings(\"unchecked\")\n        Supplier<Map<K, A>> mangledFactory = (Supplier<Map<K, A>>) mapFactory;\n\n        if (downstream.characteristics().contains(Collector.Characteristics.IDENTITY_FINISH)) {\n            return new CollectorImpl<>(mangledFactory, accumulator, merger, CH_ID);\n        }\n        else {\n            @SuppressWarnings(\"unchecked\")\n            Function<A, A> downstreamFinisher = \n                       (Function<A, A>) downstream.finisher();  //1, <------------- HERE\n            Function<Map<K, A>, M> finisher = intermediate -> {\n                intermediate.replaceAll((k, v) -> downstreamFinisher.apply(v));\n                @SuppressWarnings(\"unchecked\")\n                M castResult = (M) intermediate;\n                return castResult;\n            };\n            return new CollectorImpl<>(mangledFactory, accumulator, merger, finisher, CH_NOID);\n        }\n    }\n```\n\n\nAt ```\n//1```\n, the ```\ndownstreamFinisher```\n is of type ```\nFunction<A, D>```\n. Judging by the type parameters declarations ```\n<T, K, D, A, M extends Map<K, D>>```\n, the type ```\nD```\n does not depend on ```\nA```\n. So why do we cast it to ```\nFunction<A, A>```\n. I think, the type ```\nD```\n may not even be a subclass of ```\nA```\n.\n\nWhat did I miss?\n    ", "Answer": "\r\nThis collector is actually violating the generic type safety of the ```\nMap```\n, if the downstream collector doesn’t have an identity finisher and the finisher function returns a different type than the intermediate container type.\n\nDuring the collect operation, the map will hold objects of type ```\nA```\n, i.e. the intermediate container type. Then, at the end of the operation, ```\ngroupingBy```\n’s finisher will go through the map and apply the finisher function to each value and replace it with the final result.\n\nOf course, this can’t be implemented without unchecked operations. There are multiple ways to do it, the variant you have posted, changes the type of the map supplier from ```\nSupplier<M>```\n to ```\nSupplier<Map<K, A>>```\n (the first unchecked operation), so the compiler accepts that the map will hold values of type ```\nA```\n rather than ```\nD```\n. That’s why the ```\nfinisher```\n function must be changed to ```\nFunction<A,A>```\n (the second unchecked operation), so it can be used in the map’s ```\nreplaceAll```\n operation which appears to require objects of type ```\nA```\n despite it’s actually ```\nD```\n. Finally, the result map must be cast to ```\nM```\n (the third unchecked operation) to get an object of the expected result type ```\nM```\n, which the supplier actually supplied.\n\nThe correct type safe alternative would be to use different maps and perform the finishing operation by populating the result map with the result of converting the values of the intermediate map. Not only might this be an expensive operation, it would require a second supplier for the intermediate map, as the provided supplier only produces maps suitable for the final result. So apparently, the developers decided this to be an acceptable breach of the type safety.\n\nNote that you can notice the unsafe operation, when you try to use a ```\nMap```\n implementation which enforces type safety:\n\n```\nStream.of(\"foo\", \"bar\").collect(Collectors.groupingBy(String::length,\n    () -> Collections.checkedMap(new HashMap<>(), Integer.class, Long.class),\n    Collectors.counting()));\n```\n\n\nwill produce a ```\nClassCastException```\n with this implementation  because it tries to put an intermediate container (an array) instead of the ```\nLong```\n into the Map.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda Calculus Reduction explanation\r\n                \r\nI'm having some problems with the b-reduction of the next two lambda expressions, somebody can explain me a little bit how to solve them:\n\n```\n(λ f . λ x . f (f x))\n(λ x . x (λ x . λ y . x))\n```\n\n    ", "Answer": "\r\nSo, lambda calculus is a lot like using parameters in programming. The lambda marks a function and the name marks the parameter's name and what is is being passed to it. The rest of it is just the input to the code.\n\nSo, using your first example:\n\n(λ f . λ x . f (f x))\n\n(f x) is the input\n\nλ f is the function, think of f as the parameter name\n\nf is how it is being applied.\n\nFor now, λ x can be ignored.\n\nSo the first reduction would be taking (f x), realizing it is a parameter for f, then passing it to the f function to be applied at the second f. So after the first step, it would look something like \n\n(λ f . λ x . f (f x))\n\n(f x) passed to f\n\n((f x) . λ x . f )\n\n(f x) Being applied to the function\n\nλ x . f x\n\nThere is one more step to do but I'll let you do it.\n\nAlso, when there are no parenthesis, treat the input as two different values. This guy is really helpful on explaining it. https://www.youtube.com/watch?v=S_WzF6BHadc He does have a lecture series up.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Beta reduction of some lambda\r\n                \r\nI have following lambda calculus and want to know to beta reduction of it.\n\nThe lambda is:\n\n```\nλxy.xy\n```\n\n\nI suppose, that it can't do beta reduce because there is no substitution and the x is bound to the body.\n\nIs my assumption right?\n    ", "Answer": "\r\nYou cannot apply beta-reduction (which is probably what you are looking for). Beta-reduction can only be applied on function applications (and even then not in all cases).\n\nYou can apply eta conversion, which transforms λx.fx to f when x does not appear freely in f. Then you can convert your expression:\n\nλxy.xy = λx.λy.xy →η λx.x (= I).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Understanding the reduction process of a foldl\r\n                \r\nSince I am new in \"innermost, outermost\" I have problem with understanding the leftmost outermost style.\nI would like to understand the reduction processes for the list ```\n[5,2,1]```\n\n```\nfoldl :: ( b -> a -> b ) -> b -> [ a ] -> b\nfoldl _ e [] = e\nfoldl f e (x:xs) = foldl f (f e x) xs\n\nfoldl (\\acc x -> acc ++ [negate x]) [] [5,2,1]\n```\n\n    ", "Answer": "\r\nYou can inline the definition to get a better understanding of what is going on.\n```\nfoldl (\\acc x -> acc ++ [negate x]) [] [5,2,1]\n-- using foldl f e (x:xs) = foldl f (f e x) xs\n-- with f = (\\acc x -> acc ++ [negate x])\n--      e = []\n--      x = 5\n--      xs = [2,1]\n-- replace line 1 with foldl f (f e x) xs\nfoldl f (f [] 5) [2,1]\nfoldl f (f (f [] 5) 2) [1]\nfoldl f (f (f (f [] 5) 2) 1) []\n-- using foldl _ e [] = e\nf (f (f [] 5) 2) 1\n-- in infix style (f [] 5 == [] `f` 5)\n(([] `f` 5) `f` 2) `f` 1\n```\n\nIn general\n```\nfoldl (+) 0 [a, b, c] == ((0 + a) + b) + c\n\nfoldr (+) 0 [a, b, c] == a + (b + (c + 0))\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP custom reduction variable\r\n                \r\nI've been assigned to implement the idea of a reduction variable without using the reduction clause. I set up this basic code to test it.\n\n```\nint i = 0;\nint n = 100000000;\ndouble sum = 0.0;\ndouble val = 0.0;\nfor (int i = 0; i < n; ++i)\n{\n    val += 1;\n}\nsum += val;\n```\n\n\nso at the end ```\nsum == n```\n.\n\nEach thread should set val as a private variable, and then the addition to sum should be a critical section where the threads converge, e.g.\n\n```\nint i = 0;\nint n = 100000000;\ndouble sum = 0.0;\ndouble val = 0.0;\n#pragma omp parallel for private(i, val) shared(n) num_threads(nthreads)\nfor (int i = 0; i < n; ++i)\n{\n    val += 1;\n}\n#pragma omp critical\n{\n    sum += val;\n}\n```\n\n\nI can't figure out how to maintain the private instance of val for the critical section. I have tried surrounding the whole thing in a larger pragma, e.g.\n\n```\nint i = 0;\nint n = 100000000;\ndouble sum = 0.0;\ndouble val = 0.0;\n#pragma omp parallel private(val) shared(sum)\n{\n#pragma omp parallel for private(i) shared(n) num_threads(nthreads)\n    for (int i = 0; i < n; ++i)\n    {\n        val += 1;\n    }\n#pragma omp critical\n    {\n        sum += val;\n    }\n}\n```\n\n\nbut I don't get the correct answer. How should I set up the pragmas and clauses to do this?\n    ", "Answer": "\r\nThere are pretty much flaws in your program. Lets look at each program (flaws are written as comments).\n\nProgram one\n\n```\nint i = 0;\nint n = 100000000;\ndouble sum = 0.0;\ndouble val = 0.0;\n#pragma omp parallel for private(i, val) shared(n) num_threads(nthreads)\nfor (int i = 0; i < n; ++i)\n{\n    val += 1;\n}\n// At end of this, all the openmp threads die. \n// The reason is the \"pragma omp parallel\" creates threads, \n// and the scope of those threads were till the end of that for loop. So, the thread dies\n// So, there is only one thread (i.e. the main thread) that will enter the critical section\n#pragma omp critical\n{\n    sum += val;\n}\n```\n\n\nProgram two\n\n```\nint i = 0;\nint n = 100000000;\ndouble sum = 0.0;\ndouble val = 0.0;\n#pragma omp parallel private(val) shared(sum)\n // pragma omp parallel creates the threads\n{\n#pragma omp parallel for private(i) shared(n) num_threads(nthreads)\n  // There is no need to create another set of threads\n  // Note that \"pragma omp parallel\" always creates threads.\n  // Now you have created nested threads which is wrong\n    for (int i = 0; i < n; ++i)\n    {\n        val += 1;\n    }\n#pragma omp critical\n    {\n        sum += val;\n    }\n}\n```\n\n\nThe best solution would be\n\n```\nint n = 100000000;\ndouble sum = 0.0;\nint nThreads = 5;\n#pragma omp parallel shared(sum, n) num_threads(nThreads) // Create omp threads, and always declare the shared and private variables here.\n// Also declare the maximum number of threads.\n// Do note that num_threads(nThreads) doesn't guarantees that the number of omp threads created is nThreads. It just says that maximum number of threads that can be created is nThreads... \n// num_threads actually limits the number of threads that can be created\n{\n    double val = 0.0;  // val can be declared as local variable (for each thread) \n#pragma omp for nowait       // now pragma for  (here you don't need to create threads, that's why no \"omp parallel\" )\n    // nowait specifies that the threads don't need to wait (for other threads to complete) after for loop, the threads can go ahead and execute the critical section \n    for (int i = 0; i < n; ++i)\n    {\n        val += 1;\n    }\n#pragma omp critical\n    {\n        sum += val;\n    }\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Trigonometric argument reduction (reduction modulo 2π)\r\n                \r\nI am trying to create a calculator for sin and cosine that technically only operates on the range of 0-pi/2. Right now this might seem silly but later on it will be used so I can employ Taylor series.\n\nI have a mostly working implementation however I have a serious issue when theta is in the form of x * (pi/2) where x is an arbitrary integer. It would appear that on these values that sometimes they are pushed into a nearby quadrants they dont belong to. There are also some occasional outright errors that I cannot explain.\n\nHow can I shore this up making it more efficient and correct?\n\nHere is the code for doing this.\n\n```\n#define T_PI (2.0 * M_PI)\n#define H_PI (0.5 * M_PI)\nvoid sincos(float theta, float* cosine, float* cosine) {\n  int mode;\n  prepareForRange(&theta, cosine, sine);\n  Assert(!(f < 0.0 || f > H_PI));\n  *cosine = cos(theta);\n  *sine = sin(theta);\n  range_output(mode, cosine, sine);\n}\nvoid prepareForRange(float* theta, int* mode, float *cosine, float* sine) {\n  if (*theta < 0.0) *theta += ceil(-*theta / T_PI) * T_PI;\n  *mode = (int)floor(*theta / H_PI) % 4 + 1;\n  *theta = fmodf(*theta, H_PI);\n}\nvoid range_output(int mode, float *cos, float *sin) {\n  float temp;\n  switch (mode) {\n    case 1:\n      break;\n    case 2:\n      temp = *cos;\n      *cos = -*sin;\n      *sin = temp;\n      break;\n    case 3:\n      *cos = -*cos;\n      *sin = -*sin;\n      break;\n    case 4:\n      temp = *cos;\n      *cos = *sin;\n      *sin = -temp;\n      break;\n    default:\n      break;\n    }\n}\n```\n\n    ", "Answer": "\r\nYou are running into a long-standing and often-unrecognized problem area called range reduction. The basic problem is that a float PI constant is only defined 8 digit accuracy, so by the time you are trying to compute (x - n * PI) for n ~ 10^4, you have lost half the digits of accuracy in your results, and worse as n gets larger. There is no simple software solution to this problem. To really solve it in my own numerical library, I had to effectively implement arbitrary-precision floating point arithmetic and store a 320-digit (1078-bit) PI constant. Some implementations of libc effectively do this for you, but not all, so you can't safely assume it.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction with parallel task\r\n                \r\nI tried to use parallel task to perform reduction, but always get 0. Here is my code:\n\n```\nint sum = 0;\n#pragma omp parallel reduction(+:sum)\n#pragma omp single\nfor(int i=0; i<10; i++)\n{\n    #pragma omp task\n    {\n        printf(\"Thread: %d\\n\", omp_get_thread_num());\n        int y = 5;\n        sum += y;\n    }\n}\nprintf(\"%d\\n\", sum);\n```\n\n\nBut when I use parallel for, the result is right, which is 50. Can anyone tell me how to modify the parallel task code?\nThis is my parallel for code, which works well:\n\n```\nint sum = 0;\n#pragma omp parallel for reduction(+:sum)\nfor(int i=0; i<10; i++)\n{\n    printf(\"Thread: %d\\n\", omp_get_thread_num());\n    int y = 5;\n    sum += y;\n}\nprintf(\"%d\\n\", sum);\n```\n\n    ", "Answer": "\r\nAccording to the OpenMP standard 4.5, you cannot reduce a variable that is used in the task constructor.\n\n\n  A list item that appears in a reduction clause of the innermost\n  enclosing worksharing or parallel construct may not be accessed in an\n  explicit task.\n\n\nNevertheless, it looks like that feature will be covered by OpenMP 5.0. Looking at your code the better approach is actually to use the parallel for with the reduction clause, anyway.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "About Codd's Reduction Algorithm\r\n                \r\nCodd's Algorithm converts an expression in tuple relational calculus to Relational Algebra. I'd like to know:\n\n\nwhether there's a standard implementation of the algorithm? \nwhether this algorithm used anywhere ? (For, the industry needs only SQL & variants, I'm not sure about database theorists in academia) \nWhat's the complexity of the reduction?\n\n    ", "Answer": "\r\nImplementing Codd's algorithm should be easy enough (*), but:\n\nconverting an expression in one language to an expression in another language requires one to know what that \"other language\" is.  Can you tell ?\n\nUsing the output of such a conversion is only sensible if the output of said conversion can be included in a source code, or in some other way passed onto a compiler that understands that particular language.  Do you know of a \"standard\" algebra-based relational data language ?\n\nImo, these are the two most obvious reasons why there is hardly a point in an industrial implementation.\n\n(*) if your input is a parse tree that consists of nodes such as <universalquantification>, <existentialquantification>, <restriction>, <cartesian>, ...\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction on container elements\r\n                \r\nI have a nested loop, with few outer, and many inner iterations. In the inner loop, I need to calculate a sum, so I want to use an OpenMP reduction. The outer loop is on a container, so the reduction is supposed to happen on an element of that container.\nHere's a minimal contrived example:\n\n```\n#include <omp.h>\n#include <vector>\n#include <iostream>\n\nint main(){\n    constexpr int n { 128 };\n\n    std::vector<int> vec (4, 0);\n    for (unsigned int i {0}; i<vec.size(); ++i){\n\n        /* this does not work */\n        //#pragma omp parallel for reduction (+:vec[i])\n        //for (int j=0; j<n; ++j)\n        //  vec[i] +=j;\n\n        /* this works */\n        int* val { &vec[0] };\n        #pragma omp parallel for reduction (+:val[i])\n        for (int j=0; j<n; ++j)\n            val[i] +=j;\n\n        /* this is allowed, but looks very wrong. Produces wrong results\n         * for std::vector, but on an Eigen type, it worked. */\n        #pragma omp parallel for reduction (+:val[i])\n        for (int j=0; j<n; ++j)\n            vec[i] +=j;\n    }\n    for (unsigned int i=0; i<vec.size(); ++i) std::cout << vec[i] << \" \";\n    std::cout << \"\\n\";\n\n    return 0;\n}\n```\n\n\nThe problem is, that if I write the reduction clause as ```\n(+:vec[i])```\n, I get the error ```\n‘vec’ does not have pointer or array type```\n, which is descriptive enough to find a workaround. However, that means I have to introduce a new variable and somewhat change the code logic, and I find it less obvious to see what the code is supposed to do.\n\nMy main question is, whether there is a better/cleaner/more standard way to write a reduction for container elements.\n\nI'd also like to know why and how the third way shown in the code above somewhat works. I'm actually working with the ```\nEigen```\n library, on whose containers that variant seems to work just fine (haven't extensively tested it though), but on ```\nstd::vector```\n, it produces results somewhere between zero and the actual result (8128). I thought it should work, because ```\nvec[i]```\n and ```\nval[i]```\n should both evaluate to dereferencing the same address. But alas, apparently not.\n\nI'm using OpenMP 4.5 and gcc 9.3.0.\n    ", "Answer": "\r\nI'll answer your question in three parts:\n\n1. What is the best way to perform to OpenMP reductions in your example above with a ```\nstd::vec```\n ?\n\ni) Use your approach, i.e. create a pointer ```\nint* val { &vec[0] };```\n\n\nii) Declare a new shared variable like @1201ProgramAlarm answered.\n\niii) declare a user defined reduction (which is not really applicable in your simple case, but see 3. below for a more efficient pattern).\n\n2. Why doesn't the third loop work and why does it work with Eigen ?\n\nLike the previous answer states you are telling OpenMP to perform a reduction sum on a memory address X, but you are performing additions on memory address Y, which means that the reduction declaration is ignored and your addition is subjected to the usual thread race conditions.\n\nYou don't really provide much detail into your Eigen venture, but here are some possible explanations:\n\ni) You're not really using multiple threads (check ```\nn = Eigen::nbThreads( )```\n)\n\nii) You didn't disable Eigen's own parallelism which can disrupt your own usage of OpenMP, e.g. ```\nEIGEN_DONT_PARALLELIZE```\n compiler directive.\n\niii) The race condition is there, but you're not seeing it because Eigen operations take longer, you're using a low number of threads and only writing a low number of values => lower occurrence of threads interfering with each other to produce the wrong result.\n\n3. How should I parallelize this scenario using OpenMP (technically not a question you asked explicitly) ?\n\nInstead of parallelizing only the inner loop, you should parallelize both at the same time. The less serial code you have, the better. In this scenario each thread has its own private copy of the ```\nvec```\n vector, which gets reduced after all the elements have been summed by their respective thread. This solution is optimal for your presented example, but might run into RAM problems if you're using a very large vector and very many threads (or have very limited RAM).\n\n```\n#pragma omp parallel for collapse(2) reduction(vsum : vec)\nfor (unsigned int i {0}; i<vec.size(); ++i){\n    for (int j = 0; j < n; ++j) {\n        vec[i] += j;\n    }\n}\n```\n\n\nwhere vsum is a user defined reduction, i.e.\n\n```\n#pragma omp declare reduction(vsum : std::vector<int> : std::transform(omp_out.begin(), omp_out.end(), omp_in.begin(), omp_out.begin(), std::plus<int>())) initializer(omp_priv = decltype(omp_orig)(omp_orig.size()))\n```\n\n\nDeclare the reduction before the function where you use it, and you'll be good to go\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Is linear-time reduction symmetric?\r\n                \r\nIf a problem X reduces to a problem Y is the opposite reduction also possible? Say \n\nX = Given an array tell if all elements are distinct\n\nY = Sort an array using comparison sort\n\nNow, X reduces to Y in linear time i.e. if I can solve Y, I can solve X in linear time. Is the reverse always true? Can I solve Y, given I can solve X? If so, how? \n\nBy reduction I mean the following:\n\n```\nProblem X linear reduces to problem Y if X can be solved with:\na) Linear number of standard computational steps.\nb) Constant calls to subroutine for Y.\n```\n\n    ", "Answer": "\r\nGiven the example above:  \n\nYou can determine if all elements are distinct in ```\nO(N)```\n if you back them up with a hash table.  Which allows you to check existence in ```\nO(1)```\n + the overhead of the hash function (which generally doesn't matter).  IF you are doing a non-comparison based sort:  \n\nsorting algorithm list\n\nSpecialized sort that is linear:  \n\nFor simplicity, assume you're sorting a list of natural numbers. The sorting method is illustrated using uncooked rods of spaghetti:\n    For each number x in the list, obtain a rod of length x. (One practical way of   choosing the unit is to let the largest number m in your list correspond to one full rod of spaghetti. In this case, the full rod equals m spaghetti units. To get a rod of length x, simply break a rod in two so that one piece is of length x units; discard the other piece.)      \n\nOnce you have all your spaghetti rods, take them loosely in your fist and lower them to the table, so that they all stand upright, resting on the table surface. Now, for each rod, lower your other hand from above until it meets with a rod--this one is clearly the longest! Remove this rod and insert it into the front of the (initially empty) output list (or equivalently, place it in the last unused slot of the output array). Repeat until all rods have been removed.  \n\nSo given a very specialized case of your problem, your statement would hold.  This will not hold in the general case though, which seems to be more what you are after.  It is very similar to when people think they have solved TSP, but have instead created a constrained version of the general problem that is solvable using a special algorithm.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP min reduction and std::min\r\n                \r\nI am testing OpenMP min reduction. If I write my code like the following, it will return the correct result: res = 3.\n\n```\n#include <omp.h>\n#include <iostream>\n#include <algorithm>\n\nint main(){\n\n    omp_set_num_threads(5);\n\n    float res=10;\n\n#pragma omp parallel for default(shared) reduction(min:res)\n    for(int i1 = 0; i1 <= 10; i1++)\n        for(int i0 = 0; i0 <= 10; i0++)\n            if(res > 3.0+i1+20*i0)\n                res = 3.0+i1+20*i0;\n\n    std::cout << \"res = \" << res << std::endl;\n\n    return 0;\n}\n```\n\n\nBut If I write in an alternative way by replacing \"if\" statement with \"std::min\", then the result is wrong: res = 10.\n\n```\n#include <omp.h>\n#include <iostream>\n#include <algorithm>\n\nint main(){\n\n    omp_set_num_threads(5);\n\n    float res=10;\n\n#pragma omp parallel for default(shared) reduction(min:res)\n    for(int i1 = 0; i1 <= 10; i1++)\n        for(int i0 = 0; i0 <= 10; i0++)\n            res = std::min(res,static_cast<float>(3.0+i1+20*i0));\n\n    std::cout << \"res = \" << res << std::endl;\n\n    return 0;\n}\n```\n\n\nIs OpenMP min reduction interfering with std::min?\n    ", "Answer": "\r\nFirst of all, your code is conforming: it shouldn't matter which kind of code you have inside the parallel for.\n\nWhat the ```\nreduction```\n clause implies is that each thread will have its own private copy initialized to the neutral element of the ```\nmin```\n operator (i.e. the largest representable number in the reduction item type) and they will work with it until the end of the construct. At that point, these private copies will be reduced to the original list item using the reduction-identifier, which in your case is the ```\nmin```\n operator. So there is no race-condition here.\n\nI have executed your code using the same version as you did and it worked fine: ```\nicpc (ICC) 16.0.0```\n and OpenMP version ```\n201307```\n. Could this issue be related to the C++ standard headers that you are using?\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction with Eigen::VectorXd\r\n                \r\nI am attempting to parallelize the below loop with an OpenMP reduction;\n\n```\n#define EIGEN_DONT_PARALLELIZE\n#include <iostream>\n#include <cmath>\n#include <string>\n#include <eigen3/Eigen/Dense>\n#include <eigen3/Eigen/Eigenvalues>\n#include <omp.h>\n\nusing namespace Eigen;\nusing namespace std;\n\nVectorXd integrand(double E)\n{\n    VectorXd answer(500000);\n    double f = 5.*E + 32.*E*E*E*E;\n    for (int j = 0; j !=50; j++)\n        answer[j] =j*f;\n    return answer;\n}\n\nint main()\n{\n    omp_set_num_threads(4);\n    double start = 0.;\n    double end = 1.;\n    int n = 100;\n    double h = (end - start)/(2.*n);\n\n    VectorXd result(500000);\n    result.fill(0.);\n    double E = start;\n    result = integrand(E);\n    #pragma omp parallel\n    { \n    #pragma omp for nowait\n    for (int j = 1; j <= n; j++){\n        E = start + (2*j - 1.)*h;\n        result = result + 4.*integrand(E);\n        if (j != n){\n            E = start + 2*j*h;\n            result = result + 2.*integrand(E);\n        }\n    }\n    }\n    for (int i=0; i <50 ; ++i)\n        cout<< i+1 << \" , \"<< result[i] << endl;\n\n    return 0;\n}\n```\n\n\nThis is definitely faster in parallel than without, but with all 4 threads, the results are hugely variable. When the number of threads is set to 1, the output is correct.\nI would be most grateful if someone could assist me with this...\n\nI am using the clang compiler with compile flags;\n\n```\nclang++-3.8 energy_integration.cpp -fopenmp=libiomp5\n```\n\n\nIf this is a bust, then I'll have to learn to implement ```\nBoost::thread```\n, or ```\nstd::thread```\n...\n    ", "Answer": "\r\nYour code does not define a custom reduction for OpenMP to reduce the Eigen objects. I'm not sure if clang supports user defined reductions (see OpenMP 4 spec, page 180). If so, you can declare a reduction and add ```\nreduction(+:result)```\n to the ```\n#pragma omp for```\n line. If not, you can do it yourself by changing your code as follows:\n\n```\nVectorXd result(500000); // This is the final result, not used by the threads\nresult.fill(0.);\ndouble E = start;\nresult = integrand(E);\n#pragma omp parallel\n{\n    // This is a private copy per thread. This resolves race conditions between threads\n    VectorXd resultPrivate(500000);\n    resultPrivate.fill(0.);\n#pragma omp for nowait// reduction(+:result) // Assuming user-defined reductions aren't allowed\n    for (int j = 1; j <= n; j++) {\n        E = start + (2 * j - 1.)*h;\n        resultPrivate = resultPrivate + 4.*integrand(E);\n        if (j != n) {\n            E = start + 2 * j*h;\n            resultPrivate = resultPrivate + 2.*integrand(E);\n        }\n    }\n#pragma omp critical\n    {\n        // Here we sum the results of each thread one at a time\n        result += resultPrivate;\n    }\n}\n```\n\n\nThe error you're getting (in your comment) seems to be due to a size mismatch. While there isn't a trivial one in your code itself, don't forget that when OpenMP starts each thread, it has to initialize a private ```\nVectorXd```\n per thread. If none is supplied, the default would be ```\nVectorXd()```\n (with a size of zero). When this object is the used, the size mismatch occurs. A \"correct\" usage of ```\nomp declare reduction```\n would include the initializer part:\n\n```\n#pragma omp declare reduction (+: VectorXd: omp_out=omp_out+omp_in)\\\n     initializer(omp_priv=VectorXd::Zero(omp_orig.size()))\n```\n\n\n```\nomp_priv```\n is the name of the private variable. It gets initialized by ```\nVectorXd::Zero(...)```\n. The size is specified using ```\nomp_orig```\n. The standard \n(page 182, lines 25-27) defines this as:\n\n\n  The special identifier omp_orig can also appear in the initializer-clause and it will refer to the storage of the original variable to be reduced.\n\n\nIn our case (see full example below), this is ```\nresult```\n. So ```\nresult.size()```\n is 500000 and the private variable is initialized to the correct size.\n\n```\n#include <iostream>\n#include <string>\n#include <Eigen/Core>\n#include <omp.h>\n\nusing namespace Eigen;\nusing namespace std;\n\nVectorXd integrand(double E)\n{\n    VectorXd answer(500000);\n    double f = 5.*E + 32.*E*E*E*E;\n    for (int j = 0; j != 50; j++)   answer[j] = j*f;\n    return answer;\n}\n\n#pragma omp declare reduction (+: Eigen::VectorXd: omp_out=omp_out+omp_in)\\\n     initializer(omp_priv=VectorXd::Zero(omp_orig.size()))\n\nint main()\n{\n    omp_set_num_threads(4);\n    double start = 0.;\n    double end = 1.;\n    int n = 100;\n    double h = (end - start) / (2.*n);\n\n    VectorXd result(500000);\n    result.fill(0.);\n    double E = start;\n    result = integrand(E);\n\n#pragma omp parallel for reduction(+:result)\n    for (int j = 1; j <= n; j++) {\n        E = start + (2 * j - 1.)*h;\n        result += (4.*integrand(E)).eval();\n        if (j != n) {\n            E = start + 2 * j*h;\n            result += (2.*integrand(E)).eval();\n        }\n    }\n    for (int i = 0; i < 50; ++i)\n        cout << i + 1 << \" , \" << result[i] << endl;\n\n    return 0;\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "how to parallelize this for-loop using reduction?\r\n                \r\nI am trying to make this for-loop parallelized by using Openmp, i recognized that there reduction in this loop so i added \"#pragma omp parallel for reduction(+,ftab)\",but it did not work and it gave me this error :\nerror: user defined reduction not found for ‘ftab’.\n\n```\n   #pragma omp parallel for reduction(+:ftab)\n    for (i = 1; i <= 65536; i++) ftab[i] += ftab[i-1];\n```\n\n    ", "Answer": "\r\nThe operation you want to do is prefix sum. It can be done in parallel. A simple way is to use thrust::inclusive_scan with OpenMP or TBB backend.\n\n```\nthrust::inclusive_scan(thrust::omp::par, ftab, ftab + 65536, fab);\n```\n\n\nor\n\n```\nthrust::inclusive_scan(thrust::tbb::par, ftab, ftab + 65536, fab);\n```\n\n\nYou could also implement it by yourself as referenced in the Wikipedia page.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Beta Reduction - Lambda Calculus\r\n                \r\nWe are currently studying Lambda Calculus and have begun Beta Reduction.\nOur lecturer is using some notation that was not properly explained to us.\nBelow is what we have been given.\n\n𝛽-reduction:\n\n```\n(λ v . e₁) e₂\n    ⤳𝛽\n        [e₂/v]e₁     where [e₂/v] is an operator that replaces each v with an e₂\n```\n\n\nDefinition of operator  [e/v]\n\n```\n[e/v]v         =   e\n[e/v₂]v₁       =   v₁    (when v₁≠v₂, i.e., different identifiers)\n[e/v](e₁ e₂)   =   [e/v]e₁ [e/v]e₂\n[e₂/v](λ v . e₁) =   (λ v . e₁)\n[e₂/v₂](λ v₁ . e₁) = (λ v₁ . [e₂/v₂]e₁)    (when v₁∉FV(e₂))\n\n[x/v] (λ x . f v)\n  ⤳ λ x . f x   (WRONG, x∈FV(x)={x})\n```\n\n\nI have tried to find resources online but none seem to use the same notation as him with the '/' sign.\nAny explanation of the above code would be very much appreciated. \n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to make parallel this for in OpenMP\r\n                \r\nI know the basics of OpenMP and I know that in order to parallelize a for its iterations must not depend on previous iterations. Also one can use reductions, but they support only basic operators such as +, -,/, *, &&, ||.\n\nHow I can make this for parallel?\n\n```\nfor (i = 1; i < n; ++i) {\n    for (j = 1; j < n; ++j) {\n        // stanga\n        if (res[i][j - 1] != res[i][j]) {\n            cmin2[i][j][0] = min(cmin2_res[i][j - 1][0] + 1, cmin[i][j][0]);\n            cmin2_res[i][j][0] = min(cmin2[i][j - 1][0] + 1, cmin_res[i][j][0]);\n         } else {\n            cmin2[i][j][0] = min(cmin2[i][j - 1][0] + 1, cmin[i][j][0]);\n            cmin2_res[i][j][0] = min(cmin2_res[i][j - 1][0] + 1, cmin_res[i][j][0]);\n         }\n         // sus\n         if (res[i - 1][j] != res[i][j]) {\n             cmin2[i][j][0] = min3(cmin2[i][j][0], cmin2_res[i - 1][j][0] + 1, cmin[i][j][1]);\n             cmin2_res[i][j][0] = min3(cmin2_res[i][j][0], cmin2[i - 1][j][0] + 1, cmin_res[i][j][1]);\n         } else {\n             cmin2[i][j][0] = min3(cmin2[i][j][0], cmin2[i - 1][j][0] + 1, cmin[i][j][1]);\n             cmin2_res[i][j][0] = min3(cmin2_res[i][j][0], cmin2_res[i - 1][j][0] + 1, cmin_res[i][j][1]);\n         }\n     }\n }\n```\n\n\nMy question is rather how I can decompose this for to be able to run it in parallel (and maybe use reductions if possible).\n\nThe problem is that at each iteration the operations must be done in this order, because I have 3 more groups of for like this. \n\nP.S. min and min3 are macros.\n    ", "Answer": "\r\nThere's a brute force way to do what you want, but a better parallelization will require a little more input about what you want in and out of the routines.\n\nThe data dependencies in your loop look like this, in i-j space:\n\n```\n     i →\n  ..........\nj .....1....\n↓ ....12....\n  ...123....\n```\n\n\nwhere the value at point three depends on that those point 2s, and those depend on those at pt 1, etc.  Because of this diagonal structure, you can re-order the loops to traverse the grid diagonally, eg first iteration is over (0,1), (1,0) then over (0,2),(1,1),(2,0), and so on.   A simplified version of your problem looks like below:\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <sys/time.h>\n\nint **int2darray(int n, int m);\nvoid free2darray(int **array);\nvoid init2darray(int **array, int n, int m);\nvoid tick(struct timeval *timer);\ndouble tock(struct timeval *timer);\n\nint main(int argc, char **argv) {\n\n    const int N=10000;\n    int **serialarr, **omparr;\n\n    struct timeval serialtimer, omptimer;\n    double serialtime, omptime;\n\n    serialarr = int2darray(N,N);\n    omparr    = int2darray(N,N);\n\n    init2darray(serialarr, N, N);\n    init2darray(omparr,    N, N);\n\n    /* serial calculation */\n    tick(&serialtimer);\n    for (int i=1; i<N; i++)\n        for (int j=1; j<N; j++)\n            serialarr[i][j] = serialarr[i-1][j] + serialarr[i][j-1];\n    serialtime = tock(&serialtimer);\n\n    /* omp */\n    tick(&omptimer);\n    #pragma omp parallel shared(omparr) default(none)\n    {\n        for (int ipj=1; ipj<=N; ipj++) {\n            #pragma omp for\n            for (int j=1; j<ipj; j++) {\n                int i = ipj - j;\n                omparr[i][j] = omparr[i-1][j] + omparr[i][j-1];\n            }\n        }\n        for (int ipj=N+1; ipj<2*N-1; ipj++) {\n            #pragma omp for\n            for (int j=ipj-N+1; j<N; j++) {\n                int i = ipj - j;\n                omparr[i][j] = omparr[i-1][j] + omparr[i][j-1];\n            }\n        }\n    }\n    omptime = tock(&omptimer);\n\n    /* compare results */\n    int abserr = 0;\n    for (int i=0; i<N; i++)\n        for (int j=0; j<N; j++)\n            abserr += abs(omparr[i][j] - serialarr[i][j]);\n\n    printf(\"Difference between serial and OMP array: %d\\n\", abserr);\n    printf(\"Serial time = %lf\\n\", serialtime);\n    printf(\"OMP time    = %lf\\n\", omptime);\n\n    free2darray(omparr);\n    free2darray(serialarr);\n    return 0;\n}\n\n\nint **int2darray(int n, int m) {\n    int *data = malloc(n*m*sizeof(int));\n    int **array = malloc(n*sizeof(int*));\n    for (int i=0; i<n; i++)\n        array[i] = &(data[i*m]);\n\n    return array;\n}\n\nvoid free2darray(int **array) {\n    free(array[0]);\n    free(array);\n}\n\nvoid init2darray(int **array, int n, int m) {\n    for (int i=0; i<n; i++)\n        for (int j=0; j<m; j++)\n            array[i][j] = i*m+j;\n}\n\nvoid tick(struct timeval *timer) {\n    gettimeofday(timer, NULL);\n}\n\ndouble tock(struct timeval *timer) {\n    struct timeval now;\n    gettimeofday(&now, NULL);\n    return (now.tv_usec-timer->tv_usec)/1.0e6 + (now.tv_sec - timer->tv_sec);\n}\n```\n\n\nRunning gives:\n\n```\n$ gcc -fopenmp -Wall -O2 loops.c -o loops -std=c99 \n$ export OMP_NUM_THREADS=8\n$ ./loops\nDifference between serial and OMP array: 0\nSerial time = 0.246649\nOMP time    = 0.174936\n```\n\n\nYou'll notice the speedup is pretty poor, even with large N, because the amount of computation per iteration is small, it's the inner loop that's parallelized, and we're going through memory in a weird, cache-unfriendly order.   \n\nSome of the above could probably be fixed, but it would help a bit more to know more about what you're trying to do; eg, do you care about the cmin2_res arrays, or are they just intermediate products?  In words, what are you trying to calculate?\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Verilog -- \"and reduction\" & duty cycle\r\n                \r\nI am studying this Verilog file:\n```\n`default_nettype none\n\nmodule stroboscope(i_clk, o_led);\n    \n    input wire i_clk;\n    output wire o_led;\n\n    reg [19:0] counter;\n    initial counter = 0;\n\n    always @(posedge i_clk)\n        begin\n            counter <= counter + 1'b1;\n        end\n    \n    assign o_led = &counter[19:15];\n\nendmodule\n```\n\nI figured out already that if I change the ```\nassign```\n from it's current state to what is shown, below frequency of blinking LED ```\no_led```\n remains constant while it's duty cycle is increased to 100% (when it is turned on):\n```\nassign o_led = &counter[19:19];\n```\n\n\nIt is possible to decrease the duty cycle now by setitng the same line to:\n```\nassign o_led = &counter[19:17];\n```\n\n\nAgain... by setitng the same line to:\n```\nassign o_led = &counter[19:15];\n```\n\n\nAnd finally setting the same line as shown below will make a duty cycle of 0%:\n```\nassign o_led = &counter[19:0];\n```\n\n\n\nSo how does unary ```\n&```\n operator that represents a \"and reduction\" achieve this in the presented case?\nI know a simple example of how this works i.e.\n```\n&`4b1101 = 0\n```\n\nThe \"and reduction\" looks like this:\n```\n1101\n 101\n  01\n   0\n```\n\nBut, the example is much more complex, and I don't understand.\n    ", "Answer": "\r\n```\n&counter[19:15] \n```\n\nis the same as:\n```\ncounter[19] & counter[18] & counter[17] & counter[16] & counter[15]\n```\n\nThe reduction-AND operator just does a bit-wise AND of each of its bits.  So, your ```\no_led```\n signal is only high when all these 5 bits of ```\ncounter```\n are high, as between the cursors in the following diagram (click on the image to enlarge):\n\nThe pulsewidth of ```\no_led```\n matches that of the LSB in your reduction-AND expression.  In my code above, the LSB is 15.  If you change the LSB to 16, the pulsewidth doubles.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA reduction using registers\r\n                \r\nI need to calculate N signals' mean values using reduction. The input is a 1D array of size MN, where M is the length of each signal.\n\nOriginally I had additional shared memory to first copy the data and do the reduction on each signal. However, the original data is corrupted. \n\nMy program tries to minimize the shared memory. So I was wondering how I can use registers to do a reduction sum on N signals. I have N threads, a shared memory (float) s_m[N*M], 0....M-1 is the first signal, etc. \n\nDo I need N registers (or one) to store do mean value of N different signals? (I know how to do with sequential addition using multi-thread programming and 1 register). The next step I want to do is subtract every value in the input from its correspondent signal's mean.\n    ", "Answer": "\r\nYour problem is very small (N = 32 and M < 128).  However, some guidelines:\n\nAssuming you are reducing across N values for each of N threads.\n\n\nIf N is very large (> 10s of thousands) large, just do the reductions over M sequentially in each thread.\nIf N is < 10s of thousands, consider using one warp or one thread block to perform each of the N reductions.\nIf N is very small but M is very large, consider using multiple thread blocks per each of the N reductions.\nIf N is very small and M is very small (as your numbers are), only consider using the GPU for the reductions if the computations that generate and / or consume the input / output of the reductions are also running on the GPU.\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "What's The Mapping Reduction Function\r\n                \r\nThis is (I think) easy problem from Complexity Theory. \n\n```\n#Consider the language E over the binary alphabet\n#consisting of strings representing even non-negative\n#integers (with leading zeros allowed).  \n#I.e. E = {x | x[-1] == '0'}.\n#\n#Reduce E to the language {'Even'} by implementing\n#the function R\ndef R(x):\n    #Code\n\n\ndef main():\n    #test cases\n    assert(R('10010') in ['Even'])\n    assert(R('110011') not in ['Even'])\n\nif __name__ == '__main__':\n    main()\n```\n\n\nAccording to Mapping Reduction def:\n\"Language A is mapping reducible to language B, written A ≤ mB,\nif there is a computable function f : Σ ∗ −→Σ ∗ , where for every w,\nw ∈ A ⇐⇒ f (w) ∈ B.\nThe function f is called the reduction from A to B.\"\nA computable mapping function would be f(n) = 2n (or x << y in Python), but in that case, those assertions doesn't make sense, because every R(x) should be in {'Even'}...?\n    ", "Answer": "\r\nSo basically you have ```\nE```\n as the binary representations of integers, even numbers only. This is indicated by the last digit (integer 1) being ```\n0```\n. All other digits represent multiples of 2 and therefore do not matter.\n\nThe target \"language\" consists just of the string ```\n\"Even\"```\n. That is, every even number must map to the word ```\n\"Even\"```\n.\n\nSo the assignment is practically: if ```\nx```\n represents an even binary number, return ```\n\"Even\"```\n, otherwise return something else.\n\nThe most straightforward way is to check the definition of an even binary number:\n\n```\ndef R(x):  # shortest/fastest option\n    return 'Even' if x[-1] == 0 else ''\n\ndef Rdocs(x):\n    \"\"\"\n    Map any of {x | x[-1] == '0'} to {'Even'}\n\n    Uses the definition {x | x[-1] == '0'}\n    \"\"\"\n    if x[-1] == '0':  # check that x satisfies definition of even binary\n        return 'Even'\n    return ''\n```\n\n\nOne can also do this with an explicit mapping:\n\n```\ntranslate = {'0': 'Even', '1': ''}\ndef Rmap(x, default=''):\n    \"\"\"\n    Map any of {x | x[-1] == '0'} to {'Even'}\n\n    Uses a literal mapping of x[-1]\n    \"\"\"\n    return translate[x[-1]]\n```\n\n\nAlternatively, you can convert the binary to a number. Python's ```\nint```\n type also takes binary literals:\n\n```\ndef Rbin(x):\n    \"\"\"\n    Map any of {x | x[-1] == '0'} to {'Even'}\n\n    Uses python builtin to evaluate binary\n    \"\"\"\n    return '' if int(x, 2) % 2 else 'Even'\n```\n\n\nI guess technically speaking, ```\nR(x)```\n should be only defined for every ```\nx```\n in ```\n{x | x[-1] == '0'}```\n, unless you assume the empty word is implicitly contained in every language. Your unit tests suggest it must return something, however. You may want to return ```\n'Odd'```\n or ```\nNone```\n instead of the empty word.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda calculus expression reduction\r\n                \r\nI'm trying to learn and understand how lambda calculus reduction works. I've been reducing some expression and I've run into this one:\n\n```\n(λ x . x (λ x . λ y . x))\n```\n\n\nI have reduce it to this:\n\n```\n(λ x . λ y . x)\n```\n\n\nBut, I don't know if it's in normal form or it can be reduced more.\n    ", "Answer": "\r\nThe first expression cannot be reduced further, as the only term being applied to another term is a bound (and thus undefined) variable.  Your reduction is invalid, as ```\nx```\n could be anything, and thus we do not know what it will do to ```\n(λ x . λ y . x)```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction on container elements\r\n                \r\nI have a nested loop, with few outer, and many inner iterations. In the inner loop, I need to calculate a sum, so I want to use an OpenMP reduction. The outer loop is on a container, so the reduction is supposed to happen on an element of that container.\nHere's a minimal contrived example:\n\n```\n#include <omp.h>\n#include <vector>\n#include <iostream>\n\nint main(){\n    constexpr int n { 128 };\n\n    std::vector<int> vec (4, 0);\n    for (unsigned int i {0}; i<vec.size(); ++i){\n\n        /* this does not work */\n        //#pragma omp parallel for reduction (+:vec[i])\n        //for (int j=0; j<n; ++j)\n        //  vec[i] +=j;\n\n        /* this works */\n        int* val { &vec[0] };\n        #pragma omp parallel for reduction (+:val[i])\n        for (int j=0; j<n; ++j)\n            val[i] +=j;\n\n        /* this is allowed, but looks very wrong. Produces wrong results\n         * for std::vector, but on an Eigen type, it worked. */\n        #pragma omp parallel for reduction (+:val[i])\n        for (int j=0; j<n; ++j)\n            vec[i] +=j;\n    }\n    for (unsigned int i=0; i<vec.size(); ++i) std::cout << vec[i] << \" \";\n    std::cout << \"\\n\";\n\n    return 0;\n}\n```\n\n\nThe problem is, that if I write the reduction clause as ```\n(+:vec[i])```\n, I get the error ```\n‘vec’ does not have pointer or array type```\n, which is descriptive enough to find a workaround. However, that means I have to introduce a new variable and somewhat change the code logic, and I find it less obvious to see what the code is supposed to do.\n\nMy main question is, whether there is a better/cleaner/more standard way to write a reduction for container elements.\n\nI'd also like to know why and how the third way shown in the code above somewhat works. I'm actually working with the ```\nEigen```\n library, on whose containers that variant seems to work just fine (haven't extensively tested it though), but on ```\nstd::vector```\n, it produces results somewhere between zero and the actual result (8128). I thought it should work, because ```\nvec[i]```\n and ```\nval[i]```\n should both evaluate to dereferencing the same address. But alas, apparently not.\n\nI'm using OpenMP 4.5 and gcc 9.3.0.\n    ", "Answer": "\r\nI'll answer your question in three parts:\n\n1. What is the best way to perform to OpenMP reductions in your example above with a ```\nstd::vec```\n ?\n\ni) Use your approach, i.e. create a pointer ```\nint* val { &vec[0] };```\n\n\nii) Declare a new shared variable like @1201ProgramAlarm answered.\n\niii) declare a user defined reduction (which is not really applicable in your simple case, but see 3. below for a more efficient pattern).\n\n2. Why doesn't the third loop work and why does it work with Eigen ?\n\nLike the previous answer states you are telling OpenMP to perform a reduction sum on a memory address X, but you are performing additions on memory address Y, which means that the reduction declaration is ignored and your addition is subjected to the usual thread race conditions.\n\nYou don't really provide much detail into your Eigen venture, but here are some possible explanations:\n\ni) You're not really using multiple threads (check ```\nn = Eigen::nbThreads( )```\n)\n\nii) You didn't disable Eigen's own parallelism which can disrupt your own usage of OpenMP, e.g. ```\nEIGEN_DONT_PARALLELIZE```\n compiler directive.\n\niii) The race condition is there, but you're not seeing it because Eigen operations take longer, you're using a low number of threads and only writing a low number of values => lower occurrence of threads interfering with each other to produce the wrong result.\n\n3. How should I parallelize this scenario using OpenMP (technically not a question you asked explicitly) ?\n\nInstead of parallelizing only the inner loop, you should parallelize both at the same time. The less serial code you have, the better. In this scenario each thread has its own private copy of the ```\nvec```\n vector, which gets reduced after all the elements have been summed by their respective thread. This solution is optimal for your presented example, but might run into RAM problems if you're using a very large vector and very many threads (or have very limited RAM).\n\n```\n#pragma omp parallel for collapse(2) reduction(vsum : vec)\nfor (unsigned int i {0}; i<vec.size(); ++i){\n    for (int j = 0; j < n; ++j) {\n        vec[i] += j;\n    }\n}\n```\n\n\nwhere vsum is a user defined reduction, i.e.\n\n```\n#pragma omp declare reduction(vsum : std::vector<int> : std::transform(omp_out.begin(), omp_out.end(), omp_in.begin(), omp_out.begin(), std::plus<int>())) initializer(omp_priv = decltype(omp_orig)(omp_orig.size()))\n```\n\n\nDeclare the reduction before the function where you use it, and you'll be good to go\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Questions on PCA Dimensionality Reduction\r\n                \r\n\n\nIn machine learning, PCA is used to reduce the dimensionality of training data. However, from the above picture, I can't understand where is the reduction? \n\nThe input data x_i has D dimensions:\n\n\nThe output data x still has D dimensions:\n\n    ", "Answer": "\r\nThe crucial element here is misunderstanding what is the output, in this pseudocode the output is y (equation 29), not x (equation 30), consequently you do reduce your data to d dimensions, the final equation shows you that if you would like to move back to original space, you can do it (obviously data will be recovered with some errors, since in meantime we dropped a lot of information when going to d dimensions).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "dimension reduction of matrix TFIDF\r\n                \r\nI  calculate the TFIdf(term frequency,inverse document frequency) and i have seen that after this step it is necessary to reduce the dimension of My Matrix with  using  methods like LSI ,chi -square test...,\n\nI haven't any idea how i can implement chi square test in java for dimensionality reduction  of matrix TFIDF,if there is some library to do this or tutorial in which they explain how i can do this, tell me please \n    ", "Answer": "\r\nuse gensims library for LSA, LDA.\nIt can practically perform LSA for any large dataset. It does not load the entire corpus into memory at once but does a lazy read.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Is linear-time reduction symmetric?\r\n                \r\nIf a problem X reduces to a problem Y is the opposite reduction also possible? Say \n\nX = Given an array tell if all elements are distinct\n\nY = Sort an array using comparison sort\n\nNow, X reduces to Y in linear time i.e. if I can solve Y, I can solve X in linear time. Is the reverse always true? Can I solve Y, given I can solve X? If so, how? \n\nBy reduction I mean the following:\n\n```\nProblem X linear reduces to problem Y if X can be solved with:\na) Linear number of standard computational steps.\nb) Constant calls to subroutine for Y.\n```\n\n    ", "Answer": "\r\nGiven the example above:  \n\nYou can determine if all elements are distinct in ```\nO(N)```\n if you back them up with a hash table.  Which allows you to check existence in ```\nO(1)```\n + the overhead of the hash function (which generally doesn't matter).  IF you are doing a non-comparison based sort:  \n\nsorting algorithm list\n\nSpecialized sort that is linear:  \n\nFor simplicity, assume you're sorting a list of natural numbers. The sorting method is illustrated using uncooked rods of spaghetti:\n    For each number x in the list, obtain a rod of length x. (One practical way of   choosing the unit is to let the largest number m in your list correspond to one full rod of spaghetti. In this case, the full rod equals m spaghetti units. To get a rod of length x, simply break a rod in two so that one piece is of length x units; discard the other piece.)      \n\nOnce you have all your spaghetti rods, take them loosely in your fist and lower them to the table, so that they all stand upright, resting on the table surface. Now, for each rod, lower your other hand from above until it meets with a rod--this one is clearly the longest! Remove this rod and insert it into the front of the (initially empty) output list (or equivalently, place it in the last unused slot of the output array). Repeat until all rods have been removed.  \n\nSo given a very specialized case of your problem, your statement would hold.  This will not hold in the general case though, which seems to be more what you are after.  It is very similar to when people think they have solved TSP, but have instead created a constrained version of the general problem that is solvable using a special algorithm.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Categorical variables' dimensionality reduction\r\n                \r\nI have a manufacturing dataset which contains only 3 columns.\n\n```\nColumn 1. WorkStationID\nColumn 2. ProductID\nColumn 3. Error(1 or 0)\n```\n\n\nI'm trying to predict error(1 or 0) as a classification problem. But there are 50 unique workstation and 130 unique productID, so when I transform them to dummy variables, dataframe becomes huge.\n\nSo, my question is, are dimensionality reduction techniques suitable for dummy variables? In reality I have only 2 variable(workstation and product) sounds like no need to do any reduction. Or any feature importance techniques are suitable? What does it mean if it indicates that 5 different workstation is useless?\n\nThanks in advance\n    ", "Answer": "\r\nIf you do not want too many dummy variables, one thing to consider is binary encoding. In many cases when I had such problems, I opted for binary encoding and it worked out fine most of the times and hence is worth a shot for you perhaps.\n\nImagine you have 9 features, and you mark them from 1 to 9 and now binary encode them, you will get:\n\n```\ncat 1 - 0 0 0 1\ncat 2 - 0 0 1 0\ncat 3 - 0 0 1 1\ncat 4 - 0 1 0 0 \ncat 5 - 0 1 0 1\ncat 6 - 0 1 1 0\ncat 7 - 0 1 1 1\ncat 8 - 1 0 0 0\ncat 9 - 1 0 0 1\n```\n\n\nIn your case, if you have 50 workstations, you can reduce from 49 features (one hot) to 6 features (binary encoded, as 2 power 6 is 64).\n\nAfter doing this, you can also try out the feature-selector library from Will Koehrsen. You can plot feature importance graph to see if you can further get rid of features that do not add value to your prediction. May be you can come down from 6 to lesser number of variables. \n\nIt usually gives a beautiful bar chart which helps visualize the importance of different features, and lets us play around further with the features.\n\n\n\nPS: This is an open ended question that you have asked and the answer I have given is based on my experience. There is no particular \"right or wrong\" about it, and you can only try it and know if it works in your favor for your use case.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to Execute PaCMAP Dimension Reduction in R from Reduction Wrappers?\r\n                \r\nI am trying to Implement PaCMAP Dimension Reduction in R, as shown in this documentation https://rdrr.io/github/milescsmith/dim.reduction.wrappers/man/pacmap.html, the ```\npacmap```\n function was a wrapper function of the original python pacmap in https://github.com/YingfanWang/PaCMAP.\nBelow is Some Examples of how it should done in Python\n```\nimport pandas as pd\nimport pacmap\nfraud_data = pd.read_csv(r'C:/Users/User/Desktop/Open Source Dataset/fraud_data.csv')\nembedding = pacmap.PaCMAP(n_dims=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0) \nX_transformed = embedding.fit_transform(fraud_data.values, init=\"pca\")\n```\n\nAnd here is what I try in R version 4.1.0 to no avail:\n```\nlibrary(ReductionWrappers)\nfraud_data <- read.csv(\"fraud_data\")\npacmap_mapping <- pacmap(as.matrix(fraud_data), n_dims=2, n_neighbors=2, MN_ratio=1, FP_ratio=2) \n\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  TypeError: 'float' object cannot be interpreted as an integer \n\npacmap_mapping <- pacmap(fraud_data, n_dims=2, n_neighbors=2, MN_ratio=1, FP_ratio=2) \n\n Error in py_call_impl(callable, dots$args, dots$keywords) : \n  TypeError: '(0, slice(None, None, None))' is an invalid key\n```\n\nI didn't understand how to workaround on this error since there was a minimum examples done, what input do I need to make it work?\nHere is the Data used to do this examples: https://drive.google.com/file/d/1Yt4V1Ir00fm1vQ9futziWbwjUE9VvYK7/view?usp=sharing\n    ", "Answer": "\r\nI decided to make a recreation of PaCMAP Python using Reticulate in R, to make the function runs in a straightforward execution in R:\n```\nlibrary(reticulate)\npython_pandas <- import(\"pandas\")\npython_pacmap <- import(\"pacmap\")\npython_numpy <- import(\"numpy\")\nfraud_pandas <- reticulate::r_to_py(fraud_data)\nnparray <- fraud_pandas$values\nnparray <- nparray$astype(python_numpy$float)\nembedding <- python_pacmap$PaCMAP(n_dims=2L, n_neighbors=NULL, MN_ratio=0.5, FP_ratio=2.0) \nX_transformed <- embedding$fit_transform(nparray, init=\"pca\")\nfraud_transformed <- data.frame(X_transformed)\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Scheme lambda reduction improvement\r\n                \r\nI am rewriting this questions since it was poorly formed .\n\n```\n(define (reduce f)                                                        \n((lambda (value) (if (equal? value f) f (reduce value)))                 \n(let r ((f f) (g ()))                                                  \n(cond ((not (pair? f))                                                \n(if (null? g) f (if (eq? f (car g)) (cadr g) (r f (caddr g))))) \n((and (pair? (car f)) (= 2 (length f)) (eq? 'lambda (caar f)))  \n(r (caddar f) (list (cadar f) (r (cadr f) g) g)))              \n((and (not (null? g)) (= 3 (length f)) (eq? 'lambda (car f)))    \n(cons 'lambda (r (cdr f) (list (cadr f) (gensym (cadr f)) g))))  \n(else (map (lambda (x) (r x g)) f))))))                          \n\n; (reduce '((lambda x x) 3)) ==> 3\n; (reduce '((lambda x (x x)) (lambda x (lambda y (x y)))))\n;   ==> (lambda #[promise 2] (lambda #[promise 3] (#[promise 2] #[promise 3])))\n\n; Comments: f is the form to be evaluated, and g is the local assignment\n; function; g has the structure (variable value g2), where g2 contains\n; the rest of the assignments.  The named let function r executes one\n; pass through a form.  The arguments to r are a form f, and an\n; assignment function g.  Line 2: continue to process the form until\n; there are no more conversions left.  Line 4 (substitution): If f is\n; atomic [or if it is a promise], check to see if matches any variable\n; in g and if so replace it with the new value.  Line 6 (beta\n; reduction): if f has the form ((lambda variable body) argument), it is\n; a lambda form being applied to an argument, so perform lambda\n; conversion.  Remember to evaluate the argument too!  Line 8 (alpha\n; reduction): if f has the form (lambda variable body), replace the\n; variable and its free occurences in the body with a unique object to\n; prevent accidental variable collision.  [In this implementation a\n; unique object is constructed by building a promise.  Note that the\n; identity of the original variable can be recovered if you ever care by\n; forcing the promise.]  Line 10: recurse down the subparts of f.\n```\n\n\nI have the above code which does a lambda reduction on a lambda expression ( which is what i want). My problem here is , can someone help me rewrite this implementation(since i am not that experienced with Scheme) so that i extract from the body the part that does alpha-conversion and put it in a separate function, and the part that does the beta-reduction as well. The function reduce is recursive so , the two new created functions need to be single-step , meaning that they will convert only one bounded variable and reduce only one expression.\n    ", "Answer": "\r\nYour set of requirements makes it fairly clear to me that this is homework; if I'm mistaken, let me know where your constraints come from :).\n\nIt sounds to me like you need to understand the question better before you start developing the solution. The first thing you mention is alpha-conversion, and I think that's a good place to start. Can you write some examples of how you might call an alpha-conversion function, and what it might return?\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Pandas: Subtract-add data to consecutive rows by priority (based on condition) till value is reached\r\n                \r\nI have two dataframes\n\nMy original one \n\n```\ndf1\nOut[10]: \n  id   country   tech  priority  2020  2021  2022\n0  a    Malawi  solar         3   100   100   100\n1  b    Malawi  solar         2   100   100   100\n2  c  Maldives  solar         1   100   100   100\n3  d  Maldives   wind         2    50    50    50\n4  e  Maldives   wind         1    50    50    50\n5  f  Malaysia  solar         2    30    30    30\n6  g  Malaysia  solar         2    30    30    30\n7  h  Malaysia  solar         1    30    30    30\n```\n\n\nThe one with increments and reductions\n\n```\ndf2\nOut[11]: \n    country   tech  2020  2021  2022\n0    Malawi  solar    10    20    30\n1  Maldives   wind   -10   -50   -30\n2   Malysia  solar    10    10    10\n```\n\n\nI want to add/subtract values on the first based on values-conditions of the second\n\nFor increments\n\n\nIncrements always apply to the biggest priority id for each country and technology\nIf we have the same priority, then divide value to be added by two \nIncrements from 2021 onwards apply to already incremented 2020 column (and not to the intial df1 2021)\n\n\nFor reductions\n\n\nReductions apply to the smallest priority id for each country and technology\nIf we have the same priority, then divide value to be added by two\nIf the amount to be reduced is larger than what can be subtracted then roll to the next biggest priority level\nSubtractions from 2021 onward apply to already subtracted 2020 values (and not to initial df1 2021)\n\n\nI want my output to look like this\n\n```\ndf3\nOut[12]: \n  id   country   tech  priority  2020  2021  2022\n0  a    Malawi  solar         3   110   130   160\n1  b    Malawi  solar         2   100   100   100\n2  c  Maldives  solar         1   100   100   100\n3  d  Maldives   wind         2    50    40    10\n4  e  Maldives   wind         1    40     0     0\n5  f  Malaysia  solar         2    35    40    45\n6  g  Malaysia  solar         2    35    40    45\n7  h  Malaysia  solar         1    30    30    30\n```\n\n\nAny help or direction is much appreciated\n\n```\ndf1 = {'id':  ['a', 'b','c','d','e','f','g','h'],\n\n        'country':['Malawi','Malawi','Maldives','Maldives','Maldives','Malaysia','Malaysia','Malaysia'],\n        'tech':['solar','solar','solar','wind','wind','solar','solar','solar'],\n        'priority':[3, 2, 1, 2, 1, 2, 2, 1],\n        '2020' : [100, 100, 100, 50, 50, 30, 30, 30],\n        '2021' : [100, 100, 100, 50, 50, 30, 30, 30],\n        '2022' : [100, 100, 100, 50, 50, 30, 30, 30]}\ndf1 = pd.DataFrame (df1, columns = ['id','country','tech','priority','2020','2021','2022'])\n\ndf2 = {'country' : ['Malawi','Maldives','Malaysia'],\n        'tech':['solar','wind','solar'],\n        '2020':[10,-10,10],\n        '2021':[20,-50,10],\n        '2022':[30,-30,10]}\ndf2 = pd.DataFrame (df2, columns = ['country','tech','2020','2021','2022'])\n```\n\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Autoencoder for dimesionality reduction in DL4J\r\n                \r\nI'm trying to write an autoencoder for dimensionality reduction in DL4J, but all the autoencoder examples I can find for DL4J are for outlier detection.\nhttps://deeplearning4j.konduit.ai/v/en-1.0.0-beta6/getting-started/tutorials/basic-autoencoder\nDoes anybody have an example for dimensionality reduction in DL4J or how I can load just the encoding part of the multi layer network for evaluation after the training on the full encoder/decoder stack has been completed?\nAny help or pointers are greatly appreciated.\nThanks\n    ", "Answer": "\r\nI would use the transfer learning api for that. You can freeze layers as well as remove and add new layers. That should help with freezing or adding/removing what you need.\nIn your case with the feed forward you would need to know the the index of the layer(s) to remove to only have the encoder after you're done training. Some examples can be found here: https://github.com/eclipse/deeplearning4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/advanced/features/transferlearning\nFrom the examples repo there here's a simpler example of setting up a FineTuneConfiguration:\n```\nComputationGraph vgg16 = (ComputationGraph) zooModel.initPretrained();\n        log.info(vgg16.summary());\n\n        //Decide on a fine tune configuration to use.\n        //In cases where there already exists a setting the fine tune setting will\n        //  override the setting for all layers that are not \"frozen\".\n        FineTuneConfiguration fineTuneConf = new FineTuneConfiguration.Builder()\n            .updater(new Nesterovs(5e-5))\n            .seed(seed)\n            .build();\n\n        //Construct a new model with the intended architecture and print summary\n        ComputationGraph vgg16Transfer = new TransferLearning.GraphBuilder(vgg16)\n            .fineTuneConfiguration(fineTuneConf)\n            .setFeatureExtractor(featureExtractionLayer) //the specified layer and below are \"frozen\"\n            .removeVertexKeepConnections(\"predictions\") //replace the functionality of the final vertex\n            .addLayer(\"predictions\",\n                new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n                    .nIn(4096).nOut(numClasses)\n                    .weightInit(new NormalDistribution(0,0.2*(2.0/(4096+numClasses)))) //This weight init dist gave better results than Xavier\n                    .activation(Activation.SOFTMAX).build(),\n                \"fc2\")\n            .build();\n```\n\nThis is changing a computation graph and only changing the last layer. Note most of the examples are focused on simpler use cases like changing the last layer. Feel free to post more questions if you have them. Hopefully this helps as a starting point.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "pyCUDA reduction doesn't work\r\n                \r\nI am using reduction code basically exactly like the examples in the docs. The code below should return ```\n2^3 + 2^3 = 16```\n, but it instead returns 9. What did I do wrong?\n\n```\nimport numpy\nimport pycuda.reduction as reduct\nimport pycuda.gpuarray as gpuarray\nimport pycuda.autoinit\nfrom pycuda.compiler import SourceModule as module\n\nnewzeros = [{1,2,3},{4,5,6}]\ngpuSum = reduct.ReductionKernel(numpy.uint64, neutral=\"0\", reduce_expr=\"a+b\", map_expr=\"1 << x[i]\", arguments=\"int* x\")\nmylengths = pycuda.gpuarray.to_gpu(numpy.array(map(len,newzeros),dtype = \"uint64\",))\nsumfalse = gpuSum(mylengths).get()\nprint sumfalse\n```\n\n    ", "Answer": "\r\nI just figured it out. The argument list used when defining the kernel should be ```\nunsigned long *x```\n, not ```\nint *x```\n. I was using 64-bit integers everywhere else and it messed it up.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "wrong reduction using openmp\r\n                \r\nI am using two different versions of reduction in openmp and I get totally different results. Which one of the following is wrong?\n\n```\n    omp_set_num_threads(t);                                                                                        \n    long long unsigned int d = 0;                                                                                  \n    #pragma omp parallel for default(none) shared(some_stuff) reduction(+:d)               \n    for (int i=start; i< n; i++)                                                                   \n    {                                                                                                              \n            d += calc(i,some_stuff);                                                       \n    }                                                                                                              \n\n    cout << d << endl;\n```\n\n\nand the second version is this:\n\n```\n    omp_set_num_threads(t);\n    //reduction array\n    long long unsigned int* d = new long long unsigned int[t];\n    for(int i = 0; i < t; i++)\n            d[i] = 0;\n\n    #pragma omp parallel for default(none) shared(somestuff, d)\n    for (int i=start; i< n; i++)\n    {                                                                                                              \n            long long unsigned dd = calc(i, somestuff);\n            d[omp_get_thread_num()] += dd;\n    }\n\n    long long unsigned int res = 0;\n    for(int i = 0; i < omp_get_num_threads(); i++){\n            res += d[i];\n    }\n    delete[] d;\n\n    cout << res << endl;\n```\n\n    ", "Answer": "\r\nThe second code is wrong. ```\nomp_get_num_threads()```\n returns ```\n1```\n when called outside a parallel region and therefore your code does not reduce all values into the final result. Since you explicitly fix the number of threads to be ```\nt```\n, you should instead use:\n\n```\nfor(int i = 0; i < t; i++){\n        res += d[i];\n}\n```\n\n\nAlternatively, you could use ```\nomp_get_max_threads()```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "dimension reduction in spam filtering\r\n                \r\nI'm performing an experiment in which I need to compare classification performance of several classification algorithms for spam filtering, viz. Naive Bayes, SVM, J48, k-NN, RandomForests, etc. I'm using the WEKA data mining tool. While going through the literature I came to know about various dimension reduction methods which can be broadly classified into two types-\n\n\nFeature Reduction: Principal Component Analysis, Latent Semantic Analysis, etc.\nFeature Selection: Chi-Square, InfoGain, GainRatio, etc.\n\n\nI have also read this tutorial of WEKA by Jose Maria in his blog: http://jmgomezhidalgo.blogspot.com.es/2013/02/text-mining-in-weka-revisited-selecting.html\n\nIn this blog he writes, \"A typical text classification problem in which dimensionality reduction can be a big mistake is spam filtering\". So, now I'm confused whether dimensionality reduction is of any use in case of spam filtering or not?\n\nFurther, I have also read in the literature about Document Frequency and TF-IDF as being one of feature reduction techniques. But I'm not sure how does it work and come into play during classification.\n\nI know how to use weka, chain filters and classifiers, etc. The problem I'm facing is since I don't have enough idea about feature selection/reduction (including TF-IDF) I am unable to decide how and what feature selection techniques and classification algorithms I should combine to make my study meaningful. I also have no idea about optimal threshold value that I should use with chi-square, info gain, etc.\n\nIn StringToWordVector class, I have an option of IDFTransform, so does it makes sence to set it to TRUE and also use a feature selection technique, say InfoGain?\n\nPlease guide me and if possible please provide links to resources where I can learn about dimension reduction in detail and can plan my experiment meaningfully!\n    ", "Answer": "\r\nWell, Naive Bayes seems to work best for spam filtering, and it doesn't play nicely with dimensionality reduction.\n\nMany dimensionality reduction methods try to identify the features of the highest variance. This of course won't help a lot with spam detection, you want discriminative features.\n\nPlus, there is not only one type of spam, but many. Which is likely why naive Bayes works better than many other methods that assume there is only one type of spam.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "pyCUDA reduction doesn't work\r\n                \r\nI am using reduction code basically exactly like the examples in the docs. The code below should return ```\n2^3 + 2^3 = 16```\n, but it instead returns 9. What did I do wrong?\n\n```\nimport numpy\nimport pycuda.reduction as reduct\nimport pycuda.gpuarray as gpuarray\nimport pycuda.autoinit\nfrom pycuda.compiler import SourceModule as module\n\nnewzeros = [{1,2,3},{4,5,6}]\ngpuSum = reduct.ReductionKernel(numpy.uint64, neutral=\"0\", reduce_expr=\"a+b\", map_expr=\"1 << x[i]\", arguments=\"int* x\")\nmylengths = pycuda.gpuarray.to_gpu(numpy.array(map(len,newzeros),dtype = \"uint64\",))\nsumfalse = gpuSum(mylengths).get()\nprint sumfalse\n```\n\n    ", "Answer": "\r\nI just figured it out. The argument list used when defining the kernel should be ```\nunsigned long *x```\n, not ```\nint *x```\n. I was using 64-bit integers everywhere else and it messed it up.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Initialize variable for omp reduction\r\n                \r\nThe OpenMP standard specifies an initial value for a reduction variable. So do I have to initialize the variable and how would I do that in the following case:\n\n```\nint sum;\n//...\nfor(int it=0;i<maxIt;i++){\n#pragma omp parallel\n{\n  #pragma omp for nowait\n  for(int i=0;i<ct;i++)\n    arrayX[i]=arrayY[i];\n\n  sum = 0;\n  #pragma omp for reduction(+:sum)\n  for(int i=0;i<ct;i++)\n    sum+=arrayZ[i];\n}\n//Use sum\n}\n```\n\n\nNote that I use only 1 parallel region to minimize overhead and to allow the nowait in the first loop. Using this as-is would lead to a data race (IMO) because the threads coming from the first loop after other threads started the 2nd loop will reset sum.\nOf course I can do this at the top of the outer loop but in a general case and for large code bases you may forget that you need or had set it there which produces unexpected results.\nDoes \"omp single\" help here? I suspect that while thread A executes the single, another thread may already enter the reduction loop.\n\"omp barrier\" is possible but I want to avoid that as it defeats the \"nowait\".\n\nAnd last another example:\n\n```\n#pragma omp parallel\n{\n  sum = 0;\n  #pragma omp for reduction(+:sum)\n  for(int i=0;i<ct;i++)\n    sum+=arrayZ[i];\n  //Use sum\n  sum = 0;\n  #pragma omp for reduction(+:sum)\n  for(int i=0;i<ct;i++)\n    sum+=arrayZ[i];\n  //Use sum\n}\n```\n\n\nHow would I (re)initialize here?\n    ", "Answer": "\r\nEdit: This answer is wrong as it makes an assumption that is not in the OpenMP specification. As accepted answers cannot be deleted, I'm leaving it here as an example that one should always doubt and validate code and/or statements found on the Internet.\n\nActually, the code doesn't exhibit data races:\n\n```\n#pragma omp parallel\n{\n   ...\n   sum = 0;\n   #pragma omp for reduction(+:sum)\n   for(int i=0;i<ct;i++)\n     sum+=arrayZ[i];\n   ...\n}\n```\n\n\nWhat happens here is that a private copy of ```\nsum```\n is created inside the worksharing construct and is initialised to ```\n0```\n (the initialisation value for the ```\n+```\n operator). Each local copy is updated by the loop body. Once a given thread has finished, it waits at the implicit barrier present at the end of the ```\nfor```\n construct. Once all threads have reached the barrier, their local copies of ```\nsum```\n are summed together and the result is added to the shared value.\n\nIt doesn't matter that all threads might execute ```\nsum = 0;```\n at different time since its value is only updated once the barrier has been reached. Think of the code above performing something like:\n\n```\n...\nsum = 0;\n// Start of the for worksharing construct\nint local_sum = 0;                     // ^\nfor(int i = i_start; i < i_end; i++)   // | sum not used here\n  local_sum += arrayZ[i];              // v\n// Implicit construct barrier\n#pragma omp barrier\n// Reduction\n#pragma omp atomic update\nsum += local_sum;\n#pragma omp barrier\n// End of the worksharing construct\n...\n```\n\n\nThe same applies to the second example.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "thrust::reduce_by_key performance with few key repetitions\r\n                \r\nI have to do keyed reductions of arrays with many different keys that repeat only once in a while:\n\n```\nkeys =  {1,2,3,3,4,5,6,7,7, 8, 9, 9,10,11,...}\narray = {1,2,3,4,5,6,7,8,9,10,11,12,13,14,...}\n\n// after reduction\nresult = {1,2,7,5,6,7,17,10,23,13,14}\n```\n\n\nUsing ```\nthrust::reduce_by_key```\n (or any other segmented reduction method) is not the fastest option here as most operations are in fact just copies from one array to another.\n\nWhat would be a better approach to this problem?\n    ", "Answer": "\r\nActually, ```\nreduce_by_key```\n is the appropriate algorithm to use here.  It's just that the current implementation in Thrust is not as fast as it could be.  To elaborate, there's nothing to prevent ```\nreduce_by_key```\n from executing at memcpy speed, and I believe other implementations already achieve that rate.  Our tentative plan for Thrust v1.7 includes making performance improvements to ```\nreduce_by_key```\n and other scan-based algorithms using the codes in the related back40computing project.\n\nNote that when the segments are either (1) long or of (2) uniform length then it's possible to do better than ```\nreduce_by_key```\n.  For example, at some point it's more economical to use an offset-based segment descriptor than keys or head flags.  However, when the segments are short (as in your case) or of highly variable length, then an optimal ```\nreduce_by_key```\n implementation is really the best tool for the job.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Solving \"string reduction\" challenge\r\n                \r\nI have seen various discussions and code attempts at solving the \"String reduction\" problem from interviewstreet.com, but none of them does it via dynamic programming.\nListed under the Dynamic Programming section, the problem is described as follows:\n\nGiven a string consisting of a,b and c's, we can perform the following operation: Take any two adjacent distinct characters and replace it with the third character. For example, if 'a' and 'c' are adjacent, they can replaced with 'b'.\nWhat is the smallest string which can result by applying this operation repeatedly?\n\nThe problem can be solved using exhaustive brute-force search, effectively creating a tree of all possible substitutions:\n```\n// this is more or less pseudo code from my head\nint GetMinLength(string s)\n{\n    // solve edge cases\n    if (s.Length == 1) return 1;\n    if (s.Length == 2) return ReduceIfPossible(s);\n\n    // recursive brute force\n    int min = s.Length;\n    for (int i = 0; i<s.Length-1; i++)\n    {\n        if (s[i] != s[i+1])\n        {\n            var next = GetMinLength(\n                  s.Substring(0, i) + \n                  Reduce(s[i], s[i + 1]) +\n                  s.Substring(i + 2)\n                  );\n\n            if (next < min) min = next;\n        }\n    }\n}\n```\n\nThis obviously fails for larger ```\nN```\n (```\nN <= 100```\n), so I am trying to break it into smaller subproblems, memoize them, and merge results.\nThe problem is that I cannot determine the state which would have \"optimal substructure\", needed to apply dynamic programming (or in other words to \"merge\" results from sub-problems). Minimizing a part of the string doesn't guarantee that the final string will really be the smallest solution.\nWhat would be the subproblem \"state\" in this case, which could be merged towards the final solution?\n    ", "Answer": "\r\nWhat makes this tricky is that you need to treat this as 2 dynamic programming problems in a row.\n\n\nBuild up a table of, by character you wind up with, by start position, all of the possible end positions that are a block that can be reduced to that character.\nThe smallest length that the final ```\ni```\n characters of the string can be reduced to.  (The table that you built in step 1 can be used to recursively reduce this problem to already solved subproblems.)\n\n\nThe second provides your answer.  It is much more straightforward if you have already solved the first.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "To do beta reduction in Matlab?\r\n                \r\nHow can you do a beta reduction like the following in Matlab?\n\n\n\nMy goal is to avoid duplicate assignments and lazy-evaluate things -- perhaps related to the question multiple step anonymous functions. \n\nApparently, most functional features only supported in Matlab2013B.\n    ", "Answer": "\r\nIs this what you mean:\n\n```\nx = 3;\nf = @(y)(x+x*y);\n```\n\n\nNow ```\nf(y)```\n is the function ```\n3+3*y```\n.\n\nSo you could put this in a loop for example:\n\n```\nf = {};\nfor x = 1:5\n   f{x} = @(y)(x+x*y);\nend;\n```\n\n\nAnd then find ```\nf(2)```\n for each of those values of ```\nx```\n\n\n```\ncellfun(@(y)y(2), f)\n\nans =   \n    3    6    9   12   15\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction of NumPy bitwise_and function\r\n                \r\nConsider the following numpy array array:\n\n```\nx = np.array([2]*4, dtype=np.uint8)\n```\n\n\nwhich is just an array of four 2's.\n\nI want to perform a bitwise_and reduction of this array:\n\n```\ny = np.bitwise_and.reduce(x)\n```\n\n\nI expect the result to be:\n\n```\n2\n```\n\n\nbecause each element of the array is identical, so successive AND's should yield the same result, but instead I get:\n\n```\n0\n```\n\n\nWhy the discrepancy? \n    ", "Answer": "\r\nIn the ```\nreduce```\n docstring, it is explained that the function is equivalent to\n\n```\n r = op.identity # op = ufunc\n for i in range(len(A)):\n   r = op(r, A[i])\n return r\n```\n\n\nThe problem is that ```\nnp.bitwise_and.identity```\n is 1:\n\n```\nIn [100]: np.bitwise_and.identity\nOut[100]: 1\n```\n\n\nFor the ```\nreduce```\n method to work as you expect, the identity would have to be an integer with all bits set to 1.\n\nThe above code was run using numpy 1.11.2.  The problem has been fixed in the development version of numpy:\n\n```\nIn [3]: np.__version__\nOut[3]: '1.13.0.dev0+87c1dab'\n\nIn [4]: np.bitwise_and.identity\nOut[4]: -1\n\nIn [5]: x = np.array([2]*4, dtype=np.uint8)\n\nIn [6]: np.bitwise_and.reduce(x)\nOut[6]: 2\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenACC reduction clause with max()\r\n                \r\nI am learning OpenACC and came across the code bellow for the Jacobi iteration, provided by NVidia. From my understanding, ```\nreduction(max:err)```\n creates a private ```\nerr```\n variable for each loop iteration, and returns the max value from all of them.\nMy question is why use ```\nerr = max(err, abs(Anew[j][i] - A[j][i]);```\n, specifically, the ```\nmax()```\n function, when reduction seems to do this by itself?\n```\nwhile ( err > tol && iter < iter_max ) \n{ \nerr=0.0;\n#pragma acc kernels reduction(max:err) \nfor( int j = 1; j < n-1; j++) \n{ \n    for(int i = 1; i < m-1; i++) \n    { \n        Anew[j][i] = 0.25 * (A[j][i+1] + A[j][i-1] + A[j-1][i] + A[j+1][i]); \n        err = max(err, abs(Anew[j][i] - A[j][i]); \n    } \n} \n        \n#pragma acc kernels\nfor( int j = 1; j < n-1; j++)\n{ \n    for( int i = 1; i < m-1; i++ ) \n    { \n        A[j][i] = Anew[j][i]; \n    } \n} \niter++; \n```\n\n}\n    ", "Answer": "\r\nThe \"max\" function takes the maximum value from the arguments for a single iteration.  The max reduction finds the max across all the iterations.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Haskell foldr reduction\r\n                \r\nI am still very new to Haskell and I am trying to solve a problem. I have attempted something but I am very sure it is wrong.\n\nThe Problem:\n\n```\nfoldr : f x NIL = x\nfoldr : f x (cons a list) = f a (foldr f x list)\n```\n\n\nLet’s have some fun and combine the features of a number of different languages.\n\n(1) Borrowing from the course notes and Haskell, let’s define a function foldl,\n\n(a) ```\nfoldl f x NIL = x```\n\n\n(b) ```\nfoldl f x (cons a list) = foldl f (f x a) list```\n\n\n(2) Borrowing from Python, we can treat a string as a sequence (i.e. a list) of characters,\n\n```\n“bar” ≡ (‘b’ ‘a’ ‘r’) ≡ [‘b’, ‘a’, ‘r’]\n```\n\n\n(3) Borrowing from Lisp, we can use cons cells to represent or create lists,\n\n```\n(‘b’ ‘a’ ‘r’) ≡ (cons ‘b’ (cons ‘a’ (cons ‘r’ nil)))\n```\n\n\n(4) Borrowing from Fortran, we can define a string concatenation function, //,\n\n```\n‘concate’//’nation’ ⇒‘concatenation’\n```\n\n\n(5) Borrowing from Scheme, define a toggle function, \n\n```\nf x y = f y x,\n(define toggle (lambda (f x y)  (f y x)))\n```\n\n\nThat is, the toggle function switches its arguments, in Lambda Calculus, it’s,\n\n```\n|_f. |_x. |_y. f y x\n```\n\n\nThus, for example,\n\n```\n(f ‘back’ ‘drop’) ⇒(f ‘drop’ back’)\n```\n\n\nIn this question, please trace the execution/reduction of the following function:\n\n```\nfoldl (toggle //) nil (“foo” “bar” “baz”)\n```\n\n\nBelow is a Hint I am Given:\n\n```\n    foldl (toggle //) nil (“foo” “bar” “baz”)\n```\n\n\nStep1: Start with (1b), left side, ```\nfoldl f x (cons a list)```\n\n\n```\n        with f = (toggle //), x = nil, and list = (“foo” “bar” “baz”)\n```\n\n\nStep2: Use (3) to recursively expand the list\n\nStep3: Use (1b), right-side, foldl f (f x a) list\n\n```\n        to recursively apply f, (toggle //), to the expanded list\n```\n\n\nStep4: Use (5) to apply the toggle functions in the list expression.\n\nStep5: Use (4), //, to reduce the expression\n\nWhat I Got:\n\n```\nfoldl (toggle //) nil (“foo” “bar” “baz”)\nfoldl f x (cons a list) – via (1b)\nfoldl f x(a list)  – via (3)\nfoldl f (f x a) list – via (1b)\nfoldl (toggle //) (toggle // x a) list – via(5)\nfoldl x // // a list – via (4)\nfoldl nil a (“foo” “bar” “baz”)\n```\n\n\nI know that can't be right but I feel like I am somewhere near. If I could get some guidance I feel I could become unstuck and have a successful understanding.\n    ", "Answer": "\r\nstart with \n\n```\nfoldl (toggle //) nil (“foo” “bar” “baz”)\n{ 1b }\n= foldl (toggle //) ((toggle //) nil \"foo\") (“bar” “baz”)\n```\n\n\n...\n\nremarks/questions:\n\n\n```\n(\"foo\" \"bar\" \"baz\")```\n is not Haskell for lists! (what are the strange ```\n\"```\n?)\n```\n(toggle //)```\n is not Haskell syntax for operators either (it's ```\n(toggle (//))```\n)\nwhat strategy are you supposed to use? Depending on it you have to continue with the ```\n(toggle //) nil \"foo\")```\n or the ```\nfoldl```\n\n\n\nthe ```\ntoggle```\n would be\n\n```\n(toggle //) nil \"foo\"\n{ 5 }\n= (//) \"foo\" nil\n{ 4 }\n\"foo\"\n```\n\n\nthe other would be the same again:\n\n```\nfoldl (toggle //) ((toggle //) nil \"foo\") (“bar” “baz”)\n{ 1b }\n= foldl (toggle //) ((toggle //) ((toggle //) nil \"foo\") \"bar\") (\"baz\")\n```\n\n\nI'm sure you can work out the rest (it's really just putting the right thing into the right spot )\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "pyspark tsne dimension reduction\r\n                \r\nIs there a way to implement dimension reduction using pyspark?\nI have a dataframe and loaded into pyspark.\n```\nFILENAME = \"test.csv\"\nspark = SparkSession.builder.appName('Test')  \\\n    .getOrCreate()\n\nspark_df = spark.read.csv(FILENAME, header=True)\n# Load the embeddings from the spark_df\nembedded_df_columns = spark_df.columns[5:]\nembedded_df = spark_df.select(embedded_df_columns)\n```\n\nI dont seem to find the right pyspark.ml.features for tsne. All I get was for pca. Can anyone help please\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "python PCA dimensionality reduction\r\n                \r\ni'm learning using PCA to finish dimensionality reduction (Python3.6) but i've got very similar but different results when using different methods here's my code\n```\nfrom numpy import *\nfrom sklearn.decomposition import PCA\n\ndata_set = [[-1., -2.],\n            [-1., 0.],\n            [0., 0.],\n            [2., 1.],\n            [0., 1.]]\n# 1\npca_sk = PCA(n_components=1)\nnewmat = pca_sk.fit_transform(data_set)\nprint(newmat)\n\n# 2\nmeanVals = mean(data_set, axis=0)\nmeanRemoved = data_set - meanVals  \ncovMat = cov(meanRemoved, rowvar=0)\neigVals, eigVects = linalg.eig(mat(covMat))\neigValInd = argsort(eigVals)\neigValInd = eigValInd[:-(1 + 1):-1]\nredEigVects = eigVects[:, eigValInd]\nlowDDataMat = meanRemoved * redEigVects\nprint(lowDDataMat)\n```\n\nthe first one output\n```\n[[ 2.12132034]\n [ 0.70710678]\n [-0.        ]\n [-2.12132034]\n [-0.70710678]]\n```\n\nbut anothor output\n```\n[[-2.12132034]\n [-0.70710678]\n [ 0.        ]\n [ 2.12132034]\n [ 0.70710678]]\n```\n\nwhy dose it happen\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "What does dimensionality reduction mean?\r\n                \r\nWhat does dimensionality reduction mean exactly?\n\nI searched for its meaning, I just found that it means the transformation of raw data into a more useful form.  So what is the benefit of having data in useful form, I mean how can I use it in a practical life (application)?\n    ", "Answer": "\r\nDimensionality Reduction is about converting data of very high dimensionality into data of much lower dimensionality such that each of the lower dimensions convey much more information.   \n\nThis is typically done while solving machine learning problems to get better features for a classification or regression task.  \n\nHeres a contrived example - Suppose you have a list of 100 movies and 1000 people and for each person, you know whether they like or dislike each of the 100 movies. So for each instance (which in this case means each person) you have a binary vector of length 100 [position i is 0 if that person dislikes the i'th movie, 1 otherwise ].\nYou can perform your machine learning task on these vectors directly.. but instead you could decide upon 5 genres of movies and using the data you already have, figure out whether the person likes or dislikes the entire genre and, in this way reduce your data from a vector of size 100 into a vector of size 5 [position i is 1 if the person likes genre i]  \n\nThe vector of length 5 can be thought of as a good representative of the vector of length 100 because most people might be liking movies only in their preferred genres.\n\nHowever its not going to be an exact representative because there might be cases where a person hates all movies of a genre except one. \n\nThe point is, that the reduced vector conveys most of the information in the larger one while consuming a lot less space and being faster to compute with.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda calculus predecessor function reduction steps\r\n                \r\nI am getting stuck with the Wikipedia description of the predecessor function in lambda calculus.\nWhat Wikipedia says is the following:\n```\nPRED := λn.λf.λx. n (λg.λh. h (g f)) (λu.x) (λu.u)\n```\n\nCan someone explain reduction processes step-by-step?\nThanks.\n    ", "Answer": "\r\nOk, so the idea of Church numerals is to encode \"data\" using functions, right? The way that works is by representing a value by some generic operation you'd perform with it. We can therefore go in the other direction as well, which can sometimes make things clearer.\n\nChurch numerals are a unary representation of the natural numbers. So, let's use ```\nZ```\n to mean zero and ```\nSn```\n to represent the successor of ```\nn```\n. Now we can count like this: ```\nZ```\n, ```\nSZ```\n, ```\nSSZ```\n, ```\nSSSZ```\n... The equivalent Church numeral takes two arguments--the first corresponding to ```\nS```\n, and second to ```\nZ```\n--then uses them to construct the above pattern. So given arguments ```\nf```\n and ```\nx```\n, we can count like this: ```\nx```\n, ```\nf x```\n, ```\nf (f x)```\n, ```\nf (f (f x))```\n...\n\nLet's look at what PRED does. \n\nFirst, it creates a lambda taking three arguments--```\nn```\n is the Church numeral whose predecessor we want, of course, which means that ```\nf```\n and ```\nx```\n are the arguments to the resulting numeral, which thus means that the body of that lambda will be ```\nf```\n applied to ```\nx```\n one time fewer than ```\nn```\n would.\n\nNext, it applies ```\nn```\n to three arguments. This is the tricky part.\n\nThe second argument, that corresponds to ```\nZ```\n from earlier, is ```\nλu.x```\n--a constant function that ignores one argument and returns ```\nx```\n.\n\nThe first argument, that corresponds to ```\nS```\n from earlier, is ```\nλgh.h (g f)```\n. We can rewrite this as ```\nλg. (λh.h (g f))```\n to reflect the fact that only the outermost lambda is being applied ```\nn```\n times. What this function does is take the accumulated result so far as ```\ng```\n and return a new function taking one argument, which applies that argument to ```\ng```\n applied to ```\nf```\n. Which is absolutely baffling, of course. \n\nSo... what's going on here? Consider the direct substitution with ```\nS```\n and ```\nZ```\n. In a non-zero number ```\nSn```\n, the ```\nn```\n corresponds to the argument bound to ```\ng```\n. So, remembering that ```\nf```\n and ```\nx```\n are bound in an outside scope, we can count like this: ```\nλu.x```\n, ```\nλh. h ((λu.x) f)```\n, ```\nλh'. h' ((λh. h ((λu.x) f)) f)```\n ... Performing the obvious reductions, we get this: ```\nλu.x```\n, ```\nλh. h x```\n, ```\nλh'. h' (f x)```\n ... The pattern here is that a function is being passed \"inward\" one layer, at which point an ```\nS```\n will apply it, while a ```\nZ```\n will ignore it. So we get one application of ```\nf```\n for each ```\nS```\n except the outermost.\n\nThe third argument is simply the identity function, which is dutifully applied by the outermost ```\nS```\n, returning the final result--```\nf```\n applied one fewer times than the number of ```\nS```\n layers ```\nn```\n corresponds to.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Range reduction for trigonometric functions\r\n                \r\nI'm trying to implement range reduction for trigonometric functions.\nI found this paper http://www.computer.org/csdl/proceedings/pcspa/2010/4180/00/4180b048-abs.html which talks about using 64-bit integer arithmetic.\n\nThe idea presented should work but there seems to be some problem with equations in the paper.\nIs this efficient than the one implemented in fdlibm ?\n    ", "Answer": "\r\nShould you want to to perform a complete floating point range reduction, consult K.C. Ng's \"ARGUMENT REDUCTION FOR HUGE ARGUMENTS: Good to the Last Bit\" readily findable on the web.\n\nThe salient issue is that to do range reduction on standard trig functions such as sine(x), where x is in radians, one must do a precise ```\nmod```\n operation involving Pi.  The ```\nmod```\n needs to extend 4/Pi out to enough factional bit places to have a meaningful result.   This paper details that process and how far one needs to go.  Turns out, it is potential 100s of bits, but not millions of bits.  Possible you are aware of this issue, but if not, it is what you need to know to make a good reduction using 64-bit routines or whatever.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Different results after reduction\r\n                \r\nI've got two reduction algorithms, both are from docs.nvidia, so they should be correct but the first (very effective) gives me a wrong result. Second result is better but I expected better accuracy. Is there any error in algorithms or am I doing something in a bad way?\n\n```\n#include <stdio.h>\n#include <cuda.h>\n#include <stdlib.h>\n#include <math.h>\n#include \"cuda_error.h\"\n\n//Lock definition\n#ifndef __LOCK_H__\n#define __LOCK_H__\nstruct Lock {\nint *mutex;\nLock( void ) {\nCudaSafeCall( cudaMalloc( (void**)&mutex,\nsizeof(int) ) );\nCudaSafeCall( cudaMemset( mutex, 0, sizeof(int) ) );\n}\n~Lock( void ) {\ncudaFree( mutex );\n}\n__device__ void lock( void ) {\nwhile( atomicCAS( mutex, 0, 1 ) != 0 );\n}\n__device__ void unlock( void ) {\natomicExch( mutex, 0 );\n}\n};\n#endif\n//-------------------------\n\n\nconst int N = 507904;\nconst int blockSize = 256;\nint blocks = N/blockSize;\n\ntemplate <unsigned int blockSize>\n__global__ void reduce(Lock lock, float *g_idata, float *g_odata, int n)\n{\n      extern __shared__ int sdata[];\n      unsigned int tid = threadIdx.x;\n      unsigned int i = blockIdx.x*(blockSize*2) + tid;\n      unsigned int gridSize = blockSize*2*gridDim.x;\n      sdata[tid] = 0;\n\n      while (i < n) \n      { \n          sdata[tid] += g_idata[i] + g_idata[i+blockSize]; \n          i += gridSize; \n      }\n\n      __syncthreads();\n\n      if (blockSize >= 512) { if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); }\n      if (blockSize >= 256) { if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); }\n      if (blockSize >= 128) { if (tid < 64) { sdata[tid] += sdata[tid + 64]; } __syncthreads(); }\n\n      if (tid < 32) \n      {\n          if (blockSize >= 64) sdata[tid] += sdata[tid + 32];\n          if (blockSize >= 32) sdata[tid] += sdata[tid + 16];\n          if (blockSize >= 16) sdata[tid] += sdata[tid + 8];\n          if (blockSize >= 8) sdata[tid] += sdata[tid + 4];\n          if (blockSize >= 4) sdata[tid] += sdata[tid + 2];\n          if (blockSize >= 2) sdata[tid] += sdata[tid + 1];\n      }\n\n    if (tid == 0)\n    {\n        lock.lock();        \n        *g_odata += sdata[0];\n        lock.unlock();\n    }\n\n}\n\n__global__ void reduction_sum(Lock lock,float *in, float *out, int N) \n{\n    extern __shared__ float sf_data[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    sf_data[tid] = (i<N) ? in[i] : 0;\n\n    __syncthreads();\n\n    for (int s = blockDim.x/2; s>0; s>>=1) \n    {\n      if (tid < s) \n        {\n        sf_data[tid] += sf_data[tid + s];\n      }\n      __syncthreads();\n    }\n\n    if (tid == 0)\n    {\n        lock.lock();        \n        *out += sf_data[0];\n        lock.unlock();\n    }\n}\n//initializing function\ndouble los()\n{\n    return (double)rand()/(double)RAND_MAX;\n}\n//-------------------------------------------\n\n\nint main (void)\n{\n\n//memory allocation \n    float *a;\n    float *dev_a, *dev_blocksum1, *dev_blocksum2;\n    float s1=0, s2=0, spr=0;\n\n    a = (float*)malloc( N*sizeof(float) );\n    CudaSafeCall( cudaMalloc( (void**)&dev_a, N*sizeof(float) ) );\n    CudaSafeCall( cudaMemset( dev_a, 0, N*sizeof(float) ) );\n    CudaSafeCall( cudaMalloc( (void**)&dev_blocksum1, sizeof(float) ) );\n    CudaSafeCall( cudaMalloc(   (void**)&dev_blocksum2, sizeof(float)   )   );\n    CudaSafeCall( cudaMemset( dev_blocksum1, 0, sizeof(float) ) );\n    CudaSafeCall( cudaMemset( dev_blocksum2, 0, sizeof(float) ) );\n//--------------------\n\n//drawing, array fill\n    srand(2403);\n    int i;\n    for (i=0; i<N; i++)\n    {\n        a[i]=los();\n        spr+=a[i];\n    }\n    printf(\"CPU sum: %f \\n\", spr);\n//copy HtoD\n    CudaSafeCall( cudaMemcpy( dev_a, a, N*sizeof(float), cudaMemcpyHostToDevice ) );\n//---------------------\n\nLock lock;\n\n//reduce\n    reduce<blockSize><<<blocks, blockSize, blockSize*sizeof(float)>>>(lock, dev_a, dev_blocksum1, N);\n    CudaSafeCall(   cudaMemcpy ( &s1, dev_blocksum1, sizeof(float), cudaMemcpyDeviceToHost  )   );\n    printf(\"GPU sum1: %f \\n\", s1);\n//-----------------------\n\n//reduction_sum\n    reduction_sum<<<blocks, blockSize, blockSize*sizeof(float)>>>(lock, dev_a, dev_blocksum2, N);\n    CudaSafeCall(   cudaMemcpy ( &s2, dev_blocksum2, sizeof(float), cudaMemcpyDeviceToHost  )   );\n    printf(\"GPU sum2: %f \\n\", s2);\n//---------------------\n\n    return 0;\n}\n\n$ CPU sum: 253833.515625 \n$ GPU sum1: 14021.000000 \n$ GPU sum2: 253835.906250 \n```\n\n    ", "Answer": "\r\nThere are a number of things to mention.\n\n\nI'm not sure your error checking is valid.  You haven't shown the file that implements this, and when I run your code with ```\ncuda-memcheck```\n, I get various errors reported.  They all seem to be related to the lock function.\nI'm not sure why you would use the lock function and I don't recommend it.  I don't think it is dropping out of scope when you think it is, based on the way you are using it.  I would suggest using ```\natomicAdd```\n instead, which should be faster and simpler.  At a minimum, comment out the ```\ncudaFree()```\n statement in your destructor.\nYou are linking to an old presentation.  If you review a newer version of it then I think you will see that it is now recommending use of ```\nvolatile```\n.  I'm not going to rewrite your whole code for you, nor summarize that whitepaper again, but if you simply add ```\nvolatile```\n to the shared memory declaration for demonstration purposes, it will fix the resultant issue.\nYour shared memory variable is declared as ```\nint```\n but you are summing ```\nfloat```\n quantities.  That won't work the way you want.  You could declare it like this instead:\n\n```\nextern __shared__ volatile float sdata[];\n```\n\nThe above changes get the code \"working\" for me.  The remaining item is the discrepancy between CPU and GPU results.  I believe this is due to floating-point order of operations not being the same on the CPU (serial reduction) vs. GPU (parallel reduction).  Since the discrepancy arises in the 6th significant digit on a ```\nfloat```\n quantity, I would suggest this is well within the range of reasonableness for floating point results comparison.  If you desire more accuracy, you might try switching from ```\nfloat```\n to ```\ndouble```\n.  Also, there are various floating point papers you may wish to read that will help with understanding here, such as this one and this one.\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction with OpenMP: linear merging or log(number of threads) merging\r\n                \r\nI have a general question about reductions with OpenMP that's bothered me for a while.  My question is in regards to merging the partial sums in a reduction.  It can either be done linearly or as the log of the number of threads.\n\nLet's assume I want to do a reduction of some function ```\ndouble foo(int i)```\n.  With OpenMP I could do it like this.\n\n```\ndouble sum = 0.0;    \n#pragma omp parallel for reduction (+:sum)\nfor(int i=0; i<n; i++) {\n    sum += f(i);\n}\n```\n\n\nHowever, I claim that the following code will be just as efficient.\n\n```\ndouble sum = 0.0;\n#pragma omp parallel\n{\n    double sum_private = 0.0;\n    #pragma omp for nowait\n    for(int i=0; i<n; i++) {\n        sum_private += f(i)\n    }\n    #pragma omp critical\n    {\n        sum += sum_private;\n    }\n}\n```\n\n\nNot, only will this second code case have effectively the same performance but it's more general.  It can handle any operator I define whereas the reduction construct only works for some basic operators on plain old data types.\n\nLet's assume there are ```\nt```\n threads.  The reason I claim this second method is just as fast is that the time to merge the partial sums is negligible compared to the parallel loop. The time to do the partial sums is proportional to ```\nn/t```\n and the time to merge the sums goes as ```\nt```\n.  So as long as ```\nn>>t```\n or the time it takes to do the parallel loop (if ```\nfoo```\n is slow compare to summing) is large enough the merging will be negligible.  \n\nI have heard it's possible  to merge the partial sums in ```\nO(log(t))```\n.  However, for all practical purposes I don't see how this will help.  The maximum number of physical cores in OpenMP is on order 50, let's assume it's 64.  It won't make much difference to merge 64 values in 64 steps or in eight binary steps compared to doing the parallel loop.  Additionally, merging the values in some kind of binary tree could have an overhead which is larger than just doing the linear merge so it's not even necessarily faster.\n\nWhen would merging the partial sums in ```\nO(log(t))```\n ever help?  When would the first code case ever have a performance advantage over the second code case?\n\nI know some coworkers who merge in ```\nO(log(t))```\n on the GPU with OpenCL (by running the kernel several times for each binary merge) but I have not seen any evidence yet to show it's better than just merging linearly.\n\nEdit: Jim Cownie wanted to see an actual test rather than a claim.  Below is the results and code.  This was done with MSVC2012 64-bit release mode on a Xeon E5-1620 (Sandy Bridge) with four physical cores. Both the first and second case are about exactly 4.45x faster than without OpenMP. \n\nThe results:\n\n```\nwithout OpenMP time 1.787158 s\nfirst case     time 0.400462 s\nsecond case    time 0.400456 s\n```\n\n\nThe code:\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\ndouble foo(int i) {\n    double fi = i;\n    return 1.0*fi/(1+fi*fi);\n}\n\ndouble reduce(int n) {\n    double sum = 0.0f;\n    for(int i=0; i<n; i++) {\n        sum += foo(i);\n    }\n    return sum;\n}\n\ndouble reduce_omp(int n) {\n    double sum = 0.0f;\n    #pragma omp parallel for reduction(+:sum)\n    for(int i=0; i<n; i++) {\n        sum += foo(i);\n    }\n    return sum;\n}\n\ndouble reduce_omp2(int n) {\n    double sum = 0.0f;\n    #pragma omp parallel \n    {\n        double sum_private = 0.0f;\n        #pragma omp for nowait\n        for(int i=0; i<n; i++) {\n            sum_private += foo(i);\n        }\n        #pragma omp critical \n        {\n            sum+= sum_private;\n        }\n    }\n    return sum;\n}\n\nint main() {\n    int n,r;\n    double sum, dtime;\n    n = 1<<28;\n    r = 1;\n\n    dtime = omp_get_wtime();\n    for(int i=0; i<r; i++) sum = reduce(n);\n    dtime = omp_get_wtime() - dtime;\n    printf(\"time %f, sum %f\\n\", dtime, sum);\n\n    reduce_omp(n);  //warm omp up\n\n    dtime = omp_get_wtime();\n    for(int i=0; i<r; i++) sum = reduce_omp(n);\n    dtime = omp_get_wtime() - dtime;\n    printf(\"time %f, sum %f\\n\", dtime, sum);\n\n    dtime = omp_get_wtime();\n    for(int i=0; i<r; i++) sum = reduce_omp2(n);\n    dtime = omp_get_wtime() - dtime;\n    printf(\"time %f, sum %f\\n\", dtime, sum);\n\n\n}\n```\n\n    ", "Answer": "\r\nThe OpenMP implementation will make a decision about the best way to do the reduction based on the implementor's knowledge of the specific characteristics of the hardware it's running on. On system with a small number of CPUs, it will probably do a linear reduction. On a system with hundreds or thousands of cores (e.g. GPU, Intel Phi) it will likely do a log(n) reduction.\n\nThe time spent in the reduction might not matter for very large problems, but for smaller problems it could be add a few percent to the total runtime. Your implementation might be just as fast in many cases, but I doubt it would ever be faster, so why not let OpenMP decide on the optimal reduction strategy?\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Create an MDX calculated member to make a stacked bar chart using data with total and reductions\r\n                \r\nWe use a cube built in SSAS and we would like to show the effect of deltas against a forecast.\n\nSo, we have data like this:\n\n\nYear    Type        Amount\n----    ---------   ---------\n2008    Forecast    +1000\n2008    Delta 1     - 100\n2008    Delta 2     - 200\n2009    ...\n\n\nWe would like to make a stacked bar chart, so we need data like this:\n\n\nYear    Type        Amount\n----    ---------   ---------\n2008    Result      +700\n2008    Delta 1     +100\n2008    Delta 2     +200\n\n\nYou can see that 'Result' equals the sum of forecast and the deltas, at a particular granularity (in this this example case it is just time, we will also have place as an added dimension).\n\nThe catch is we'd like to be able to filter out a delta and have the bar chart update, so then 'Result' would need to then to change so that the sum is still the forecast for that granularity:\n\n\nType          Amount\n---------   ---------\nResult         +900\nDelta 1        +100\n\n\nWe could do this in a view, but then we'd have to generate a result for every possibility of delta selects and deselects, which doesn't seem optimal.  \n\nIs there a way to do this in the cube? Our best guess is some kind of MDX calculated member but we are open to other solutions as well.\n\nThanks!\n\nUPDATE 1:\nThe problem I see with a calculated field is that it adds a member (i.e. a column) to the cube, where what we really need to do is add rows with a UNION...but in a cube.  Here's some psuedo SQL as an example:\n\n\nSELECT \n  Year,\n  'Result' AS Type,\n  SUM(Amount) AS Amount\nFROM Table1\nWHERE 1=1\n  --AND Type 'Delta 2' \nGROUP BY Year\n\nUNION ALL\n\nSELECT \n  Year,\n  Type,\n  -1*Amount AS Amount\nFROM Table1\nWHERE Type LIKE 'Delta%'\n  --AND Type 'Delta 2'\n\n--Note the commented out WHERE clauses are where you would de-select a delta if desired.\n\n\nCan that be translated to MDX for an on the fly calculation?\n\nUPDATE 2:\n\nThis is the closest I've gotten so far, but it's not perfect:\n\n\nCREATE MEMBER CURRENTCUBE.[Measures].[Adjusted]\n AS CASE [Measure Type DEV].[Measure Type] /* This is the 'Type' column above */\n    WHEN [Measure Type DEV].[Measure Type].&[2] /* Forecast is key 2 */\n    THEN ([Measure Type DEV].[Measure Type].&[2],\n        [Time].[Year].CURRENTMEMBER,[Measures].[Amount]) \n      + ([Measure Type DEV].[Measure Type].&[3], /* Delta is key 3 */\n         [Time].[Year].CURRENTMEMBER,[Measures].[Amount]) END\n\n    ", "Answer": "\r\nThere is no real great way in MDX to do this but when I had to create forcasts I used data mining to accomplish this task with this article as my main referencing point:\n\nhttp://cwebbbi.spaces.live.com/Blog/cns!7B84B0F2C239489A!2795.entry?a665aac0&wa=wsignin1.0\n\nI hope this helps and it may be quicker than attempting to solve it with straight MDX.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction with OpenMP: linear merging or log(number of threads) merging\r\n                \r\nI have a general question about reductions with OpenMP that's bothered me for a while.  My question is in regards to merging the partial sums in a reduction.  It can either be done linearly or as the log of the number of threads.\n\nLet's assume I want to do a reduction of some function ```\ndouble foo(int i)```\n.  With OpenMP I could do it like this.\n\n```\ndouble sum = 0.0;    \n#pragma omp parallel for reduction (+:sum)\nfor(int i=0; i<n; i++) {\n    sum += f(i);\n}\n```\n\n\nHowever, I claim that the following code will be just as efficient.\n\n```\ndouble sum = 0.0;\n#pragma omp parallel\n{\n    double sum_private = 0.0;\n    #pragma omp for nowait\n    for(int i=0; i<n; i++) {\n        sum_private += f(i)\n    }\n    #pragma omp critical\n    {\n        sum += sum_private;\n    }\n}\n```\n\n\nNot, only will this second code case have effectively the same performance but it's more general.  It can handle any operator I define whereas the reduction construct only works for some basic operators on plain old data types.\n\nLet's assume there are ```\nt```\n threads.  The reason I claim this second method is just as fast is that the time to merge the partial sums is negligible compared to the parallel loop. The time to do the partial sums is proportional to ```\nn/t```\n and the time to merge the sums goes as ```\nt```\n.  So as long as ```\nn>>t```\n or the time it takes to do the parallel loop (if ```\nfoo```\n is slow compare to summing) is large enough the merging will be negligible.  \n\nI have heard it's possible  to merge the partial sums in ```\nO(log(t))```\n.  However, for all practical purposes I don't see how this will help.  The maximum number of physical cores in OpenMP is on order 50, let's assume it's 64.  It won't make much difference to merge 64 values in 64 steps or in eight binary steps compared to doing the parallel loop.  Additionally, merging the values in some kind of binary tree could have an overhead which is larger than just doing the linear merge so it's not even necessarily faster.\n\nWhen would merging the partial sums in ```\nO(log(t))```\n ever help?  When would the first code case ever have a performance advantage over the second code case?\n\nI know some coworkers who merge in ```\nO(log(t))```\n on the GPU with OpenCL (by running the kernel several times for each binary merge) but I have not seen any evidence yet to show it's better than just merging linearly.\n\nEdit: Jim Cownie wanted to see an actual test rather than a claim.  Below is the results and code.  This was done with MSVC2012 64-bit release mode on a Xeon E5-1620 (Sandy Bridge) with four physical cores. Both the first and second case are about exactly 4.45x faster than without OpenMP. \n\nThe results:\n\n```\nwithout OpenMP time 1.787158 s\nfirst case     time 0.400462 s\nsecond case    time 0.400456 s\n```\n\n\nThe code:\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\n\ndouble foo(int i) {\n    double fi = i;\n    return 1.0*fi/(1+fi*fi);\n}\n\ndouble reduce(int n) {\n    double sum = 0.0f;\n    for(int i=0; i<n; i++) {\n        sum += foo(i);\n    }\n    return sum;\n}\n\ndouble reduce_omp(int n) {\n    double sum = 0.0f;\n    #pragma omp parallel for reduction(+:sum)\n    for(int i=0; i<n; i++) {\n        sum += foo(i);\n    }\n    return sum;\n}\n\ndouble reduce_omp2(int n) {\n    double sum = 0.0f;\n    #pragma omp parallel \n    {\n        double sum_private = 0.0f;\n        #pragma omp for nowait\n        for(int i=0; i<n; i++) {\n            sum_private += foo(i);\n        }\n        #pragma omp critical \n        {\n            sum+= sum_private;\n        }\n    }\n    return sum;\n}\n\nint main() {\n    int n,r;\n    double sum, dtime;\n    n = 1<<28;\n    r = 1;\n\n    dtime = omp_get_wtime();\n    for(int i=0; i<r; i++) sum = reduce(n);\n    dtime = omp_get_wtime() - dtime;\n    printf(\"time %f, sum %f\\n\", dtime, sum);\n\n    reduce_omp(n);  //warm omp up\n\n    dtime = omp_get_wtime();\n    for(int i=0; i<r; i++) sum = reduce_omp(n);\n    dtime = omp_get_wtime() - dtime;\n    printf(\"time %f, sum %f\\n\", dtime, sum);\n\n    dtime = omp_get_wtime();\n    for(int i=0; i<r; i++) sum = reduce_omp2(n);\n    dtime = omp_get_wtime() - dtime;\n    printf(\"time %f, sum %f\\n\", dtime, sum);\n\n\n}\n```\n\n    ", "Answer": "\r\nThe OpenMP implementation will make a decision about the best way to do the reduction based on the implementor's knowledge of the specific characteristics of the hardware it's running on. On system with a small number of CPUs, it will probably do a linear reduction. On a system with hundreds or thousands of cores (e.g. GPU, Intel Phi) it will likely do a log(n) reduction.\n\nThe time spent in the reduction might not matter for very large problems, but for smaller problems it could be add a few percent to the total runtime. Your implementation might be just as fast in many cases, but I doubt it would ever be faster, so why not let OpenMP decide on the optimal reduction strategy?\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "reduction of eigen based templates\r\n                \r\nI am trying to do a reduction based on eigen matrix.\n\n```\n#include <iostream>\n#include <Eigen/Dense>\n#include <type_traits>\n\ntemplate<typename T1, typename T2, int n1, int n2>\nauto reduction(Eigen::Matrix<T1, n1, n2> &a1,\n               Eigen::Matrix<T2, n1, n2> &a2)\n     -> decltype(T1{}*T2{})\n{\n  using BaseT3 = \n    typename std::remove_cv<typename std::remove_reference<decltype(T1{}*T2{})>::type>::type;\n\n  BaseT3 res = a1(0, 0)*a2(0, 0);\n\n  for (int i=0; i<n1; ++i)\n    for (int j=0; j<n2; ++j)\n      if (i+j)\n        res = res + a1(i, j)*a2(i, j);\n\n  return res;\n}\n\nint main()\n{\n  Eigen::Matrix<double, 3, 3> m;\n  Eigen::Matrix<Eigen::Vector3d, 3, 3> n;\n\n  std::cout << reduction(m, n) << std::endl;\n}\n```\n\n\nBasically, Im a trying to get ```\nsum_{i, j} a1[i, j] * a2[i, j]```\n where ```\na1```\n and ```\na2```\n are some eigen mathix but I get compilation errors. The error I get is\n\n```\nerror: no match for ‘operator=’ (operand types are ‘BaseT3 {aka \nEigen::CwiseUnaryOp<Eigen::internal::scalar_multiple_op<double>, \nconst Eigen::Matrix<double, 3, 1> >}’ \nand \n‘const Eigen::CwiseBinaryOp<Eigen::internal::scalar_sum_op<double>, \nconst Eigen::CwiseUnaryOp<Eigen::internal::scalar_multiple_op<double>, \nconst Eigen::Matrix<double, 3, 1> >, \nconst Eigen::CwiseUnaryOp<Eigen::internal::scalar_multiple_op<double>, \nconst Eigen::Matrix<double, 3, 1> > >’)\n         res = res + a1(i, j)*a2(i, j);\n             ^\n```\n\n\nIf I am not mistaken, for the given ```\nmain```\n, type ```\nBaseT3```\n should have been ```\nEigen::Vector3d```\n. I also tried to static cast so the ```\noperator=```\n should not fail but I then get other errors.\n\nThis is c++11, I use Eigen3 and the compiler is g++ 5.4.1.\n    ", "Answer": "\r\nThe decltype of T1 * T2 isn't what you expect here - Eigen heavily uses expression templates.  The CWiseUnaryOp and CWiseBinaryOp types in your error are indicative of that.  In other words, the result of \"double * Vector3d\" isn't what you'd expect (it's not a Vector3d, it's a cwisebinaryop).\n\nSee also: Writing functions taking Eigen Types.\n\nIn this specific case you may find a solution by creating partial specializations for Eigen base types for both the first and second parameters of your template function.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Strided reduction by CUDA Thrust\r\n                \r\nI have an array of vertices with this kind of structure:\n\n```\n[x0, y0, z0, empty float, x1, y1, z1, empty float, x2, y2, z2, empty float, ...]```\n\n\nI need to find ```\nminX```\n, ```\nminY```\n, ```\nminZ```\n, ```\nmaxX```\n, ```\nmaxY```\n and ```\nmaxZ```\n using CUDA. I wrote a proper reduction algorithm, but it occurs to be a little too slow. I decided to use the THRUST library. There is a highly optimized ```\nreduce()```\n, or even better ```\nminmax_element()```\n, method which is a way to find max and min of an array simultaneously, but I can't find a fast way to use then only every ```\n4```\nth index. Copying data to ```\n3```\n separated arrays is not a solution I'm looking for.\n\nIs there a way (some kind of tricks with Thrust iterators or something like this) to pass a stride to ```\nreduce()```\n?\n    ", "Answer": "\r\nYou can use a strided range method, along with 3 calls to thrust::minmax_element, to get your desired result without modifying data storage.\n\nHere's a worked example:\n\n```\n$ cat t491.cu \n#include <thrust/device_vector.h>\n#include <thrust/host_vector.h>\n#include <iostream>\n#include <thrust/copy.h>\n#include <thrust/iterator/permutation_iterator.h>\n#include <thrust/iterator/counting_iterator.h>\n#include <thrust/iterator/transform_iterator.h>\n#include <thrust/functional.h>\n#include <thrust/extrema.h>\n#include <thrust/transform_reduce.h>\n\n#define DSIZE (1048576*2)\n#define SSIZE 4\n#define FSIZE (DSIZE*SSIZE)\n#define nTPB 256\n#define BSIZE nTPB\n#define nBLKS 64\n#define FLOAT_MIN (-99999)\n#define FLOAT_MAX 99999\n\n\ntypedef thrust::tuple<float, float, float, float, float, float> tpl6;\n\nstruct expand_functor\n{\n  __host__ __device__\n  tpl6 operator()(const float4 a){\n    tpl6 result;\n    result.get<0>() = a.x;\n    result.get<1>() = a.x;\n    result.get<2>() = a.y;\n    result.get<3>() = a.y;\n    result.get<4>() = a.z;\n    result.get<5>() = a.z;\n    return result;\n  }\n};\n\n\nstruct minmax3_functor\n{\n  __host__ __device__\n  tpl6 operator()(const tpl6 a, const tpl6 b) {\n    tpl6 result;\n    result.get<0>() = (a.get<0>() < b.get<0>()) ? a.get<0>():b.get<0>();\n    result.get<1>() = (a.get<1>() > b.get<1>()) ? a.get<1>():b.get<1>();\n    result.get<2>() = (a.get<2>() < b.get<2>()) ? a.get<2>():b.get<2>();\n    result.get<3>() = (a.get<3>() > b.get<3>()) ? a.get<3>():b.get<3>();\n    result.get<4>() = (a.get<4>() < b.get<4>()) ? a.get<4>():b.get<4>();\n    result.get<5>() = (a.get<5>() > b.get<5>()) ? a.get<5>():b.get<5>();\n    return result;\n  }\n};\n\n__device__ int bcount = 0;\n__device__ float xmins[nBLKS];\n__device__ float xmaxs[nBLKS];\n__device__ float ymins[nBLKS];\n__device__ float ymaxs[nBLKS];\n__device__ float zmins[nBLKS];\n__device__ float zmaxs[nBLKS];\n\n__global__ void my_minmax3(float4 *data, float *results, size_t dsize){\n  // assumes power-of-2 threadblock size\n  // assumes nBLKS <= nTPB, nBLKS also power-of-2\n  __shared__ float xmin[BSIZE], xmax[BSIZE], ymin[BSIZE], ymax[BSIZE], zmin[BSIZE], zmax[BSIZE];\n  __shared__ int lblock;\n\n  float my_xmin = FLOAT_MAX;\n  float my_ymin = FLOAT_MAX;\n  float my_zmin = FLOAT_MAX;\n  float my_xmax = FLOAT_MIN;\n  float my_ymax = FLOAT_MIN;\n  float my_zmax = FLOAT_MIN;\n  int idx = threadIdx.x+blockDim.x*blockIdx.x;\n  while (idx < dsize){\n    float4 my_temp = data[idx];\n    if (my_xmin > my_temp.x) my_xmin = my_temp.x;\n    if (my_ymin > my_temp.y) my_ymin = my_temp.y;\n    if (my_zmin > my_temp.z) my_zmin = my_temp.z;\n    if (my_xmax < my_temp.x) my_xmax = my_temp.x;\n    if (my_ymax < my_temp.y) my_ymax = my_temp.y;\n    if (my_zmax < my_temp.z) my_zmax = my_temp.z;\n    idx += blockDim.x*gridDim.x;}\n  xmin[threadIdx.x] = my_xmin;\n  ymin[threadIdx.x] = my_ymin;\n  zmin[threadIdx.x] = my_zmin;\n  xmax[threadIdx.x] = my_xmax;\n  ymax[threadIdx.x] = my_ymax;\n  zmax[threadIdx.x] = my_zmax;\n  __syncthreads();\n  for (int i = blockDim.x/2; i > 0; i>>=1){\n    if (threadIdx.x < i){\n      if (xmin[threadIdx.x] > xmin[threadIdx.x+i]) xmin[threadIdx.x] = xmin[threadIdx.x + i];\n      if (ymin[threadIdx.x] > ymin[threadIdx.x+i]) ymin[threadIdx.x] = ymin[threadIdx.x + i];\n      if (zmin[threadIdx.x] > zmin[threadIdx.x+i]) zmin[threadIdx.x] = zmin[threadIdx.x + i];\n      if (xmax[threadIdx.x] < xmax[threadIdx.x+i]) xmax[threadIdx.x] = xmax[threadIdx.x + i];\n      if (ymax[threadIdx.x] < ymax[threadIdx.x+i]) ymax[threadIdx.x] = ymax[threadIdx.x + i];\n      if (zmax[threadIdx.x] < zmax[threadIdx.x+i]) zmax[threadIdx.x] = zmax[threadIdx.x + i];\n      }\n    __syncthreads();\n    }\n  if (!threadIdx.x){\n    xmins[blockIdx.x] = xmin[0];\n    xmaxs[blockIdx.x] = xmax[0];\n    ymins[blockIdx.x] = ymin[0];\n    ymaxs[blockIdx.x] = ymax[0];\n    zmins[blockIdx.x] = zmin[0];\n    zmaxs[blockIdx.x] = zmax[0];\n    __threadfence();\n    if (atomicAdd(&bcount, 1) == (nBLKS-1)) lblock = 1;\n    else lblock = 0;\n    }\n  __syncthreads();\n  if (lblock){ // last block does final reduction\n    if (threadIdx.x < nBLKS){\n      xmin[threadIdx.x] = xmins[threadIdx.x];\n      xmax[threadIdx.x] = xmaxs[threadIdx.x];\n      ymin[threadIdx.x] = ymins[threadIdx.x];\n      ymax[threadIdx.x] = ymaxs[threadIdx.x];\n      zmin[threadIdx.x] = zmins[threadIdx.x];\n      zmax[threadIdx.x] = zmaxs[threadIdx.x];}\n    __syncthreads();\n    for (int i = nBLKS/2; i > 0; i>>=1){\n      if (threadIdx.x < i){\n        if (xmin[threadIdx.x] > xmin[threadIdx.x+i]) xmin[threadIdx.x] = xmin[threadIdx.x + i];\n        if (ymin[threadIdx.x] > ymin[threadIdx.x+i]) ymin[threadIdx.x] = ymin[threadIdx.x + i];\n        if (zmin[threadIdx.x] > zmin[threadIdx.x+i]) zmin[threadIdx.x] = zmin[threadIdx.x + i];\n        if (xmax[threadIdx.x] < xmax[threadIdx.x+i]) xmax[threadIdx.x] = xmax[threadIdx.x + i];\n        if (ymax[threadIdx.x] < ymax[threadIdx.x+i]) ymax[threadIdx.x] = ymax[threadIdx.x + i];\n        if (zmax[threadIdx.x] < zmax[threadIdx.x+i]) zmax[threadIdx.x] = zmax[threadIdx.x + i];\n        }\n      __syncthreads();\n      }\n    if (!threadIdx.x){\n      results[0] = xmin[0];\n      results[1] = xmax[0];\n      results[2] = ymin[0];\n      results[3] = ymax[0];\n      results[4] = zmin[0];\n      results[5] = zmax[0];\n      }\n\n   }\n}\n\ntemplate <typename Iterator>\nclass strided_range\n{\n    public:\n\n    typedef typename thrust::iterator_difference<Iterator>::type difference_type;\n\n    struct stride_functor : public thrust::unary_function<difference_type,difference_type>\n    {\n        difference_type stride;\n\n        stride_functor(difference_type stride)\n            : stride(stride) {}\n\n        __host__ __device__\n        difference_type operator()(const difference_type& i) const\n        {\n            return stride * i;\n        }\n    };\n\n    typedef typename thrust::counting_iterator<difference_type>                   CountingIterator;\n    typedef typename thrust::transform_iterator<stride_functor, CountingIterator> TransformIterator;\n    typedef typename thrust::permutation_iterator<Iterator,TransformIterator>     PermutationIterator;\n\n    // type of the strided_range iterator\n    typedef PermutationIterator iterator;\n\n    // construct strided_range for the range [first,last)\n    strided_range(Iterator first, Iterator last, difference_type stride)\n        : first(first), last(last), stride(stride) {}\n\n    iterator begin(void) const\n    {\n        return PermutationIterator(first, TransformIterator(CountingIterator(0), stride_functor(stride)));\n    }\n\n    iterator end(void) const\n    {\n        return begin() + ((last - first) + (stride - 1)) / stride;\n    }\n\n    protected:\n    Iterator first;\n    Iterator last;\n    difference_type stride;\n};\n\ntypedef thrust::device_vector<float>::iterator Iter;\ntypedef strided_range<Iter>::iterator sIter;\n\nint main(){\n// set up test data\n  cudaEvent_t start, stop;\n  float et;\n  cudaEventCreate(&start); cudaEventCreate(&stop);\n  thrust::host_vector<float> h_vals(FSIZE);\n  for (int i = 0; i < DSIZE; i ++) {\n    h_vals[i*SSIZE + 0] = rand()/(float)RAND_MAX;\n    h_vals[i*SSIZE + 1] = rand()/(float)RAND_MAX;\n    h_vals[i*SSIZE + 2] = rand()/(float)RAND_MAX;\n    h_vals[i*SSIZE + 3] = 0.0f;}\n  thrust::device_vector<float> d_vals = h_vals;\n// set up strided ranges\n  strided_range<Iter> item_x(d_vals.begin()  , d_vals.end(), SSIZE);\n  strided_range<Iter> item_y(d_vals.begin()+1, d_vals.end(), SSIZE);\n  strided_range<Iter> item_z(d_vals.begin()+2, d_vals.end(), SSIZE);\n// find min and max\n  cudaEventRecord(start);\n  thrust::pair<sIter, sIter> result_x = thrust::minmax_element(item_x.begin(), item_x.end());\n  thrust::pair<sIter, sIter> result_y = thrust::minmax_element(item_y.begin(), item_y.end());\n  thrust::pair<sIter, sIter> result_z = thrust::minmax_element(item_z.begin(), item_z.end());\n  cudaEventRecord(stop);\n  cudaEventSynchronize(stop);\n  cudaEventElapsedTime(&et, start, stop);\n  std::cout << \"thrust elapsed time: \" << et << \"ms\" << std::endl;\n  std::cout << \"thrust results: \" << std::endl;\n  std::cout << \"x min element = \" << *(result_x.first)  << std::endl;\n  std::cout << \"x max element = \" << *(result_x.second) << std::endl;\n  std::cout << \"y min element = \" << *(result_y.first)  << std::endl;\n  std::cout << \"y max element = \" << *(result_y.second) << std::endl;\n  std::cout << \"z min element = \" << *(result_z.first)  << std::endl;\n  std::cout << \"z max element = \" << *(result_z.second) << std::endl;\n\n  float *h_results, *d_results;\n  h_results = new float[6];\n  cudaMalloc(&d_results, 6*sizeof(float));\n  cudaEventRecord(start);\n  my_minmax3<<<nBLKS,nTPB>>>((float4 *)thrust::raw_pointer_cast(d_vals.data()), d_results, DSIZE);\n  cudaEventRecord(stop);\n  cudaEventSynchronize(stop);\n  cudaEventElapsedTime(&et, start, stop);\n  cudaMemcpy(h_results, d_results, 6*sizeof(float), cudaMemcpyDeviceToHost);\n  std::cout << \"kernel elapsed time: \" << et << \"ms\" << std::endl;\n  std::cout << \"kernel results: \" << std::endl;\n  std::cout << \"x min element = \" << h_results[0]  << std::endl;\n  std::cout << \"x max element = \" << h_results[1]  << std::endl;\n  std::cout << \"y min element = \" << h_results[2]  << std::endl;\n  std::cout << \"y max element = \" << h_results[3]  << std::endl;\n  std::cout << \"z min element = \" << h_results[4]  << std::endl;\n  std::cout << \"z max element = \" << h_results[5]  << std::endl;\n\n  thrust::device_ptr<float4> dptr_vals = thrust::device_pointer_cast(reinterpret_cast<float4 *>( thrust::raw_pointer_cast(d_vals.data())));\n  tpl6 my_init;\n  my_init.get<0>() = FLOAT_MAX;\n  my_init.get<1>() = FLOAT_MIN;\n  my_init.get<2>() = FLOAT_MAX;\n  my_init.get<3>() = FLOAT_MIN;\n  my_init.get<4>() = FLOAT_MAX;\n  my_init.get<5>() = FLOAT_MIN;\n  cudaEventRecord(start);\n  tpl6 my_result = thrust::transform_reduce(dptr_vals, dptr_vals + DSIZE, expand_functor(),  my_init, minmax3_functor());\n  cudaEventRecord(stop);\n  cudaEventSynchronize(stop);\n  cudaEventElapsedTime(&et, start, stop);\n  cudaMemcpy(h_results, d_results, 6*sizeof(float), cudaMemcpyDeviceToHost);\n  std::cout << \"thrust2 elapsed time: \" << et << \"ms\" << std::endl;\n  std::cout << \"thrust2 results: \" << std::endl;\n  std::cout << \"x min element = \" << my_result.get<0>()  << std::endl;\n  std::cout << \"x max element = \" << my_result.get<1>()  << std::endl;\n  std::cout << \"y min element = \" << my_result.get<2>()  << std::endl;\n  std::cout << \"y max element = \" << my_result.get<3>()  << std::endl;\n  std::cout << \"z min element = \" << my_result.get<4>()  << std::endl;\n  std::cout << \"z max element = \" << my_result.get<5>()  << std::endl;\n  return 0;\n}\n$ nvcc -O3 -arch=sm_20 -o t491 t491.cu\n$ ./t491\nthrust elapsed time: 3.88784ms\nthrust results:\nx min element = 1.16788e-06\nx max element = 0.999998\ny min element = 2.85916e-07\ny max element = 1\nz min element = 1.72295e-08\nz max element = 0.999999\nkernel elapsed time: 0.462848ms\nkernel results:\nx min element = 1.16788e-06\nx max element = 0.999998\ny min element = 2.85916e-07\ny max element = 1\nz min element = 1.72295e-08\nz max element = 0.999999\nthrust2 elapsed time: 1.29728ms\nthrust2 results:\nx min element = 1.16788e-06\nx max element = 0.999998\ny min element = 2.85916e-07\ny max element = 1\nz min element = 1.72295e-08\nz max element = 0.999999\n$\n```\n\n\nI've updated the above example to include for comparison an \"optimized\" reduction kernel that does all 6 reductions (min and max operations) in a single kernel call.\n\nAs expected, this approach runs faster than 3 individual thrust calls to produce the same result, about 5-8 times faster in my case (RHEL5.5, Quadro5000, CUDA 6.5RC), depending on data size.  Note that although I have chosen a data size (```\nDSIZE```\n) here that is a power of 2, the entire example works correctly for arbitrary data sizes.  I have dispensed with proper cuda error checking for the sake of brevity of presentation.\n\nEDIT: Following the suggestion by @JaredHoberock I have added a 3rd approach that allows a single call to ```\nthrust::transform_reduce```\n to produce all 6 results.  These are the \"thrust2\" results above.  This method is about 3x faster than the first (three-thrust-call) method.  Still not as fast as the cuda kernel method, but perhaps this thrust approach can be optimized further.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Data reduction via IMPORTRANGE\r\n                \r\nI am trying to do some data reduction in my Google Sheets by using the following IMPORTRANGE formula:\n\n```\n=query(importrange(\"https://docs.google.com/a/ap.averydennison.com/spreadsheets/d/1xz1lXY-w5Ii_aWqVAhHgRCmeoes9ltSUtibE4kzhMHA/edit#gid=2051232966\",\"SF_Flex_Rel!a:l\"), \n \"select * where Col1 = '\"&text(B1,\"###\")&\"'\",1)\n```\n\n\nThe 'source' sheet has a whole lot of sales data records. What I am trying to do in the new sheet via this formula is only bring in the sales records from the source sheet that match the customer number specified in cell B1.\n\nIt seems to work OK if I limit the IMPORTRANGE to only query about 10,000 rows. Once I go over around 20,000 rows the screen will briefly flash up the records, then a small progress bar shows in the top right corner of the sheet and the records disappear. The cell with the formula just shows ```\n#ERROR!```\n with no other comments to tell me why.\n\nIs there something wrong with my formula syntax?\n\nIs there a better way to achieve this data reduction?\n\nIs there some undocumented data limitation on IMPORTRANGE function (I am using 'new' Google Sheets)?\n    ", "Answer": "\r\ntry like my example :\n\n```\n  =QUERY(                                                // data\n              IMPORTRANGE(\n                \"Spreadsheet Key\",    // spreadsheet key \n                \"DATA!A:C\"                                         // datarange\n              ), \n              \"SELECT Col1 WHERE Col2=\" & \"'\" & B2 & \"'\"           // query\n            )\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Variable Domain Reduction in Halide\r\n                \r\nRight now I'm trying to write some Halide code that subsamples an image. Basically I want every 2 by 2 square of the image to be reduced to one pixel that contains the maximum. A simple example would be transforming\n\n```\n1 2 3 4\n5 6 7 8\n9 0 1 2\n4 3 5 6\n```\n\n\ninto\n\n```\n6 8\n9 6\n```\n\n\nRight now I'm trying something along the lines of (I'm aware that this would give the sum instead of the maximum, but it's a toy example of the same process):\n\n```\nHalide::Image<uint8_t> input = load<uint8_t>(\"test.png\");        \nHalide::Image<uint8_t> output(input.width() / 2, input.height() / 2, input.channels());\n\n Halide::Func subsample;\n Halide::Var c;\n\n for (int i = 0; i < input.height(); i += 2) {\n     for (int j = 0; j < input.width(); j += 2) {\n         Halide::RDom r = Halide::RDom(i, 2, j, 2);\n         subsample(i, j, c) += input(r.x, r.y, c);\n     }\n }\n\n subsample.realize(output);\n save(output, \"test.png\");\n```\n\n\nHowever, this code runs infinitely. (I'm not sure why). I know I can use Halide::RDom to represent a reduce operation over some range. However, in no example I find can you pass a variable into a random domain object. \n\nEDIT:\n\nAfter playing around with Halide some more, I was able to construct this:\n\n```\nsubsample(x, y, c) = Halide::max(input(2*x,2*y,c),input(2*x+1,2*y,c));\nsubsample(x, y, c) = Halide::max(subsample(x,y,c),input(2*x,2*y+1,c));\nsubsample(x, y, c) = Halide::max(subsample(x,y,c),input(2*x+1,2*y+1,c));\n```\n\n\nTo get a 2x2 max reduction. However, when I put this in a loop, it won't call because it cannot be defined. Is there anyway to put this in terms of a domain reduction?\n    ", "Answer": "\r\nI think argmax (which is embedded Halide function) can be used for what you want :)\n\n```\n#include \"Halide.h\"\n#include <stdio.h>\n\nuint8_t data[16] = {\n    1, 2, 3, 4,\n    5, 6, 7, 8,\n    9, 0, 1, 2,\n    3, 4, 5, 6\n};\n\nusing namespace Halide;\n\nint main(int argc, char** argv) {\n    Halide::Image<uint8_t> input(4, 4); \n    for(int j = 0; j < 4; j++) {\n        for(int i = 0; i < 4; i++) {\n            input(j, i) = data[j*4 + i];\n        }\n    }\n\n    Halide::Func f, max2x2;\n    Halide::Var x, y, dx, dy;\n    Halide::Expr x_ = x * 2;\n    Halide::Expr y_ = y * 2;\n\n    f(x, y, dx, dy) = input(x_ + dx, y_ + dy);\n\n    RDom r(0, 2, 0, 2);\n    max2x2(x, y) = argmax(f(x, y, r.x, r.y))[2];\n\n    Halide::Image<uint8_t> output(2, 2);\n    max2x2.realize(output);\n\n    for(int j = 0; j < 2; j++) {\n        for(int i = 0; i < 2; i++) {\n            printf(\"%d \", output(j, i));\n        }\n        printf(\"\\n\");\n    }\n    return 0;\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Erlang: Will Process Priority Affect Long Running Tasks?\r\n                \r\nWhen a process is scheduled with low priority in erlang, it is scheduled into the low/normal queue with a count of 8. It has to be picked from the queue 8 times before getting scheduled.\n\nOnce it's scheduled, a counter for 2000 reductions is started. After the 2000 reductions the process will be suspended and rescheduled. When the process is rescheduled, is it scheduled with the same priority that the original process was?\n\nThat would make sense to me but I can't find this detail and it would have a big impact for long running computational tasks (not that it's a good idea to use BEAM for heavy computation!) .\n    ", "Answer": "\r\nWhen the process is rescheduled, is it scheduled with the same priority that the original process was?\n\nWhat do you mean by the original process? It is the same process so it retains its priority.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Beta reduction Lambda calculus\r\n                \r\nI am trying to reduce the following using beta reduction:\n\n```\n(λx.x x) (λx. λy.x x)```\n\n\nI am getting stuck after the first substitution since it seems to be giving ```\n(λx. λy.x x)(λx. λy.x x)```\n  which would end in kind of a loop. What am I doing wrong?\n    ", "Answer": "\r\nHere's an illustration of the evaluation\n\n```\nbeta reduction 1\n(λx.x x) (λx.λy.x x)              →β x [x := (λx.λy.x x)]\n(λx.(λx.λy.x x) (λx.λy.x x))\n\nbeta reduction 2\n(λx.λy.x x) (λx.λy.x x)           →β x [x := (λx.λy.x x)]\n(λx.λy.(λx.λy.x x) (λx.λy.x x))\n\nresult\nλy.(λx.λy.x x) (λx.λy.x x)```\n\n\nNow we have reached Weak Head Normal Form – ie, we have a lambda ```\nλy```\n without any arguments to apply it to.\n\nTo get to Head Normal Form, we can attempt to reduce under the lambda ...\n\n```\nreduction 1\nλy.(λx.λy.x x) (λx.λy.x x)       →β x [x := (λx.λy.x x)]\nλy.(λx.λy.(λx.λy.x x) (λx.λy.x x))\n\nreduction 2 ...\nλy.λy.(λx.λy.x x) (λx.λy.x x)\n```\n\n\nOk, we can immediately see that this pattern will repeat itself. Each time we try to reduce under the lambda, the result gets wrapped in another ```\nλy```\n.\n\nSo, this particular lambda expression does not have a Head Normal Form – ie, the evaluation of this expression (when applied to an argument) will never terminate; it will never reach Normal Form.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "boosting parallel reduction OpenCL\r\n                \r\nI have an algorithm, performing two-staged parallel reduction on GPU to find the smallest elemnt in a string. I know that there is a hint on how to make it work faster, but I don't know what it is. Any ideas on how I can tune this kernel to speed my program up? It is not necessary to actually change algorithm, may be there are other tricks. All ideas are welcome.\n\nThank you!\n\n```\n__kernel\nvoid reduce(__global float* buffer,\n            __local float* scratch,\n            __const int length,\n            __global float* result) {    \n    int global_index = get_global_id(0);\n    float accumulator = INFINITY\n        while (global_index < length) {\n            float element = buffer[global_index];\n            accumulator = (accumulator < element) ? accumulator : element;\n            global_index += get_global_size(0);\n    }\n    int local_index = get_local_id(0);\n    scratch[local_index] = accumulator;\n    barrier(CLK_LOCAL_MEM_FENCE);\n    for(int offset = get_local_size(0) / 2;\n        offset > 0;\n        offset = offset / 2) {\n            if (local_index < offset) {\n                float other = scratch[local_index + offset];\n                float mine = scratch[local_index];\n                scratch[local_index] = (mine < other) ? mine : other;\n            }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n    if (local_index == 0) {\n        result[get_group_id(0)] = scratch[0];\n    }\n}\n```\n\n    ", "Answer": "\r\n```\naccumulator = (accumulator < element) ? accumulator : element;\n```\n\n\nUse fmin function - it is exactly what you need, and it may result in faster code (call to built-in instruction, if available, instead of costly branching)\n\n```\nglobal_index += get_global_size(0);\n```\n\n\nWhat is your typical ```\nget_global_size(0)```\n?\n\nThough your access pattern is not very bad (it is coalesced, 128byte chunks for 32-warp) - it is better to access memory sequentially whenever possible. For instance, sequential access may aid memory prefetching (note, ```\nOpenCL```\n code can be executed on any device, including CPU).\n\nConsider following scheme: each thread would process range\n\n```\n[ get_global_id(0)*delta ,  (get_global_id(0)+1)*delta )\n```\n\n\nIt will result in fully sequential access.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA - Parallel Reduction Sum\r\n                \r\nI am trying to implement a parallel reduction sum in CUDA 7.5. I have been trying to follow the NVIDIA PDF that walks you through the initial algorithm and then steadily more optimised versions. I am currently making an array that is filled with 1 as the value in every array position so that I can check the output is correct but I am getting a value of -842159451 for an array of size 64. I am expecting that the kernel code is correct as I have followed the exact code from NVIDIA for it but here is my kernel:\n\n```\n__global__ void reduce0(int *input, int *output) {\n    extern __shared__ int sdata[];\n\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    sdata[tid] = input[i];\n\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            sdata[tid] += sdata[tid + s];\n        }\n\n        __syncthreads();\n    }\n\n    if (tid == 0) output[blockIdx.x] = sdata[0];\n}\n```\n\n\nHere is my code calling the kernel, which is where I expect my problem to be:\n\n```\nint main()\n{\n    int numThreadsPerBlock = 1024;\n\n    int *hostInput;\n    int *hostOutput; \n    int *deviceInput; \n    int *deviceOutput; \n\n    int numInputElements = 64;\n    int numOutputElements; // number of elements in the output list, initialised below\n\n    numOutputElements = numInputElements / (numThreadsPerBlock / 2);\n    if (numInputElements % (numThreadsPerBlock / 2)) {\n        numOutputElements++;\n    }\n\n    hostInput = (int *)malloc(numInputElements * sizeof(int));\n    hostOutput = (int *)malloc(numOutputElements * sizeof(int));\n\n\n    for (int i = 0; i < numInputElements; ++i) {\n        hostInput[i] = 1;\n    }\n\n    const dim3 blockSize(numThreadsPerBlock, 1, 1);\n    const dim3 gridSize(numOutputElements, 1, 1);\n\n    cudaMalloc((void **)&deviceInput, numInputElements * sizeof(int));\n    cudaMalloc((void **)&deviceOutput, numOutputElements * sizeof(int));\n\n    cudaMemcpy(deviceInput, hostInput, numInputElements * sizeof(int), cudaMemcpyHostToDevice);\n\n    reduce0 << <gridSize, blockSize >> >(deviceInput, deviceOutput);\n\n    cudaMemcpy(hostOutput, deviceOutput, numOutputElements * sizeof(int), cudaMemcpyDeviceToHost);\n\n    for (int ii = 1; ii < numOutputElements; ii++) {\n        hostOutput[0] += hostOutput[ii]; //accumulates the sum in the first element\n    }\n\n    int sumGPU = hostOutput[0];\n\n    printf(\"GPU Result: %d\\n\", sumGPU);\n\n    std::string wait;\n    std::cin >> wait;\n\n    return 0;\n}\n```\n\n\nI have also tried bigger and smaller array sizes for the input and I get the same result of a very large negative value no matter the size of the array.\n    ", "Answer": "\r\nSeems you are using a dynamically allocated shared array:\n\n```\nextern __shared__ int sdata[];\n```\n\n\nbut you are not allocating it in the kernel invocation:\n\n```\nreduce0 <<<gridSize, blockSize >>>(deviceInput, deviceOutput);\n```\n\n\nYou have two options:\n\nOption 1\n\nAllocate the shared memory statically in the kernel, e.g.\n\n```\nconstexpr int threadsPerBlock = 1024;\n__shared__ int sdata[threadsPerBlock];\n```\n\n\nMore often than not I find this the cleanest approach, as it works without a problem when you have multiple arrays in shared memory. The drawback is that while the size usually depends on the number of threads in the block, you need the size to be known at compile-time.\n\nOption 2\n\nSpecify the amount of dynamically allocated shared memory in the kernel invocation. \n\n```\nreduce0 <<<gridSize, blockSize, numThreadsPerBlock*sizeof(int) >>>(deviceInput, deviceOutput);\n```\n\n\nThis will work for any value of ```\nnumThreadsPerBlock```\n (provided it is within the allowed range of course). The drawback is that if you have multiple extern shared arrays, you need to figure out how to put then in the memory yourself, so that one does not overwrite the other.\n\n\n\nNote, there may be other problems in your code. I didn't test it. This is something I spotted immediately upon glancing over your code.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Why is OpenMP reduction failing with error message 'user defined reduction not found for 'avgs'?'\r\n                \r\nWhy is the reduction in this code throwing an error:\n```\nuser defined reduction not found for 'avgs'\n```\n\n\n```\nvoid parallel_avg_pixel(long img[DIM_ROW][DIM_COL][DIM_RGB], long *avgs) {\n    int row, col, pixel;\n    long count = 0;\n\n#pragma omp parallel for reduction(+:avgs)\n    for (row = 0; row < DIM_ROW; row++) {\n        for (col = 0; col < DIM_COL; col++) {\n            for (pixel = 0; pixel < DIM_RGB; pixel++){\n                avgs[pixel] += img[row][col][pixel];\n                count++;\n            }\n        }\n    }\n\n    count /= 3;\n\n    for (pixel = 0; pixel < DIM_RGB; pixel++) {\n        avgs[pixel] /= count;\n    }\n}\n```\n\nI expected this code to make the value of ```\navgs```\n private for each thread in parallel and then sum them all together at the end of execution of the for loops.\n    ", "Answer": "\r\nYour variable ```\navgs```\n is a pointer. You can not reduce on pointers. OpenMP is telling you that if you want to reduce pointers, you need to define a reduction for them.\nOk, you're using it as an array but then the problem is OpenMP doesn't know how long it is.\nUse the following syntax:\n```\n#pragma omp reduction(+:avgs[0:DIM_RGB])\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA reduction - race condition?\r\n                \r\nPlease consider the following code and accompanying explanatory image that I have obtained from a tutorial.  Its purpose is to demonstrate a parallel reduction in CUDA.\n\n```\n#include \"cuda_runtime.h\"\n#include \"device_launch_parameters.h\"\n\n#include <iostream>\n#include <numeric>\nusing namespace std;\n\n__global__ void sumSingleBlock(int* d)\n{\n  int tid = threadIdx.x;\n\n  // Number of participating threads (tc) halves on each iteration\n  for (int tc = blockDim.x, stepSize = 1; tc > 0; tc >>= 1, stepSize <<= 1)\n  {\n    // Thread must be allowed to write\n    if (tid < tc)\n    {\n      // We need to do A + B, where B is the element following A, so first we \n      // need to find the position of element A and of element B      \n      int posA = tid * stepSize * 2;\n      int posB = posA + stepSize;\n\n      // Update the value at posA by adding the value at posB to it\n      d[posA] += d[posB];\n    }\n  }\n}\n\nint main()\n{\n  cudaError_t status;\n\n  const int count = 8;\n  const int size = count * sizeof(int);\n  int* h = new int[count];\n  for (int i = 0; i < count; ++i)\n    h[i] = i+1;\n\n  int* d;\n  status = cudaMalloc(&d, size);\n\n  status = cudaMemcpy(d,h,size, cudaMemcpyHostToDevice);\n\n  sumSingleBlock<<<1,count/2>>>(d);\n\n  int result;\n  status = cudaMemcpy(&result,d,sizeof(int),cudaMemcpyDeviceToHost);\n\n  cout << \"Sum is \" << result << endl;\n\n  getchar();\n\n  cudaFree(d);\n  delete [] h;\n\n  return 0;\n}\n```\n\n\n\n\nNow, I can understand the general principle of reduction as outlined in the diagram.  What I don't get is how there is no race condition in the additions (*):\n\nSo clearly all four threads will run the loop the same number of times; only if ```\ntid < tc```\n will they do something useful.  Thread #0 adds 1 and 2 and stores the result in element 0.  Its second iteration then accesses element 2.  Meanwhile, thread #1's first iteration is adding 3 and 4 and storing the result in element 2.\n\nWhat if thread #0 starts iteration 2 before thread #1 has finished iteration 1?  That would mean thread #0 could read 3 instead of 7, or possibly a torn value(?) There isn't any synchronisation here, so is the code just wrong?\n\n(*) Note: I don't know for sure that there are no race conditions, I am trusting purely in the tutorial being correct with safe code.\n    ", "Answer": "\r\nThe code is wrong, and it needs a ```\n__syncthreads()```\n call as shown below.\n\n```\n__global__ void sumSingleBlock(int* d)\n{\n  int tid = threadIdx.x;\n\n  // Number of participating threads (tc) halves on each iteration\n  for (int tc = blockDim.x, stepSize = 1; tc > 0; tc >>= 1, stepSize <<= 1)\n  {\n    // Thread must be allowed to write\n    if (tid < tc)\n    {\n      // We need to do A + B, where B is the element following A, so first we \n      // need to find the position of element A and of element B      \n      int posA = tid * stepSize * 2;\n      int posB = posA + stepSize;\n\n      // Update the value at posA by adding the value at posB to it\n      d[posA] += d[posB];\n    }\n     __syncthreads();\n  }\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Result of reduction#1 is wrong\r\n                \r\nI implemented the reduction#1 form the well-known slides by Mark Harris, but I obtain 0 as result. I filled the input array with the same values shown in the slides. I compiled with cuda 7.0 using the command nvcc reduction1.cu -o red1. Where is the mistake? Thanks.\n\n```\n#include <stdio.h>\n#include <cuda_runtime.h>\n\n#define THREADS_PER_BLOCK 16\n\n__global__ void reduce1(int *g_idata, int *g_odata) {\n    extern __shared__ int sdata[];\n    // each thread loads one element from global to shared mem\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n    sdata[tid] = g_idata[i];\n    __syncthreads();\n\n    // do reduction in shared mem\n    for(unsigned int s=1; s < blockDim.x; s *= 2) \n    {\n        if (tid % (2*s) == 0) sdata[tid] += sdata[tid + s];\n            __syncthreads();\n    }\n\n    // write result for this block to global mem\n    if (tid == 0) g_odata[blockIdx.x] = sdata[0];\n}\n\nint main()\n{\n    int inputLength=16;\n    int hostInput[16]={10,1,8,-1,0,-2,3,5,-2,-3,2,7,0,11,0,2};\n    int hostOutput=0;\n    int *deviceInput;\n    int *deviceOutput;\n\n    cudaMalloc((void **)&deviceInput, inputLength * sizeof(int));\n    cudaMalloc((void **)&deviceOutput, sizeof(int));\n\n    cudaMemcpy(deviceInput, hostInput, inputLength * sizeof(int),cudaMemcpyHostToDevice);\n\n    reduce1<<<1,THREADS_PER_BLOCK>>>(deviceInput, deviceOutput);\n\n    cudaDeviceSynchronize();\n\n    cudaMemcpy(&hostOutput, deviceOutput,sizeof(int), cudaMemcpyDeviceToHost);\n\n    printf(\"%d\\n\",hostOutput);\n\n    cudaFree(deviceInput);\n    cudaFree(deviceOutput);\n\n    return 0;\n}\n```\n\n    ", "Answer": "\r\nAs talonmies said, you are using dynamic shared memory, but you are not allocating any memory space for it. You have to specify the size of this memory as the third argument of your kernel execution configuration.\n\n```\nreduce1<<<1, THREADS_PER_BLOCK, 64>>>(deviceInput, deviceOutput);\n                                ^^\n```\n\n\nAnother way to fix this code is to use static shared memory. Declare your shared memory like this:\n\n```\n__shared__ int sdata[16];\n```\n\n\nPlease read this before asking questions for CUDA.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP: task_reduction=reduction? What is 'in_reduction'?\r\n                \r\nIs 'task_reduction' same as 'reduction ([ reduction-modifier ,] reduction\nidentifier : list )' with task reduction-modifier?\nIf it is same, then why do we need 'task_reduction'?\nWhat is 'in_reduction' doing?\nIn the text says 'The in_reduction clause specifies that a task participates in a reduction '\nBut what does that mean? In 'in_reduction' we need same clause as reduction.\n```\nin_reduction(identifier : list)\n```\n\nBut if we can put reduction variables in 'list', then what does that do with 'task participates in reduction'...?\nI can imagine how reduction works but I can't imagine with 'in_reduction'.\nWhy do we need that?\n======================================\nI made an example. This code should give sum of num at even index number\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\nint main(int argc, char* argv[]){\n\n        int Array [10]= {0,1,2,3,4,5,6,7,8,9};\n        int Array_length = 10;\n        int counter = 0;\n        int result = 0;\n\n        #pragma omp parallel\n        #pragma omp single \n\n        {\n        #pragma omp taskgroup task_reduction(+:result)\n        //#pragma omp parallel reduction(task,+:result) this can work same as taskgroup task_reduction?\n        {\n             while (Array_length!=counter){\n                  if (counter%2==0){\n                      #pragma omp task in_reduction(+:result)\n                      {\n                          result+=Array[counter];\n                      }\n                  }else {\n                          result+=Array[counter];\n                  }\n                  counter=counter+1;\n             }\n        }\n    }\n    printf(\"The sum of all array elements is equal to %d.\\n\", result);\n}\n```\n\nAnd I also made an illustration, so I could visualize my comprehension.\n\nSo… task_reduction create a reduction, so that local result can be contributed and only task with in_reduction will participate in contribution of Reduction?\nIf I understand correctly, then this code should give 20 as result. However, my code gives 45, which is sum of 0 to 9.\nWhere did I make a mistake?\nBy the way, what happens if I didn’t write ‘in_reduction’ at all? Then the result is 0?\n    ", "Answer": "\r\nThe way task reduction works is that the task needs to know where to contribute its local result to.  SO, what you have to do is to have an taskgroup that \"creates\" the reduction and then have tasks contribute to it:\n```\nvoid example() {\n    int result = 0;\n#pragma omp parallel   // create parallel team\n#pragma omp single     // have only one task creator\n    {\n        #pragma omp taskgroup task_reduction(+:result)\n        {\n            while(have_to_create_tasks()) {\n                #pragma omp task in_reduction(+:result)\n                {   // this tasks contribute to the reduction\n                    result = do_something();\n                }\n                #pragma omp task firstprivate(result)\n                {   // this task does not contribute to the reduction\n                    result = do_something_else();\n                }\n            }\n        }\n    }\n}\n```\n\nSo, the ```\nin_reduction```\n is needed for a task to contribute to a reduction that has been created by a ```\ntask_reduction```\n clause of the enclosing ```\ntaskgroup```\n region.\nThe ```\nreduction```\n clause cannot be used with the ```\ntask```\n construct, but only worksharing constructs and other loop constructs.\nThe only tasking construct that has the ```\nreduction```\n clause, is the ```\ntaskloop```\n construct that uses it for a short cut for a hidden ```\ntask_reduction```\n construct that encloses all the loop constructs that it creates and that then have a hidden ```\nin_reduction```\n clause, too.\nUPDATE (to cover the edits by the original poster):\nThe problem with the code is now that you have two things happening (see the inline comments in your updated code):\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\nint main(int argc, char* argv[]){\n    int Array [10]= {0,1,2,3,4,5,6,7,8,9};\n    int Array_length = 10;\n    int counter = 0;\n    int result = 0;\n\n    #pragma omp parallel\n    #pragma omp single\n    {\n        #pragma omp taskgroup task_reduction(+:result)\n        {\n            // \"result\" is a shared variable in this parallel region\n            while (Array_length!=counter) {\n                if (counter%2==0){\n                    #pragma omp task in_reduction(+:result)\n                    {\n                        // This task will contribute to the reduction result\n                        // as you would expect.\n                        result+=Array[counter];\n                    }\n                } else {\n                    // This addition to \"result\" is performed by the \"single\"\n                    // thread and thus hits the shared variable.  You can see\n                    // this when you print the address of \"result\" here\n                    // and before the parallel region.\n                    result+=Array[counter];\n                }\n                counter=counter+1;\n            }\n        } // Here the \"single\" thread waits for the taskgroup to complete\n          // and the reduction to happen.  So, here the shared variable\n          // \"result\" is added to the value of \"result\" coming from the\n          // task reduction.  So, result = 25 from the \"single\" thread and\n          // result = 20 are added up to result =45\n    }\n    printf(\"The sum of all array elements is equal to %d.\\n\", result);\n}\n```\n\nThe addition at the end of the task reduction seems to be a race condition as the updates coming from the ```\nsingle```\n thread and the updates coming from the end of the taskgroup are not synchronized.  I guess that the race does not show up, as the code is too fast to clearly expose it.\nTo fix the code, you'd have to also have a ```\ntask```\n construct around the update for odd numbers, like so:\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\nint main(int argc, char* argv[]){\n    int Array [10]= {0,1,2,3,4,5,6,7,8,9};\n    int Array_length = 10;\n    int counter = 0;\n    int result = 0;\n\n    #pragma omp parallel\n    #pragma omp single\n    {\n        #pragma omp taskgroup task_reduction(+:result)\n        {\n            // \"result\" is a shared variable in this parallel region\n            while (Array_length!=counter) {\n                if (counter%2==0){\n                    #pragma omp task in_reduction(+:result)\n                    {\n                        // This task will contribute to the reduction result\n                        // as you would expect.\n                        result+=Array[counter];\n                    }\n                } else {\n                    #pragma omp task firstprivate(result)\n                    {\n                        // \"result\" is now a task-local variable that is not\n                        // shared.  If you remove the firstprivate, then the\n                        // race condition on the shared variable \"result\" is\n                        // back.\n                        result+=Array[counter];\n                    }\n                }\n                counter=counter+1;\n            }\n        } // Here the \"single\" thread waits for the taskgroup to complete\n          // and the reduction to happen.  So, here the shared variable\n          // \"result\" is added to the value of \"result\" coming from the\n          // task reduction.  So, result = 25 from the \"single\" thread and\n          // result = 20 are added up to result =45\n    }\n    printf(\"The sum of all array elements is equal to %d.\\n\", result);\n}\n```\n\nIn my first answer, I failed to add a proper ```\nfirstprivate```\n or ```\nprivate```\n clause to the task.  I'm sorry about that.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Beta reduction of Lambda Calculus\r\n                \r\nI have the following lambda calculus: \n\n1) λx . katze(x)(Garfield)\n\n2) λP . λx . P(x)(tea)\n\n3) λy . λx . likes(x, y)(Mia)\n\nHow do I reduce them with the Beta Reduction?\n\nMy solutions: \n\n1) katze (Garfield)\n\n2) tea\n\n3) likes(Mia)\n    ", "Answer": "\r\nWhen performing beta reduction, you substitute the bound variable to the lambda function with the value supplied. The notation for that is ```\n[param := value]```\n and you pick up the first variable that is given. \n\nIn the case\n```\nλx . katze(x)(Garfield)```\n -> ```\nkatze (Garfield)```\n the reduction is correct. We've substituted the ```\nx```\n variable for ```\nGarfield```\n and removed ```\nλx```\n in the process leaving just the expression inside. Here are the steps that would be taken:\n\n```\nλx . katze(x)(Garfield)```\n\n\n= ```\nkatze(x)[x := Garfield]```\n\n\n= ```\nkatze(Garfield)```\n\n\nHowever, the other two are not correct. You are forgetting that you have a lambda function where the expression inside is another lambda function. Since you have a single input, you only have to reduce one function - the first one, leaving the other. You can think of it of peeling off the outer one and exposing the inner.\n\nIn the case of ```\nλP . λx . P(x)(tea)```\n this can be better represented as ```\n(λP . (λx . P(x)))(tea)```\n where now each lambda function is surrounded by brackets. Since we supply a single input ```\ntea```\n, we only resolve the outer function with parameter ```\nP```\n (leaving the brackets for some clarity):\n\n```\n(λP . (λx . P(x)))(tea)```\n\n\n= ```\n(λx . P(x))[P := tea]```\n\n\n= ```\n(λx . P(x))```\n\n\n= ```\nλx . tea(x)```\n\n\nOr without the brackets:\n\n```\nλP . λx . P(x)(tea)```\n\n\n= ```\nλx . P(x)[P := tea]```\n\n\n= ```\nλx . tea(x)```\n\n\nAs for the final function, it still has the same problem that you are removing both functions, when only one input is given. The correct reduction steps are:\n\n```\nλy . λx . likes(x, y)(Mia)```\n\n\n= ```\nλx . likes(x, y)[y := Mia]```\n\n\n= ```\nλx . likes(x, Mia)```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Why is OpenMP reduction failing with error message 'user defined reduction not found for 'avgs'?'\r\n                \r\nWhy is the reduction in this code throwing an error:\n```\nuser defined reduction not found for 'avgs'\n```\n\n\n```\nvoid parallel_avg_pixel(long img[DIM_ROW][DIM_COL][DIM_RGB], long *avgs) {\n    int row, col, pixel;\n    long count = 0;\n\n#pragma omp parallel for reduction(+:avgs)\n    for (row = 0; row < DIM_ROW; row++) {\n        for (col = 0; col < DIM_COL; col++) {\n            for (pixel = 0; pixel < DIM_RGB; pixel++){\n                avgs[pixel] += img[row][col][pixel];\n                count++;\n            }\n        }\n    }\n\n    count /= 3;\n\n    for (pixel = 0; pixel < DIM_RGB; pixel++) {\n        avgs[pixel] /= count;\n    }\n}\n```\n\nI expected this code to make the value of ```\navgs```\n private for each thread in parallel and then sum them all together at the end of execution of the for loops.\n    ", "Answer": "\r\nYour variable ```\navgs```\n is a pointer. You can not reduce on pointers. OpenMP is telling you that if you want to reduce pointers, you need to define a reduction for them.\nOk, you're using it as an array but then the problem is OpenMP doesn't know how long it is.\nUse the following syntax:\n```\n#pragma omp reduction(+:avgs[0:DIM_RGB])\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Prove NP-completeness of CLIQUE-OR-INDEPENDENT-SET\r\n                \r\nFirst of all, I want to mention that this is my homework. However, to solve my problem I can use any literature I want.\n\nEven though I think that problem is clear from its name, I will give it description: \"For given undirected graph G and given integer k, does G contain totally connected (clique) subgraph of size k or totally disconnected subgraph (independent set) of size k.\"\n\nI know about polynomial reductions from ```\n3-SAT```\n to ```\nCLIQUE```\n and from ```\n3-SAT```\n to ```\nINDEPENDENT-SET```\n. (http://mlnotes.com/2013/04/29/npc.html) However, I have problem with this one because I cannot combine those two reductions. I also tried reduction from ```\nCLIQUE```\n to ```\nCLIQUE-OR-INDEPENDENT-SET```\n but without much success.\n\nSo I would really appreciate any hints!\n\nThanks in advance.\n    ", "Answer": "\r\nI found out reduction from problem ```\nINDEPENDENT-SET```\n to ```\nCLIQUE-OR-INDEPENDENT-SET```\n. All you need to do is to add ```\nn```\n isolated vertices to graph ```\nG```\n (which is an instance of ```\nINDEPENDENT-SET```\n and has ```\nn```\n vertices). Let call this newly created graph ```\nG'```\n (instance of ```\nCLIQUE-OR-INDEPENDENT-SET```\n). Then it is not hard to prove that ```\nG```\n has ```\nk```\n independent-set iff ```\nG'```\n has ```\nn+k```\n independent-set of clique (since, by construction, it cannot have ```\nn+k```\nclique).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP: task_reduction=reduction? What is 'in_reduction'?\r\n                \r\nIs 'task_reduction' same as 'reduction ([ reduction-modifier ,] reduction\nidentifier : list )' with task reduction-modifier?\nIf it is same, then why do we need 'task_reduction'?\nWhat is 'in_reduction' doing?\nIn the text says 'The in_reduction clause specifies that a task participates in a reduction '\nBut what does that mean? In 'in_reduction' we need same clause as reduction.\n```\nin_reduction(identifier : list)\n```\n\nBut if we can put reduction variables in 'list', then what does that do with 'task participates in reduction'...?\nI can imagine how reduction works but I can't imagine with 'in_reduction'.\nWhy do we need that?\n======================================\nI made an example. This code should give sum of num at even index number\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\nint main(int argc, char* argv[]){\n\n        int Array [10]= {0,1,2,3,4,5,6,7,8,9};\n        int Array_length = 10;\n        int counter = 0;\n        int result = 0;\n\n        #pragma omp parallel\n        #pragma omp single \n\n        {\n        #pragma omp taskgroup task_reduction(+:result)\n        //#pragma omp parallel reduction(task,+:result) this can work same as taskgroup task_reduction?\n        {\n             while (Array_length!=counter){\n                  if (counter%2==0){\n                      #pragma omp task in_reduction(+:result)\n                      {\n                          result+=Array[counter];\n                      }\n                  }else {\n                          result+=Array[counter];\n                  }\n                  counter=counter+1;\n             }\n        }\n    }\n    printf(\"The sum of all array elements is equal to %d.\\n\", result);\n}\n```\n\nAnd I also made an illustration, so I could visualize my comprehension.\n\nSo… task_reduction create a reduction, so that local result can be contributed and only task with in_reduction will participate in contribution of Reduction?\nIf I understand correctly, then this code should give 20 as result. However, my code gives 45, which is sum of 0 to 9.\nWhere did I make a mistake?\nBy the way, what happens if I didn’t write ‘in_reduction’ at all? Then the result is 0?\n    ", "Answer": "\r\nThe way task reduction works is that the task needs to know where to contribute its local result to.  SO, what you have to do is to have an taskgroup that \"creates\" the reduction and then have tasks contribute to it:\n```\nvoid example() {\n    int result = 0;\n#pragma omp parallel   // create parallel team\n#pragma omp single     // have only one task creator\n    {\n        #pragma omp taskgroup task_reduction(+:result)\n        {\n            while(have_to_create_tasks()) {\n                #pragma omp task in_reduction(+:result)\n                {   // this tasks contribute to the reduction\n                    result = do_something();\n                }\n                #pragma omp task firstprivate(result)\n                {   // this task does not contribute to the reduction\n                    result = do_something_else();\n                }\n            }\n        }\n    }\n}\n```\n\nSo, the ```\nin_reduction```\n is needed for a task to contribute to a reduction that has been created by a ```\ntask_reduction```\n clause of the enclosing ```\ntaskgroup```\n region.\nThe ```\nreduction```\n clause cannot be used with the ```\ntask```\n construct, but only worksharing constructs and other loop constructs.\nThe only tasking construct that has the ```\nreduction```\n clause, is the ```\ntaskloop```\n construct that uses it for a short cut for a hidden ```\ntask_reduction```\n construct that encloses all the loop constructs that it creates and that then have a hidden ```\nin_reduction```\n clause, too.\nUPDATE (to cover the edits by the original poster):\nThe problem with the code is now that you have two things happening (see the inline comments in your updated code):\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\nint main(int argc, char* argv[]){\n    int Array [10]= {0,1,2,3,4,5,6,7,8,9};\n    int Array_length = 10;\n    int counter = 0;\n    int result = 0;\n\n    #pragma omp parallel\n    #pragma omp single\n    {\n        #pragma omp taskgroup task_reduction(+:result)\n        {\n            // \"result\" is a shared variable in this parallel region\n            while (Array_length!=counter) {\n                if (counter%2==0){\n                    #pragma omp task in_reduction(+:result)\n                    {\n                        // This task will contribute to the reduction result\n                        // as you would expect.\n                        result+=Array[counter];\n                    }\n                } else {\n                    // This addition to \"result\" is performed by the \"single\"\n                    // thread and thus hits the shared variable.  You can see\n                    // this when you print the address of \"result\" here\n                    // and before the parallel region.\n                    result+=Array[counter];\n                }\n                counter=counter+1;\n            }\n        } // Here the \"single\" thread waits for the taskgroup to complete\n          // and the reduction to happen.  So, here the shared variable\n          // \"result\" is added to the value of \"result\" coming from the\n          // task reduction.  So, result = 25 from the \"single\" thread and\n          // result = 20 are added up to result =45\n    }\n    printf(\"The sum of all array elements is equal to %d.\\n\", result);\n}\n```\n\nThe addition at the end of the task reduction seems to be a race condition as the updates coming from the ```\nsingle```\n thread and the updates coming from the end of the taskgroup are not synchronized.  I guess that the race does not show up, as the code is too fast to clearly expose it.\nTo fix the code, you'd have to also have a ```\ntask```\n construct around the update for odd numbers, like so:\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <omp.h>\nint main(int argc, char* argv[]){\n    int Array [10]= {0,1,2,3,4,5,6,7,8,9};\n    int Array_length = 10;\n    int counter = 0;\n    int result = 0;\n\n    #pragma omp parallel\n    #pragma omp single\n    {\n        #pragma omp taskgroup task_reduction(+:result)\n        {\n            // \"result\" is a shared variable in this parallel region\n            while (Array_length!=counter) {\n                if (counter%2==0){\n                    #pragma omp task in_reduction(+:result)\n                    {\n                        // This task will contribute to the reduction result\n                        // as you would expect.\n                        result+=Array[counter];\n                    }\n                } else {\n                    #pragma omp task firstprivate(result)\n                    {\n                        // \"result\" is now a task-local variable that is not\n                        // shared.  If you remove the firstprivate, then the\n                        // race condition on the shared variable \"result\" is\n                        // back.\n                        result+=Array[counter];\n                    }\n                }\n                counter=counter+1;\n            }\n        } // Here the \"single\" thread waits for the taskgroup to complete\n          // and the reduction to happen.  So, here the shared variable\n          // \"result\" is added to the value of \"result\" coming from the\n          // task reduction.  So, result = 25 from the \"single\" thread and\n          // result = 20 are added up to result =45\n    }\n    printf(\"The sum of all array elements is equal to %d.\\n\", result);\n}\n```\n\nIn my first answer, I failed to add a proper ```\nfirstprivate```\n or ```\nprivate```\n clause to the task.  I'm sorry about that.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Floating point range reduction\r\n                \r\nI'm implementing some 32-bit float trigonometry in C# using Mono, hopefully utilizing Mono.Simd. I'm only missing solid range reduction currently.\nI'm rather stuck now, because apparently Mono's SIMD extensions does not include conversions between floats and integers, meaning I have no access to rounding/truncation which would be the usual method. I can however convert bitwise between ints and floats.\nCan something like this be done? I can scale the domain up and down if needed, but ideally the range reduction should result in a domain of [0, 2 pi] or [-pi, pi]. I have a hunch that it would be possible to do some IEEE magic with the exponent, if the domain is a power of 2, but I'm really not sure how to.\nEdit:\nOkay, I've tried messing around with this C code and it feels like I'm on the verge of something (it doesn't work but the fractional part is always correct, in decimal / base10 at least...). The core principle seems to be getting the exponent difference between your domain and the input exponent, and composing a new float with a shifted mantissa and an adjusted exponent.. But it won't work for negatives, and I have no idea how to handle non-powers of 2 (or anything fractional - in fact, anything else than 2 doesn't work!).\n```\n// here's another more correct attempt:\nfloat fmodulus(float val, int domain)\n{\n    const int mantissaMask = 0x7FFFFF;\n    const int exponentMask = 0x7F800000;\n\n    int ival = *(int*)&val;\n\n    int mantissa = ival & mantissaMask;\n    int rawExponent = ival & exponentMask;\n    int exponent = (rawExponent >> 23) - (129 - domain);\n    // powers over one:\n    int p = exponent;\n\n    mantissa <<= p;\n    rawExponent = exponent >> p;\n    rawExponent += 127;\n    rawExponent <<= 23;\n\n    int newVal = rawExponent & exponentMask;\n    newVal |= mantissa & mantissaMask;\n\n    float ret = *(float*)&newVal;\n    \n    return ret;\n}\n\nfloat range_reduce(float value, int range )\n{\n    const int mantissaMask = 0x7FFFFF;\n    const int exponentMask = 0x7F800000;\n\n    int ival = *(int*)&value;\n    // grab exponent:\n    unsigned exponent = (ival & exponentMask) >> 23;\n    // grab mantissa:\n    unsigned mantissa = ival & mantissaMask;\n\n    // remove bias, and see how much the exponent is over range/domain\n    unsigned char erange = (unsigned char)(exponent - (125 + range));\n    // check if sign bit is set - that is, the exponent is under our range\n    if (erange & 0x80)\n    {\n        // don't do anything then.\n        erange = 0;\n    }\n\n    // shift mantissa (and chop off bits) by the reduced amount\n    int inewVal = (mantissa << (erange)) & mantissaMask;\n    // add exponent, and subtract the amount we reduced the argument with\n    inewVal |= ((exponent - erange) << 23) & exponentMask;\n\n    // reinterpret\n    float newValue = *(float*)&inewVal;\n    return newValue;\n    //return newValue - ((erange) & 0x1 ? 1.0f : 0.0f);\n}\n\nint main()\n{\n    float val = 2.687f;\n    int ival = *(int*)&val;\n    float correct = fmod(val, 2);\n    float own = range_reduce(val, 2);\n\n    getc(stdin);\n}\n```\n\nEdit 2:\nOkay, I'm really trying to understand this in terms of the ieee binary system. If we write the modulus operation like this:\n```\noutput = input % 2\n\n[exponent] + [mantissa_bit_n_times_exponent]\n\n3.5     = [2] + [1 + 0.5]                   ->[1] + [0.5]       = 1.5\n4.5     = [4] + [0 + 0 + 0.5]               ->[0.5] + [0]       = 0.5\n5.5     = [4] + [0 + 1 + 0.5]               ->[1] + [0.5]       = 1.5\n2.5     = [2] + [0 + 0.5]                   ->[0.5] + [0]       = 0.5\n2.25    = [2] + [0 + 0 + 0.25]              ->[0.25]            = 0.25\n2.375   = [2] + [0 + 0 + 0.25 + 0.125]      ->[0.25] + [0.125]  = 0.375\n13.5    = [8] + [4 + 0 + 1 + 0.5]           ->[1] + [0.5]       = 1.5\n56.5    = [32] + [16 + 8 + 0 + 0 + 0 + 0.5] ->[0.5]             = 0.5\n```\n\nWe can see the output in all cases is a new number, with no original exponent and the mantissa shifted an amount (that is based on the exponent and the first non-zero bits of the mantissa after the first exponent-bits of the mantissa is ignored) into the exponent. But I'm not really sure if this is the correct approach, it just works out nicely on paper.\nEdit3:\nI'm stuck on Mono version 2.0.50727.1433\n    ", "Answer": "\r\nYou can reduce the problem to taking a float mod 1. To simplify that, you can compute the floor of the float using bit operations, then use a floating point subtraction. The following is (unsafe) C# code for these operations:\n\n```\n// domain is assumed to be positive\n// returns value in [0,domain)\npublic float fmodulus(float val, float domain)\n{\n    if (val < 0)\n    {\n        float negative = fmodulus(-val, domain);\n        if (domain - negative == domain)\n            return 0;\n        else\n            return domain-negative;\n    }\n\n    if (val < domain)\n        return val; // this avoids losing accuracy\n\n    return fmodOne(val / domain) * domain;\n}\n\n// assumes val >= 1, so val is positive and the exponent is at least 0 \nunsafe public float fmodOne(float val)\n{\n    int iVal = *(int*)&val;\n    int uncenteredExponent = iVal >> 23;\n    int exponent = uncenteredExponent - 127; // 127 corresponds to 2^0 times the mantissa\n    if (exponent >= 23) \n        return 0; // not enough precision to distinguish val from an integer\n\n    int unneededBits = 23 - exponent; // between 0 and 23\n    int iFloorVal = (iVal >> unneededBits) << unneededBits; // equivalent to using a mask to zero the bottom bits of the mantissa\n    float floorVal = *(float*)&iFloorVal; // convert the bit pattern back to a float\n\n    return val-floorVal;\n}\n```\n\n\nFor example, fmodulus(100.1f, 1) is 0.09999847. The bit pattern of 100.1f is \n\n0 10000101 10010000011001100110011\n\nThe bit pattern of floorVal (100f) is \n\n0 10000101 10010000000000000000000\n\nA floating point subtraction gives something close to 0.1f:\n\n0 01111011 10011001100110000000000\n\nActually, I was surprised that the last 8 bits were zeroed out. I thought only the last 6 bits of 0.1f were supposed to be replaced with 0. Perhaps one can do better than relying on the floating point subtraction. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda calculus reduction of functions\r\n                \r\nI'm very new to lambda calculus and while I was reading a tutorial , came across with this.\nHere is my equation.\n\n```\nY = ƛf.( ƛx.f(xx)) ( ƛx.f(xx))\n```\n\n\nNow if we apply another term, let's say F (YF), then how can we reduce this.If I'm correct according to beta reduction , we can replace all the f in  ( ƛx.f(xx)) by ( ƛx.f(xx)), is this correct and if so how can we do that.   \n\nThanks\n    ", "Answer": "\r\nReuction steps:\n\n```\nY = ƛf.( ƛx.f(xx)) ( ƛx.f(xx)) = ƛf.( f ( ƛx.f(xx) ƛx.f(xx) ) ) \n  = ƛf.( f ( f (ƛx.f(xx) ƛx.f(xx)))) \n  = ƛf.( f ( f ( f (ƛx.f(xx) ƛx.f(xx)))) \n  = ƛf.( f ( f ( f ( f (ƛx.f(xx) ƛx.f(xx))))) = ...\n```\n\n\nSo this Lambda term goes into an infinite loop...\n\nExplanation:\nLet's look on the term ```\n( ƛx.f(xx) ƛx.f(xx) )```\n we substitute ```\nƛx.f(xx)```\n with ```\nf'```\n which means ```\n(f' f')```\n => activating the term ```\nf'```\n on itself.\nIt might be easier to look at like this:\n```\n( ƛy.f(yy) ƛx.f(xx) )```\n now when you activate the ```\nƛy.f(yy)```\n and provide the input (which substitutes ```\ny```\n with ```\nƛx.f(xx)```\n ) the outcome is: ```\nf(ƛx.f(xx) ƛx.f(xx))```\n which in turn, can go over the same process again and again and the lambda-expression will only expend...\n\n\nRemark:\nIt's wrong to write:\n```\nY = ƛf.( ƛx.f(xx)) ( ƛx.f(xx))```\n\nit should actually be:\n```\nY = ƛf.(ƛx.f(xx) ƛx.f(xx))```\n\nThe difference between ```\nƛx.f(xx)```\n and ```\n(ƛx.f(xx))```\n is that the latter is an activation of ```\nƛx.f(xx)```\n - it's meaningless to activate it like this ```\n(ƛx.f(xx))```\n since we need an ```\nx```\n (input) to activate it on.\nFinally:\n```\nY = ƛf.( ƛx.f(xx)) ( ƛx.f(xx)) = ƛf.( f ( ƛx.f(xx) ƛx.f(xx) ) )```\n \nmeaning:\n```\nYF = ( ƛx.F(xx)) ( ƛx.F(xx)) = F(ƛx.F(xx)) ( ƛx.F(xx)) = F(YF)```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Vectorized transition matrix reduction\r\n                \r\nI'm trying to find a wizard's vectorization for the following iterative computation (please look at the later edit):\n\n\n```\n% A is a logical matrix of size NxN\nB = false(size(A));\nfor k1 = 1:N, for k2 = 1:N, for k3 = 1:N\n        B(k1,k2) = A(k1,k2) && ~A(k1,k3) && ~A(k3,k2);\nend; end; end;\n```\n\n\n\nI must stress out that solutions using ```\narrayfun```\n (or ```\nstructfun```\n or ```\ncellfun```\n) are not feasible since they are slow, and I'm looking for performance improvement, not expressiveness enhancement. Also, I'd like to avoid the obvious:\n\n```\nB = A & logical((1-A)^2)\n```\n\n\nbecause the memory footprint for computing this is 17 times the original's (and I work with big matrices in an eventually fragmented memory resource).\n\nA positive answer (i.e. a solution) or a negative one (i.e. an explanation why this cannot work) are both greatly appreciated.\n\nLater edit\n\nThanks to H.Muster I became aware of a bug in my initial code. The iteration to be vectorized is actually:\n\n```\n% A is a logical matrix of size NxN\nB = A;\nfor k1 = 1:N, for k2 = 1:N, for k3 = 1:N\n        B(k1,k2) = B(k1,k2) && ~(A(k1,k3) && A(k3,k2));\nend; end; end;\n```\n\n\nA faster iteration is welcome also (I'm studying this right now, if I find something I will post as comment/edit).\n\nEven later edit\n\nFor those who are interested in the purpose of the code, it's supposed to compute the transitive reduction ```\nB```\n of a relationship graph ```\nA```\n. ```\nA(k1,k2)=true```\n means that ```\nk1```\n \"relates to\" ```\nk2```\n (the reciprocal is not true). ```\nB(k1,k2)=true```\nmeans that ```\nk1```\n \"relates to\" ```\nk2```\n and there is no other element ```\nk3```\n \"between\" them, i.e. ```\nk2```\n is the \"next\" after ```\nk1```\n. One must note that, if defined like this, an element may benefit of several \"next\" elements, not only one. The transitive reduction helps creating \"non-deterministic iterators\" (next is a set, not a single element) into set structures \"induced\" by a non-symmetric dyadic relation.\n    ", "Answer": "\r\nA small vectorization in the inner loop would be:\n\n```\nfor k1 = 1:N, for k2 = 1:N\n    B(k1,k2) = B(k1,k2) && ~all( A(k1,:) & A(:,k2)' );\nend; end;\n```\n\n\nI'm not sure if there's a good way vectorize the outer loops.\n\nEdit\n\nActually it's easy to vectorize both inner loops:\n\n```\nfor k1 = 1:N\n    B(k1,:) = ~all( bsxfun(@and, A(k1,:), A(:,:)' ) );\nend;\nB = A & B;\n```\n\n\nI am pretty sure that vectorizing everything would have to involve either matrix multiplication or a 3-d matrix which would take up a lot more space (assuming N is large).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Explaining the Church-Rosser Theorem in basic terms\r\n                \r\nI would like to know how the Church-Rosser theorem is used within programming, specifically functional programming. I've looked up information but can only find sources referring to lambda calculus (limited knowledge) and beta-reductions. \n\nIf anyone could explain where lambda calculus comes into this and what reductions are, I think that would clear things up.\n\nMy initial thoughts of the Church-Rosser theorem is that it's to do with order of evalutation and execution of functions but I'm not entirely sure whether this is accurate information.\n\nThanks.\n\nNote: I'm currently studying Standard ML \n    ", "Answer": "\r\nYour initial thoughts are fine.\n\n\nThe lambda calculus is the formal foundation on which functional programming is built.\nThe lambda calculus is a term rewriting system, and a reduction means following a rewrite rule.\nThe lambda calculus does not dictate a specific order of evaluation, so given a lambda expression where multiple reductions are possible, the Church-Rosser theorem says that you can pick either one. This gives functional programming language designers quite a lot of freedom to design their evaluation semantics. For example, in f (g x), assuming both are pure functions, whether you reduce f or g first are equivalent.\n\nWikipedia's description of the Church-Rosser theorem is:\n\n\n  [I]f there are two distinct reductions or sequences of reductions that can be applied to the same term, then there exists a term that is reachable from both results, by applying (possibly empty) sequences of additional reductions.\n\n\nThe example f (g x) where f is x² and g is 2x:\n\n```\n(λx.x*x) ((λy.y+y) 2)  ~β~>  ((λy.y+y) 2)*((λy.y+y) 2)\n                       ~β~>  (2+2)*((λy.y+y) 2)\n                       ~β~>  (2+2)*(2+2)\n\n(λx.x*x) ((λy.y+y) 2)  ~β~>  (λx.x*x) (2+2)\n                       ~β~>  (2+2)*(2+2)\n```\n\n\nIn this example, the two distinct reductions are β-reduction on either of the lambdas, and one term among others that is reachable from both reductions is ```\n(2+2)*(2+2)```\n.\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction issue with Cython\r\n                \r\nI am facing an unexpected behavior of Cython (v. 0.29.13) when compiling OpenMP parallel code that is expect to perform a reduction:\n\n```\nimport cython\nfrom cython.parallel import prange, parallel\n\ncpdef omp_test1(n):\n    cdef int sum = 0, i, imax\n    imax = <int>n\n    for i in prange(imax, nogil=True):\n        sum += 1\n    return sum\n\n\ncpdef omp_test2(n):\n    cdef int sum = 0, i, imax\n    imax = <int>n\n    with nogil, parallel():\n        for i in prange(imax):\n            sum += 1\n    return sum\n```\n\n\nWhen calling both functions I would expect to have as a return value the input argument ```\nn```\n. Instead, the returned value is ```\nn * num_threads```\n. Surprisingly, on my Mac this unexpected behavior is observed only with GCC (v. 9.2.0_2); the clang (v. 11.0.0) returns the expected value ```\nn```\n.\n\nIs there something wrong I am doing or is there a problem with the generated Cython code or GCC compiler?\n    ", "Answer": "\r\nYour code is fine. I've also run it successfully, as DavidW has.\n\nI am testing OpenMP parallelised Cython code in an automatic way on multiple operating systems and with different Python versions. Once in a while, we have problems on MacOS, but with clang.\n\nThis won't solve your problem, but as a hint, you should look at the compilers or your OpenMP version.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction with template type\r\n                \r\n```\ntemplate <typename T, std::size_t N>\nstatic T sum(const std::array<T, N>& a)\n{\n    T result;\n\n    // type of result (T) is not determined when pre-process?\n    #pragma omp parallel for reduction(+: result)\n    for(int i = 0; i < static_cast<int>(N); i++)\n    {\n        result += a[i];\n    }\n    return result;\n}\n```\n\n\nI can compile and run above code with MSVC and gcc. Yes, it's excellent!\n\nBut my question is in the code comment; \"Because the type of result (T) is not determined while pre-processing '#pragma', how does the compiler validate that the type of result is suited to OpenMP reduction?\".\n\nI'm sure it's OK if T=double and NG if T=std::string, but how does the pre-processor know the type of T?\n\nI remember I couldn't compile the above code with some minor c++ compiler a long time ago.\n\nLet me ask which behavior (compilable or uncompilable) is correct in the context of C++/OpenMP specifications.\n    ", "Answer": "\r\nIt's unspecified (for OpenMP 3.0 or later) or undefined (for OpenMP 2.5)\n\n```\nreduction```\n is one of data-sharing attribute clauses, and OpenMP Application Program Interface Version 2.5 says:\n\n\n  2.8.3 Data-Sharing Attribute Clauses\n  ---- C/C++ ----\n  If a variable referenced in a data-sharing attribute clause has a type derived from a template, and there are no other references to that variable in the program, then any behavior related to that variable is undefined.\n  ---- C/C++ ----\n\n\nOpenMP Application\nProgram Interface Version 3.0 says:\n\n\n  2.9.3 Data-Sharing Attribute Clauses\n  ---- C/C++ ----\n  If a variable referenced in a data-sharing attribute clause has a type derived from a template, and there are no other references to that variable in the program, then any behavior related to that variable is unspecified.\n  ---- C/C++ ----\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA: In warp reduction and volatile keyword\r\n                \r\nAfter reading the question and its answer from the following\nLINK\n\nI still have a question remaining in my mind. From my background in C/C++; I understand that using ```\nvolatile```\n has it's demerits. And also it is pointed in the answers that in case of CUDA, the optimizations can replace shared array with registers to keep data if ```\nvolatile```\n keyword is not used.\n\nI want to know what would be the performance issues that can be encountered when calculating (sum) reduction. e.g.\n\n```\n__device__ void sum(volatile int *s_data, int tid)\n{\n    if (tid < 16)\n    {\n        s_data[tid] += s_data[tid + 16];\n        s_data[tid] += s_data[tid +  8];\n        s_data[tid] += s_data[tid +  4];\n        s_data[tid] += s_data[tid +  2];\n        s_data[tid] += s_data[tid +  1];\n    }\n}\n```\n\n\nI am using in warp reduction. Since all the threads with in warp are in sync, therefore I believe there is no need to use ```\nsyncthreads()```\n construct. \n\nI want to know will removing the keyword ```\nvolatile```\n mess up my sum (due to cuda optimizations)? Can I use reduction such as this without ```\nvolatile```\n keyword.\n\nSince I use this reduction function multiple time, will ```\nvolatile```\n keyword cause any performance degradation?\n    ", "Answer": "\r\nRemoving the volatile keyword from that code could break that code on Fermi and Kepler GPUS. Those GPUs lack instructions to directly operate on shared memory. Instead, the compiler must emit a load/store pair to and from register. \n\nWhat the volatile keyword does in this context is make the compiler honour that load-operate-store cycle and not perform an optimisation that would keep the value of ```\ns_data[tid]```\n in register. To keep the sum accumulating in register would break the implicit memory syncronisation required to make that warp level shared memory summation work correctly. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "learning prefix sum by tree reduction\r\n                \r\nI need to learn about prefix sum by tree reduction and write an MPI code in C for that. I already know prefix sum by recursive doubling or scan, and have some background in programming by MPI. Here is the structure of tree reduction which I should learn about:\n\n\n\nAnyone can suggest some nice materials for learning prefix sum by tree reduction or explain it here? I googled it but couldn't find a good note with clear explanation!\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP: nowait and reduction clauses on the same pragma\r\n                \r\nI am studying OpenMP, and came across the following example:\n\n```\n#pragma omp parallel shared(n,a,b,c,d,sum) private(i)\n{\n    #pragma omp for nowait\n    for (i=0; i<n; i++)\n        a[i] += b[i];\n\n    #pragma omp for nowait\n    for (i=0; i<n; i++)\n        c[i] += d[i];\n    #pragma omp barrier\n\n    #pragma omp for nowait reduction(+:sum)\n    for (i=0; i<n; i++)\n        sum += a[i] + c[i];\n} /*-- End of parallel region --*/\n```\n\n\nIn the last for loop, there is a nowait and a reduction clause. Is this correct? Doesn't the reduction clause need to be syncronized?\n    ", "Answer": "\r\nThe ```\nnowait```\ns in the second and last loop are somewhat redundant. The OpenMP spec mentions ```\nnowait```\n before the end of the region so perhaps this can stay in.\n\nBut the ```\nnowait```\n before the second loop and the explicit barrier after it  cancel each other out.\n\nLastly, about the ```\nshared```\n and ```\nprivate```\n clauses. In your code, ```\nshared```\n has no effect, and ```\nprivate```\n simply shouldn’t be used at all: If you need a thread-private variable, just declare it inside the parallel region. In particular, you should declare loop variables inside the loop, not before.\n\nTo make ```\nshared```\n useful, you need to tell OpenMP that it shouldn’t share anything by default. You should do this to avoid bugs due to accidentally shared variables. This is done by specifying ```\ndefault(none)```\n. This leaves us with:\n\n```\n#pragma omp parallel default(none) shared(n, a, b, c, d, sum)\n{\n    #pragma omp for nowait\n    for (int i = 0; i < n; ++i)\n        a[i] += b[i];\n\n    #pragma omp for\n    for (int i = 0; i < n; ++i)\n        c[i] += d[i];\n\n    #pragma omp for nowait reduction(+:sum)\n    for (int i = 0; i < n; ++i)\n        sum += a[i] + c[i];\n} // End of parallel region\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Array reduction with OpenMP leads to \"user defined reduction not found for\"\r\n                \r\nI'm doing a scholar work and I have to obtain the histogram from a IMAGE.\nAll is going well, but when I tried to make the code parallel with the OpenMP, the compiler returns me this error: ```\nuser defined reduction not found for 'histog'```\n\nThe code that I used is this:\n```\nvoid HistogramaParaleloRed(int *histog)\n{\n\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < NG; i++)\n        {\n            histog[i] = 0;\n        }\n\n        #pragma omp for reduction(+ : histog)\n        for (int i = 0; i < N; i++)\n        {\n            for (int j = 0; j < N; j++)\n            {\n                histog[IMAGEN[i][j]]++;\n            }\n        }\n    }\n}\n```\n\nAnd the call to the function in Main is: ```\nHistogramaParaleloRed(histog_pal_red); ```\n\n    ", "Answer": "\r\nThe error\n```\nuser defined reduction not found for\n```\n\ncan happen because either the code was compiled with a compiler that does not support the OpenMP 4.5 array reduction feature (or that compiler is misconfigured) or because your are trying the reduce a naked pointer (like it is the case of your example). In the latter, the compiler cannot tell how many elements are to be reduce.\nSo either you use a compiler that supports ```\nOpenMP 5.0```\n and take advantage of array sections feature as follows:\n```\nvoid HistogramaParaleloRed(int *histog)\n{\n\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < NG; i++)\n        {\n            histog[i] = 0;\n        }\n\n        #pragma omp for reduction(+ : histog[:N])\n        for (int i = 0; i < N; i++)\n        {\n            for (int j = 0; j < N; j++)\n            {\n                histog[IMAGEN[i][j]]++;\n            }\n        }\n    }\n}\n```\n\nor alternatively, implement the reduction yourself.\nImplement the Reduction manually\nOne approach is to create a shared structure among threads (i.e., thread_histog), then each thread updates its position, and afterward, threads reduce the values of the shared structure into the original histog array.\n```\nvoid HistogramaParaleloRed(int *histog, int number_threads)\n{\n    int thread_histog[number_threads][NG] = {{0}};\n    #pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        #pragma omp for \n        for (int i = 0; i < N; i++)\n          for (int j = 0; j < N; j++)\n                thread_histog[thread_id][IMAGEN[i][j]]++;\n\n       #pragma omp for no_wait\n       for (int i = 0; i < NG; i++)\n           for(int j = 0; j < number_threads; j++)\n              histog[i] += thread_histog[j][i]\n    }\n}\n```\n\nAnother approach is to create an array of locks, one for each element of the ```\nhistog```\n array. Whenever a thread updates a given ```\nhistog```\n position, first acquires the lock corresponded to that position so that no other thread will be updating concurrently the same array position.\n```\nvoid HistogramaParaleloRed(int *histog)\n{\n    omp_lock_t locks[NG];\n    #pragma omp parallel\n    {\n       #pragma omp for\n       for (int i = 0; i < NG; i++)\n            omp_init_lock(&locks[i]);\n\n        int thread_id = omp_get_thread_num();\n        #pragma omp for \n        for (int i = 0; i < N; i++)\n          for (int j = 0; j < N; j++){\n              int pos = IMAGEN[i][j]\n              omp_set_lock(&locks[pos]);\n              thread_histog[thread_id][pos]++; \n              omp_unset_lock(&locks[pos]);\n          }\n\n       #pragma omp for no_wait\n       for (int i = 0; i < NG; i++)\n            omp_destroy_lock(&locks[i]);\n    }\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA: reduction or atomic operations?\r\n                \r\nI'm writing a CUDA kernel which involves calculating the maximum value on a given matrix and I'm evaluating possibilities. The best way I could find is:\n\nForcing every thread to store a value in the shared memory and using a reduction algorithm after that to determine the maximum (pro: minimum divergence cons: shared memory is limited to 48Kb on 2.0 devices)\n\nI couldn't use atomic operations because there are both a reading and a writing operation, so threads could not be synchronized by synchthreads.\n\nAny other idea come into your mind?\n    ", "Answer": "\r\nYou may also want to use the reduction routines that comes w/ CUDA Thrust which is a part of CUDA 4.0 or available here. \n\nThe library is written by a pair of nVidia engineers and compares favorably with heavily hand optimized code. I believe there is also some auto-tuning of grid/block size going on.\n\nYou can interface with your own kernel easily by wrapping your raw device pointers.\n\nThis is strictly from a rapid integration point of view. For the theory, see tkerwin's answer.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "reduction of eigen based templates\r\n                \r\nI am trying to do a reduction based on eigen matrix.\n\n```\n#include <iostream>\n#include <Eigen/Dense>\n#include <type_traits>\n\ntemplate<typename T1, typename T2, int n1, int n2>\nauto reduction(Eigen::Matrix<T1, n1, n2> &a1,\n               Eigen::Matrix<T2, n1, n2> &a2)\n     -> decltype(T1{}*T2{})\n{\n  using BaseT3 = \n    typename std::remove_cv<typename std::remove_reference<decltype(T1{}*T2{})>::type>::type;\n\n  BaseT3 res = a1(0, 0)*a2(0, 0);\n\n  for (int i=0; i<n1; ++i)\n    for (int j=0; j<n2; ++j)\n      if (i+j)\n        res = res + a1(i, j)*a2(i, j);\n\n  return res;\n}\n\nint main()\n{\n  Eigen::Matrix<double, 3, 3> m;\n  Eigen::Matrix<Eigen::Vector3d, 3, 3> n;\n\n  std::cout << reduction(m, n) << std::endl;\n}\n```\n\n\nBasically, Im a trying to get ```\nsum_{i, j} a1[i, j] * a2[i, j]```\n where ```\na1```\n and ```\na2```\n are some eigen mathix but I get compilation errors. The error I get is\n\n```\nerror: no match for ‘operator=’ (operand types are ‘BaseT3 {aka \nEigen::CwiseUnaryOp<Eigen::internal::scalar_multiple_op<double>, \nconst Eigen::Matrix<double, 3, 1> >}’ \nand \n‘const Eigen::CwiseBinaryOp<Eigen::internal::scalar_sum_op<double>, \nconst Eigen::CwiseUnaryOp<Eigen::internal::scalar_multiple_op<double>, \nconst Eigen::Matrix<double, 3, 1> >, \nconst Eigen::CwiseUnaryOp<Eigen::internal::scalar_multiple_op<double>, \nconst Eigen::Matrix<double, 3, 1> > >’)\n         res = res + a1(i, j)*a2(i, j);\n             ^\n```\n\n\nIf I am not mistaken, for the given ```\nmain```\n, type ```\nBaseT3```\n should have been ```\nEigen::Vector3d```\n. I also tried to static cast so the ```\noperator=```\n should not fail but I then get other errors.\n\nThis is c++11, I use Eigen3 and the compiler is g++ 5.4.1.\n    ", "Answer": "\r\nThe decltype of T1 * T2 isn't what you expect here - Eigen heavily uses expression templates.  The CWiseUnaryOp and CWiseBinaryOp types in your error are indicative of that.  In other words, the result of \"double * Vector3d\" isn't what you'd expect (it's not a Vector3d, it's a cwisebinaryop).\n\nSee also: Writing functions taking Eigen Types.\n\nIn this specific case you may find a solution by creating partial specializations for Eigen base types for both the first and second parameters of your template function.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Loop dependencis when parallelizing loops with OpenMP\r\n                \r\nI tried parallelizing the following loop:\n```\n    #pragma omp parallel for shared(v, g, p)\n    for (i=1; i<=imax-1; i++) {\n        for (j=1; j<=jmax-1; j++) {  // combined loops\n            /* only if both adjacent cells are fluid cells */\n            if ((flag[i][j] & C_F) && (flag[i][j+1] & C_F)) {\n                v[i][j] = g[i][j]-(p[i][j+1]-p[i][j])*del_t/dely;\n            }\n            if ((flag[i][j] & C_F) && (flag[i+1][j] & C_F)) {\n                u[i][j] = f[i][j]-(p[i+1][j]-p[i][j])*del_t/delx;\n            }\n        }\n    }\n```\n\nBut my program does not run as expected, probably because of loop dependencies. Is there a way to parallelize this loop with reductions?\nAny help will be greatly appreciated.\n    ", "Answer": "\r\n\nBut my program does not run as expected, probably because of loop\ndependencies. Is there a way to parallelize this loop with reductions?\n\nThere is a race-condition in one of the shared variables (i.e., the loop iterator ```\nj```\n) being updated concurrently by multiple threads.\n```\n   #pragma omp parallel for shared(v, g, p)\n   for (i=1; i<=imax-1; i++) {\n        for (j=1; j<=jmax-1; j++) {  // <---- Race-condition \n            /* only if both adjacent cells are fluid cells */\n            if ((flag[i][j] & C_F) && (flag[i][j+1] & C_F)) {\n                v[i][j] = g[i][j]-(p[i][j+1]-p[i][j])*del_t/dely;\n            }\n            if ((flag[i][j] & C_F) && (flag[i+1][j] & C_F)) {\n                u[i][j] = f[i][j]-(p[i+1][j]-p[i][j])*del_t/delx;\n            }\n        }\n    }\n```\n\nThe variable ```\ni```\n of the outermost loop does not have a race-condition because it belongs to the loop being parallelized, and in that case, the OpenMP standard states that that variable will be implicitly private.\nTo solve the race-condition you need to make the variable ```\nj```\n of the innermost loop private. Either by doing:\n```\n   #pragma omp parallel for shared(v, g, p) private(j)\n   for (i=1; i<=imax-1; i++) {\n        for (j=1; j<=jmax-1; j++) {\n```\n\nor (depending in your compiler version):\n```\n   #pragma omp parallel for shared(v, g, p)\n   for (i=1; i<=imax-1; i++) {\n        for (int j=1; j<=jmax-1; j++) {\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Data reduction via IMPORTRANGE\r\n                \r\nI am trying to do some data reduction in my Google Sheets by using the following IMPORTRANGE formula:\n\n```\n=query(importrange(\"https://docs.google.com/a/ap.averydennison.com/spreadsheets/d/1xz1lXY-w5Ii_aWqVAhHgRCmeoes9ltSUtibE4kzhMHA/edit#gid=2051232966\",\"SF_Flex_Rel!a:l\"), \n \"select * where Col1 = '\"&text(B1,\"###\")&\"'\",1)\n```\n\n\nThe 'source' sheet has a whole lot of sales data records. What I am trying to do in the new sheet via this formula is only bring in the sales records from the source sheet that match the customer number specified in cell B1.\n\nIt seems to work OK if I limit the IMPORTRANGE to only query about 10,000 rows. Once I go over around 20,000 rows the screen will briefly flash up the records, then a small progress bar shows in the top right corner of the sheet and the records disappear. The cell with the formula just shows ```\n#ERROR!```\n with no other comments to tell me why.\n\nIs there something wrong with my formula syntax?\n\nIs there a better way to achieve this data reduction?\n\nIs there some undocumented data limitation on IMPORTRANGE function (I am using 'new' Google Sheets)?\n    ", "Answer": "\r\ntry like my example :\n\n```\n  =QUERY(                                                // data\n              IMPORTRANGE(\n                \"Spreadsheet Key\",    // spreadsheet key \n                \"DATA!A:C\"                                         // datarange\n              ), \n              \"SELECT Col1 WHERE Col2=\" & \"'\" & B2 & \"'\"           // query\n            )\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Is Stream.toArray() a thread safe mutable reduction?\r\n                \r\nThe collect operation in Java 8 Stream API is defined as a mutable reduction that can be safely executed in parallel, even if the resulting ```\nCollection```\n is not thread safe.\n\nCan we say the same about the ```\nStream.toArray()```\n method? \n\nIs this method a mutable reduction that is thread safe even if the ```\nStream```\n is a parallel stream and the resulting array is not thread safe? \n    ", "Answer": "\r\nAccording to https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html#toArray-java.util.function.IntFunction- it should be since they create \n\n\n  ... any additional arrays that might be required for a partitioned execution or for resizing\n\n\nAnd in deduction, since ```\nStream.toArray()```\n is nothing but ```\nstream.toArray(Object[]::new)```\n it should hold for ```\nStream.toArray()```\n too.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Color reduction with ImageMagick\r\n                \r\nThere's an image I convert to grey in a first step using this:\n\n```\nconvert out.jpg -colorspace GRAY -normalize png:out.png\n```\n\n\nIn another step I would like to reduce colors to 12:\n\n```\nconvert out.png +dither -colors 12 -filter box -normalize png:out.png\n```\n\n\nThis works perfectly with a very old version of GraphicsMagick which I have installed on one machine. On another machine is the latest version of ImageMagick. Here the resulting image just has 8 colors.\n\nIs there a way I can force ImageMagick to make exactly 12 colors? Not more, not less?\n    ", "Answer": "\r\nUse the ```\n-posterize```\n switch\n\n```\nconvert colors.png -colorspace gray +dither -posterize 12 mono12.png```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction for Longest Common Subsequence (LCS)\r\n                \r\nI am working on a problem regarding LCS of two strings, and I was wondering if there is any reduction from the general case of LCS to its binary version, i.e., by solving LCS for bit-strings we can also solve LCS with an arbitrary (but finite) alphabet cardinal.\n\nIt seems reasonable for me that such a reduction exists (based on the complexity of algorithms for various versions of the problem), however, I couldn't find something like that.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Strategy for doing final reduction\r\n                \r\nI am trying to implement an OpenCL version for doing reduction of a array of float.\nTo achieve it, I took the following code snippet found on the web :\n```\n__kernel void sumGPU ( __global const double *input, \n                       __global double *partialSums,\n               __local double *localSums)\n {\n  uint local_id = get_local_id(0);\n  uint group_size = get_local_size(0);\n\n  // Copy from global memory to local memory\n  localSums[local_id] = input[get_global_id(0)];\n\n  // Loop for computing localSums\n  for (uint stride = group_size/2; stride>0; stride /=2)\n     {\n      // Waiting for each 2x2 addition into given workgroup\n      barrier(CLK_LOCAL_MEM_FENCE);\n\n      // Divide WorkGroup into 2 parts and add elements 2 by 2\n      // between local_id and local_id + stride\n      if (local_id < stride)\n        localSums[local_id] += localSums[local_id + stride];\n     }\n\n  // Write result into partialSums[nWorkGroups]\n  if (local_id == 0)\n    partialSums[get_group_id(0)] = localSums[0];\n }                  \n```\n\nThis kernel code works well but I would like to compute the final sum by adding all the partial sums of each work group.\nCurrently, I do this step of final sum by CPU with a simple loop and  iterations ```\nnWorkGroups```\n.\nI saw also another solution with atomic functions but it seems to be implemented for int, not for floats. I think that only CUDA provides atomic functions for float.\nI saw also that I could another kernel code which performs this operation of sum but I would like to avoid this solution in order to keep a simple readable source. Maybe I cannot do without this solution...\nI must tell you that I use OpenCL 1.2 (returned by ```\nclinfo```\n) on a Radeon HD 7970 Tahiti 3GB (I think that OpenCL 2.0 is not supported with my card).\nMore generally, I would like to get advice about the simplest method to perform this last final summation with my graphics card model and OpenCL 1.2.\n    ", "Answer": "\r\nIf that float's order of magnitude is smaller than ```\nexa```\n scale, then:\n\nInstead of\n\n```\nif (local_id == 0)\n  partialSums[get_group_id(0)] = localSums[0];\n```\n\n\nYou could use\n\n```\nif (local_id == 0)\n{\n    if(strategy==ATOMIC)\n    {\n        long integer_part=getIntegerPart(localSums[0]);\n        atom_add (&totalSumIntegerPart[0] ,integer_part);\n        long float_part=1000000*getFloatPart(localSums[0]);\n         // 1000000 for saving meaningful 7 digits as integer\n        atom_add (&totalSumFloatPart[0] ,float_part);\n    }\n}\n```\n\n\nthis will overflow float part so when you divide it by 1000000 in another kernel, it may have more than 1000000 value so you get its integer part and add it to the real integer part:\n\n```\n   float value=0;\n   if(strategy==ATOMIC)\n   {\n       float float_part=getFloatPart_(totalSumFloatPart[0]);\n       float integer_part=getIntegerPart_(totalSumFloatPart[0])\n       + totalSumIntegerPart[0];\n       value=integer_part+float_part;\n   }\n```\n\n\njust a few atomic operations shouldn't be effective on whole kernel time.\n\nSome of these ```\nget___part```\n can be written easily already using floor and similar functions. Some need a divide by 1M.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Dimensionality Reduction (Heterogenous Data)\r\n                \r\nI have few notions of machine learning and data science and I need to make a dimensional reduction of a data set, corresponding to the TV consumption of users. I have approximately 20 columns (features) and hundreds of thousands of samples.\nThe problem is that the features are of different kinds. For example, the region, the date, the type of device, the duration of consumption, etc.\nWhat algorithms could I implement in this particular case to reduce the number of features?\n    ", "Answer": "\r\nLook into feature selection algorithms, there are a ton of articles and public libraries that have implementations of these.  Support Vector Machine's (SVM) is one that is commonly used. Take a look at sklearn/tensorflow/etc. docs to see implementation details and pick which one is best for your problem.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Dimension Reduction with SVD in R\r\n                \r\nI am trying to use SVD in R for dimension Reduction of a Matrix. I am able to find D, U, V matrix for \"MovMat\" Matrix. I want to reduce some dimensions that their values in D matrix is less than a \"treshhold\".\nI wrote the code below. But I do not know how I can find values less than threshold in \"MovMat\" Matrix.\n\n```\nlibrary(cluster)\nlibrary(fpc)\n\n# \"MovMat\" is a users-movies Matrix. \n# It is contain the rating score which each user gives for each movie.\nsvdAllDimensions = svd(MovMat)\nd=diag(svd$d) # Finding D, U, V\nu=svd$u\nv=svd$v\n```\n\n    ", "Answer": "\r\nI assigned the values of D which is less than the Threshold and again multiply D, V, U with each other and find new matrix with less dimension.\n\n```\n  for(i in rowOfD){\n   for(j in columnOfD){\n     if (i==j){\n      if(d[i,j]<Threshold){\n       d[i,j] = 0\n      }   \n     }\n   }   \n }\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "red.shared instruction\r\n                \r\nI am learning CUDA and after looking through the PTX manual, \nI found that there is an instruction called red.shared which performs\nreduction across a warp. \n\nI am curious whether or not the hardware has native support for reductions.\nAnd if it does, how can one use it in CUDA code? Perhaps someone has experimented with that?\n    ", "Answer": "\r\nactually it happens to be so that I also experimented with 'red' instruction out of curiosity. I don't know how this is on Kepler but on Fermi architecture 'red' instruction just maps to a sequence of another instructions. Maybe they left it for future GPUs. Here is the code I played with:\n\n```\n#define WS 32\n#define HF 16\n\n__global__ void test_red_kernel(unsigned *g_R, const unsigned *g_U) {\n\n  extern __shared__ unsigned shared[];\n\n  unsigned thid = threadIdx.x, bidx_x = blockIdx.x;\n  unsigned *r = shared;\n  unsigned ofs = bidx_x << 7, thid_in_warp = thid & WS-1;\n\n  unsigned a = (g_U + ofs)[thid];\n\n  volatile unsigned *t = (volatile unsigned *)r + HF + UMUL(thid >> 5,\n        WS + HF + 1) + thid_in_warp;\n\n  t[-HF] = 0;\nt[0] = a;\n// warp reduction\na = a + t[-HF], t[0] = a;\na = a + t[-8], t[0] = a;\na = a + t[-4], t[0] = a;\na = a + t[-2], t[0] = a;\na = a + t[-1], t[0] = a;\n\nCU_SYNC\n\nvolatile unsigned *t2 = r + HF + UMUL(WS*4 >> 5, WS + HF + 1);\n\nif(thid < 4) {\n\n    unsigned loc_ofs = HF + WS-1 + UMUL(thid, WS + HF + 1);\n    unsigned a2;\n\n    volatile unsigned *ps = t2 + thid;\n    ps[-2] = 0;\n\n    a2 = r[loc_ofs]; ps[0] = a2;\n    a2 = a2 + ps[-2], ps[0] = a2;\n    a2 = a2 + ps[-1], ps[0] = a2;\n}\n\nCU_SYNC\n\na = a + t2[(thid >> 5) - 1];\n\nunsigned b;      \nasm volatile(\"mov.u32 %r11, shared;\" : );\nasm volatile(\"red.shared.add.u32 [%r11], %0;\" :\n            \"+r\"(b) : );\n\nb = r[0]; // results of 'red.shared', compare it with a\n\n(g_R + ofs)[thid] = a - b; \n}\n```\n\n\nto see how 'red' instruction is implemented in the hardware you can use cuobjdump tool on \nthe produced 'cubin' file (use option -keep with nvcc)\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Count reduction using Thrust\r\n                \r\nGiven some input keys and values, I am trying to count how many consecutive values with the same key exist. I will give an example to make this more clear.\n\nInput keys: ```\n{ 1, 4, 4, 4, 2, 2, 1 }```\n\n\nInput values: ```\n{ 9, 8, 7, 6, 5, 4, 3 }```\n\n\nExpected output keys: ```\n{ 1, 4, 2, 1 }```\n\n\nExpected output values: ```\n{ 1, 3, 2, 1 }```\n\n\nI am trying to solve this problem on a GPU using CUDA. The reduction capabilities of the Thrust library seemed like a good solution for this and I got to the following:\n\n```\n#include <thrust/reduce.h>\n#include <thrust/functional.h>\n\nstruct count_functor : public thrust::binary_function<int, int, int>\n{\n    __host__ __device__\n        int operator()(int input, int counter)\n    {\n        return counter + 1;\n    }\n};\n\nconst int N = 7;\nint A[N] = { 1, 4, 4, 4, 2, 2, 1 }; // input keys\nint B[N] = { 9, 8, 7, 6, 5, 4, 3 }; // input values\nint C[N];                         // output keys\nint D[N];                         // output values\n\nthrust::pair<int*, int*> new_end;\nthrust::equal_to<int> binary_pred;\ncount_functor binary_op;\nnew_end = thrust::reduce_by_key(A, A + N, B, C, D, binary_pred, binary_op);\nfor (int i = 0; i < new_end.first - C; i++) {\n    std::cout << C[i] << \" - \" << D[i] << \"\\n\";\n}\n```\n\n\nThis code is pretty similar to an example from the Thrust documentation. However, instead of the ```\nplus```\n operation, I am trying to count. The output from this code is the following:\n\n```\n1 - 9\n4 - 7\n2 - 5\n1 - 3\n```\n\n\nHowever, I would expected the second column to contain the values ```\n1, 3, 2, 1```\n. I think the counts are off because the reduction starts with the first value it finds and does not apply the operator until it has a second value, but I am not sure this is the case. \n\nAm I overlooking something about the ```\nreduce_by_key```\n function that could solve this problem or should I use a completely different function to achieve what I want?\n    ", "Answer": "\r\nFor your use case you don't need the values of ```\nB```\n, the values of ```\nD```\n are only dependent on the values of ```\nA```\n.\n\nIn order to count how many consecutive values are in ```\nA```\n you can supply a ```\nthrust::constant_iterator```\n as the input values and apply ```\nthrust::reduce_by_key```\n:\n\n```\n#include <thrust/reduce.h>\n#include <thrust/functional.h>\n#include <iostream>\n#include <thrust/iterator/constant_iterator.h>\n\nint main()\n{\nconst int N = 7;\nint A[N] = { 1, 4, 4, 4, 2, 2, 1 }; \nint C[N];\nint D[N];\n\nthrust::pair<int*, int*> new_end;\nthrust::equal_to<int> binary_pred;\nthrust::plus<int> binary_op;\nnew_end = thrust::reduce_by_key(A, A + N, thrust::make_constant_iterator(1), C, D, binary_pred, binary_op);\n\nfor (int i = 0; i < new_end.first - C; i++) {\n    std::cout << C[i] << \" - \" << D[i] << \"\\n\";\n}\nreturn 0;\n}\n```\n\n\noutput\n\n```\n1 - 1\n4 - 3\n2 - 2\n1 - 1\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Parallelize transitive reduction\r\n                \r\nI have a ```\nDictionary<int, List<int>>```\n, where the Key represents an element of a set (or a vertex in an oriented graph) and the List is a set of other elements which are in relation with the Key (so there are oriented edges from Key to Values). The dictionary is optimized for creating a Hasse diagram, so the Values are always smaller than the Key.\n\nI have also a simple sequential algorithm, that removes all transitive edges (e.g. I have relations 1->2, 2->3 and 1->3. I can remove the edge 1->3, because I have a path between 1 and 3 via 2). \n\n```\nfor(int i = 1; i < dictionary.Count; i++)\n{\n    for(int j = 0; j < i; j++)\n    {\n        if(dictionary[i].Contains(j))\n                dictionary[i].RemoveAll(r => dictionary[j].Contains(r));\n    }\n}\n```\n\n\nWould it be possible to parallelize the algorithm? I could do Parallel.For for the inner loop. However, this is not recommended (https://msdn.microsoft.com/en-us/library/dd997392(v=vs.110).aspx#Anchor_2) and the resulting speed would not increase significantly (+ there might be problems with locking). Could I parallelize the outer loop?\n    ", "Answer": "\r\nThere is simple way to solve the parallelization problem, separate data. Read from original data structure and write to new. That way You can run it in parallel without even need to lock.\n\nBut probably the parallelization is not even necessary, the data structures are not efficient. You use dictionary where array would be sufficient (as I understand the code You have vertices ```\n0..result.Count-1```\n). And ```\nList<int>```\n for lookups. ```\nList.Contains```\n is very inefficient. ```\nHashSet```\n would be better. Or, for more dense graphs, ```\nBitArray```\n. So instead of ```\nDictionary<int, List<int>>```\n You can use ```\nBitArray[]```\n.\n\nI rewrote the algorithm and made some optimizations. It does not make plain copy of the graph and delete edges, it just construct the new graph from only the right edges. It uses ```\nBitArray[]```\n for input graph and ```\nList<int>[]```\n for final graph, as the latter one is far more sparse.\n\n```\nint sizeOfGraph = 1000;\n\n//create vertices of a graph\nBitArray[] inputGraph = new BitArray[sizeOfGraph];\nfor (int i = 0; i < inputGraph.Length; ++i)\n{\n    inputGraph[i] = new BitArray(i);\n}\n\n//fill random edges\nRandom rand = new Random(10);\nfor (int i = 1; i < inputGraph.Length; ++i)\n{\n    BitArray vertex_i = inputGraph[i];\n    for(int j = 0; j < vertex_i.Count; ++j)\n    {\n        if(rand.Next(0, 100) < 50) //50% fill ratio\n        {\n            vertex_i[j] = true;\n        }\n    }\n}\n\n//create transitive closure\nfor (int i = 0; i < sizeOfGraph; ++i)\n{\n    BitArray vertex_i = inputGraph[i];\n    for (int j = 0; j < i; ++j)\n    {\n        if (vertex_i[j]) { continue; }\n        for (int r = j + 1; r < i; ++r)\n        {\n            if (vertex_i[r] && inputGraph[r][j])\n            {\n                vertex_i[j] = true;\n                break;\n            }\n        }\n    }\n}\n\n//create transitive reduction\nList<int>[] reducedGraph = new List<int>[sizeOfGraph];\nParallel.ForEach(inputGraph, (vertex_i, state, ii) =>\n{\n    {\n        int i = (int)ii;\n        List<int> reducedVertex = reducedGraph[i] = new List<int>();\n\n        for (int j = i - 1; j >= 0; --j)\n        {\n            if (vertex_i[j])\n            {\n                bool ok = true;\n                for (int x = 0; x < reducedVertex.Count; ++x)\n                {\n                    if (inputGraph[reducedVertex[x]][j])\n                    {\n                        ok = false;\n                        break;\n                    }\n                }\n                if (ok)\n                {\n                    reducedVertex.Add(j);\n                }\n            }\n        }\n    }\n});\n\nMessageBox.Show(\"Finished, reduced graph has \"\n    + reducedGraph.Sum(s => s.Count()) + \" edges.\");\n```\n\n\nEDIT\n\nI wrote this:\nThe code has some problems. With the direction ```\ni```\n goes now, You can delete edges You would need and the result would be incorrect. This turned out to be a mistake. I was thinking this way, lets have a graph\n\n```\n1->0\n2->1, 2->0\n3->2, 3->1, 3->0\n```\n\n\nVertex 2 gets reduced by vertex 1, so we have\n\n```\n1->0\n2->1\n3->2, 3->1, 3->0\n```\n\n\nNow vertex 3 gets reduced by vertex 2\n\n```\n1->0\n2->1\n3->2, 3->0\n```\n\n\nAnd we have a problem, as we can not reduce ```\n3->0```\n which stayed here because of reduced ```\n2->0```\n. But it is my mistake, this would never happen. The inner cycle goes strictly from lower to higher, so instead\n\nVertex 3 gets reduced by vertex 1\n\n```\n1->0\n2->1\n3->2, 3->1\n```\n\n\nand now by vertex 2\n\n```\n1->0\n2->1\n3->2\n```\n\n\nAnd the result is correct. I apologize for the error.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Can Java 8 streams cause a O(1) memory reduction on unbounded data to become O(n) memory because of the underlying ForkJoin implementation\r\n                \r\nI've written a streams implementation that performs four simple reductions (+ and <) on the lines of a file.\n\nAt first I performed four streams, but I decided to write my own accumulator and combiner so that I could perform all four reductions in one stream. On small data sets (10,000,000 lines) this reduces runtime to about 1/4 as expected, and runs in 14 seconds on my hardware.\n\n```\nfileIn = new BufferedReader(new InputStreamReader(\n            new URL(args[0].trim()).openStream()));\n\nfinal Results results = fileIn.lines()\n        .parallel()\n        .skip(1)\n        .map(User::parse)\n        .filter(Optional::isPresent)\n        .map(Optional::get)\n        .collect(Results::new, Results::accumulate, Results::combine);\n```\n\n\n```\nResults::accumulate```\n and ```\nResults::combine```\n correctly combine Users into Results and Results with Results respectively, and this implementation works\ngreat for small data sets.\n\nI tried using ```\n.reduce()```\n as well, and results are similar, but I tried ```\n.collect()```\n to reduce the creation of short-lived objects.\n\nThe problem is that when I use real-world sized data with 1 billion lines I am hitting an issue that suggests that Java 8 streams are incapable of the task. The heap memory is observed in JConsole to climb to the allocated 12 GB in a roughly linear fashion and then OOM.\n\nI was under the impression that the collector or reducer would provide performance comparable to an iterative solution, which should be bounded by CPU and IO but not memory, because the reduction step produces a Result that doesn't grow, it's a reduction!\n\nWhen I take a heap dump and put it into jhat I see that about 7GB is taken up with Strings, and these strings can clearly be seen to be the lines of the input file. I feel they should not be in memory at all, but jhat shows a very large ForkJoin related structure being accumulated in memory:\n\n```\nStatic reference from java.util.concurrent.ForkJoinPool.common (from class java.util.concurrent.ForkJoinPool) :\n\n--> java.util.concurrent.ForkJoinPool@0x786d41db0 (76 bytes) (field workQueues:)\n--> [Ljava.util.concurrent.ForkJoinPool$WorkQueue;@0x786eda598 (144 bytes) (Element 3 of [Ljava.util.concurrent.ForkJoinPool$WorkQueue;@0x786eda598:)\n--> java.util.concurrent.ForkJoinPool$WorkQueue@0x786d41ee8 (96 bytes) (field currentSteal:)\n--> java.util.stream.SliceOps$SliceTask@0x7b4ac6cb0 (130 bytes) (field completer:)\n--> java.util.stream.SliceOps$SliceTask@0x7b379ad18 (130 bytes) (field completer:)\n--> java.util.stream.SliceOps$SliceTask@0x7b25bdb68 (130 bytes) (field leftChild:)\n--> java.util.stream.SliceOps$SliceTask@0x7b379acb8 (130 bytes) (field localResult:)\n--> java.util.stream.Nodes$SpinedNodeBuilder@0x7b25fdda0 (53 bytes) (field spine:)\n--> [[Ljava.lang.Object;@0x7b25ffe48 (144 bytes) (Element 12 of [[Ljava.lang.Object;@0x7b25ffe48:)\n--> [Ljava.lang.Object;@0x7b37c4f20 (262160 bytes) (Element 19598 of [Ljava.lang.Object;@0x7b37c4f20:)\n--> 31ea87ba876505645342b31928394b3c,2013-11-24T23:02:17+00:00,898,22200,1314,700 (28 bytes) (field value:)\n--> [C@0x7b2ffff88 (170 bytes) // <<<< There are thousands of these\n```\n\n\nThere are other references in ApplicationShutdownHooks, Local references, and System Classes, but this one I show is the crux of the problem, and it causes the memory to grow O(n) when \n\nDoes the streams implementation make this O(1) memory problem O(n) memory by holding all the Strings in the ForkJoin classes?? I love streams and I don't want this to be so :(\n    ", "Answer": "\r\nThanks to Marko Topolnik and Holger for coming to the correct answer. Though neither posted an answer for me to accept, so I'll try to tie this up for posterity :)\n\nThe ```\n.skip(1)```\n is very expensive on a parallel stream because it requires ordering to skip exactly the first entry, as per the Javadoc for Stream.skip()\n\nReading the first line of the BufferedReader before calling ```\n.lines()```\n on it does successfully skip the first line in my implementation.\n\nThen removing the ```\n.skip()```\n solves the memory problem, and it is observed in JConsole to bounce around nicely and return to < 1GB on every garbage collection even if the program processes 1 billion lines. This is desired behaviour and is close enough to O(1) memory for my purposes.\n\nContrary to a suggestion above, the relative locations of ```\n.parallel()```\n and ```\n.skip(1)```\n do not matter, you cannot re-order them to make the ```\n.skip(1)```\n happen \"before\" the ```\n.parallel().```\n The builder pattern suggests that ordering is important, and it is for other intermediate operations, but not for this one. I remember this subtlety from my OCP certification materials, but it doesn't appear to be in the Javadoc, hence no reference. I have, however, confirmed this experimentally by making the isolated change and observing the regression in JConsole, and associated OOM.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "When is theoretical computer science useful?\r\n                \r\nIn class, we learned about the halting problem, Turing machines, reductions, etc. A lot of classmates are saying these are all abstract and useless concepts, and there's no real point in knowing them (i.e., you can forget them once the course is over and not lose anything).\n\nWhy is theory useful? Do you ever use it in your day-to-day coding? \n    ", "Answer": "\r\nTrue story:\n\nWhen I got my first programming job out of graduate school, the guys that owned the company that I worked for were pilots. A few weeks after I was hired, one of them asked me this question:\n\n\n  There are 106 airports in Arkansas.\n  Could you write a program that would\n  find the shortest rout necessary to\n  land at each one of them?\n\n\nI seriously thought he was quizzing me on my knowledge of the Traveling Salesman Problem and NP-Completeness. But it turns out he wasn't. He didn't know anything about it. He really wanted a program that would find the shortest path. He was surprised when I explained that there were 106-factorial solutions and finding the best one was a well-known computationally intractable problem.\n\nSo that's one example.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda calculus reduction of expression\r\n                \r\nI have the following expression and need help doing the reduction. The left hand expression has to equal to the right hand expression.\n\n\n(λn.(λs.(s (λx.λy.y))n) λx.x)(λx.λy.y) = λx.x\n(λn.(λs.(s (λxy.y))n) λx.x)(λxy.y) = λx.x\n\n    ", "Answer": "\r\nHere are the reduction steps for each:\n\n```\n(λn.(λs.(s (λx.λy.y))n) λx.x)(λx.λy.y)\n(λs.(s (λx.λy.y)) λx.x)(λx.λy.y)\n(λx.x (λx.λy.y))(λx.λy.y)\n(λx.λy.y)(λx.λy.y)\nλy.y\n\n(λn.(λs.(s (λxy.y))n) λx.x)(λxy.y) \n(λs.(s (λxy.y)) λx.x)(λxy.y)\n(λx.x (λxy.y))(λxy.y)\nλxy.y (λxy.y) \nλy.y\n```\n\n\nSide note: 1 and 2 are equivalent since (λx.λy.y) = (λxy.y)\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Parallelize transitive reduction\r\n                \r\nI have a ```\nDictionary<int, List<int>>```\n, where the Key represents an element of a set (or a vertex in an oriented graph) and the List is a set of other elements which are in relation with the Key (so there are oriented edges from Key to Values). The dictionary is optimized for creating a Hasse diagram, so the Values are always smaller than the Key.\n\nI have also a simple sequential algorithm, that removes all transitive edges (e.g. I have relations 1->2, 2->3 and 1->3. I can remove the edge 1->3, because I have a path between 1 and 3 via 2). \n\n```\nfor(int i = 1; i < dictionary.Count; i++)\n{\n    for(int j = 0; j < i; j++)\n    {\n        if(dictionary[i].Contains(j))\n                dictionary[i].RemoveAll(r => dictionary[j].Contains(r));\n    }\n}\n```\n\n\nWould it be possible to parallelize the algorithm? I could do Parallel.For for the inner loop. However, this is not recommended (https://msdn.microsoft.com/en-us/library/dd997392(v=vs.110).aspx#Anchor_2) and the resulting speed would not increase significantly (+ there might be problems with locking). Could I parallelize the outer loop?\n    ", "Answer": "\r\nThere is simple way to solve the parallelization problem, separate data. Read from original data structure and write to new. That way You can run it in parallel without even need to lock.\n\nBut probably the parallelization is not even necessary, the data structures are not efficient. You use dictionary where array would be sufficient (as I understand the code You have vertices ```\n0..result.Count-1```\n). And ```\nList<int>```\n for lookups. ```\nList.Contains```\n is very inefficient. ```\nHashSet```\n would be better. Or, for more dense graphs, ```\nBitArray```\n. So instead of ```\nDictionary<int, List<int>>```\n You can use ```\nBitArray[]```\n.\n\nI rewrote the algorithm and made some optimizations. It does not make plain copy of the graph and delete edges, it just construct the new graph from only the right edges. It uses ```\nBitArray[]```\n for input graph and ```\nList<int>[]```\n for final graph, as the latter one is far more sparse.\n\n```\nint sizeOfGraph = 1000;\n\n//create vertices of a graph\nBitArray[] inputGraph = new BitArray[sizeOfGraph];\nfor (int i = 0; i < inputGraph.Length; ++i)\n{\n    inputGraph[i] = new BitArray(i);\n}\n\n//fill random edges\nRandom rand = new Random(10);\nfor (int i = 1; i < inputGraph.Length; ++i)\n{\n    BitArray vertex_i = inputGraph[i];\n    for(int j = 0; j < vertex_i.Count; ++j)\n    {\n        if(rand.Next(0, 100) < 50) //50% fill ratio\n        {\n            vertex_i[j] = true;\n        }\n    }\n}\n\n//create transitive closure\nfor (int i = 0; i < sizeOfGraph; ++i)\n{\n    BitArray vertex_i = inputGraph[i];\n    for (int j = 0; j < i; ++j)\n    {\n        if (vertex_i[j]) { continue; }\n        for (int r = j + 1; r < i; ++r)\n        {\n            if (vertex_i[r] && inputGraph[r][j])\n            {\n                vertex_i[j] = true;\n                break;\n            }\n        }\n    }\n}\n\n//create transitive reduction\nList<int>[] reducedGraph = new List<int>[sizeOfGraph];\nParallel.ForEach(inputGraph, (vertex_i, state, ii) =>\n{\n    {\n        int i = (int)ii;\n        List<int> reducedVertex = reducedGraph[i] = new List<int>();\n\n        for (int j = i - 1; j >= 0; --j)\n        {\n            if (vertex_i[j])\n            {\n                bool ok = true;\n                for (int x = 0; x < reducedVertex.Count; ++x)\n                {\n                    if (inputGraph[reducedVertex[x]][j])\n                    {\n                        ok = false;\n                        break;\n                    }\n                }\n                if (ok)\n                {\n                    reducedVertex.Add(j);\n                }\n            }\n        }\n    }\n});\n\nMessageBox.Show(\"Finished, reduced graph has \"\n    + reducedGraph.Sum(s => s.Count()) + \" edges.\");\n```\n\n\nEDIT\n\nI wrote this:\nThe code has some problems. With the direction ```\ni```\n goes now, You can delete edges You would need and the result would be incorrect. This turned out to be a mistake. I was thinking this way, lets have a graph\n\n```\n1->0\n2->1, 2->0\n3->2, 3->1, 3->0\n```\n\n\nVertex 2 gets reduced by vertex 1, so we have\n\n```\n1->0\n2->1\n3->2, 3->1, 3->0\n```\n\n\nNow vertex 3 gets reduced by vertex 2\n\n```\n1->0\n2->1\n3->2, 3->0\n```\n\n\nAnd we have a problem, as we can not reduce ```\n3->0```\n which stayed here because of reduced ```\n2->0```\n. But it is my mistake, this would never happen. The inner cycle goes strictly from lower to higher, so instead\n\nVertex 3 gets reduced by vertex 1\n\n```\n1->0\n2->1\n3->2, 3->1\n```\n\n\nand now by vertex 2\n\n```\n1->0\n2->1\n3->2\n```\n\n\nAnd the result is correct. I apologize for the error.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Can Java 8 streams cause a O(1) memory reduction on unbounded data to become O(n) memory because of the underlying ForkJoin implementation\r\n                \r\nI've written a streams implementation that performs four simple reductions (+ and <) on the lines of a file.\n\nAt first I performed four streams, but I decided to write my own accumulator and combiner so that I could perform all four reductions in one stream. On small data sets (10,000,000 lines) this reduces runtime to about 1/4 as expected, and runs in 14 seconds on my hardware.\n\n```\nfileIn = new BufferedReader(new InputStreamReader(\n            new URL(args[0].trim()).openStream()));\n\nfinal Results results = fileIn.lines()\n        .parallel()\n        .skip(1)\n        .map(User::parse)\n        .filter(Optional::isPresent)\n        .map(Optional::get)\n        .collect(Results::new, Results::accumulate, Results::combine);\n```\n\n\n```\nResults::accumulate```\n and ```\nResults::combine```\n correctly combine Users into Results and Results with Results respectively, and this implementation works\ngreat for small data sets.\n\nI tried using ```\n.reduce()```\n as well, and results are similar, but I tried ```\n.collect()```\n to reduce the creation of short-lived objects.\n\nThe problem is that when I use real-world sized data with 1 billion lines I am hitting an issue that suggests that Java 8 streams are incapable of the task. The heap memory is observed in JConsole to climb to the allocated 12 GB in a roughly linear fashion and then OOM.\n\nI was under the impression that the collector or reducer would provide performance comparable to an iterative solution, which should be bounded by CPU and IO but not memory, because the reduction step produces a Result that doesn't grow, it's a reduction!\n\nWhen I take a heap dump and put it into jhat I see that about 7GB is taken up with Strings, and these strings can clearly be seen to be the lines of the input file. I feel they should not be in memory at all, but jhat shows a very large ForkJoin related structure being accumulated in memory:\n\n```\nStatic reference from java.util.concurrent.ForkJoinPool.common (from class java.util.concurrent.ForkJoinPool) :\n\n--> java.util.concurrent.ForkJoinPool@0x786d41db0 (76 bytes) (field workQueues:)\n--> [Ljava.util.concurrent.ForkJoinPool$WorkQueue;@0x786eda598 (144 bytes) (Element 3 of [Ljava.util.concurrent.ForkJoinPool$WorkQueue;@0x786eda598:)\n--> java.util.concurrent.ForkJoinPool$WorkQueue@0x786d41ee8 (96 bytes) (field currentSteal:)\n--> java.util.stream.SliceOps$SliceTask@0x7b4ac6cb0 (130 bytes) (field completer:)\n--> java.util.stream.SliceOps$SliceTask@0x7b379ad18 (130 bytes) (field completer:)\n--> java.util.stream.SliceOps$SliceTask@0x7b25bdb68 (130 bytes) (field leftChild:)\n--> java.util.stream.SliceOps$SliceTask@0x7b379acb8 (130 bytes) (field localResult:)\n--> java.util.stream.Nodes$SpinedNodeBuilder@0x7b25fdda0 (53 bytes) (field spine:)\n--> [[Ljava.lang.Object;@0x7b25ffe48 (144 bytes) (Element 12 of [[Ljava.lang.Object;@0x7b25ffe48:)\n--> [Ljava.lang.Object;@0x7b37c4f20 (262160 bytes) (Element 19598 of [Ljava.lang.Object;@0x7b37c4f20:)\n--> 31ea87ba876505645342b31928394b3c,2013-11-24T23:02:17+00:00,898,22200,1314,700 (28 bytes) (field value:)\n--> [C@0x7b2ffff88 (170 bytes) // <<<< There are thousands of these\n```\n\n\nThere are other references in ApplicationShutdownHooks, Local references, and System Classes, but this one I show is the crux of the problem, and it causes the memory to grow O(n) when \n\nDoes the streams implementation make this O(1) memory problem O(n) memory by holding all the Strings in the ForkJoin classes?? I love streams and I don't want this to be so :(\n    ", "Answer": "\r\nThanks to Marko Topolnik and Holger for coming to the correct answer. Though neither posted an answer for me to accept, so I'll try to tie this up for posterity :)\n\nThe ```\n.skip(1)```\n is very expensive on a parallel stream because it requires ordering to skip exactly the first entry, as per the Javadoc for Stream.skip()\n\nReading the first line of the BufferedReader before calling ```\n.lines()```\n on it does successfully skip the first line in my implementation.\n\nThen removing the ```\n.skip()```\n solves the memory problem, and it is observed in JConsole to bounce around nicely and return to < 1GB on every garbage collection even if the program processes 1 billion lines. This is desired behaviour and is close enough to O(1) memory for my purposes.\n\nContrary to a suggestion above, the relative locations of ```\n.parallel()```\n and ```\n.skip(1)```\n do not matter, you cannot re-order them to make the ```\n.skip(1)```\n happen \"before\" the ```\n.parallel().```\n The builder pattern suggests that ordering is important, and it is for other intermediate operations, but not for this one. I remember this subtlety from my OCP certification materials, but it doesn't appear to be in the Javadoc, hence no reference. I have, however, confirmed this experimentally by making the isolated change and observing the regression in JConsole, and associated OOM.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "When is theoretical computer science useful?\r\n                \r\nIn class, we learned about the halting problem, Turing machines, reductions, etc. A lot of classmates are saying these are all abstract and useless concepts, and there's no real point in knowing them (i.e., you can forget them once the course is over and not lose anything).\n\nWhy is theory useful? Do you ever use it in your day-to-day coding? \n    ", "Answer": "\r\nTrue story:\n\nWhen I got my first programming job out of graduate school, the guys that owned the company that I worked for were pilots. A few weeks after I was hired, one of them asked me this question:\n\n\n  There are 106 airports in Arkansas.\n  Could you write a program that would\n  find the shortest rout necessary to\n  land at each one of them?\n\n\nI seriously thought he was quizzing me on my knowledge of the Traveling Salesman Problem and NP-Completeness. But it turns out he wasn't. He didn't know anything about it. He really wanted a program that would find the shortest path. He was surprised when I explained that there were 106-factorial solutions and finding the best one was a well-known computationally intractable problem.\n\nSo that's one example.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lambda calculus reduction of expression\r\n                \r\nI have the following expression and need help doing the reduction. The left hand expression has to equal to the right hand expression.\n\n\n(λn.(λs.(s (λx.λy.y))n) λx.x)(λx.λy.y) = λx.x\n(λn.(λs.(s (λxy.y))n) λx.x)(λxy.y) = λx.x\n\n    ", "Answer": "\r\nHere are the reduction steps for each:\n\n```\n(λn.(λs.(s (λx.λy.y))n) λx.x)(λx.λy.y)\n(λs.(s (λx.λy.y)) λx.x)(λx.λy.y)\n(λx.x (λx.λy.y))(λx.λy.y)\n(λx.λy.y)(λx.λy.y)\nλy.y\n\n(λn.(λs.(s (λxy.y))n) λx.x)(λxy.y) \n(λs.(s (λxy.y)) λx.x)(λxy.y)\n(λx.x (λxy.y))(λxy.y)\nλxy.y (λxy.y) \nλy.y\n```\n\n\nSide note: 1 and 2 are equivalent since (λx.λy.y) = (λxy.y)\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "error: reduction variable is private in outer context (omp reduction)\r\n                \r\nI am confused about the data sharing scope of the variable acc in the flowing two cases. In the case 1 I get following compilation error: ```\nerror: reduction variable ‘acc’ is private in outer context```\n, whereas the case 2 compiles without any issues.\nAccording to this article variables defined outside parallel region are shared.\nWhy is adding for-loop parallelism privatizing acc?\nHow can I in this case accumulate the result calculated in the the for-loop and distribute a loop's iteration space across a thread team?\ncase 1\n```\n            float acc = 0.0f;\n            \n            #pragma omp for simd reduction(+: acc)\n            for (int k = 0; k < MATRIX_SIZE; k++) {\n                float mul = alpha;\n                mul *=  a[i * MATRIX_SIZE + k];\n                mul *=  b[j * MATRIX_SIZE + k];\n                acc += mul;\n            }\n\n```\n\ncase 2\n```\n            float acc = 0.0f;\n            \n            #pragma omp simd reduction(+: acc)\n            for (int k = 0; k < MATRIX_SIZE; k++) {\n                float mul = alpha;\n                mul *=  a[i * MATRIX_SIZE + k];\n                mul *=  b[j * MATRIX_SIZE + k];\n                acc += mul;\n            }\n\n\n```\n\n    ", "Answer": "\r\nYour case 1 is violating OpenMP semantics, as there's an implicit parallel region (see OpenMP Language Terminology, \"sequential part\") that contains the definition of ```\nacc```\n.  Thus, ```\nacc```\n is indeed private to that implicit parallel region.  This is what the compiler complains about.\nYour case 2 is different in that the ```\nsimd```\n construct is not a worksharing construct and thus has a different definition of the semantics of the ```\nreduction```\n clause.\nCase 1 would be correct if you wrote it this way:\n```\nvoid example(void) {\n    float acc = 0.0f;\n\n    #pragma omp parallel for simd reduction(+: acc)\n    for (int k = 0; k < MATRIX_SIZE; k++) {\n        float mul = alpha;\n        mul *=  a[i * MATRIX_SIZE + k];\n        mul *=  b[j * MATRIX_SIZE + k];\n        acc += mul;\n    }\n}\n```\n\nThe ```\nacc```\n variable is now defined outside of the ```\nparallel```\n that the ```\nfor simd```\n construct binds to.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Sum reduction with CUDA: What is N?\r\n                \r\nAccording to NVIDIA, this is the fastest sum reduction kernel:\n\n```\ntemplate <unsigned int blockSize>\n__device__ void warpReduce(volatile int *sdata, unsigned int tid) {\nif (blockSize >=  64) sdata[tid] += sdata[tid + 32];\nif (blockSize >=  32) sdata[tid] += sdata[tid + 16];\nif (blockSize >=  16) sdata[tid] += sdata[tid +  8];\nif (blockSize >=    8) sdata[tid] += sdata[tid +  4];\nif (blockSize >=    4) sdata[tid] += sdata[tid +  2];\nif (blockSize >=    2) sdata[tid] += sdata[tid +  1];\n}\ntemplate <unsigned int blockSize>\n__global__ void reduce6(int *g_idata, int *g_odata, unsigned int n) {\nextern __shared__ int sdata[];\nunsigned int tid = threadIdx.x;\nunsigned int i = blockIdx.x*(blockSize*2) + tid;\nunsigned int gridSize = blockSize*2*gridDim.x;\nsdata[tid] = 0;\nwhile (i < n) { sdata[tid] += g_idata[i] + g_idata[i+blockSize];  i += gridSize;  }\n__syncthreads();\nif (blockSize >= 512) { if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); }\nif (blockSize >= 256) { if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); }\nif (blockSize >= 128) { if (tid <   64) { sdata[tid] += sdata[tid +   64]; } __syncthreads(); }\nif (tid < 32) warpReduce(sdata, tid);\nif (tid == 0) g_odata[blockIdx.x] = sdata[0];\n}\n```\n\n\nHowever, I don't understand the \"n\" parameter. Any clues? I don't think it's the size of the array to reduce, since in the while loop there would be a buffer overflow.\n    ", "Answer": "\r\nI believe you've discovered a typo in the slides (it should probably be something like ```\nwhile(i + blockDim.x < n)```\n).\n\nIf you take a look at the source code in the CUDA SDK sample \"reduction\", the body of the most recent ```\nreduce6```\n looks like this:\n\n```\ntemplate <class T, unsigned int blockSize, bool nIsPow2>\n__global__ void\nreduce6(T *g_idata, T *g_odata, unsigned int n)\n{\n    T *sdata = SharedMemory<T>();\n\n    // perform first level of reduction,\n    // reading from global memory, writing to shared memory\n    ...\n\n    T mySum = 0;\n\n    // we reduce multiple elements per thread.  The number is determined by the \n    // number of active thread blocks (via gridDim).  More blocks will result\n    // in a larger gridSize and therefore fewer elements per thread\n    while (i < n)\n    {         \n        mySum += g_idata[i];\n        // ensure we don't read out of bounds -- this is optimized away for powerOf2 sized arrays\n        if (nIsPow2 || i + blockSize < n) \n            mySum += g_idata[i+blockSize];  \n        i += gridSize;\n    } \n```\n\n\nNote the explicit check within the ```\nwhile```\n which prevents out of bounds access to ```\ng_idata```\n. Your initial suspicion is correct; ```\nn```\n is simply the size of the ```\ng_idata```\n array.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Cyclomatic Complexity reduction\r\n                \r\nI have a block of code that I am having an issue reducing the cyclomatic complexity of.  Because of the multiple conditions that have to match, I am not sure the best way to break it down further.  Complicating matters is that in 2 of the cases a new object is created, but not in the third (it calls out to another method).  This is the pseudocode:\n\n```\n    if (!cond3 && !cond1 && cond2 && cond4) {\n        // actions to perform\n        calculateValues();\n        return result;\n    } else if (!cond1 && cond2 && cond3) {\n        // actions to perform\n        Object result = new Result();\n        return result;\n    } else if (!cond4 && cond3 && cond1 && cond5) {\n        // actions to perform\n        Object result = new Result();\n        return result;\n    } else {\n        // throw error because inputs are invalid\n    }\n```\n\n    ", "Answer": "\r\nYou should refactor that code, using methods to abstract those conditions, high cyclomatic indicates that the codes need refactoring. For example, lets say that: ```\n!cond4 && cond3 && cond1 && cond5```\n tests if the logged user has a car, then you should refactor that combination of conditions to a method:\n\n```\nprivate boolean loggedUserHasCar() {\n    return !cond4 && cond3 && cond1 && cond5;\n}\n```\n\n\nDo the same thing to the other conditions. ```\nif```\n statements with 4 conditions are hard to read. Extracting those statements will reduce your method cyclomatic complexity and make your code more readable\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Min reduction cuda does not work\r\n                \r\nI wrote a code to find the minimum by reduction. However the result is always zero. I don't know what is the problem. Please help me. \n\nHere it is the kernel code: I modified the sum reduction code from the Nvidia. \n\n```\n#include <limits.h>\n\n#define NumThread 128\n#define NumBlock 32\n\n__global__ void min_reduce(int* In, int* Out, int n){\n  __shared__ int sdata[NumThread];\n  unsigned int i = blockIdx.x * NumThread + threadIdx.x;\n  unsigned int tid = threadIdx.x;\n  unsigned int gridSize = NumBlock * NumThread;\n  int myMin = INT_MAX;\n\n  while (i < n){\n    if(In[i] < myMin)\n    myMin = In[i];\n    i += gridSize;\n  }\n  sdata[tid] = myMin;\n  __syncthreads();\n\n  if (NumThread >= 1024){\n    if (tid < 512)\n    if(sdata[tid] > sdata[tid + 512] ) sdata[tid] = sdata[tid + 512];\n    __syncthreads();\n  }\n  if (NumThread >= 512){\n    if(sdata[tid] > sdata[tid + 256] ) sdata[tid] = sdata[tid + 256];\n    __syncthreads();\n  }\n  if (NumThread >= 256){\n    if(sdata[tid] > sdata[tid + 128] && sdata[tid + 128] !=0) sdata[tid] =  sdata[tid + 128];\n    __syncthreads();\n  }\n  if (NumThread >= 128){\n    if(sdata[tid] > sdata[tid + 64] ) sdata[tid] =    sdata[tid + 64];\n    __syncthreads();\n  }\n  //the following practice is deprecated\n   if (tid < 32){\n    volatile int *smem = sdata;\n    if (NumThread >= 64) if(smem[tid] > smem[tid + 32] ) smem[tid] =  smem[tid+32];\n    if (NumThread >= 32) if(smem[tid] > smem[tid + 16]) smem[tid] =  smem[tid+16];\n    if (NumThread >= 16) if(smem[tid] > smem[tid + 8]) smem[tid] =  smem[tid+8];\n    if (NumThread >= 8) if(smem[tid] > smem[tid + 4] ) smem[tid] =  smem[tid+4];\n    if (NumThread >= 4) if(smem[tid] > smem[tid + 2] ) smem[tid] =  smem[tid+2];\n    if (NumThread >= 2) if(smem[tid] > smem[tid + 1] )      smem[tid] =  smem[tid+1];\n  }\n  if (tid == 0)\n    if(sdata[0] < sdata[1] ) Out[blockIdx.x] = sdata[0];\n    else Out[blockIdx.x] = sdata[1];      \n}\n```\n\n\nAnd here it is my main code:\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n\n#include \"min_reduction.cu\"\n\nint main(int argc, char* argv[]){\n  unsigned int length = 1048576;\n  int i, Size, min;\n  int *a, *out, *gpuA, *gpuOut;\n\n  cudaSetDevice(0);\n  Size = length * sizeof(int);\n  a = (int*)malloc(Size);\n  out = (int*)malloc(NumBlock*sizeof(int));\n  for(i=0;i<length;i++) a[i] = (i + 10);\n\n  cudaMalloc((void**)&gpuA,Size);\n  cudaMalloc((void**)&gpuOut,NumBlock*sizeof(int));\n  cudaMemcpy(gpuA,a,Size,cudaMemcpyHostToDevice);\n  min_reduce<<<NumBlock,NumThread>>>(gpuA,gpuOut,length);\n  cudaDeviceSynchronize();\n  cudaMemcpy(out,gpuOut,NumBlock*sizeof(int),cudaMemcpyDeviceToHost);\n\n  min = out[0];\n  for(i=1;i<NumBlock;i++) if(min < out[i]) min = out[i];\n  return 0;\n}\n```\n\n    ", "Answer": "\r\nI'm not sure I agree with everything that @HubertApplebaum said, but I can agree with the suggestion to use proper cuda error checking.   And as you mention in the code, warp synchronous programming can be considered to be deprecated but I cannot support the claim that it is broken (yet).  However I don't wish to argue about that; it's not central to your question here.\n\nAnother useful debugging suggestion would be to follow the steps here to compile your code with ```\n-lineinfo```\n and run your code with ```\ncuda-memcheck```\n.  If you did that, you would see many reports like this:\n\n```\n========= Invalid __shared__ read of size 4\n=========     at 0x000001e0 in /home/bob/misc/t1074.cu:39:min_reduce(int*, int*, int)\n=========     by thread (64,0,0) in block (24,0,0)\n=========     Address 0x00000200 is out of bounds\n=========     Saved host backtrace up to driver entry point at kernel launch time\n=========     Host Frame:/lib64/libcuda.so.1 (cuLaunchKernel + 0x2cd) [0x15859d]\n=========     Host Frame:./t1074 [0x16dc1]\n=========     Host Frame:./t1074 [0x315d3]\n=========     Host Frame:./t1074 [0x28f5]\n=========     Host Frame:./t1074 [0x2623]\n=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xf5) [0x21d65]\n=========     Host Frame:./t1074 [0x271d]\n```\n\n\nwhich would indicate both that a primary problem in your code is that you are incorrectly indexing into your ```\n__shared__```\n  memory array as well as the specific line of code where that is taking place.  Neat!  (It's line 39 in my case, but it would be a different line probably in your case).   If you then drill into that line, you will want to study this section of code:\n\n```\n  #define NumThread 128\n  ...\n  __shared__ int sdata[NumThread];\n  ...\n  if (NumThread >= 128){\n    if(sdata[tid] > sdata[tid + 64] ) sdata[tid] =    sdata[tid + 64]; //line 39 in my case\n    __syncthreads();\n  }\n```\n\n\nYou have defined ```\nNumThread```\n at 128, and have statically allocated a shared memory array of that many ```\nint```\n quantities.  All well and good.  What about the code in the if-statement?  That if-condition will be satisfied, which means that all 128 threads in the block will execute the body of that if-statement.  However, you are reading ```\nsdata[tid + 64]```\n from shared memory, and for threads whose ```\ntid```\n is greater than 63 (i.e. half of the threads in each block), this will generate an index into shared memory of greater than 127 (which is out-of-bounds, i.e. illegal).\n\nThe fix (for this specific code that you have shown) is fairly simple, just add another if-test:\n\n```\n  if (NumThread >= 128){\n    if (tid < 64)\n      if(sdata[tid] > sdata[tid + 64] ) sdata[tid] =    sdata[tid + 64];\n    __syncthreads();\n  }\n```\n\n\nIf you make that modification to your code, and rerun the ```\ncuda-memcheck```\n test, you'll see that all the runtime-reported errors are gone.  Yay!\n\nBut the code still doesn't produce the right answer yet.  You've made another error here:\n\n```\n  for(i=1;i<NumBlock;i++) if(min < out[i]) min = out[i];\n```\n\n\nIf you want to find the minimum value, and think about that logic carefully, you'll realize you should have done this:\n\n```\n  for(i=1;i<NumBlock;i++) if(min > out[i]) min = out[i];\n                                 ^\n                                 |\n                              greater than\n```\n\n\nWith those two changes, your code produces the correct result for me:\n\n```\n$ cat t1074.cu\n#include <stdio.h>\n#include <stdlib.h>\n\n\n#include <limits.h>\n\n#define NumThread 128\n#define NumBlock 32\n\n__global__ void min_reduce(int* In, int* Out, int n){\n  __shared__ int sdata[NumThread];\n  unsigned int i = blockIdx.x * NumThread + threadIdx.x;\n  unsigned int tid = threadIdx.x;\n  unsigned int gridSize = NumBlock * NumThread;\n  int myMin = INT_MAX;\n\n  while (i < n){\n    if(In[i] < myMin)\n    myMin = In[i];\n    i += gridSize;\n  }\n  sdata[tid] = myMin;\n  __syncthreads();\n\n  if (NumThread >= 1024){\n    if (tid < 512)\n    if(sdata[tid] > sdata[tid + 512] ) sdata[tid] = sdata[tid + 512];\n    __syncthreads();\n  }\n  if (NumThread >= 512){\n    if(sdata[tid] > sdata[tid + 256] ) sdata[tid] = sdata[tid + 256];\n    __syncthreads();\n  }\n  if (NumThread >= 256){\n    if(sdata[tid] > sdata[tid + 128] && sdata[tid + 128] !=0) sdata[tid] =  sdata[tid + 128];\n    __syncthreads();\n  }\n  if (NumThread >= 128){\n    if (tid < 64)\n    if(sdata[tid] > sdata[tid + 64] ) sdata[tid] =    sdata[tid + 64];\n    __syncthreads();\n  }\n  //the following practice is deprecated\n   if (tid < 32){\n    volatile int *smem = sdata;\n    if (NumThread >= 64) if(smem[tid] > smem[tid + 32] ) smem[tid] =  smem[tid+32];\n    if (NumThread >= 32) if(smem[tid] > smem[tid + 16]) smem[tid] =  smem[tid+16];\n    if (NumThread >= 16) if(smem[tid] > smem[tid + 8]) smem[tid] =  smem[tid+8];\n    if (NumThread >= 8) if(smem[tid] > smem[tid + 4] ) smem[tid] =  smem[tid+4];\n    if (NumThread >= 4) if(smem[tid] > smem[tid + 2] ) smem[tid] =  smem[tid+2];\n    if (NumThread >= 2) if(smem[tid] > smem[tid + 1] )      smem[tid] =  smem[tid+1];\n  }\n  if (tid == 0)\n    if(sdata[0] < sdata[1] ) Out[blockIdx.x] = sdata[0];\n    else Out[blockIdx.x] = sdata[1];\n}\n\nint main(int argc, char* argv[]){\n  unsigned int length = 1048576;\n  int i, Size, min;\n  int *a, *out, *gpuA, *gpuOut;\n\n  cudaSetDevice(0);\n  Size = length * sizeof(int);\n  a = (int*)malloc(Size);\n  out = (int*)malloc(NumBlock*sizeof(int));\n  for(i=0;i<length;i++) a[i] = (i + 10);\n  a[10]=5;\n  cudaMalloc((void**)&gpuA,Size);\n  cudaMalloc((void**)&gpuOut,NumBlock*sizeof(int));\n  cudaMemcpy(gpuA,a,Size,cudaMemcpyHostToDevice);\n  min_reduce<<<NumBlock,NumThread>>>(gpuA,gpuOut,length);\n  cudaDeviceSynchronize();\n  cudaMemcpy(out,gpuOut,NumBlock*sizeof(int),cudaMemcpyDeviceToHost);\n\n  min = out[0];\n  for(i=1;i<NumBlock;i++) if(min > out[i]) min = out[i];\n  printf(\"min = %d\\n\", min);\n  return 0;\n}\n$ nvcc -o t1074 t1074.cu\n$ cuda-memcheck ./t1074\n========= CUDA-MEMCHECK\nmin = 5\n========= ERROR SUMMARY: 0 errors\n$\n```\n\n\nNote that you already have the if-check in the 1024 threads case, you may want to add an appropriate if-check to the 512 and 256 threads case, just as I have added it for the 128 threads case above.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Haskell - Eta reduction and Eta expansion\r\n                \r\nI've been studying functional program optimization, and have been digging into the GHC source. I (mostly) understand what eta reduction and eta expansion are. Eta reduction only removes redundant lambdas:\n```\n\\x -> abs x\n=>\nabs\n```\n\nEta expansion is the opposite of eta reduction and does things like this (correct me if I'm incorrect):\n```\nabs\n=>\n\\x -> abs x\n-----------------------------------------------\nfoo = abs\n=>\nfoo x = abs x\n-----------------------------------------------\nfoo = bar 100\nbar x y = x + y\n=>\nfoo y = bar 10 y\nbar x y = x + y\n-----------------------------------------------\netc...\n```\n\nWhat I don't get is how they don't get in each other's way and send the compiler into an infinite loop. For example, first, a value is eta expanded, and then it is eta reduced, and so on. So, how do the two optimizations not get in each other's way?\n    ", "Answer": "\r\nI think I found an answer. I found a thesis from a contributor to GHC (don't remember what it was called), and in it, it mentioned that GHC doesn't do eta reduction. Instead, it does eta expansion, and beta reduction (IIRC); Beta reduction does most of the job.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Solving String reduction Algorithm\r\n                \r\nI am preparing myself for an interview that I have on monday and I found this problem to resolve called \"String Reduction\". The problem is stated like this : \n\n\n  Given a string consisting of a,b and c's, we can perform the following\n  operation: Take any two adjacent distinct characters and replace it\n  with the third character. For example, if 'a' and 'c' are adjacent,\n  they can replaced with 'b'. What is the smallest string which can\n  result by applying this operation repeatedly?\n  \n  For instance, cab -> cc or cab -> bb, resulting in a string of length\n  2. For this one, one optimal solution is: bcab -> aab -> ac -> b. No more operations can be applied and the resultant string has length 1.\n  If the string is = CCCCC, no operations can be performed and so the\n  answer is 5.\n\n\nI have seen a lot questions and answers on stackoverflow but I would like to verify my own algorithm. Here is my algorithm in pseudo code. In my code\n\n\nS is my string to reduce\nS[i] is the character at index i\nP is a stack:\nredux is the function that reduces the characters.\n\n```\nfunction reduction(S[1..n]){        \nP = create_empty_stack();\nfor i = 1 to n\ndo\n   car = S[i];\n   while (notEmpty(P))\n   do\n      head = peek(p);\n      if( head == car) break;\n      else {\n         popped = pop(P);\n         car = redux (car, popped);\n       }  \n   done\n   push(car)\ndone\nreturn size(P)}\n```\n\n\n\nThe worst-case of my algorithms is O(n) because all the operations on the stack P is on O(1). I tried this algorithm in the examples above, I get the expected answers. \nLet me execute my algo with this example \" abacbcaa\" :\n\n```\ni = 1 :\n   car = S[i] = a, P = {∅}\n   P is empty, P = P U {car} -> P = {a}\n\n i = 2 :\n   car = S[i] = b, P = {a}\n   P is not empty :\n       head = a\n       head != car ->\n            popped = Pop(P) = a \n            car = reduction (car, popped) = reduction (a,b) = c\n            P = {∅}\n\n    push(car, P) -> P = {c}\n\n\n\ni = 3 :\n   car = S[i] = a, P = {c}\n   P is not empty :\n       head = c\n       head != car ->\n            popped = Pop(P) = c \n            car = reduction (car, popped) = reduction (a,c) = b\n            P = {∅}\n\n    push(car, P) -> P = {b}\n\n\n ...\n\n\n i = 5 : (interesting case)\n  car = S[i] = c, P = {c}\n   P is not empty :\n       head = c\n       head == car -> break\n\n    push(car, P) -> P = {c, c}\n\n\n i = 6 :\n  car = S[i] = b, P = {c, c}\n   P is not empty :\n       head = c\n       head != car ->\n            popped = Pop(P) = c \n            car = reduction (car, popped) = reduction (b,c) = a\n            P = {c}\n\n   P is not empty : // (note in this case car = a)\n       head = c\n       head != car ->\n            popped = Pop(P) = c \n            car = reduction (car, popped) = reduction (a,c) = b\n            P = {∅}\n    push(car, P) -> P = {b}\n\n... and it continues until n\n```\n\n\nI have run this algorithm on various examples like this, it seems to work. \nI have written a code in Java that test this algorithm, when I submit my code to the system, I am getting wrong answers. I have posted the java code on gisthub  so you can see it. \n\nCan someone tell me what is wrong with my algorithm. \n    ", "Answer": "\r\nI am going to try to explain what ```\nnhahtdh```\n means. There are multiple reasons why your algorithm fails. But the most fundamental one is that at each point in time, only the first character observed has a chance to be pushed on the stack ```\np```\n. It should not be this way, as you can start a reduction basically from any position. \n\nLet me give you the string  ```\nabcc```\n. If I breakpoint at \n\n```\ncar = S[i];\n```\n\n\nThe algo run as :\n\n```\np = {∅}, s = _abcc //underscore is the position\np = {a}, s = a_bcc  \np = {c}, s = ab_cc  \n```\n\n\nAt this point you are stuck with a reduction ```\nccc```\n\n\nBut there is another reduction : ```\nabcc -> aac ->ab ->c```\n\n\nBesides, returning the size of the stack ```\nP```\n is wrong. ```\ncc```\n cannot be reduced, but\nthe algorithm will return ```\n1```\n. You should also count the number of times you skip.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Parallel list reduction in CUDA\r\n                \r\nI am working through the Cuda Parallel reduction Whitepaper, but unfortunately my algorithm seems to repeatedly produce incorrect results, and I can not seem to figure out why(surely a textbook example must work? Surely I'm just doing something very obvious wrong?). Here is my kernel function:\n\nMy define:\n\n```\n #define BLOCK_SIZE 512\n```\n\n\nMy Kernel function:\n\n```\n __global__ void total(float * inputList, float * outputList, int len) {\n      __shared__ float sdata[2*BLOCK_SIZE];\n      unsigned int tid = threadIdx.x;\n      unsigned int i = blockIdx.x*(blockDim.x*2) + threadIdx.x;\n      sdata[t] = inputList[i]+inputList[i+blockDim.x];\n      __syncthreads();\n      for (unsigned int s=blockDim.x/2; s>0; s>>=1) {\n        if (tid < s) {\n          sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n      }\n      if (tid == 0) \n        outputList[blockIdx.x] = sdata[0];\n}\n```\n\n\nMy memory allocation:\n\n```\n  outputSize = inputSize / (BLOCK_SIZE<<1);\n  cudaMalloc((void**) &deviceInput, inputSize*sizeof(float));\n  cudaMalloc((void**) &deviceOutput, outputSize*sizeof(float));\n  cudaMemcpy(deviceInput, hostInput, inputSize*sizeof(float), cudaMemcpyHostToDevice);\n```\n\n\nMy device call:\n\n```\n dim3 dimGrid((inputSize-1)/BLOCK_SIZE +1, 1, 1);\n dim3 dimBlock(BLOCK_SIZE,1,1);\n\n total<<<dimBlock, dimGrid>>>(deviceInput, deviceOutput, outputSize);\n cudaDeviceSynchronize();\n```\n\n\nMy memory fetch:\n\n```\n cudaMemcpy(hostOutput, deviceOutput, outputSize*sizeof(float), cudaMemcpyDeviceToHost);\n```\n\n\nAnd finally my final calculation:\n\n```\n for (int counter = 1; counter < outputSize; counter++) {\n    hostOutput[0] += hostOutput[counter];\n }\n```\n\n\nAny help would be appreciated.\n    ", "Answer": "\r\nYour kernel launch configuration in the following line of your code is incorrect.\n\n```\ntotal<<<dimBlock, dimGrid>>>(deviceInput, deviceOutput, outputSize); \n```\n\n\nThe first argument of kernel configuration is the grid size and the second argument is the block size.\n\nYou should be doing this:\n\n```\ntotal<<<dimGrid, dimBlock>>>(deviceInput, deviceOutput, outputSize); \n```\n\n\nPlease always perform error checking on CUDA Runtime function calls and check the returned error codes to get the reason for the failure of your program. \n\nYour kernel launch should fail in your current code. An error checking on the ```\ncudaDeviceSynchronize```\n call would have led you to the reason of incorrect results.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Conditional reduction in CUDA\r\n                \r\nI need to sum about ```\n100000```\n values stored in an array, but with conditions.\n\nIs there a way to do that in CUDA to produce fast results?\n\nCan anyone post a small code to do that?\n    ", "Answer": "\r\nI think that, to perform conditional reduction, you can directly introduce the condition as a multiplication by ```\n0```\n (false) or ```\n1```\n (true) to the addends. In other words, suppose that the condition you would like to meet is that the addends be smaller than ```\n10.f```\n. In this case, borrowing the first code at Optimizing Parallel Reduction in CUDA by M. Harris, then the above would mean\n\n```\n__global__ void reduce0(int *g_idata, int *g_odata) {\n\n    extern __shared__ int sdata[];\n\n    // each thread loads one element from global to shared mem\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n    sdata[tid] = g_idata[i]*(g_data[i]<10.f);\n    __syncthreads();\n\n    // do reduction in shared mem\n    for(unsigned int s=1; s < blockDim.x; s *= 2) {\n        if (tid % (2*s) == 0) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n\n    // write result for this block to global mem\n    if (tid == 0) g_odata[blockIdx.x] = sdata[0];\n}\n```\n\n\nIf you wish to use CUDA Thrust to perform conditional reduction, you can do the same by using ```\nthrust::transform_reduce```\n. Alternatively, you can create a new vector ```\nd_b```\n copying in that all the elements of ```\nd_a```\n satisfying the predicate by ```\nthrust::copy_if```\n and then applying ```\nthrust::reduce```\n on ```\nd_b```\n. I haven't checked which solution performs the best. Perhaps, the second solution will perform better on sparse arrays. Below is an example with an implementation of both the approaches.\n\n```\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <thrust/reduce.h>\n#include <thrust/count.h>\n#include <thrust/copy.h>\n\n// --- Operator for the first approach\nstruct conditional_operator { \n    __host__ __device__ float operator()(const float a) const {\n    return a*(a<10.f);\n    }\n};\n\n// --- Operator for the second approach\nstruct is_smaller_than_10 {\n    __host__ __device__ bool operator()(const float a) const {\n        return (a<10.f);\n    }\n};\n\nvoid main(void) \n{\n    int N = 20;\n\n    // --- Host side allocation and vector initialization\n    thrust::host_vector<float> h_a(N,1.f);\n    h_a[0] = 20.f;\n    h_a[1] = 20.f;\n\n    // --- Device side allocation and vector initialization\n    thrust::device_vector<float> d_a(h_a);\n\n    // --- First approach\n    float sum = thrust::transform_reduce(d_a.begin(), d_a.end(), conditional_operator(), 0.f, thrust::plus<float>());\n    printf(\"Result = %f\\n\",sum);\n\n    // --- Second approach\n    int N_prime = thrust::count_if(d_a.begin(), d_a.end(), is_smaller_than_10());\n    thrust::device_vector<float> d_b(N_prime);\n    thrust::copy_if(d_a.begin(), d_a.begin() + N, d_b.begin(), is_smaller_than_10());\n    sum = thrust::reduce(d_b.begin(), d_b.begin() + N_prime, 0.f);\n    printf(\"Result = %f\\n\",sum);\n\n    getchar();\n\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "'Reduction' parameter in tf.keras.losses\r\n                \r\nAccording to the docs, the ```\nReduction```\n parameter takes on 3 values - ```\nSUM_OVER_BATCH_SIZE```\n, ```\nSUM```\n and ```\nNONE```\n.\n```\ny_true = [[0., 2.], [0., 0.]]\ny_pred = [[3., 1.], [2., 5.]]\n\nmae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)\nmae(y_true, y_pred).numpy()\n> 5.5\n\nmae = tf.keras.losses.MeanAbsoluteError()\nmae(y_true, y_pred).numpy()\n> 2.75\n```\n\nWhat I could infer about the calculation after various trials, is this:-\n\nwhen ```\nREDUCTION = SUM```\n,\n```\nLoss = Sum over all samples {(Sum of differences between y_pred and y_target vector of each sample / No of element in y_target of the sample )} = { (abs(3-0) + abs(1-2))/2 } + { (abs(2-0) + abs(5-0))/2 } = {4/2} + {7/2} = 5.5```\n.\n\nwhen ```\nREDUCTION = SUM_OVER_BATCH_SIZE```\n,\n```\nLoss = [Sum over all samples {(Sum of differences between y_pred and y_target vector of each sample / No of element in y_target of the sample )}] / Batch_size or No of Samples  = [ { (abs(3-0)} + abs(1-2))/2 } + { (abs(2-0) + abs(5-0))/2 } ]/2 = [ {4/2} + {7/2} ]/2 = [5.5]/2 = 2.75```\n.\n\n\nAs a result, ```\nSUM_OVER_BATCH_SIZE```\n is nothing but ```\nSUM/batch_size```\n. Then, why is it called ```\nSUM_OVER_BATCH_SIZE```\n when ```\nSUM```\n actually adds up the losses over the entire batch, while ```\nSUM_OVER_BATCH_SIZE```\n calculates the average loss of the batch.\nIs my assumption regarding the workings of ```\nSUM_OVER_BATCH_SIZE```\n and ```\nSUM```\n at all correct?\n    ", "Answer": "\r\nYour assumption is correct as far as I understand.\nIf you check the github [keras/losses_utils.py][1] lines 260-269\nyou will see that it does performs as expected.\n```\nSUM```\n will sum up the losses in the batch dimension, and ```\nSUM_OVER_BATCH_SIZE```\n would divide ```\nSUM```\n by the number of total losses (batch size).\n```\ndef reduce_weighted_loss(weighted_losses,\n                     reduction=ReductionV2.SUM_OVER_BATCH_SIZE):\n  if reduction == ReductionV2.NONE:\n     loss = weighted_losses\n  else:\n     loss = tf.reduce_sum(weighted_losses)\n     if reduction == ReductionV2.SUM_OVER_BATCH_SIZE:\n        loss = _safe_mean(loss, _num_elements(weighted_losses))\n  return loss\n```\n\nYou can do a easy checking with your previous example just by adding one pair of outputs with 0 loss.\n```\ny_true = [[0., 2.], [0., 0.],[1.,1.]]\ny_pred = [[3., 1.], [2., 5.],[1.,1.]]\n\nmae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)\nmae(y_true, y_pred).numpy()\n> 5.5\n\nmae = tf.keras.losses.MeanAbsoluteError()\nmae(y_true, y_pred).numpy()\n> 1.8333\n```\n\nSo, your assumption is correct.\n[1]: https://github.com/keras-team/keras/blob/v2.7.0/keras/utils/losses_utils.py#L25-L84\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Path Simplification/Reduction\r\n                \r\nI'm writing some code to manage a custom on disk file structure and syncronize it to unconnected systems.  One of my requirements is to be able to estimate the size of a sync prior to actually generating the sync content.  As a simple solution, I've put together a map with full path filenames as the key for effecient lookup of already scanned content.\n\nI run into problems with this when I have multiple files in my file structure referenced from different places in different ways.  For example:\n\n```\nC:\\DataSource\\files\\samplefile.txt\nC:\\DataSource\\data\\samples\\..\\..\\files\\samplefile.txt\nC:\\DataSource\\etc\\..\\files\\samplefile.txt\n```\n\n\nThese 3 path strings all reference the same file on-disk, however their string representation is different.  If I drop these into a map verbatim, I'll count the size of samplefile.txt 3 times, and my estimate will be wrong.\n\nIn an attempt to find a way around this, I was hoping boost::filesystem::path provided a function to reduce or simplify a path, but I didn't see anything of the sort.  Using the path decomposition table and path iterators, I wrote up the following function (for use in a Windows environment):\n\n```\nstd::string ReducePath( std::string Path )\n{\n    bfs::path input( Path );\n    bfs::path result( \"\" );\n    bfs::path::iterator it, endIt;\n    for( it = input.begin( ), endIt = input.end( ); it != endIt; it ++ )\n    {\n        if( (*it) == \"..\" )\n        {\n            // Remove the leaf directory.\n            result = result.parent_path( );\n        }\n        else if( (*it) == \".\" )\n        {\n            // Just ignore.\n        }\n        else\n        {\n            // Append the element to the end of the current result.\n            result /= (*it);\n        }\n    }\n\n    return result.string( ).c_str( );\n}\n```\n\n\nI have two questions.\n\nOne, is there a standard function that provides this sort of functionality, or does this already exist in boost or another library somewhere?\n\nTwo, I'm not entirely confident that the function I wrote will work in all cases, and I'd like some more eyes on it.  It works in my tests.  Does anyone see a case where it'll break down?\n    ", "Answer": "\r\nThere is a function in boost\n```\nbool equivalent(const Path1& p1, const Path2& p2);\n```\n\nThat checks if two paths are equal.  That would be ideal except that there is no equivalent < operator(and perhaps cannot be).\n\nDoes anyone see a case where this will break ...\n\nMaybe; if you have input like \"../test.txt\", parent path might not do what you want.  I would recommend completing the path first.\nSee \"complete\" in the filesystem library.\nGood luck\n--Robert Nelson\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Candidate Keys / Reduction\r\n                \r\nI have a relation\n\nR = (S, T, U, V, W, X, Y, Z) which has the FD's\n\nTW -> Y\nTW -> Z\nY -> X\nY -> S\nV -> U\nX -> T \n\nI'm trying to find the candidate key for R but because of the relationship\n\nTW -> Y -> X -> T\n\nI can't figure out what it is. I know it has to have 'V' and 'W' in it but I don't know if I should also have 'X' or 'T'. \n    ", "Answer": "\r\nYou say that ```\nW```\n is dead sure inside any candidate key.\n\nTherefore,\n\n```\nTW -> Y -> X -> T\n```\n\n\nbecomes\n\n```\nT -> Y -> X -> T\n```\n\n\nSo, which one do you choose, ```\nX```\n or ```\nT```\n?\n\nAnd why not ```\nY```\n ?\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenCV: Grayscale color reduction\r\n                \r\nI'm trying to reduce my grayscale image color from 256 to 4 using this formula\nfrom http://docs.opencv.org/2.4/doc/tutorials/core/how_to_scan_images/how_to_scan_images.html\n\n\nI assume that n is the reduction factor, for this case, it will be 10 color from the formula.\nMy code is as below.\n\n```\nvoid Reduction(Mat image1)\n{\nfor (int r = 0;r < image1.rows;r++) {\n\n    for (int c = 0;c < image1.cols;c++) {\n        // get pixel\n\n        int tmp = (image1.at<Vec3b>(r, c)[0] + image1.at<Vec3b>(r, c)[1] + image1.at<Vec3b>(r, c)[2])/3 ;\n        tmp =  (tmp/4)* 4;\n        image1.at<Vec3b>(r, c)[0] = tmp;\n        image1.at<Vec3b>(r, c)[1] = tmp;\n        image1.at<Vec3b>(r, c)[2] = tmp;\n    }\n\n    }\n}\n```\n\n\nmy expected result is \n\nbut from tmp = (tmp/4)*4; or tmp = ( tmp/8)*8;\n\nmy image looks the same as the original image;\n\nthen i tried changing it to tmp = (tmp/40)*40; \n\nand I got this as the result which is similar to what I wanted for my result.\n\nHow does the formula works and what should I edit from my code to accurately get the result I wanted? ( like the expected result above)\n    ", "Answer": "\r\nThis is a color quantization. This is a simple technique to reduce the number of colors in an image that relies on the integer division\n\nSince the starting range has ```\n256```\n values, if you want to end up with ```\nN```\n colors, you need to integer divide and then multiply by ```\nK = 256 / N```\n.\n\nSo in your case, for ```\nN=8```\n you need a ```\nK = 256 / 8 = 32```\n, and for ```\nN = 4```\n you need ```\nK = 256 / 4 = 64```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to PCA reduction in Matlab\r\n                \r\nI am new to Matlab and have some problems using built in packages for PCA reduction. I have 37 objects each represented by 161 dimensional vector, that means i have 161 x 37 matrix called P. I need to reduce vector dimension to 3. so that each object will be represented by 3 dimensional vector. I have tried something with princomp(P) but i don't know which output to take.\n\n[COEFF,SCORE] = princomp(P); \nnewData=SCORE(:,1:3);\n\nI think newData are not the right vectors ?\n    ", "Answer": "\r\nYou have to transpose your data because ```\nprincomp```\n expects observations on rows:\n\n```\n[COEFF,SCORE] = princomp(P.');\nnewData=SCORE(1:3.:).';\n```\n\n\nAlternatively you can use ```\npca```\n function to give you only the first 3 principal components:\n\n```\n[COEFF,SCORE] = pca(P.', 'NumComponents', 3)\nnewData=SCORE.';\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Min reduction cuda does not work\r\n                \r\nI wrote a code to find the minimum by reduction. However the result is always zero. I don't know what is the problem. Please help me. \n\nHere it is the kernel code: I modified the sum reduction code from the Nvidia. \n\n```\n#include <limits.h>\n\n#define NumThread 128\n#define NumBlock 32\n\n__global__ void min_reduce(int* In, int* Out, int n){\n  __shared__ int sdata[NumThread];\n  unsigned int i = blockIdx.x * NumThread + threadIdx.x;\n  unsigned int tid = threadIdx.x;\n  unsigned int gridSize = NumBlock * NumThread;\n  int myMin = INT_MAX;\n\n  while (i < n){\n    if(In[i] < myMin)\n    myMin = In[i];\n    i += gridSize;\n  }\n  sdata[tid] = myMin;\n  __syncthreads();\n\n  if (NumThread >= 1024){\n    if (tid < 512)\n    if(sdata[tid] > sdata[tid + 512] ) sdata[tid] = sdata[tid + 512];\n    __syncthreads();\n  }\n  if (NumThread >= 512){\n    if(sdata[tid] > sdata[tid + 256] ) sdata[tid] = sdata[tid + 256];\n    __syncthreads();\n  }\n  if (NumThread >= 256){\n    if(sdata[tid] > sdata[tid + 128] && sdata[tid + 128] !=0) sdata[tid] =  sdata[tid + 128];\n    __syncthreads();\n  }\n  if (NumThread >= 128){\n    if(sdata[tid] > sdata[tid + 64] ) sdata[tid] =    sdata[tid + 64];\n    __syncthreads();\n  }\n  //the following practice is deprecated\n   if (tid < 32){\n    volatile int *smem = sdata;\n    if (NumThread >= 64) if(smem[tid] > smem[tid + 32] ) smem[tid] =  smem[tid+32];\n    if (NumThread >= 32) if(smem[tid] > smem[tid + 16]) smem[tid] =  smem[tid+16];\n    if (NumThread >= 16) if(smem[tid] > smem[tid + 8]) smem[tid] =  smem[tid+8];\n    if (NumThread >= 8) if(smem[tid] > smem[tid + 4] ) smem[tid] =  smem[tid+4];\n    if (NumThread >= 4) if(smem[tid] > smem[tid + 2] ) smem[tid] =  smem[tid+2];\n    if (NumThread >= 2) if(smem[tid] > smem[tid + 1] )      smem[tid] =  smem[tid+1];\n  }\n  if (tid == 0)\n    if(sdata[0] < sdata[1] ) Out[blockIdx.x] = sdata[0];\n    else Out[blockIdx.x] = sdata[1];      \n}\n```\n\n\nAnd here it is my main code:\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n\n#include \"min_reduction.cu\"\n\nint main(int argc, char* argv[]){\n  unsigned int length = 1048576;\n  int i, Size, min;\n  int *a, *out, *gpuA, *gpuOut;\n\n  cudaSetDevice(0);\n  Size = length * sizeof(int);\n  a = (int*)malloc(Size);\n  out = (int*)malloc(NumBlock*sizeof(int));\n  for(i=0;i<length;i++) a[i] = (i + 10);\n\n  cudaMalloc((void**)&gpuA,Size);\n  cudaMalloc((void**)&gpuOut,NumBlock*sizeof(int));\n  cudaMemcpy(gpuA,a,Size,cudaMemcpyHostToDevice);\n  min_reduce<<<NumBlock,NumThread>>>(gpuA,gpuOut,length);\n  cudaDeviceSynchronize();\n  cudaMemcpy(out,gpuOut,NumBlock*sizeof(int),cudaMemcpyDeviceToHost);\n\n  min = out[0];\n  for(i=1;i<NumBlock;i++) if(min < out[i]) min = out[i];\n  return 0;\n}\n```\n\n    ", "Answer": "\r\nI'm not sure I agree with everything that @HubertApplebaum said, but I can agree with the suggestion to use proper cuda error checking.   And as you mention in the code, warp synchronous programming can be considered to be deprecated but I cannot support the claim that it is broken (yet).  However I don't wish to argue about that; it's not central to your question here.\n\nAnother useful debugging suggestion would be to follow the steps here to compile your code with ```\n-lineinfo```\n and run your code with ```\ncuda-memcheck```\n.  If you did that, you would see many reports like this:\n\n```\n========= Invalid __shared__ read of size 4\n=========     at 0x000001e0 in /home/bob/misc/t1074.cu:39:min_reduce(int*, int*, int)\n=========     by thread (64,0,0) in block (24,0,0)\n=========     Address 0x00000200 is out of bounds\n=========     Saved host backtrace up to driver entry point at kernel launch time\n=========     Host Frame:/lib64/libcuda.so.1 (cuLaunchKernel + 0x2cd) [0x15859d]\n=========     Host Frame:./t1074 [0x16dc1]\n=========     Host Frame:./t1074 [0x315d3]\n=========     Host Frame:./t1074 [0x28f5]\n=========     Host Frame:./t1074 [0x2623]\n=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xf5) [0x21d65]\n=========     Host Frame:./t1074 [0x271d]\n```\n\n\nwhich would indicate both that a primary problem in your code is that you are incorrectly indexing into your ```\n__shared__```\n  memory array as well as the specific line of code where that is taking place.  Neat!  (It's line 39 in my case, but it would be a different line probably in your case).   If you then drill into that line, you will want to study this section of code:\n\n```\n  #define NumThread 128\n  ...\n  __shared__ int sdata[NumThread];\n  ...\n  if (NumThread >= 128){\n    if(sdata[tid] > sdata[tid + 64] ) sdata[tid] =    sdata[tid + 64]; //line 39 in my case\n    __syncthreads();\n  }\n```\n\n\nYou have defined ```\nNumThread```\n at 128, and have statically allocated a shared memory array of that many ```\nint```\n quantities.  All well and good.  What about the code in the if-statement?  That if-condition will be satisfied, which means that all 128 threads in the block will execute the body of that if-statement.  However, you are reading ```\nsdata[tid + 64]```\n from shared memory, and for threads whose ```\ntid```\n is greater than 63 (i.e. half of the threads in each block), this will generate an index into shared memory of greater than 127 (which is out-of-bounds, i.e. illegal).\n\nThe fix (for this specific code that you have shown) is fairly simple, just add another if-test:\n\n```\n  if (NumThread >= 128){\n    if (tid < 64)\n      if(sdata[tid] > sdata[tid + 64] ) sdata[tid] =    sdata[tid + 64];\n    __syncthreads();\n  }\n```\n\n\nIf you make that modification to your code, and rerun the ```\ncuda-memcheck```\n test, you'll see that all the runtime-reported errors are gone.  Yay!\n\nBut the code still doesn't produce the right answer yet.  You've made another error here:\n\n```\n  for(i=1;i<NumBlock;i++) if(min < out[i]) min = out[i];\n```\n\n\nIf you want to find the minimum value, and think about that logic carefully, you'll realize you should have done this:\n\n```\n  for(i=1;i<NumBlock;i++) if(min > out[i]) min = out[i];\n                                 ^\n                                 |\n                              greater than\n```\n\n\nWith those two changes, your code produces the correct result for me:\n\n```\n$ cat t1074.cu\n#include <stdio.h>\n#include <stdlib.h>\n\n\n#include <limits.h>\n\n#define NumThread 128\n#define NumBlock 32\n\n__global__ void min_reduce(int* In, int* Out, int n){\n  __shared__ int sdata[NumThread];\n  unsigned int i = blockIdx.x * NumThread + threadIdx.x;\n  unsigned int tid = threadIdx.x;\n  unsigned int gridSize = NumBlock * NumThread;\n  int myMin = INT_MAX;\n\n  while (i < n){\n    if(In[i] < myMin)\n    myMin = In[i];\n    i += gridSize;\n  }\n  sdata[tid] = myMin;\n  __syncthreads();\n\n  if (NumThread >= 1024){\n    if (tid < 512)\n    if(sdata[tid] > sdata[tid + 512] ) sdata[tid] = sdata[tid + 512];\n    __syncthreads();\n  }\n  if (NumThread >= 512){\n    if(sdata[tid] > sdata[tid + 256] ) sdata[tid] = sdata[tid + 256];\n    __syncthreads();\n  }\n  if (NumThread >= 256){\n    if(sdata[tid] > sdata[tid + 128] && sdata[tid + 128] !=0) sdata[tid] =  sdata[tid + 128];\n    __syncthreads();\n  }\n  if (NumThread >= 128){\n    if (tid < 64)\n    if(sdata[tid] > sdata[tid + 64] ) sdata[tid] =    sdata[tid + 64];\n    __syncthreads();\n  }\n  //the following practice is deprecated\n   if (tid < 32){\n    volatile int *smem = sdata;\n    if (NumThread >= 64) if(smem[tid] > smem[tid + 32] ) smem[tid] =  smem[tid+32];\n    if (NumThread >= 32) if(smem[tid] > smem[tid + 16]) smem[tid] =  smem[tid+16];\n    if (NumThread >= 16) if(smem[tid] > smem[tid + 8]) smem[tid] =  smem[tid+8];\n    if (NumThread >= 8) if(smem[tid] > smem[tid + 4] ) smem[tid] =  smem[tid+4];\n    if (NumThread >= 4) if(smem[tid] > smem[tid + 2] ) smem[tid] =  smem[tid+2];\n    if (NumThread >= 2) if(smem[tid] > smem[tid + 1] )      smem[tid] =  smem[tid+1];\n  }\n  if (tid == 0)\n    if(sdata[0] < sdata[1] ) Out[blockIdx.x] = sdata[0];\n    else Out[blockIdx.x] = sdata[1];\n}\n\nint main(int argc, char* argv[]){\n  unsigned int length = 1048576;\n  int i, Size, min;\n  int *a, *out, *gpuA, *gpuOut;\n\n  cudaSetDevice(0);\n  Size = length * sizeof(int);\n  a = (int*)malloc(Size);\n  out = (int*)malloc(NumBlock*sizeof(int));\n  for(i=0;i<length;i++) a[i] = (i + 10);\n  a[10]=5;\n  cudaMalloc((void**)&gpuA,Size);\n  cudaMalloc((void**)&gpuOut,NumBlock*sizeof(int));\n  cudaMemcpy(gpuA,a,Size,cudaMemcpyHostToDevice);\n  min_reduce<<<NumBlock,NumThread>>>(gpuA,gpuOut,length);\n  cudaDeviceSynchronize();\n  cudaMemcpy(out,gpuOut,NumBlock*sizeof(int),cudaMemcpyDeviceToHost);\n\n  min = out[0];\n  for(i=1;i<NumBlock;i++) if(min > out[i]) min = out[i];\n  printf(\"min = %d\\n\", min);\n  return 0;\n}\n$ nvcc -o t1074 t1074.cu\n$ cuda-memcheck ./t1074\n========= CUDA-MEMCHECK\nmin = 5\n========= ERROR SUMMARY: 0 errors\n$\n```\n\n\nNote that you already have the if-check in the 1024 threads case, you may want to add an appropriate if-check to the 512 and 256 threads case, just as I have added it for the 128 threads case above.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Direct Reduction, Turing machine and a DFA\r\n                \r\nI have been reading and I am trying to understand the reduction when it comes to truing machine. This is how I understand it: it means that it reduces problem A into problem C. But I am not quite sure how it totally works.\nlets see an example:\n\nGiven the language L:\n\n```\nL ={<M,D>| M is s TM and D is a DFA so that L(M) = L(D)}, \n```\n\n\nusing reduction how to prove ```\nAtm < L.```\n\n\nMy solution:\n\nM is a Turing machine that accepts any string and it halts on that string.\nD is DFA hast accepts the language L and its equivalent to TM M.\nAtm is a TM, M that accepts string w.\n\nHow can you prove using a direct reduction that ```\nAtm < L??```\n\n    ", "Answer": "\r\nWe need to show that a decider for L can be used to decide instances of Atm, that is, a decider for L can be used to answer the question \"does a given TM accept a given input string?\"\n\nGiven an instance ```\n<M, w>```\n of Atm, we need to transform it into an instance ```\n<M', D>```\n of this problem so that a solution to this problem will answer the other.\n\nFirst, construct ```\nM'```\n so that ```\nL(M') = L(M) intersect {w}```\n. This can be done as follows. Create a TM that scans the input tape and ensures that ```\nw```\n is the input. If ```\nw```\n is not the input, halt-reject. Otherwise, return to the front of the tape and transition to the formerly initial state of ```\nM```\n. Then, run ```\nM```\n as normal. Clearly, this can only possibly accept ```\nw```\n, since we rejected everything else; and if ```\nM```\n accepts ```\nw```\n, this will as well, since after the initial phase ```\nM```\n runs normally.\n\nSecond, construct ```\nD```\n so that ```\nL(D) = {w}```\n. This ```\nDFA```\n will have ```\n|w| + 2```\n states: an initial state, a dead state, and one state per symbol in ```\nw```\n.\n\nNow, use our decider for ```\nL```\n on the instance ```\n<M', D>```\n. It will halt-accept if and only if ```\nL(M') = L(D) = {w}```\n, which is only possible if ```\nL(M) intersect {w} = {w}```\n which in turn is only satisfied if ```\nL(M)```\n contains ```\nw```\n. So if we halt-accept, then we have an answer to our instance ```\n<M, w>```\n of Atm.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Existence of \"simd reduction(:)\" In GCC and MSVC?\r\n                \r\nsimd pragma can be used with icc compiler to perform a reduction operator:\n\n```\n#pragma simd\n#pragma simd reduction(+:acc)\n#pragma ivdep\nfor(int i( 0 ); i < N; ++i )\n{\n  acc += x[i];\n}\n```\n\n\nIs there any equivalent solution in msvc or/and gcc? \n\nRef(p28): http://d3f8ykwhia686p.cloudfront.net/1live/intel/CompilerAutovectorizationGuide.pdf\n    ", "Answer": "\r\nFor Visual Studio 2012:\nWith options ```\n/O1 /O2/GL```\n, to report vectorization use ```\n/Qvec-report:(1/2)```\n\n\n```\nint s = 0; \nfor ( int i = 0; i < 1000; ++i ) \n{ \ns += A[i]; // vectorizable \n}\n```\n\n\nIn the case of reductions over \"```\nfloat```\n\" or \"```\ndouble```\n\" types, vectorization requires that the ```\n/fp:fast```\n switch is thrown. This is because vectorizing the reduction operation depends upon \"floating point reassociation\". Reassociation is only allowed when ```\n/fp:fast```\n is thrown\n\n\n  Ref(associated doc;p12) http://blogs.msdn.com/b/nativeconcurrency/archive/2012/07/10/auto-vectorizer-in-visual-studio-11-cookbook.aspx\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Dimensionality Reduction Autoencoder Pytorch\r\n                \r\nI'm trying to use the Autoencoder which code you can see below as a tool for Dimensionality Reduction,\nI was wondering how can I \"extract\" the hidden layer and use it for my purpose\nMy original Dataset went under Standard Scaling\nHere I define a Dictionary to centralize the values\n```\nCONFIG = {\n        'BATCH_SIZE' : 1024,\n        'LR' : 1e-4,\n        'WD' : 1e-8,\n        'EPOCHS': 50\n        }\n```\n\nHere I convert the values of my train and test dataframes into tensors\n```\nt_test = torch.FloatTensor(test.values)\nt_train = torch.FloatTensor(train.values)\n```\n\nHere I create data loaders\n```\nloader_test = torch.utils.data.DataLoader(dataset = t_test,\n                                 batch_size = CONFIG['BATCH_SIZE'],\n                                     shuffle = True)\n\nloader_train = torch.utils.data.DataLoader(dataset = t_train,\n                                     batch_size = CONFIG['BATCH_SIZE'],\n                                     shuffle = True)\n```\n\nHere I create the class AutoEncoder (AE)\n```\nclass AE(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.encoder = torch.nn.Sequential(\n            torch.nn.Linear(31,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 8),\n            torch.nn.ReLU(),\n            torch.nn.Linear(8, 4),\n        )\n    \n        self.decoder = torch.nn.Sequential(  \n            torch.nn.Linear(4, 8),\n            torch.nn.ReLU(),\n            torch.nn.Linear(8, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 31),\n\n        )\n \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n```\n\nHere I define model loss_funcion and the optimizer\n```\nmodel = AE()\n\nloss_function = torch.nn.MSELoss()\n\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr = CONFIG['LR'],\n                             weight_decay = CONFIG['WD'])\n```\n\nHere I compute the algorithm\n```\nepochs = CONFIG['EPOCHS']\ndict_list = []\nfor epoch in range(epochs):\n    for (ix, batch) in enumerate(loader_train):\n        \n        model.train()\n        reconstructed = model(batch)\n    \n        loss = loss_function(reconstructed, batch)\n    \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        temp_dict = {'Epoch':epoch,'Batch_N':ix,'Batch_L':batch.shape[0],'loss':loss.detach().numpy()}\n        dict_list.append(temp_dict)\n        \ndf_learning_o = pd.DataFrame(dict_list)\n```\n\n    ", "Answer": "\r\nYou can simply return not just the decoded output, but also the encoded embedding layer, like this:\n```\nclass AE(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.encoder = torch.nn.Sequential(\n            torch.nn.Linear(31,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 8),\n            torch.nn.ReLU(),\n            torch.nn.Linear(8, 4),\n        )\n    \n        self.decoder = torch.nn.Sequential(  \n            torch.nn.Linear(4, 8),\n            torch.nn.ReLU(),\n            torch.nn.Linear(8, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 31),\n\n        )\n \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return encoded, decoded\n```\n\nWhen you pass something to your model (in the train loop for example), you would have to change it to the following:\n```\nencoded, reconstructed = model(batch)\n```\n\nNow you can do whatever you'd like with the encoded embedding, i.e. which is the dimensionally reduced input.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Order of execution in Reduction Operation in OpenMP\r\n                \r\nIs there a way to know the order of execution for a reduction operator in OpenMP? In other words, I would like to know how the threads execute reduction operation- is it left to right? What happens when there are numbers that are not power of 2?\n    ", "Answer": "\r\nI think you'll find that OpenMP will only reduce on associative operations, such as ```\n+```\n and ```\n*```\n (or addition and multiplication if you prefer) which means that it can proceed oblivious to the order of evaluation of the component parts of the reduction expression across threads.  \n\nI strongly suggest that you proceed in the same way when using OpenMP, trying to either figure out or constrain the order of execution will at best turn your parallel program into a sequential one, at worst proceed to give you effectively random results.\n\nI fail to understand your last sentence about numbers that are not powers of 2. \n\nAs @JimCownie has pointed out, operations such as ```\n+```\n and ```\n*```\n are not strictly associative on floating-point numbers.  Construe my reference to associative operations in the first sentence to mean operations which are, when applied to real numbers, associative, but which, for reasons well understood by skilled practitioners of numerical computing on modern computers, fail to be associative when applied to floating-point numbers.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "segmented reduction with scattered segments\r\n                \r\nI got to solve a pretty standard problem on the GPU, but I'm quite new to practical GPGPU, so I'm looking for ideas to approach this problem.\n\nI have many points in 3-space which are assigned to a very small number of groups (each point belongs to one group), specifically 15 in this case (doesn't ever change). Now I want to compute the mean and covariance matrix of all the groups. So on the CPU it's roughly the same as:\n\n```\nfor each point p\n{\n    mean[p.group] += p.pos;\n    covariance[p.group] += p.pos * p.pos;\n    ++count[p.group];\n}\n\nfor each group g\n{\n    mean[g] /= count[g];\n    covariance[g] = covariance[g]/count[g] - mean[g]*mean[g];\n}\n```\n\n\nSince the number of groups is extremely small, the last step can be done on the CPU (I need those values on the CPU, anyway). The first step is actually just a segmented reduction, but with the segments scattered around.\n\nSo the first idea I came up with, was to first sort the points by their groups. I thought about a simple bucket sort using ```\natomic_inc```\n to compute bucket sizes and per-point relocation indices (got a better idea for sorting?, atomics may not be the best idea). After that they're sorted by groups and I could possibly come up with an adaption of the segmented scan algorithms presented here.\n\nBut in this special case, I got a very large amount of data per point (9-10 floats, maybe even doubles if the need arises), so the standard algorithms using a shared memory element per thread and a thread per point might make problems regarding per-multiprocessor resources as shared memory or registers (Ok, much more on compute capability 1.x than 2.x, but still).\n\nDue to the very small and constant number of groups I thought there might be better approaches. Maybe there are already existing ideas suited for these specific properties of such a standard problem. Or maybe my general approach isn't that bad and you got ideas for improving the individual steps, like a good sorting algorithm  suited for a very small number of keys or some segmented reduction algorithm minimizing shared memory/register usage.\n\nI'm looking for general approaches and don't want to use external libraries. FWIW I'm using OpenCL, but it shouldn't really matter as the general concepts of GPU computing don't really differ over the major frameworks.\n    ", "Answer": "\r\nEven though there are few groups, I don't think you will be able to avoid the initial sorting into groups while still keeping the reduction step efficient. You will probably also want to perform the full sort, not just sorting indexes, because that will help keep memory access efficient in the reduction step.\n\nFor sorting, read about general strategies here:\n\nhttp://http.developer.nvidia.com/GPUGems2/gpugems2_chapter46.html\n\nFor reduction (old but still good):\n\nhttp://developer.download.nvidia.com/compute/cuda/1.1-Beta/x86_website/projects/reduction/doc/reduction.pdf\n\nFor an example implementation of parallel reduction:\n\nhttp://developer.nvidia.com/cuda-cc-sdk-code-samples#reduction\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Conv1D as dimensionality reduction for LSTM\r\n                \r\nI was hoping to use CNN as a dimensionality reduction for my LSTM layers.\n\nI have a panel dataset as the following:\n\n```\nsequence of days = 5065\nlags = 14 days (those are time series lags)\nfeatures = 2767\n```\n\n\nThus, ```\n[5065, 14, 2767]```\n\n\nAs you can see I have more than half as many features as data points, and I wanted to reduce that. Ideally, I wanted to feed my LSTM layers with compressed feature information with something like 32 features, hopefully in the following shape:\n\n```\n[5065, 14, 32]\n```\n\n\nHowever, when setting up the CNN, I understand that filters should be 32, but what about my kernel size? I'm not sure I'm doing the right thing.\n    ", "Answer": "\r\nAt CNN, it is common to do dimensionality reduction  with a kernel size of ```\n1x1```\n. Thereby only the filter/feature map dimension is affected and the spatial information is kept intact, because the input is 1:1 mapped to the output. \n\nHereby a good example is the Inception architecture, which uses 1x1 convolutions to reduce the dimensionality in the inception modules. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Is polynomial reduction reversible?\r\n                \r\nIs this statement true or false : \"If a problem A is polynomially reducible to a problem B, then problem B must also be polynomially reducible to A\". \n    ", "Answer": "\r\nThis is wrong, consider the reducible to relationship as its hardness is less than or equal. For example, if A is polynomially reducible to B, it means that A <= B in terms of hardness (amount of computation needed to solve it). If A is reducible to B it means that A is simpler than (or as hard as) B, which means if you can solve B you can solve A, as well.\n\nSome supplementary information:\nAny problem in P, which are problems that are simple and can be solved in polynomial time, is reducible to any problem in NP-complete (e.g. SAT). This means that problems in P are simpler than problems in NP-complete. Now, if your statement was true then problems in NP-complete would have been solved in polynomial time, which is seemingly impossible (no one has proved or disproved it). And if anybody solves it there will be chaos!!!\n\nhttps://en.wikipedia.org/wiki/P_versus_NP_problem\n\nSAT problem\n\nA world with P=NP\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Pythonic way for recursive reduction in python\r\n                \r\nWhile reducing a list in python, I have thought about creating a multiple list reduction and wrote the following snippet.\n\n```\ndef multiply(a, b): return a * b\n\ndef recursive_reduce(reduce_func, *args):\n     ret_val = reduce(reduce_func, *args)\n     if type(ret_val) == list:\n         ret_val = recursive_reduce(reduce_func, ret_val)\n     return ret_val\n\na = [1, 1, 3]\nb = [4, 5, 6]\n\nrecursive_reduce(multiply, a, b)\n```\n\n\nThis works. However I would like to know whether defining the logic for iteration based on the type of the return value is pythonic or not.\n\nDo we have any other way which accomplishes recursive reduction in more elegant way?\n    ", "Answer": "\r\nI think what you're trying to do is do a recursive version of reduce.  \n\n```\ndef rreduce(f, init, default=None):                     \n    if default is None:\n        default = init[0]\n        init = init[1:]\n    if len(init) == 0:\n         return default\n    return rreduce(f, init[1:], f(default, init[0]))\n\n\n>>> rreduce(lambda a, b: a*b, range(1,10))\n362880\n>>> rreduce(lambda a, b: a+b, ['t', 'a', 'c', 'o'])\n'taco'\n```\n\n\nWhile recursion is great, this is not the preferred way in Python for ```\nreduce```\n type functions because it is slow and you will hit a STACK OVERFLOW (HAA) \n\n```\n>>> rreduce(lambda a, b: a + [b], list(range(1, 10000)), [])\n---------------------------------------------------------------------------\nRecursionError                            Traceback (most recent call last)\n<ipython-input-41-7dc07c5d9246> in <module>()\n----> 1 rreduce(lambda a, b: a + [b], list(range(1, 10000)), [])\n\n<ipython-input-33-37206eb8e39f> in rreduce(f, init, default)\n      5     if len(init) == 0:\n      6         return default\n----> 7     return rreduce(f, init[1:], f(default, init[0]))\n\n... last 1 frames repeated, from the frame below ...\n\n<ipython-input-33-37206eb8e39f> in rreduce(f, init, default)\n      5     if len(init) == 0:\n      6         return default\n----> 7     return rreduce(f, init[1:], f(default, init[0]))\n\nRecursionError: maximum recursion depth exceeded in comparison\n```\n\n\nTo answer your ACTUAL question...\n\n```\ndef lreduce(f, init, default=None):\n        if default is None:\n            return reduce(lambda x, a: x + [reduce(f, a)], init, [])\n        else:\n            return reduce(lambda x, a: x + [reduce(f, a, default)], init, [])\n```\n\n\nwill ```\nreduce```\n a list of lists.  \n\n```\n>>> lreduce(lambda a, b: a + b, [range(10), range(10), range(10)])\n[45, 45, 45]\n```\n\n\nThe reason the ```\nif/else```\n is necessary is because ```\nreduce```\n as a ```\nbuiltin```\n does not accept keyword arguments:\n\n```\nIn [56]: reduce(function=lambda a, b: a + b, sequence=range(10), initial=0)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-56-9fa3ed177831> in <module>()\n----> 1 reduce(function=lambda a, b: a + b, sequence=range(10), initial=0)\n\nTypeError: reduce() takes no keyword arguments\n```\n\n\nand then if you wanted to do one step further...\n\n```\ndef lreduceall(f, init, default=None):\n    if default is None:\n        return reduce(f, reduce(lambda x, a: x + [reduce(f, a)], init, []))\n    else:\n        return reduce(f, reduce(lambda x, a: x + [reduce(f, a, default)], init, []), default)\n```\n\n\nfinally:\n\n```\n>>> lreduceall(lambda a, b: a + b, [range(10), range(10), range(10)])\n135\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "role of _ and array reduction\r\n                \r\nI have a code example using jsx and redux to set the reducer as follows:\n\n```\n       case Test:\n        const { [action.payload.id]: _, ...filteredAlerts } = state.alerts;\n\n        return {\n            ...state,\n            ...{ \n                alerts:\n                {\n                    ...filteredAlerts\n                }\n            }\n        };\n    default:\n        return state;\n```\n\n\nEverything works fine but I do not understand this line at all:\n\n```\n const { [action.payload.id]: _, ...filteredAlerts } = state.alerts;\n```\n\n\nCan anyone provide a simple and clear explanation on that?\nFor example what does _ mean here?\n\n```\n[action.payload.id]: _\n```\n\n\nand what does this line do? \n\n```\nconst { [action.payload.id]: _, ...filteredAlerts } = state.alerts;\n```\n\n    ", "Answer": "\r\nAll it's doing is assigning a certain item in ```\nstate.alerts```\n to a local variable with the name ```\n_```\n, then collecting the rest of the items with  ```\n...```\n. Essentially, this is copying all the properties in ```\nstate.alerts```\n except ```\naction.payload.id```\n to the new ```\nfilteredAlerts```\n variable. It's a long form of writing this:\n\n```\nlet filteredAlerts = {};\n\nfor (let key in state.alerts) {\n    if (key != \"action.payload.id\") {\n        filteredAlerts[key] = state.alerts[key];\n    }\n}\n```\n\n\nExcept you can still access the ```\naction.payload.id```\n with ```\n_```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to use reduction on an array in Fortran?\r\n                \r\nI'm just starting to learn openMP and I have the following...\n\n```\n    do 100 k=1,lines\n\n    !$OMP PARALLEL DO PRIVATE(dotprod) REDUCTION(+:co(k),si(k))\n     do 110,i=1,ION_COUNT\n      dotprod=(rx(k)*x(i)+ry(k)*y(i)...)\n      co(k)=co(k)+COS(dotprod)\n      si(k)=si(k)+SIN(dotprod)\n     110 continue\n    !$OMP END PARALLEL DO\n```\n\n\nI've figured out (i think) that I need to do a reduction on co(k) and si(k) if I want to add them correctly, but as far as I can tell, you can't have an array like that within the reduction clause. How can I go about doing this?\n    ", "Answer": "\r\nIf I understand correctly, using temporary variables would work:\n\n```\ndo 100 k=1,lines\n\nco_tmp = 0.0\nsi_tmp = 0.0\n!$OMP PARALLEL DO PRIVATE(dotprod) REDUCTION(+:co_tmp,si_tmp)\n do 110,i=1,ION_COUNT\n  dotprod=(rx(k)*x(i)+ry(k)*y(i)...)\n  co_tmp=co_tmp+COS(dotprod)\n  si_tmp=si_tmp+SIN(dotprod)\n 110 continue\n!$OMP END PARALLEL DO\n\nco(k) = co_tmp\nsi(k) = si_tmp\n\n100 continue\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Multiple If Statement Reduction\r\n                \r\nI'm looking for some help cleaning up the code below and reducing the number of lines. Is there a way to not set anything if the get returns a null?\n\n```\n            if (map.get(\"cpn_rate\") != null) {\n                collateral.setCoupon(new BigDecimal(map.get(\"cpn_rate\")));\n            }\n            if (map.get(\"Price\") != null) {\n                collateral.setPrice(new BigDecimal(map.get(\"Price\")));\n            }\n            if (map.get(\"Par\") != null) {\n                collateral.setPar(new BigDecimal(map.get(\"Par\")));\n            }\n            if (map.get(\"mkt_val\") != null) {\n                collateral.setMarketValue(new BigDecimal(map.get(\"mkt_val\")));\n            }\n            if (map.get(\"Accrued Intr\") != null) {\n                collateral.setAccurInterest(new BigDecimal(map.get(\"Accrued Intr\")));\n            }\n            if (map.get(\"Total Market Value\") != null) {\n                collateral.setTotMktValue(new BigDecimal(map.get(\"Total Market Value\")));\n            }\n```\n\n    ", "Answer": "\r\nThe simple answer to the overt question of, \"can I make this more concise/terse\" is \"no\".  You're not really going to get what you're looking for in making this more terse or concise, nor would ```\ncomputeIfPresent```\n really give you what you're looking for and keep your code readable.\n\nThe issue is that, when you go to retrieve a key from your map, you're putting it in a different field in your ```\ncollateral```\n instance.  This means that trivial solutions such as looping over the map won't satisfy since you're not going to be able to get the exact field you need to map to without getting deep into reflection.\n\nThe code you have here, albeit verbose, is perfectly readable and reasonable to any other maintainer to understand what's going on.  I see no incentive to change it.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Library for noise reduction? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 5 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI am currently doing a project capturing audio stream from microphone.\nThe stream I captured was full of background noise. I am wondering if there is library provided for removing the noise.\n\nIf not, what kinds of algorithm I should look for?\n    ", "Answer": "\r\nI haven't found a good library for audio noise reduction. However SoX is a really good open source command line sound processing utility. Check out their documentation page, specifically noiseprof and noisered. \n\nWhat I would suggest doing to integrate it into your application, is to take the SoX executable and make it an embedded resource in your application. Then whenever you need to perform noise reduction, extract the resource to a temporary directory and call Process.Start() in order to execute SoX.\n\nHope this helps!\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Spark ML (Dataframe) and dimension reduction\r\n                \r\nThere does not appear to be classes available to perform dimension reduction if using the newer Dataframe APIs (i.e., not RDD). Nor is there any documentation (https://spark.apache.org/docs/latest/ml-guide.html).\n\nCan someone tell me the approach for dimension reduction using classes from spark.ml (Dataframe) packages rather than spark.mllib (RDD)...? Is it possible?\n    ", "Answer": "\r\n```\nPCA```\n is available for new ML API. You'll find details in:\n\n\nML guide\nAPI docs (Scala, Python)\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Cuda reduction in nested for loops\r\n                \r\nI have a problem concerning some kind of reduction in CUDA. \n\n```\ndistance```\n is a matrix with ```\ngridSize*numberOfAngles```\n elements, ```\nfftData```\n is a matrix with ```\nnumberOfAngles*NFFT```\n elements. ```\ngrid_magnitude```\n is the result-matrix where I want to store my calculation result and it has ```\ngridSize```\n elements.\n\nI want to calculate the index in ```\nfftData```\n that corresponds to one specific value in ```\ndistance```\n. After that, the value at this index in ```\nfftData```\n is added to ```\ngrid_magnitude```\n at the corresponding ```\ngridPoint```\n. \n\nThis is my code so far:\n\n```\n__global__ void calcBackProjection(cuFloatComplex* fftData, \n                                   cuFloatComplex* grid_magnitude,\n                                   float* distance,\n                                   int gridSize,\n                                   int numberOfAngles,\n                                   float rangeBin,\n                                   int NFFT)\n{\n int gridPointIdx = threadIdx.x + blockIdx.x*blockDim.x;\n\n while(gridPointIdx < gridSize)\n {\n    for(int angleIdx = 0; angleIdx < numberOfAngles; angleIdx++)\n    {       \n        //find rangeBin in fftData corresponding to distance\n        float curDistance = distance[gridPointIdx + angleIdx*gridSize];\n        int wantedIdx = floor(curDistance / rangeBin);\n        if(wantedIdx < NFFT)\n            {                                   \n                grid_magnitude[gridPointIdx + angleIdx*gridSize] =  \n              addCmplx(fftData[wantedIdx + angleIdx*NFFT], grid_magnitude[gridPointIdx +     \n                angleIdx*gridSize]);\n\n            }                   \n    }\n    gridPointIdx += blockDim.x * gridDim.x;     \n }   \n}\n```\n\n\n```\ngridPointIdx```\n should be unique for every thread and so each thread should be writing at a different location in ```\ngrid_magnitude```\n. But it seems that this is not working, because no change is made on ```\ngrid_magnitude```\n. \n\nWhat am I missing?\n\nI didn't manage to do this in full parallel 2-dimensional indexing, maybe I should use shared memory, but how do I part grid_magnitude to be used partly by the threads?\n    ", "Answer": "\r\nI changed the code a litte.\n\n```\n__global__ void calcBackProjection(cuFloatComplex* fftData, cuFloatComplex* grid_magnitude,\nfloat* distance, int gridSize, int numberOfAngles, float rangeBin, int NFFT){\nint gridPointIdx = threadIdx.x + blockIdx.x*blockDim.x;\nwhile(gridPointIdx < gridSize){\n    for(int angleIdx = 0; angleIdx < numberOfAngles; angleIdx++){       \n        float curDistance = distance[gridPointIdx + angleIdx*gridSize];\n        int wantedIdx = ceil(curDistance / rangeBin) - 1;\n        if(wantedIdx){\n            int fftIdx = wantedIdx + angleIdx*NFFT;\n            int gridIdx=  gridPointIdx + angleIdx*gridSize;\n            if((fftIdx < NFFT*numberOfAngles) && (gridIdx < gridSize*numberOfAngles)){                  \n                grid_magnitude[gridIdx] = cuCaddf(fftData[fftIdx], grid_magnitude[gridIdx]);\n            }\n        }\n    }\n    gridPointIdx += blockDim.x * gridDim.x;     }}\n```\n\n\nThe problem seems to be, that the variables curDistance and wantedIdx are not evaluated by the compiler. When I want to know the values, it says \"has no value at the target location\". This seems to be the reason why there is an access violation detected at ```\ngrid_magnitude[gridIdx] = cuCaddf(fftData[fftIdx], grid_magnitude[gridIdx]);```\n\nI looked at some other answers about this problem, like here and here, but these have not helped me a lot.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "PCA Dimensionality Reduction\r\n                \r\nI am trying to perform PCA reducing 900 dimensions to 10. So far I have:\n\n```\ncovariancex = cov(labels);\n[V, d] = eigs(covariancex, 40);\n\npcatrain = (trainingData - repmat(mean(traingData), 699, 1)) * V;\npcatest = (test - repmat(mean(trainingData), 225, 1)) * V;\n```\n\n\nWhere ```\nlabels```\n are ```\n1x699```\n labels for chars (1-26). ```\ntrainingData```\n is ```\n699x900```\n, 900-dimensional data for the images of 699 chars. ```\ntest```\n is ```\n225x900```\n, 225 900-dimensional chars.\n\nBasically I want to reduce this down to ```\n225x10```\n i.e. 10 dimensions but am kind of stuck at this point.\n    ", "Answer": "\r\nThe covariance is supposed to implemented in your ```\ntrainingData```\n:\n\n```\nX = bsxfun(@minus, trainingData, mean(trainingData,1));           \ncovariancex = (X'*X)./(size(X,1)-1);                 \n\n[V D] = eigs(covariancex, 10);   % reduce to 10 dimension\n\nXtest = bsxfun(@minus, test, mean(trainingData,1));  \npcatest = Xtest*V;\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Multiple If Statement Reduction\r\n                \r\nI'm looking for some help cleaning up the code below and reducing the number of lines. Is there a way to not set anything if the get returns a null?\n\n```\n            if (map.get(\"cpn_rate\") != null) {\n                collateral.setCoupon(new BigDecimal(map.get(\"cpn_rate\")));\n            }\n            if (map.get(\"Price\") != null) {\n                collateral.setPrice(new BigDecimal(map.get(\"Price\")));\n            }\n            if (map.get(\"Par\") != null) {\n                collateral.setPar(new BigDecimal(map.get(\"Par\")));\n            }\n            if (map.get(\"mkt_val\") != null) {\n                collateral.setMarketValue(new BigDecimal(map.get(\"mkt_val\")));\n            }\n            if (map.get(\"Accrued Intr\") != null) {\n                collateral.setAccurInterest(new BigDecimal(map.get(\"Accrued Intr\")));\n            }\n            if (map.get(\"Total Market Value\") != null) {\n                collateral.setTotMktValue(new BigDecimal(map.get(\"Total Market Value\")));\n            }\n```\n\n    ", "Answer": "\r\nThe simple answer to the overt question of, \"can I make this more concise/terse\" is \"no\".  You're not really going to get what you're looking for in making this more terse or concise, nor would ```\ncomputeIfPresent```\n really give you what you're looking for and keep your code readable.\n\nThe issue is that, when you go to retrieve a key from your map, you're putting it in a different field in your ```\ncollateral```\n instance.  This means that trivial solutions such as looping over the map won't satisfy since you're not going to be able to get the exact field you need to map to without getting deep into reflection.\n\nThe code you have here, albeit verbose, is perfectly readable and reasonable to any other maintainer to understand what's going on.  I see no incentive to change it.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Library for noise reduction? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 5 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI am currently doing a project capturing audio stream from microphone.\nThe stream I captured was full of background noise. I am wondering if there is library provided for removing the noise.\n\nIf not, what kinds of algorithm I should look for?\n    ", "Answer": "\r\nI haven't found a good library for audio noise reduction. However SoX is a really good open source command line sound processing utility. Check out their documentation page, specifically noiseprof and noisered. \n\nWhat I would suggest doing to integrate it into your application, is to take the SoX executable and make it an embedded resource in your application. Then whenever you need to perform noise reduction, extract the resource to a temporary directory and call Process.Start() in order to execute SoX.\n\nHope this helps!\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "What is the OpenCL-equivalent of gpu::reduce()? (OpenCV 2.4)\r\n                \r\nI want to rewrite my nVIDIA-enabled code and its ```\ngpu::```\n functions as OpenCL-enabled code. I can't find an ```\nocl::reduce()```\n function on OCL's Matrix reductions to do a row sum, akin to ```\ngpu::reduce()```\n. Any ideas?\n    ", "Answer": "\r\nSince OpenCL is a pretty basic language and does not provide that much high level functions you may need to write the Kernel etc. by yourself.\n\nYou may want to check this answer:\n\nReduction of matrix rows in OpenCL \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "rabbitmq-server start up error at application_master.erl, line 138\r\n                \r\nrabbitmq-server start up error,who can explain this message.\n\n```\n    =CRASH REPORT==== 16-Jun-2017::15:28:12 ===\n          crasher:\n            initial call: application_master:init/4\n            pid: <0.154.0>\n            registered_name: []\n            exception exit: {bad_return,\n                             {{rabbit,start,[normal,[]]},\n                              {'EXIT',\n                               {rabbit,failure_during_boot,\n                                {boot_step,recovery,\n                                 {badmatch,\n                                  {error,\n                                   {{{function_clause,\n                                      [{rabbit_queue_index,journal_minus_segment1,\n                                        [{no_pub,no_del,ack,'...'},\n                                         {{'...'},no_del,'...'}],\n                                        []},\n                                       {rabbit_queue_index,\n                                        '-journal_minus_segment/2-fun-0-',4,[]},\n                                       {array,sparse_foldl_3,7,[{'...'},{'...'}]},\n                                       {array,sparse_foldl_2,9,[{'...'},{'...'}]},\n                                       {rabbit_queue_index,\n                                        '-recover_journal/1-fun-0-',1,[]},\n                                       {lists,map,2,[{'...'},{'...'},'...']},\n                                       {rabbit_queue_index,segment_map,2,[]},\n                                       {rabbit_queue_index,recover_journal,1,[]}]},\n                                     {gen_server2,call,[<0.216.0>,out,infinity]}},\n                                    {child,undefined,msg_store_persistent,\n                                     {rabbit_msg_store,start_link,\n                                      [msg_store_persistent,\"/srv/r...\",[],\n                                       {#Fun<rabbit_queue_index.2.60982745>,\n                                        {start,'...'}}]},\n                                     transient,4294967295,worker,\n                                     [rabbit_msg_store]}}}}}}}}}\n              in function  application_master:init/4 (application_master.erl, line 138)\n            ancestors: [<0.153.0>]\n            messages: [{'EXIT',<0.155.0>,normal}]\n            links: [<0.153.0>,<0.7.0>]\n            dictionary: []\n```\n\n\ntrap_exit: true\n    status: running\n    heap_size: 121393\n    stack_size: 24\n    reductions: 6273\n  neighbours:\n\nWe'd like to understand what this crash report means.\n    ", "Answer": "\r\nI recommend asking questions like these on the RabbitMQ Users list. You did not provide many details, such as RabbitMQ server version, OS version, environment, configuration, etc etc. A stack trace is rarely enough to diagnose or reproduce a problem.\n\nBased on the stack trace, it looks as though a directory whose name starts with ```\n/srv/r...```\n may be missing or not have expected data in it.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CRC calculation reduction\r\n                \r\nI have one math and programming related question about CRC calculations, to avoid recompute full CRC for a block when you must change only a small portion of it.\n\nMy problem is the following: I have a 1K block of 4 byte structures, each one representing a data field. The full 1K block has a CRC16 block at the end, computed over the full 1K. When I have to change only a 4 byte structure, I should recompute the CRC of the full block but I'm searching for a more efficient solution to this problem. Something where:\n\n\nI take the full 1K block current CRC16\nI compute something on the old 4 byte block\nI \"subtract\" something obtained at step 2 from the full 1K CRC16\nI compute something on the new 4 byte block\nI \"add\" something obtained at step 4 to the result obtained at step 3\n\n\nTo summarize, I am thinking about something like this:\n\nCRC(new-full) = [CRC(old-full) - CRC(block-old) + CRC(block-new)]\n\nBut I'm missing the math behind and what to do to obtain this result, considering also a \"general formula\".\n\nThanks in advance.\n    ", "Answer": "\r\nTake your initial 1024-byte block A and your new 1024-byte block B. Exclusive-or them to get block C. Since you only changed four bytes, C will be bunch of zeros, four bytes which are the exclusive-or of the previous and new four bytes, and a bunch more zeros.\n\nNow compute the CRC-16 of block C, but without any pre or post-processing. We will call that CRC-16'. (I would need to see the specific CRC-16 you're using to see what that processing is, if anything.) Exclusive-or the CRC-16 of block A with the CRC-16' of block C, and you now have the CRC-16 of block B.\n\nAt first glance, this may not seem like much of a gain compared to just computing the CRC of block B. However there are tricks to rapidly computing the CRC of a bunch of zeros. First off, the zeros preceding the four bytes that were changed give a CRC-16' of zero, regardless of how many zeros there are. So you just start computing the CRC-16' with the exclusive-or of the previous and new four bytes.\n\nNow you continue to compute the CRC-16' on the remaining n zeros after the changed bytes. Normally it takes O(n) time to compute a CRC on n bytes. However if you know that they are all zeros (or all some constant value), then it can be computed in O(log n) time. You can see an example of how this is done in zlib's ```\ncrc32_combine()```\n routine, and apply that to your CRC.\n\nGiven your CRC-16/DNP parameters, the ```\nzeros()```\n routine below will apply the requested number of zero bytes to the CRC in O(log n) time.\n\n```\n// Return a(x) multiplied by b(x) modulo p(x), where p(x) is the CRC\n// polynomial, reflected. For speed, this requires that a not be zero.\nuint16_t multmodp(uint16_t a, uint16_t b) {\n    uint16_t m = (uint16_t)1 << 15;\n    uint16_t p = 0;\n    for (;;) {\n        if (a & m) {\n            p ^= b;\n            if ((a & (m - 1)) == 0)\n                break;\n        }\n        m >>= 1;\n        b = b & 1 ? (b >> 1) ^ 0xa6bc : b >> 1;\n    }\n    return p;\n}\n\n// Table of x^2^n modulo p(x).\nuint16_t const x2n_table[] = {\n    0x4000, 0x2000, 0x0800, 0x0080, 0xa6bc, 0x55a7, 0xfc4f, 0x1f78,\n    0xa31f, 0x78c1, 0xbe76, 0xac8f, 0xb26b, 0x3370, 0xb090\n};\n\n// Return x^(n*2^k) modulo p(x).\nuint16_t x2nmodp(size_t n, unsigned k) {\n    k %= 15;\n    uint16_t p = (uint16_t)1 << 15;\n    for (;;) {\n        if (n & 1)\n            p = multmodp(x2n_table[k], p);\n        n >>= 1;\n        if (n == 0)\n            break;\n        if (++k == 15)\n            k = 0;\n    }\n    return p;\n}\n\n// Apply n zero bytes to crc.\nuint16_t zeros(uint16_t crc, size_t n) {\n    return multmodp(x2nmodp(n, 3), crc);\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA reduction - basics\r\n                \r\nI'm trying to sum an array with this code and I am stuck. I probably need some \"CUDA for dummies tutorial\", because I spent so much time with such basic operation and I can't make it work.\n\nHere is a list of things I don't understand or I'm unsure of:\n\n\nWhat number of blocks (dimGrid) should I use?\nI think that should be ```\nN/dimBlock.x/2```\n (N=length of input array), because at the beginning of the kernel, data are loaded and added to shared memory from two \"blocks\" of global memory\nIn original code there was ```\nblockSize```\n. I replaced it with ```\nblockDim.x```\n because I don't know how these variables differ. But when ```\nblockSize```\n = ```\nblockDim.x```\n, then ```\ngridSize = blockDim.x*2*gridDim.x```\n doesn't make sense to me - ```\ngridSize```\n will be greater than N. What is the difference between *Dim.x and *Size in a context of 1D array?\nMain logic - in kernel, each block sums 2*dimBlock(threads in block) numbers. When N = 262144 and dimBlock = 128, kernel returns 1024 array of partial sums. Then I run kernel again, result = 4 partial sums. Finally, in last run, single sum is returned, because array is processed by single block.\nI sum binary array. In the first run, I can use ```\nuchar4```\n for input data. In second and third run, I will use ```\nint```\n.\n\n\nTell me please what am I missing\n\nThanks\n\n```\n__global__ void sum_reduction(uchar4* g_idata, int* g_odata, int N) { \n\nextern __shared__ int s_data[]; \n\nunsigned int tid = threadIdx.x;\nunsigned int i = blockIdx.x*(blockDim.x*2) + tid;\nunsigned int gridSize = blockDim.x*2*gridDim.x;\n\nwhile (i < N) {\n    s_data[tid] += g_idata[i].x + g_idata[i+blockDim.x].x +\n            g_idata[i].y + g_idata[i+blockDim.x].y +\n            g_idata[i].z + g_idata[i+blockDim.x].z +\n            g_idata[i].w + g_idata[i+blockDim.x].w;\n    i += gridSize;\n}\n__syncthreads();\n\nif (tid < 64) {\n    s_data[tid] += s_data[tid + 64];\n}\n__syncthreads(); \n\nif (tid < 32) { \n    volatile int *s_ptr = s_data; \n    s_ptr[tid] += s_ptr[tid + 32];\n    s_ptr[tid] += s_ptr[tid + 16];\n    s_ptr[tid] += s_ptr[tid + 8]; \n    s_ptr[tid] += s_ptr[tid + 4];\n    s_ptr[tid] += s_ptr[tid + 2]; \n    s_ptr[tid] += s_ptr[tid + 1]; \n} \nif (tid == 0) {\n    g_odata[blockIdx.x] = s_data[0];\n} \n}\n\n\nmain{\n...\ndim3 dimBlock(128);\ndim3 dimGrid(N/dimBlock.x);\nsum_reduction<<<dimGrid, dimBlock>>>(in, out, N);\n...\n}\n```\n\n    ", "Answer": "\r\nCalling the kernel like this fixes the problem.\n\n```\ndim3 dimBlock(128);\ndim3 dimGrid(N/dimBlock.x);\nint smemSize = dimBlock.x * sizeof(int);\nsum_reduction<<<dimGrid, dimBlock, smemSize>>>(in, out, N);    \n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction to Clique prob\r\n                \r\nSubgraph isomorphism \n\nWe have the graphs G_1=(V_1,E_1), G_2=(V_2,E_2). \n\nQuestion: Is the graph G_1 isomorphic to a subgraph of G_2 ? \n\n(i.e. is there a subset of vertices of G_2, V ⊆ V_2 and subset of the edges of G_2, E ⊆ E_2 such that |V|=|V_1| and |E|=|E_1| and is there a one-to-one matching of the vertices of G_1 at the subset of vertices V of G_2, f:V_1 -> V such that {u,v} ∈ E_1 <=> { f(u),f(v) } ∈ E)\n\n\nShow that the problem Subgraph isomorphism belongs to NP.\nShow that the problem is NP-complete reducing the problem Clique to it. (Hint: consider that the graph G_1 is complete)\n\n\nI have tried the following:\n\n\nA non-deterministic Turing machine first \"guesses\" the subset of nodes V and the subset of edges E of G_2 and after that it verifies that |V|=|V_1| and |E|=|E_1| and that there is a one-to-one correspondence f: V_1 -> V such that {u,v} ∈ E_1 <=> { f(u), f(v) } ∈ E .\n\n\nSince there are O(|V_2|^2) different pairs of vertices, the check requires polynomial time. So the problem belongs to NP.\n\n\nLet (G,k) an arbitrary instance of the clique problem, where k is the number of vertices of the clique.\n\n\nWe can construct an instance of the Subgraph isomorphism problem in polynomial time as follows:\nG_2 is a graph on n vertices.\n\nG_1 is a complete graph on k vertices, for some k <= n.\nLet G=G_2.\nThe problem Subgraph Isomorphism has a solution iff there is a complete subgraph of G_2 with k vertices, i.e. iff the graph G has a complete subgraph with k vertices. \n\nThus, the instance of the problem Subgraph Isomorphism has a solution iff the initial instance of the problem Clique has a solution.\n\nTherefore, the problem Subgraph Isomorphism is NP-complete.\n\nCould you tell me if it is right or if I could improve something?\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "C#/.net audio noise reduction [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is off-topic. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\r\n                \r\n                    \r\n                        Closed 11 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI'm developing a class library, that allows to do some manipulations with streamed audio (it is working with buffered samples, retrieved from recording device - so basically I've got an bytes array) for researsh purposes. \n\nThe problem is that i need to add noise reduction - there is need to capture noise profile, and apply noise reduction to each sample before applying sample processing & analysis, and I was not able to find any algorithms/samples/libraries that could be applied.\n\nAny help would be appreciated. Thanks in advance.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Opencl Reduction is not as expected\r\n                \r\nI'm pretty a novice about opencl. I have tried about \"get the summation of all cubes of every element in an array\". Here's my kernel code:\n\n```\nkernel void cubeSum(global float *input,\n                    local float *prods,\n                    global float *output )\n{\n    int gid = get_global_id( 0 );\n    int tnum = get_local_id( 0 ); // thread number\n    int wgNum = get_group_id( 0 ); // work-group number\n    int numItems = get_local_size( 0 );\n    prods[ tnum ] = input[ gid ] * input[ gid ] * input[gid]; // cube\n\n    for (int offset = 1; offset < numItems; offset *= 2) {\n        int mask = 2 * offset - 1;\n        barrier(CLK_LOCAL_MEM_FENCE);\n        if ( (tnum & mask) == 0 ) {\n            prods[tnum] += prods[tnum + offset];\n        }\n    }\n    barrier(CLK_LOCAL_MEM_FENCE);\n\n    if ( tnum == 0 )\n        output[wgNum] = prods[0];\n}\n```\n\n\nI can't figure out why my result is not the same with sequential result. When the array is from 0 to 511, my result is sequential result minus 2048; when the array is from 0 to 1023, my result is sequential result plus 16384.\n\nI will try to figure it out myself while I'm waiting for you answers.\n\nAnother question is I found it is hard to debug kernel code since the dataset is quite big and it runs concurrently. Any advice for debugging?\n\nAll the advices are appreciated =).\n\nBy the way, here's my host code:\n\n```\n#include <stdio.h>\n#include <stdio.h>\n#include <math.h>\n#include <string.h>\n#include <stdlib.h>\n#include <OpenCL/opencl.h>\n\n#define NUM_ELEMENTS (512)\n#define LOCAL_SIZE (512)\n#define MAX_SOURCE_SIZE (0x100000)\n\nint main(int argc, const char * argv[])\n{\n    float data[NUM_ELEMENTS];           //hA\n    float sum;\n    float sumTest;\n\n    size_t global;\n    size_t local;\n    size_t numWorkGroups;\n    size_t dataSize;\n    size_t resultsSize;\n\n    cl_device_id device;\n    cl_context context;\n    cl_command_queue cmdQueue;\n    cl_program program;\n    cl_kernel kernel;\n\n    cl_mem input;\n    cl_mem output;\n\n    FILE *fp;\n    //failed to use relative path here. permission problem?\n    char fileName[] = \"/Users/sure/USC/590/cubeSum/cubeSum/cubeSum.cl\";\n    char *source_str;\n    size_t source_size;\n\n    /* カーネルを含むソースコードをロード */\n    fp = fopen(fileName, \"r\");\n    if (!fp) {\n        fprintf(stderr, \"Failed to load kernel.\\n\");\n        exit(1);\n    }\n    source_str = (char*)malloc(MAX_SOURCE_SIZE);\n    source_size = fread( source_str, 1, MAX_SOURCE_SIZE, fp);\n    fclose( fp );\n\n    //allocate the host memory buffers:\n    int i = 0;\n    unsigned int count = NUM_ELEMENTS;\n    for (i = 0; i < count; i++) {\n        data[i] = i;\n    }\n\n    //array size in bytes (will need this later):\n    dataSize = NUM_ELEMENTS * sizeof(float);\n\n    //opencl function status\n    cl_int status;\n\n    // Connect to a compute device\n    //\n    int gpu = 1;\n\n    status = clGetDeviceIDs(NULL, gpu ? CL_DEVICE_TYPE_GPU : CL_DEVICE_TYPE_CPU, 1, &device, NULL);\n    if (status != CL_SUCCESS)\n    {\n        printf(\"Error: Failed to create a device group!\\n\");\n        return EXIT_FAILURE;\n    }\n\n    //create an Opencl context\n    context = clCreateContext(NULL, 1, &device, NULL, NULL, &status);\n\n    //create a command queue\n    cmdQueue = clCreateCommandQueue( context, device, 0, &status );\n\n    //allocate memory buffers on the device\n    input = clCreateBuffer( context, CL_MEM_READ_ONLY, dataSize, NULL, &status );   //dA\n\n    //TODO: at this line, I don't have the value of local which is calculated by clGetKernelWorkGroupInfo\n    //need to figure out a way to avoid hardcode it.\n    output = clCreateBuffer( context, CL_MEM_WRITE_ONLY, sizeof(float) * NUM_ELEMENTS / LOCAL_SIZE, NULL, &status ); //dC\n\n    // enqueue the 2 commands to write data into the device buffers:\n    status = clEnqueueWriteBuffer( cmdQueue, input, CL_FALSE, 0, dataSize, data, 0, NULL, NULL );\n\n    // create the kernel program on the device:\n    program = clCreateProgramWithSource(context, 1, (const char **) & source_str, (const size_t *)&source_size, &status);\n    if (!program)\n    {\n        printf(\"Error: Failed to create compute program!\\n\");\n        return EXIT_FAILURE;\n    }\n\n\n    // Build the program executable\n    //\n    status = clBuildProgram(program, 0, NULL, NULL, NULL, NULL);\n    if (status != CL_SUCCESS)\n    {\n        size_t len;\n        char buffer[2048];\n\n        printf(\"Error: Failed to build program executable!\\n\");\n        clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, sizeof(buffer), buffer, &len);\n        printf(\"%s\\n\", buffer);\n        exit(1);\n    }\n\n    //create compute kernel\n    kernel = clCreateKernel( program, \"cubeSum\", &status );\n\n    // Get the maximum work group size for executing the kernel on the device\n    //\n    status = clGetKernelWorkGroupInfo(kernel, device, CL_KERNEL_WORK_GROUP_SIZE, sizeof(local), &local, NULL);\n    if (status != CL_SUCCESS)\n    {\n        printf(\"Error: Failed to retrieve kernel work group info! %d\\n\", status);\n        exit(1);\n    }\n\n    global = count;\n\n    numWorkGroups = global / local;\n    float results[numWorkGroups];        //hC\n    resultsSize = numWorkGroups * sizeof(float);\n\n    //set kernel parameter\n    status = clSetKernelArg( kernel, 0, sizeof(cl_mem), &input );\n    status = clSetKernelArg( kernel, 1, sizeof(float), NULL );\n    status = clSetKernelArg( kernel, 2, sizeof(cl_mem), &output );\n\n    // Execute the kernel over the entire range of our 1d input data set\n    // using the maximum number of work group items for this device\n    //\n    status = clEnqueueNDRangeKernel(cmdQueue, kernel, 1, NULL, &global, &local, 0, NULL, NULL);\n    if (status)\n    {\n        printf(\"Error: Failed to execute kernel!\\n\");\n        return EXIT_FAILURE;\n    }\n\n    clFinish(cmdQueue);\n    status = clEnqueueReadBuffer( cmdQueue, output, CL_TRUE, 0, resultsSize, results, 0, NULL, NULL );\n\n    // Validate our results\n    //\n    sum = 0;\n\n    for (int i=0; i<numWorkGroups; i++) {\n        sum += results[i];\n    }\n\n    sumTest = 0;\n    for(i = 0; i < count; i++)\n    {\n        sumTest += data[i] * data[i] * data[i];\n    }\n\n    // Print a brief summary detailing the results\n    //\n    printf(\"Computed '%f/%f'!\\n\", sum, sumTest);\n\n    // Shutdown and cleanup\n    //\n    clReleaseMemObject(input);\n    clReleaseMemObject(output);\n    clReleaseProgram(program);\n    clReleaseKernel(kernel);\n    clReleaseCommandQueue(cmdQueue);\n    clReleaseContext(context);\n\n    return 0;\n\n}\n```\n\n\nEDIT: Just found another thing. My code is correct if I just sum all element without cube/square. Thus, I'm gonna figure out how cube affect to my program.\n    ", "Answer": "\r\nYou appear to only be allocating 4-bytes of local memory:\n\n```\nstatus = clSetKernelArg( kernel, 1, sizeof(float), NULL );\n```\n\n\nThis should be the total amount of local memory required for that argument by the entire work-group. In the case of your kernel, this is ```\n(work-group-size * sizeof(float))```\n.\n\nSo, you should instead have something like this:\n\n```\nstatus = clSetKernelArg( kernel, 1, local*sizeof(float), NULL );\n```\n\n\n\n\nThe discrepancies you are seeing are likely coming from the limitations of floating point, since you are summing some very large numbers. If you initialise your inputs with smaller numbers (e.g. ```\ndata[i] = i*0.01;```\n), you should get results equal to your sequential implementation (I've verified this on my own system). This is why you don't see the errors when you remove the cube.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Is the accumulator of reduce in Java 8 allowed to modify its arguments?\r\n                \r\nIn Java 8, Stream has a method reduce:\n\n```\nT reduce(T identity, BinaryOperator<T> accumulator);\n```\n\n\nIs the accumulator operator allowed to modify either of its arguments?  I presume not since the JavaDoc says the accumulator should be NonInterfering, though all examples talk of modifying the collection, rather than modifying the elements of the collection.\n\nSo, for a concrete example, if we have\n\n```\n integers.reduce(0, Integer::sum);\n```\n\n\nand suppose for a moment that ```\nInteger```\n was mutable, would ```\nsum```\n be allowed to modify its first parameter by adding to it (in place) the value of its second parameter?\n\nI presume not, but I would also like an example of where this Interfering causes a problem.\n    ", "Answer": "\r\nNo.  The accumulator should not modify its arguments; it takes two values and produces a new value.  If you want to use mutation in the course of accumulation (e.g., accumulating strings into a StringBuffer instead of concatenating), use ```\nStream.collect()```\n, which is designed for this.\n\nHere's an example of code that produces the wrong answer if you try this.  Let's say you want to do addition with a hypothetical MutableInteger class: \n\n```\n// Don't do this\nMutableInteger result = stream.reduce(new MutableInteger(0), (a,b) -> a.add(b.get()));\n```\n\n\nOne reason this gets the wrong answer is that if we break the computation up in parallel, now two computations are sharing the same mutable starting value.  Note that:\n\n```\na + b + c + d\n= 0 + a + b + 0 + c + d  // 0 denotes identity\n= (0 + a + b) + (0 + c + d) // associativity\n```\n\n\nso we are free to split the stream, compute the partial sums ```\n0 + a + b```\n and ```\n0 + c + d```\n, and then add the results.  But if they are sharing the same identity value, and that value is mutated as a result of one of the computations, the other may start with the wrong value.  \n\n(Note further that the implementation would be allowed to do this even for sequential computations, if it thought that was worthwhile.)\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Dimensionality reduction in Matlab\r\n                \r\nI want to reduce the dimension of data to ndim dimensions in MATLAB. I am using ```\npcares```\n to reduce dimension but the result (i.e. residuals,reconstructed) has the same dimensions as the data and not ```\nndim```\n. How can I project the residuals to ```\nndim```\n dimensions only.\n\n```\n[residuals,reconstructed] = pcares(X,ndim)\n```\n\n\nSample code\n\n```\nMU = [0 0];\nSIGMA = [4/3 2/3; 2/3 4/3];\nX = mvnrnd(MU,SIGMA,1000);\n[residuals,reconstructed] = pcares(X,1)\n```\n\n\nNow I expect the residuals to have 1 dimensions i.e. the data ```\nX```\n projected to prime component as I specified it as ```\npcares(X,1)```\n. But here both residuals and reconstructed have the same of 2. \n    ", "Answer": "\r\n```\npcares```\n is doing its job.  If you read the documentation, you call the function this way:\n\n```\n[RESIDUALS,RECONSTRUCTED] = pcares(X,NDIM);\n```\n\n\n```\nRESIDUALS```\n returns the residuals for each data point by retaining the first ```\nNDIM```\n dimensions of your data and ```\nRECONSTRUCTED```\n is the reconstructed data using the first ```\nNDIM```\n principal components.\n\nIf you want the actual projection vectors, you need to use ```\npca```\n instead.  You'd call it this way:\n\n```\n[coeff,score] = pca(x);\n```\n\n\nIn fact, this is what ```\npcares```\n does under the hood but it also reconstructs the data for you using the above outputs.  ```\ncoeff```\n returns the principal coefficients for your data while ```\nscore```\n returns the actual projection vectors themselves.  ```\nscore```\n is such that each column  is a single projection vector.  It should be noted that these are ordered with respect to dominance as you'd expect with PCA... and so the first column is the most dominant direction, second column second dominant direction, etc.\n\nOnce you call the above, you simply index into ```\ncoeff```\n and ```\nscore```\n to retain whatever components you want.  In your case, you just want the first component, and so do this:\n\n```\nc = coeff(1);\ns = score(:,1);\n```\n\n\nIf you want to reconstruct the data given your projection vectors, referring to the second last line of code, it's simply:\n\n```\n[coeff,score] = pca(x);\nn = size(X,1);\nndim = 1; %// For your case\nreconstructed = repmat(mean(X,1),n,1) + score(:,1:ndim)*coeff(:,1:ndim)';\n```\n\n\nThe above is basically what ```\npcares```\n does under the hood.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP: reduction and sharing\r\n                \r\nI have following code, which is resulting into erroneous output.\n\n```\n#pragma omp parallel private(i,piold) shared(pi,sign)\n{\n#pragma omp for reduction(+:pi) schedule (static)\n  for (i = 0; i < 100000; i++){\n        piold = pi;\n        pi += sign/(2*i+1);\n        sign=-sign;\n  }\n}\n  pi = 4*pi;\n```\n\n\nI am a bit lost as I am new to OpenMP. What is confusing me is how to pass ```\nsign```\n, ```\npi```\n and ```\npiold```\n between the threads? They cannot be shared as alternate iterations need different values. So one way is to have iterations separated as odd and even, but that seems very inefficient.\n\nAny suggestions in this case?\n    ", "Answer": "\r\nLook like you try to use Leiniz formula to approximate pi. \n\n\nC++ support declaring iterative in for scope, so remove i from the private.\n\nfor (unsigned int i = 0; i < 10000; i++)\nsign can be know base on odd/even property of i, so use (i & 1) check instead and remove the sign from the private\n\n```\n    int sign = 1 - (i & 1);\n    pi += (sign == 1 ? 1f : -1f) / (2 * i + 1);\n```\n\npiold doesn't affect final result pi, remove from the private\n\n\nYou can read http://bisqwit.iki.fi/story/howto/openmp/ for more. Since I don't have a compiler support OpenMP now so I can't test and give you the sample code. But I suggest two case:\n\n\nSeparate (+) and (-) of formula into 2 threads by odd/even of i using openmp section. You will have two section\n\n\ndouble plus = 0.0;\n\n```\nfor (int i = 0; i < 5000; i++)\n        plus += (4 * i + 1);\n```\n\n\nand\n\n```\ndouble minus = 0.0;\nfor (int i = 0; i < 5000; i++)\n   minus += (4 * i + 3);\n```\n\n\nthen \n\n```\npi = plus - minus;\n```\n\n\n\nUse the reduction clause for pi \n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "LDA and Dimensionality Reduction [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     This question does not appear to be about programming within the scope defined in the help center.\r\n                \r\n                    \r\n                        Closed 9 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI have dataset consisting of about 300 objects with 84 features for each object. The objects are already separated into two classes. With PCA I´m able to reduce the dimensionality to about 24. I´m using 3 principle components covering about 96% of the variance of the original data. The problem I have is that PCA doesnt care about the ability to separate the classes from each other. Is there a way to combine PCA for reducing feature space and LDA for finding a discriminance function for those two classes ? \nOr is there a way to use LDA for finding the features that separate two classes in threedimensional space in the best manner ? \n\nI´m kind of irritated because I found this paper but I´m not really understanding. http://faculty.ist.psu.edu/jessieli/Publications/ecmlpkdd11_qgu.pdf\n\nThanks in advance.\n    ", "Answer": "\r\nYou should have a look at this article on principle component regression (PCR, what you want if the variable to be explained is scalar) and partial least squares regression (PLSR) with MATLAB's statistics toolbox. In PCR essentially, you choose the principal components as they most explain the dependent variable. They may not be the ones with the largest variance.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "beta reduction strategies in Haskell\r\n                \r\nThis is my first time learning functional programming. I do understand how simple beta reduction works.\n\nfor example:\n\n```\n(\\x->2*x)5\n```\n\n\nmeans that you substitute the xs with 5.\n\n```\n2*5=10\n```\n\n\nHowever, other examples confuse me\n\n```\n(\\f->f(f 0))(\\x->x+1)\n```\n\n\nWe have learned about some evaluation strategies, head normal form and weak head normal forms.\n\nfrom my notes, I understand that head normal form means no redex expression, while weak head normal form means there is a lambda abstraction.\n\nThis doesn't make any sense to me. Do one of the two apply to this last example? If so, what would be an example of the other strategy?\n    ", "Answer": "\r\nThe term\n\n```\n(\\f -> f (f 0)) (\\x -> x+1)\n```\n\n\nis neither in head normal form nor in weak head normal form. This term is the application of a lambda (specifically, ```\n\\f -> f (f 0)```\n) to a term (specifically, ```\n\\x -> x+1```\n), and so:\n\n\nThere is a redex. Recall that a redex is defined as the application of a lambda to a term. Since there is a redex somewhere in the expression -- and in particular, at the very top level, in this case -- this is not in head normal form.\nThe top level of the term is an application, not a lambda, so this is not in weak head normal form.\n\n\nNeither \"head normal form\" nor \"weak head normal form\" is an evaluation strategy. Forms are adjectives which describe terms; evaluation strategies, in general, are verbs which describe how to change one term into another term.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "PyOpenCL reduction algorithm error\r\n                \r\nRecently I've been trying to learn gpu programming with PyOpenCl, but despite my best efforts, I haven't been able to get the reduction algorithm shown in the code below to run.  Instead, the code returns \n\n\n  RuntimeError: clEnqueueReadBuffer failed: OUT_OF_RESOURCES\n\n\nMy understanding of this error is that it is indicative of either insufficient memory allocation or out of bounds indexing in the Kernel.  For small global sizes (that is, small ```\n(N,A,t)```\n) the code will run successfully, so I suspect the former.  I allocate ```\nnp.dtype(np.float32).itemsize*t```\n bytes to local memory, however, for a work-group size of ```\n(1,1,t)```\n, which I believe should be sufficient.  Does anyone know then why I'm getting this error?  I'm running the Kernel on a NVIDIA GeForce GTX 960 if that helps.\n\n```\nimport numpy as np\nimport pyopencl as cl\n\nnp.random.seed(5)\n\nN=2500*56\nA=6\nt=64\n\nplat = cl.get_platforms()\ndevices = plat[0].get_devices()\nctx = cl.Context([devices[0]])\nqueue = cl.CommandQueue(ctx)\n\nactions=np.random.randint(0,2,(N,A,t)).flatten(order='F')\ntau=np.arange(1,np.add(t,1))\nd=np.random.rand(N).astype(np.float32)\nbaseAct=np.empty((N,A)).astype(np.float32).flatten(order='F')\n\nmf = cl.mem_flags\nactions_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, \nhostbuf=actions)\ntau_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=tau)\nd_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=d)\nloc_buf = cl.LocalMemory(np.dtype(np.float32).itemsize*t)\nbaseAct_buf = cl.Buffer(ctx, mf.WRITE_ONLY, baseAct.nbytes)\n\nprg = cl.Program(ctx, \"\"\"\n    __kernel void calc_baseAct(__global const int *actions,\n    __global const int *tau,\n    __global const float *d,\n    __local float *loc,\n    __global float *baseAct,\n    int N,\n    int A,\n    int t)\n    {\n      int xg = get_global_id(0);\n      int yg = get_global_id(1);\n      int zg = get_global_id(2);\n      int xl = get_local_id(0);\n      int yl = get_local_id(1);\n      int zl = get_local_id(2);\n      int xw = get_group_id(0);\n      int yw = get_group_id(1);\n      int zw = get_group_id(2);\n\n      loc[xl+N*yl+N*A*zl] = actions[xg+N*yg+N*A*zg]*pow(tau[zg],-d[xg]);\n      barrier(CLK_LOCAL_MEM_FENCE);\n\n\n      for(uint s = t/2; s > 0; s >>= 1) {\n        if(zl < s) {\n          loc[xl+N*yl+N*A*zl] += loc[xl+N*yl+N*A*(zl+s)];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n      }    \n      if(zl == 0) baseAct[xw+N*yw+N*A*zw] = loc[xl+N*yl+N*A*zl];\n\n    }\n    \"\"\").build()\n\nprg.calc_baseAct(queue, (N,A,t), (1,1,t), actions_buf, tau_buf, d_buf, \nloc_buf, baseAct_buf, np.int32(N), np.int32(A), np.int32(t))\ncl.enqueue_copy(queue, baseAct, baseAct_buf)\n\nbaseAct=baseAct.reshape((N,A), order='F')\n```\n\n    ", "Answer": "\r\nClearly out of bound access for ```\nloc```\n which is allocated to have 64 elements per workgroup and is accessed with index of ```\nxl+N*yl+N*A*zl```\n where ```\nzl```\n is in range ```\n[0,63]```\n multiplied by ```\nN=2500*56```\n and ```\nA=6```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Cuda reduction in nested for loops\r\n                \r\nI have a problem concerning some kind of reduction in CUDA. \n\n```\ndistance```\n is a matrix with ```\ngridSize*numberOfAngles```\n elements, ```\nfftData```\n is a matrix with ```\nnumberOfAngles*NFFT```\n elements. ```\ngrid_magnitude```\n is the result-matrix where I want to store my calculation result and it has ```\ngridSize```\n elements.\n\nI want to calculate the index in ```\nfftData```\n that corresponds to one specific value in ```\ndistance```\n. After that, the value at this index in ```\nfftData```\n is added to ```\ngrid_magnitude```\n at the corresponding ```\ngridPoint```\n. \n\nThis is my code so far:\n\n```\n__global__ void calcBackProjection(cuFloatComplex* fftData, \n                                   cuFloatComplex* grid_magnitude,\n                                   float* distance,\n                                   int gridSize,\n                                   int numberOfAngles,\n                                   float rangeBin,\n                                   int NFFT)\n{\n int gridPointIdx = threadIdx.x + blockIdx.x*blockDim.x;\n\n while(gridPointIdx < gridSize)\n {\n    for(int angleIdx = 0; angleIdx < numberOfAngles; angleIdx++)\n    {       \n        //find rangeBin in fftData corresponding to distance\n        float curDistance = distance[gridPointIdx + angleIdx*gridSize];\n        int wantedIdx = floor(curDistance / rangeBin);\n        if(wantedIdx < NFFT)\n            {                                   \n                grid_magnitude[gridPointIdx + angleIdx*gridSize] =  \n              addCmplx(fftData[wantedIdx + angleIdx*NFFT], grid_magnitude[gridPointIdx +     \n                angleIdx*gridSize]);\n\n            }                   \n    }\n    gridPointIdx += blockDim.x * gridDim.x;     \n }   \n}\n```\n\n\n```\ngridPointIdx```\n should be unique for every thread and so each thread should be writing at a different location in ```\ngrid_magnitude```\n. But it seems that this is not working, because no change is made on ```\ngrid_magnitude```\n. \n\nWhat am I missing?\n\nI didn't manage to do this in full parallel 2-dimensional indexing, maybe I should use shared memory, but how do I part grid_magnitude to be used partly by the threads?\n    ", "Answer": "\r\nI changed the code a litte.\n\n```\n__global__ void calcBackProjection(cuFloatComplex* fftData, cuFloatComplex* grid_magnitude,\nfloat* distance, int gridSize, int numberOfAngles, float rangeBin, int NFFT){\nint gridPointIdx = threadIdx.x + blockIdx.x*blockDim.x;\nwhile(gridPointIdx < gridSize){\n    for(int angleIdx = 0; angleIdx < numberOfAngles; angleIdx++){       \n        float curDistance = distance[gridPointIdx + angleIdx*gridSize];\n        int wantedIdx = ceil(curDistance / rangeBin) - 1;\n        if(wantedIdx){\n            int fftIdx = wantedIdx + angleIdx*NFFT;\n            int gridIdx=  gridPointIdx + angleIdx*gridSize;\n            if((fftIdx < NFFT*numberOfAngles) && (gridIdx < gridSize*numberOfAngles)){                  \n                grid_magnitude[gridIdx] = cuCaddf(fftData[fftIdx], grid_magnitude[gridIdx]);\n            }\n        }\n    }\n    gridPointIdx += blockDim.x * gridDim.x;     }}\n```\n\n\nThe problem seems to be, that the variables curDistance and wantedIdx are not evaluated by the compiler. When I want to know the values, it says \"has no value at the target location\". This seems to be the reason why there is an access violation detected at ```\ngrid_magnitude[gridIdx] = cuCaddf(fftData[fftIdx], grid_magnitude[gridIdx]);```\n\nI looked at some other answers about this problem, like here and here, but these have not helped me a lot.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Rounding error reduction?\r\n                \r\nConsider the following functions:\n\n```\n#include <iostream>\n#include <iomanip>\n#include <cmath>\n#include <limits>\n\ntemplate <typename Type>\ninline Type a(const Type dx, const Type a0, const Type z0, const Type b1)\n{\n    return (std::sqrt(std::abs(2*b1-z0))*dx)+a0;\n}\n\ntemplate <typename Type>\ninline Type b(const Type dx, const Type a0, const Type z0, const Type a1)\n{\n    return (std::pow((a1-a0)/dx, 2)+ z0)/2;\n}\n\nint main(int argc, char* argv[])\n{\n    double dx = 1.E-6;\n    double a0 = 1;\n    double a1 = 2;\n    double z0 = -1.E7;\n    double b1 = -10;\n    std::cout<<std::scientific;\n    std::cout<<std::setprecision(std::numeric_limits<double>::digits10);\n    std::cout<<a1-a(dx, a0, z0, b(dx, a0, z0, a1))<<std::endl;\n    std::cout<<b1-b(dx, a0, z0, a(dx, a0, z0, b1))<<std::endl;\n    return 0;\n}\n```\n\n\nOn my machine, it returns:\n\n```\n0.000000000000000e+00\n-1.806765794754028e-07\n```\n\n\nInstead of (0, 0). There is a large rounding error with the second expression. \n\nMy question is: how to reduce the rounding error of each function without changing the type (I need to keep these 2 functions declarations (but the formulas can be rearanged): they come from a larger program)?\n    ", "Answer": "\r\nSadly, all of the floating point types are notorious for rounding error.  They can't even store 0.1 without it (you can prove this using long division by hand: the binary equivalent is 0b0.0001100110011001100...).  You might try some workarounds like expanding that pow to a hard-coded multiplication, but you'll ultimately need to code your program to anticipate and minimize the effects of rounding error.  Here are a couple ideas:\n\n\nNever compare floating point values for equality.  Some alternative comparisons I have seen include: abs(a-b) < delta, or percent_difference (a,b) < delta or even abs(a/b-1) < delta, where delta is a \"suitably small\" value you have determined works for this specific test.\nAvoid adding long arrays of numbers into an accumulator; the end of the array may be completely lost to rounding error as the accumulator grows large. In \"Cuda by Example\" by Jason Sanders and Edward Kandrot, the authors recommend recursively adding each pair of elements individually so that each step produces an array half the size of the previous step, until you get a one-element array.\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Data reduction/transformation\r\n                \r\nHas anyone seen any method to reduce the data for reducing the computation amount? What I mean by that is when number of features are huge, one may apply PCA to reduce the dimension and computation. What if we have a handful of features but huge number of data points (time series).\n How can one reduce that?\n    ", "Answer": "\r\nSubsampling is fairly common.\n\nMany statistical properties are well preserved when you subsample. If you have 1000000 points, the mean estimated from just 10000 is already very close; and maybe well within the reliability of your data.\n\nAnother approach is clustering with a simple and fast method such as k-means - and a large k, say sqrt(N). This will approximate your data with a least-squares objective using k data points. (You should also use the weights afterwards, as the resulting vectors will reflect different amounts of data).\n\nLast but not least, many reduction techniques - probably including PCA - can be used on the transposed matrix. Then you reduce the number of instances, not the number of variables. But PCA is fairly expensive and on the transposed matrix, it would scale O(n³). So I would rather consider directly working with a truncated SVD.\n\nBut apparently your data are time series. I would suggest to look for data reduction that integrates your knowledge about what is important here.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Dimensionality reduction in Matlab\r\n                \r\nI want to reduce the dimension of data to ndim dimensions in MATLAB. I am using ```\npcares```\n to reduce dimension but the result (i.e. residuals,reconstructed) has the same dimensions as the data and not ```\nndim```\n. How can I project the residuals to ```\nndim```\n dimensions only.\n\n```\n[residuals,reconstructed] = pcares(X,ndim)\n```\n\n\nSample code\n\n```\nMU = [0 0];\nSIGMA = [4/3 2/3; 2/3 4/3];\nX = mvnrnd(MU,SIGMA,1000);\n[residuals,reconstructed] = pcares(X,1)\n```\n\n\nNow I expect the residuals to have 1 dimensions i.e. the data ```\nX```\n projected to prime component as I specified it as ```\npcares(X,1)```\n. But here both residuals and reconstructed have the same of 2. \n    ", "Answer": "\r\n```\npcares```\n is doing its job.  If you read the documentation, you call the function this way:\n\n```\n[RESIDUALS,RECONSTRUCTED] = pcares(X,NDIM);\n```\n\n\n```\nRESIDUALS```\n returns the residuals for each data point by retaining the first ```\nNDIM```\n dimensions of your data and ```\nRECONSTRUCTED```\n is the reconstructed data using the first ```\nNDIM```\n principal components.\n\nIf you want the actual projection vectors, you need to use ```\npca```\n instead.  You'd call it this way:\n\n```\n[coeff,score] = pca(x);\n```\n\n\nIn fact, this is what ```\npcares```\n does under the hood but it also reconstructs the data for you using the above outputs.  ```\ncoeff```\n returns the principal coefficients for your data while ```\nscore```\n returns the actual projection vectors themselves.  ```\nscore```\n is such that each column  is a single projection vector.  It should be noted that these are ordered with respect to dominance as you'd expect with PCA... and so the first column is the most dominant direction, second column second dominant direction, etc.\n\nOnce you call the above, you simply index into ```\ncoeff```\n and ```\nscore```\n to retain whatever components you want.  In your case, you just want the first component, and so do this:\n\n```\nc = coeff(1);\ns = score(:,1);\n```\n\n\nIf you want to reconstruct the data given your projection vectors, referring to the second last line of code, it's simply:\n\n```\n[coeff,score] = pca(x);\nn = size(X,1);\nndim = 1; %// For your case\nreconstructed = repmat(mean(X,1),n,1) + score(:,1:ndim)*coeff(:,1:ndim)';\n```\n\n\nThe above is basically what ```\npcares```\n does under the hood.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Opencl Reduction is not as expected\r\n                \r\nI'm pretty a novice about opencl. I have tried about \"get the summation of all cubes of every element in an array\". Here's my kernel code:\n\n```\nkernel void cubeSum(global float *input,\n                    local float *prods,\n                    global float *output )\n{\n    int gid = get_global_id( 0 );\n    int tnum = get_local_id( 0 ); // thread number\n    int wgNum = get_group_id( 0 ); // work-group number\n    int numItems = get_local_size( 0 );\n    prods[ tnum ] = input[ gid ] * input[ gid ] * input[gid]; // cube\n\n    for (int offset = 1; offset < numItems; offset *= 2) {\n        int mask = 2 * offset - 1;\n        barrier(CLK_LOCAL_MEM_FENCE);\n        if ( (tnum & mask) == 0 ) {\n            prods[tnum] += prods[tnum + offset];\n        }\n    }\n    barrier(CLK_LOCAL_MEM_FENCE);\n\n    if ( tnum == 0 )\n        output[wgNum] = prods[0];\n}\n```\n\n\nI can't figure out why my result is not the same with sequential result. When the array is from 0 to 511, my result is sequential result minus 2048; when the array is from 0 to 1023, my result is sequential result plus 16384.\n\nI will try to figure it out myself while I'm waiting for you answers.\n\nAnother question is I found it is hard to debug kernel code since the dataset is quite big and it runs concurrently. Any advice for debugging?\n\nAll the advices are appreciated =).\n\nBy the way, here's my host code:\n\n```\n#include <stdio.h>\n#include <stdio.h>\n#include <math.h>\n#include <string.h>\n#include <stdlib.h>\n#include <OpenCL/opencl.h>\n\n#define NUM_ELEMENTS (512)\n#define LOCAL_SIZE (512)\n#define MAX_SOURCE_SIZE (0x100000)\n\nint main(int argc, const char * argv[])\n{\n    float data[NUM_ELEMENTS];           //hA\n    float sum;\n    float sumTest;\n\n    size_t global;\n    size_t local;\n    size_t numWorkGroups;\n    size_t dataSize;\n    size_t resultsSize;\n\n    cl_device_id device;\n    cl_context context;\n    cl_command_queue cmdQueue;\n    cl_program program;\n    cl_kernel kernel;\n\n    cl_mem input;\n    cl_mem output;\n\n    FILE *fp;\n    //failed to use relative path here. permission problem?\n    char fileName[] = \"/Users/sure/USC/590/cubeSum/cubeSum/cubeSum.cl\";\n    char *source_str;\n    size_t source_size;\n\n    /* カーネルを含むソースコードをロード */\n    fp = fopen(fileName, \"r\");\n    if (!fp) {\n        fprintf(stderr, \"Failed to load kernel.\\n\");\n        exit(1);\n    }\n    source_str = (char*)malloc(MAX_SOURCE_SIZE);\n    source_size = fread( source_str, 1, MAX_SOURCE_SIZE, fp);\n    fclose( fp );\n\n    //allocate the host memory buffers:\n    int i = 0;\n    unsigned int count = NUM_ELEMENTS;\n    for (i = 0; i < count; i++) {\n        data[i] = i;\n    }\n\n    //array size in bytes (will need this later):\n    dataSize = NUM_ELEMENTS * sizeof(float);\n\n    //opencl function status\n    cl_int status;\n\n    // Connect to a compute device\n    //\n    int gpu = 1;\n\n    status = clGetDeviceIDs(NULL, gpu ? CL_DEVICE_TYPE_GPU : CL_DEVICE_TYPE_CPU, 1, &device, NULL);\n    if (status != CL_SUCCESS)\n    {\n        printf(\"Error: Failed to create a device group!\\n\");\n        return EXIT_FAILURE;\n    }\n\n    //create an Opencl context\n    context = clCreateContext(NULL, 1, &device, NULL, NULL, &status);\n\n    //create a command queue\n    cmdQueue = clCreateCommandQueue( context, device, 0, &status );\n\n    //allocate memory buffers on the device\n    input = clCreateBuffer( context, CL_MEM_READ_ONLY, dataSize, NULL, &status );   //dA\n\n    //TODO: at this line, I don't have the value of local which is calculated by clGetKernelWorkGroupInfo\n    //need to figure out a way to avoid hardcode it.\n    output = clCreateBuffer( context, CL_MEM_WRITE_ONLY, sizeof(float) * NUM_ELEMENTS / LOCAL_SIZE, NULL, &status ); //dC\n\n    // enqueue the 2 commands to write data into the device buffers:\n    status = clEnqueueWriteBuffer( cmdQueue, input, CL_FALSE, 0, dataSize, data, 0, NULL, NULL );\n\n    // create the kernel program on the device:\n    program = clCreateProgramWithSource(context, 1, (const char **) & source_str, (const size_t *)&source_size, &status);\n    if (!program)\n    {\n        printf(\"Error: Failed to create compute program!\\n\");\n        return EXIT_FAILURE;\n    }\n\n\n    // Build the program executable\n    //\n    status = clBuildProgram(program, 0, NULL, NULL, NULL, NULL);\n    if (status != CL_SUCCESS)\n    {\n        size_t len;\n        char buffer[2048];\n\n        printf(\"Error: Failed to build program executable!\\n\");\n        clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, sizeof(buffer), buffer, &len);\n        printf(\"%s\\n\", buffer);\n        exit(1);\n    }\n\n    //create compute kernel\n    kernel = clCreateKernel( program, \"cubeSum\", &status );\n\n    // Get the maximum work group size for executing the kernel on the device\n    //\n    status = clGetKernelWorkGroupInfo(kernel, device, CL_KERNEL_WORK_GROUP_SIZE, sizeof(local), &local, NULL);\n    if (status != CL_SUCCESS)\n    {\n        printf(\"Error: Failed to retrieve kernel work group info! %d\\n\", status);\n        exit(1);\n    }\n\n    global = count;\n\n    numWorkGroups = global / local;\n    float results[numWorkGroups];        //hC\n    resultsSize = numWorkGroups * sizeof(float);\n\n    //set kernel parameter\n    status = clSetKernelArg( kernel, 0, sizeof(cl_mem), &input );\n    status = clSetKernelArg( kernel, 1, sizeof(float), NULL );\n    status = clSetKernelArg( kernel, 2, sizeof(cl_mem), &output );\n\n    // Execute the kernel over the entire range of our 1d input data set\n    // using the maximum number of work group items for this device\n    //\n    status = clEnqueueNDRangeKernel(cmdQueue, kernel, 1, NULL, &global, &local, 0, NULL, NULL);\n    if (status)\n    {\n        printf(\"Error: Failed to execute kernel!\\n\");\n        return EXIT_FAILURE;\n    }\n\n    clFinish(cmdQueue);\n    status = clEnqueueReadBuffer( cmdQueue, output, CL_TRUE, 0, resultsSize, results, 0, NULL, NULL );\n\n    // Validate our results\n    //\n    sum = 0;\n\n    for (int i=0; i<numWorkGroups; i++) {\n        sum += results[i];\n    }\n\n    sumTest = 0;\n    for(i = 0; i < count; i++)\n    {\n        sumTest += data[i] * data[i] * data[i];\n    }\n\n    // Print a brief summary detailing the results\n    //\n    printf(\"Computed '%f/%f'!\\n\", sum, sumTest);\n\n    // Shutdown and cleanup\n    //\n    clReleaseMemObject(input);\n    clReleaseMemObject(output);\n    clReleaseProgram(program);\n    clReleaseKernel(kernel);\n    clReleaseCommandQueue(cmdQueue);\n    clReleaseContext(context);\n\n    return 0;\n\n}\n```\n\n\nEDIT: Just found another thing. My code is correct if I just sum all element without cube/square. Thus, I'm gonna figure out how cube affect to my program.\n    ", "Answer": "\r\nYou appear to only be allocating 4-bytes of local memory:\n\n```\nstatus = clSetKernelArg( kernel, 1, sizeof(float), NULL );\n```\n\n\nThis should be the total amount of local memory required for that argument by the entire work-group. In the case of your kernel, this is ```\n(work-group-size * sizeof(float))```\n.\n\nSo, you should instead have something like this:\n\n```\nstatus = clSetKernelArg( kernel, 1, local*sizeof(float), NULL );\n```\n\n\n\n\nThe discrepancies you are seeing are likely coming from the limitations of floating point, since you are summing some very large numbers. If you initialise your inputs with smaller numbers (e.g. ```\ndata[i] = i*0.01;```\n), you should get results equal to your sequential implementation (I've verified this on my own system). This is why you don't see the errors when you remove the cube.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Is the accumulator of reduce in Java 8 allowed to modify its arguments?\r\n                \r\nIn Java 8, Stream has a method reduce:\n\n```\nT reduce(T identity, BinaryOperator<T> accumulator);\n```\n\n\nIs the accumulator operator allowed to modify either of its arguments?  I presume not since the JavaDoc says the accumulator should be NonInterfering, though all examples talk of modifying the collection, rather than modifying the elements of the collection.\n\nSo, for a concrete example, if we have\n\n```\n integers.reduce(0, Integer::sum);\n```\n\n\nand suppose for a moment that ```\nInteger```\n was mutable, would ```\nsum```\n be allowed to modify its first parameter by adding to it (in place) the value of its second parameter?\n\nI presume not, but I would also like an example of where this Interfering causes a problem.\n    ", "Answer": "\r\nNo.  The accumulator should not modify its arguments; it takes two values and produces a new value.  If you want to use mutation in the course of accumulation (e.g., accumulating strings into a StringBuffer instead of concatenating), use ```\nStream.collect()```\n, which is designed for this.\n\nHere's an example of code that produces the wrong answer if you try this.  Let's say you want to do addition with a hypothetical MutableInteger class: \n\n```\n// Don't do this\nMutableInteger result = stream.reduce(new MutableInteger(0), (a,b) -> a.add(b.get()));\n```\n\n\nOne reason this gets the wrong answer is that if we break the computation up in parallel, now two computations are sharing the same mutable starting value.  Note that:\n\n```\na + b + c + d\n= 0 + a + b + 0 + c + d  // 0 denotes identity\n= (0 + a + b) + (0 + c + d) // associativity\n```\n\n\nso we are free to split the stream, compute the partial sums ```\n0 + a + b```\n and ```\n0 + c + d```\n, and then add the results.  But if they are sharing the same identity value, and that value is mutated as a result of one of the computations, the other may start with the wrong value.  \n\n(Note further that the implementation would be allowed to do this even for sequential computations, if it thought that was worthwhile.)\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Resolution reduction in JBIG for sequential mode\r\n                \r\nIn JBIG the primarily modes in which the algorithm runs are sequential and progressive mode. In the document provided for the JBIG i.e., T.82 by ITU there is no specific mention that resolution reduction is to be used only for progressive mode.\nDoes that mean that we can also use it to implement the sequential mode too?\nIf yes then am I correct to think that while implementing resolution reduction for the sequential mode we only need to record the number of times the resolution reduction is performed for an image as we do not need to maintain any buffer.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction of internal width\r\n                \r\nCan someone explain to me why I can not reduce the internal width of the borders to make the text more centralized? For some hours I can not find a solution. I am a beginner in this world and would like some help, please.\n\nhttps://i.stack.imgur.com/zRVG6.png\n\nThis is my HTML code:\n\n```\n<!DOCTYPE html>\n<html lang=\"pt-br\">\n<head>\n<meta charset=\"UTF-8\">\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no\">\n<meta name=\"keywords\" content=\"piauí fábio abreu segurança eleições\">\n        <title>Fábio Abreu</title>\n        <meta property=\"og:title\" content=\"Fábio Abreu\">\n<meta name=\"twitter:title\" content=\"Fábio Abreu\">\n        <meta name=\"description\" content=\"Site do deputado federal e secretário de segurança do estado do Piauí, Fábio Abreu.\">\n        <meta property=\"og:description\" content=\"Site do deputado federal e secretário de segurança do estado do Piauí, Fábio Abreu.\">\n<meta name=\"twitter:description\" content=\"Site do deputado federal e secretário de segurança do estado do Piauí, Fábio Abreu.\"> \n\n        <meta property=\"og:image\" content=\"https://inovaweb-production.s3.amazonaws.com/attachment/image/medium_caecb618-6523-409d-8af6-d31dbd7351b0.png\">\n<meta name=\"twitter:image\" content=\"https://inovaweb-production.s3.amazonaws.com/attachment/image/medium_caecb618-6523-409d-8af6-d31dbd7351b0.png\">\n        <meta property=\"og:image:secure_url\" content=\"https://inovaweb-production.s3.amazonaws.com/attachment/image/medium_caecb618-6523-409d-8af6-d31dbd7351b0.png\">\n\n        <meta property=\"og:image:width\" content=\"600\">\n\n        <meta name=\"twitter:card\" content=\"summary_large_image\">\n<meta property=\"og:site_name\" content=\"Fábio Abreu\">\n<meta property=\"og:url\" content=\"http://www.fabioabreupi.com.br/\">\n<meta property=\"og:type\" content=\"website\">\n\n<meta name=\"csrf-param\" content=\"authenticity_token\" />\n<meta name=\"csrf-token\" content=\"vpi2ZdaYpPrfr3GiW3yekW5e5L4yLKfTfcU56OPrD8jGzbLTzDbW9jk89Lge96/mnQzU4zRCbJY2MuADwoWYIw==\" />\n<link rel=\"stylesheet\" media=\"all\" href=\"/assets/site-dc393f9cdad26bd31342ace6c0e567e3893492c1ec06c16976bea61958ce06ad.css\" />\n<link rel=\"stylesheet\" media=\"all\" href=\"/assets/default/css/main-5c3e617fe669fbfd1915fd6ba42ddae97daaf842e3037fde45d55bfdd0f7b4aa.css\" />\n\n<link rel=\"canonical\" href=\"http://www.fabioabreupi.com.br/\">\n\n<link rel='shortcut icon' type='image/x-icon' href='https://inovaweb-production.s3.amazonaws.com/attachment/image/original_95e76849-329a-449b-8e1b-f118083035a4.png' /> <link href='https://fonts.googleapis.com/css?family=Roboto Condensed:400,400i,700,700i' rel='stylesheet'>   <style>\n\n\n            #TopMenu ul#menu-main li a,#TopMenu ul#menu-main li a span.caret{font-size:14px !important;text-transform:uppercase !important}\n            #MainHeader{background-color:rgba(0, 0, 0, 0) !important}\n            h3{font-size:15px !important;font-family:'Roboto Condensed' !important;font-weight:bold !important}\n\n            footer#MainFooter{background-color:rgba(255, 255, 255, 1) !important;border-color:rgba(255, 255, 255, 1) !important}\n    </style>\n\n<style>#TopMenu{\n/* Permalink - use to edit and share this gradient: http://colorzilla.com/gradient-editor/#2722ad+28,ff3b21+100 */\nbackground: #2722ad; /* Old browsers */\nbackground: -moz-linear-gradient(left, #2722ad 28%, #ff3b21 100%); /* FF3.6-15 */\nbackground: -webkit-linear-gradient(left, #2722ad 28%,#ff3b21 100%); /* Chrome10-25,Safari5.1-6 */\nbackground: linear-gradient(to right, #2722ad 28%,#ff3b21 100%); /* W3C, IE10+, FF16+, Chrome26+, Opera12+, Safari7+ */\nfilter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#2722ad', endColorstr='#ff3b21',GradientType=1 ); /* IE6-9 */\n}\n\n#menu-main{\n  padding: 0px 10px 0px 9px;\n  margin: 0px 10px 0px 9px;\n  word-spacing: 0px;\n}\n\n#menu-main li:last-child{\n  /*padding-left: 178px;*/\n  margin-left: 376px;\n  padding-top: 0px;\n  margin-top: 1px; \n}\n\n#menu-main li:last-child{\n    /*background-color: #ffffff;\n    border-radius: 42px;\n    border-color: #ffffff;\n    text-indent:-3.44px;\n    display:inline-block;\n    color:#666666;\n    height:32px;\n    line-height:32px;\n    width:200px;\n    text-decoration:none;\n    text-align:center;\n}/*.classname:hover {\n    background-color:#e9e9e9;\n}.classname:active {\n    position:relative;\n    top:1px;*/\n\n    box-shadow:inset 0px 0px 0px 0px #none;\n    box-shadow:inset 0px 0px 0px 0px #none;\n    box-shadow:inset 0px 0px 0px 0px #none;\n    background-color: none;\n    border-radius:1px;\n    border-radius:1px;\n    border-radius: 66px;\n    border: 1px solid #ffffff;\n    display:inline-block;\n    color:#ffffff;\n    padding:1px 22px;\n}\n.myButton:hover {\n    background-color:#ff00ff;\n}\n.myButton:active {\n    position:relative;\n    top:0px;\n}\n\n#menu-main li:nth-last-child(2){\n  padding-left: 20px;\n}\n#menu-main li:nth-last-child(3){\n  padding-left: 20px;\n}\n#menu-main li:nth-last-child(4){\n  padding-left: 20px;\n}\n#menu-main li:nth-last-child(5){\n  padding-left: 20px;\n}\n\n#MainHeader{\n padding: 0px 0px 0px 5px; \n}\n\n#MainFooter{\n background-color: #00004d \n}\n\n#GroupList1{\n background-color: #ffffff; \n box-shadow: none;\n}\n\n#lws-homesection-title{\n  text-align: center;\n}\n\n#GroupList2{\n display: none; \n}\n.lws-homesection-body{\n font-size: 40px;\n line-height: 1.8;\n}</style>\n</head>\n\n<body class=\"palette-red\">\n\n\n    <div id=\"fb-root\"></div>\n\n    <header id=\"MainHeader\">\n    <div class=\"container\">\n        <div class=\"row vertical-align\">\n            <div class=\"col-xs-12 col-sm-3\">\n                <div class=\"LogoArea\">\n                        <a href=\"/\" id=\"BrandImage\">\n                            <img src=\"https://inovaweb-production.s3.amazonaws.com/attachment/image/medium_caecb618-6523-409d-8af6-d31dbd7351b0.png\" alt=\"Fábio Abreu\" class=\"img-responsive center-block\">\n                        </a>\n\n                </div>\n            </div>\n\n            <div class=\"col-xs-12 col-sm-9\">\n                <div id=\"ContactsTop\">\n\n\n                </div>\n            </div>\n        </div>\n    </div>\n</header>\n\n<nav id=\"TopMenu\" class=\"navbar navbar-inverse\">\n  <div class=\"container\">\n        <div class=\"navbar-header\">\n        <button type=\"button\" class=\"navbar-toggle collapsed\" data-toggle=\"collapse\" data-target=\"#navbar\" aria-expanded=\"false\" aria-controls=\"navbar\">\n          <span class=\"sr-only\">Toggle navigation</span>\n          <span class=\"icon-bar\"></span>\n          <span class=\"icon-bar\"></span>\n          <span class=\"icon-bar\"></span>\n        </button>\n      </div>\n      <div id=\"navbar\" class=\"collapse navbar-collapse\">\n        <ul id=\"menu-main\" class=\"nav navbar-nav\" data-hover=\"dropdown\" data-animations=\"zoomIn\">\n          <li><a href=\"/\" target=\"\">Início</a></li>\n              <li><a href=\"/categories/noticias\">NOTÍCIAS</a></li>\n\n              <li><a href=\"/categories/fotos\">FOTOS</a></li>\n\n              <li><a href=\"/categories/projetos\">PROJETOS</a></li>\n\n              <li><a href=\"/biografia\">BIOGRAFIA</a></li>\n\n              <li><a href=\"www.whatsapp.com.br\">ENTRE EM CONTATO</a></li>\n\n        </ul>\n      </div>\n  </div>\n</nav>\n\n<div id=\"mySlide\">\n    <div style=\"width:100%;\" id=\"bf494d0c-d537-4f36-bdf2-4754dbee0839\" class=\"inner-content-slide carousel slide\" data-ride=\"carousel\" data-interval=\"5000\" data-pause=\"hover\"><ol class=\"carousel-indicators\"><li data-target=\"#bf494d0c-d537-4f36-bdf2-4754dbee0839\" data-slide-to=\"0\" class=\"active\"></li><li data-target=\"#bf494d0c-d537-4f36-bdf2-4754dbee0839\" data-slide-to=\"1\" class=\"\"></li><li data-target=\"#bf494d0c-d537-4f36-bdf2-4754dbee0839\" data-slide-to=\"2\" class=\"\"></li></ol><div class=\"carousel-inner\" role=\"listbox\"><div class=\"item active\"><div data-img-height=\"481\" data-img-width=\"1000\" class=\"item-img\" role=\"img\" aria-label=\"\" style=\"height:500px;background-image:url(https://inovaweb-production.s3.amazonaws.com/attachment/image/large_f5a4a97f-204c-4538-bc79-d4bed4be75f3.jpeg);background-size:cover\"></div></div><div class=\"item \"><div data-img-height=\"480\" data-img-width=\"1000\" class=\"item-img\" role=\"img\" aria-label=\"\" style=\"height:500px;background-image:url(https://inovaweb-production.s3.amazonaws.com/attachment/image/large_06fa6929-4e05-4d40-aebd-99233e3852d6.JPG);background-size:cover\"></div></div><div class=\"item \"><div data-img-height=\"488\" data-img-width=\"1000\" class=\"item-img\" role=\"img\" aria-label=\"\" style=\"height:500px;background-image:url(https://inovaweb-production.s3.amazonaws.com/attachment/image/large_471118d4-7857-496c-b0db-21d5479de784.jpg);background-size:cover\"></div></div></div><a class=\"left carousel-control\" href=\"#bf494d0c-d537-4f36-bdf2-4754dbee0839\" role=\"button\" data-slide=\"prev\"><i class=\"icon-prev\"></i><span class=\"sr-only\">Anterior</span></a><a class=\"right carousel-control\" href=\"#bf494d0c-d537-4f36-bdf2-4754dbee0839\" role=\"button\" data-slide=\"next\"><i class=\"icon-next\"></i><span class=\"sr-only\">Próximo</span></a></div>\n</div>\n\n<div class=\"marketing\">\n    <div class=\"container\"><section data-section-area='V10'><div data-section='home-section' data-area='V10' data-section-id='c60fb099-5b76-4835-a254-f7ead8bde197'><div class='lws-homesection-text-image'><div class='row'><div class='col-xs-12 col-sm-12 col-md-6'><img src='https://inovaweb-production.s3.amazonaws.com/attachment/image/medium_edab1478-6055-4a29-aa01-1fe4426e6ff2.png' class='img-responsive'></div><div class='col-xs-12 col-sm-12 col-md-6'><div class='lws-homesection-body'>Ingressou na Polícia Militar do Piauí em 1993 e possui especialização em Segurança Pública pela Universidade Estadual do Piauí em 2013. Ao tempo de sua eleição como deputado federal pelo PTB em 2014, ostentava a patente de capitão. Licenciou-se do mandato no terceiro governo Wellington Dias para assumir a Secretaria de Segurança Pública sendo convocado o suplente Silas Freire. Ingressou na Polícia Militar do Piauí em 1993 e possui especialização em Segurança Pública pela Universidade Estadual do Piauí em 2013. Ao tempo de sua eleição como deputado federal pelo PTB em 2014, ostentava a patente de capitão. Licenciou-se do mandato no terceiro governo Wellington Dias para assumir a Secretaria de Segurança Pública sendo convocado o suplente Silas Freire.\nIngressou na Polícia Militar do Piauí em 1993 e possui especialização em Segurança Pública pela Universidade Estadual do Piauíem 2013. Ao tempo de sua eleição como deputado federal pelo PTB em 2014, ostentava a patente de capitão. Licenciou-se do mandato no terceiro governo Wellington Dias para assumir a Secretaria de Segurança Pública sendo convocado o suplente Silas Freire.\nIngressou na Polícia Militar do Piauí em 1993 e possui especialização em Segurança Pública pela Universidade Estadual do Piauíem 2013. Ao tempo de sua eleição como deputado federal pelo PTB em 2014, ostentava a patente de capitão. Licenciou-se do mandato no terceiro governo Wellington Dias para assumir a Secretaria de Segurança Pública sendo convocado o suplente Silas Freire.</div></div></div></div></div></section></div>\n\n    <div id=\"GroupList1\">\n        <div class=\"container\">\n            <div class=\"row\">\n                <div class=\"col-xs-12 col-sm-4 col-md-4 col-lg-4\">\n                    <div data-iw-group=\"group1\">\n                        <div role=\"img\" aria-label=\"\" style=\"background-image: url(https://inovaweb-production.s3.amazonaws.com/attachment/image/large_6310756a-7f23-4ba3-bedb-2b317c786ed3.png); background-size: cover\" data-iw-bg=\"https://inovaweb-production.s3.amazonaws.com/attachment/image/large_6310756a-7f23-4ba3-bedb-2b317c786ed3.png\" data-iw-image=\"image_group1\"  data-iw-version=\"medium\" class=\"area-img\"> </div>\n                        <h2 data-iw-line=\"title_group1\">Projetos</h2>\n                        <p data-iw-multiline=\"body_group1\"></p>\n                        <a data-iw-link=\"link_group1\" class=\" btn btn-primary\" href=\"categories/projetos\" role=\"button\">Saiba mais</a>\n                    </div>\n                </div>\n\n                <div class=\"col-xs-12 col-sm-4 col-md-4 col-lg-4\">\n                    <div data-iw-group=\"group2\">\n                        <div role=\"img\" aria-label=\"\" style=\"background-image: url(https://inovaweb-production.s3.amazonaws.com/attachment/image/large_b665fc4e-f95d-4b91-9dea-138d5b287022.png); background-size: cover\" data-iw-bg=\"https://inovaweb-production.s3.amazonaws.com/attachment/image/large_b665fc4e-f95d-4b91-9dea-138d5b287022.png\" data-iw-image=\"image_group2\" data-iw-version=\"medium\" class=\"area-img\"> </div>\n                        <h2 data-iw-line=\"title_group2\">Biografia</h2>\n                        <p data-iw-multiline=\"body_group2\"></p>\n                        <a data-iw-link=\"link_group2\" class=\" btn btn-primary\" href=\"biografia\" role=\"button\">Saiba mais</a>\n                    </div>\n                </div>\n\n                <div class=\"col-xs-12 col-sm-4 col-md-4 col-lg-4\">\n                    <div data-iw-group=\"group3\">\n                        <div role=\"img\" aria-label=\"\" style=\"background-image: url(https://inovaweb-production.s3.amazonaws.com/attachment/image/large_08c91774-9593-498f-b984-e4ac21991226.png); background-size: cover\" data-iw-bg=\"https://inovaweb-production.s3.amazonaws.com/attachment/image/large_08c91774-9593-498f-b984-e4ac21991226.png\" data-iw-image=\"image_group3\" data-iw-version=\"medium\" class=\"area-img\"> </div>\n                        <h2 data-iw-line=\"title_group3\">Fotos</h2>\n                        <p data-iw-multiline=\"body_group3\"></p>\n                        <a data-iw-link=\"link_group3\" class=\" btn btn-primary\" href=\"categories/fotos\" role=\"button\">Saiba mais</a>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n\n    <div class=\"container\"><section data-section-area='V20'><div data-section='home-section' data-area='V20' data-section-id='a40ab6b2-e909-4021-99b5-ac1ebde763d7'><h2 class='lws-homesection-title'>Notícias destaque</h2><div class='lws-homesection-resource-list'><div class='row'><div class='col-xs-12 col-sm-6 col-md-4'><div class='lws-homesection-wrapper-item'><div role='img' aria-label='' class='resource-list-post-thumbnail' style='background-image:url(https://inovaweb-production.s3.amazonaws.com/attachment/image/medium_dc88125b-b8f3-4455-bcbc-de7d9d0720ca.jpg);background-size:cover'></div><h4 class='resource-list-title'>Fábio Abreu troca PTB pelo PR e tenta reeleição a deputado federal</h4><div class='resource-list-excerpt'>No evento de filiação ao PR, Fábio Abreu anunciou sua pré-candidatura a deputado federal e afirmou que está preparado para disputar a reeleição. O ...</div><a href='/posts/fabio-abreu-troca-ptb-pelo-pr-e-tenta-reeleicao-a-deputado-federal' class='btn btn-primary'>Veja mais</a></div></div><div class='col-xs-12 col-sm-6 col-md-4'><div class='lws-homesection-wrapper-item'><div role='img' aria-label='' class='resource-list-post-thumbnail' style='background-image:url(https://inovaweb-production.s3.amazonaws.com/attachment/image/medium_d445acdc-1a7f-4b75-a96c-76f3e0cf67d3.jpg);background-size:cover'></div><h4 class='resource-list-title'>Antes de entregar cargo, Fábio Abreu apresenta Plano de Segurança</h4><div class='resource-list-excerpt'>\nO documento traça uma linha de trabalho para o setor pelos próximoa s20 anos e prevê investimentos que começam com um empréstimo junto ao BNDES p...</div><a href='/posts/antes-de-entregar-cargo-fabio-abreu-apresenta-plano-de-seguranca' class='btn btn-primary'>Veja mais</a></div></div><div class=\"hidden-xs col-sm-12 hidden-md hidden-lg\"></div><div class='col-xs-12 col-sm-6 col-md-4'><div class='lws-homesection-wrapper-item'><div role='img' aria-label='' class='resource-list-post-thumbnail' style='background-image:url(https://inovaweb-production.s3.amazonaws.com/attachment/image/medium_56e67980-ecd2-444e-a0d9-e2c8a8735420.jpg);background-size:cover'></div><h4 class='resource-list-title'>Fábio Abreu destaca falta de atenção a segurança pública</h4><div class='resource-list-excerpt'>\nEm entrevista ao Acorda Piauí, o deputado federal Fábio Abreu (PR), destacou que, enquanto a falta de segurança pública é um dos problemas mais a...</div><a href='/posts/fabio-abreu-destaca-falta-de-atencao-a-seguranca-publica' class='btn btn-primary'>Veja mais</a></div></div><div class=\"hidden-xs hidden-sm col-md-12 col-lg-12\"></div></div></div></div></section></div>\n\n    <div id=\"GroupList2\">\n        <div class=\"container\">\n            <div class=\"GL2-item\" data-iw-group=\"group4\">\n                <div class=\"row\">\n                    <div class=\"col-md-7\">\n                        <h2 data-iw-line=\"title_group4\" class=\"featurette-heading\"></h2>\n                        <p data-iw-multiline=\"body_group4\" class=\"lead\"></p>\n                        <a data-iw-link=\"link_group4\" class=\"hidden btn btn-primary btn-lg\" href=\"\" role=\"button\"></a>\n                    </div>\n                    <div class=\"col-md-5\">\n                        <div role=\"img\" aria-label=\"\" style=\"background-image: url(/assets/no-pixel-d69f97510d506fdff62059b955a572bfebc93e9c0a243ec79beaf245e737b59a.gif); background-size: cover\" data-iw-image=\"image_group4\" data-iw-version=\"medium\"data-iw-bg=\"/assets/no-pixel-d69f97510d506fdff62059b955a572bfebc93e9c0a243ec79beaf245e737b59a.gif\" class=\"imagem-topicos\"></div>\n                    </div>\n                </div>\n            </div>\n\n            <div class=\"GL2-item\" data-iw-group=\"group5\">\n                <div class=\"row\">\n                    <div class=\"col-md-7\">\n                        <h2 data-iw-line=\"title_group5\" class=\"featurette-heading\"></h2>\n                        <p data-iw-multiline=\"body_group5\" class=\"lead\"></p>\n                        <a data-iw-link=\"link_group5\" class=\"hidden btn btn-primary btn-lg\" href=\"\" role=\"button\"></a>\n                    </div>\n                    <div class=\"col-md-5\">\n                        <div role=\"img\" aria-label=\"\" style=\"background-image: url(/assets/no-pixel-d69f97510d506fdff62059b955a572bfebc93e9c0a243ec79beaf245e737b59a.gif); background-size: cover\" data-iw-image=\"image_group5\" data-iw-version=\"medium\"data-iw-bg=\"/assets/no-pixel-d69f97510d506fdff62059b955a572bfebc93e9c0a243ec79beaf245e737b59a.gif\" class=\"imagem-topicos\"></div>\n                    </div>\n                </div>\n            </div>\n\n            <div class=\"GL2-item\" data-iw-group=\"group6\">\n                <div class=\"row\">\n                    <div class=\"col-md-7\">\n                        <h2 data-iw-line=\"title_group6\" class=\"featurette-heading\"></h2>\n                        <p data-iw-multiline=\"body_group6\" class=\"lead\"></p>\n                        <a data-iw-link=\"link_group6\" class=\"hidden btn btn-primary btn-lg\" href=\"\" role=\"button\"></a>\n                    </div>\n                    <div class=\"col-md-5\">\n                        <div role=\"img\" aria-label=\"\" style=\"background-image: url(/assets/no-pixel-d69f97510d506fdff62059b955a572bfebc93e9c0a243ec79beaf245e737b59a.gif); background-size: cover\" data-iw-image=\"image_group6\" data-iw-version=\"medium\"data-iw-bg=\"/assets/no-pixel-d69f97510d506fdff62059b955a572bfebc93e9c0a243ec79beaf245e737b59a.gif\" class=\"imagem-topicos\"></div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n\n    <div class=\"container\"><section data-section-area='V30'></section></div>\n</div>\n\n<footer id=\"MainFooter\">\n    <div class=\"container\">\n        <div class=\"lws-home-sections-footer\">\n            <div class=\"lws-1-col-footer lws-cols-footer-hs\">\n                <div class=\"row\">\n                    <div class=\"col-xs-12\"><section data-section-area='F10'><div data-section='home-section' data-area='F10' data-section-id='f97be27b-c0e9-4229-b584-64ee920ef08a'><img src='https://inovaweb-production.s3.amazonaws.com/attachment/image/original_3939ae3f-43d1-4f63-a724-741c5ae11f47.png' class='img-responsive'></div></section></div>\n                </div>\n            </div>\n\n            <div class=\"lws-2-cols-footer lws-cols-footer-hs\">\n                <div class=\"row\">\n                    <div class=\"col-xs-12 col-sm-6\"><section data-section-area='F20'></section></div>\n                    <div class=\"col-xs-12 col-sm-6\"><section data-section-area='F30'></section></div>\n                </div>\n            </div>\n\n            <div class=\"lws-4-cols-footer lws-cols-footer-hs\">\n                <div class=\"row\">\n                    <div class=\"col-xs-12 col-sm-6 col-md-3\"><section data-section-area='F40'></section></div>\n                    <div class=\"col-xs-12 col-sm-6 col-md-3\"><section data-section-area='F50'></section></div>\n                    <div class=\"col-xs-12 col-sm-6 col-md-3\"><section data-section-area='F60'></section></div>\n                    <div class=\"col-xs-12 col-sm-6 col-md-3\"><section data-section-area='F70'></section></div>\n                </div>\n            </div>\n        </div>\n\n        <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-6\">\n                <ul class=\"contacts_footer list-inline\">\n                    <li></li>\n                    <li></li>\n                    <li></li>\n                </ul>\n\n                <p>© 2018 - Fábio Abreu</p>\n            </div>\n\n            <div class=\"col-xs-12 col-sm-6\">\n            </div>\n        </div>\n\n        <div class=\"lws-home-sections-footer\">\n            <div class=\"lws-1-col-footer lws-cols-footer-hs\">\n                <div class=\"row\">\n                    <div class=\"col-xs-12\"><section data-section-area='F80'><div data-section='home-section' data-area='F80' data-section-id='76c13c4e-249f-4cac-b9ae-e2a56220e80c'><div class='lws-homesection-body'>Todos os direitos reservados.\n</div></div></section></div>\n                </div>\n            </div>\n\n            <div class=\"lws-2-cols-footer lws-cols-footer-hs\">\n                <div class=\"row\">\n                    <div class=\"col-xs-12 col-sm-6\"><section data-section-area='F90'></section></div>\n                    <div class=\"col-xs-12 col-sm-6\"><section data-section-area='F100'></section></div>\n                </div>\n            </div>\n\n            <div class=\"lws-4-cols-footer lws-cols-footer-hs\">\n                <div class=\"row\">\n                    <div class=\"col-xs-12 col-sm-6 col-md-3\"><section data-section-area='F110'></section></div>\n                    <div class=\"col-xs-12 col-sm-6 col-md-3\"><section data-section-area='F120'></section></div>\n                    <div class=\"col-xs-12 col-sm-6 col-md-3\"><section data-section-area='F130'></section></div>\n                    <div class=\"col-xs-12 col-sm-6 col-md-3\"><section data-section-area='F140'></section></div>\n                </div>\n            </div>\n        </div>\n    </div>\n</footer>\n\n\n    <script src=\"/assets/site-fd70af901fb20b8029f75be40e0f5ff5848d5415b704279fd88ad5bc9c0ec4ec.js\"></script>\n\n    <script src=\"/assets/default/js/main-efff309d3722bafb3f5fa5a2092f366e799cac0d724996ca31c7e52b408475ec.js\"></script>\n\n\n\n\n        <div id=\"side-social-networks-loguei\" role=\"group\" class=\"list-unstyled btn-group-vertical\">\n    </div>\n\n\n\n    <aside id=\"AVS-ContactButtons\" class=\"force-neg-right\">\n    <ul class=\"list-unstyled\">\n\n\n\n    </ul>\n</aside>\n\n<script>\n    setTimeout(function(){\n        $('#AVS-ContactButtons ul li a span').css({\n            right: '-200px'\n        });\n    }, 4000);\n</script>\n\n\n\n    <script>\n    window.fbAsyncInit = function() {\n        FB.init({\n            appId      : '1982872918652221',\n            xfbml      : true,\n            version    : 'v2.11'\n        });\n    };\n\n    (function(d, s, id) {\n      var js, fjs = d.getElementsByTagName(s)[0];\n      if (d.getElementById(id)) return;\n      js = d.createElement(s); js.id = id;\n      js.onload = function() { fbAsyncInit(); }\n      js.src = 'https://connect.facebook.net/pt_BR/sdk.js#xfbml=1&version=v2.11&appId=1982872918652221';\n      fjs.parentNode.insertBefore(js, fjs);\n    }(document, 'script', 'facebook-jssdk'));\n</script>   \n    <script type=\"text/javascript\">\n\n            var NEWSLETTER_URL = '/fabio-abreu-1/newsletters';\n\n        var DEFAULT_ADDRESS = '';\n\n        $('.iw-contact-form').submit(function(e){\n            e.preventDefault();var f = $(this);$.ajax({\n                url:'/contact/fabio-abreu-1?authenticity_token=PEYv9F1ndM5YEDywj8kpZEV5LBE4n7ifIIRuXvvtMk1EEytCR8kGwr6DuarKQhgTtiscTD7xc9prc7e12oOlpg%3D%3D',\n                type:'POST',data:f.serialize(),success:function(){\n                    var redir = $('#iw_redir_loguei_cf').length;\n                    if (redir == 0) {alert('Email enviado com sucesso');f[0].reset();} else {\n                        var to = $('#iw_redir_loguei_cf').val(); window.location=to;\n                    }\n                },error:function(){alert('Erro ao enviar email');}\n            });\n        });\n    </script>\n\n\n</body>\n</html>\n```\n\n\nAnd this is my CSS code in moment:\n\n```\n#menu-main li:last-child{\n  margin-left: 376px;\n  padding-top: 0px;\n  margin-top: 1px; \n}\n\n#menu-main li:last-child{\n    box-shadow:inset 0px 0px 0px 0px #none;\n    box-shadow:inset 0px 0px 0px 0px #none;\n    box-shadow:inset 0px 0px 0px 0px #none;\n    background-color: none;\n    border-radius:1px;\n    border-radius:1px;\n    border-radius: 66px;\n    border: 1px solid #ffffff;\n    display:inline-block;\n    color:#ffffff;\n    padding:1px 22px;\n}\n.myButton:hover {\n    background-color:#ff00ff;\n}\n.myButton:active {\n    position:relative;\n    top:0px;\n}\n```\n\n    ", "Answer": "\r\nEDIT (After commenting back and forth):\n\nTo get the text centered and to have the hover effect cover the whole inside of the button, while keeping the same width of the button, change the css like this:\n\nCurrent:\n\n```\n#menu-main li:last-child a {\n    padding: 7px 19px 5px 15px;\n    box-sizing: border-box;\n    padding-border: 42px;\n}\n\n#menu-main li:last-child {\n    box-shadow: inset 0px 0px 0px 0px #none;\n    box-shadow: inset 0px 0px 0px 0px #none;\n    box-shadow: inset 0px 0px 0px 0px #none;\n    background-color: none;\n    border-radius: 8px;\n    border-radius: 8px;\n    border-radius: 66px;\n    border: 1px solid #ffffff;\n    display: flex;\n    color: #ffffff;\n    padding: 0px 20px;\n    text-align: center justify-content:center;\n    align-items: center;\n    height: 40px;\n    line-height: 0px;\n    width: 200px;\n    box-sizing: border-box;\n}\n```\n\n\nChange to (Remove padding on button and text, put border radius on text, set text width and height to the same as the button, and I also deleted stuff that wasn't doing anything/was incorrectly written)\n\nNEW NEW:(Add ```\nwidth: 100%```\n to navbar and a media query for ```\nfloat: right```\n to button on screens of ```\n768px```\n and up):\n\n```\n#menu-main {\n    width: 100%;\n}\n\n#menu-main li:last-child a {\n    padding: 0px;\n    line-height: 40px;\n    border-radius: 66px;\n    height: 100%;\n    width: 100%;\n}\n\n#menu-main li:last-child {\n    border-radius: 66px;\n    border: 1px solid #ffffff;\n    text-align: center;\n    height: 40px;\n    width: 175px;\n    margin-left: 20px;\n    margin-top: 5px;\n    margin-bottom: 5px;\n}\n\n@media only screen and (min-width: 768px) {\n    #menu-main li:last-child {\n        float: right;\n    }\n}\n```\n\n\nhttp://res.cloudinary.com/maxmaxs5/image/upload/v1533423313/Test.png\n\nGlad to help!\n\n\n\nI believe you are wanting to change the padding value on this section?:\n\n```\n#menu-main li:last-child{\n    box-shadow:inset 0px 0px 0px 0px #none;\n    box-shadow:inset 0px 0px 0px 0px #none;\n    box-shadow:inset 0px 0px 0px 0px #none;\n    background-color: none;\n    border-radius:1px;\n    border-radius:1px;\n    border-radius: 66px;\n    border: 1px solid #ffffff;\n    display:inline-block;\n    color:#ffffff;\n    //padding:1px 22px;\n    padding:1px 10px;\n}\n```\n\n\nYou might want to fiddle with the second pixel value till it gets what you want but 10px might be it.. ```\npadding:1px 10px;```\n\n\nWhen setting ```\npadding```\n, the first value is top/bottom, and the second is left/right. So adjusting the second value will change how much padding the sides of the text get.\n\nHopefully this helps, let me know if you have any questions!\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "crossfilter reduction is crashing\r\n                \r\nI am unable to render a dc.js stacked bar chart successfully and I receive a console error \n\n\n  unable to read property 'Total' of undefined\n\n\nI am new to the library and suspect my group or reduce is not successfully specified.\n\nHow do I resolve this issue?  \n\n```\n $scope.riskStatusByMonth = function(){\n\n    var data = [ \n                    {\"Month\":\"Jan\",\"High\":12},{\"Month\":\"Jan\",\"Med\":14},{\"Month\":\"Jan\",\"Low\":2},{\"Month\":\"Jan\",\"Closed\":8},\n                    {\"Month\":\"Feb\",\"High\":12},{\"Month\":\"Feb\",\"Med\":14},{\"Month\":\"Feb\",\"Low\":2},{\"Month\":\"Feb\",\"Closed\":8},\n                    {\"Month\":\"Mar\",\"High\":12},{\"Month\":\"Mar\",\"Med\":14},{\"Month\":\"Mar\",\"Low\":2},{\"Month\":\"Mar\",\"Closed\":8},\n                    {\"Month\":\"Apr\",\"High\":12},{\"Month\":\"Apr\",\"Med\":14},{\"Month\":\"Apr\",\"Low\":2},{\"Month\":\"Apr\",\"Closed\":8},\n                    {\"Month\":\"May\",\"High\":12},{\"Month\":\"May\",\"Med\":14},{\"Month\":\"May\",\"Low\":2},{\"Month\":\"May\",\"Closed\":8},\n                    {\"Month\":\"Jun\",\"High\":12},{\"Month\":\"Jun\",\"Med\":14},{\"Month\":\"Jun\",\"Low\":2},{\"Month\":\"Jun\",\"Closed\":8},\n                    {\"Month\":\"Jul\",\"High\":12},{\"Month\":\"Jul\",\"Med\":14},{\"Month\":\"Jul\",\"Low\":2},{\"Month\":\"Jul\",\"Closed\":8},\n                    {\"Month\":\"Aug\",\"High\":12},{\"Month\":\"Aug\",\"Med\":14},{\"Month\":\"Aug\",\"Low\":2},{\"Month\":\"Aug\",\"Closed\":8},\n                    {\"Month\":\"Sep\",\"High\":12},{\"Month\":\"Sep\",\"Med\":14},{\"Month\":\"Sep\",\"Low\":2},{\"Month\":\"Sep\",\"Closed\":8},\n                    {\"Month\":\"Oct\",\"High\":12},{\"Month\":\"Oct\",\"Med\":14},{\"Month\":\"Oct\",\"Low\":2},{\"Month\":\"Oct\",\"Closed\":8},\n                    {\"Month\":\"Nov\",\"High\":12},{\"Month\":\"Nov\",\"Med\":14},{\"Month\":\"Nov\",\"Low\":2},{\"Month\":\"Nov\",\"Closed\":8},\n                    {\"Month\":\"Dec\",\"High\":8},{\"Month\":\"Dec\",\"Med\":6},{\"Month\":\"Dec\",\"Low\":13},{\"Month\":\"Dec\",\"Closed\":8},\n               ]\n\n    data.forEach(function(x) {\n      x.Total = 0;\n    });\n\n    var ndx = crossfilter(data)\n\n    var xdim = ndx.dimension(function (d) {return d.Month;});\n\n    function root_function(dim,stack_name) {\n        return dim.group().reduce(\n      function(p, v) {\n        p[v[stack_name]] = (p[v[stack_name]] || 0) + v.High;\n        return p;},\n        function(p, v) {\n        p[v[stack_name]] = (p[v[stack_name]] || 0) + v.Med;\n        return p;},\n        function(p, v) {\n        p[v[stack_name]] = (p[v[stack_name]] || 0) + v.Low;     <-------------------here is where error occurs\n        return p;},\n        function(p, v) {\n        p[v[stack_name]] = (p[v[stack_name]] || 0) + v.Closed;\n        return p;}, \n      function() {\n        return {};\n      });}\n\n    var ydim = root_function(xdim,'Total')\n\n    function sel_stack(i) {\n    return function(d) {\n      return d.value[i];\n    };}\n\n    $scope.monthlyRiskStatus = dc.barChart(\"#risk-status-by-month\");\n\n    $scope.monthlyRiskStatus\n      .x(d3.scaleLinear().domain(xdim))\n      .dimension(xdim)\n      .group(ydim, '1', sel_stack(\"Jan\"))\n      .xUnits(dc.units.ordinal);\n\n\n    month = [null,'Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'];\n    for(var i = 2; i<=12; ++i)\n      $scope.monthlyRiskStatus.stack(ydim, ''+i, sel_stack(month[i]));\n\n    $scope.monthlyRiskStatus.render();\n}\n```\n\n    ", "Answer": "\r\ngroup.reduce() takes three arguments: add, remove, init.\n\nYou are passing 5.\n\nLooks like it is trying to call the third one as the initializer, with no arguments, so therefore ```\nv```\n is undefined.\n\nhow to stack by level\n\nIt looks like what you're really trying to do is group by month (X axis) and then stack by status or level. Here's one way to do that.\n\nFirst, you're on the right track with a function that takes a stack name, but we'll want it to take all of the stack names:\n\n```\nfunction root_function(dim,stack_names) {\n    return dim.group().reduce(\n  function(p, v) {\n    stack_names.forEach(stack_name => { // 1\n      if(v[stack_name] !== undefined) // 2\n          p[stack_name] = (p[v[stack_name]] || 0) + v[stack_name] // 3\n    });\n    return p;}, \n  function(p, v) {\n    stack_names.forEach(stack_name => { // 1\n      if(v[stack_name] !== undefined) // 2\n          p[stack_name] = (p[v[stack_name]] || 0) + v[stack_name] // 3\n    });\n    return p;}, \n  function() {\n    return {};\n  });}\n```\n\n\n\nIn the add and reduce functions, we'll loop over all the stack names\nStack names are fields which may or may not exist in each row. If the stack name exists in the current row...\nWe'll add or subtract the row field ```\nstack_name```\n from the field with the same name in the current bin.\n\n\nWe'll define both ```\nlevels```\n and ```\nmonths```\n arrays. ```\nlevels```\n will be used for stacking and ```\nmonths```\n will be used for the ordinal X domain:\n\n```\nvar levels = ['High', 'Med', 'Low', 'Closed']\nvar months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'];\n```\n\n\nWhen we define the group, we'll pass ```\nlevels```\n to ```\nroot_function()```\n:\n\n```\nvar ygroup = root_function(xdim,levels)\n```\n\n\nI see you had some confusion between the English/math definition of \"dimension\" and the crossfilter dimension. Yes, in English \"Y\" would be a dimension, but in crossfilter and dc.js, \"dimensions\" are what you aggregate on, and groups are the aggregations that often go into Y. (Naming things is difficult.)\n\nWe'll use an ordinal scale (you had half ordinal half linear, which won't work):\n\n```\n$scope.monthlyRiskStatus\n  .x(d3.scaleOrdinal().domain(months))\n  .dimension(xdim)\n  .group(ygroup, levels[0], sel_stack(levels[0]))\n  .xUnits(dc.units.ordinal);\n```\n\n\nPassing the months to the domain of the ordinal scale tells dc.js to draw the bars in that order. (Warning: it's a little more complicated for line charts because you also have to sort the input data.)\n\nNote we are stacking by level, not by month. Also here:\n\n```\nfor(var i = 1; i<levels.length; ++i)\n  $scope.monthlyRiskStatus.stack(ygroup, levels[i], sel_stack(levels[i]));\n```\n\n\nLet's also add a legend, too, so we know what we're looking at:\n\n```\n  .margins({left:75, top: 0, right: 0, bottom: 20})\n  .legend(dc.legend())\n```\n\n\n\n\nDemo fiddle.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMp with custom reduction for GMP addition\r\n                \r\nI have\n\n```\nmpf_t omp_mpf_add(mpf_t out, mpf_t in) {\n  mpf_add(out, out, in);\n  return out;\n}\n```\n\n\nAnd i want to make this function as an openmp reduction\n\n```\n# pragma omp declare reduction (mpf_add:mpf_t:omp_mpf_add(omp_out,omp_in)) \\\n    initializer(omp_priv=omp_orig)\n```\n\n\nBut i get ```\nerror: reduction type cannot be an array type```\n error.\n\nwhat should i change so that the function will work and i can use it with\n\n```\n#pragma omp parallel for reduction(mpf_add:x)\n```\n\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CRC calculation reduction\r\n                \r\nI have one math and programming related question about CRC calculations, to avoid recompute full CRC for a block when you must change only a small portion of it.\n\nMy problem is the following: I have a 1K block of 4 byte structures, each one representing a data field. The full 1K block has a CRC16 block at the end, computed over the full 1K. When I have to change only a 4 byte structure, I should recompute the CRC of the full block but I'm searching for a more efficient solution to this problem. Something where:\n\n\nI take the full 1K block current CRC16\nI compute something on the old 4 byte block\nI \"subtract\" something obtained at step 2 from the full 1K CRC16\nI compute something on the new 4 byte block\nI \"add\" something obtained at step 4 to the result obtained at step 3\n\n\nTo summarize, I am thinking about something like this:\n\nCRC(new-full) = [CRC(old-full) - CRC(block-old) + CRC(block-new)]\n\nBut I'm missing the math behind and what to do to obtain this result, considering also a \"general formula\".\n\nThanks in advance.\n    ", "Answer": "\r\nTake your initial 1024-byte block A and your new 1024-byte block B. Exclusive-or them to get block C. Since you only changed four bytes, C will be bunch of zeros, four bytes which are the exclusive-or of the previous and new four bytes, and a bunch more zeros.\n\nNow compute the CRC-16 of block C, but without any pre or post-processing. We will call that CRC-16'. (I would need to see the specific CRC-16 you're using to see what that processing is, if anything.) Exclusive-or the CRC-16 of block A with the CRC-16' of block C, and you now have the CRC-16 of block B.\n\nAt first glance, this may not seem like much of a gain compared to just computing the CRC of block B. However there are tricks to rapidly computing the CRC of a bunch of zeros. First off, the zeros preceding the four bytes that were changed give a CRC-16' of zero, regardless of how many zeros there are. So you just start computing the CRC-16' with the exclusive-or of the previous and new four bytes.\n\nNow you continue to compute the CRC-16' on the remaining n zeros after the changed bytes. Normally it takes O(n) time to compute a CRC on n bytes. However if you know that they are all zeros (or all some constant value), then it can be computed in O(log n) time. You can see an example of how this is done in zlib's ```\ncrc32_combine()```\n routine, and apply that to your CRC.\n\nGiven your CRC-16/DNP parameters, the ```\nzeros()```\n routine below will apply the requested number of zero bytes to the CRC in O(log n) time.\n\n```\n// Return a(x) multiplied by b(x) modulo p(x), where p(x) is the CRC\n// polynomial, reflected. For speed, this requires that a not be zero.\nuint16_t multmodp(uint16_t a, uint16_t b) {\n    uint16_t m = (uint16_t)1 << 15;\n    uint16_t p = 0;\n    for (;;) {\n        if (a & m) {\n            p ^= b;\n            if ((a & (m - 1)) == 0)\n                break;\n        }\n        m >>= 1;\n        b = b & 1 ? (b >> 1) ^ 0xa6bc : b >> 1;\n    }\n    return p;\n}\n\n// Table of x^2^n modulo p(x).\nuint16_t const x2n_table[] = {\n    0x4000, 0x2000, 0x0800, 0x0080, 0xa6bc, 0x55a7, 0xfc4f, 0x1f78,\n    0xa31f, 0x78c1, 0xbe76, 0xac8f, 0xb26b, 0x3370, 0xb090\n};\n\n// Return x^(n*2^k) modulo p(x).\nuint16_t x2nmodp(size_t n, unsigned k) {\n    k %= 15;\n    uint16_t p = (uint16_t)1 << 15;\n    for (;;) {\n        if (n & 1)\n            p = multmodp(x2n_table[k], p);\n        n >>= 1;\n        if (n == 0)\n            break;\n        if (++k == 15)\n            k = 0;\n    }\n    return p;\n}\n\n// Apply n zero bytes to crc.\nuint16_t zeros(uint16_t crc, size_t n) {\n    return multmodp(x2nmodp(n, 3), crc);\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "warp shuffling to reduction of arrays with any length\r\n                \r\nI am working on a Cuda kernel which performs vector dot product (A x B). I assumed that the length of each vector is multiple of 32 (32,64, ...) and defined the block size to be equal to the length of the array. Each thread in the block multiplies one element of A to the corresponding element of B (thread i ==>psum = A[i]xB[i]). After multiplication, I used the following functions which used warp shuffling technique to perform reduction and calculate the sum all multiplications.\n\n```\n__inline__ __device__\nfloat warpReduceSum(float val) {\n    int warpSize =32;\n    for (int offset = warpSize/2; offset > 0; offset /= 2)\n        val += __shfl_down(val, offset);\n    return val;\n}\n\n__inline__ __device__\nfloat blockReduceSum(float val) {\n    static __shared__ int shared[32]; // Shared mem for 32 partial sums\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n    val = warpReduceSum(val);         // Each warp performs partial reduction\n    if (lane==0) \n        shared[wid]=val;              // Write reduced value to shared memory\n    __syncthreads();                  // Wait for all partial reductions\n    //read from shared memory only if that warp existed\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;\n    if (wid==0) \n        val = warpReduceSum(val);     // Final reduce within first warp\n    return val;\n}\n```\n\n\nI simply call blockReduceSum(psum) which psum is the multiplication of two elements by a thread. \n\nThis approach doesn't work when the length of the array is not multiple of 32, so my question is, can we change this code so that it also works for any length? or is it impossible because if the length of the array is not multiple of 32, some warps have elements belonging more than one array?\n    ", "Answer": "\r\nFirst of all, depending on the GPU you are using, performing dot product with just 1 block will probably not be very efficient (as long as you are not batching several dot products in 1 kernel, each done by a single block).\n\nTo answer your question: you can reuse the code you have written by just calling your kernel with the number of threads being the closest multiple of 32 higher than ```\nN```\n (length of the array) and introducing ```\nif```\n statement before calling to ```\nblockReduceSum```\n that would like this:\n\n```\n__global__ void kernel(float * A, float * B, int N) {\n    float psum = 0;\n    if(threadIdx.x < N) //threadIDx.x because your are using single block, you will need to change it to more general id once you move to multiple blocks\n        psum = A[threadIdx.x] * B[threadIdx.x];\n    blockReduceSum(psum);\n    //The rest of computation\n}\n```\n\n\nThat way, threads that do not have array element associated with them, but that need to be there due to use of ```\n__shfl```\n, will contribute 0 to the sum.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CRC calculation reduction\r\n                \r\nI have one math and programming related question about CRC calculations, to avoid recompute full CRC for a block when you must change only a small portion of it.\n\nMy problem is the following: I have a 1K block of 4 byte structures, each one representing a data field. The full 1K block has a CRC16 block at the end, computed over the full 1K. When I have to change only a 4 byte structure, I should recompute the CRC of the full block but I'm searching for a more efficient solution to this problem. Something where:\n\n\nI take the full 1K block current CRC16\nI compute something on the old 4 byte block\nI \"subtract\" something obtained at step 2 from the full 1K CRC16\nI compute something on the new 4 byte block\nI \"add\" something obtained at step 4 to the result obtained at step 3\n\n\nTo summarize, I am thinking about something like this:\n\nCRC(new-full) = [CRC(old-full) - CRC(block-old) + CRC(block-new)]\n\nBut I'm missing the math behind and what to do to obtain this result, considering also a \"general formula\".\n\nThanks in advance.\n    ", "Answer": "\r\nTake your initial 1024-byte block A and your new 1024-byte block B. Exclusive-or them to get block C. Since you only changed four bytes, C will be bunch of zeros, four bytes which are the exclusive-or of the previous and new four bytes, and a bunch more zeros.\n\nNow compute the CRC-16 of block C, but without any pre or post-processing. We will call that CRC-16'. (I would need to see the specific CRC-16 you're using to see what that processing is, if anything.) Exclusive-or the CRC-16 of block A with the CRC-16' of block C, and you now have the CRC-16 of block B.\n\nAt first glance, this may not seem like much of a gain compared to just computing the CRC of block B. However there are tricks to rapidly computing the CRC of a bunch of zeros. First off, the zeros preceding the four bytes that were changed give a CRC-16' of zero, regardless of how many zeros there are. So you just start computing the CRC-16' with the exclusive-or of the previous and new four bytes.\n\nNow you continue to compute the CRC-16' on the remaining n zeros after the changed bytes. Normally it takes O(n) time to compute a CRC on n bytes. However if you know that they are all zeros (or all some constant value), then it can be computed in O(log n) time. You can see an example of how this is done in zlib's ```\ncrc32_combine()```\n routine, and apply that to your CRC.\n\nGiven your CRC-16/DNP parameters, the ```\nzeros()```\n routine below will apply the requested number of zero bytes to the CRC in O(log n) time.\n\n```\n// Return a(x) multiplied by b(x) modulo p(x), where p(x) is the CRC\n// polynomial, reflected. For speed, this requires that a not be zero.\nuint16_t multmodp(uint16_t a, uint16_t b) {\n    uint16_t m = (uint16_t)1 << 15;\n    uint16_t p = 0;\n    for (;;) {\n        if (a & m) {\n            p ^= b;\n            if ((a & (m - 1)) == 0)\n                break;\n        }\n        m >>= 1;\n        b = b & 1 ? (b >> 1) ^ 0xa6bc : b >> 1;\n    }\n    return p;\n}\n\n// Table of x^2^n modulo p(x).\nuint16_t const x2n_table[] = {\n    0x4000, 0x2000, 0x0800, 0x0080, 0xa6bc, 0x55a7, 0xfc4f, 0x1f78,\n    0xa31f, 0x78c1, 0xbe76, 0xac8f, 0xb26b, 0x3370, 0xb090\n};\n\n// Return x^(n*2^k) modulo p(x).\nuint16_t x2nmodp(size_t n, unsigned k) {\n    k %= 15;\n    uint16_t p = (uint16_t)1 << 15;\n    for (;;) {\n        if (n & 1)\n            p = multmodp(x2n_table[k], p);\n        n >>= 1;\n        if (n == 0)\n            break;\n        if (++k == 15)\n            k = 0;\n    }\n    return p;\n}\n\n// Apply n zero bytes to crc.\nuint16_t zeros(uint16_t crc, size_t n) {\n    return multmodp(x2nmodp(n, 3), crc);\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "warp shuffling to reduction of arrays with any length\r\n                \r\nI am working on a Cuda kernel which performs vector dot product (A x B). I assumed that the length of each vector is multiple of 32 (32,64, ...) and defined the block size to be equal to the length of the array. Each thread in the block multiplies one element of A to the corresponding element of B (thread i ==>psum = A[i]xB[i]). After multiplication, I used the following functions which used warp shuffling technique to perform reduction and calculate the sum all multiplications.\n\n```\n__inline__ __device__\nfloat warpReduceSum(float val) {\n    int warpSize =32;\n    for (int offset = warpSize/2; offset > 0; offset /= 2)\n        val += __shfl_down(val, offset);\n    return val;\n}\n\n__inline__ __device__\nfloat blockReduceSum(float val) {\n    static __shared__ int shared[32]; // Shared mem for 32 partial sums\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n    val = warpReduceSum(val);         // Each warp performs partial reduction\n    if (lane==0) \n        shared[wid]=val;              // Write reduced value to shared memory\n    __syncthreads();                  // Wait for all partial reductions\n    //read from shared memory only if that warp existed\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;\n    if (wid==0) \n        val = warpReduceSum(val);     // Final reduce within first warp\n    return val;\n}\n```\n\n\nI simply call blockReduceSum(psum) which psum is the multiplication of two elements by a thread. \n\nThis approach doesn't work when the length of the array is not multiple of 32, so my question is, can we change this code so that it also works for any length? or is it impossible because if the length of the array is not multiple of 32, some warps have elements belonging more than one array?\n    ", "Answer": "\r\nFirst of all, depending on the GPU you are using, performing dot product with just 1 block will probably not be very efficient (as long as you are not batching several dot products in 1 kernel, each done by a single block).\n\nTo answer your question: you can reuse the code you have written by just calling your kernel with the number of threads being the closest multiple of 32 higher than ```\nN```\n (length of the array) and introducing ```\nif```\n statement before calling to ```\nblockReduceSum```\n that would like this:\n\n```\n__global__ void kernel(float * A, float * B, int N) {\n    float psum = 0;\n    if(threadIdx.x < N) //threadIDx.x because your are using single block, you will need to change it to more general id once you move to multiple blocks\n        psum = A[threadIdx.x] * B[threadIdx.x];\n    blockReduceSum(psum);\n    //The rest of computation\n}\n```\n\n\nThat way, threads that do not have array element associated with them, but that need to be there due to use of ```\n__shfl```\n, will contribute 0 to the sum.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Color reduction (in Java)\r\n                \r\nI would like to find a way to take JPEG (or GIF/PNG) images and reduce the amount of colors to e.g. 20. Could someone recommend some library or other reference? Also source codes in other languages are welcome.\n    ", "Answer": "\r\nTake a look at the Java Advanced Imaging API.  There are a number of algorithms implemented in that API for doing color reduction.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "JavaScript array reduction\r\n                \r\n```\nfunction init(){\nvar panel = document.getElementById(\"panel\");\nvar i;\n\nvar week = [\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\"];\nvar weekend = [\"saturday\",\"sunday\"];\n\npanel.innerHTML+=\"weekdays: \"+ week;\npanel.innerHTML+=\"<hr>weekend days: \"+ weekend;\n\nfor(i=0; i<weekend.length; i++)\n{\n    week[week.length]=weekend[i];\n}\n\npanel.innerHTML+=\"<hr>increased with weekend days: \"+week;\n\nweek-=2;\n\npanel.innerHTML+=\"<hr>reduced back to weekdays: \"+week;\n}\nwindow.onload=init;\n```\n\n\nthis is the main js code it is loaded from another html file. everything is working fine until the end. the last output should be \"reduced back to weekdays: monday,tuesday,wednesday,thursday,friday\" but what im getting is this \"reduced back to weekdays: NaN\", what does this error mean and how can i fix it?\n    ", "Answer": "\r\nYou should reduce the array length by 2, not the array itself:\n\n```\nweek.length -= 2;```\n\n\nExample:\n\n```\nvar week = [1,2,3,4,5,6,7];\nweek.length -= 2;\n\nconsole.log(week);\n//[1, 2, 3, 4, 5]\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "C#/.net audio noise reduction [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is off-topic. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\r\n                \r\n                    \r\n                        Closed 11 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI'm developing a class library, that allows to do some manipulations with streamed audio (it is working with buffered samples, retrieved from recording device - so basically I've got an bytes array) for researsh purposes. \n\nThe problem is that i need to add noise reduction - there is need to capture noise profile, and apply noise reduction to each sample before applying sample processing & analysis, and I was not able to find any algorithms/samples/libraries that could be applied.\n\nAny help would be appreciated. Thanks in advance.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Selective string reduction\r\n                \r\nI would like to know how to strip all non-alphanumeric characters from a string except for underscores and dashes in PHP.\n    ", "Answer": "\r\nUse ```\npreg_replace```\n with ```\n/[^a-zA-Z0-9_\\-]/```\n as the pattern and ```\n''```\n as the replacement.\n\n```\n$string = preg_replace('/[^a-zA-Z0-9_\\-]/', '', $string);\n```\n\n\nEDIT\n\nAs skippy said, you can use the ```\ni```\n modifier for case insensitivity:\n\n```\n$string = preg_replace('/[^a-z0-9_\\-]/i', '', $string);\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "C++ Best approach for reduction operation\r\n                \r\nConsider a course of operation that takes in any number of parameters whose size is KNOWN at compile time. If it's one, return directly; if it is more than one, apply a user provided reduction operator that is capable of returning a type that is different than any one of the parameters'.\nSo the question is: What's the best approach?\nI can think of two as below:\n\nconstexpr if checks on the size of the parameters. If it's one, pass thru; If it's two, apply reduction op.\nHave a generic perfect pass-thru function that serves as the default reduction op.\n\nIllustrative code as below: https://godbolt.org/z/o19sc5q19\n```\n#include <tuple>\n\n// Approach 1\ntemplate <typename F, typename... Args>\nauto ApproachOne(std::tuple<Args...> intermediate_results, F&& reduction_op) {\n    if constexpr (sizeof...(Args) == 1) {\n        return std::get<0>(intermediate_results);\n    } else {\n        return std::apply(reduction_op, intermediate_results);\n    }\n}\n\n// Approach 2\ntemplate <typename F = decltype([]<typename T>(T&& t) {\n              return std::forward<T>(t);\n          }),\n          typename... Args>\nauto ApproachTwo(std::tuple<Args...> intermediate_results,\n                 F reduction_op = F{}) {\n    return std::apply(reduction_op, intermediate_results);\n}\n```\n\nFor short example driver code provided in the link, GCC and Clang have identical assemblies on either approach with optimization on. I'm wondering for large projects will there be any differences regarding the inlining and such? Or compiler is capable of translating this routine to identical code no matter what.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Set Cover Reduction\r\n                \r\nLet's say we have a set U = {x1, x2, x3} and a set S = {{x1},{x1, x2},{x1, x3},{x1,x1,x3}}.\nThis is purely an example and the problem is for the general problem. This looks just like a regular set cover problem, which is why I figure reducing it to a true set cover problem is feasible. The twist is that the elements in U needs to be 'picked' z amount of times, where z is different for each x1, x2, x3.... and so forth.\n\nAny subset in S can only pick up the elements they have inside them ONCE. Given a number 'k', is it possible to form a collection of subsets in S so that\n\n\nEvery element in U is included.\nEvery element is included z amount of times, where z is different for all x'es.\n\n\nIf i can formulate the Set Cover problem in such a way that'd be great, but im stuck at this part.\n    ", "Answer": "\r\nThe covering problem is NP-complete, because of it, heuristic methods are employed to found good solutions.\"The algorithm design Manual\" from Skiena have a detailed discussion about the subject.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "reduction of each row in the table of database\r\n                \r\ni have a table temporary as follow as:\n\n```\nstudent    |      Data     |    number           \n-----------|---------------|--------------\n1          |   book        |      2          \n1          |   book        |      5    \n1          |   book        |      9     \n2          |   book        |      1         \n2          |   book        |      5     \n```\n\n\ni will show reduction of column in like as output column as follow as:\n\n```\nstudent    |   Data        |    number      |output (number column of next row-previous line )\n-----------|---------------|----------------|--------------\n1          |   book        |      2         |     0\n1          |   book        |      5         |     3  (result of (5-2=3)\n1          |   book        |      9         |     4  (result of (9-5=4)\n2          |   book        |      1         |     0\n2          |   book        |      5         |     4  (result of (5-1=4)\n```\n\n\nhow are writing of php's script is correct? because i'm confused\n    ", "Answer": "\r\nYou didn't mention your DBMS, so this is standard SQL:\n\n```\nselect student,\n       data, \n       number,\n       number - lag(number,1,number) over (partition by student order by id) as output\nfrom the_table\norder by student, id\n```\n\n\nSQLFiddle example\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction on multiple variables (array)\r\n                \r\nI am trying to do a reduction on multiple variables (an array) using OMP, but wasn't sure how to implement it with OMP. See the code below.\n```\n#pramga omp parallel for reduction( ??? )\nfor (int i = 0; i < n; i++) {\n        for (int j = 0; j < m; j++) {\n                [ compute value ... ]\n\n                y[j] += value\n        }\n}\n```\n\nI thought I could do something like this, with the atomic keyword, but realised this would prevent two threads from updating y at the same time even if they are updating different values.\n```\n#pramga omp parallel for\nfor (int i = 0; i < n; i++) {\n        for (int j = 0; j < m; j++) {\n                [ compute value ... ]\n\n                #pragma omp atomic\n                y[j] += value\n        }\n}\n```\n\nDoes OMP have any functionality for something like this or otherwise how would I achieve this optimally without OMP's reduction keyword?\n    ", "Answer": "\r\nThere is an array reduction available in OpenMP since version 4.5:\n```\n#pramga omp parallel for reduction(+:y[:m])\n```\n\nwhere m is the size of the array. The only limitation here is that the local array used in reduction is always reserved on the stack, so it cannot be used in the case of large arrays.\nThe atomic operation you mentioned should work fine, but it may be less efficient than reduction. Of course, it depends on the actual circumstances (e.g. actual value of n and m, time to compute value, false sharing, etc.).\n```\n#pragma omp atomic\n  y[j] += value\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "tf.Print() output reduction\r\n                \r\nI am using ```\ntf.Print()```\n to debug a model, but ```\ntf.Print()```\n is printing too often. What are some good strategies to limit the output of this function, perhaps to be invoked on every nth epoch?\n    ", "Answer": "\r\nYou can print output on every nth epoch as you mentioned in the question. Here is the basic example:\n\n```\nif epoch % n == 0:\n    ...\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Dimensionality Reduction ValueError\r\n                \r\nNew to this subject. I was trying to use PCA, from sklearn to reduce my data dimensionally. As I dont know another method I am trying to use PCA to guess how much dimensions it should be used.\nMy data is an ndarray with shape (51, 2928). With the next code I try to fit the data\n```\npca = PCA(n_components='mle', svd_solver='full')\npca.fit(data)\n```\n\nBut I deal with the following error when trying to fit the data:\n\nValueError: n_components='mle' is only supported if n_samples >= n_features\n\nWhat am I doing wrong?\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "C#/.net audio noise reduction [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is off-topic. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\r\n                \r\n                    \r\n                        Closed 11 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI'm developing a class library, that allows to do some manipulations with streamed audio (it is working with buffered samples, retrieved from recording device - so basically I've got an bytes array) for researsh purposes. \n\nThe problem is that i need to add noise reduction - there is need to capture noise profile, and apply noise reduction to each sample before applying sample processing & analysis, and I was not able to find any algorithms/samples/libraries that could be applied.\n\nAny help would be appreciated. Thanks in advance.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Set Cover Reduction\r\n                \r\nLet's say we have a set U = {x1, x2, x3} and a set S = {{x1},{x1, x2},{x1, x3},{x1,x1,x3}}.\nThis is purely an example and the problem is for the general problem. This looks just like a regular set cover problem, which is why I figure reducing it to a true set cover problem is feasible. The twist is that the elements in U needs to be 'picked' z amount of times, where z is different for each x1, x2, x3.... and so forth.\n\nAny subset in S can only pick up the elements they have inside them ONCE. Given a number 'k', is it possible to form a collection of subsets in S so that\n\n\nEvery element in U is included.\nEvery element is included z amount of times, where z is different for all x'es.\n\n\nIf i can formulate the Set Cover problem in such a way that'd be great, but im stuck at this part.\n    ", "Answer": "\r\nThe covering problem is NP-complete, because of it, heuristic methods are employed to found good solutions.\"The algorithm design Manual\" from Skiena have a detailed discussion about the subject.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction on multiple variables (array)\r\n                \r\nI am trying to do a reduction on multiple variables (an array) using OMP, but wasn't sure how to implement it with OMP. See the code below.\n```\n#pramga omp parallel for reduction( ??? )\nfor (int i = 0; i < n; i++) {\n        for (int j = 0; j < m; j++) {\n                [ compute value ... ]\n\n                y[j] += value\n        }\n}\n```\n\nI thought I could do something like this, with the atomic keyword, but realised this would prevent two threads from updating y at the same time even if they are updating different values.\n```\n#pramga omp parallel for\nfor (int i = 0; i < n; i++) {\n        for (int j = 0; j < m; j++) {\n                [ compute value ... ]\n\n                #pragma omp atomic\n                y[j] += value\n        }\n}\n```\n\nDoes OMP have any functionality for something like this or otherwise how would I achieve this optimally without OMP's reduction keyword?\n    ", "Answer": "\r\nThere is an array reduction available in OpenMP since version 4.5:\n```\n#pramga omp parallel for reduction(+:y[:m])\n```\n\nwhere m is the size of the array. The only limitation here is that the local array used in reduction is always reserved on the stack, so it cannot be used in the case of large arrays.\nThe atomic operation you mentioned should work fine, but it may be less efficient than reduction. Of course, it depends on the actual circumstances (e.g. actual value of n and m, time to compute value, false sharing, etc.).\n```\n#pragma omp atomic\n  y[j] += value\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "LDA and Dimensionality Reduction [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     This question does not appear to be about programming within the scope defined in the help center.\r\n                \r\n                    \r\n                        Closed 9 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI have dataset consisting of about 300 objects with 84 features for each object. The objects are already separated into two classes. With PCA I´m able to reduce the dimensionality to about 24. I´m using 3 principle components covering about 96% of the variance of the original data. The problem I have is that PCA doesnt care about the ability to separate the classes from each other. Is there a way to combine PCA for reducing feature space and LDA for finding a discriminance function for those two classes ? \nOr is there a way to use LDA for finding the features that separate two classes in threedimensional space in the best manner ? \n\nI´m kind of irritated because I found this paper but I´m not really understanding. http://faculty.ist.psu.edu/jessieli/Publications/ecmlpkdd11_qgu.pdf\n\nThanks in advance.\n    ", "Answer": "\r\nYou should have a look at this article on principle component regression (PCR, what you want if the variable to be explained is scalar) and partial least squares regression (PLSR) with MATLAB's statistics toolbox. In PCR essentially, you choose the principal components as they most explain the dependent variable. They may not be the ones with the largest variance.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "XHR Bandwidth reduction\r\n                \r\nSo were using XHR to validate pages exists and they have content, but as we do a lot of request we wanted to trim down some of the bandwidth used.\n\nWe thought about using a ```\nHEAD```\n request to check for ```\n!200```\n and then thought well that's still 2 request's if the page exists and then we come up with this sample code\n\n```\nAjax.prototype.get = function (location, callback)\n{\n    var Request = new XMLHttpRequest();\n\n    Request.open(\"GET\", location, true);\n    Request.onreadystatechange = function ()\n    {\n        if(Request.readyState === Request.HEADERS_RECEIVED)\n        {\n            if(Request.status != 200)\n            {\n                //Ignore the data to save bandwidth\n                callback(Request);\n                Request.abort();\n            }\n            else\n            {\n                //@Overide the callback here to assure asingle callback fire\n                Request.onreadystatechange = function()\n                {\n                    if (Request.readyState === Request.DONE)\n                    {\n                        callback(Request);\n                    }\n                }\n            }\n        }\n    }\n    Request.send(null);\n}\n```\n\n\nWhat I would like to know is does this actually work, or does the response body always come back to the client.\n\nThanks\n    ", "Answer": "\r\nI won't give a definitve answer but I have some thoughts that are to long for a comment.\n\nTheoretically, a abortion of the request should cause the underlying connection to be closed. Assuming a TCP based communication that means sending a FIN to the server, which should then stop sending data and ACKs the FIN. But this is HTTP and there might be other magic going on (like connection pipelining, etc.)...\n\nAnyway, when you close the connection early, the client will receive all data that was send in ```\ncommunication delay```\n as the server will at least keep sending until he gets the the STOP signal. If you have a medium delay and a high bandwith connection this could be a lot of data and will, depending on the amount of data, most likely be a good portion of the complete data.\n\nNote that, while the code will not receive any of this data, it will be transferred to the network device of the client and will be at least passed a little bit up the network stack. So, while this data never receives you application level, the bandwith will be consumed anyway.\n\nMy (educated) guess is that it will not save as much as you would like (under \"normal\" conditions). I would suggest that you do a real world test and see if it is worth the afford. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Scala Recursive Reduction\r\n                \r\nI have a case class that is recursive and looks like this:\n\n```\ncase class Config(name: String, isEnabled: Boolean, elems: Map[String, MyCase])\n\ncase class MyCase(\n  id: String,\n  isActive: Boolean,\n  elems: Option[Map[String, MyCase]])\n```\n\n\nWhere the ```\nConfig```\n contains the ```\nid```\n of the ```\nMyCase```\n entries contained as a ```\nMap```\n. I have to iterate over this structure and come up with a ```\nMap```\n that contains the parent child relations. Say for example., if I have the ```\nConfig```\n class represented as below (for simplicity, I have just given the ```\nid```\n's):\n\n```\nConfig(Map(\"5\" -> myCase1, \"6\" -> myCase2))\n\n5 - Config\n 1\n  1\n  2 \n 2\n6\n 1\n```\n\n\nwhere ```\nid```\n's 5 and 6 are top level entries which in turn has a recursive structure. I have to now come up with a ```\nMap```\n that contains the parent child relationship for the ```\nid```\n's. So for the case above, I will expect a ```\nMap```\n that looks like:\n\n```\nMap(\n \"5\" -> Seq(\"5.1\"),\n \"5.1\" -> Seq(\"5.1.1\", \"5.1.2\"),\n \"5.1.1\" -> Seq.empty[String],\n \"5.1.2\" -> Seq.empty[String],\n \"6\" -> Seq(\"6.1\"),\n \"6.1\" -> Seq.empty[String]\n)\n```\n\n\nNotice how I append the parent ```\nid```\n to the child. \n\nGiven a ```\nSeq```\n of such ```\nConfig```\ns, \n\n```\nval configs = Seq(Config1, Config2)\n```\n\n\nthis is what I have come up with:\n\n```\ndef relationsFor(config: Config): Map[String, Seq[String]] = {\n\n    def prepareChildren(parentId: String, acc: Map[String, Seq[String]], subElems: Map[String, MyCase]): Map[String, Seq[String]] = {\n      subElems.collect {\n        case (id, elem) if elem.isActive =>\n          val newAcc = acc ++ Map(parentId -> subElems.keys.map(subElemId => s\"$parentId.$subElemId\").toSeq)\n          if (elem.elems.isDefined) {\n            val newParentId = s\"$parentId.$id\"\n            val newAcc1 = newAcc ++ Map(s\"$parentId.$id\" -> elem.elems.get.keys.map(subElemId => s\"$newParentId.$subElemId\").toSeq)\n            prepareChildren(s\"$parentId.$id\", newAcc1, elem.elems.get)\n          } else {\n            acc ++ Map(s\"$parentId.$id\" -> Seq.empty[String])\n          }\n      }\n    }.flatten.toMap\n\n    configs.collect {\n      case config if config.isEnabled =>\n        config.elems.collect {\n          case (parentId, elemCase) if elemCase.elems.isDefined =>\n            prepareChildren(parentId, Map.empty[String, Seq[String]], elemCase.elems.get)\n          case (parentId, _) =>\n            Map(parentId -> Seq.empty[String])\n        }\n    }.flatten.flatten.toMap\n  }\n```\n\n\nCould this be simplified ? I have already tested this and it works as expected, but I find it a little bit hard to understand. Can this be made much more elegant? I mean can the ```\nrelationsFor```\n method be made more elegant ?\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Error reduction in GBM\r\n                \r\nI would like to know how ErrorReduction is calculated in pretty.gbm.tree function: https://github.com/gbm-developers/gbm3/blob/62c8dafd87b16fe1d2079cdd5058169f1f08967b/R/pretty-gbm-tree.r#L32\n\nThis post helped me a lot, but did not quite answer my question. \nUnderstanding tree structure in R gbm package\n\nHere is an output of pretty.gbm.tree function for the first tree:\noutput table\n\nHow this value of 19167.524 was calculated?\n\nThanks\n    ", "Answer": "\r\nThe ErrorReduction is calculated as following:\n\nnumber of samples in a current node * mse - number of samples in the left child node * mse - number of samples in the right child node *mse \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Can't compile RenderScript reduction kernel\r\n                \r\n:app:compileDebugRenderscript task fails when I try to compile my reduction kernel. I created a new project with addint.rs file in the renderscript folder. My Gradle version is 3.5.3\n\n```\napply plugin: 'com.android.application'\n\nandroid {\n    compileSdkVersion 29\n    buildToolsVersion \"29.0.2\"\n    defaultConfig {\n        applicationId \"com.test.testrenderscript\"\n        minSdkVersion 26\n        targetSdkVersion 29\n        versionCode 1\n        versionName \"1.0\"\n        targetSdkVersion 29\n        renderscriptTargetApi 26\n        testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\"\n    }\n    buildTypes {\n        release {\n            minifyEnabled false\n            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'\n        }\n    }\n}\n\ndependencies {\n    implementation fileTree(dir: 'libs', include: ['*.jar'])\n    implementation 'androidx.appcompat:appcompat:1.1.0'\n    implementation 'androidx.constraintlayout:constraintlayout:1.1.3'\n    implementation 'com.google.android.material:material:1.0.0'\n    testImplementation 'junit:junit:4.12'\n    androidTestImplementation 'androidx.test.ext:junit:1.1.1'\n    androidTestImplementation 'androidx.test.espresso:espresso-core:3.2.0'\n}\n```\n\n\nI used the simplest reduce kernel from the Google's example:\n\n```\n#pragma rs reduce(addint) accumulator(addintAccum)\n\nstatic void addintAccum(int *accum, int val) {\n  *accum += val;\n}\n```\n\n\nUsual mapping kernels compile just fine, but unfortunately reduction kernel would suit much better for my task, so I would really appreciate any input on how to fix this issue to be able to build reduction kernels. Project builds properly if I remove #pragma rs reduce(addint) accumulator(addintAccum) line though obviously I can't use it as a viable render script anymore.\n\nOtherwise I get this error:\n\n```\norg.gradle.internal.UncheckedException: com.android.ide.common.process.ProcessException: Error while executing process /Users/sergeylazarev/Library/Android/sdk/build-tools/29.0.2/llvm-rs-cc with arguments {-O 3 -I /Users/sergeylazarev/Library/Android/sdk/build-tools/29.0.2/renderscript/include/ -I /Users/sergeylazarev/Library/Android/sdk/build-tools/29.0.2/renderscript/clang-include/ -p /Users/sergeylazarev/Documents/AlgoProjects/TestRenderscript/app/build/generated/renderscript_source_output_dir/debug/compileDebugRenderscript/out -o /Users/sergeylazarev/Documents/AlgoProjects/TestRenderscript/app/build/generated/res/rs/debug/raw -target-api 26 /Users/sergeylazarev/Documents/AlgoProjects/TestRenderscript/app/src/main/rs/addint.rs}\n    at org.gradle.internal.UncheckedException.throwAsUncheckedException(UncheckedException.java:67)\n    at org.gradle.internal.UncheckedException.throwAsUncheckedException(UncheckedException.java:41)\n    at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:106)\n    at org.gradle.api.internal.project.taskfactory.StandardTaskAction.doExecute(StandardTaskAction.java:48)\n    at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:41)\n    at org.gradle.api.internal.project.taskfactory.StandardTaskAction.execute(StandardTaskAction.java:28)\n    at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:702)\n    at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:669)\n    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$5.run(ExecuteActionsTaskExecuter.java:404)\n    at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:402)\n    at org.gradle.internal.operations.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:394)\n    at org.gradle.internal.operations.DefaultBuildOperationExecutor$1.execute(DefaultBuildOperationExecutor.java:165)\n    at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:250)\n    at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:158)\n    at org.gradle.internal.operations.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:92)\n    at org.gradle.internal.operations.DelegatingBuildOperationExecutor.run(DelegatingBuildOperationExecutor.java:31)\n    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:393)\n    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:376)\n    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.access$200(ExecuteActionsTaskExecuter.java:80)\n    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$TaskExecution.execute(ExecuteActionsTaskExecuter.java:213)\n    at org.gradle.internal.execution.steps.ExecuteStep.lambda$execute$1(ExecuteStep.java:33)\n    at java.util.Optional.orElseGet(Optional.java:267)\n    at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:33)\n    at org.gradle.internal.execution.steps.ExecuteStep.execute(ExecuteStep.java:26)\n    at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:58)\n    at org.gradle.internal.execution.steps.CleanupOutputsStep.execute(CleanupOutputsStep.java:35)\n    at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:48)\n    at org.gradle.internal.execution.steps.ResolveInputChangesStep.execute(ResolveInputChangesStep.java:33)\n    at org.gradle.internal.execution.steps.CancelExecutionStep.execute(CancelExecutionStep.java:39)\n    at org.gradle.internal.execution.steps.TimeoutStep.executeWithoutTimeout(TimeoutStep.java:73)\n    at org.gradle.internal.execution.steps.TimeoutStep.execute(TimeoutStep.java:54)\n    at org.gradle.internal.execution.steps.CatchExceptionStep.execute(CatchExceptionStep.java:35)\n    at org.gradle.internal.execution.steps.CreateOutputsStep.execute(CreateOutputsStep.java:51)\n    at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:45)\n    at org.gradle.internal.execution.steps.SnapshotOutputsStep.execute(SnapshotOutputsStep.java:31)\n    at org.gradle.internal.execution.steps.CacheStep.executeWithoutCache(CacheStep.java:201)\n    at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:70)\n    at org.gradle.internal.execution.steps.CacheStep.execute(CacheStep.java:45)\n    at org.gradle.internal.execution.steps.BroadcastChangingOutputsStep.execute(BroadcastChangingOutputsStep.java:49)\n    at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:43)\n    at org.gradle.internal.execution.steps.StoreSnapshotsStep.execute(StoreSnapshotsStep.java:32)\n    at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:38)\n    at org.gradle.internal.execution.steps.RecordOutputsStep.execute(RecordOutputsStep.java:24)\n    at org.gradle.internal.execution.steps.SkipUpToDateStep.executeBecause(SkipUpToDateStep.java:96)\n    at org.gradle.internal.execution.steps.SkipUpToDateStep.lambda$execute$0(SkipUpToDateStep.java:89)\n    at java.util.Optional.map(Optional.java:215)\n    at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:54)\n    at org.gradle.internal.execution.steps.SkipUpToDateStep.execute(SkipUpToDateStep.java:38)\n    at org.gradle.internal.execution.steps.ResolveChangesStep.execute(ResolveChangesStep.java:77)\n    at org.gradle.internal.execution.steps.ResolveChangesStep.execute(ResolveChangesStep.java:37)\n    at org.gradle.internal.execution.steps.legacy.MarkSnapshottingInputsFinishedStep.execute(MarkSnapshottingInputsFinishedStep.java:36)\n    at org.gradle.internal.execution.steps.legacy.MarkSnapshottingInputsFinishedStep.execute(MarkSnapshottingInputsFinishedStep.java:26)\n    at org.gradle.internal.execution.steps.ResolveCachingStateStep.execute(ResolveCachingStateStep.java:90)\n    at org.gradle.internal.execution.steps.ResolveCachingStateStep.execute(ResolveCachingStateStep.java:48)\n    at org.gradle.internal.execution.impl.DefaultWorkExecutor.execute(DefaultWorkExecutor.java:33)\n    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:120)\n    at org.gradle.api.internal.tasks.execution.ResolveBeforeExecutionStateTaskExecuter.execute(ResolveBeforeExecutionStateTaskExecuter.java:75)\n    at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:62)\n    at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:108)\n    at org.gradle.api.internal.tasks.execution.ResolveBeforeExecutionOutputsTaskExecuter.execute(ResolveBeforeExecutionOutputsTaskExecuter.java:67)\n    at org.gradle.api.internal.tasks.execution.ResolveAfterPreviousExecutionStateTaskExecuter.execute(ResolveAfterPreviousExecutionStateTaskExecuter.java:46)\n    at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute(CleanupStaleOutputsExecuter.java:94)\n    at org.gradle.api.internal.tasks.execution.FinalizePropertiesTaskExecuter.execute(FinalizePropertiesTaskExecuter.java:46)\n    at org.gradle.api.internal.tasks.execution.ResolveTaskExecutionModeExecuter.execute(ResolveTaskExecutionModeExecuter.java:95)\n    at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:57)\n    at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:56)\n    at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:36)\n    at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.executeTask(EventFiringTaskExecuter.java:73)\n    at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:52)\n    at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter$1.call(EventFiringTaskExecuter.java:49)\n    at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:416)\n    at org.gradle.internal.operations.DefaultBuildOperationExecutor$CallableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:406)\n    at org.gradle.internal.operations.DefaultBuildOperationExecutor$1.execute(DefaultBuildOperationExecutor.java:165)\n    at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:250)\n    at org.gradle.internal.operations.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:158)\n    at org.gradle.internal.operations.DefaultBuildOperationExecutor.call(DefaultBuildOperationExecutor.java:102)\n    at org.gradle.internal.operations.DelegatingBuildOperationExecutor.call(DelegatingBuildOperationExecutor.java:36)\n    at org.gradle.api.internal.tasks.execution.EventFiringTaskExecuter.execute(EventFiringTaskExecuter.java:49)\n    at org.gradle.execution.plan.LocalTaskNodeExecutor.execute(LocalTaskNodeExecutor.java:43)\n    at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:355)\n    at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$InvokeNodeExecutorsAction.execute(DefaultTaskExecutionGraph.java:343)\n    at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:336)\n    at org.gradle.execution.taskgraph.DefaultTaskExecutionGraph$BuildOperationAwareExecutionAction.execute(DefaultTaskExecutionGraph.java:322)\n    at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:134)\n    at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker$1.execute(DefaultPlanExecutor.java:129)\n    at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.execute(DefaultPlanExecutor.java:202)\n    at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.executeNextNode(DefaultPlanExecutor.java:193)\n    at org.gradle.execution.plan.DefaultPlanExecutor$ExecutorWorker.run(DefaultPlanExecutor.java:129)\n    at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)\n    at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)\n    at java.lang.Thread.run(Thread.java:748)\nCaused by: com.android.ide.common.process.ProcessException: Error while executing process /Users/sergeylazarev/Library/Android/sdk/build-tools/29.0.2/llvm-rs-cc with arguments {-O 3 -I /Users/sergeylazarev/Library/Android/sdk/build-tools/29.0.2/renderscript/include/ -I /Users/sergeylazarev/Library/Android/sdk/build-tools/29.0.2/renderscript/clang-include/ -p /Users/sergeylazarev/Documents/AlgoProjects/TestRenderscript/app/build/generated/renderscript_source_output_dir/debug/compileDebugRenderscript/out -o /Users/sergeylazarev/Documents/AlgoProjects/TestRenderscript/app/build/generated/res/rs/debug/raw -target-api 26 /Users/sergeylazarev/Documents/AlgoProjects/TestRenderscript/app/src/main/rs/addint.rs}\n    at com.android.build.gradle.internal.process.GradleProcessResult.buildProcessException(GradleProcessResult.java:73)\n    at com.android.build.gradle.internal.process.GradleProcessResult.assertNormalExitValue(GradleProcessResult.java:48)\n    at com.android.builder.internal.compiler.RenderScriptProcessor.doMainCompilation(RenderScriptProcessor.java:310)\n    at com.android.builder.internal.compiler.RenderScriptProcessor.build(RenderScriptProcessor.java:226)\n    at com.android.build.gradle.tasks.RenderscriptCompile.compileAllRenderscriptFiles(RenderscriptCompile.java:300)\n    at com.android.build.gradle.tasks.RenderscriptCompile.doTaskAction(RenderscriptCompile.java:216)\n    at com.android.build.gradle.internal.tasks.NonIncrementalTask$taskAction$$inlined$recordTaskAction$1.invoke(AndroidVariantTask.kt:51)\n    at com.android.build.gradle.internal.tasks.NonIncrementalTask$taskAction$$inlined$recordTaskAction$1.invoke(AndroidVariantTask.kt:31)\n    at com.android.build.gradle.internal.tasks.Blocks.recordSpan(Blocks.java:91)\n    at com.android.build.gradle.internal.tasks.NonIncrementalTask.taskAction(NonIncrementalTask.kt:34)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:103)\n    ... 91 more\nCaused by: org.gradle.process.internal.ExecException: Process 'command '/Users/sergeylazarev/Library/Android/sdk/build-tools/29.0.2/llvm-rs-cc'' finished with non-zero exit value 1\n    at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:396)\n    at com.android.build.gradle.internal.process.GradleProcessResult.assertNormalExitValue(GradleProcessResult.java:46)\n    ... 104 more\n```\n\n    ", "Answer": "\r\nIt seems that I made an error while writing the package name inside the kernel. \nSo, the working basic kernel looks like this:\n\n```\n#pragma version(1)\n#pragma rs java_package_name(com.test.testrenderscript) //error was here\n#pragma rs reduce(addint) accumulator(addintAccum)\n\nstatic void addintAccum(int *accum, int val) {\n  *accum += val;\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Fastest way to do horizontal SSE vector sum (or other reduction)\r\n                \r\nGiven a vector of three (or four) floats. What is the fastest way to sum them?\n\nIs SSE (movaps, shuffle, add, movd) always faster than x87? Are the horizontal-add instructions in SSE3 worth it?\n\nWhat's the cost to moving to the FPU, then faddp, faddp? What's the fastest specific instruction sequence?\n\n\"Try to arrange things so you can sum four vectors at a time\" will not be accepted as an answer. :-)  e.g. for summing an array, you can use multiple vector accumulators for vertical sums (to hide addps latency), and reduce down to one after the loop, but then you need to horizontally sum that last vector.\n    ", "Answer": "\r\nIn general for any kind of vector horizontal reduction, extract / shuffle high half to line up with low, then vertical add (or min/max/or/and/xor/multiply/whatever); repeat until a there's just a single element (with high garbage in the rest of the vector).\nIf you start with vectors wider than 128-bit, narrow in half until you get to 128 (then you can use one of the functions in this answer on that vector).  But if you need the result broadcast to all elements at the end, then you can consider doing full-width shuffles all the way.\nRelated Q&As for wider vectors, and integers, and FP\n\n```\n__m128```\n and ```\n__m128d```\n This answer (see below)\n\n```\n__m256d```\n with perf analysis for Ryzen 1 vs. Intel (showing why ```\nvextractf128```\n is vastly better than ```\nvperm2f128```\n) Get sum of values stored in __m256d with SSE/AVX\n\n```\n__m256```\n How to sum __m256 horizontally?\n\nIntel AVX: 256-bits version of dot product for double precision floating point variables of single vectors.\n\nDot product of arrays (not just a single vector of 3 or 4 elements): do vertical mul/add or FMA into multiple accumulators, and hsum at the end.  Complete AVX+FMA array dot-product example, including an efficient hsum after the loop.  (For the simple sum or other reduction of an array, use that pattern but without the multiply part, e.g. add instead of fma).  Do not do the horizontal work separately for each SIMD vector; do it once at the end.\nHow to count character occurrences using SIMD as an integer example of counting ```\n_mm256_cmpeq_epi8```\n matches, again over a whole array, only hsumming at the end.  (Worth special mention for doing some 8-bit accumulation then widening 8 -> 64-bit to avoid overflow without doing a full hsum at that point.)\n\n\nInteger\n\n```\n__m128i```\n 32-bit elements: this answer (see below).  64-bit elements should be obvious: only one pshufd/paddq step.\n\n```\n__m128i```\n 8-bit unsigned ```\nuint8_t```\n elements without wrapping/overflow: ```\npsadbw```\n against ```\n_mm_setzero_si128()```\n, then hsum the two qword halves (or 4 or 8 for wider vectors). Fastest way to horizontally sum SSE unsigned byte vector shows 128-bit with SSE2.\nSumming 8-bit integers in __m512i with AVX intrinsics has an AVX512 example.  How to count character occurrences using SIMD has an AVX2 ```\n__m256i```\n example.\n(For ```\nint8_t```\n signed bytes you can XOR set1_epi8(0x80) to flip to unsigned before SAD, then subtract the bias from the final hsum; see details here, also showing an optimization for doing only 9 bytes from memory instead of 16).\n\n16-bit unsigned: ```\n_mm_madd_epi16```\n with set1_epi16(1) is a single-uop widening horizontal add: SIMD: Accumulate Adjacent Pairs.  Then proceed with a 32-bit hsum.\n\n```\n__m256i```\n and ```\n__m512i```\n with 32-bit elements.\nFastest method to calculate sum of all packed 32-bit integers using AVX512 or AVX2.  For AVX512, Intel added a bunch of \"reduce\" inline functions (not hardware instructions) that do this for you, like ```\n_mm512_reduce_add_ps```\n (and pd, epi32, and epi64).  Also reduce_min/max/mul/and/or.  Doing it manually leads to basically the same asm.\n\nhorizontal max (instead of add): Getting max value in a __m128i vector with SSE?\n\n\n\nMain answer to this question: mostly float and ```\n__m128```\n\nHere are some versions tuned based on Agner Fog's microarch guide's microarch guide and instruction tables.  See also the x86 tag wiki.  They should be efficient on any CPU, with no major bottlenecks.  (e.g. I avoided things that would help one uarch a bit but be slow on another uarch).  Code-size is also minimized.\nThe common SSE3 / SSSE3 2x ```\nhadd```\n idiom is only good for code-size, not speed on any existing CPUs.  There are use-cases for it (like transpose and add, see below), but a single vector isn't one of them.\nI've also included an AVX version.  Any kind of horizontal reduction with  AVX / AVX2 should start with a ```\nvextractf128```\n and a \"vertical\" operation to reduce down to one XMM (```\n__m128```\n) vector.  In general for wide vectors, your best bet is to narrow in half repeatedly until you're down to a 128-bit vector, regardless of element type.  (Except for 8-bit integer, then ```\nvpsadbw```\n as a first step if you want to hsum without overflow to wider elements.)\nSee the asm output from all this code on the Godbolt Compiler Explorer.  See also my improvements to Agner Fog's C++ Vector Class Library ```\nhorizontal_add```\n functions.  (message board thread, and code on github).  I used CPP macros to select optimal shuffles for code-size for SSE2, SSE4, and AVX, and for avoiding ```\nmovdqa```\n when AVX isn't available.\n\nThere are tradeoffs to consider:\n\ncode size: smaller is better for L1 I-cache reasons, and for code fetch from disk (smaller binaries).  Total binary size mostly matters for compiler decisions made repeatedly all over a program.  If you're bothering to hand-code something with intrinsics, it's worth spending a few code bytes if it gives any speedup for the whole program (be careful of microbenchmarks that make unrolling look good).\nuop-cache size: Often more precious than L1 I$.  4 single-uop instructions can take less space than 2 ```\nhaddps```\n, so this is highly relevant here.\nlatency: Sometimes relevant\nthroughput (back-end ports): usually irrelevant, horizontal sums shouldn't be the only thing in an innermost loop.  Port pressure matters only as part of the whole loop that contains this.\nthroughput (total front-end fused-domain uops): If surrounding code doesn't bottleneck on the same port that the hsum uses, this is a proxy for the impact of the hsum on the throughput of the whole thing.\n\nWhen a horizontal add is infrequent:\nCPUs with no uop-cache might favour 2x ```\nhaddps```\n if it's very rarely used: It's slowish when it does run, but that's not often.  Being only 2 instructions minimizes the impact on the surrounding code (I$ size).\nCPUs with a uop-cache will probably favour something that takes fewer uops, even if it's more instructions / more x86 code-size.  Total uops cache-lines used is what we want to minimize, which isn't as simple as minimizing total uops (taken branches and 32B boundaries always start a new uop cache line).\nAnyway, with that said, horizontal sums come up a lot, so here's my attempt at carefully crafting some versions that compile nicely.  Not benchmarked on any real hardware, or even carefully tested.  There might be bugs in the shuffle constants or something.\n\nIf you're making a fallback / baseline version of your code, remember that only old CPUs will run it; newer CPUs will run your AVX version, or SSE4.1 or whatever.\nOld CPUs like K8, and Core2(merom) and earlier only have 64bit shuffle units.  Core2 has 128bit execution units for most instructions, but not for shuffles.  (Pentium M and K8 handle all 128b vector instructions as two 64bit halves).\nShuffles like ```\nmovhlps```\n that move data in 64-bit chunks (no shuffling within 64-bit halves) are fast, too.\nRelated: shuffles on new CPUs, and tricks for avoiding 1/clock shuffle throughput bottleneck on Haswell and later: Do 128bit cross lane operations in AVX512 give better performance?\nOn old CPUs with slow shuffles:\n\n```\nmovhlps```\n (Merom: 1uop) is significantly faster than ```\nshufps```\n (Merom: 3uops).  On Pentium-M, cheaper than ```\nmovaps```\n.  Also, it runs in the FP domain on Core2, avoiding the bypass delays from other shuffles.\n```\nunpcklpd```\n is faster than ```\nunpcklps```\n.\n```\npshufd```\n is slow, ```\npshuflw```\n/```\npshufhw```\n are fast (because they only shuffle a 64bit half)\n```\npshufb mm0```\n (MMX) is fast, ```\npshufb xmm0```\n is slow.\n```\nhaddps```\n is very slow (6uops on Merom and Pentium M)\n```\nmovshdup```\n (Merom: 1uop) is interesting: It's the only 1uop insn that shuffles within 64b elements.\n\n```\nshufps```\n on Core2(including Penryn) brings data into the integer domain, causing a bypass delay to get it back to the FP execution units for ```\naddps```\n, but ```\nmovhlps```\n is entirely in the FP domain.  ```\nshufpd```\n also runs in the float domain.\n```\nmovshdup```\n runs in the integer domain, but is only one uop.\nAMD K10, Intel Core2(Penryn/Wolfdale), and all later CPUs, run all xmm shuffles as a single uop.  (But note the bypass delay with ```\nshufps```\n on Penryn, avoided with ```\nmovhlps```\n)\n\nWithout AVX, avoiding wasted ```\nmovaps```\n/```\nmovdqa```\n instructions requires careful choice of shuffles.  Only a few shuffles work as a copy-and-shuffle, rather than modifying the destination.  Shuffles that combine data from two inputs (like ```\nunpck*```\n or ```\nmovhlps```\n) can be used with a tmp variable that's no longer needed instead of ```\n_mm_movehl_ps(same,same)```\n.\nSome of these can be made faster (save a MOVAPS) but uglier / less \"clean\" by taking a dummy arg for use as a destination for an initial shuffle.  For example:\n```\n// Use dummy = a recently-dead variable that vec depends on,\n//  so it doesn't introduce a false dependency,\n//  and the compiler probably still has it in a register\n__m128d highhalf_pd(__m128d dummy, __m128d vec) {\n#ifdef __AVX__\n    // With 3-operand AVX instructions, don't create an extra dependency on something we don't need anymore.\n    (void)dummy;\n    return _mm_unpackhi_pd(vec, vec);\n#else\n    // Without AVX, we can save a MOVAPS with MOVHLPS into a dead register\n    __m128 tmp = _mm_castpd_ps(dummy);\n    __m128d high = _mm_castps_pd(_mm_movehl_ps(tmp, _mm_castpd_ps(vec)));\n    return high;\n#endif\n}\n```\n\n\nSSE1 (aka SSE):\n```\nfloat hsum_ps_sse1(__m128 v) {                                  // v = [ D C | B A ]\n    __m128 shuf   = _mm_shuffle_ps(v, v, _MM_SHUFFLE(2, 3, 0, 1));  // [ C D | A B ]\n    __m128 sums   = _mm_add_ps(v, shuf);      // sums = [ D+C C+D | B+A A+B ]\n    shuf          = _mm_movehl_ps(shuf, sums);      //  [   C   D | D+C C+D ]  // let the compiler avoid a mov by reusing shuf\n    sums          = _mm_add_ss(sums, shuf);\n    return    _mm_cvtss_f32(sums);\n}\n    # gcc 5.3 -O3:  looks optimal\n    movaps  xmm1, xmm0     # I think one movaps is unavoidable, unless we have a 2nd register with known-safe floats in the upper 2 elements\n    shufps  xmm1, xmm0, 177\n    addps   xmm0, xmm1\n    movhlps xmm1, xmm0     # note the reuse of shuf, avoiding a movaps\n    addss   xmm0, xmm1\n\n    # clang 3.7.1 -O3:  \n    movaps  xmm1, xmm0\n    shufps  xmm1, xmm1, 177\n    addps   xmm1, xmm0\n    movaps  xmm0, xmm1\n    shufpd  xmm0, xmm0, 1\n    addss   xmm0, xmm1\n```\n\nI reported a clang bug about pessimizing the shuffles.  It has its own internal representation for shuffling, and turns that back into shuffles.  gcc more often uses the instructions that directly match the intrinsic you used.\nOften clang does better than gcc, in code where the instruction choice isn't hand-tuned, or constant-propagation can simplify things even when the intrinsics are optimal for the non-constant case.  Overall it's a good thing that compilers work like a proper compiler for intrinsics, not just an assembler.  Compilers can often generate good asm from scalar C that doesn't even try to work the way good asm would.  Eventually compilers will treat intrinsics as just another C operator as input for the optimizer.\n\nSSE3\n```\nfloat hsum_ps_sse3(__m128 v) {\n    __m128 shuf = _mm_movehdup_ps(v);        // broadcast elements 3,1 to 2,0\n    __m128 sums = _mm_add_ps(v, shuf);\n    shuf        = _mm_movehl_ps(shuf, sums); // high half -> low half\n    sums        = _mm_add_ss(sums, shuf);\n    return        _mm_cvtss_f32(sums);\n}\n\n    # gcc 5.3 -O3: perfectly optimal code\n    movshdup    xmm1, xmm0\n    addps       xmm0, xmm1\n    movhlps     xmm1, xmm0\n    addss       xmm0, xmm1\n```\n\nThis has several advantages:\n\ndoesn't require any ```\nmovaps```\n copies to work around destructive shuffles (without AVX): ```\nmovshdup xmm1, xmm2```\n's destination is write-only, so it creates ```\ntmp```\n out of a dead register for us.  This is also why I used ```\nmovehl_ps(tmp, sums)```\n instead of ```\nmovehl_ps(sums, sums)```\n.\n\nsmall code-size.  The shuffling instructions are small:  ```\nmovhlps```\n is 3 bytes, ```\nmovshdup```\n is 4 bytes (same as ```\nshufps```\n).  No immediate byte is required, so with AVX, ```\nvshufps```\n is 5 bytes but ```\nvmovhlps```\n and ```\nvmovshdup```\n are both 4.\n\n\nI could save another byte with ```\naddps```\n instead of ```\naddss```\n.  Since this won't be used inside inner loops, the extra energy to switch the extra transistors is probably negligible.  FP exceptions from the upper 3 elements aren't a risk, because all elements hold valid FP data.  However, clang/LLVM actually \"understands\" vector shuffles, and emits better code if it knows that only the low element matters.\nLike the SSE1 version, adding the odd elements to themselves may cause FP exceptions (like overflow) that wouldn't happen otherwise, but this shouldn't be a problem.  Denormals are slow, but IIRC producing a +Inf result isn't on most uarches.\n\nSSE3 optimizing for code-size\nIf code-size is your major concern, two ```\nhaddps```\n (```\n_mm_hadd_ps```\n) instructions will do the trick (Paul R's answer).  This is also the easiest to type and remember.  It is not fast, though.  Even Intel Skylake still decodes each ```\nhaddps```\n to 3 uops, with 6 cycle latency.  So even though it saves machine-code bytes (L1 I-cache), it takes up more space in the more-valuable uop-cache.  Real use-cases for ```\nhaddps```\n: a transpose-and-sum problem, or doing some scaling at an intermediate step in this SSE ```\natoi()```\n implementation.\n\nAVX:\nThis version saves a code byte vs. Marat's answer to the AVX question.\n```\n#ifdef __AVX__\nfloat hsum256_ps_avx(__m256 v) {\n    __m128 vlow  = _mm256_castps256_ps128(v);\n    __m128 vhigh = _mm256_extractf128_ps(v, 1); // high 128\n           vlow  = _mm_add_ps(vlow, vhigh);     // add the low 128\n    return hsum_ps_sse3(vlow);         // and inline the sse3 version, which is optimal for AVX\n    // (no wasted instructions, and all of them are the 4B minimum)\n}\n#endif\n\n vmovaps xmm1,xmm0               # huh, what the heck gcc?  Just extract to xmm1\n vextractf128 xmm0,ymm0,0x1\n vaddps xmm0,xmm1,xmm0\n vmovshdup xmm1,xmm0\n vaddps xmm0,xmm1,xmm0\n vmovhlps xmm1,xmm1,xmm0\n vaddss xmm0,xmm0,xmm1\n vzeroupper \n ret\n```\n\n\nDouble-precision:\n```\ndouble hsum_pd_sse2(__m128d vd) {                      // v = [ B | A ]\n    __m128 undef  = _mm_undefined_ps();                       // don't worry, we only use addSD, never touching the garbage bits with an FP add\n    __m128 shuftmp= _mm_movehl_ps(undef, _mm_castpd_ps(vd));  // there is no movhlpd\n    __m128d shuf  = _mm_castps_pd(shuftmp);\n    return  _mm_cvtsd_f64(_mm_add_sd(vd, shuf));\n}\n\n# gcc 5.3.0 -O3\n    pxor    xmm1, xmm1          # hopefully when inlined, gcc could pick a register it knew wouldn't cause a false dep problem, and avoid the zeroing\n    movhlps xmm1, xmm0\n    addsd   xmm0, xmm1\n\n\n# clang 3.7.1 -O3 again doesn't use movhlps:\n    xorpd   xmm2, xmm2          # with  #define _mm_undefined_ps _mm_setzero_ps\n    movapd  xmm1, xmm0\n    unpckhpd        xmm1, xmm2\n    addsd   xmm1, xmm0\n    movapd  xmm0, xmm1    # another clang bug: wrong choice of operand order\n\n\n// This doesn't compile the way it's written\ndouble hsum_pd_scalar_sse2(__m128d vd) {\n    double tmp;\n    _mm_storeh_pd(&tmp, vd);       // store the high half\n    double lo = _mm_cvtsd_f64(vd); // cast the low half\n    return lo+tmp;\n}\n\n    # gcc 5.3 -O3\n    haddpd  xmm0, xmm0   # Lower latency but less throughput than storing to memory\n\n    # ICC13\n    movhpd    QWORD PTR [-8+rsp], xmm0    # only needs the store port, not the shuffle unit\n    addsd     xmm0, QWORD PTR [-8+rsp]\n```\n\nStoring to memory and back avoids an ALU uop.  That's good if shuffle port pressure, or ALU uops in general, are a bottleneck.  (Note that it doesn't need to ```\nsub rsp, 8```\n or anything because the x86-64 SysV ABI provides a red-zone that signal handlers won't step on.)\nSome people store to an array and sum all the elements, but compilers usually don't realize that the low element of the array is still there in a register from before the store.\n\nInteger:\n```\npshufd```\n is a convenient copy-and-shuffle.  Bit and byte shifts are unfortunately in-place, and ```\npunpckhqdq```\n puts the high half of the destination in the low half of the result, opposite of the way ```\nmovhlps```\n can extract the high half into a different register.\nUsing ```\nmovhlps```\n for the first step might be good on some CPUs, but only if we have a scratch reg.  ```\npshufd```\n is a safe choice, and fast on everything after Merom.\n```\nint hsum_epi32_sse2(__m128i x) {\n#ifdef __AVX__\n    __m128i hi64  = _mm_unpackhi_epi64(x, x);           // 3-operand non-destructive AVX lets us save a byte without needing a mov\n#else\n    __m128i hi64  = _mm_shuffle_epi32(x, _MM_SHUFFLE(1, 0, 3, 2));\n#endif\n    __m128i sum64 = _mm_add_epi32(hi64, x);\n    __m128i hi32  = _mm_shufflelo_epi16(sum64, _MM_SHUFFLE(1, 0, 3, 2));    // Swap the low two elements\n    __m128i sum32 = _mm_add_epi32(sum64, hi32);\n    return _mm_cvtsi128_si32(sum32);       // SSE2 movd\n    //return _mm_extract_epi32(hl, 0);     // SSE4, even though it compiles to movd instead of a literal pextrd r32,xmm,0\n}\n\n    # gcc 5.3 -O3\n    pshufd xmm1,xmm0,0x4e\n    paddd  xmm0,xmm1\n    pshuflw xmm1,xmm0,0x4e\n    paddd  xmm0,xmm1\n    movd   eax,xmm0\n\nint hsum_epi32_ssse3_slow_smallcode(__m128i x){\n    x = _mm_hadd_epi32(x, x);\n    x = _mm_hadd_epi32(x, x);\n    return _mm_cvtsi128_si32(x);\n}\n```\n\nOn some CPUs, it's safe to use FP shuffles on integer data.  I didn't do this, since on modern CPUs that will at most save 1 or 2 code bytes, with no speed gains (other than code size/alignment effects).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "If / Else if statement reduction\r\n                \r\nFrom my server, I get a XML list with \"Timelines\" that are running.\nWhen a Timeline is in a \"Running\"or \"Held at end\" state a button will be in an active state.\n\nI have done it this way and it works.\nBut I would like to turn down the \"Else\" statements in my code.\nAny ideas?\n\n```\n function statusCheck()\n{\n    $.ajax({\n        type: \"GET\",\n        url: \"/query/timelineStatus?id=1-30\",\n        dataType: \"xml\",\n        success: function (xml) {\n            $(xml).find(\"timelineStatus\").each(function(){\n                var timelineId = parseInt($(this).attr(\"id\"));\n                var playState = $(this).find(\"playState\").text();\n                if (timelineId == 1) // timeline_1\n                {\n                    changeJQMThemeSwatch(\"#timeline_1\", (playState == \"Running\" || playState == \"Held at end\") ? \"b\" : \"a\");\n                }\n                else if (timelineId == 2) // timeline_2\n                {\n                    changeJQMThemeSwatch(\"#timeline_2\", (playState == \"Running\" || playState == \"Held at end\") ? \"b\" : \"a\");\n                }\n                else if (timelineId == 3) // timeline_3\n                {\n                    changeJQMThemeSwatch(\"#timeline_3\", (playState == \"Running\" || playState == \"Held at end\") ? \"b\" : \"a\");\n                }\n                else if (timelineId == 4) // timeline_4\n                {\n                    changeJQMThemeSwatch(\"#timeline_4\", (playState == \"Running\" || playState == \"Held at end\") ? \"b\" : \"a\");\n                }\n                else if (timelineId == 5) // timeline_5\n                {\n                    changeJQMThemeSwatch(\"#timeline_5\", (playState == \"Running\" || playState == \"Held at end\") ? \"b\" : \"a\");\n                }\n                else if (timelineId == 6) // timeline_6\n                {\n                    changeJQMThemeSwatch(\"#timeline_6\", (playState == \"Running\" || playState == \"Held at end\") ? \"b\" : \"a\");\n                }\n\n        }\n    })\n}\n```\n\n    ", "Answer": "\r\nDepending on how standard your id naming is, you could just use a small string concatenation.\n\n```\nfunction statusCheck()\n{\n    $.ajax({\n        type: \"GET\",\n        url: \"/query/timelineStatus?id=1-30\",\n        dataType: \"xml\",\n        success: function (xml) {\n            $(xml).find(\"timelineStatus\").each(function(){\n                var timelineId = parseInt($(this).attr(\"id\"));\n                var playState = $(this).find(\"playState\").text();\n                if (timelineId > 0 && timelineId <= 6) {\n                    changeJQMThemeSwatch(\"#timeline_\" + timelineId, (playState == \"Running\" || playState == \"Held at end\") ? \"b\" : \"a\");\n                }\n            });\n        }\n    });\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Repetitive if statements reduction\r\n                \r\nWhat are the ways of reducing code quantity in a method that contains schematic conditional statements? \n\nWhat would be a good practice of placing such code. Should it be in a different method/class?\n\nExample code in Java. It contains one scheme repeated twice:\n\n```\nif (calculation[i].equals(\"*\")) {\n    if (stack.canDoOperation()) {\n        times();\n    } else {\n        operationFailed = true;\n    }\n} else if (calculation[i].equals(\"+\")) {\n    if (stack.canDoOperation()) {\n        sum();\n    } else {\n        operationFailed = true;\n    }\n}\n```\n\n    ", "Answer": "\r\nYou could write a helper method, wrapping the given operation method into the commonly required checks. You could also put a ```\ntry/catch```\n in there, in case the operation can fail.\n\n```\nprivate boolean tryOperation(Runnable operation) {\n    if (stack.canDoOperation()) {\n        operation.run();\n        return true;\n    } else {\n        return false;\n    }\n}\n```\n\n\nAnd use like this:\n\n```\nif (calculation[i].equals(\"*\")) {\n    operationFailed = ! tryOperation(this::times);\n} else if (calculation[i].equals(\"+\")) {\n    operationFailed = ! tryOperation(this::sum);\n}\n```\n\n\nOr, with Java 8, you can put the method references to those operations into a ```\nMap```\n:\n\n```\nMap<String, Runnable> operations = new HashMap<>();\noperations.put(\"*\", this::times);\noperations.put(\"+\", this::sum);\n...\n\nRunnable operation = operations.get(calculation[i]);\nif (operation != null && stack.canDoOperation()) {\n    operation.run();\n} else {\n    operationFailed = true;\n}\n```\n\n\nYou can also combine the two approaches:\n\n```\noperationFailed = ! tryOperation(operations.get(calculation[i]);\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "omp max reduction with storage of index\r\n                \r\nUsing c++ openmp 3.1 I implemented a max reduction which stores the maximum value of integer variable (score) of an vector of objects (s). But I also want to store the vector index to acces the (s) object with the maximum score.\nMy current unsuccesfull implementation looks like this:     \n\n```\n//s is a vector of sol objects which contain apart from other variables an  integer     score    variable s[].score            \nint bestscore = 0;\nint bestant = 0;                \n#pragma omp parallel shared(bestant)\n{//start parallel session\n    #pragma omp for    nowait reduction(max : bestscore)\n    for (int ant = 0; ant<maxsols; ++ant) // for all ants\n    {\n        //procedures on s[ant] object which update the  int s[ant].score\n        if (s[ant].score > bestscore)\n        {\n            //find the object with the highest score\n            bestscore = s[ant].score;\n            bestant = ant;//i also want know which ant has the highest score\n        }\n    }\n}\n```\n\n\nThe code compiles and runs. the maximum bestscore is found but bestant gets a random index. The ant linked to the fastest thread to finish gets stored in bestant.\nbestscore start with a value of 0 so in most cases s[ant].score will have a higher score and bestscore and bestant are updated.\n I think I need a reduction operator for bestant like \"on update of bestscore\".\n    ", "Answer": "\r\nThe reason (as you suspect) why ```\nbestant```\n gets a random index ```\ni```\n is because ```\nbestant```\n is shared and does not benefit from the reduction clause as ```\nbestscore```\n does. The solution proposed by Z boson is fine: the ```\ncritical```\n instruction block is executed only once by thread so that the overhead should be limited.\nYou were using a OpenMP 3.1 runtime at that time. I wanted to post to explain that this limitation has been addressed since OpenMP 4.0. You can now write a user defined operator (see 2.19.5.7 declare reduction Directive).\nIn your case, a solution can be to pack the two values in a struct and\ndefine how two such struct elements combine in the end of the ```\n#pragma parallel for```\n loop.\n```\n//s is a vector of sol objects which contain apart from other variables an  integer     score    variable s[].score\n\ntypedef struct {\n  int score;\n  int ant;\n} best_t;\n\nbest_t best = { 0, 0 };\n\n// we declare our user reduction operator :\n// it is called get_max, return a a value of type best_t.\n// omp_out and omp_in are the predefined names of two private elements\n// to combine in the end to the final shared variable.\n\n#pragma omp declare reduction(get_max : best_t :\\\n    omp_out = omp_out.score > omp_in.score ? omp_out : omp_in)\\\n    initializer (omp_priv=(omp_orig))\n\n                \n#pragma omp parallel \n{//start parallel session\n    #pragma omp for    nowait reduction(get_max : best)\n    for (int ant = 0; ant<maxsols; ++ant) // for all ants\n    {\n        //procedures on s[ant] object which update the  int s[ant].score\n        if (s[ant].score > best.score)\n        {\n            //find the object with the highest score\n            best.score = s[ant].score;\n            best.ant = ant;\n        }\n    }\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA summation reduction puzzle\r\n                \r\nReduction in CUDA has utterly baffled me! First off, both this tutorial by Mark Harris and this one by Mike Giles make use of the declaration ```\nextern __shared__ temp[]```\n. The keyword ```\nextern```\n is used in C when a declaration is made, but allocation takes place \"elsewhre\" (e.g. in another C file context in general). What is the relevance of ```\nextern```\n here? Why don't we use:\n\n```\n__shared__ float temp[N/2];\n```\n\n\nfor instance? Or why don't we declare ```\ntemp```\n to be a global variable, e.g.\n\n```\n#define N 1024\n__shared__ float temp[N/2];\n\n__global__ void sum(float *sum,  float *data){ ... }\n\nint main(){\n ...\n sum<<<M,L>>>(sum, data);\n}\n```\n\n\nI have yet another question? How many blocks and threads per block should one use to invoke the summation kernel? I tried this example (based on this).\n\nNote: You can find information about my devices here.\n    ", "Answer": "\r\nThe answer to the first question is that CUDA supports dynamic shared memory allocation at runtime (see this SO question and the documentation for more details). The declaration of shared memory using ```\nextern```\n denotes to the compiler that shared memory size will be determined at kernel launch, passed in bytes as an argument to the ```\n<<< >>>```\n syntax (or equivalently via an API function), something like:\n\n```\nsum<<< gridsize, blocksize, sharedmem_size >>>(....);\n```\n\n\nThe second question is normally to launch the number of blocks which will completely fill all the streaming multiprocessors on your GPU. Most sensibly written reduction kernels will accumulate many values per thread and then perform a shared memory reduction. The reduction requires that the number of threads per block be a power of two: That usually gives you 32, 64, 128, 256, 512 (or 1024 if you have a Fermi or Kepler GPU). It is a very finite search space, just benchmark to see what works best on your hardware. You can find a more general discussion about block and grid sizing here and here.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "reduction of each row in the table of database\r\n                \r\ni have a table temporary as follow as:\n\n```\nstudent    |      Data     |    number           \n-----------|---------------|--------------\n1          |   book        |      2          \n1          |   book        |      5    \n1          |   book        |      9     \n2          |   book        |      1         \n2          |   book        |      5     \n```\n\n\ni will show reduction of column in like as output column as follow as:\n\n```\nstudent    |   Data        |    number      |output (number column of next row-previous line )\n-----------|---------------|----------------|--------------\n1          |   book        |      2         |     0\n1          |   book        |      5         |     3  (result of (5-2=3)\n1          |   book        |      9         |     4  (result of (9-5=4)\n2          |   book        |      1         |     0\n2          |   book        |      5         |     4  (result of (5-1=4)\n```\n\n\nhow are writing of php's script is correct? because i'm confused\n    ", "Answer": "\r\nYou didn't mention your DBMS, so this is standard SQL:\n\n```\nselect student,\n       data, \n       number,\n       number - lag(number,1,number) over (partition by student order by id) as output\nfrom the_table\norder by student, id\n```\n\n\nSQLFiddle example\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Fastest way to do horizontal SSE vector sum (or other reduction)\r\n                \r\nGiven a vector of three (or four) floats. What is the fastest way to sum them?\n\nIs SSE (movaps, shuffle, add, movd) always faster than x87? Are the horizontal-add instructions in SSE3 worth it?\n\nWhat's the cost to moving to the FPU, then faddp, faddp? What's the fastest specific instruction sequence?\n\n\"Try to arrange things so you can sum four vectors at a time\" will not be accepted as an answer. :-)  e.g. for summing an array, you can use multiple vector accumulators for vertical sums (to hide addps latency), and reduce down to one after the loop, but then you need to horizontally sum that last vector.\n    ", "Answer": "\r\nIn general for any kind of vector horizontal reduction, extract / shuffle high half to line up with low, then vertical add (or min/max/or/and/xor/multiply/whatever); repeat until a there's just a single element (with high garbage in the rest of the vector).\nIf you start with vectors wider than 128-bit, narrow in half until you get to 128 (then you can use one of the functions in this answer on that vector).  But if you need the result broadcast to all elements at the end, then you can consider doing full-width shuffles all the way.\nRelated Q&As for wider vectors, and integers, and FP\n\n```\n__m128```\n and ```\n__m128d```\n This answer (see below)\n\n```\n__m256d```\n with perf analysis for Ryzen 1 vs. Intel (showing why ```\nvextractf128```\n is vastly better than ```\nvperm2f128```\n) Get sum of values stored in __m256d with SSE/AVX\n\n```\n__m256```\n How to sum __m256 horizontally?\n\nIntel AVX: 256-bits version of dot product for double precision floating point variables of single vectors.\n\nDot product of arrays (not just a single vector of 3 or 4 elements): do vertical mul/add or FMA into multiple accumulators, and hsum at the end.  Complete AVX+FMA array dot-product example, including an efficient hsum after the loop.  (For the simple sum or other reduction of an array, use that pattern but without the multiply part, e.g. add instead of fma).  Do not do the horizontal work separately for each SIMD vector; do it once at the end.\nHow to count character occurrences using SIMD as an integer example of counting ```\n_mm256_cmpeq_epi8```\n matches, again over a whole array, only hsumming at the end.  (Worth special mention for doing some 8-bit accumulation then widening 8 -> 64-bit to avoid overflow without doing a full hsum at that point.)\n\n\nInteger\n\n```\n__m128i```\n 32-bit elements: this answer (see below).  64-bit elements should be obvious: only one pshufd/paddq step.\n\n```\n__m128i```\n 8-bit unsigned ```\nuint8_t```\n elements without wrapping/overflow: ```\npsadbw```\n against ```\n_mm_setzero_si128()```\n, then hsum the two qword halves (or 4 or 8 for wider vectors). Fastest way to horizontally sum SSE unsigned byte vector shows 128-bit with SSE2.\nSumming 8-bit integers in __m512i with AVX intrinsics has an AVX512 example.  How to count character occurrences using SIMD has an AVX2 ```\n__m256i```\n example.\n(For ```\nint8_t```\n signed bytes you can XOR set1_epi8(0x80) to flip to unsigned before SAD, then subtract the bias from the final hsum; see details here, also showing an optimization for doing only 9 bytes from memory instead of 16).\n\n16-bit unsigned: ```\n_mm_madd_epi16```\n with set1_epi16(1) is a single-uop widening horizontal add: SIMD: Accumulate Adjacent Pairs.  Then proceed with a 32-bit hsum.\n\n```\n__m256i```\n and ```\n__m512i```\n with 32-bit elements.\nFastest method to calculate sum of all packed 32-bit integers using AVX512 or AVX2.  For AVX512, Intel added a bunch of \"reduce\" inline functions (not hardware instructions) that do this for you, like ```\n_mm512_reduce_add_ps```\n (and pd, epi32, and epi64).  Also reduce_min/max/mul/and/or.  Doing it manually leads to basically the same asm.\n\nhorizontal max (instead of add): Getting max value in a __m128i vector with SSE?\n\n\n\nMain answer to this question: mostly float and ```\n__m128```\n\nHere are some versions tuned based on Agner Fog's microarch guide's microarch guide and instruction tables.  See also the x86 tag wiki.  They should be efficient on any CPU, with no major bottlenecks.  (e.g. I avoided things that would help one uarch a bit but be slow on another uarch).  Code-size is also minimized.\nThe common SSE3 / SSSE3 2x ```\nhadd```\n idiom is only good for code-size, not speed on any existing CPUs.  There are use-cases for it (like transpose and add, see below), but a single vector isn't one of them.\nI've also included an AVX version.  Any kind of horizontal reduction with  AVX / AVX2 should start with a ```\nvextractf128```\n and a \"vertical\" operation to reduce down to one XMM (```\n__m128```\n) vector.  In general for wide vectors, your best bet is to narrow in half repeatedly until you're down to a 128-bit vector, regardless of element type.  (Except for 8-bit integer, then ```\nvpsadbw```\n as a first step if you want to hsum without overflow to wider elements.)\nSee the asm output from all this code on the Godbolt Compiler Explorer.  See also my improvements to Agner Fog's C++ Vector Class Library ```\nhorizontal_add```\n functions.  (message board thread, and code on github).  I used CPP macros to select optimal shuffles for code-size for SSE2, SSE4, and AVX, and for avoiding ```\nmovdqa```\n when AVX isn't available.\n\nThere are tradeoffs to consider:\n\ncode size: smaller is better for L1 I-cache reasons, and for code fetch from disk (smaller binaries).  Total binary size mostly matters for compiler decisions made repeatedly all over a program.  If you're bothering to hand-code something with intrinsics, it's worth spending a few code bytes if it gives any speedup for the whole program (be careful of microbenchmarks that make unrolling look good).\nuop-cache size: Often more precious than L1 I$.  4 single-uop instructions can take less space than 2 ```\nhaddps```\n, so this is highly relevant here.\nlatency: Sometimes relevant\nthroughput (back-end ports): usually irrelevant, horizontal sums shouldn't be the only thing in an innermost loop.  Port pressure matters only as part of the whole loop that contains this.\nthroughput (total front-end fused-domain uops): If surrounding code doesn't bottleneck on the same port that the hsum uses, this is a proxy for the impact of the hsum on the throughput of the whole thing.\n\nWhen a horizontal add is infrequent:\nCPUs with no uop-cache might favour 2x ```\nhaddps```\n if it's very rarely used: It's slowish when it does run, but that's not often.  Being only 2 instructions minimizes the impact on the surrounding code (I$ size).\nCPUs with a uop-cache will probably favour something that takes fewer uops, even if it's more instructions / more x86 code-size.  Total uops cache-lines used is what we want to minimize, which isn't as simple as minimizing total uops (taken branches and 32B boundaries always start a new uop cache line).\nAnyway, with that said, horizontal sums come up a lot, so here's my attempt at carefully crafting some versions that compile nicely.  Not benchmarked on any real hardware, or even carefully tested.  There might be bugs in the shuffle constants or something.\n\nIf you're making a fallback / baseline version of your code, remember that only old CPUs will run it; newer CPUs will run your AVX version, or SSE4.1 or whatever.\nOld CPUs like K8, and Core2(merom) and earlier only have 64bit shuffle units.  Core2 has 128bit execution units for most instructions, but not for shuffles.  (Pentium M and K8 handle all 128b vector instructions as two 64bit halves).\nShuffles like ```\nmovhlps```\n that move data in 64-bit chunks (no shuffling within 64-bit halves) are fast, too.\nRelated: shuffles on new CPUs, and tricks for avoiding 1/clock shuffle throughput bottleneck on Haswell and later: Do 128bit cross lane operations in AVX512 give better performance?\nOn old CPUs with slow shuffles:\n\n```\nmovhlps```\n (Merom: 1uop) is significantly faster than ```\nshufps```\n (Merom: 3uops).  On Pentium-M, cheaper than ```\nmovaps```\n.  Also, it runs in the FP domain on Core2, avoiding the bypass delays from other shuffles.\n```\nunpcklpd```\n is faster than ```\nunpcklps```\n.\n```\npshufd```\n is slow, ```\npshuflw```\n/```\npshufhw```\n are fast (because they only shuffle a 64bit half)\n```\npshufb mm0```\n (MMX) is fast, ```\npshufb xmm0```\n is slow.\n```\nhaddps```\n is very slow (6uops on Merom and Pentium M)\n```\nmovshdup```\n (Merom: 1uop) is interesting: It's the only 1uop insn that shuffles within 64b elements.\n\n```\nshufps```\n on Core2(including Penryn) brings data into the integer domain, causing a bypass delay to get it back to the FP execution units for ```\naddps```\n, but ```\nmovhlps```\n is entirely in the FP domain.  ```\nshufpd```\n also runs in the float domain.\n```\nmovshdup```\n runs in the integer domain, but is only one uop.\nAMD K10, Intel Core2(Penryn/Wolfdale), and all later CPUs, run all xmm shuffles as a single uop.  (But note the bypass delay with ```\nshufps```\n on Penryn, avoided with ```\nmovhlps```\n)\n\nWithout AVX, avoiding wasted ```\nmovaps```\n/```\nmovdqa```\n instructions requires careful choice of shuffles.  Only a few shuffles work as a copy-and-shuffle, rather than modifying the destination.  Shuffles that combine data from two inputs (like ```\nunpck*```\n or ```\nmovhlps```\n) can be used with a tmp variable that's no longer needed instead of ```\n_mm_movehl_ps(same,same)```\n.\nSome of these can be made faster (save a MOVAPS) but uglier / less \"clean\" by taking a dummy arg for use as a destination for an initial shuffle.  For example:\n```\n// Use dummy = a recently-dead variable that vec depends on,\n//  so it doesn't introduce a false dependency,\n//  and the compiler probably still has it in a register\n__m128d highhalf_pd(__m128d dummy, __m128d vec) {\n#ifdef __AVX__\n    // With 3-operand AVX instructions, don't create an extra dependency on something we don't need anymore.\n    (void)dummy;\n    return _mm_unpackhi_pd(vec, vec);\n#else\n    // Without AVX, we can save a MOVAPS with MOVHLPS into a dead register\n    __m128 tmp = _mm_castpd_ps(dummy);\n    __m128d high = _mm_castps_pd(_mm_movehl_ps(tmp, _mm_castpd_ps(vec)));\n    return high;\n#endif\n}\n```\n\n\nSSE1 (aka SSE):\n```\nfloat hsum_ps_sse1(__m128 v) {                                  // v = [ D C | B A ]\n    __m128 shuf   = _mm_shuffle_ps(v, v, _MM_SHUFFLE(2, 3, 0, 1));  // [ C D | A B ]\n    __m128 sums   = _mm_add_ps(v, shuf);      // sums = [ D+C C+D | B+A A+B ]\n    shuf          = _mm_movehl_ps(shuf, sums);      //  [   C   D | D+C C+D ]  // let the compiler avoid a mov by reusing shuf\n    sums          = _mm_add_ss(sums, shuf);\n    return    _mm_cvtss_f32(sums);\n}\n    # gcc 5.3 -O3:  looks optimal\n    movaps  xmm1, xmm0     # I think one movaps is unavoidable, unless we have a 2nd register with known-safe floats in the upper 2 elements\n    shufps  xmm1, xmm0, 177\n    addps   xmm0, xmm1\n    movhlps xmm1, xmm0     # note the reuse of shuf, avoiding a movaps\n    addss   xmm0, xmm1\n\n    # clang 3.7.1 -O3:  \n    movaps  xmm1, xmm0\n    shufps  xmm1, xmm1, 177\n    addps   xmm1, xmm0\n    movaps  xmm0, xmm1\n    shufpd  xmm0, xmm0, 1\n    addss   xmm0, xmm1\n```\n\nI reported a clang bug about pessimizing the shuffles.  It has its own internal representation for shuffling, and turns that back into shuffles.  gcc more often uses the instructions that directly match the intrinsic you used.\nOften clang does better than gcc, in code where the instruction choice isn't hand-tuned, or constant-propagation can simplify things even when the intrinsics are optimal for the non-constant case.  Overall it's a good thing that compilers work like a proper compiler for intrinsics, not just an assembler.  Compilers can often generate good asm from scalar C that doesn't even try to work the way good asm would.  Eventually compilers will treat intrinsics as just another C operator as input for the optimizer.\n\nSSE3\n```\nfloat hsum_ps_sse3(__m128 v) {\n    __m128 shuf = _mm_movehdup_ps(v);        // broadcast elements 3,1 to 2,0\n    __m128 sums = _mm_add_ps(v, shuf);\n    shuf        = _mm_movehl_ps(shuf, sums); // high half -> low half\n    sums        = _mm_add_ss(sums, shuf);\n    return        _mm_cvtss_f32(sums);\n}\n\n    # gcc 5.3 -O3: perfectly optimal code\n    movshdup    xmm1, xmm0\n    addps       xmm0, xmm1\n    movhlps     xmm1, xmm0\n    addss       xmm0, xmm1\n```\n\nThis has several advantages:\n\ndoesn't require any ```\nmovaps```\n copies to work around destructive shuffles (without AVX): ```\nmovshdup xmm1, xmm2```\n's destination is write-only, so it creates ```\ntmp```\n out of a dead register for us.  This is also why I used ```\nmovehl_ps(tmp, sums)```\n instead of ```\nmovehl_ps(sums, sums)```\n.\n\nsmall code-size.  The shuffling instructions are small:  ```\nmovhlps```\n is 3 bytes, ```\nmovshdup```\n is 4 bytes (same as ```\nshufps```\n).  No immediate byte is required, so with AVX, ```\nvshufps```\n is 5 bytes but ```\nvmovhlps```\n and ```\nvmovshdup```\n are both 4.\n\n\nI could save another byte with ```\naddps```\n instead of ```\naddss```\n.  Since this won't be used inside inner loops, the extra energy to switch the extra transistors is probably negligible.  FP exceptions from the upper 3 elements aren't a risk, because all elements hold valid FP data.  However, clang/LLVM actually \"understands\" vector shuffles, and emits better code if it knows that only the low element matters.\nLike the SSE1 version, adding the odd elements to themselves may cause FP exceptions (like overflow) that wouldn't happen otherwise, but this shouldn't be a problem.  Denormals are slow, but IIRC producing a +Inf result isn't on most uarches.\n\nSSE3 optimizing for code-size\nIf code-size is your major concern, two ```\nhaddps```\n (```\n_mm_hadd_ps```\n) instructions will do the trick (Paul R's answer).  This is also the easiest to type and remember.  It is not fast, though.  Even Intel Skylake still decodes each ```\nhaddps```\n to 3 uops, with 6 cycle latency.  So even though it saves machine-code bytes (L1 I-cache), it takes up more space in the more-valuable uop-cache.  Real use-cases for ```\nhaddps```\n: a transpose-and-sum problem, or doing some scaling at an intermediate step in this SSE ```\natoi()```\n implementation.\n\nAVX:\nThis version saves a code byte vs. Marat's answer to the AVX question.\n```\n#ifdef __AVX__\nfloat hsum256_ps_avx(__m256 v) {\n    __m128 vlow  = _mm256_castps256_ps128(v);\n    __m128 vhigh = _mm256_extractf128_ps(v, 1); // high 128\n           vlow  = _mm_add_ps(vlow, vhigh);     // add the low 128\n    return hsum_ps_sse3(vlow);         // and inline the sse3 version, which is optimal for AVX\n    // (no wasted instructions, and all of them are the 4B minimum)\n}\n#endif\n\n vmovaps xmm1,xmm0               # huh, what the heck gcc?  Just extract to xmm1\n vextractf128 xmm0,ymm0,0x1\n vaddps xmm0,xmm1,xmm0\n vmovshdup xmm1,xmm0\n vaddps xmm0,xmm1,xmm0\n vmovhlps xmm1,xmm1,xmm0\n vaddss xmm0,xmm0,xmm1\n vzeroupper \n ret\n```\n\n\nDouble-precision:\n```\ndouble hsum_pd_sse2(__m128d vd) {                      // v = [ B | A ]\n    __m128 undef  = _mm_undefined_ps();                       // don't worry, we only use addSD, never touching the garbage bits with an FP add\n    __m128 shuftmp= _mm_movehl_ps(undef, _mm_castpd_ps(vd));  // there is no movhlpd\n    __m128d shuf  = _mm_castps_pd(shuftmp);\n    return  _mm_cvtsd_f64(_mm_add_sd(vd, shuf));\n}\n\n# gcc 5.3.0 -O3\n    pxor    xmm1, xmm1          # hopefully when inlined, gcc could pick a register it knew wouldn't cause a false dep problem, and avoid the zeroing\n    movhlps xmm1, xmm0\n    addsd   xmm0, xmm1\n\n\n# clang 3.7.1 -O3 again doesn't use movhlps:\n    xorpd   xmm2, xmm2          # with  #define _mm_undefined_ps _mm_setzero_ps\n    movapd  xmm1, xmm0\n    unpckhpd        xmm1, xmm2\n    addsd   xmm1, xmm0\n    movapd  xmm0, xmm1    # another clang bug: wrong choice of operand order\n\n\n// This doesn't compile the way it's written\ndouble hsum_pd_scalar_sse2(__m128d vd) {\n    double tmp;\n    _mm_storeh_pd(&tmp, vd);       // store the high half\n    double lo = _mm_cvtsd_f64(vd); // cast the low half\n    return lo+tmp;\n}\n\n    # gcc 5.3 -O3\n    haddpd  xmm0, xmm0   # Lower latency but less throughput than storing to memory\n\n    # ICC13\n    movhpd    QWORD PTR [-8+rsp], xmm0    # only needs the store port, not the shuffle unit\n    addsd     xmm0, QWORD PTR [-8+rsp]\n```\n\nStoring to memory and back avoids an ALU uop.  That's good if shuffle port pressure, or ALU uops in general, are a bottleneck.  (Note that it doesn't need to ```\nsub rsp, 8```\n or anything because the x86-64 SysV ABI provides a red-zone that signal handlers won't step on.)\nSome people store to an array and sum all the elements, but compilers usually don't realize that the low element of the array is still there in a register from before the store.\n\nInteger:\n```\npshufd```\n is a convenient copy-and-shuffle.  Bit and byte shifts are unfortunately in-place, and ```\npunpckhqdq```\n puts the high half of the destination in the low half of the result, opposite of the way ```\nmovhlps```\n can extract the high half into a different register.\nUsing ```\nmovhlps```\n for the first step might be good on some CPUs, but only if we have a scratch reg.  ```\npshufd```\n is a safe choice, and fast on everything after Merom.\n```\nint hsum_epi32_sse2(__m128i x) {\n#ifdef __AVX__\n    __m128i hi64  = _mm_unpackhi_epi64(x, x);           // 3-operand non-destructive AVX lets us save a byte without needing a mov\n#else\n    __m128i hi64  = _mm_shuffle_epi32(x, _MM_SHUFFLE(1, 0, 3, 2));\n#endif\n    __m128i sum64 = _mm_add_epi32(hi64, x);\n    __m128i hi32  = _mm_shufflelo_epi16(sum64, _MM_SHUFFLE(1, 0, 3, 2));    // Swap the low two elements\n    __m128i sum32 = _mm_add_epi32(sum64, hi32);\n    return _mm_cvtsi128_si32(sum32);       // SSE2 movd\n    //return _mm_extract_epi32(hl, 0);     // SSE4, even though it compiles to movd instead of a literal pextrd r32,xmm,0\n}\n\n    # gcc 5.3 -O3\n    pshufd xmm1,xmm0,0x4e\n    paddd  xmm0,xmm1\n    pshuflw xmm1,xmm0,0x4e\n    paddd  xmm0,xmm1\n    movd   eax,xmm0\n\nint hsum_epi32_ssse3_slow_smallcode(__m128i x){\n    x = _mm_hadd_epi32(x, x);\n    x = _mm_hadd_epi32(x, x);\n    return _mm_cvtsi128_si32(x);\n}\n```\n\nOn some CPUs, it's safe to use FP shuffles on integer data.  I didn't do this, since on modern CPUs that will at most save 1 or 2 code bytes, with no speed gains (other than code size/alignment effects).\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "\"reduction on infixes\" in Racket?\r\n                \r\nI am working on a compiler pass and want to remove redundant ```\nmovq```\n instructions. For example, this list:\n\n```\n((movq a b)\n (movq b c)\n (movq c d)\n (movq d e))\n```\n\n\nshould become ```\n((movq a e))```\n.\n\nLikewise, the list\n\n```\n((movq a b)\n (movq b c)\n (addq 20 c)\n (movq a b)\n (movq c d)\n (movq d e))\n```\n\n\nshould reduce to this:\n\n```\n((movq a c)\n (addq 20 c)\n (movq a b)\n (movq c e))\n```\n\n\nMy current (working) approach fuses the current and next ```\nmovq```\n instructions if the destination of the current matches the source of the next:\n\n```\n(define (fuse-movq lst) ;; ((movq x y) (movq y z)) => ((movq x z))\n  (match lst\n    [`(,x) `(,x)]\n    [else\n     (define-values (x z) (values (car lst) (cddr lst)))\n     (match* (x (cadr lst))\n       [(`(movq ,a ,b) `(movq ,b ,c)) (fuse-movq (cons `(movq ,a ,c) z))]\n       [(_ _) (append (list x) (fuse-movq (cdr lst)))])]))\n\n```\n\n\nThis is fine, but I would much prefer to have the logic separate from the list traversal, something more like this:\n\n```\n;; core logic\n(define (fuse-movq x y)\n  (match* (x y)\n    [(`(movq ,a ,b) `(movq ,b ,c)) `(movq ,a ,c)]\n    [(_ _) (x y)]))\n\n;; list traversal handled by `foldl`\n(foldl fuse-movq '() '((movq a b) (movq b c) (movq c d)))\n```\n\n\nUnfortunately ```\nfoldl```\n doesn't seem quite right, and ```\nmap```\n doesn't work because I want to process \"this and the next element\".\n\nI tagged this with APL and J because the idiomatic way to apply infix function ```\nf```\n to list ```\nlst```\n in J is ```\nf/\\lst```\n.  Where ```\nf/```\n is roughly translated as ```\napply f between the next 2 elements```\n and ```\n\\```\n is ```\nprefix scan```\n. It's so common in those languages that I hoped to find a similar idiom in Racket.\n\nIn Racket, is there a way to decouple this \"infix\" kind of function behavior from the list traversal?\n    ", "Answer": "\r\nYou should be able to adjust this for your use case.\n\n```\n#lang racket\n\n(define (transform xs)\n  (for/fold ([prev (list (first xs))] #:result (reverse prev))\n            ([item (in-list (rest xs))])\n    (match* (prev item)\n      [((list (list p q) xs ...) (list q r)) (cons (list p r) xs)]\n      [(xs item) (cons item xs)])))\n\n(transform '([a b] [c d] [d e] [e f] [s t] [g h] [h k] [x y] [y z]))\n;=> '((a b) (c f) (s t) (g k) (x z))\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction of rows in a table\r\n                \r\nHow would I make this so that it just contains one row without any null values. I am unsure how to go about doing this.\n```\n\n+----+-------------+--------------+-----------------------+\n| id |    Col1     |     Col2     |         Col3          |\n+----+-------------+--------------+-----------------------+\n|  1 | description | null         | null                  |\n|  1 | null        | descriptions | null                  |\n|  1 | null        | null         | different description |\n+----+-------------+--------------+-----------------------+\n```\n\n\nExample of what I want to have\n```\n\n+----+-------------+--------------+-----------------------+\n| id |    Col1     |     Col2     |         Col3          |\n+----+-------------+--------------+-----------------------+\n|  1 | description | descriptions | different description |\n+----+-------------+--------------+-----------------------+\n```\n\n    ", "Answer": "\r\nYou can do with two query\nfirst an insert/select with (fake) aggregation function and group by for populate all the column of a row\n\n```\n  insert into my_table (id, col1, col2,col3)\n  select id, max(col1), max(col2), max(col3)\n  from my_table \n  group by id\n  ;\n```\n\n\nsecond  a delete for remove the row with some null coll\n\n```\n  delete from \n  my_table \n  where col1 is null\n  or col2 is null\n  or col3 is null\n  ;\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Example of stream reduction with distinct combiner and accumulator\r\n                \r\nThe question is about ```\njava.util.stream.Stream.reduce(U identity,BiFunction<U, ? super T, U> accumulator, BinaryOperator<U> combiner)```\n method.\n\nOne of the requirements is that the combiner function must be compatible with the accumulator function; for all u and t, the following must hold:\n\n```\n combiner.apply(u, accumulator.apply(identity, t)) == accumulator.apply(u, t) (*) \n```\n\n\nIf the ```\ncombiner```\n and ```\naccumulator```\n are the same, the above equality is automatically true.\n\nA ```\nBinaryOperator```\n is actually extending BiFunction, therefore I can use it when ```\nBiFunction```\n is required. If U and T are identical, the following is always legal:\n\n```\noperator<T> op = (x,y) -> something;\n\nstream.reduce(id, op, op);\n```\n\n\nOf course, one cannot always use the ```\ncombiner```\n as ```\nacumulator```\n since, in the general case, they serve for different purposes and are different Java types.\n\nMy question\n\nIs there an example of stream reduction with distinct ```\ncombiner```\n and ```\naccumulator```\n?\n\nAlso, I'm not interested in trivial examples, but natural examples that I can encounter in practice while doing reduction on parallel streams.\n\nFor trivial examples, there are many tutorials, like this one\n\nWhy am I asking this question\n\nBasically, the reason this reduction method exists is for parallel streams. It seems to me the condition (*) is so strong that, in practice, it renders this reduction useless since rarely the reduction operations fulfill it.\n    ", "Answer": "\r\nIf the combiner and accumulator are the same? You are confusing things here.\n\n```\naccumulator```\n transforms from ```\nX```\n to ```\nY```\n for example (using the identity), while ```\ncombiner```\n merges two ```\nY```\n into one. Also notice that one is a ```\nBiFunction```\n and the other one is a ```\nBinaryOperator```\n (which is actually a ```\nBiFunction<T, T, T>```\n).\n\n\n  Is there an example of stream reduction with distinct combiner and accumulator?\n\n\nThese look pretty different to me:\n\n```\n    Stream.of(\"1\", \"2\")\n          .reduce(0, (x, y) -> x + y.length(), Integer::sum);\n```\n\n\nI think you might be confused with things like:\n\n```\nStream.of(\"1\", \"2\")\n      .reduce(\"\", String::concat, String::concat);\n```\n\n\nHow is it possible to do?\n\n```\nBiFunction<String, String, String> bi = String::concat;\n```\n\n\nWell there is a hint here. \n\nEDIT\n\nAddressing the part where \"different\" means different operations, ```\naccumulator```\n might ```\nsum```\n, while ```\naccumulator```\n might ```\nmultiply```\n. This is exactly what the rule :\n\n\n  combiner.apply(u, accumulator.apply(identity, t)) == accumulator.apply(u, t)\n\n\nis about, to protected itself from two separate associative functions, but different operations. Let's take an example of two lists (equal, but with different order). This, btw, would be a lot funner with a ```\nSet::of```\n from ```\njava-9```\n that adds an internal randomization, so theoretically for the same exact input, you would get different result on the same VM from run to run. But to keep it simple:\n\n```\nList.of(\"a\", \"bb\", \"ccc\", \"dddd\");\nList.of(\"dddd\", \"a\", \"bb\", \"ccc\");\n```\n\n\nAnd we want to perform:\n\n```\n....stream()\n   .parallel()\n   .reduce(0,\n          (x, y) -> x + y.length(),\n          (x, y) -> x * y);\n```\n\n\nUnder the current implementation, this will yield the same result for both lists; but that is an implementation artifact.  \n\nThere is nothing stopping an internal implementation in saying: \"I will split the list to the smallest chunk possible, but not smaller than two elements in each of them\". In such a case, this could have been translated to these splits:\n\n```\n[\"a\",    \"bb\"]     [\"ccc\", \"dddd\"]\n[\"dddd\", \"a\" ]     [\"bb\" , \"ccc\" ]   \n```\n\n\nNow, \"accumulate\" those splits:\n\n```\n0 + \"a\".length   = 1 ; 1 + \"bb\".length   = 3 // thus chunk result is 3\n0 + \"ccc\".length = 3 ; 3 + \"dddd\".length = 7 // thus chunk result is 7 \n```\n\n\nNow we \"combine\" these chunks:  ```\n3 * 7 = 21```\n.\n\nI am pretty sure you already see that the second list in such a scenario would result in ```\n25```\n; as such different operations in the accumulator and combiner can result in wrong results. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Hackerrank string reduction\r\n                \r\nI am working on the following problem https://www.hackerrank.com/challenges/reduced-string . \n\nI want to solve the above problem recursively . My code is as follows .\n\n```\nimport java.io.*;\nimport java.util.*;\nimport java.text.*;\nimport java.math.*;\nimport java.util.regex.*;\n\npublic class Solution {\n    public static void main(String[] args) throws Exception {\n        BufferedReader br = new BufferedReader(neInputStreamReader(System.in));\n        String line = br.readLine();\n        System.out.print(reduce(line));  \n    }\n\n    public static String reduce (String str) {\n        if (str.equals(\"\")) return \"Empty String\";\n        if (str.length()<2) return str;\n        if (str.charAt(0) == str.charAt(1)) return reduce(str.substring(2));\n        return str.charAt(0) + reduce(str.substring(1));\n    } \n}\n```\n\n\nThe above code fails for the following test case\nbaab\n\nCould any one point out what is the problem in my code ?\n    ", "Answer": "\r\nYour problem is that:\n\n```\nreduce(\"baab\") = 'b' + reduce(\"aab\") = 'b' + reduce(\"b\") = 'b' + 'b' = \"bb\"\n```\n\n\nYou only look at your first character until you can't immediately remove it anymore. Then you never look at it again, even if at some point afterwards you actually could remove it.\n\nI suppose you could do something like this instead:\n\n```\npublic static String reduce(String str){\n    if(str.equals(\"\"))\n        return \"Empty String\";\n    if(str.length()<2)\n        return str;\n    if(str.charAt(0)==str.charAt(1))\n        return reduce(str.substring(2));\n\n    String rest = str.substring(1);\n    String reducedRest = reduce(rest);\n\n    if (rest.equals(reducedRest)) \n        return str;\n    else \n        return reduce(str.charAt(0) + reducedRest);\n} \n```\n\n\nIt's not great but it should show you the general idea. Now you only ignore the first char if you don't change the rest. If you do change it, you need to look at the whole thing again.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "If-statement condtion reduction\r\n                \r\nSuppose it is:\n\n```\n...\nuse Config::Properties::Simple;\n...\nmy $p = Config::Properties::Simple->new( file => $propfile );\n...\n$str = $p->getProperty('prop');\n...\n```\n\n\n.\n\nDoes\n\n```\n ...\nif ( defined $str and $str ne \"\" ) { #1\n ...\n```\n\n\nequal to\n\n```\n...\nif ($str) { #2\n...\n```\n\n\n?\n\nIf not, is there a way to simplify the ```\n#1```\n marked statement?\n    ", "Answer": "\r\nNo, they're not the same if ```\n$str```\n is ```\n\"0\"```\n.\n\nYou can simplify the statement by just checking the length:\n\n```\nif (length $str) { ...\n```\n\n\nIn recent versions of Perl, ```\nlength(undef)```\n is ```\nundef```\n without any warning generated.  And using ```\nundef```\n as a boolean doesn't generate a warning either.\n\n(By \"recent\" I mean 5.12 and up.  Previously, ```\nlength(undef)```\n would produce \"Use of uninitialized value in length\" if you have warnings turned on, which you should.)\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to evaluate dimensionality reduction technique?\r\n                \r\nI have a dataset of NxM data in binary form. I apply a variety of dimensionality techniques on it and I plot the first two dimensions. This is how I get an intuition of whether the technique is suitable for my dataset or not. Is there a more appropriate/methodical/heuristic/formal way to test the suitability of the dimensionality reduction techniques I use? \n    ", "Answer": "\r\nThe main purpose of applying dimensionality reduction on data is to capture the original data's distribution as much as possible even after the dimensionality reduction. Therefore, we want to make sure that we capture the variance of the data as much as we can. \n\nLet's say you have a N*N matrix, and we perform SVD (Singular Value Decomposition) on X. Then, we'll observe the singular values, the diagonal entries in the resulting S matrix. (X = USV)\n\nAnd you want to cut them off at some index K based on the desired percentage variance captured: \n\n∑ i=1 K  sigma(i) / ∑ i=1 N  sigma(i) \n\nIf you select the first K columns of U, then you are reducing your original N-dimension to K-dimension.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction link in category\r\n                \r\nIf i create new category, for example article i have all posts in this category on link:\n\n```\nhttp://www.mypage.com/category/article\n```\n\n\nhow can i make:\n\n```\nhttp://www.mypage.com/article\n```\n\n\nwithout \"category\"?\n    ", "Answer": "\r\nIt's been awhile since I have used Wordpress, but if I'm not mistaken, you should be able to edit your URL in wp-admin. You can use this page to help you locate the section in the admin.\n\nYou would use a URL structure as such...\n\n```\n%postname%\n```\n\n\nThat will remove the category from the URL. Just keep in mind that changing this could mess up the permalinks for any previous posts. You can either modify the permalinks in wp-admin or through the database using a tool such as phpMyAdmin. Alternatively, I believe there are plugins that will redirect old permalinks, but like I said, it's a been a while since the last time I used Wordpress.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OMP parallel for reduction\r\n                \r\nI'm trying to write a k-means clustering class. I want to make my function parallel.\n\n```\nvoid kMeans::findNearestCluster()\n{   \n    short closest;\n    int moves = 0;\n    #pragma omp parallel for reduction(+:moves)\n    for(int i = 0; i < n; i++)\n    {      \n        float min_dist=FLT_MAX;\n        for(int k=0; k < clusters; k++)\n        {\n            float dist_sum = 0.0, dist2 = 0.0;\n            for(int j = 0; j < dim; j++)\n            {\n                dist_sum = centroids[k*dim+j] - data[i*dim+j];\n                dist2 +=  dist_sum * dist_sum;\n\n            }\n            if (dist2 < min_dist)\n            {\n                min_dist = dist2;\n                closest = k;\n\n            }\n\n        }\n\n        if (assignment[i] != closest)\n        {\n            assignment[i] = closest;\n            moves++;\n        }        \n\n    }\n\n    this->moves = moves;\n\n}\n```\n\n\nThis is how it should work:\n\n\nStep 1: Find nearest cluster\n\n\nLoop through all the datapoints, and compare the distances between all centroids. \nWhen the closest centroid is found, it is stored in the variable called ```\nclosest```\n.\nCheck if this point is assigned to the newly found closest cluster. If not, move it the the new one. (increment moves)\n\nStep 2: Recalculate the centroids, based on the new assignments. (function is not shown)\nRepeat Step 1 and Step 2 until no more movement happens.\n\n\nWithout ```\n#parallel```\n ```\nmoves```\n converges to zero. If I have ```\n#parallel```\n moves have random values. I think because the parallel loops have conflict to update ```\nmove```\n. \n\nMaybe a better strategy would be to have each thread its own move variable, and at the end they would some up.\n    ", "Answer": "\r\nYou use the ```\nclosest```\n variable inside parallel loop, both writing to it and using it as a check before increasing your ```\nmoves```\n variable.  But it is declared outside the loop, so all iterations use the same variable!  As all iterations are executed (theoretically) in parallel, you cannot expect that any iteration sees what any other iteration wrote to ```\nclosest```\n in branch condition ```\nif (assignment[i] != closest)```\n.  This variable becomes randomly updated by racing parallel threads.  Therefore your ```\nmoves```\n evaluates to junk value.\n\nMoving a declaration of ```\nclosest```\n inside the outer loop or declaring it as ```\nprivate(closest)```\n in OpenMP pragma may solve your problem.\n\nBy the way,  ```\nclosest```\n may be uninitialized, and should be better of the same type as ```\nk```\n, i.e. ```\nint```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Functional Dependency Reduction\r\n                \r\nI'm supposed to find the minimal cover for this set of FD's. Please let me know if my answer is correct.\n\n\nXZ->Z \nXZ->Y\nXZ->B\nYA->C\nYA->G\nC->W\nB->G\nXZ->G\n\n\nMy answer:\n\n\nX->Z (deleted Z attribute, trivial FD)\nZ->Y (deleted X, since it entails X->Z->Y from 1.)\nZ->B (same here)\nYA->C\nYA->G\nC->W\nB->G\n(Deleted, since X->Z->B->G)\n\n    ", "Answer": "\r\nIt seem to me that the first one is not correct:\n\n\n```\nXZ->Z```\n does not imply anything, since XZ contains Z, thus it is trivial.\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "SQL code reduction\r\n                \r\nThe below SQL code is in the where clause. Is there any other way that we can write to cut short this.The below code is repeating many times. I wanted to cut short this code\n\n```\n((Business_Unit = 'A'\nand ((\nconvert(money, case when substring(ltrim(rtrim(premium_change)),len(ltrim(rtrim(premium_change))),1 ) = '-'\nthen substring(ltrim(rtrim(premium_change)),1,len(ltrim(rtrim(premium_change))) -1) else\nsubstring(ltrim(rtrim(premium_change)),1,len(ltrim(rtrim(premium_change)))) end)\n) >= 100))\nOR\n\n\n(Business_Unit = 'B'\nAND ((\nconvert(money, case when substring(ltrim(rtrim(premium_change)),len(ltrim(rtrim(premium_change))),1 ) = '-'\nthen substring(ltrim(rtrim(premium_change)),1,len(ltrim(rtrim(premium_change))) -1) else\nsubstring(ltrim(rtrim(premium_change)),1,len(ltrim(rtrim(premium_change)))) end)\n) >=100.00)\nAND (text7 != 'N')\nOR \n\nBusiness_Unit = 'B'\nAND ((\nconvert(money, case when substring(ltrim(rtrim(premium_change)),len(ltrim(rtrim(premium_change))),1 ) = '-'\nthen substring(ltrim(rtrim(premium_change)),1,len(ltrim(rtrim(premium_change))) -1) else\nsubstring(ltrim(rtrim(premium_change)),1,len(ltrim(rtrim(premium_change)))) end)\n) >=100.00)\nAND (text7 = 'N')\nand (text4 != '')\nOR Business_Unit = 'B'\nAND ((\nconvert(money, case when substring(ltrim(rtrim(premium_change)),len(ltrim(rtrim(premium_change))),1 ) = '-'\nthen substring(ltrim(rtrim(premium_change)),1,len(ltrim(rtrim(premium_change))) -1) else\nsubstring(ltrim(rtrim(premium_change)),1,len(ltrim(rtrim(premium_change)))) end)\n) > 300.00)\nAND (text7 = 'N')\nand (text4 = ''))\nOR\n(Business_Unit = 'C'\nand ((\nconvert(money, case when substring(ltrim(rtrim(premium_change)),len(ltrim(rtrim(premium_change))),1 ) = '-'\nthen substring(ltrim(rtrim(premium_change)),1,len(ltrim(rtrim(premium_change))) -1) else\nsubstring(ltrim(rtrim(premium_change)),1,len(ltrim(rtrim(premium_change)))) end)\n) >= 150)`enter code here`\nand text1 != 'N')\n```\n\n    ", "Answer": "\r\nI would use a ```\nCTE```\n here. The idea is to prepare the data for your where clause:\n\n```\n;with cte as \n(--your current query here without where clasuse\n --just add the following to the select list \n convert(money, case when substring(ltrim(rtrim(premium_change)),\n                                   len(ltrim(rtrim(premium_change))),\n                                   1) = '-'\n                    then substring(ltrim(rtrim(premium_change)), 1,\n                                   len(ltrim(rtrim(premium_change)))\n                                   - 1)\n                    else substring(ltrim(rtrim(premium_change)), 1,\n                                   len(ltrim(rtrim(premium_change))))\n                end) as pc\n)\nselect * from cte\nwhere \n    (Business_Unit = 'A' and pc >= 100) or\n    (Business_Unit = 'B' and pc >= 100 and text7 != 'N') or\n    ...\n```\n\n\nI am failing to parse your weird ```\nAND OR```\n combinations(that seems incorrect anyway).\n\nIf using a ```\nCTE```\n is not possible you can shorten the following:\n\n```\nsubstring(ltrim(rtrim(premium_change)),len(ltrim(rtrim(premium_change))),1 ) \n```\n\n\nbecomes:\n\n```\nright(ltrim(rtrim(premium_change)), 1 )\n```\n\n\nand:\n\n```\nsubstring(ltrim(rtrim(premium_change)),1,len(ltrim(rtrim(premium_change))) -1)\n```\n\n\nbecomes:\n\n```\nleft(ltrim(rtrim(premium_change)), len(ltrim(rtrim(premium_change))) -1)\n```\n\n\nand:\n\n```\nsubstring(ltrim(rtrim(premium_change)),1,len(ltrim(rtrim(premium_change))))\n```\n\n\nbecomes:\n\n```\nltrim(rtrim(premium_change))\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA reduction, approach for big arrays\r\n                \r\nI have the following \"Frankenstein\" sum reduction code, taken partly from the common CUDA reduction slices, partly from the CUDA samples.\n\n```\n    __global__ void  reduce6(float *g_idata, float *g_odata, unsigned int n)\n{\n    extern __shared__ float sdata[];\n\n    // perform first level of reduction,\n    // reading from global memory, writing to shared memory\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x*blockSize*2 + threadIdx.x;\n    unsigned int gridSize = blockSize*2*gridDim.x;\n    sdata[tid] = 0;\n    float mySum = 0;   \n\n    while (i < n) { \n        sdata[tid] += g_idata[i] + g_idata[i+MAXTREADS]; \n        i += gridSize; \n    }\n   __syncthreads();\n\n\n    // do reduction in shared mem\n    if (tid < 256)\n        sdata[tid] += sdata[tid + 256];\n    __syncthreads();\n\n    if (tid < 128)\n        sdata[tid] +=  sdata[tid + 128];\n     __syncthreads();\n\n    if (tid <  64)\n       sdata[tid] += sdata[tid +  64];\n    __syncthreads();\n\n\n#if (__CUDA_ARCH__ >= 300 )\n    if ( tid < 32 )\n    {\n        // Fetch final intermediate sum from 2nd warp\n        mySum = sdata[tid]+ sdata[tid + 32];\n        // Reduce final warp using shuffle\n        for (int offset = warpSize/2; offset > 0; offset /= 2) \n            mySum += __shfl_down(mySum, offset);\n    }\n    sdata[0]=mySum;\n#else\n\n    // fully unroll reduction within a single warp\n    if (tid < 32) {\n       sdata[tid] += sdata[tid + 32];\n       sdata[tid] += sdata[tid + 16];\n       sdata[tid] += sdata[tid + 8];\n       sdata[tid] += sdata[tid + 4];\n       sdata[tid] += sdata[tid + 2];\n       sdata[tid] += sdata[tid + 1];\n    }\n#endif\n    // write result for this block to global mem\n    if (tid == 0) g_odata[blockIdx.x] = sdata[0];\n  }\n```\n\n\nI will be using this to reduce an unrolled array of big size (e.g. ```\n512^3 = 134217728 = n```\n) on a Tesla k40 GPU.\n\nI have some questions regarding the ```\nblockSize```\n variable, and its value. \n\nFrom here on, I will try to explain my understanding (either right or wrong) on how it works:\n\nThe bigger I choose ```\nblockSize```\n, the faster this code will execute, as it will spend less time in the whole loop, but it will not finish reducing the whole array, but it will return a smaller array of size ```\ndimBlock.x```\n, right? If I use ```\nblockSize=1```\n this code would return in 1 call the reduction value, but it will be really slow because its not exploiting the power of CUDA almost anything. Therefore I need to call the reduction kernel several times, each of the time with a smaller ```\nblokSize```\n, and reducing the result of the previous call to reduce, until I get to the smallest point.\n\nsomething like (pesudocode)\n\n```\nblocks=number; //where do we start? why?\nwhile(not the min){\n\n    dim3 dimBlock( blocks );\n    dim3 dimGrid(n/dimBlock.x);\n    int smemSize = dimBlock.x * sizeof(float);\n    reduce6<<<dimGrid, dimBlock, smemSize>>>(in, out, n);\n\n    in=out;\n\n    n=dimGrid.x; \n    dimGrid.x=n/dimBlock.x; // is this right? Should I also change dimBlock?\n}\n```\n\n\nIn which value should I start? I guess this is GPU dependent. Which values shoudl it be for a Tesla k40  (just for me to understand how this values are chosen)?\n\nIs my logic somehow flawed? how?\n    ", "Answer": "\r\nThere is a CUDA tool to get good grid and block sizes for you : Cuda Occupancy API.\n\nIn response to \"The bigger I choose blockSize, the faster this code will execute\" -- Not necessarily, as you want the sizes which give max occupancy (the ratio of active warps to the total number of possible active warps). \n\nSee this answer for additional information How do I choose grid and block dimensions for CUDA kernels?.\n\nLastly, for Nvidia GPUs supporting Kelper or later, there are shuffle intrinsics to make reductions easier and faster. Here is an article on how to use the shuffle intrinsics : Faster Parallel Reductions on Kepler.\n\nUpdate for choosing number of threads:\n\nYou might not want to use the maximum number of threads if it results in a less efficient use of the registers. From the link on occupancy :\n\nFor purposes of calculating occupancy, the number of registers used by each thread is one of the key factors. For example, devices with compute capability 1.1 have 8,192 32-bit registers per multiprocessor and can have a maximum of 768 simultaneous threads resident (24 warps x 32 threads per warp). This means that in one of these devices, for a multiprocessor to have 100% occupancy, each thread can use at most 10 registers. However, this approach of determining how register count affects occupancy does not take into account the register allocation granularity. For example, on a device of compute capability 1.1, a kernel with 128-thread blocks using 12 registers per thread results in an occupancy of 83% with 5 active 128-thread blocks per multi-processor, whereas a kernel with 256-thread blocks using the same 12 registers per thread results in an occupancy of 66% because only two 256-thread blocks can reside on a multiprocessor.\n\nSo the way I understand it is that an increased number of threads has the potential to limit performance because of the way the registers can be allocated. However, this is not always the case, and you need to do the calculation (as in the above statement) yourself to determine the optimal number of threads per block.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Dimension Reduction in CLDNN (tensorflow)\r\n                \r\nI'm trying to write an implementation of CLDNN with tensorflow, like the one in this scheme. I am having a problem with the dimension reduction layer.\n\nAs far as I understand it, it is made with several stacked Restricted Boltzmann Machines (RBMs) and works like an autoencoder. The decoder part of the layer is only here to train the encoder to reduce well dimensions. Meaning that you want to \"plug\" the encoder's output into the next layer's input.\n\nI can define a loss function that will train the autoencoder (by comparing input from decoded output), and an other loss function that will train the whole graph. I there a way to train these two loss functions ?\nOr maybe I am misunderstanding the problem here, but its feels to me that the decoder part of the autoencoder is kinda left \"outside the loop\" and won't be trained.\n\nI have found implementation of such autoencoders, and convolutionnal layers, etc... but I don't really understand how to \"insert\" the autoencoder inside the network (like in the scheme)\n    ", "Answer": "\r\nPaper says\n\n\n  The Computational Network Toolkit (CNTK) [24] is used for neural network training. As [14] suggests, we apply uniform random weight initialization for all layers without either generative or discriminative pretraining [1].\n\n\nDimension reduction in diagram is simply a dense projection layer. So they do not train any autoencoders, they just configure the network architecture and train the network from the random initial state.\n\nAutoencoders were used before for subnetwork initialization, but they are not very popular now.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Time complexity of Parallel Reduction Algorithm\r\n                \r\nCurrently I am studying GPU architecture and its concepts. In parallel Reduction technique, how is the time complexity shown on 29th slide in following NVIDIA guide come O(N/P + log N)? I know that for N threads, it will be O(log N). If we have P threads parallel available then time complexity should be O((N/P)*log P). Right? Where am I wrong here? \n\nParallel Reduction Techniques \n    ", "Answer": "\r\nI would like to explain this with an example, consider this array with N=8 elements\n\n```\n1  2  3  4  5  6  7  8\n```\n\n\nThe parallel reduction will occur in following steps\n\n```\n1  2  3  4  5  6  7  8\n  3    7      11   15\n    10          26\n          36\n```\n\n\nIf you count the number of reduction operations, we have 4,2 and 1 on first, second and third step respectively. So total number of operations we have is 4+2+1=7=N-1 and we do all the reductions in O(N) and we also have log(8)=3 (this is log to base 2) steps so we pay a cost to do these steps which is O(logN). Hence if we used a single thread to reduce in this way we add the two costs as they occur separately of each other and we have O(N+logN). Where O(N) is cost for doing all operations and O(logN) is cost for doing all steps. Now there is no way to parallelize the cost for steps since they have to happen sequentially. However we can use multiple threads to do the operations and divide the O(N) cost to O(N/P). Therefore we have\n\n```\nTotal cost = O(N/P + logN)\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Matrix: Gauss reduction\r\n                \r\nI'm trying to make a MATLAB function work that takes as input a matrix and returns the triangulated matrix. That's the function:\n\n```\nfunction T = gauss_method(A)\n\n    [row, col] = size(A);\n\n    T = zeros(size(A));\n\n    for j = 1:col-1\n        perno = A(j,j);\n\n        for i = j+1:row\n            mult = A(i,j) / perno;\n            for x=i:col\n                T(i,:) = A(i,:) - mult * A(j,:);\n            end\n        end\n    end\n```\n\n\nInput is:\n\n```\nA = [3  1 -1   0;\n     0  7 -3   0;\n     0 -3  9  -2;\n     0  0  4 -10];\n```\n\n\nOutput is:\n\n```\n0         0         0         0\n0    7.0000   -3.0000         0\n0         0    7.7143   -2.0000\n0    1.3333         0   -9.1111\n```\n\n\nWhy doesn't it work? Where are the mistakes?\n    ", "Answer": "\r\nYour implementation is close to Gauss. However, there are some parts broken. Let's try to name them:\n\n\nGauss is a recursive method in a sense that each step depends on the preceding ones. Therefore, assembling ```\nT```\n from ```\nA```\n without updating ```\nA```\n cannot work. Solution: I suggest to work inplace, but you may also refer to ```\nT```\n on the right side of your assembling-equation.\nYou never touch the first line of ```\nT```\n as you start with ```\nj = 1```\n and ```\ni = j + 1```\n. Solution:  One solution could be to copy the first line of ```\nA```\n to ```\nT```\n initially, or work inplace instead.\nYou take the pivot element (```\nperno```\n) from the diagonal element. However, it is not said that all ```\nA(i,i) ~= 0```\n. When dividing by ```\npernot```\n, you introduce a potential division by ```\n0```\n. Solution:  You can avoid division at all, when implementing Gauss. Just scale the other line. In the end it is all about bringing ```\nA```\n to triangle form by any form of linear combination.\nTalking about Gauss, I personally have the application of solving a linear equation system ```\nA * x = b```\n in mind. Suggestion: Maybe you want to enable this by providing your function an optional argument ```\nb```\n and transforming it along with ```\nA```\n.\nThe spacing is pretty broken, but this will hopefully be fixed with the outstanding edit of your question. Suggestion: Never use tabs for indents.\n\n\nOne proposal how to fix your implementation staying close to your current version:\n\n```\nfunction A = gauss_method(A)\n    [row, col] = size(A);\n    for j = 1:col-1\n        perno = A(j,j);\n        for i = j+1:row\n            % alternative w/o division:\n            %  A(i,:) = perno * A(i,:) - A(i,j) * A(j,:);\n\n            % alternative w/ division:\n            if A(i,j) ~= 0\n                mult = perno / A(i,j);\n                A(i,:) = mult * A(i,:) - A(j,:);\n            end\n        end\n    end\nend\n```\n\n\nOutput:\n\n```\n>> gauss_method(A)\n\nans =\n\n    3.0000    1.0000   -1.0000         0\n         0    7.0000   -3.0000         0\n         0         0  -18.0000    4.6667\n         0         0         0   40.3333\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction clause\r\n                \r\nI have a loop which I am trying to parallelize with OpenMP. The construct I have in code is this,\n\n```\nfor(int x = 0; x < <condition>; x++) \n{\n    aggregate += <return value from very costly function(param1)>;\n}\n```\n\n\nI am trying to parallelize it as follows,\n\n```\n#pragma omp parallel for\nfor(int x = 0; x < <condition>; x++) \n{\n    #pragma omp parallel private(param1)\n    aggregate += <return value from very costly function(param1)>;\n}\n```\n\n\nIt crashes if I dont make param1 private since all threads use the same memory. But also I always get aggregate's value as 0. I understand that this is because I have not reduced it to sum each thread's local calculated value of aggregate. I am confused at this, how do I do that to sum all the values from the threads?\n\nThis is in Visual Studio 13 C++.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "KML size reduction\r\n                \r\nenter image description here\nI need help to reduce the KML file size, when i created the KML from xl macro, it is generating large KML file, is there any way to reduce the kml size, please....,\n\nThe KML code is....\nIt is creating the 350+ coordinates for each sector, may be this is causing higher file size.\n\n```\n { << ? xml version = \"1.0\"\n    encoding = \"Windows-936\" ? > < kml >\n    xmlns = \"http://earth.google.com/kml/2.0\" > < Document > < description > < ![CDATA[Total >\n        339 sectors 3 / 22 / 2018]] > < /description><Style >\n    id = \"my_style1\" > < LineStyle >\n    >\n    < color > ff00ff00 < /color><width>2</width > < /LineStyle><PolyStyle><fill>0</fill > < /PolyStyle></Style > < Style >\n    id = \"my_style2\" > < LineStyle > < color > ff0000ff < /color><width>2</width > < /LineStyle><PolyStyle><fill>0</fill > < /PolyStyle></Style > < Style >\n    id = \"my_style3\" > < LineStyle > < color > ff00ffff < /color><width>2</width > < /LineStyle><PolyStyle><fill>0</fill > < /PolyStyle></Style > < Style >\n    id = \"my_style4\" > < LineStyle > < color > ff00aa00 < /color><width>2</width > < /LineStyle><PolyStyle><fill>0</fill > < /PolyStyle></Style > < Style >\n    id = \"my_style5\" >\n    >\n    < LineStyle > < color > ffff0000 < /color><width>2</width > < /LineStyle><PolyStyle><fill>0</fill > < /PolyStyle></Style > < Style >\n    id = \"my_style5\" > < LineStyle > < color > ffff0000 < /color><width>2</width > < /LineStyle><PolyStyle><fill>0</fill > < /PolyStyle></Style > < Style >\n    id = \"my_style6\" > < LineStyle > < color > ffff00aa < /color><color>ffff00aa</color > < width > 2 < /width></LineStyle > < PolyStyle > < fill > 0 < /fill></PolyStyle > < /Style><Style >\n    id = \"my_style7\" > < LineStyle > < color > 501478 FF < /color><width>2</width > < /LineStyle><PolyStyle><fill>0</fill > < /PolyStyle></Style > < Style >\n    id = \"my_style8\" > < LineStyle > < color > ffff0ff0 < /color><width>2</width > < /LineStyle><PolyStyle><fill>0</fill > < /PolyStyle></Style > < Style >\n    id = \"my_style9\" > < LineStyle > < color > ffaa00aa < /color><width>2</width > < /LineStyle><PolyStyle><fill>0</fill > < /PolyStyle></Style > < Style >\n    id = \"my_style10\" > < LineStyle > < color > ffffff00 < /color><width>2</width > < /LineStyle><PolyStyle><fill>0</fill > < /PolyStyle></Style > < Placemark > < name > < ![CDATA[Palu_Infill_1_31]] > < /name><description><![CDATA[<br><br><br><table >\n    border = \"1\"\n    padding = \"0\" > < tr > < td > Sector >\n    Name < /td><td>Palu_Infill_1_31</td > < /tr><tr><td>Longitude</td > < td > 119.877 < /td></tr > < tr > < td > Latitude < /td><td>-0.886656</td > < /tr><tr><td>Bearing</td > < td > 0 < /td></tr > < tr > < td > Cell >\n    ID < /td><td>1</td > < /tr><tr><td>Beamwidth</td > < td > 60 < /td></tr > < tr > < td > LAC < /td><td>7777</td > < /tr><tr><td>CI</td > < td > 7777 < /td></tr > < tr > < td > DL Frequency < /td><td>7777</td > < /tr><tr><td>MAXTXPOWER</td > < td > 7777 < /td></tr > < tr > < td > PCPICHPOWER < /td><td>7777</td > < /tr><tr><td>RNC >\n    Name < /td><td>7777</td > < /tr><tr><td>Frequency_Carrier</td > < td > F1 < /td></tr > < tr > < td > U2000 < /td><td>7777</td > < /tr><tr><td>ACTSTATUS</td > < td > 7777 < /td></tr > < tr > < td > Antenna Height < /td><td>7777</td > < /tr><tr><td>Antenna >\n    Downtilt < /td><td>7777</td > < /tr><tr><td>Equipment</td > < td > 7777 < /td></tr > < /table>]]></description > < styleUrl > #my_style1 < /styleUrl><Polygon><outerBoundaryIs><LinearRing><coordinates>119.877,-0.886656,0119.87675,-0.886222987298108,0119.876751260639,-0.8862222619106,0119.876752523382,-0.886221540193205,0119.87675378822,-0.88622082215203,0119.876755055141,-0.886220107793152,0119.876756324134,-0.886219397122614,0119.87675759519,-0.88621869014643,0119.876758868296,-0.886217986870583,0119.876760143443,-0.886217287301023,0119.87676142062,-0.886216591443669,0119.876762699815,-0.88621589930441,0119.876763981018,-0.886215210889102,0119.876765264219,-0.886214526203571,0119.876766549405,-0.886213845253609,0119.876767836567,-0.886213168044979,0119.876769125693,-0.886212494583411,0119.876770416773,-0.886211824874603,0119.876771709796,-0.886211158924223,0119.87677300475,-0.886210496737906,0119.876774301625,-0.886209838321254,0119.87677560041,-0.886209183679838,0119.876776901093,-0.886208532819199,0119.876778203665,-0.886207885744843,0119.876779508113,-0.886207242462246,0119.876780814427,-0.88620660297685,0119.876782122595,-0.886205967294068,0119.876783432607,-0.886205335419277,0119.876784744452,-0.886204707357825,0119.876786058118,-0.886204083115026,0119.876787373594,-0.886203462696161,0119.876788690869,-0.886202846106482,0119.876790009933,-0.886202233351204,0119.876791330773,-0.886201624435513,0119.876792653379,-0.886201019364562,0119.876793977739,-0.886200418143469,0119.876795303843,-0.886199820777324,0119.876796631678,-0.886199227271179,0119.876797961235,-0.886198637630057,0119.876799292501,-0.886198051858948,0119.876800625466,-0.886197469962807,0119.876801960117,-0.88619689194656,0119.876803296444,-0.886196317815096,0119.876804634436,-0.886195747573274,0119.87680597408,-0.886195181225918,0119.876807315367,-0.886194618777822,0119.876808658284,-0.886194060233744,0119.87681000282,-0.886193505598411,0119.876811348963,-0.886192954876515,0119.876812696703,-0.886192408072717,0119.876814046028,-0.886191865191643,0119.876815396926,-0.886191326237887,0119.876816749387,-0.886190791216009,0119.876818103398,-0.886190260130537,0119.876819458948,-0.886189732985964,0119.876820816025,-0.886189209786751,0119.876822174619,-0.886188690537326,0119.876823534718,-0.88618817524208,0119.876824896309,-0.886187663905376,0119.876826259383,-0.886187156531539,0119.876827623926,-0.886186653124863,0119.876828989928,-0.886186153689607,0119.876830357377,-0.886185658229998,0119.876831726262,-0.886185166750227,0119.87683309657,-0.886184679254454,0119.876834468291,-0.886184195746803,0119.876835841412,-0.886183716231366,0119.876837215923,-0.8861832407122,0119.876838591811,-0.886182769193329,0119.876839969064,-0.886182301678743,0119.876841347672,-0.886181838172397,0119.876842727622,-0.886181378678213,0119.876844108903,-0.886180923200081,0119.876845491503,-0.886180471741852,0119.87684687541,-0.886180024307349,0119.876848260613,-0.886179580900356,0119.8768496471,-0.886179141524626,0119.876851034859,-0.886178706183876,0119.876852423879,-0.88617827488179,0119.876853814148,-0.886177847622018,0119.876855205653,-0.886177424408175,0119.876856598384,-0.886177005243842,0119.876857992328,-0.886176590132566,0119.876859387473,-0.886176179077859,0119.876860783809,-0.886175772083199,0119.876862181322,-0.886175369152031,0119.876863580002,-0.886174970287763,0119.876864979836,-0.886174575493771,0119.876866380812,-0.886174184773396,0119.876867782919,-0.886173798129943,0119.876869186145,-0.886173415566683,0119.876870590477,-0.886173037086855,0119.876871995905,-0.886172662693661,0119.876873402416,-0.886172292390269,0119.876874809998,-0.886171926179811,0119.876876218639,-0.886171564065387,0119.876877628328,-0.886171206050061,0119.876879039052,-0.886170852136862,0119.8768804508,-0.886170502328785,0119.876881863559,-0.88617015662879,0119.876883277318,-0.886169815039801,0119.876884692065,-0.88616947756471,0119.876886107787,-0.886169144206372,0119.876887524473,-0.886168814967607,0119.87688894211,-0.886168489851202,0119.876890360688,-0.886168168859908,0119.876891780193,-0.88616785199644,0119.876893200614,-0.88616753926348,0119.876894621938,-0.886167230663674,0119.876896044155,-0.886166926199633,0119.87689746725,-0.886166625873934,0119.876898891214,-0.886166329689117,0119.876900316033,-0.88616603764769,0119.876901741695,-0.886165749752122,0119.876903168189,-0.886165466004851,0119.876904595502,-0.886165186408276,0119.876906023623,-0.886164910964765,0119.876907452538,-0.886164639676647,0119.876908882237,-0.886164372546218,0119.876910312707,-0.886164109575739,0119.876911743936,-0.886163850767434,0119.876913175911,-0.886163596123494,0119.876914608621,-0.886163345646073,0119.876916042054,-0.886163099337291,0119.876917476197,-0.886162857199231,0119.876918911038,-0.886162619233944,0119.876920346566,-0.886162385443441,0119.876921782767,-0.886162155829702,0119.876923219631,-0.88616193039467,0119.876924657144,-0.886161709140252,0119.876926095294,-0.886161492068319,0119.87692753407,-0.88616127918071,0119.876928973459,-0.886161070479224,0119.87693041345,-0.886160865965629,0119.876931854028,-0.886160665641655,0119.876933295184,-0.886160469508996,0119.876934736904,-0.886160277569313,0119.876936179176,-0.886160089824229,0119.876937621988,-0.886159906275334,0119.876939065328,-0.886159726924179,0119.876940509184,-0.886159551772284,0119.876941953543,-0.886159380821129,0119.876943398393,-0.886159214072162,0119.876944843722,-0.886159051526793,0119.876946289518,-0.886158893186398,0119.876947735768,-0.886158739052316,0119.876949182461,-0.886158589125852,0119.876950629583,-0.886158443408275,0119.876952077124,-0.886158301900816,0119.87695352507,-0.886158164604675,0119.876954973409,-0.886158031521012,0119.876956422129,-0.886157902650954,0119.876957871217,-0.886157777995591,0119.876959320663,-0.886157657555978,0119.876960770452,-0.886157541333133,0119.876962220574,-0.886157429328042,0119.876963671015,-0.88615732154165,0119.876965121763,-0.88615721797487,0119.876966572807,-0.886157118628579,0119.876968024133,-0.886157023503617,0119.87696947573,-0.886156932600789,0119.876970927586,-0.886156845920864,0119.876972379687,-0.886156763464576,0119.876973832022,-0.886156685232623,0119.876975284578,-0.886156611225665,0119.876976737344,-0.886156541444331,0119.876978190306,-0.886156475889209,0119.876979643453,-0.886156414560855,0119.876981096772,-0.886156357459788,0119.876982550252,-0.88615630458649,0119.876984003878,-0.88615625594141,0119.876985457641,-0.886156211524959,0119.876986911526,-0.886156171337512,0119.876988365522,-0.88615613537941,0119.876989819616,-0.886156103650957,0119.876991273797,-0.886156076152422,0119.876992728051,-0.886156052884036,0119.876994182367,-0.886156033845998,0119.876995636732,-0.886156019038468,0119.876997091134,-0.886156008461571,0119.876998545561,-0.886156002115397,0119.877,-0.886156,0119.877001454439,-0.886156002115397,0119.877002908866,-0.886156008461571,0119.877004363268,-0.886156019038468,0119.877005817633,-0.886156033845998,0119.877007271949,-0.886156052884036,0119.877008726203,-0.886156076152422,0119.877010180384,-0.886156103650957,0119.877011634478,-0.88615613537941,0119.877013088474,-0.886156171337512,0119.877014542359,-0.886156211524959,0119.877015996122,-0.88615625594141,0119.877017449748,-0.88615630458649,0119.877018903228,-0.886156357459788,0119.877020356547,-0.886156414560855,0119.877021809694,-0.886156475889209,0119.877023262656,-0.886156541444331,0119.877024715422,-0.886156611225665,0119.877026167978,-0.886156685232623,0119.877027620313,-0.886156763464576,0119.877029072414,-0.886156845920864,0119.87703052427,-0.886156932600789,0119.877031975867,-0.886157023503617,0119.877033427193,-0.886157118628579,0119.877034878237,-0.88615721797487,0119.877036328985,-0.88615732154165,0119.877037779426,-0.886157429328042,0119.877039229548,-0.886157541333133,0119.877040679337,-0.886157657555978,0119.877042128783,-0.886157777995591,0119.877043577871,-0.886157902650954,0119.877045026591,-0.886158031521012,0119.87704647493,-0.886158164604675,0119.877047922876,-0.886158301900816,0119.877049370417,-0.886158443408275,0119.877050817539,-0.886158589125852,0119.877052264232,-0.886158739052316,0119.877053710482,-0.886158893186398,0119.877055156278,-0.886159051526793,0119.877056601607,-0.886159214072162,0119.877058046457,-0.886159380821129,0119.877059490816,-0.886159551772284,0119.877060934672,-0.886159726924179,0119.877062378012,-0.886159906275334,0119.877063820824,-0.886160089824229,0119.877065263096,-0.886160277569313,0119.877066704816,-0.886160469508996,0119.877068145972,-0.886160665641655,0119.87706958655,-0.886160865965629,0119.877071026541,-0.886161070479224,0119.87707246593,-0.88616127918071,0119.877073904706,-0.886161492068319,0119.877075342856,-0.886161709140252,0119.877076780369,-0.88616193039467,0119.877078217233,-0.886162155829702,0119.877079653434,-0.886162385443441,0119.877081088962,-0.886162619233944,0119.877082523803,-0.886162857199231,0119.877083957946,-0.886163099337291,0119.877085391379,-0.886163345646073,0119.877086824089,-0.886163596123494,0119.877088256064,-0.886163850767434,0119.877089687293,-0.886164109575739,0119.877091117763,-0.886164372546218,0119.877092547462,-0.886164639676647,0119.877093976377,-0.886164910964765,0119.877095404498,-0.886165186408276,0119.877096831811,-0.886165466004851,0119.877098258305,-0.886165749752122,0119.877099683967,-0.88616603764769,0119.877101108786,-0.886166329689117,0119.87710253275,-0.886166625873934,0119.877103955845,-0.886166926199633,0119.877105378062,-0.886167230663674,0119.877106799386,-0.88616753926348,0119.877108219807,-0.88616785199644,0119.877109639312,-0.886168168859908,0119.877111057889,-0.886168489851202,0119.877112475527,-0.886168814967607,0119.877113892213,-0.886169144206372,0119.877115307935,-0.88616947756471,0119.877116722682,-0.886169815039801,0119.877118136441,-0.88617015662879,0119.8771195492,-0.886170502328785,0119.877120960948,-0.886170852136862,0119.877122371672,-0.886171206050061,0119.877123781361,-0.886171564065387,0119.877125190002,-0.886171926179811,0119.877126597584,-0.886172292390269,0119.877128004095,-0.886172662693661,0119.877129409523,-0.886173037086855,0119.877130813855,-0.886173415566683,0119.877132217081,-0.886173798129943,0119.877133619188,-0.886174184773396,0119.877135020164,-0.886174575493771,0119.877136419998,-0.886174970287763,0119.877137818678,-0.886175369152031,0119.877139216191,-0.886175772083199,0119.877140612527,-0.886176179077859,0119.877142007672,-0.886176590132566,0119.877143401616,-0.886177005243842,0119.877144794347,-0.886177424408175,0119.877146185852,-0.886177847622018,0119.877147576121,-0.88617827488179,0119.877148965141,-0.886178706183876,0119.8771503529,-0.886179141524626,0119.877151739387,-0.886179580900356,0119.87715312459,-0.886180024307349,0119.877154508497,-0.886180471741852,0119.877155891097,-0.886180923200081,0119.877157272378,-0.886181378678213,0119.877158652328,-0.886181838172397,0119.877160030936,-0.886182301678743,0119.877161408189,-0.886182769193329,0119.877162784077,-0.8861832407122,0119.877164158588,-0.886183716231366,0119.877165531709,-0.886184195746803,0119.87716690343,-0.886184679254454,0119.877168273738,-0.886185166750227,0119.877169642623,-0.886185658229998,0119.877171010072,-0.886186153689607,0119.877172376074,-0.886186653124863,0119.877173740617,-0.886187156531539,0119.877175103691,-0.886187663905376,0119.877176465282,-0.88618817524208,0119.877177825381,-0.886188690537326,0119.877179183975,-0.886189209786751,0119.877180541052,-0.886189732985964,0119.877181896602,-0.886190260130537,0119.877183250613,-0.886190791216009,0119.877184603074,-0.886191326237887,0119.877185953972,-0.886191865191643,0119.877187303297,-0.886192408072717,0119.877188651037,-0.886192954876515,0119.87718999718,-0.886193505598411,0119.877191341716,-0.886194060233744,0119.877192684633,-0.886194618777822,0119.87719402592,-0.886195181225918,0119.877195365564,-0.886195747573274,0119.877196703556,-0.886196317815096,0119.877198039883,-0.88619689194656,0119.877199374534,-0.886197469962807,0119.877200707499,-0.886198051858948,0119.877202038765,-0.886198637630057,0119.877203368322,-0.886199227271179,0119.877204696157,-0.886199820777324,0119.877206022261,-0.886200418143469,0119.877207346621,-0.886201019364562,0119.877208669227,-0.886201624435513,0119.877209990067,-0.886202233351204,0119.877211309131,-0.886202846106482,0119.877212626406,-0.886203462696161,0119.877213941882,-0.886204083115026,0119.877215255548,-0.886204707357825,0119.877216567393,-0.886205335419277,0119.877217877405,-0.886205967294068,0119.877219185573,-0.88620660297685,0119.877220491887,-0.886207242462246,0119.877221796335,-0.886207885744843,0119.877223098907,-0.886208532819199,0119.87722439959,-0.886209183679838,0119.877225698375,-0.886209838321254,0119.87722699525,-0.886210496737906,0119.877228290204,-0.886211158924223,0119.877229583227,-0.886211824874603,0119.877230874307,-0.886212494583411,0119.877232163433,-0.886213168044979,0119.877233450595,-0.886213845253609,0119.877234735781,-0.886214526203571,0119.877236018982,-0.886215210889102,0119.877237300185,-0.88621589930441,0119.87723857938,-0.886216591443669,0119.877239856557,-0.886217287301023,0119.877241131704,-0.886217986870583,0119.87724240481,-0.88621869014643,0119.877243675866,-0.886219397122614,0119.877244944859,-0.886220107793152,0119.87724621178,-0.88622082215203,0119.877247476618,-0.886221540193205,0119.877248739361,-0.8862222619106,0119.877,-0.886656,0</coordinates > < /LinearRing></outerBoundaryIs > < /Polygon></Placemark > < /Document></kml >\n    >\n}\n```\n\n    ", "Answer": "\r\n\nRemove all the unneeded styles (you are putting 10 styles in the file, but only using one).\nremove the optional altitude from the coordinates [longitude,latitude,[optional altitude]]  \n\n\n\n\n```\n119.87675,-0.886222987298108,0  \n```\n\n\nis the same as:\n\n```\n119.87675,-0.886222987298108  \n```\n\n\n\nonly output 6 digits after the decimal point for the numbers (that will give you millimeter accuracy, should be good enough):\n\n\n\n\n```\n119.87675,-0.886222 \n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Show reduction process step by step\r\n                \r\nI'm doing exercise 1.20 from SICP in Haskell and wondering if there is a way to visualize the reduction steps programmatically? It should look like this: http://community.schemewiki.org/?sicp-ex-1.20.\n\nInsted of ```\n\"b is 6\"```\n I want to see uncomputed (or curried) expression like ```\ngcd' 40 (rem 206 40)```\n.\n\nSample program:\n\n```\nimport Debug.Trace\n\ngcd' a b =\n    if b == 0\n        then a\n        else trace (\"b is \" ++ show b) gcd' b (a `rem` b)\n\nmain = print $ gcd' 206 40\n```\n\n\nOriginal definition of the problem can be found here: http://sarabander.github.io/sicp/html/1_002e2.xhtml#g_t1_002e2_002e5\n    ", "Answer": "\r\nThe right way to do this is to use the ```\nWriter```\n monad.  You can use it to add logging functionality to your code rather easily:\n\n```\nimport Control.Monad.Writer\n\ngcd' :: Int -> Int -> (Int, [String])\ngcd' a b = runWriter $ gcdW a b\n    where\n        gcdW :: Int -> Int -> Writer [String] Int\n        gcdW a 0 = return a\n        gcdW a b = do\n            tell [\"b is \" ++ show b]\n            gcdW b (a `rem` b)\n\nmain :: IO ()\nmain = do\n    let (result, logs) = gcd' 206 40\n    mapM_ putStrLn logs\n    print result\n```\n\n\n\n\nWhat this implementation does is wrap the GCD algorithm in the ```\nWriter```\n monad, which is used for aggregating some kind of result over many computations.  Quite often, it's used for logging messages or outputting intermediate results.  Here what we want is to get the result and a log of each of the intermediate values of ```\nb```\n in a pretty printed form.  Since each of the messages is a ```\nString```\n, we want to build up a list of ```\nString```\ns, which is why it must be wrapped in square brackets in ```\ntell```\n.  The rest of the algorithm is basically identical, simply ```\nreturn a```\n when ```\nb == 0```\n and otherwise perform the recursion in exactly the same way as before.  The ```\ngcd'```\n function just wraps up the ```\nWriter```\n monad so that this is transparent to the user.  Then we can get the result and the log messages and print them out however we want.  The ```\ntrace```\n function is usually reserved for debugging only, especially since it makes use of ```\nunsafePerformIO```\n.  Using the ```\nWriter```\n monad lets you re-use this code without having to worry about it printing out the steps every time.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "CUDA array reduction optimisation\r\n                \r\nI have two arrays ```\nx```\n (of size N ~1-100 millions) and ```\na```\n (greatly smaller Na ~1000-10000) and I want to use ```\nx```\n to define ```\na```\n as\n```\nfor(int j = 0; j < N; j++) {\n  float  i = floor( x[j] / da); // in principle i < size(a)\n\n  a[(int)i] += 0.5;\n  a[(int)i+1] += 0.5; // I simplify the problem\n}\n```\n\nFor the context ```\nx```\n are the particle positions and ```\na```\n are the number of particles per cell.\nI want to execute this function in CUDA. The main issue is I can have several modifications of the same memory at the same time since ```\nx```\nis not sorted.\nI found the following solution, but I find it very slow.\nI define a temporary array ```\nd_temp_a```\n of size Na * number of threads used. Then, I reduce it to my full array.\nHere is the code (use ```\nnvcc -std=c++11 example_reduce.cu -o example_reduce.out```\n)\n```\n#include \"stdio.h\"\n#include <cuda.h>\n#include <random>\nusing namespace std;\n\n\n__global__ void getA(float *d_x, float *d_a, float *d_temp_a, int N, int Na, float da)\n{\n// Get our global thread ID\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n\n  float ix ;\n\n  // Compute a\n  for(int x = index; x < N; x += stride) {\n      ix = floor( d_x[x] / da );\n\n      d_temp_a[((int)ix) + Na * index] += 0.5;\n      d_temp_a[((int)ix + 1) + Na * index] += 0.5;\n  }\n  __syncthreads();\n\n\n  // Reduce\n  for(int l = index; l < Na; l += stride) {\n      for(int m = 0; m < stride; m += 1) {\n          d_a[l] += d_temp_a[l + Na * m];\n      }\n  }\n  __syncthreads();\n}\n\n\nint main(int argc, char **argv)\n{\n\n  int N = 1000000;   \n  int Na = 4096;   \n\n  float L = 50; // box size\n  float dxMesh = L / Na; // cell size\n\n  float *h_x, *h_a;  // host data\n\n  h_x = (float *)malloc(N * sizeof(float));\n  h_a = (float *)malloc(Na * sizeof(float));\n\n  /* Initialize random seed: */\n  std::default_random_engine generator;\n  std::uniform_real_distribution<float> generate_unif_dist(0.0,1.0);\n\n  // h_x random initialisation\n  for(int x = 0; x < N; x++) {\n        float random = generate_unif_dist(generator);\n        h_x[x] = random * L;\n    }\n\n \n  int blockSize = 512; // Number of threads in each thread block\n  int gridSize = (int)ceil((float) N /blockSize); // Number of thread blocks in grid\n\n  float *d_x, *d_a;  // device data\n\n  cudaMalloc((void **) &d_x, N * sizeof(float));\n  cudaMalloc((void **) &d_a, Na * sizeof(float));\n\n  cudaMemcpy(d_x, h_x, N * sizeof(float), cudaMemcpyHostToDevice);\n\n  // Create temp d_a array\n  float *d_temp_a;\n  cudaMalloc((void **) &d_temp_a, Na * blockSize * gridSize * sizeof(float));\n\n  getA<<<gridSize,blockSize>>>(d_x, d_a, d_temp_a, N, Na, da);\n\n  cudaMemcpy(h_a, d_a, Na * sizeof(float), cudaMemcpyDeviceToHost);\n\n  free(h_x);\n  free(h_a);\n\n  cudaFree(d_x);\n  cudaFree(d_a);\n  cudaFree(d_temp_a);\n\n  return 0;\n}\n```\n\nIt is slow because I only use 1 thread for every element of my array.\nMy question: Is there a way to optimize this reduction? I also found inefficient to have this extremely large array of size Na * number of threads. Is there a way to avoid using it ?\nNote that, I intend to write later a 2D version with ```\nx```\n and ```\ny```\n defining ```\na[i][j]```\n.\n    ", "Answer": "\r\nI think your approach might be a bit of an overkill for this problem.\nLike the other people in the comments, I also thought that you could implement your idea of reduction with thrust. However, my approach involved counting up the appearances of each idx and then inserting these counts (see Counting occurrences of numbers in a CUDA array)\nThrust reduction approach\nSo i implemented this almost exclusively with thrust methods (fill, transform, sort, reduce_by_key) and ran a final kernel over the end result to split the values between the two neighbouring cells. This worked and was much faster than your CUDA approach, but it was still much slower than the simple CPU implementation. The big problem was the sorting and reduce_by_key of the N values.\n```\nstruct custom_functor{\n    float factor;\n    custom_functor(float _factor){\n      factor = _factor;\n    }\n    __host__ __device__ int operator()(float &x) const {\n        return (int) floor(x / factor);\n    }\n};\n\n__global__ void thrust_reduce_kernel(float *d_a, int* d_a_idxs, int* d_a_cnts, int N, int Na, int n_entries)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (index >= n_entries)\n    return;\n\n  int a_idx = d_a_idxs[index];\n  int a_cnt = d_a_cnts[index];\n\n  if ((a_idx + 1) >= Na || a_idx < 0 || a_idx >= Na || (a_idx + 1) < 0)\n  {\n    printf(\"Should not happen according to you!\\n\");\n    return;\n  }\n\n  atomicAdd(&d_a[a_idx], a_cnt * 0.5f);\n  atomicAdd(&d_a[a_idx+1], a_cnt * 0.5f);\n}\n\nvoid test_thrust_reduce(float *d_x, float *d_a, float *h_a, int N, int Na, float da)\n{\n  int *d_xi, *d_ones;\n  int *d_a_cnt_keys, *d_a_cnt_vals;\n\n  cudaMalloc((void**) &d_xi, N * sizeof(int));\n  cudaMalloc((void**) &d_ones, N * sizeof(float));\n\n  cudaMalloc((void**) &d_a_cnt_keys, Na * sizeof(int));\n  cudaMalloc((void**) &d_a_cnt_vals, Na * sizeof(int));\n  CUDA_CHECK;\n\n  thrust::device_ptr<float> dt_x(d_x);\n  thrust::device_ptr<float> dt_a(d_a);\n  thrust::device_ptr<int> dt_xi(d_xi);\n  thrust::device_ptr<int> dt_ones(d_ones);\n  thrust::device_ptr<int> dt_a_cnt_keys(d_a_cnt_keys);\n  thrust::device_ptr<int> dt_a_cnt_vals(d_a_cnt_vals);\n\n  custom_functor f(da);\n  thrust::fill(thrust::device, dt_a, dt_a + Na, 0.0f);\n  thrust::fill(thrust::device, dt_ones, dt_ones + N, 1);\n  thrust::fill(thrust::device, dt_a_cnt_keys, dt_a_cnt_keys + Na, -1);\n  thrust::fill(thrust::device, dt_a_cnt_vals, dt_a_cnt_vals + Na, 0);\n\n  thrust::transform(thrust::device, dt_x, dt_x + N, dt_xi, f);\n  thrust::sort(thrust::device, dt_xi, dt_xi + N);\n\n  thrust::pair<thrust::device_ptr<int>,thrust::device_ptr<int>> new_end;\n  new_end = thrust::reduce_by_key(thrust::device, dt_xi, dt_xi + N, dt_ones, \n                                  dt_a_cnt_keys, dt_a_cnt_vals);\n\n  int n_entries = new_end.first - dt_a_cnt_keys;\n  int n_entries_2 = new_end.first - dt_a_cnt_keys;\n\n  dim3 dimBlock(256);\n  dim3 dimGrid((n_entries + dimBlock.x - 1) / dimBlock.x);\n  thrust_reduce_kernel<<<dimGrid, dimBlock>>>(d_a, d_a_cnt_keys, d_a_cnt_vals, N, Na, n_entries);\n  cudaMemcpy(h_a, d_a, Na * sizeof(float), cudaMemcpyDeviceToHost);\n\n  cudaFree(d_xi);\n  cudaFree(d_ones);\n  cudaFree(d_a_cnt_keys);\n  cudaFree(d_a_cnt_vals);\n}\n```\n\nSimple atomicAdd approach\nSo i was curious if you could just use a simple atomicAdd per entry in d_x and this proved to be the fastest solution of all of them.\n```\n__global__ void simple_atomicAdd_kernel(const float *d_x, float *d_a, float da, int N, int Na)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (index >= N)\n    return;\n\n  int a_idx = floor(d_x[index] / da); // in principle i < size(a)\n\n  atomicAdd(&d_a[a_idx], 0.5f);\n  atomicAdd(&d_a[a_idx+1], 0.5f);\n} \n\nvoid test_simple_atomicAdd(float *d_x, float *d_a, float *h_a, int N, int Na, float da)\n{\n  cudaMemset(d_a, 0, Na * sizeof(float));\n\n  dim3 dimBlock(256);\n  dim3 dimGrid((N + dimBlock.x - 1) / dimBlock.x);\n  simple_atomicAdd_kernel<<<dimGrid, dimBlock>>>(d_x, d_a, da, N, Na);\n  cudaMemcpy(h_a, d_a, Na * sizeof(float), cudaMemcpyDeviceToHost);\n}\n```\n\nResults\nYou can see my times for N=100,000 and da=0.1 below. Your initial value of N = 1,000,000 resulted in an out_of_memory exception for me. All\n```\nTimes: \n- CPU Reference:         912 us\n- CUDA Custom reduce:    34275 us\n- CUDA Thrust reduce:    2144 us\n- CUDA Simple atomicAdd: 59 us\n```\n\nLooking at higher values for N, the Thrust reduce approach starts to get better, because we have much more conflicts in the atomicAdd approach. This is very dependent on your x values and the value of da:\n```\nTimes (N=1,000,000, da=0.1): \n- CPU Reference:         9398 us\n- CUDA Thrust reduce:    1287 us\n- CUDA Simple atomicAdd: 409 us\n\nTimes (N=10,000,000, da=0.1): \n- CPU Reference:         92068 us\n- CUDA Thrust reduce:    3879 us\n- CUDA Simple atomicAdd: 3851 us\n\nTimes (N=100,000,000, da=0.1): \n- CPU Reference:         918950 us\n- CUDA Thrust reduce:    21051 us\n- CUDA Simple atomicAdd: 38583 us\n```\n\nDisclaimer: I am far from an expert in CUDA programming and there might be something important that I am missing. These are just my findings and I am certain that there exist approaches that work much better in your case. However, a simple atomicAdd approach might be a fast and easy solution to your problem.\nYou can checkout the full code here: https://github.com/steimich96/cuda_reduction_experiments\nI hope this is helpful.\nCheers, Michael\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Split reduction by items in shopping cart\r\n                \r\nI have an issue related to an ecommerce shopping cart. In certain circumstances (usually involving special offers or vouchers) a reduction might be made to the overall cart total - e.g. £10 voucher off total cart price. To calculate the tax part of this cart one of the suggestions by the tax office in the UK is to divide the reduction by the number of products in your cart and use this to calculate a new Gross (inc. tax) price for each product, from which you can calculate a new Net (excl. tax) price. For example: a £10 discount would be (in the case of 2 prods) £5 off gross of each. Obviously one problem here could be if one of the items is very cheap, e.g. £2.99 and therefore cannot take the £5 reduction. \n\nI need a method of calculating what this reduction should be, so it splits evenly between the total number of products but never makes the cost of a single item less than zero. Help!\n\nFWIW i will be programming this in PHP\n\nEDIT\n\nOswald's example would give me something like this in PHP:\n\n```\n$prodA = 2.99;\n$prodB = 20.00;\n$total = ($prodA + $prodB);\n$reduction = 5.00;\n\n$newA = ($prodA - (($prodA/$total) * $reduction));\n$newB = ($prodB - (($prodB/$total) * $reduction));\n```\n\n\nIt seems to add up..\n    ", "Answer": "\r\nIf ```\nitem```\n is the gross price of an item, ```\ntotal```\n is the sum of all gross prices, and ```\nreduction```\n is the amount you want to subtract from the bill, then the new gross price for the item is\n\n```\nitem - item/total * reduction\n```\n\n\nThis obviously does not split the reduction evenly among the items. But splitting the reduction evenly among the items leads to problems (as you have noticed yourself).\n\nRather the method splits the reduction proportionally according to the price of the items. E.g if an item of 10$ is reduced by 1$, then an item of 20$ will be reduced by 2$ and item item of 5$ will be reduced by 0.5$.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Time complexity of Parallel Reduction Algorithm\r\n                \r\nCurrently I am studying GPU architecture and its concepts. In parallel Reduction technique, how is the time complexity shown on 29th slide in following NVIDIA guide come O(N/P + log N)? I know that for N threads, it will be O(log N). If we have P threads parallel available then time complexity should be O((N/P)*log P). Right? Where am I wrong here? \n\nParallel Reduction Techniques \n    ", "Answer": "\r\nI would like to explain this with an example, consider this array with N=8 elements\n\n```\n1  2  3  4  5  6  7  8\n```\n\n\nThe parallel reduction will occur in following steps\n\n```\n1  2  3  4  5  6  7  8\n  3    7      11   15\n    10          26\n          36\n```\n\n\nIf you count the number of reduction operations, we have 4,2 and 1 on first, second and third step respectively. So total number of operations we have is 4+2+1=7=N-1 and we do all the reductions in O(N) and we also have log(8)=3 (this is log to base 2) steps so we pay a cost to do these steps which is O(logN). Hence if we used a single thread to reduce in this way we add the two costs as they occur separately of each other and we have O(N+logN). Where O(N) is cost for doing all operations and O(logN) is cost for doing all steps. Now there is no way to parallelize the cost for steps since they have to happen sequentially. However we can use multiple threads to do the operations and divide the O(N) cost to O(N/P). Therefore we have\n\n```\nTotal cost = O(N/P + logN)\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction link in category\r\n                \r\nIf i create new category, for example article i have all posts in this category on link:\n\n```\nhttp://www.mypage.com/category/article\n```\n\n\nhow can i make:\n\n```\nhttp://www.mypage.com/article\n```\n\n\nwithout \"category\"?\n    ", "Answer": "\r\nIt's been awhile since I have used Wordpress, but if I'm not mistaken, you should be able to edit your URL in wp-admin. You can use this page to help you locate the section in the admin.\n\nYou would use a URL structure as such...\n\n```\n%postname%\n```\n\n\nThat will remove the category from the URL. Just keep in mind that changing this could mess up the permalinks for any previous posts. You can either modify the permalinks in wp-admin or through the database using a tool such as phpMyAdmin. Alternatively, I believe there are plugins that will redirect old permalinks, but like I said, it's a been a while since the last time I used Wordpress.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OMP parallel for reduction\r\n                \r\nI'm trying to write a k-means clustering class. I want to make my function parallel.\n\n```\nvoid kMeans::findNearestCluster()\n{   \n    short closest;\n    int moves = 0;\n    #pragma omp parallel for reduction(+:moves)\n    for(int i = 0; i < n; i++)\n    {      \n        float min_dist=FLT_MAX;\n        for(int k=0; k < clusters; k++)\n        {\n            float dist_sum = 0.0, dist2 = 0.0;\n            for(int j = 0; j < dim; j++)\n            {\n                dist_sum = centroids[k*dim+j] - data[i*dim+j];\n                dist2 +=  dist_sum * dist_sum;\n\n            }\n            if (dist2 < min_dist)\n            {\n                min_dist = dist2;\n                closest = k;\n\n            }\n\n        }\n\n        if (assignment[i] != closest)\n        {\n            assignment[i] = closest;\n            moves++;\n        }        \n\n    }\n\n    this->moves = moves;\n\n}\n```\n\n\nThis is how it should work:\n\n\nStep 1: Find nearest cluster\n\n\nLoop through all the datapoints, and compare the distances between all centroids. \nWhen the closest centroid is found, it is stored in the variable called ```\nclosest```\n.\nCheck if this point is assigned to the newly found closest cluster. If not, move it the the new one. (increment moves)\n\nStep 2: Recalculate the centroids, based on the new assignments. (function is not shown)\nRepeat Step 1 and Step 2 until no more movement happens.\n\n\nWithout ```\n#parallel```\n ```\nmoves```\n converges to zero. If I have ```\n#parallel```\n moves have random values. I think because the parallel loops have conflict to update ```\nmove```\n. \n\nMaybe a better strategy would be to have each thread its own move variable, and at the end they would some up.\n    ", "Answer": "\r\nYou use the ```\nclosest```\n variable inside parallel loop, both writing to it and using it as a check before increasing your ```\nmoves```\n variable.  But it is declared outside the loop, so all iterations use the same variable!  As all iterations are executed (theoretically) in parallel, you cannot expect that any iteration sees what any other iteration wrote to ```\nclosest```\n in branch condition ```\nif (assignment[i] != closest)```\n.  This variable becomes randomly updated by racing parallel threads.  Therefore your ```\nmoves```\n evaluates to junk value.\n\nMoving a declaration of ```\nclosest```\n inside the outer loop or declaring it as ```\nprivate(closest)```\n in OpenMP pragma may solve your problem.\n\nBy the way,  ```\nclosest```\n may be uninitialized, and should be better of the same type as ```\nk```\n, i.e. ```\nint```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "color depth reduction with opencv and LUT\r\n                \r\nI'd like to perform a color reduction via color depth scaling.\n\nLike this example:\n\n\nthe first image is CGA resolution, the second is EGA, the third is HAM.\nI'd like to do it with cv::LUT because i think it is the betterway to do it.\nI can do with greyscale with this code:\n\n```\nMat img = imread(\"test1.jpg\", 0);\nuchar* p;\nMat lookUpTable(1, 256, CV_8U);\np = lookUpTable.data;\nfor( int i = 0; i < 256; ++i)\n    p[i] = 16 * (i/16)\nLUT(img, lookUpTable, reduced);\n```\n\n\noriginal:\n\ncolor reduced: \n\nbut if i try to do it with color I get strange result..\n\n\n\nwith this code:\n\n```\nimgColor = imread(\"test1.jpg\");\nMat reducedColor;\nint n = 16;\nfor (int i=0; i<256; i++) {\n    uchar value = floor(i/n) * n;\n    cout << (int)value << endl;\n    lut.at<Vec3b>(i)[2]= (value >> 16) & 0xff;\n    lut.at<Vec3b>(i)[1]= (value >> 8) & 0xff;\n    lut.at<Vec3b>(i)[0]= value & 0xff;\n} \nLUT(imgColor, lut, reducedColor);\n```\n\n    ", "Answer": "\r\nYou'll probably have moved on by now, but the root of the problem is that you are doing a 16-bit shift to ```\nuchar value```\n, which is just 8-bits long. Even an 8-bit shift in this case is too much, as you'll erase all the bits in the ```\nuchar```\n. Then there is the fact that the ```\ncv::LUT```\n documentation explicitly states that ```\nsrc```\n must be an \"input array of 8-bit elements\", which clearly isn't the case in your code. The net result is that only the first channel of the color image (the Blue channel) is transformed by ```\ncv::LUT```\n.\n\nThe best way to work around these limitations is to split color images across channels, transform each channel separately, and then merge the transformed channels into a new color image. See the code below:\n\n```\n/*\nCalculates a table of 256 assignments with the given number of distinct values.\n\nValues are taken at equal intervals from the ranges [0, 128) and [128, 256),\nsuch that both 0 and 255 are always included in the range.\n*/\ncv::Mat lookupTable(int levels) {\n    int factor = 256 / levels;\n    cv::Mat table(1, 256, CV_8U);\n    uchar *p = table.data;\n\n    for(int i = 0; i < 128; ++i) {\n        p[i] = factor * (i / factor);\n    }\n\n    for(int i = 128; i < 256; ++i) {\n        p[i] = factor * (1 + (i / factor)) - 1;\n    }\n\n    return table;\n}\n\n/*\nTruncates channel levels in the given image to the given number of\nequally-spaced values.\n\nArguments:\n\nimage\n    Input multi-channel image. The specific color space is not\n    important, as long as all channels are encoded from 0 to 255.\n\nlevels\n    The number of distinct values for the channels of the output\n    image. Output values are drawn from the range [0, 255] from\n    the extremes inwards, resulting in a nearly equally-spaced scale\n    where the smallest and largest values are always 0 and 255.\n\nReturns:\n\nMulti-channel images with values truncated to the specified number of\ndistinct levels.\n*/\ncv::Mat colorReduce(const cv::Mat &image, int levels) {\n    cv::Mat table = lookupTable(levels);\n\n    std::vector<cv::Mat> c;\n    cv::split(image, c);\n    for (std::vector<cv::Mat>::iterator i = c.begin(), n = c.end(); i != n; ++i) {\n        cv::Mat &channel = *i;\n        cv::LUT(channel.clone(), table, channel);\n    }\n\n    cv::Mat reduced;\n    cv::merge(c, reduced);\n    return reduced;\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "C OpenMP - Reduction scalability\r\n                \r\nI'm testing the performance speedup of some algorithms when using OpenMP and one of then is not scaling. Am I doing something wrong?\n\nPC Details:\n\n\nMemory: 7,7 GiB\nProcessor: Intel® Core™ i7-4770 CPU @ 3.40GHz × 8 \nOS: Ubuntu 15.04 64-bit\ngcc: gcc (Ubuntu 4.8.2-19ubuntu1) 4.8.2\n\n\nCode:\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <omp.h>\n\nint main(int argc, char **argv) {\n  int test_size, i;\n  double *vector, mean, stddeviation, start_time, duration;\n\n  if (argc != 2) {\n    printf(\"Usage: %s <test_size>\\n\", argv[0]);\n    return 1;\n  }\n\n  srand((int) omp_get_wtime());\n\n  test_size = atoi(argv[1]);\n  printf(\"Test Size: %d\\n\", test_size);\n\n  vector = (double *) malloc(test_size * sizeof(double));\n  for (i = 0; i < test_size; i++) {\n    vector[i] = rand();\n  }\n\n  start_time = omp_get_wtime();\n  mean = 0;\n  stddeviation = 0;\n#pragma omp parallel default(shared) private(i)\n  {\n#pragma omp for reduction(+:mean)\n    for (i = 0; i < test_size; i++) {\n      mean += vector[i];\n    }\n#pragma omp single\n    mean /= test_size;\n\n#pragma omp for reduction(+:stddeviation)\n    for (i = 0; i < test_size; i++) {\n      stddeviation += (vector[i] - mean)*(vector[i] - mean);\n    }\n  }\n  stddeviation = sqrt(stddeviation / test_size);\n  duration = omp_get_wtime() - start_time;\n\n  printf(\"Std. Deviation = %lf\\n\", stddeviation);\n  printf(\"Duration: %fms\\n\", duration*1000);\n\n  return 0;\n}\n```\n\n\nCompilation line\n\n```\ngcc -c -o main.o main.c -fopenmp -lm -O3\ngcc -o dp main.o -fopenmp -lm -O3\n```\n\n\nResults\n\n```\n$ OMP_NUM_THREADS=1 ./dp 100000000\n166.224199ms\n\n$ OMP_NUM_THREADS=2 ./dp 100000000\n157.924034ms\n\n$ OMP_NUM_THREADS=4 ./dp 100000000\n159.056189ms\n```\n\n    ", "Answer": "\r\nI am not reproducing your results with Ubuntu 14.04.2 LTS, gcc 4.8, and a 2.3 GHz Intel Core i7.  Here are the results that I get:\n\n\n$ OMP_NUM_THREADS=1 ./so30627170 100000000\nTest Size: 100000000\nStd. Deviation = 619920018.463329\nDuration: 206.301721ms\n$ OMP_NUM_THREADS=2 ./so30627170 100000000\nTest Size: 100000000\nStd. Deviation = 619901821.463117\nDuration: 110.381279ms\n$ OMP_NUM_THREADS=4 ./so30627170 100000000\nTest Size: 100000000\nStd. Deviation = 619883614.594906\nDuration: 78.241708ms\n\n\nBecause the output listed in the \"Results\" section of your question could not match the output from the code as listed, you may be running an old version of your code.\n\nI thought about possibly using X86 intrinsics within the parallel ```\nfor```\n loops, but examining the assembly output, gcc already uses SIMD instructions in this case.  Without march options, I was seeing gcc use SSE2 instructions.  Compiling with ```\n-march=native```\n or ```\n-mavx```\n, gcc would use AVX instructions.\n\nEDIT: Running the Go version of your program, I get:\n\n\n$ ./tcc-go-desvio-padrao -w 1 -n 15 -t 100000000\n2015/06/07 08:26:43 Workers: 1\n2015/06/07 08:26:43 Tests: [100000000]\n2015/06/07 08:26:43 # of executions of each test: 15\n2015/06/07 08:26:43 Time to allocate memory: 584.477µs\n2015/06/07 08:26:43 ===========================================\n2015/06/07 08:26:43 Current test size: 100000000\n2015/06/07 08:27:05 Time to fill the array: 1.322556083s\n2015/06/07 08:27:05 Time to calculate: 194.10728ms\n$ ./tcc-go-desvio-padrao -w 2 -n 15 -t 100000000\n2015/06/07 08:27:10 Workers: 2\n2015/06/07 08:27:10 Tests: [100000000]\n2015/06/07 08:27:10 # of executions of each test: 15\n2015/06/07 08:27:10 Time to allocate memory: 565.273µs\n2015/06/07 08:27:10 ===========================================\n2015/06/07 08:27:10 Current test size: 100000000\n2015/06/07 08:27:22 Time to fill the array: 677.755324ms\n2015/06/07 08:27:22 Time to calculate: 113.095753ms\n$ ./tcc-go-desvio-padrao -w 4 -n 15 -t 100000000\n2015/06/07 08:27:28 Workers: 4\n2015/06/07 08:27:28 Tests: [100000000]\n2015/06/07 08:27:28 # of executions of each test: 15\n2015/06/07 08:27:28 Time to allocate memory: 576.568µs\n2015/06/07 08:27:28 ===========================================\n2015/06/07 08:27:28 Current test size: 100000000\n2015/06/07 08:27:34 Time to fill the array: 353.646193ms\n2015/06/07 08:27:34 Time to calculate: 79.86221ms\n\n\nThe timings appear about the same as the OpenMP version.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP to CUDA: Reduction\r\n                \r\nI'm trying to figure out how I can use OpenMP's ```\nfor reduction()```\n equivalent in CUDA.  I've done some research online, and none of what I've tried worked.  The code:\n\n```\n    #pragma omp parallel for reduction(+:sum)\n    for (i = 0; i < N; i++)\n    {\n        float f = ...  //store return from function to f\n        out[i] = f;    //store f to out[i]\n        sum += f;      //add f to sum and store in sum\n    }\n```\n\n\nI know what ```\nfor reduction()```\n does in OpenMP....it makes the last line of the for loop possible.  But how can I use CUDA to express the same thing?\n\nThanks!\n    ", "Answer": "\r\nUse Thrust, An STL inspired library that comes with CUDA. See the Quick Start Guide for examples on how to perform reductions.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction in Solr size\r\n                \r\nThere was a query that I had regarding the size of Solr data backup. We take Solr backups once a day. We could observed that the size of Solr backup was reduced by 1 GB from that of the previous day, but there had been no deletions or updations made on Solr that day. We checked the number of documents also for both the days. It was more for the backup with lesser size. Is it because of any optimization that Solr is doing internally? \n    ", "Answer": "\r\nDeleted documents (and remember that an update is a delete + an add internally) are not removed before ```\noptimize```\n is called on the index or the mergeFactor is hit. This causes the index files to be rewritten to disk, and any deleted content is expunged.\n\nAfter the index files have been rewritten, the old files are removed and the new index files does not contain the old, deleted documents.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Picture dimension reduction\r\n                \r\nI need to reduce my picture dimension by means of singular value decomposition.\nI've seen a lot of sources for this theme. They advise to use the following:\n```\nm, n = img.shape\nU, S, V = np.linalg.svd(img)\n#U.shape = m * m; S.shape = m * n; V.shape = n * n\nimg_cut = U[:, :x] @ np.diag(S[:x, :x]) @ V[:x, :]\n```\n\nThis is supposed to reduce the size of the image down to ```\nx```\n. But the size of the image will be the same ```\nm * n```\n, because ```\nU.shape[0]```\n and ```\nV.shape[1]```\n are remained the same. What can I do?\n    ", "Answer": "\r\nAs you have realized the dimension is the same.\nThe principal of SVD is that you decompose a matrix into three parts ```\nM=USV^T```\n.\nBy using only the first x singular vectors and values you can reduce a data point to ```\nx```\n dimensions.\nFor this you only use two of the matrices:\n```\nimg_cut = U[:, :x] @ np.diag(S[:x, :x])\n```\n\nYou can likewise do it with V which is a reduction.\nWhen thinking about tabular data, doing it this was is not a reduction in features but in samples.\n\nBy using the other matrix you can reconstruct your original data - with some error depending on x. This reconstructed data is a low rank approximation with rank x.\n```\nimg_reconstructed = img_cut @ V[:x, :]\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Multiprocessing performance reduction\r\n                \r\nI'm trying to speed up a nested loop execution by parallelising an inner loop with ```\nmultiprocessing```\n module. In the simple example N jobs are repeatedly running multiple times and the problem is that elapsed time of each iteration is dramatically increasing. I think the reason is a growth of a global ```\nglobal_res```\n list which being copied in every thread, but I don't know how to prevent it. Is there any way how to directly tell ```\nmp.Process```\n which global variables are needed?\n```\nimport time\nimport numpy as np\nimport multiprocessing as mp\n\ndef process_with_big_output(ret_list, i):\n    aux = np.random.randint(0, 10, size=100000)\n    ret_list[i] = aux\n\nrepeats=100\nelapsed=[]\nonly_parallel=[]\n\nglobal_res = [None for _ in range(repeats)]\nfor itr in range(repeats):\n    start = time.time()\n    N = 1000\n    all_res = [0 for _ in range(N)]\n    manager = mp.Manager()\n    return_list = manager.list(range(N))\n\n    jobs = []\n    for i in range(N):\n        p = mp.Process(target=process_with_big_output,\n                       args=(return_list, i))\n        jobs.append(p)\n\n    for j in jobs:\n        j.start()\n    for j in jobs:\n        j.join()\n\n    for i in range(N):\n        all_res[i] = return_list[i]\n        \n    only_parallel.append(time.time()-start)\n        \n    global_res[itr] = all_res\n    \n    elapsed.append(time.time()-start)\n    if itr%10:\n        print(only_parallel[-1], elapsed[-1])\n```\n\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Documents-terms matrix dimensionality reduction\r\n                \r\nI am working with text documents clustering, with a Hierarchical Clustering approach, in Python.\n\nI have a corpus of 10k documents and have constructed a documents-terms matrix over a dictionary based on a collection of terms classified as 'keyword' for the entire corpus.\nThe matrix has a shape: [10000 x 2000] and is very sparse. (let's call it dtm)\n\n```\nid    0    1    2    4    ...    1998    1999\n 0    0    0    0    1    ...       0       0\n 1    0    1    0    0    ...       0       1\n 2    1    0    0    0    ...       1       0\n ..    ..      ...        ...      ..      ..\n9999  0    0    0    0    ...       0       0\n```\n\n\nI think that applying some dimensionality reduction techniques could lead to an enhancement in the precision of clustering.\nI have tried using some MDS approach like this\n\n```\ndef select_n_components(var_ratio, goal_var: float) -> int:\n# Set initial variance explained so far\ntotal_variance = 0.0\n\n# Set initial number of features\nn_components = 0\n\n# For the explained variance of each feature:\nfor explained_variance in var_ratio:\n\n    # Add the explained variance to the total\n    total_variance += explained_variance\n\n    # Add one to the number of components\n    n_components += 1\n\n    # If we reach our goal level of explained variance\n    if total_variance >= goal_var:\n        # End the loop\n        break\n\n# Return the number of components\nreturn n_components\n\n\ndef do_MDS(dtm):\n    # scale dtm in range [0:1] to better variance maximization\n    scl = MinMaxScaler(feature_range=[0, 1])\n    data_rescaled = scl.fit_transform(dtm)\n\n    tsvd = TruncatedSVD(n_components=data_rescaled.shape[1] - 1)\n    X_tsvd = tsvd.fit(data_rescaled)\n\n    # List of explained variances\n    tsvd_var_ratios = tsvd.explained_variance_ratio_\n\n    optimal_components = select_n_components(tsvd_var_ratios, 0.95)\n\n    from sklearn.manifold import MDS\n    mds = MDS(n_components=optimal_components, dissimilarity=\"euclidean\", random_state=1)\n    pos = mds.fit_transform(dtm.values)\n\n    U_df = pd.DataFrame(pos)\n    U_df_transposed = U_df.T  # for consistency with pipeline workflow, export tdm matrix\n    return U_df_transposed\n```\n\n\nThe objective is to automatically detect an optimal number of components and apply the dimensionality reduction. But the output has not shown a tangible enhancement.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction parameter of the negative log likelihood\r\n                \r\nWhat is the intuitive explanation of the reduction parameter in negative log likelihood loss function in PyTorch? The parameter can take values such as 'mean' or 'sum'. Is it summing over the elements of the batch?\n\n```\ntorch.nn.functional.nll_loss(outputs.mean(0), target, reduction=\"sum\")\n```\n\n    ", "Answer": "\r\nFrom the documentation: \n\nSpecifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n\nIf you use none, the output will be the same as batch size, \n\nIf you use mean, it will be the mean (sum divided by batch)\n\nIf you use sum, it will be the sum of all elements. \n\nYou can also verify this with following code:\n\n```\nimport torch \nlogit = torch.rand(100,10)\ntarget = torch.randint(10, size=(100,)) \nm = torch.nn.functional.nll_loss(logit, target)\ns = torch.nn.functional.nll_loss(logit, target, reduction=\"sum\") \nl = torch.nn.functional.nll_loss(logit, target, reduction=\"none\")\nprint(torch.abs(m-s/100))\nprint(torch.abs(l.mean()-m))\n```\n\n\nThe output should be 0 or very close to 0. \n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Can Z3 apply bit-width reduction techniques to solve a bit-vector equivalence?\r\n                \r\nDuring a program synthesis task I hit the following equivalence, which is based on mixed-boolean-arithmetic logic and that I can't seem to be able to prove with Z3 in a reasonable amount of time (some seconds or minutes). I'm following a counter-example driven approach (CEGIS), feeding the original expression and the synthesis candidate to a distinct query: if no counter-examples are found the synthesis is correct.\n```\n(set-logic QF_BV)\n(declare-fun y () (_ BitVec 64))\n(declare-fun x () (_ BitVec 64))\n(assert (let ((a!1 (bvsub (bvsub (bvsub x #x0000000000000002) y)\n                  (bvshl (bvor x (bvnot y)) #x0000000000000001))))\n(let ((a!2 (bvmul (bvand (bvxor a!1 x) (bvnot y))\n                  (bvand y (bvnot (bvxor a!1 x))))))\n(let ((a!3 (bvadd (bvmul (bvor (bvxor a!1 x) y) (bvand (bvxor a!1 x) y)) a!2)))\n  (distinct a!3 (bvmul y y))))))\n(check-sat)\n```\n\nI know the equivalence to hold just fine for reduced bit-width bit-vectors (e.g. 8 or 16), so I was wondering if Z3 is employing any technique to first prove the bit-width reduced version of the query and then extend the result to the full-width version.\nI tried the Z3 bv-size-reduction tactic, but it doesn't seem to fit my requirements because it's meant to reduce the size of the constants and not of the symbolic bit-vectors. Moreover, the implementation of the Speeding-up Quantified Bit-Vector Solving Bit-Width Reductions and Extensions paper doesn't seem to work out for this specific case, because no useful counter-model is generated, keeping the size of the problem unaltered.\nI'm not a SAT/SMT person, as I barely use tools like Z3 as black boxes to play around with synthesis problems, so I was wondering:\n\nIs there anything I can do to reduce the bit-width of the bit-vectors, prove the reduced query and extend the results to the full-width query?\nIs there any other viable way to prove the unsatisfiability of the aforementioned query?\n\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Can PCA be used for row reduction\r\n                \r\nI had a doubt about Principle Component analysis. If the variables are along the row:\n```\n            delhi| kolkata|  up|  mp|  bihar|  assam|\npopolation   1.2 |   2.2  | 1.3| 1.4|   2   |   1.1 |\n      crop    a  |    b   |  c |   a|   b   |     c |\n  avg temp    1  |    2   |  3 |   4|   5   |     6 |\n soil ph      1  |    2   |  1 |   3|   2   |     1 |\n```\n\nAnd one wants to do PCA to obtain most important uncorrelated variables, can one do that. The idea is not to reduce the columns but rows.\nIf anyone could explain this concept to me it would be very helpful as my understanding is variables exist only along columns and there are many code examples in python for column dimension reduction using pca. But I am not sure if row reduction is the same thing.\nThanks in advance.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Array extension/reduction\r\n                \r\nI work with Oracle JGeometry method which returns (or takes as method argument) 2D segments as double array (x1,y1,x2,y2 ... xn,yn). I would like to extend this array to contain 3D segments as double array (x1,y1,z1,x2,y2,z2 ... xn,yn,zn) by default Z value or reduce to 2D (by removing of all Z coordinate). I wrote simple utility methods for making this. Is there any easier or smarter way to do this?\nThe conversion from 2D to 3D:\n```\npublic static double[] to3D(double z, double[] inputArray) {\n    List<Double> convertedItems = new ArrayList<>();\n    for (int i = 0; i < inputArray.length; i++) {\n        convertedItems.add(inputArray[i]);\n        if ((i + 1) % 2 == 0) {\n            convertedItems.add(z);\n        }\n    }\n    return convertedItems.stream().mapToDouble(Double::doubleValue).toArray();\n}\n```\n\nThe conversion from 3D to 2D:\n```\npublic static double[] to2D(double[] inputArray) {\n    List<Double> convertedItems = new ArrayList<>();\n    for (int i = 0; i < inputArray.length; i++) {\n       if ((i + 1) % 3 == 0) {\n           continue;\n       }\n       convertedItems.add(inputArray[i]);     \n    }\n    return convertedItems.stream().mapToDouble(Double::doubleValue).toArray();\n}\n```\n\n    ", "Answer": "\r\nThis should offer some performance improvements.\n```\npublic static double[] to3D(double z, double[] inputArray) {\n    if (inputArray.length % 2 != 0) throw new IllegalArgumentException();\n    double[] convertedArray = new double[inputArray.length / 2 * 3];\n    for (int i = 0; i < inputArray.length / 2; i++) {\n        System.arraycopy(inputArray, i * 2, convertedArray, i * 3, 2);\n        convertedArray[i * 3 + 2] = z;\n    }\n    return convertedArray;\n}\n\npublic static double[] to2D(double[] inputArray) {\n    if (inputArray.length % 3 != 0) throw new IllegalArgumentException();\n    double[] convertedArray = new double[inputArray.length / 3 * 2];\n    for (int i = 0; i < inputArray.length / 3; i++) {\n        System.arraycopy(inputArray, i * 3, convertedArray, i * 2, 2);\n    }\n    return convertedArray;\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP reduction clause\r\n                \r\nI have a loop which I am trying to parallelize with OpenMP. The construct I have in code is this,\n\n```\nfor(int x = 0; x < <condition>; x++) \n{\n    aggregate += <return value from very costly function(param1)>;\n}\n```\n\n\nI am trying to parallelize it as follows,\n\n```\n#pragma omp parallel for\nfor(int x = 0; x < <condition>; x++) \n{\n    #pragma omp parallel private(param1)\n    aggregate += <return value from very costly function(param1)>;\n}\n```\n\n\nIt crashes if I dont make param1 private since all threads use the same memory. But also I always get aggregate's value as 0. I understand that this is because I have not reduced it to sum each thread's local calculated value of aggregate. I am confused at this, how do I do that to sum all the values from the threads?\n\nThis is in Visual Studio 13 C++.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "WxPython Size Reduction\r\n                \r\nIn have built wxpython phoenix on Linux CentOS 7 but the size of the complete module is around 250 MB and we don't need all the widgets.\n\nIs there anyway to reduce the size of Wxpython? \n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Np-hardness reduction\r\n                \r\nIf I want to show that a problem is np-hard is it ok to use a existing np-hard problem multiple times? For example use Hamiltonian Cycle n times in a graph where n is the number of vertices? Or do I need to transform the graph into something that can easily be solved by an existing np-hard problem used 1 time?\n    ", "Answer": "\r\nYou need to show the exact oposite.\n\nIt doesn't prove anything if you prove you can solve your problem with an NP-Hard problem. [You can solve every problem in NP using SAT, by Cook-Levin Theorem].\n\nYou need to show that if your problem is solvable in polynomial time - so is an NP-Hard problem. That what a reduction actually does.\n\nFor example: If I can show I can solve shortest path, using TSP - does it make shortest path NP-Hard? Of course not! It only shows TSP is at least as hard as shortest path!\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Java script reduction\r\n                \r\nI was wondering if there was anyway of simplifying the code below without using jQuery?\n\nStill very inexperienced with JavaScript so any help is much appreciated! Thank you in advance everyone :D\n\n```\n    if (name === \"\") {\n        document.getElementById(\"name\").focus();\n        alert(\"Name must be filled out.\");\n        return false;\n    } else if (!(/\\S/.test(name))) {\n        document.getElementById(\"name\").focus();\n        alert(\"Name cannot be blank.\");\n        return false;\n    } else if (!(/^([^0-9]*)$/.test(name))) {\n        document.getElementById(\"name\").focus();\n        alert(\"Name cannot contain numbers.\");\n        return false;\n    } else if (email === \"\") {\n        document.getElementById(\"email\").focus();\n        alert(\"Please enter your email address.\");\n        return false;\n    } else if (/^\\S+@\\S+\\.\\S+$/.test(email) === false) {\n        document.getElementById(\"email\").focus();\n        alert(\"Please enter a valid email address.\");\n        return false;\n    } else if (basecamp === \"\") {\n        document.getElementById(\"basecamp\").focus();\n        alert(\"Please select a base camp.\");\n        return false;\n    } else if (max == 0) {\n        document.getElementById(\"basecamp\").focus();\n        alert(\"This base camp has run out of slots, please select another base camp.\");\n        return false;\n    } else if (package === \"\") {\n        document.getElementById(\"package\").focus();\n        alert(\"Please select a package.\");\n        return false;\n    } else if (validdate === \"\") {\n        document.getElementById(\"date\").focus();\n        alert(\"Please select a date.\");\n        return false;\n    } else if (groupsize === \"\") {\n        document.getElementById(\"groupsize\").focus();\n        alert(\"Please select a group size.\");\n        return false;\n    } else if (groupsize <= 0) {\n        document.getElementById(\"groupsize\").focus();\n        alert(\"Please select a postitve number.\");\n        return false;\n    } else {\n        updateData();\n    }\n}\n```\n\n    ", "Answer": "\r\nYou might use an array of conditions, where each subarray (or subobject) contains the condition to test (what's in your ```\nif```\n / ```\nelse if```\n at the moment), the ID to focus if the condition is true, and the message to ```\nalert```\n. Then, iterate over it, finding the first truthy condition - if found, ```\nalert```\n the associated message, focus the element, and ```\nreturn false```\n. Otherwise, if none of the bad conditions were found, call ```\nupdateData```\n:\n\n```\nconst arr = [\n  [\n    name === \"\",\n    'name',\n    \"Name must be filled out.\"\n  ],\n  [\n    !(/\\S/.test(name)),\n    'name',\n    'Name cannot be blank.'\n  ],\n  [\n    !(/^([^0-9]*)$/.test(name)),\n    'name',\n    'Name cannot contain numbers.'\n  ]\n  // etc\n];\n\nconst firstBadCondition = arr.find(([cond]) => cond);\nif (firstBadCondition) {\n  const [, idToFocus, errorMessage] = firstBadCondition;\n  document.getElementById(idToFocus).focus();\n  alert(errorMessage);\n  return false;\n} else {\n  updateData();\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "no speedup for omp simd reduction\r\n                \r\nI am trying to use vectorization (openmp simd) in order to speed up matrix multiplication. In order to make use of vectorization, I transpose the second matrix (to make the fastest-changing index go over contiguous memory). I'm running my tests on 3000 x 3000 arrays. Because I wasn't able to measure a difference in wall time when having vs not having the open mp pragma I wanted to confirm that I am actually getting a speedup for individual arrays that I am multiplying (which wasn't the case). Because of that, I inserted some dummy arrays of the same size in order to check if get a speedup with SIMD on them which I did (at least when using the reduction clause).\nNow my question is what is my problem that is hindering the SIMD speedup my only guess is that it must be the two-dimensionality of the array but I don't fully get why that would cause a slowdown.\nOr is there another problem with my code that I don't see?\n```\n#include <stdlib.h>\n#include <stdio.h>\n#include <omp.h>\n\n#include <time.h>\n\nconst int N = 3000;\n\nstruct timespec begin, end;\n\ndouble **create_a() {\n    double *a = (double *)aligned_alloc(32, sizeof(double) * N * N);\n    double **a_indexed = (double **)aligned_alloc(32, sizeof(double *) * N);\n    for (int i = 0; i<N; i++){\n        a_indexed[i] = a+i*N;\n    }\n\n    for (int i = 0; i< N; i++) {\n        for (int j = 0; j<N; j++) {\n            a_indexed[i][j] = i * j;\n        }\n    }\n    return a_indexed;\n}\n\ndouble **create_b(){\n    double *b = (double *)aligned_alloc(32, sizeof(double) * N * N);\n    double **b_indexed = (double **)aligned_alloc(32, sizeof(double *) * N);\n    for (int i = 0; i<N; i++){\n        b_indexed[i] = b+i*N;\n    }\n\n    for (int i = 0; i< N; i++) {\n        for (int j = 0; j<N; j++) {\n            b_indexed[i][j] = (i == j) ? 1:0;\n        }\n    }\n    return b_indexed;\n}\n\ndouble **transpose( double **matrix) {\n    double *t = (double *)aligned_alloc(32, sizeof(double) * N * N);\n    double **t_indexed = (double **)aligned_alloc(32, sizeof(double *) * N);\n    for (int i = 0; i<N; i++){\n        t_indexed[i] = t+i*N;\n    }\n\n    for (int i = 0; i< N; i++) {\n        for (int j = 0; j<N; j++) {\n            t_indexed[i][j] = matrix[j][i];\n        }\n    }\n    return t_indexed;\n}\n\ndouble **mul(double **a, double **b) {\n    double **b_t = transpose(b);\n    double *res = (double *)aligned_alloc(32, sizeof(double) * N * N);\n    double **res_indexed = (double **)aligned_alloc(32, sizeof(double *) * N);\n    for (int i = 0; i<N; i++){\n        res_indexed[i] = res+i*N;\n    }\n\n    for (int row = 0; row< 1; row++) {\n        for (int col = 0; col < 1; col++) {\n    double cell_res = 0;\n\n    // problematic calculation that I can't get to speed up no matter what pragma I use\n    clock_gettime(CLOCK_REALTIME, &begin);\n    #pragma omp simd aligned(a, b_t:32) reduction(+:cell_res)\n    for (int i = 0; i < N; i++) {\n        cell_res += a[0][i] * b_t[0][i];\n    }\n    clock_gettime(CLOCK_REALTIME, &end);\n\n    long seconds = end.tv_sec - begin.tv_sec;\n    long nanoseconds = end.tv_nsec - begin.tv_nsec;\n    double elapsed = seconds + nanoseconds*1e-9;\n\n    printf(\"Time simd reduce measured: %.9f seconds.\\n\", elapsed);\n\n\n    // dummy array measurements\n\n    struct timespec begin2, end2;\n    struct timespec begin3, end3;\n    struct timespec begin4, end4;\n\n    double *a2 = (double *)aligned_alloc(32, sizeof(double) * N);\n    double *b2 = (double *)aligned_alloc(32, sizeof(double) * N);\n    for (int i = 0; i < N ; i++){\n        a2[i] = 1;\n        b2[i] = 1;\n    }\n\n    // measurement with reduction is significantly faster than others\n    clock_gettime(CLOCK_REALTIME, &begin2);\n    #pragma omp simd aligned(a2, b2:32) reduction(+:cell_res)\n    for (int i = 0; i < N; i++) {\n        cell_res += a2[i] * b2[i];\n    }\n\n    clock_gettime(CLOCK_REALTIME, &end2);\n    long seconds2 = end2.tv_sec - begin2.tv_sec;\n    long nanoseconds2 = end2.tv_nsec - begin2.tv_nsec;\n    double elapsed2 = seconds2 + nanoseconds2*1e-9;\n    \n    printf(\"time2 (simd reduction): %.9f seconds.\\n\", elapsed2);\n\n    // no speedup compared to without simd (slightly faster than problematic calculation)\n    clock_gettime(CLOCK_REALTIME, &begin3);\n    #pragma omp simd aligned(a2, b2:32)\n    for (int i = 0; i < N; i++) {\n        cell_res += a2[i] * b2[i];\n    }\n    clock_gettime(CLOCK_REALTIME, &end3);\n\n    long seconds3 = end3.tv_sec - begin3.tv_sec;\n    long nanoseconds3 = end3.tv_nsec - begin3.tv_nsec;\n    double elapsed3 = seconds3 + nanoseconds3*1e-9;\n    printf(\"time3 (simd): %.9f seconds.\\n\", elapsed3);\n\n    // no pragma (slightly faster than problematic calculation)\n    clock_gettime(CLOCK_REALTIME, &begin4);\n    for (int i = 0; i < N; i++) {\n        cell_res += a2[i] * b2[i];\n    }\n    clock_gettime(CLOCK_REALTIME, &end4);\n\n    long seconds4 = end4.tv_sec - begin4.tv_sec;\n    long nanoseconds4 = end4.tv_nsec - begin4.tv_nsec;\n    double elapsed4 = seconds4 + nanoseconds4*1e-9;\n    printf(\"time4: %.9f seconds.\\n\", elapsed4);\n\n\n    res_indexed[0][0] = cell_res;\n        }\n    }\n    return res_indexed;\n}\n\n\nint main (int argc, char **argv) {\n    //init a(i,j) = i * j\n    double **a = create_a();\n\n    //init b(i,j) = (i == j) ? 1:0;\n    double **b = create_b();\n\n    //multiply\n    double **res = mul(a,b);\n}\n```\n\n```\nTime simd reduce measured: 0.000004188 seconds. // problematic\ntime2 (simd reduction): 0.000001762 seconds. // faster\ntime3 (simd): 0.000003475 seconds. //slightly faster\ntime4: 0.000003476 seconds. //slightly faster\n```\n\n    ", "Answer": "\r\nI have tested the first two loops on my machine, and I could reproduce the same behavior.\n```\nTime simd reduce measured: 0.000006000 seconds.\ntime2 (simd reduction): 0.000004000 seconds.\n```\n\nMy guess is that there are two issues:\nFirst Problem:\nThe difference among execution times of the multiple version seems to be more related to caching rather than vectorization. So when you tested with the dummy arrays with 3000 elements (24 kilobytes):\n```\ndouble *a2 = (double *)aligned_alloc(32, sizeof(double) * N);\ndouble *b2 = (double *)aligned_alloc(32, sizeof(double) * N);\nfor (int i = 0; i < N ; i++){\n    a2[i] = 1;\n    b2[i] = 1;\n}\n```\n\nThose small arrays are loaded into the cache during their initialization (i.e.,   ```\na2[i] = 1```\n). The matrix ```\na```\n and ```\nb_t```\n, on the other hand, have a size of ```\n3000 x 3000```\n (72 megabytes), which makes it unlikely for the first row to be fully on the cache (although this depends on the size of the caches of your testing environment).\nI change the following loop :\n```\n// problematic calculation that I can't get to speed up no matter what pragma I use\nclock_gettime(CLOCK_REALTIME, &begin);\n#pragma omp simd aligned(a, b_t:32) reduction(+:cell_res)\nfor (int i = 0; i < N; i++) {\n    cell_res += a[0][i] * b_t[0][i];\n}\nclock_gettime(CLOCK_REALTIME, &end);\n```\n\nby packing the first row of the matrices ```\na```\n and ```\nb_t```\n, respectively, into two new matrices ```\na_2```\n and ```\nb_2```\n, namely:\n```\nfor(int i = 0; i < N; i++){\n      a_2[0][i]  = a[0][i];\n      bt_2[0][i] = b_t[0][i];\n }\n\n// problematic calculation that I can't get to speed up no matter what pragma I use\nclock_gettime(CLOCK_REALTIME, &begin);\n#pragma omp simd aligned(a_2, bt_2) reduction(+:cell_res)\nfor (int i = 0; i < N; i++) {\n    cell_res += a_2[0][i] * bt_2[0][i];\n}\n```\n\nBy packing those matrices into smaller matrices with only one row, I can load those matrices into the cache, and consequently, reducing the execution time of the first version. The new results:\n```\nTime simd reduce measured: 0.000004000 seconds.\ntime2 (simd reduction): 0.000004000 seconds.\n```\n\nIMO you should not have tested all those loops in the same function because the compiler might optimize those loops differently, then there is the problem of caching those values, and so on.  I would have tested them in separate runs.\n\nNow my question is what is my problem that is hindering the SIMD\nspeedup my only guess is that it must be the two-dimensionality of the\narray but I don't fully get why that would cause a slowdown.\n\nI also tested with packing directly the first row of the matrices ```\na```\n and ```\nb_t```\n into separate 1D arrays (instead of a matrix) but the results were exactly the same. Take for what it is worth. Now you should profile in your environment, namely testing the cache misses.\nMore importantly, test this version :\n```\nclock_gettime(CLOCK_REALTIME, &begin);\nfor (int i = 0; i < N; i++) {\n    cell_res += a[0][i] * b_t[0][i];\n}\nclock_gettime(CLOCK_REALTIME, &end);\n```\n\nwith and without ```\n#pragma omp simd aligned(a_2, bt_2:32) reduction(+:cell_res)```\n, and with and without the packing to the array, but tests all those versions, separately. Moreover, test them for different input sizes.\nSecond Problem:\nAnother problematic thing is that:\n```\nfor (int i = 0; i < N; i++) {\n        cell_res += a[0][i] * b_t[0][i];\n} \n```\n\nis memory-bound, so there is less opportunity for gains with ```\nSIMD```\n, one should not expect much gains with ```\nSIMD```\n from a double-precision floating dot product. A workaround is to change the matrices from double to floats, consequently, reducing to half the memory bandwidth necessary, and doubling the number of ```\nSIMD```\n operations that you can perform. Nonetheless, the aforementioned code snippet will still be memory-bound. Notwithstanding, you might achieve some gains, mainly for when the values are in cache.\nIn my machine, changing from double to floats, made the ```\nSIMD```\n version noticeable faster than the version without it, even without using the packing. This might also be issue that you have.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Rainbow Tables - Hash Reduction - Java\r\n                \r\nCan you give me some advice on a good reduction function for rainbow table hashes to implement in java?\nThe reduction functions I saw in C++ mostly used unsigned 64bit integer what Java isn't capable of natively now and I'd like to find another solution. (Should be available at Java 8.)\n\nAlternatively: If someone knows a library in Java that supports the - at least not inefficient - usage (and maybe cration) of rainbow tables, please tell me. Would be great not to implement that on my own.\n\nSadly I'm bound to Java as a platform so I can not use the good C++ implementations on the market.\n    ", "Answer": "\r\nIf the algorithm you used consists of additions, subtractions, multiplications and bitwise operations, it is an easy matter to translate it to work with Java ```\nlong```\n instead of unsigned.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP offloading to Nvidia wrong reduction\r\n                \r\nI am interested in offloading work to the GPU with OpenMP. \n\nThe code below gives the correct value of ```\nsum```\n on the CPU\n\n```\n//g++ -O3 -Wall foo.cpp -fopenmp\n#pragma omp parallel for reduction(+:sum)                                                                                                                                    \nfor(int i = 0 ; i < 2000000000; i++) sum += i%11;\n```\n\n\nIt also works on the GPU with OpenACC like this\n\n```\n//g++ -O3 -Wall foo.cpp -fopenacc   \n#pragma acc parallel loop reduction(+:sum)                                                                                                                                    \nfor(int i = 0 ; i < 2000000000; i++) sum += i%11;\n```\n\n\n```\nnvprof```\n shows that it runs on the GPU and it's also faster than OpenMP on the CPU.\n\nHowever when I try to offload to the GPU with OpenMP like this\n\n```\n//g++ -O3 -Wall foo.cpp -fopenmp -fno-stack-protector\n#pragma omp target teams distribute parallel for reduction(+:sum)\nfor(int i = 0 ; i < 2000000000; i++) sum += i%11;\n```\n\n\nit gets the wrong result for ```\nsum```\n (it just returns zero). ```\nnvprof```\n seems to show that it runs on the GPU but it's much slower than OpenMP on the CPU.\n\nWhy is the reduction failing with OpenMP on the GPU?\n\nHere is the full code I used to test this\n\n```\n#include <stdio.h>\n//g++ -O3 -Wall acc2.cpp -fopenmp -fno-stack-protector                                                                                                                           \n//sudo nvprof ./a.out                                                                                                                                                            \nint main (void) {\n  int sum = 0;\n  //#pragma omp parallel for reduction(+:sum)                                                                                                                                    \n  //#pragma acc parallel loop reduction(+:sum)                                                                                                                                   \n  #pragma omp target teams distribute parallel for reduction(+:sum)\n  for(int i = 0 ; i < 2000000000; i++) {\n    sum += i%11;\n  }\n  printf(\"sum = %d\\n\",sum);\n  return 0;\n}\n```\n\n\nUsing GCC 7.2.0, Ubuntu 17.10, along with gcc-offload-nvptx\n    ", "Answer": "\r\nThe solution was to add the clause ```\nmap(tofrom:sum)```\n like this:\n\n```\n//g++ -O3 -Wall foo.cpp -fopenmp -fno-stack-protector\n#pragma omp target teams distribute parallel for reduction(+:sum) map(tofrom:sum)\nfor(int i = 0 ; i < 2000000000; i++) sum += i%11;\n```\n\n\nThis gets the correct result for ```\nsum```\n however the code is still much slower than with OpenACC or OpenMP without ```\ntarget```\n.  \n\nUpdate: the solution to the speed was to add the ```\nsimd```\n clause. See the end of this answer for more information.\n\n\n\nThe solution above has a lot of clauses on one line. It can be broken up like this:\n\n```\n#pragma omp target data map(tofrom: sum)\n#pragma omp target teams distribute parallel for reduction(+:sum)\nfor(int i = 0 ; i < 2000000000; i++) sum += i%11;\n```\n\n\n\n\nAnother option is to use ```\ndefaultmap(tofrom:scalar)```\n\n\n```\n#pragma omp target teams distribute parallel for reduction(+:sum) defaultmap(tofrom:scalar)\n```\n\n\nApparently, scalar variables in OpenMP 4.5 are ```\nfirstprivate```\n by default.\nhttps://developers.redhat.com/blog/2016/03/22/what-is-new-in-openmp-4-5-3/\n\n```\ndefaultmap(tofrom:scalar)```\n is convenient if you have multiple scalar values you want shared.\n\n\n\nI also implemented the reduction manually to see if I could speed it up.  I have not managed to speed it up but here is the code anyway (there are other optimizations I have tried but none of them have helped).\n\n```\n#include <omp.h>\n#include <stdio.h>\n\n//g++ -O3 -Wall acc2.cpp -fopenmp -fno-stack-protector\n//sudo nvprof ./a.out\n\nstatic inline int foo(int a, int b, int c) {\n  return a > b ? (a/c)*b + (a%c)*b/c : (b/c)*a + (b%c)*a/c;\n}\n\nint main (void) {\n  int nteams = 0, nthreads = 0;\n\n  #pragma omp target teams map(tofrom: nteams) map(tofrom:nthreads)\n  {\n    nteams = omp_get_num_teams();\n    #pragma omp parallel\n    #pragma omp single\n    nthreads = omp_get_num_threads();\n  }\n  int N = 2000000000;\n  int sum = 0;\n\n  #pragma omp declare target(foo)  \n\n  #pragma omp target teams map(tofrom: sum)\n  {\n    int nteams = omp_get_num_teams();\n    int iteam = omp_get_team_num();\n    int start  = foo(iteam+0, N, nteams);\n    int finish = foo(iteam+1, N, nteams);    \n    int n2 = finish - start;\n    #pragma omp parallel\n    {\n      int sum_team = 0;\n      int ithread = omp_get_thread_num();\n      int nthreads = omp_get_num_threads();\n      int start2  = foo(ithread+0, n2, nthreads) + start;\n      int finish2 = foo(ithread+1, n2, nthreads) + start;\n      for(int i=start2; i<finish2; i++) sum_team += i%11;\n      #pragma omp atomic\n      sum += sum_team;\n    }   \n  }   \n\n  printf(\"devices %d\\n\", omp_get_num_devices());\n  printf(\"default device %d\\n\", omp_get_default_device());\n  printf(\"device id %d\\n\", omp_get_initial_device());\n  printf(\"nteams %d\\n\", nteams);\n  printf(\"nthreads per team %d\\n\", nthreads);\n  printf(\"total threads %d\\n\", nteams*nthreads);\n  printf(\"sum %d\\n\", sum);\n  return 0;\n}\n```\n\n\n```\nnvprof```\n shows that most of the time is spend with ```\ncuCtxSynchronize```\n. With OpenACC it's about half of that.\n\n\n\nI finally managed to dramatically speed up the reduction. The solution was to add the ```\nsimd```\n clause\n\n```\n#pragma omp target teams distribute parallel for simd reduction(+:sum) map(tofrom:sum).\n```\n\n\nThat's nine clauses on one line. A slightly shorter solution is\n\n```\n#pragma omp target map(tofrom:sum)\n#pragma omp teams distribute parallel for simd reduction(+:sum)\n```\n\n\nThe times are\n\n```\nOMP_GPU    0.25 s\nACC        0.47 s\nOMP_CPU    0.64 s\n```\n\n\nOpenMP on the GPU now is much faster than OpenACC and OpenMP on the CPU . I don't know if OpenACC can be sped up with with some additional clauses.  \n\nHopefully, Ubuntu 18.04 fixes ```\ngcc-offload-nvptx```\n so that it does not need ```\n-fno-stack-protector```\n.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Beta Reduction steps on Y combinator\r\n                \r\nI am new to studying Lambda Calculus as part of my CompSci degree. In the course material (this is not a graded assignment no worries!) the following beta reduction came up:\n𝜆𝑓.𝑊𝑊 →𝛽 𝜆𝑓.𝑓(𝑊𝑊) →𝛽 𝜆𝑓.𝑓(𝑓(𝑊𝑊)) →𝛽 𝜆𝑓.𝑓(𝑓(𝑓(𝑊𝑊)))\nI am slightly confused as I didn't think we could just add something in a beta reduction step. Anyone can explain this a little to me or give me hints on why this behaves that way? Any help is appreciated. Thank you!\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "c# byte[] leftshift for an output reduction\r\n                \r\nI'm trying to left shift bits to create a type of \"reduction\"\nexample:\nbyte[] key =>  00000001 00000001 00000000 00000001 00000000 00000000 00000001 00000001\nthe wanted output => 11010011\nI'm trying\n```\nfor (int i = key.Length - 1; i >= 0; i--){\n\n  key[i] = (byte)((key[i] << 7) & ((i == 0) ? 0 : key[i - 1] >> 0));\n\n}\n```\n\n    ", "Answer": "\r\nThis work, ty @jdweng\n```\nbyte[] input = { 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1 }; \nbyte[] output = input.Select((x,i) => new { x = x, i = i}).GroupBy(x => x.i/ 8) .Select(x => (byte)x.Select((y, i) => y.x << (7 - i)).Sum()).ToArray(); \n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Autoencoders for Dimensionality reduction of mixed data\r\n                \r\nThe task is to use Autoencoders as a dimensionality reduction technique, but I have data of continuous and categorical(nominal and ordinal) type and I am confused as to how to apply the autoencoders on this type of data? Which Loss function should I choose?\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Using an equivalence in the context to force reduction\r\n                \r\nThe setting for this question is the same \"merge of sorted lists\" example from this earlier question. \n\n```\n{-# OPTIONS --sized-types #-}\n\nopen import Relation.Binary\nopen import Relation.Binary.PropositionalEquality as P hiding (trans)\n\nmodule ListMerge\n   {𝒂 ℓ}\n   (A : Set 𝒂)\n   {_<_ : Rel A ℓ}\n   (isStrictTotalOrder : IsStrictTotalOrder _≡_ _<_) where\n\n   open import Data.Product\n   open import Data.Unit\n   open import Level\n   open import Size\n\n   data SortedList (l u : A) : {ι : Size} → Set (𝒂 ⊔ ℓ) where\n      [] : {ι : _} → .(l < u) → SortedList l u {↑ ι}\n      _∷[_]_ : {ι : _} (x : A) → .(l < x) → (xs : SortedList x u {ι}) →\n               SortedList l u {↑ ι}\n```\n\n\nAs before, I'm using sized types so that Agda can determine that the following ```\nmerge```\n function terminates:\n\n```\n   open IsStrictTotalOrder isStrictTotalOrder\n\n   merge : ∀ {l u} → {ι : _} → SortedList l u {ι} →\n                      {ι′ : _} → SortedList l u {ι′} → SortedList l u\n   merge xs ([] _) = xs\n   merge ([] _) ys = ys\n   merge (x ∷[ l<x ] xs) (y ∷[ l<y ] ys) with compare x y\n   ... | tri< _ _ _ = x ∷[ l<x ] (merge xs (y ∷[ _ ] ys))\n   merge (x ∷[ l<x ] xs) (.x ∷[ _ ] ys) | tri≈ _ P.refl _ =\n      x ∷[ l<x ] (merge xs ys)\n   ... | tri> _ _ _ = y ∷[ l<y ] (merge (x ∷[ _ ] xs) ys)\n```\n\n\nWhat I'm trying to do is prove the following associativity theorem:\n\n```\n   assoc : ∀ {l u} → {ι₁ : _} → (x : SortedList l u {ι₁}) →\n                     {ι₂ : _} → (y : SortedList l u {ι₂}) →\n                     {ι₃ : _} → (z : SortedList l u {ι₃}) →\n           merge (merge x y) z ≡ merge x (merge y z)\n```\n\n\nThe cases where at least one list is ```\n[]```\n follow easily by definition, but I'll include them for completeness.\n\n```\n   assoc ([] _) ([] _) ([] _) = P.refl\n   assoc ([] _) ([] _) (_ ∷[ _ ] _) = P.refl\n   assoc ([] _) (_ ∷[ _ ] _) ([] _) = P.refl\n   assoc (_ ∷[ _ ] _) ([] _) ([] _) = P.refl\n   assoc ([] _) (y ∷[ _ ] _) (z ∷[ _ ] _) with compare y z\n   assoc ([] _) (y ∷[ _ ] ys) (.y ∷[ _ ] zs) | tri≈ _ P.refl _ = P.refl\n   ... | tri< _ _ _ = P.refl\n   ... | tri> _ _ _ = P.refl\n   assoc (x ∷[ _ ] _) ([] _) (z ∷[ _ ] _) with compare x z\n   assoc (x ∷[ _ ] xs) ([] _) (.x ∷[ _ ] zs) | tri≈ _ P.refl _ = P.refl\n   ... | tri< _ _ _ = P.refl\n   ... | tri> _ _ _ = P.refl\n   assoc (x ∷[ _ ] _) (y ∷[ _ ] _) ([] _) with compare x y\n   assoc (x ∷[ _ ] xs) (.x ∷[ _ ] ys) ([] _) | tri≈ _ P.refl _ = P.refl\n   ... | tri< _ _ _ = P.refl\n   ... | tri> _ _ _ = P.refl\n```\n\n\nHowever, I'm getting stuck trying to prove the remaining case, which has many sub-cases. In particular, I don't know how to \"reuse\" facts such as ```\ncompare x y ≡ tri< .a .¬b .¬c```\n inside proof contexts beneath the top level (without, say, introducing an auxiliary lemma). \n\nI'm aware of, and have had some success with, the ```\ninspect```\n (on steroids) idiom mentioned here, but my problem seems to be that the context in which I want to \"reuse\" the relevant fact isn't yet established when I use ```\nrewrite```\n to simplify with the equality I've saved using ```\ninspect```\n.\n\nSo for example in the following sub-case, I can capture the values of ```\ncompare x y```\n and ```\ncompare y z```\n using the following ```\ninspect```\n calls:\n\n```\n   assoc (x ∷[ _ ] _) (y ∷[ _ ] _) (z ∷[ _ ] _)\n         with compare x y | compare y z\n         | P.inspect (hide (compare x) y) unit\n         | P.inspect (hide (compare y) z) unit\n```\n\n\nand then ```\nrewrite```\n to simplify:\n\n```\n   assoc {l} {u} (x ∷[ l<x ] xs) (y ∷[ _ ] ys) (.y ∷[ _ ] zs)\n         | tri< _ _ _ | tri≈ _ P.refl _ | P.[ eq ] | P.[ eq′ ] rewrite eq | eq′ =\n```\n\n\nBut I think the ```\nrewrite```\n will only affect the goal which is active at that point. In particular, if in the body of the proof I use ```\ncong```\n to shift into a nested context which permits more reduction, I may expose new occurrences of those comparisons which will not have been rewritten. (See the ```\n{!!}```\n below for the location I mean.) My understanding of exactly how reduction proceeds is a little hazy, so I'd welcome any correction or clarification on this.\n\n```\n      begin\n         x ∷[ _ ] merge (merge xs (y ∷[ _ ] ys)) (y ∷[ _ ] zs)\n      ≡⟨ P.cong (λ xs → x ∷[ l<x ] xs) (assoc xs (y ∷[ _ ] ys) (y ∷[ _ ] zs)) ⟩\n         x ∷[ _ ] merge xs (merge (y ∷[ _ ] ys) (y ∷[ _ ] zs))\n      ≡⟨ P.cong (λ xs′ → x ∷[ _ ] merge xs xs′)\n                {merge (y ∷[ _ ] ys) (y ∷[ _ ] zs)} {y ∷[ _ ] merge ys zs} {!!} ⟩\n         x ∷[ _ ] merge xs (y ∷[ _ ] merge ys zs)\n      ∎ where open import Relation.Binary.EqReasoning (P.setoid (SortedList l u))\n```\n\n\n(The implicit arguments to ```\ncong```\n need to be made explicit here.) \n\nWhen I put the cursor in the hole, I can see that the goal (somewhat paraphrased) is \n\n```\nmerge (y ∷[ _ ] ys) (y ∷[ _ ] zs) | compare y y ≡ y ∷[ _ ] merge ys zs\n```\n\n\ndespite the earlier ```\nrewrite eq′```\n. Moreover in my context I have\n\n```\neq′  : compare y y ≡ tri≈ .¬a refl .¬c\n```\n\n\nwhich seems to be exactly what I need to allow reduction to make progress so that ```\nrefl```\n will complete the proof-case.\n\nHere's the placeholder for the remaining sub-cases.\n\n```\n   assoc (x ∷[ _ ] _) (_ ∷[ _ ] _) (z ∷[ _ ] _) | _ | _ | _ | _ = {!!}\n```\n\n\nI'm a little unconfident here; I don't know whether I'm misusing ```\ninspect```\n on steroids, going about the proof the wrong way entirely, or just being stupid.\n\nIs there a way to use the ```\neq′```\n equivalence from the context to allow reduction to proceed?\n    ", "Answer": "\r\nYes, as per Vitus' comment, one needs to pattern-match again on the outcome of the comparison. I ended up defining 3 helper lemmas, one for each branch of the trichotomy, and then using each lemma twice in the final proof.\n\n```\nmerge≡ : ∀ {x l u} (l<x : l < x) {ι₁ : _} (xs : SortedList x u {ι₁}) {ι₂ : _} (ys : SortedList x u {ι₂}) →\n         merge (x ∷[ l<x ] xs) (x ∷[ l<x ] ys) ≡ x ∷[ l<x ] merge xs ys\nmerge≡ {x} _ _ _ with compare x x\nmerge≡ _ _ _ | tri< _ x≢x _ = ⊥-elim (x≢x refl)\nmerge≡ _ _ _ | tri≈ _ refl _ = refl\nmerge≡ _ _ _ | tri> _ x≢x _ = ⊥-elim (x≢x refl)\n\nmerge< : ∀ {x y l u} (l<x : l < x) (l<y : l < y) (x<y : x < y)\n         {ι₁ : _} (xs : SortedList x u {ι₁}) {ι₂ : _} (ys : SortedList y u {ι₂}) →\n         merge (x ∷[ l<x ] xs) (y ∷[ l<y ] ys) ≡ x ∷[ l<x ] merge xs (y ∷[ x<y ] ys)\nmerge< {x} {y} _ _ _ _ _ with compare x y\nmerge< _ _ _ _ _ | tri< _ _ _ = refl\nmerge< _ _ x<y _ _ | tri≈ x≮y _ _ = ⊥-elim (x≮y x<y)\nmerge< _ _ x<y _ _ | tri> x≮y _ _ = ⊥-elim (x≮y x<y)\n\nmerge> : ∀ {x y l u} (l<x : l < x) (l<y : l < y) (y<x : y < x)\n         {ι₁ : _} (xs : SortedList x u {ι₁}) {ι₂ : _} (ys : SortedList y u {ι₂}) →\n         merge (x ∷[ l<x ] xs) (y ∷[ l<y ] ys) ≡ y ∷[ l<y ] merge (x ∷[ y<x ] xs) ys\nmerge> {x} {y} _ _ _ _ _ with compare x y\nmerge> _ _ y<x _ _ | tri< _ _ y≮x = ⊥-elim (y≮x y<x)\nmerge> _ _ y<x _ _ | tri≈ _ _ y≮x = ⊥-elim (y≮x y<x)\nmerge> _ _ _ _ _ | tri> _ _ _ = refl\n```\n\n\nStill, this is an unsatisfying amount of boilerplate; I'm guessing (with little first-hand experience) that Coq would fare better.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "MATLAB: PCA for Dimensionality Reduction\r\n                \r\nI have computed colour descriptors of a dataset of images and generated a 152×320 matrix (152 samples and 320 features). I would like to use PCA  to reduce the dimensionality of my image descriptors space. I know that I could implement this using Matlab PCA built-in function but as I have just started learning about this concept I would like to implement the Matlab code without the built-in function so I can have a clear understanding how the function works. I tried to find how to do that online but all I could find is the either the general concept of PCA or the implementation of it with the built-in functions without explaining clearly how it works. Anyone could help me with a step by step instructions or a link that could explain a simple way on how to implement PCA for dimensionality reduction. The reason why I'm so confused is because there are so many uses for PCA and methods to implement it and the more I read about it the more confused I get.\n    ", "Answer": "\r\nPCA is basically taking the dominant eigen vectors of the data (Or better yet their projection of the dominant Eigen Vectors of the covariance matrix).  \n\nWhat you can do is use the SVD (Singular Value Decomposition).  \n\nTo imitate MATLAB's ```\npca()```\n function here what you should do:\n\n\nCenter all features (Each column of your data should have zero mean).\nApply the ```\nsvd()```\n function on your data.\nUse the V Matrix (Its columns) as your vectors to project your data on. Chose the number of columns to use according to the dimension of the data you'd like to have.\n\n\nThe projected data is now you new dimensionality reduction data.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "How to implement user-defined reduction with OpenACC?\r\n                \r\nIs there a way to implement a user-defined reduction with OpenACC similar to declare reduction in OpenMP?\n\nSo that I could write something like\n```\n#pragma acc loop reduction(my_function:my_result)```\n\n\nOr what would be the appropriate way to implement efficient reduction without the predefined operators? \n    ", "Answer": "\r\nUser defined reductions aren't yet part of the OpenACC standard.  While I'm not part of the OpenACC technical committee, I believe they have received requests for this but not sure if it's something being considered for the 3.0 standard.\n\nSince the OpenACC standard is largely user driven, I'd suggest you send a note to the OpenACC folks requesting this support.  The more folks that request it, the more likely it is to be adopted in the standard.\n\nContact info for OpenACC can be found at the bottom of https://www.openacc.org/about\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Beta reduction Lambda calculus example\r\n                \r\n(λw. w) (λu. λv. u) (λu. λv. v) (λu. λv. u)\n\nCan anyone give me a step by step Beta reduction of this example?\nI'm really confused\n    ", "Answer": "\r\n```\n(λw. w) (λu. λv. u) (λu. λv. v) (λu. λv. u)```\n\n\nApply (λw. w) to (λu. λv. u). As this is the identity function, nothing changes.\n\n```\n(λu. λv. u) (λu. λv. v) (λu. λv. u)```\n\n\nApply (λu. λv. u) to (λu. λv. v). It is substituted for u in the expression. Since we now might get confused by the multiple v's in our first expression, we can alpha rename the v's in the inner expression to another variable, x. \n\n```\n(λv. (λu. λx. x)) (λu. λv. u)```\n\n\nApply (λv. (λu. λx. x)) to (λu. λv. u). Since v does not appear in the lambda's body, the argument is effectively discarded.\n\n```\n(λu. λx. x)```\n\n\nWe can now (optionally) alpha rename again to restore the original variable names.\n\n```\n(λu. λv. v)```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMP argmin reduction for multiple values\r\n                \r\nI have a routine that uses a loop to compute the minimum height of a particle given a surface of particles beneath. This routine tries random positions and compute the minimum height and then returns the ```\nx, y, z```\n values, where ```\nz```\n is the minimum height found. \n\nThis routine can be parallelized with ```\nomp parallel for```\n. But I am having problems figuring out how to get the triplet ```\n(x, y, z)```\n, not just the minimum ```\nz```\n (because the minimum ```\nz```\n of course corresponds to a given ```\nx, y```\n coordinates). I can actually get the smallest ```\nz```\n by using a reduction operation as follows\n\n```\ndouble x = 0, y = 0, z = 1.0e300; // initially z is large\n#pragma omp parallel for reduction(min:z)\nfor(int trial = 0; trial < NTRIALS; ++trial) {\n    // long routine that, at the end, computes x, y, z \n    // and selects only the x, y, z corresponding to the \n    // smallest z\n}\n```\n\n\nBut I cannot get the corresponding ```\nx```\n and ```\ny```\n. At the end I just end up with a random ```\nx```\n and ```\ny```\n written by one of the threads. \n\nIs it possible to get also those values? How? I was thinking on having an array where each thread stores their value of ```\nx, y, zmin```\n and then, after the reduction operation, compare each thread ```\nzmin```\n with the reduced global value and then get those that correspond to the choosing one. Is there a better way in the sense that OpenMP does it so I don't need to define this dynamic array and compare floats?\n    ", "Answer": "\r\n\n\nYou can implement an argmin for multiple values using user defined reduction (available since OpenMP 4.0). For that you have to put the triple in one type. It is helpful to define a convenience function.\n\n```\nstruct xyz {\n    double x; double y; double z;\n}\n\nstruct xyz xyz_min2(struct xyz a, struct xyz b) {\n    return a.z < b.z ? a : b; \n}\n\n#pragma omp declare reduction(xyz_min: struct xyz: omp_out=xyz_min2(omp_out, omp_in))\\\n    initializer(omp_priv={0, 0, DBL_MAX})\n\nstruct xyz value = {0, 0, DBL_MAX};\n#pragma omp parallel for reduction(xyz_min:value)\nfor (int trial = 0; trial < NTRIALS; ++trial) {\n    struct xyz new_value = ...;\n    value = xyz_min2(value, new_value);\n}\n```\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "MPI reduction on user defined communicator\r\n                \r\nCurrently I am working on an MPI code in Fortran. After using ```\nmpi_cart_create```\n then ```\nmpi_group_excl```\n to create a new group with half of the nodes in it, I am trying to perform a reduction using this communicator but I am obviously doing something wrong.\n\nWith the code\n\n```\ncall MPI_cart_create(MPI_comm_world, 2, dims, (/.false.,.false./), reorder, comm_cart, ierr)\nif (ierr/=0) stop 'Error with MPI_cart_create'\n\ncall MPI_group_excl(group_world, dims(2), excl_a, division_comm_a, ierr)\nif (ierr/=0) stop 'Error with MPI_group_excl - division_comm_a'\n\ncall MPI_group_excl(group_world, dims(2), excl_b, division_comm_b, ierr)\nif (ierr/=0) stop 'Error with MPI_group_excl - division_comm_b'\n\nif (div_a_rank .gt. 0) then\n    call MPI_reduce(division_a(1), division_a(1), L_outer_y, MPI_DOUBLE_PRECISION, MPI_SUM, &\n    & 0, division_comm_a, ierr)\n    if (ierr/=0) stop 'Error with MPI_reduce on division_comm_a'\nend if\n```\n\n\nthe error I am getting is :\n\n```\n*** An error occurred in MPI_Reduce\n*** on communicator MPI_COMM_WORLD\n*** MPI_ERR_ARG: invalid argument of some other kind\n*** MPI_ERRORS_ARE_FATAL: your MPI job will now abort\n```\n\n\nFollowing an answer I have used ```\nMPI_comm_create_group```\n, however I am still getting\n\n```\n*** An error occurred in MPI_Reduce\n*** reported by process [140046521663489,140045998620672]\n*** on communicator MPI_COMM_WORLD\n*** MPI_ERR_ARG: invalid argument of some other kind\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n```\n\n    ", "Answer": "\r\nThe problem is that you're mixing groups and communicators. In MPI, a group is just a logical collection of processes. It can't be used to communicate.\n\nIf you want to create a new communicator from your new group, you should use the function ```\nMPI_COMM_CREATE_GROUP```\n. You can pass your new group into that function to create a new communicator that you can use for your reduction.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Reduction procedure for MethodReference Expressions\r\n                \r\nConsider the following article from JLS that describes the reduction procedure for the MethodReference expressions during Type Inference process-:\n\nA constraint formula of the form ‹MethodReference → T›, where T mentions at least one inference variable, is reduced as follows:\n...\nOtherwise, if the method reference is exact (§15.13.1), then let P1, ..., Pn be the parameter types of the function type of T, and let F1, ..., Fk be the parameter types of the potentially applicable method. The constraint reduces to a new set of constraints, as follows:\n– In the special case where n = k+1, the parameter of type P1 is to act as the target reference of the invocation. The method reference expression necessarily has the form ReferenceType :: [TypeArguments] Identifier. The constraint reduces to ‹P1 <: ReferenceType› and, for all i (2 ≤ i ≤ n), ‹Pi → Fi-1›.\nIn all other cases, n = k, and the constraint reduces to, for all i (1 ≤ i ≤ n), ‹Pi → Fi›.\n\nHere I would like to know some example cases on how the formation of the following are assumed and fitted in the clauses:\n\nGeneric Function type\nGeneric Class type (whose method will be used as reference)\nGeneric method type (that will be used as a reference)\n\nFor example - below are my questions based on the interpretation:\n\n```\nP1, ..., Pn```\n be the parameter types of the function type of T, and let ```\nF1, ..., Fk```\n be the parameter types of the potentially applicable method.\nHere I am assuming that the formation takes shape like:\n\n```\nFunctionType<P1,P2 ... Pn> funType = ClassCls::MethodRef;\n...\nclass ClassCls {\n...\n public static <F1, ..., Fk> RetType potentiallyApplicbMethod(...){}\n}\n```\n\n\nnow when we take the case of ```\nn=k+1```\n\n\nWhy will FunctionType have more number of arguments than the ```\npotentiallyApplicbMethod```\n?\nWhy is ```\nP1 is to act as the target reference of the invocation```\n? I guess here P1 is assumed to be the return type (```\nRetType```\n) of the ```\npotentiallyApplicbMethod```\n?\n\n\nfor the case when ```\nn=k```\n\n\nI assume this is given as when all types specified in FunctionType (may or may not include the return type) matches exactly with all the types specified for ```\npotentiallyApplicbMethod```\n?\n\n\n\n    ", "Answer": "\r\n\nWhy will ```\nFunctionType```\n have more number of arguments than the ```\npotentiallyApplicbMethod```\n?\n\nThere are different forms of method references. One of the forms is ```\nReferenceType::InstanceMethod```\n, or as the Java Tutorials call it, \"reference to an instance method of an arbitrary object of a particular type\".\nExample:\n```\npublic class Main {\n\n    public static void main(String[] args) {\n        // this is a \"reference to an instance method of an arbitrary object of a particular type\"\n        BiConsumer<Main, String> bar = Main::foo;\n    }\n\n    private <T> void foo(T t) {\n\n    }\n}\n```\n\nClearly, the function type of ```\nBiConsumer```\n has 2 parameters, but ```\nfoo```\n only has one. The extra first parameter of the function type is needed because you need an instance to call ```\nfoo```\n, and we haven't specified that instance in the method reference ```\nMain::foo```\n. ```\nbar.accept(new Main(), \"foo\")```\n would be the same as calling ```\nnew Main().foo(\"foo\")```\n. This is what the JLS meant by \"target reference\" - it's the object on which you call the method. It has nothing to do with return types.\nNow you should also be able to see why the JLS says that the method reference in the n=k+1 case must be of the form ```\nReferenceType :: [TypeArguments] Identifier```\n, as opposed to, say, ```\nPrimary :: [TypeArguments] Identifier```\n.\n\nfor the case when n=k I assume this is given as when all types specified in ```\nFunctionType```\n matches exactly with all the types specified for potentiallyApplicbMethod?\n\nNo, this case just refers to the other kinds of method references:\n\nReference to a static method\nReference to an instance method of a particular object\nReference to a constructor of a class or array\n\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenCL shared memory reduction correctness\r\n                \r\nWhile playing with OpenCL, I encountered a bug that I can't explain. Below is a reduction algorithm simply adapted to GPU like accelerators. You can see two versions of the reduction algorithm. V0 uses shared memory. V1 uses the work_group_reduce_<> feature of OpenCL 2.0.\nV0 fails when I use a work group larger than 64. Note that the shared memory array contains 128 ```\nunsigned int```\n and I change this capacity to fit the size of the work group.\nThe V1 code works as expected for various work group sizes, at least powers of two and > 64.\nV0 fails for work group sizes > 64. For example, the code below expects to run on work groups of 128 work items. Now, if I have the ```\nif(x_local_coordinate < i)```\n check before the shared memory reduction, the code works as expected.\nI am interested to know why V0 does not work as expected when what seems to me to be a redundant check (```\nif(x_local_coordinate < i)```\n) is not used and the kernel is run with a work group size > 64.\nI have looked at similar implementation without understanding why the code below does not work. Maybe it comes from the host but then why does V0 always works.\nThank you.\n```\ntypedef unsigned int Type;\n\nkernel void ReduceBulk(global const Type* restrict buffer,\n                       unsigned long buffer_size,\n                       global Type* restrict reduction,\n                       unsigned long sum_per_thread) {\n    const unsigned long x_coordinate           = get_global_id(0);\n    const unsigned long x_dimension_size       = get_global_size(0);\n    const unsigned long x_local_dimension_size = get_local_size(0);\n    const unsigned long x_local_coordinate     = get_local_id(0);\n\n    Type sum = 0;\n\n    for(unsigned long i = 0; i < sum_per_thread; ++i) {\n        const unsigned long index = x_local_dimension_size * i + x_coordinate; // Coalesced\n\n        // Dont check if out of bound\n        // if(index >= buffer_size) {\n        //     break;\n        // }\n\n        sum += buffer[index];\n    }\n\n    // V0, fails when WGs is > 64.\n\n    local Type a_scratch_buffer[128];\n    a_scratch_buffer[x_local_coordinate] = sum;\n\n    if(x_local_coordinate < (x_local_dimension_size / 2)) {\n        for(unsigned long i = x_local_dimension_size / 2; i != 0; i /= 2) {\n            barrier(CLK_LOCAL_MEM_FENCE);\n\n            // if(x_local_coordinate < i) {\n                // Without this additional check (redundant due to the one before the for loop) we get wrong result when\n                // using WGs > 64\n                a_scratch_buffer[x_local_coordinate] += a_scratch_buffer[x_local_coordinate + i];\n            // }\n        }\n\n        if(x_local_coordinate == 0) {\n            atomic_add(reduction, a_scratch_buffer[0]);\n        }\n    }\n\n    // // V1\n\n    // barrier(0);\n\n    // sum = work_group_reduce_add(sum);\n\n    // if(x_local_coordinate == 0) {\n    //     atomic_add(reduction, sum);\n    // }\n}\n```\n\n    ", "Answer": "\r\nActually, the check ```\nif(x_local_coordinate < i)```\n is really necessary in the loop to avoid the data race.\nLet's assume that we have one work group with the size 8 and let's review the execution of the following code for this work group. Please note that this code is executed in parallel for each work group item:\n```\nif(x_local_coordinate < (x_local_dimension_size / 2)) {\n    for(unsigned long i = x_local_dimension_size / 2; i != 0; i /= 2) {\n        barrier(CLK_LOCAL_MEM_FENCE);\n        a_scratch_buffer[x_local_coordinate] += a_scratch_buffer[x_local_coordinate + i];\n    }\n    ...\n}\n```\n\nFirst of all, the code contains the condition that is true only for first half of work group items. That means that only first four work group items will execute the loop at the first iteration when ```\ni```\n equals ```\n(8 / 2) = 4```\n:\n\nAt the second iteration, ```\ni```\n will equal ```\n((8 / 2) / 2) = 2```\n and the same first four work group items will execute the loop:\n\nAs you can see, the 3th and 4th elements are updated by 3th and 4th work group items and also these elements are read by 1st and 2nd work group items in the work group. So, it leads to the data race.\nAnd let's review the execution of the code with the additional check:\n```\nif(x_local_coordinate < (x_local_dimension_size / 2)) {\n    for(unsigned long i = x_local_dimension_size / 2; i != 0; i /= 2) {\n        barrier(CLK_LOCAL_MEM_FENCE);\n        if(x_local_coordinate < i) {\n            a_scratch_buffer[x_local_coordinate] += a_scratch_buffer[x_local_coordinate + i];\n        }\n    }\n    ...\n}\n```\n\nThe first iteration will be the same, because the outer and the inner conditions will be the same. But at the second iteration, when ```\ni```\n equals 2 the behavior of work group items will be different. In accordance to the additional condition, only two work items (1st and 2nd) will execute the body of the additional if statement:\n\nAnd it will help to avoid the data race.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Lookup table size reduction\r\n                \r\nI have an application in which I have to store a couple of millions of integers, I have to store them in a Look up table, obviously I cannot store such amount of data in memory and in my requirements I am very limited I have to store the data in an embebedded system so I am very limited in the space, so I would like to ask you about recommended methods that I can use for the reduction of the look up table. I cannot use function approximation such as neural networks,  the values needs to be in a table. The range of the integers is not known at the moment. When I say integers I mean a 32 bit value.\nBasically the idea is use some copmpression method to reduce the amount of memory but without losing many precision. This thing needs to run in hardware so the computation overhead cannot be very high.\nIn my algorithm I have to access to one value of the table do some operations with it and after update the value. In the end what I should have is a function which I pass an index to it and then I get a value, and after I have to use another function to write a value in the table.\nI found one called tile coding , this one is based on several look up tables, does anyone know any other method?.\nThanks.\n    ", "Answer": "\r\nI'd look at the types of numbers you need to store and pull out the information that's common for many of them.  For example, if they're tightly clustered, you can take the mean, store it, and store the offsets.  The offsets will have fewer bits than the original numbers.  Or, if they're more or less uniformly distributed, you can store the first number and then store the offset to the next number.\n\nIt would help to know what your key is to look up the numbers.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Dimensional reduction through subspace clustering\r\n                \r\nI am trying to write a framework in Python to compare different Dimensional-Reduction-Algorithms and I'm looking for a tutorial or implementation which uses subspace clustering Algorithms such as TSC, SSC, SSC-OMP for this goal. There is some Code written in Matlab related to this paper, but they are for me a little complicated to understand. I was wondering, if there is any library or implementaion in python for these.\n    ", "Answer": "", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "Looking for a fast reduction algorithm\r\n                \r\nI am looking for data structures and an algorithm for a Python/numpy/numba/C-extension implementation to improve the performance over my current approach to solve the following reduction problem:\nInput\nI have a very large structured (Numpy) array of records in the format ``\n```\niarr = numpy.array(\n    [([entityId, subentityId], subentityValue),\n     ...,\n     ...\n    ], dtype=[('e', '<2u4'), ('r', '<f4')])\n```\n\n\nThere are ```\nm```\n entities (order of millions) and ```\nn```\n subentities (<20).\nThere are no duplicate entity/subentity combinations.\nIt is not known beforehad what ```\nm```\n or ```\nn```\n is.\nThe number of subentities varies from entity to entity but is pre-dominantly 8 or 6 per entity.\nThe array is unordered.\n\nExpected Output\nI need to find the maximum or minimum ```\nsubentityValue```\n per ```\nentityId```\n.\nI don't need to retain the information from which ```\nsubentityId```\n the value came from.\nThe result should be an array of records like this:\n```\noarr = numpy.array(\n    [(entityId, subentityValue),\n     ...,\n     ...\n    ], dtype=[('e', '<u4'), ('r', '<f4')])\n```\n\n\nThe result array does not need to be ordered.\nThe array is created either for max-values or for min-values, hence ```\nentityId```\ns in the array are unique.\n\nEqually well, the output could be a dictionary with ```\nentityId```\ns as keys and max or min ```\nsubentityValues```\n as values.\nCurrent Implementation (slow!)\nMy initial approach using Python, Numpy and Numba was (described here for finding the maximum ```\nsubentityValue```\n per ```\nentitId```\n):\n\nInitialise a dictionary (```\nnumba.typed.Dict```\n) with keys being unique ```\nentityId```\ns and an initial value that is garanteed to be smaller than any ```\nsubentityValue```\n found in the array (for example -99999.9).\n```\nodict = numba.typed.Dict.empty(key_type=nb.int64, value_type=nb.float64)  # types for compatability to Python's dict\n\nsmallest_r = nb.float64(-99999.9)\n\nfor entity_id in np.unique(iarr['e'].astype(np.int64)):\n         odict[entity_id] = smallest_r \n```\n\n\nLoop through the records in the input array and compare the value of ```\ndictionary[entityId]```\n with the record's ```\nentityValue```\n and\na) if ```\ndictionary[entityId]```\n is larger than ```\nentityValue```\n don't do anything,\nb) if ```\ndictionary[entityId]```\n is smaller than ```\nentityValue```\n overwrite it with the ```\nentityValue```\n.\n```\nfor i in numba.prange(iarr['e'].shape[0]):\n    if odict[iarr['e'][i]] < iarr['r'][i]:\n        odict[iarr['e'][i]] = iarr['r'][i]\n```\n\n\nReturn the dictionary ```\nodict```\n as a result.\n\n\nThis works fine but is by far the biggest bottleneck in the system.\nTo improve performance I attempted to parallize this (```\n@numba.jit(..., parallel=True)```\n), only to find out that ```\nnumba```\n's ```\ntyped.Dict```\n is not thread-safe and giving me incorrect results in that case.\n\nI am perfectly happy to ditch my solution completely in favour of something better (faster).\nAny suggestions?\n    ", "Answer": "\r\nTo group source rows (by the first element of e) and then compute\nboth min and max, for each group, it is more covenient to use\nPandas instead of Numpy.\nStart from necessary imports:\n```\nimport numpy as np\nimport pandas as pd\n```\n\nFor the test purpose, I created the source array as:\n```\niarr = np.array([\n    ([10, 1], 10.5), ([10, 1], 9.5), ([10, 1], 10.0),\n    ([10, 2], 9.1),  ([10, 2], 9.2), ([10, 2], 9.4),\n    ([10, 3], 7.5),  ([10, 3], 9.7), ([10, 3], 8.0),\n    ([20, 2], 7.3),  ([20, 2], 7.1), ([20, 2], 8.0),\n    ([20, 3], 7.3),  ([20, 3], 9.7), ([20, 3], 8.0)],\n    dtype=[('e', '<u4', (2,)), ('r', '<f4')])\n```\n\nThe first step is to create a pandasonic Series with:\n\nvalues from r column,\nthe index (actually a MultiIndex) created from e column.\n\nThe code to do it is:\n```\ns = pd.Series(iarr['r'], index=pd.MultiIndex.from_arrays(iarr['e'].T))\n```\n\nThen, to get the result, with both min and max, as a DataFrame, run:\n```\nresult = s.groupby(level=0).agg(['min', 'max'])\n```\n\nThe index (the leftmost, unnamed column) holds entityId and \"actual\"\ncolumns contain both min and max.\nThe result is:\n```\n    min   max\n10  7.5  10.5\n20  7.1   9.7\n```\n\nIf you need, you can convert it to a Numpy array:\n```\noarr = np.core.records.fromarrays(\n    result.reset_index().values.T,\n    names='entityId, min, max', formats='u4, f4, f4')\n```\n\nMy code should operate substantially faster than plain pythonic\nsolution.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
{"Question": "OpenMDAO 1.x relevance reduction\r\n                \r\nI have a component in OpenMDAO without outputs that serves to provide inputs to the rest of the group.  apply_linear in that component is being called despite the fact that the output of it is not connected.  Shouldn't the relevance reduction algorithm in OpenMDAO 1.x figure out that apply_linear for this method never needs to be called?\n    ", "Answer": "\r\nAs it turns out, relevance reduction on a per-variable basis isn't turned on by default. You can turn it on with:\n\n```\n    prob.root.ln_solver = LinearGaussSeidel()\n    prob.root.ln_solver.options['single_voi_relevance_reduction'] = True\n```\n\n\nThis options is set to False by default because it does use more memory by allocating separate vectors for each quantity of interest (though each vector is smaller because it only contains relevant variables, but the total size may be larger.) Also, relevance-reduction is only applicable when using Linear Gauss Seidel as the top linear solver.\n    ", "Knowledge_point": "Reductions", "Tag": "算法分析"}
