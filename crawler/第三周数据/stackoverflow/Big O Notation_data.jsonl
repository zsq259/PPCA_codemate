{"Question": "What is a plain English explanation of \"Big O\" notation?\r\n                \r\nI'd prefer as little formal definition as possible and simple mathematics.\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What does O(log n) mean exactly?\r\n                \r\nI am learning about Big O Notation running times and amortized times.  I understand the notion of O(n) linear time, meaning that the size of the input affects the growth of the algorithm proportionally...and the same goes for, for example, quadratic time O(n2) etc..even algorithms, such as permutation generators, with O(n!) times, that grow by factorials.\n\nFor example, the following function is O(n) because the algorithm grows in proportion to its input n:\n\n```\nf(int n) {\n  int i;\n  for (i = 0; i < n; ++i)\n    printf(\"%d\", i);\n}\n```\n\n\nSimilarly, if there was a nested loop, the time would be O(n2).\n\nBut what exactly is O(log n)?  For example, what does it mean to say that the height of a complete binary tree is O(log n)?\n\nI do know (maybe not in great detail) what Logarithm is, in the sense that:  log10 100 = 2, but I cannot understand how to identify a function with a logarithmic time.\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Difference between Big-O and Little-O Notation\r\n                \r\nWhat is the difference between Big-O notation ```\nO(n)```\n and Little-O notation ```\no(n)```\n?\n    ", "Answer": "\r\nf ∈ O(g) says, essentially\n\n\n  For at least one choice of a constant k > 0, you can find a constant a such that the inequality 0 <= f(x) <= k g(x) holds for all x > a. \n\n\nNote that O(g) is the set of all functions for which this condition holds.\n\nf ∈ o(g) says, essentially\n\n\n  For every choice of a constant k > 0, you can find a constant a such that the inequality 0 <= f(x) < k g(x) holds for all x > a.\n\n\nOnce again, note that o(g) is a set.\n\nIn Big-O, it is only necessary that you find a particular multiplier k for which the inequality holds beyond some minimum x. \n\nIn Little-o, it must be that there is a minimum x after which the inequality holds no matter how small you make k, as long as it is not negative or zero.\n\nThese both describe upper bounds, although somewhat counter-intuitively, Little-o is the stronger statement. There is a much larger gap between the growth rates of f and g if f ∈ o(g) than if f ∈ O(g). \n\nOne illustration of the disparity is this: f ∈ O(f) is true, but f ∈ o(f) is false. Therefore, Big-O can be read as \"f ∈ O(g) means that f's asymptotic growth is no faster than g's\", whereas \"f ∈ o(g) means that f's asymptotic growth is strictly slower than g's\". It's like ```\n<=```\n versus ```\n<```\n.\n\nMore specifically, if the value of g(x) is a constant multiple of the value of f(x), then f ∈ O(g) is true. This is why you can drop constants when working with big-O notation.\n\nHowever, for f ∈ o(g) to be true, then g must include a higher power of x in its formula, and so the relative separation between f(x) and g(x) must actually get larger as x gets larger.\n\nTo use purely math examples (rather than referring to algorithms):\n\nThe following are true for Big-O, but would not be true if you used little-o:\n\n\nx² ∈ O(x²) \nx²  ∈ O(x² + x)\nx²  ∈ O(200 * x²)\n\n\nThe following are true for little-o:\n\n\nx² ∈ o(x³)\nx² ∈ o(x!)\nln(x) ∈ o(x)\n\n\nNote that if f ∈ o(g), this implies f ∈ O(g). e.g. x² ∈ o(x³) so it is also true that x² ∈ O(x³), (again, think of O as ```\n<=```\n and o as ```\n<```\n)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Determining complexity for recursive functions (Big O notation)\r\n                \r\nI have a Computer Science Midterm tomorrow and I need help determining the complexity of these recursive functions. I know how to solve simple cases, but I am still trying to learn how to solve these harder cases. These were just a few of the example problems that I could not figure out. Any help would be much appreciated and would greatly help in my studies, thank you!\n```\nint recursiveFun1(int n)\n{\n    if (n <= 0)\n        return 1;\n    else\n        return 1 + recursiveFun1(n-1);\n}\n\nint recursiveFun2(int n)\n{\n    if (n <= 0)\n        return 1;\n    else\n        return 1 + recursiveFun2(n-5);\n}\n\nint recursiveFun3(int n)\n{\n    if (n <= 0)\n        return 1;\n    else\n        return 1 + recursiveFun3(n/5);\n}\n\nvoid recursiveFun4(int n, int m, int o)\n{\n    if (n <= 0)\n    {\n        printf(\"%d, %d\\n\",m, o);\n    }\n    else\n    {\n        recursiveFun4(n-1, m+1, o);\n        recursiveFun4(n-1, m, o+1);\n    }\n}\n\nint recursiveFun5(int n)\n{\n    for (i = 0; i < n; i += 2) {\n        // do something\n    }\n\n    if (n <= 0)\n        return 1;\n    else\n        return 1 + recursiveFun5(n-5);\n}\n```\n\n    ", "Answer": "\r\nThe time complexity, in Big O notation, for each function:\n\n```\nint recursiveFun1(int n)\n{\n    if (n <= 0)\n        return 1;\n    else\n        return 1 + recursiveFun1(n-1);\n}\n```\n\nThis function is being called recursively n times before reaching the base case so its ```\nO(n)```\n, often called linear.\n\n```\nint recursiveFun2(int n)\n{\n    if (n <= 0)\n        return 1;\n    else\n        return 1 + recursiveFun2(n-5);\n}\n```\n\nThis function is called n-5 for each time, so we deduct five from n before calling the function, but n-5 is also ```\nO(n)```\n.\n(Actually called order of n/5 times. And, O(n/5) = O(n) ).\n\n```\nint recursiveFun3(int n)\n{\n    if (n <= 0)\n        return 1;\n    else\n        return 1 + recursiveFun3(n/5);\n}\n```\n\nThis function is log(n) base 5, for every time we divide by 5\nbefore calling the function so its ```\nO(log(n))```\n(base 5), often called logarithmic and most often Big O notation and complexity analysis uses base 2.\n\n```\nvoid recursiveFun4(int n, int m, int o)\n{\n    if (n <= 0)\n    {\n        printf(\"%d, %d\\n\",m, o);\n    }\n    else\n    {\n        recursiveFun4(n-1, m+1, o);\n        recursiveFun4(n-1, m, o+1);\n    }\n}\n```\n\nHere, it's ```\nO(2^n)```\n, or exponential, since each function call calls itself twice unless it has been recursed n times.\n\n```\n\nint recursiveFun5(int n)\n{\n    for (i = 0; i < n; i += 2) {\n        // do something\n    }\n\n    if (n <= 0)\n        return 1;\n    else\n        return 1 + recursiveFun5(n-5);\n}\n```\n\nAnd here the for loop takes n/2 since we're increasing by 2, and the recursion takes n/5 and since the for loop is called recursively, therefore, the time complexity is in\n(n/5) * (n/2) = n^2/10,\ndue to Asymptotic behavior and worst-case scenario considerations or the upper bound that big O is striving for, we are only interested in the largest term so ```\nO(n^2)```\n.\n\nGood luck on your midterms ;)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is Big O Notation? [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 12 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\n\n  Possible Duplicate:\n  Plain English explanation of Big O  \n\n\n\n\nI know Big O notation is used to assess how efficient an algorithm is, but I do not understand how you read Big O notation or what exactly how efficient an algorithm is. Can somebody perhaps explain the basics of Big O notation? Thanks.\n    ", "Answer": "\r\nWhat is a plain English explanation of \"Big O\" notation? is a good explanation of what Big-O notation is and how to use it.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Determining Big O Notation\r\n                \r\nI need help understanding/doing Big O Notation. I understand the purpose of it, I just don't know how to \"determine the complexity given a piece of code\".\n\nDetermine the Big O notation for each of the following\n\na. \n\n```\nn=6;\ncout<<n<<endl;\n```\n\n\nb. \n\n```\nn=16;\nfor (i=0; i<n; i++)\n    cout<<i<<endl;\n```\n\n\nc. \n\n```\ni=6;\nn=23;\nwhile (i<n) {\n    cout<<i-6<<endl;\n    i++;\n}\n```\n\n\nd. \n\n```\nint a[ ] = {1, 3, 5, 7, 9, 11, 13, 15, 17, 19};\nn=10;\nfor (i=0; i<n; i++)\n    a[i]=a[i]*2;\nfor (i=9; i>=0; i--)\n    cout<<a[i]<<endl;\n```\n\n\ne. \n\n```\nsum=0;\nn=6;\nk=pow(2,n);\nfor (i=0;i<k;i++)\n    sum=sum+k;\n```\n\n    ", "Answer": "\r\nBig O indicates the order of the complexity of your algorithm.\n\nBasic things :\n\n\nThis complexity is measured regarding to the entry size\nYou choose a unit operation (usually affectation or comparison)\nYou count how much time this operation is called\nA constant term or constant factor is usually ignored when using complexity so if the number of operation is 3*n^3 + 12 it's simplified to n^3 also marked O(n^3)\n\n\na.) Will just run once, no loop, complexity is trivial here ```\nO(1)```\n\n\nb.) Call n times in the loop: ```\nO(n)```\n\n\nc.) Here we choose to analyze n (because it's usually the incrementing variable in an algorithm).  The number of calls is n - 6 so this is ```\nO(n)```\n.\n\nd.) Let's suppose here that 10 (n) is the size of your array and nine (i) this size minus one.  For each value to n, we have to go from 0 to n then n-1 to 0.  n * (n-1) operations, technically: ```\nO(n * 2)```\n which some people approximate as ```\nO(n)```\n.  Both are called Linear Time, what is different is the slope of the line which BigO doesn't care about.\n\ne.) The loop goes from 0 to the pow(2, n), which is 1 to 2^n, summarized as ```\nO(2^n)```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation and Recursion\r\n                \r\nI am having some trouble working out the Big O notation for these 2 recursive functions:\n\n```\nint calc (int n) \n{\n  if (n <= 0)\n    return 0 ;\n  else if (n > 10) \n    return n ;\n  else\n    return calc (5 + calc(5n));\n}\n```\n\n\nIn the case above I think the Big O notation might be O(n^2) because of the nested iterations in the data set?\n\n```\nboolean method (int k ,int [] arr, int i, int j)\n{\n    if (i > j)\n       return false;\n    if (arr [(i+j)/2] == k)\n       return true;\n    if (arr [(i+j)/2] < k)\n       return method (k, arr, i, ( (i+j)/2) - 1) ;\n    else\n       return method (k, arr, ((i+j)/2)+1, j) ;\n}\n```\n\n\nHere I think the big O notation might be O(log N) because the input data set is halved with each iteration?\n\nI am, however, very new to Big O notation and any help or explanations would be much appreciated!\n    ", "Answer": "\r\nFor ```\ncalc```\n:\n\nThis function will never be called more than 5 times during recursion. It's easy to see from a brief analysis and substituting a few values for ```\nn```\n. Thus it's ```\nO(1)```\n. Hint: the function will get called more times the smaller ```\nn```\n is (above some threshold).\n\nMaybe a bit of a bold statement, but I believe any function (assuming ```\nn```\n is the input / input size) with ```\nif (n > max) return const;```\n has to be ```\nO(1)```\n (just let the \"constant\" be the maximum time taken for ```\nn <= max```\n).\n\nFor ```\nmethod```\n:\n\nYes, it's ```\nO(log n)```\n.\n\nThe function is actually binary search, which is a good thing to know.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation check understanding\r\n                \r\nI want to check my understanding of Big-O notation. If I have code:\n\n```\n for(int bound = 1; bound <= n; bound *= 2){\n      for( int i = 0; i < bound; i++) {\n            for(int j = 0; j < n; j += 2){\n                   .....Code\n            }\n            for(int j = 1; j < n; j *= 2){\n                   ......Code\n            }\n      }\n }\n```\n\n\nis the Big-O notation for this N3?  \n    ", "Answer": "\r\nNot quite. The outer loop increment is ```\nbound *= 2```\n, so that loop is O(log n).  The two inner loops (```\ni```\n and the first ```\nj```\n loop) are both O(n), so when nested they're O(n2). (You can ignore the ```\nj *= 2```\n inner loop because it's faster than the ```\nj += 2```\n loop and won't significantly contribute to the program's run time.)\n\nPut this all together and the whole program is O(log n * n2).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What exactly does big Ө notation represent?\r\n                \r\nI'm really confused about the differences between big O, big Omega, and big Theta notation. \n\nI understand that big O is the upper bound and big Omega is the lower bound, but what exactly does big Ө (theta) represent? \n\nI have read that it means tight bound, but what does that mean?\n    ", "Answer": "\r\nFirst let's understand what big O, big Theta and big Omega are. They are all sets of functions.\nBig O is giving upper asymptotic bound, while big Omega is giving a lower bound. Big Theta gives both.\nEverything that is ```\nӨ(f(n))```\n is also ```\nO(f(n))```\n, but not the other way around.\n```\nT(n)```\n is said to be in ```\nӨ(f(n))```\n if it is both in ```\nO(f(n))```\n and in ```\nOmega(f(n))```\n. In sets terminology, ```\nӨ(f(n))```\n is the intersection of ```\nO(f(n))```\n and ```\nOmega(f(n))```\n\nFor example, merge sort worst case is both ```\nO(n*log(n))```\n and ```\nOmega(n*log(n))```\n - and thus is also ```\nӨ(n*log(n))```\n, but it is also ```\nO(n^2)```\n, since ```\nn^2```\n is asymptotically \"bigger\" than it. However, it is not ```\nӨ(n^2)```\n, Since the algorithm is not ```\nOmega(n^2)```\n.\nA bit deeper mathematic explanation\n```\nO(n)```\n is asymptotic upper bound. If ```\nT(n)```\n is ```\nO(f(n))```\n, it means that from a certain ```\nn0```\n, there is a constant ```\nC```\n such that ```\nT(n) <= C * f(n)```\n. On the other hand,  big-Omega says there is a constant ```\nC2```\n such that ```\nT(n) >= C2 * f(n))```\n).\nDo not confuse!\nNot to be confused with worst, best and average cases analysis: all three (Omega, O, Theta) notation are not related to the best, worst and average cases analysis of algorithms. Each one of these can be applied to each analysis.\nWe usually use it to analyze complexity of algorithms (like the merge sort example above). When we say \"Algorithm A is ```\nO(f(n))```\n\", what we really mean is \"The algorithms complexity under the worst1 case analysis is ```\nO(f(n))```\n\" - meaning - it scales \"similar\" (or formally, not worse than) the function ```\nf(n)```\n.\nWhy we care for the asymptotic bound of an algorithm?\nWell, there are many reasons for it, but I believe the most important of them are:\n\nIt is much harder to determine the exact complexity function, thus we \"compromise\" on the big-O/big-Theta notations, which are informative enough theoretically.\nThe exact number of ops is also platform dependent. For example, if we have a vector (list) of 16 numbers. How much ops will it take? The answer is: it depends. Some CPUs allow vector additions, while other don't, so the answer varies between different implementations and different machines, which is an undesired property. The big-O notation however is much more constant between machines and implementations.\n\nTo demonstrate this issue, have a look at the following graphs:\n\nIt is clear that ```\nf(n) = 2*n```\n is \"worse\" than ```\nf(n) = n```\n. But the difference is not quite as drastic as it is from the other function. We can see that ```\nf(n)=logn```\n quickly getting much lower than the other functions, and ```\nf(n) = n^2```\n is quickly getting much higher than the others.\nSo - because of the reasons above, we \"ignore\" the constant factors (2* in the graphs example), and take only the big-O notation.\nIn the above example, ```\nf(n)=n, f(n)=2*n```\n will both be in ```\nO(n)```\n and in ```\nOmega(n)```\n - and thus will also be in ```\nTheta(n)```\n.\nOn the other hand - ```\nf(n)=logn```\n will be in ```\nO(n)```\n (it is \"better\" than ```\nf(n)=n```\n), but will NOT be in ```\nOmega(n)```\n - and thus will also NOT be in ```\nTheta(n)```\n.\nSymmetrically, ```\nf(n)=n^2```\n will be in ```\nOmega(n)```\n, but NOT in ```\nO(n)```\n, and thus - is also NOT ```\nTheta(n)```\n.\n\n1Usually, though not always. when the analysis class (worst, average and best) is missing, we really mean the worst case.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation Help\r\n                \r\n```\nwhile (n >= 1)\n\nn /= 2;\n```\n\n\nI am unable to get the Big-O notation for this\n    ", "Answer": "\r\nI'll just follow Pointy's advice for the sake of exposition.\n\nTry 8.\n\n```\n4 2 1 0: 4 iterations.\n```\n\n\nTry 32.\n\n```\n16 8 4 2 1 0: 6 iterations.\n```\n\n\nTry 66.\n\n```\n33 16 8 4 2 1 0: 7 iterations.\n```\n\n\nSo… how are the initial numbers changing, and how are the numbers of iterations changing?\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is Big O notation? Do you use it? [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        What is a plain English explanation of \"Big O\" notation?\r\n                            \r\n                                (43 answers)\r\n                            \r\n                    \r\n                Closed 9 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nWhat is Big O notation? Do you use it?\nI missed this university class I guess :D\nDoes anyone use it and give some real life examples of where they used it?\n\nSee also:\nBig-O for Eight Year Olds?\nBig O, how do you calculate/approximate it?\nDid you apply computational complexity theory in real life?\n    ", "Answer": "\r\nOne important thing most people forget when talking about Big-O, thus I feel the need to mention that:\n\nYou cannot use Big-O to compare the speed of two algorithms. Big-O only says how much slower an algorithm will get (approximately) if you double the number of items processed, or how much faster it will get if you cut the number in half.\n\nHowever, if you have two entirely different algorithms and one (```\nA```\n) is ```\nO(n^2)```\n and the other one (```\nB```\n) is ```\nO(log n)```\n, it is not said that ```\nA```\n is slower than ```\nB```\n. Actually, with 100 items, ```\nA```\n might be ten times faster than ```\nB```\n. It only says that with 200 items, ```\nA```\n will grow slower by the factor ```\nn^2```\n and ```\nB```\n will grow slower by the factor ```\nlog n```\n. So, if you benchmark both and you know how much time ```\nA```\n takes to process 100 items, and how much time ```\nB```\n needs for the same 100 items, and ```\nA```\n is faster than ```\nB```\n, you can calculate at what amount of items ```\nB```\n will overtake ```\nA```\n in speed (as the speed of ```\nB```\n decreases much slower than the one of ```\nA```\n, it will overtake ```\nA```\n sooner or later—this is for sure).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation (Algorithms)\r\n                \r\nHi I am new to Big O Notation and having trouble specifying big O for the following if someone could kindly explain to me how to work it out please\n```\nint sum=1;\nfor(int count=n; count>0; count/=2) {\n    sum=sum*count;\n}\n```\n\n    ", "Answer": "\r\nAs each time ```\ncount```\n is divided by 2, it will be ```\nTheta(log n)```\n (from ```\nn```\n to ```\n0```\n). Hence, it is in ```\nO(log(n))```\n as well.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "C++ - Big-O Notation\r\n                \r\nFor some reason im unable to solve this.\nwhat will be the Big-o Notation\n\n```\nfor (int i = 0; i < n; i++)\n    for (int j = 0; j < n; j++) {\n        c[i][j] = 0;\n        for (int k = 0; k < n; k++)\n            c[i][j] += a[i][k] * b[k][j];\n    }\n```\n\n    ", "Answer": "\r\n```\nfor (int i = 0; i < n; i++)                  // N times\n    for (int j = 0; j < n; j++) {            // N times (\n        c[i][j] = 0;                         // Constant plus\n        for (int k = 0; k < n; k++)          // N times\n            c[i][j] += a[i][k] * b[k][j];    // Constant\n    }                                        // )\n```\n\n\nOr O(n · n · (1 + n · 1)) which is equivalent to O(n · n · n) or O(n3) after collapsing the constant operations.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation and algorithms\r\n                \r\nI'm currently studying and trying to implement some algorithms. I'm trying to understand Big O notation and I can't figure out the Big O complexity for the algorithm below:\n\n```\nwhile (a != 0 && b != 0)\n{\n    if (a > b)\n        a %= b;\n    else\n        b %= a;\n}\n\nif (a == 0)\n    common=b;\nelse\n    common=a;\n```\n\n    ", "Answer": "\r\nIt's easy to see that after two iterations the least of the numbers becomes at least twice smaller. If it was equal m at the beginning, then after 2K iterations it will be no more than m/2^K. If we put K = [log_2(m)] + 1 here, we'll see that after 2K iterations the least of the numbers becomes zero, and the loop terminates. Hence the number of iterations is no more than 2(log_2 m + 1) = O(log m).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Proving Big O notation\r\n                \r\nProve 5n^2+ 2n -1 = O(n^2).\nThis is what I have attempted so far:\n\n```\n    5n^2 + 2n -1 <= 5n^2 + 2n^2 -n2 for n>1\n    5n^2+ 2n -1 <= 6n^2  for n>1\n    5n^2+ 2n -1 = O(n^2) [ c = 6n^2, n0 = 1 ]\n```\n\n\nIs this the right way of proving Big O notation?\n    ", "Answer": "\r\nTo prove that your expression is ```\nO(n^2)```\n, you need to show that it is bounded by ```\nM*n^2```\n, for some constant ```\nM```\n and some minimum ```\nn```\n value.  By inspection, we can show that your expression is bounded by ```\n10*n^2```\n, for ```\nn=10```\n:\n\nFor ```\nn = 10```\n:\n\n```\n5n^2 + 2n -1 <= 10*n^2\n500 + 20 - 1 <= 1000\n519 <= 1000\n```\n\n\nWe can also show that the expression is bounded by ```\n10*n^2```\n for any value ```\nn```\n greater than 10:\n\nFor ```\nn > 10```\n:\n\n```\n5n^2 + 2n -1 <= 10*n^2\n5*(10+i)^2 + 2*(10+i) -1      <= 10*(10+i)^2\n5*(i^2 + 20i + 100) + 2i + 19 <= 10*(i^2 + 20i + 100)\n2i + 19 <= 5*(i^2 + 20i + 100)\n2i + 19 <= 5i^2 + 100i + 500\n5i^2 + 98i + 481 >= 0, which is true for `i > 0`\n```\n\n\nHere is a link to the Wikipedia article on Big-O notation:\n\nhttps://en.m.wikipedia.org/wiki/Big_O_notation\n\nUpdate:\n\nNote that in practice in order to label your expression ```\nO(n^2)```\n we won't resort to such an ugly written proof.  Instead, we will just recognize that the ```\nn^2```\n term will dominate the expression for large ```\nn```\n, and that the overall behavior will be ```\nO(n^2)```\n.  And your expression is also ```\nO(n^3)```\n (and ```\nO(n^4)```\n, etc.).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation for a For-Loop\r\n                \r\nHow do I find the Big O Notation for this for-loop line of code \n\n```\nfor (int j = 0; pow(j,2) < n; j++) ?```\n \n\nDoes anyone know?\n\nI have read a little on Big O Notation and it’s a very confusing topic to understand. I know that usually for-loop like this one → ```\nfor (int n = 0; n < 20; ++n)```\n, have a Big O notation of O(1), as input increases by 13 so does its output, with linear complexity. Is that the same situation as above?\n    ", "Answer": "\r\nA loop like this:\n\n```\nfor (int i = 0; i < n; ++i) {\n    doSomething(i);\n}\n```\n\n\niterates n times, so if ```\ndoSomething```\n has O(1) running time, then the loop as a whole has O(n) running time.\n\nSimilarly, a loop like this:\n\n```\nfor (int j = 0; pow(j, 2) < n; j++) {\n    doSomething(j);\n}\n```\n\n\niterates ⌈√n⌉ times, so if ```\ndoSomething```\n has O(1) running time, then the loop as a whole has O(√n) running time.\n\nBy the way, note that although ```\npow(j, 2)```\n is O(1) running time — so it doesn't affect your loop's asymptotic complexity — it's nonetheless pretty slow, because it involves logarithms and exponentiation. For most purposes, I'd recommend this instead:\n\n```\nfor (int j = 0; j * j < n; j++) {\n    doSomething(j);\n}\n```\n\n\nor perhaps this:\n\n```\nfor (int j = 0; 1.0 * j * j < n; j++) {\n    doSomething(j);\n}\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "When does Big-O notation fail?\r\n                \r\nWhat are some examples where Big-O notation[1] fails in practice?\n\nThat is to say: when will the Big-O running time of algorithms predict algorithm A to be faster than algorithm B, yet in practice algorithm B is faster when you run it?\n\nSlightly broader: when do theoretical predictions about algorithm performance mismatch observed running times? A non-Big-O prediction might be based on the average/expected number of rotations in a search tree, or the number of comparisons in a sorting algorithm, expressed as a factor times the number of elements.\n\nClarification:\n\nDespite what some of the answers say, the Big-O notation is meant to predict algorithm performance.  That said, it's a flawed tool: it only speaks about asymptotic performance, and it blurs out the constant factors.  It does this for a reason: it's meant to predict algorithmic performance independent of which computer you execute the algorithm on.\n\nWhat I want to know is this: when do the flaws of this tool show themselves?  I've found Big-O notation to be reasonably useful, but far from perfect.  What are the pitfalls, the edge cases, the gotchas?\n\nAn example of what I'm looking for: running Dijkstra's shortest path algorithm with a Fibonacci heap instead of a binary heap, you get O(m + n log n) time versus O((m+n) log n), for n vertices and m edges.  You'd expect a speed increase from the Fibonacci heap sooner or later, yet said speed increase never materialized in my experiments.\n\n(Experimental evidence, without proof, suggests that binary heaps operating on uniformly random edge weights spend O(1) time rather than O(log n) time; that's one big gotcha for the experiments.  Another one that's a bitch to count is the expected number of calls to DecreaseKey).\n\n[1] Really it isn't the notation that fails, but the concepts the notation stands for, and the theoretical approach to predicting algorithm performance.  </anti-pedantry>\n\nOn the accepted answer:\n\nI've accepted an answer to highlight the kind of answers I was hoping for.  Many different answers which are just as good exist :)  What I like about the answer is that it suggests a general rule for when Big-O notation \"fails\" (when cache misses dominate execution time) which might also increase understanding (in some sense I'm not sure how to best express ATM).\n    ", "Answer": "\r\nIt fails in exactly one case: When people try to use it for something it's not meant for.\n\nIt tells you how an algorithm scales. It does not tell you how fast it is.\n\nBig-O notation doesn't tell you which algorithm will be faster in any specific case. It only tells you that for sufficiently large input, one will be faster than the other.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "The uniqueness of big O notation\r\n                \r\nIs big O notation unique? Apparently, by the defintion of big O notation\n```\nf(x) = O(n(x)) if limsup(x->inf) |f(x)|/n(x) = 1\n```\n\nGiven ```\nf(x) = O(n(x))```\n we also have ```\nf(x)=O(n(x)+1)```\n, such ```\nn(x)+1```\n satisfies the definition, too.\nThe problem is, does there exist a set of all ```\nn(x)```\n such that for any ```\nf(x)```\n there exists and exists only one ```\nn(x)```\n in the set such that ```\nf(x)=O(n(x))```\n?\nFor example, ```\nS = {n | n(x) = x^a log(x)^b, (a,b) are both integers}```\n\nThanks in advance.\n    ", "Answer": "\r\nAs you say, for any growth order O(f(n)) there exists an infinity of variants, such as O(f(n)+g(n)) where g(n)=o(f(n)). Presumably, these form equivalence classes and you can choose a \"canonical\" member from each to ensure uniqueness.\nIn any case, the number of classes is uncountably infinite, just for the fact that O(n^α) exist for all real α, and I don't see how in practice you could specify these canonical members.\nThe question is slightly different if you consider the order of growth of the running times of algorithms rather than general functions, and/or the order functions having a closed-from expression, as the algorithms and the formulas are countable.\n\nQuiz:\nWhat is the canonical representation of the order of growth of f(n) if f(n)^f(n²)=n ?\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation data structures\r\n                \r\nI have a doubt related to Big O Notation\nfor the equation\n\n```\nf(n)=3n+8\n```\n\n\nhow do we find the upper bound and how do we find big O notation\nThe solution given is\n\n```\n3n+8<=4n```\n and ```\nn>8```\n, where ```\nc=4```\n and ```\nn_0=8```\n\n\nI am not sure how this solution was found. Can you please explain how this solution was reached?\n    ", "Answer": "\r\nMathematical definition of Big-Oh notation from CLRS:\n\n```\nO(g(n)) = { f(n) : there exists positive constants c and n_0 such that 0 <= f(n) <= c.g(n) for all n >= n_0 }\n```\n\n\nLet ```\nf(n)=3n+8```\n.\n\nThen, we have to find positive constants ```\nc```\n and ```\nn_0```\n such that ```\n0 <= 3n+8 <= c.g(n) for all n >= n_0```\n.\n\nLet ```\ng(n)=n```\n. (you can try it out for various functions if you want)\n\nThen, ```\n0 <= 3n+8 <= cn for all n >= n_0```\n.\n\nThis expression will hold true only when the value of ```\nc```\n is greater than ```\n3```\n and value of ```\nn```\n is greater than or equal to ```\n8```\n. If this is not the case, then this linear inequality will fail. For instance:\n\n\nif ```\nc = 3```\n, then we will get ```\n0 <= 3n+8 <= 3n```\n which is false.\nif ```\nc = 4```\n and ```\nn_0 = 7```\n, then we will get ```\n0 <= 3n+8 <= 4n```\n which implies ```\n0 <= 3.7+8 <= 4.7```\n which gives ```\n0 <= 29 <= 28```\n which again, is false.\n\n\nSo, we can conclude that ```\nf(n) = O(n) for c = 4 and n_0 = 8```\n.\n\nThis is a great resource if you wish to learn more.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation algebra\r\n                \r\nI have a couple of questions regarding some algebra using big O notation:\n\nif ```\nf(n)=O(g(n))```\n\n\nis ```\nlog(f(n)) = O(log(g(n)))```\n?\n\nis ```\nN^{f(n)}=O(N^{g(n)})```\n? (where N is any real number)\n    ", "Answer": "\r\n\nIs ```\nlog(f(n)) = O(log(g(n)))```\n ? No, it is not essential, for example:\n\n```\nf(n)= n```\n  and  ```\ng(n) = n^2```\n. Here ```\nf(n) = O(g(n))```\n\nIs ```\nN^{f(n)}=O(N^{g(n)})```\n ? No, this is also not true as \n\n\nfor two algorithms , the ratio may remain constant, but the ratio of each raised to certain power will never be constant. \n\nTake\n\nf\n(\nn\n) = 2\nn\nand\ng\n(\nn\n) =\nn\n.\n\nIt is true that ```\n2n```\n is ```\nO(n)```\n. But consider\n\n\n\nThis limit is not bounded - it goes to infinity as n goes to infinity. So, \n\n```\n2^2n```\n is not ```\nO(2n)```\n i.e. ```\n2f(n)```\n is not ```\nO(2g(n))```\n in this case.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of a constant\r\n                \r\nI calculate my runtime complexity to be 4, what is the Big O notation of this?  \n\nFor example if my runtime complexity is 4 + n then its Big O = O(n). \n    ", "Answer": "\r\nLet's look loosely at the definition of what we mean by ```\nf(n) is in O(g(n))```\n:\n\n\n  ```\nf(n)```\n is in ```\nO(g(n))```\n means that ```\nc · g(n)```\n is an upper bound on ```\nf(n)```\n. Thus there\n  exists some constant ```\nc```\n such that ```\nf(n) ≤ c · g(n)```\n holds for\n  sufficiently large ```\nn```\n (i.e. , ```\nn ≥ n0```\n for some constant ```\nn0```\n).\n\n\nYou can treat a constant function just as any other function, w.r.t. analysing its asymptotic behaviour using e.g. big-O notation.\n\n```\nf(n) = 4\ng(n) = 1\n\nf(n) ≤ c · g(n) = c · 1, for c ≥ 4 and for all n           (*)\n\n     (*) with e.g. n0=0 and c=4 => f(n) is in O(1)\n```\n\n\n\n\nNote: as Ctx notes in the comments below, ```\nO(1)```\n (or e.g. ```\nO(n)```\n) describes a set of functions, so to be fully correct, ```\nf```\n should be described to be in ```\nO(1)```\n (```\nf ∈ O(n)```\n, ```\nf```\n:s set membership in ```\nO(1)```\n), rather than \"```\nf(n)```\n being in ```\nO(1)```\n\". You can, however, probably expect to see the less rigorous version \"```\nf(n)```\n is in ```\nO(1)```\n\" (or some ```\nO(g(n))```\n) just as frequently at the web, at least outside of the scope of scientific articles.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation with recursion\r\n                \r\nI have a question about the Big O notation of this code:\n\n```\nint strange_sumA(int[] arr) {\n     if (arr.length == 1) {\n        return arr[0];\n        } else {\n             int newlen = arr.length/2;\n             int[] arrLeft = new int[newlen];\n             int[] arrRight = new int[newlen];\n         for (int i=0; i<newlen; i++) {\n             arrLeft[i] = arr[i];\n         }\n         for (int i=newlen; i<arr.length-1; i++) {\n             arrRight[i-newlen] = arr[i];\n         }\n         return strange_sumA(arrLeft) + strange_sumA(arrRight);\n    }\n}\n```\n\n\nFrom what I understand, the first for loop is O(n/2) and the second for loop is O(n/2), making the entire first run O(n). Then, after the first recursion, the big o of the next two recursion will still be O(n) since 2[n/2] = n and the next with be too since 4[n/4] = n. So, will the entire big O notation of this algorithm be O(n^2)? I think the code will run N times but I am not sure\n    ", "Answer": "\r\nWhen doing runtime analysis, it is important to think about what it is that you are measuring. For example...It looks like you are summing all of the digits in the array. However, you are not doing it iteratively - You are doing it recursively. So, if your most \"expensive\" operation (step that takes the most time) is a function call...then you may choose to express run time measured in function calls. \n\nSince you are dividing your array in half every time, then it is logarithmic.\n\n```\nO(log n)\n```\n\n\nNow, if you also want to take into account each array operation. \n\n```\narrLeft[i] = arr[i];\n```\n\n\nyou do this O(n/2) operation twice, so O(n), for each function call. So each function call has O(n) array operations.\n\n```\nO(n)\n```\n\n\nFor overall array operations, we must multiply the # of array operations per  function call by the # of total function calls.\n\n```\nO(n * log n) \n```\n\n\nYou can also prove this via the master theorm\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Complexity of algorithms in Big O notation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        What is a plain English explanation of \"Big O\" notation?\r\n                            \r\n                                (43 answers)\r\n                            \r\n                    \r\n                Closed 9 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nWhat is Big O notation and why do we measure complexity of any algorithm in Big O notation?\nAn example will do the good.\n    ", "Answer": "\r\nYou must check wiki\n\n\n  In mathematics, big O notation describes the limiting behavior of a\n  function when the argument tends towards a particular value or\n  infinity, usually in terms of simpler functions. It is a member of a\n  larger family of notations that is called Landau notation,\n  Bachmann–Landau notation (after Edmund Landau and Paul Bachmann), or\n  asymptotic notation. In computer science, big O notation is used to\n  classify algorithms by how they respond (e.g., in their processing\n  time or working space requirements) to changes in input size. In\n  analytic number theory, it is used to estimate the \"error committed\"\n  while replacing the asymptotic size, or asymptotic mean size, of an\n  arithmetical function, by the value, or mean value, it takes at a\n  large finite argument. A famous example is the problem of estimating\n  the remainder term in the prime number theorem.\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation Calculation\r\n                \r\nI'm stuck determining the big o notation for the below fragmented code, the given expression is part of I'm trying to figure out. I know given two plain, default ```\nfor```\n loops results in ```\nO(n^2)```\n but the latter is entirely different. Here are the instructions. \n\nThe algorithm of\n\n```\nfor (j = 0; j < n; j++)\n{\n  for (k = j; k < n; k++)\n  {\n  }\n}\n```\n\n\nwill result in a number of iterations of given by the expression:\n\n```\n= n + (n-1) + (n-2) + (n-3) + ........ + (n - n)\n```\n\n\n\nReduce the above series expression to an algebraic expression, without summation.\nAfter determining the algebraic expression express the performance in Big O Notation.\n\n    ", "Answer": "\r\nYou can use this method (supposedly applied by Gauss when he was a wee lad).\n\nIf you sum all the numbers twice, you have\n\n```\n     1   +   2   +   3   + ... +  n\n+    n   + (n-1) + (n-2) + ... +  1\n—————————————————————————————————————--\n   (n+1) + (n+1) + (n+1) + ... + (n+1)   = n(n+1)\n```\n\n\nThus, \n\n```\n1 + 2 + 3 + ... + n = n(n+1)/2\n```\n\n\nand ```\nn(n+1)/2```\n is ```\n(n^2)/2 + n/2```\n, so it is in ```\nO(n^2)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Confused about Big-O notation\r\n                \r\nI am new to Big-O notation. While reading I came across an example :\n\n```\nQus : Find upper bound for f(n) = n^2 + 1\n\nSol : n^2 + 1 <= 2n^2  for all n >= 1\n\nso f(n) = O(n^2) with c = 2 and n0 = 1\n```\n\n\nafter this I came across second example :\n\n```\nQus : Find upper bound for f(n) = n\n\nSol : n <= n^2 for all n >=1\n\nso f(n) = O(n^2) with c = 1 and n0 = 1\n```\n\n\nI am confused that according to Big-O notation if f(n) = n then f(n) = O(n) so I want to know if the following solution to second example is right ?\n\n```\nSol : n <= 2n for all n >=1\n\nso f(n) = O(n) with c = 2 and n0 = 1\n```\n\n    ", "Answer": "\r\nA complement of those comments:\n\nStart from the definition of Big O\nwe can say that f(x) is O(g(x)) if and only if there exist a positive \nnumber B and a non-negative real number b such that:\n\n```\n|f(x)|is less or equal to B|g(x)| for all numbers x that large than b```\n\n\nTherefore, we could say that for ```\nf(n)=n f(n)=O(n)```\n,\nsince there exists a number B (for instance \"2\") make that ```\nn<=Bn```\n.\n(n<=2n, for all the n=>0)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Database indexes and their Big-O notation\r\n                \r\nI'm trying to understand the performance of database indexes in terms of Big-O notation. Without knowing much about it, I would guess that:\n\n\nQuerying on a primary key or unique index will give you a O(1) lookup time.\nQuerying on a non-unique index will also give a O(1) time, albeit maybe the '1' is slower than for the unique index (?)\nQuerying on a column without an index will give a O(N) lookup time (full table scan).\n\n\nIs this generally correct ? Will querying on a primary key ever give worse performance than O(1) ?  My specific concern is for SQLite, but I'd be interested in knowing to what extent this varies between different databases too. \n    ", "Answer": "\r\nMost relational databases structure indices as B-trees.\n\nIf a table has a clustering index, the data pages are stored as the leaf nodes of the B-tree. Essentially, the clustering index becomes the table.\n\nFor tables w/o a clustering index, the data pages of the table are stored in a heap. Any non-clustered indices are B-trees where the leaf node of the B-tree identifies a particular page in the heap.\n\nThe worst case height of a B-tree is O(log n), and since a search is dependent on height,  B-tree lookups run in something like (on the average)\n\nO(logt n)\n\nwhere t is the minimization factor ( each node must have at least t-1 keys and at most 2*t* -1 keys (e.g., 2*t* children).\n\nThat's the way I understand it.\n\nAnd different database systems, of course, may well use different data structures under the hood.\n\nAnd if the query does not use an index, of course, then the search is an iteration over the heap or B-tree containing the data pages.\n\nSearches are a little cheaper if the index used can satisfy the query; otherwise, a lookaside to fetch the corresponding datapage in memory is required.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Step Complexity in Big O Notation\r\n                \r\nI only ask this question because my instructor hardly taught us about Big O notation or how to find time/step complexities within code.\nGive the step complexity in Big O natation of the following code snippet:\n```\nint Bar(std::vector<int> x, std::vector<int> y) {\n    int answer = 0;\n    // Assume x.size() == y.size()\n    if (x.size() != y.size()) {\n        throw std::runtime_error(“x and y size mismatch”);\n    }\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = 0; j < y.size(); j++) {\n            answer += x.at(i) * y.at(j);\n        } \n    }\n    return answer;\n}\n```\n\nIf anyone can explain Big O notation and how to approach this problem, it would be greatly appreciated.\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation for an Algorithm [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                This question is unlikely to help any future visitors; it is only relevant to a small geographic area, a specific moment in time,  or an extraordinarily narrow situation that is not generally applicable to the worldwide audience of the internet. For help making  this question more broadly applicable, visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI can't solve a problem; can someone help me?\n\nWhat is the Big O notation for the below statement:-\n\n```\nfor (int i=2;i<=n;i=i*4)\n    sum++;\n```\n\n    ", "Answer": "\r\nOnce i grows exponentially, it is O(log(n)).\n\nIf n is 16 times larger, it is expected to run the loop only two more times than it would.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big o notation work\r\n                \r\ni am new in time complexity using Big-O notation \ni have three examples\nand i tries to figure out the Big(o)\n\n the first example is \n\n```\n sum = 0;\n for(i=0; i<m/3; i++){\n System.out.println(“*”);\n for(j=0; j<n; j++)\n for(k=0; k<10; k=k+2)\n sum++;\n }\n```\n\n\nI think this one is O(mn), first loop works m/3 times, second loop works n times, third loops works 10 times \nthen  O(mn)   \n\n the second example is \n\n```\nsum = 100;\nfor(i=0; i<n; i++){\nsum++; \n}\nfor(j=10; j<n; j++)\nfor(k=1; k<m; k*=2)\n sum++;\n```\n\n\nBig-O = O(log(m)), where the number of operations executed being n + ( (n - 10) * log(m) ) \n\n the third example is \n\n```\n  sum = 0;\n  for(i=2; i<n; i++){\n  for(j=3; j<n; j+=2){\n         sum++;\n  }}\n for(k=1; k<m; k*=2)\n sum++;\n```\n\n\nhere i think Big-O = log(m)n^2 ???\n\nis it correct???\n    ", "Answer": "\r\nHere it is:\n\n\nO(m/3 * n * 5) = O(mn * C) = O(mn)\nO(n + (n - 10) * log(m)) = O(n*log(m))\nO((n-2)*(n-3)/2 + log(m)) = O(n2 + log(m)) = O(n2)\n\n\nNext time, please, format your code blocks defined by braces clearer.\nIn 3. O(n2 + log(m)) → O(n2) because f(x) = x2 > g(x) = log(x), when x → +∞.\n\nUPD. Your code (formatted a little bit nicer):\n\n```\nsum = 0;\n// let's go from inner most loop: (n - 3)/2 actions or simpler n/2 or just n\nfor(i=2; i<n; i++) {\n    for(j=3; j<n; j+=2) {\n        sum++;\n    }\n}\n```\n\n\nBecause in big-O constants don't matter, i.e. O(5) = O(1) or O(18 * x5) = O(x5).\nAnd for example that's why ```\nlog```\n in big-O doesn't have base: O(log2x) = O(log(x) / log(2)) = O(log(x) * Const) = O(log(x)), where ```\nlog```\n - is natural logarithm (base is ```\ne```\n)\n\nLet's go again: \n\n```\nsum = 0;    \n// n actions in inner loop n times. So it's O(n^2)\nfor(i=2; i<n; i++) {\n    for(j=3; j<n; j+=2) {\n        sum++;\n    }\n}\n// THEN there're another log(m) actions\nfor(k=1; k<m; k*=2) {\n    sum++;\n}\n```\n\n\nSo we sum it: O(n2 + log(m)).\n\nNow let's take a look to functions x2 and log(x). As you see, x2 grows much faster than log(x). The proof can be achieved by researching the limit of l(x) = log(x) / x2, when x → +∞. It equals zero.\nThat's why in the sum x2 + log(x) the first term dominates. So [x2 + log(x)] / x2 = 1 + o-small(x), i.e. they're equal in terms of complexity. That's why O(n2 + log(m)) = O(n2).\n\nOriginal equation has two different variables it depends on. If they're both independent, it's better to \"count\" them both: O(n2 + log(m)).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation for an Algorithm [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                This question is unlikely to help any future visitors; it is only relevant to a small geographic area, a specific moment in time,  or an extraordinarily narrow situation that is not generally applicable to the worldwide audience of the internet. For help making  this question more broadly applicable, visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI can't solve a problem; can someone help me?\n\nWhat is the Big O notation for the below statement:-\n\n```\nfor (int i=2;i<=n;i=i*4)\n    sum++;\n```\n\n    ", "Answer": "\r\nOnce i grows exponentially, it is O(log(n)).\n\nIf n is 16 times larger, it is expected to run the loop only two more times than it would.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation?\r\n                \r\nI had to create a function for an assignment and I have no idea how to figure out the complexity of it. Could someone please explain the Big O notation of the following function and give a little explanation why (so I know next time). Thank you.\n\n```\npublic static int findRange(int[] thisArray) {\n        int range = 0, max = 0, min = 0;\n        max = thisArray[0];\n        min = thisArray[3];\n        range = max - min;\n        return range;\n```\n\n    ", "Answer": "\r\nPosting as answer cause a lot more information: \n\nYour code is O(1) as mentioned in other comments. What I would like to say is that the code seems incorrect. Not sure exactly what you need to do but you just seem to be returning the difference between the first and 4th element of the array. To find the min and max, you need to loop through the array and find the min and max. That would be O(n) in the best case.\n\nSince this is an assignment, I am not going to write the code but just a guideline on what you need to be doing. \n\n\nInitialize two variables for max and min and set them to lowest and highest possible value. \nLoop through the array and compare each element to see if its greater than max or less than min.\nWhen the loop is finished, max and min will have your actual max and min values. \nUse these variables to calculate and return range. \n\n\nWith this algorithm, you will be looping through the array once and will be ```\nO(n)```\n since the number of iterations will increase as the length of the array increases. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation for two non-nested loops\r\n                \r\nWhat would the Big O notation be for two for loops that aren't nested?\n\nExample:\n\n```\nfor(int i=0; i<n; i++){\n   System.out.println(i);\n}\n\nfor(int j=0; j<n; j++){\n   System.out.println(j);\n}\n```\n\n    ", "Answer": "\r\nLinear\n\n```\nO(n) + O(n) = 2*O(n) = O(n)\n```\n\n\nIt does not matter how many non nested loops do you have (if this number is a constant and does not depends on ```\nn```\n) the complexity would be linear and would equal to the maximum number of iterations in the loop.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation Explain?\r\n                \r\nWhen I take the Algorithm course in Coursera, I met a question about Big-O notation that says O(n2) = O(n). I check some other answers in Stack overflow and some posts said that Big Notation means the \"upper bound\".  Based on this def: could I sai O(n) = O(2^n) because O(n)<= O(2^n)?\nenter image description here\n    ", "Answer": "\r\nIn some cases polynomials are considered pretty much equivalent with regards to anything exponential.\n\nBut most probably 2 was a scalar, O(2*N) is the same as O(N) because constant factors are usually ignored in big O notation.\n\nIn any case, no O(n) and O(2 to the n) are not =, O(n) is however a subset of O(higher order N).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Help with big O notation\r\n                \r\nI've been having some problems trying to grasp the concept of big O notation. So, by definition big O is as follows, ```\nT(n) ∈ O(G(n)) if T(n) <= G(n) * C```\n. \n\nSince the the constant \"C\" can be any integer > 0, wouldn't this following example be true as well?\n\nExample:\n\n```\nn log n ∈ O(log n)\nn log n <= log n * c\n```\n\n\nWhere C is equal to the value of n.\n\nI know that the answer is that ```\nn log n ∉ O(log n)```\n but I don't understand how since C can be any constant.\n\nThanks in advance for your help :D\n    ", "Answer": "\r\nc is just that, a constant.  This means that you can't say \"let c be the value of n\", because you must choose some c first and then allow the comparison to hold for all n.\n\nIn other words, in order for some T(n) to be O(G(n)), there must exist no constant c such that G(n)*c is greater than T(n) for all n.\n\nThus n log n is not O(log n) because, no matter what constant you choose, n > c will cause n log n to be greater than c log n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Calculating Big O Notation for a algorithm [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Big O, how do you calculate/approximate it?\r\n                            \r\n                                (24 answers)\r\n                            \r\n                    \r\n                Closed 5 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI have been asked to explaine about Big O notation and to calculate Big o Notation for an algorithm. I'm done with the  defining part but I'm still wondering how I can calculate it. Can someone help me to calculate the Big O for the below given code?\n\n```\nnew = int (input(\"enter number\" ))\nif new <= 10000:\n    comm=new*2/100\n    print (comm)\nelse :\n    comm= new*5/100\n    print (comm)\n```\n\n    ", "Answer": "\r\nSince there is no loop, it is O(1).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation (on both sides)\r\n                \r\nI need step-by-step prove that: 7 + O(n) = O(n)\n\nI have no idea how to do it when there are Big O on both sides. \n\nI think I understand concept of big O notation but why is there Big O on both sides?\n    ", "Answer": "\r\nThis use of notation does not accord with the formal definition of big O notation, so it's not clear what level of formality the proof needs to be. Also, since the notation is not formal, it takes some interpretation to decide what actually needs to be proved.\n\nI interpret this as asking for a proof that, given any function which is in O(n), if you add 7 to all of the values of that function, then the new function is also in O(n). More formally, for all f in O(n), the function g(n) = f(n) + 7 is also in O(n).\n\nThis can be proved straightforwardly from the definition:\n\n\nWe need to show that, for every f in O(n), the function g(n) = f(n) + 7 is also in O(n).\nSince f is in O(n), there exist constants c and N such that for all n >= N, the value of f(n) <= cN.\nIt follows that g(n) = f(n) + 7 <= cN + 7 <= (c + 7)N for all n >= N, so long as N >= 1.\n\n\nSo given constants c and N proving f is in O(n), the constants (c + 7) and max(N, 1) work for proving that g is in O(n). QED.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation in PAC Learning\r\n                \r\nThis may be a very basic question, but I'm currently going through Andrew Ng's CS229 notes on Learning Theory (specifically PAC Learning). What I see is that the error on a given hypothesis is less than or equal to the error on the best possible hypothesis + an expression inside Big O Notation:\n\n\n\nFrom my understanding, Big O notation has to do with convergence of some function. How do I interpret this Big O notation? As someone without a heavy math background, I don't know if I should allow all the vars d, m, and delta to approach infinity, or just plug in values there and ignore the O\n    ", "Answer": "\r\nYou need a bit more information here to answer the question, but looking at the notes we have:\n\n\n```\nh^```\n: a specific hypothesis drawn from domain H\n```\nh*```\n: the best hypothesis in domain H\n```\nd```\n: the number of parameters used to define ```\nh^```\n and ```\nh*```\n\n```\nm```\n: the number of samples used to learn ```\nh^```\n\n```\ndelta```\n: the bound on probability that our inequality holds\n\n\nBasically what the equation says is that with probability ```\n1 - delta```\n, you can guarantee that prediction error of a hypothesis drawn from a given domain of hypotheses is bounded uniformly by the prediction error of the best hypotheses in this domain as ```\nm```\n grows.\n\nThe interesting thing about this is that it allows you to plan your data collection around what kind of generalization error guarantees you want to achieve. So if you wanted to know the bounds on your error to within 99% probability for an algorithm that is parameterized by 10 parameters dependent on how many data samples you have, you would set ```\ndelta = 0.01```\n, ```\nd = 10```\n, and then calculate the part in the ```\nO(...)```\n as you increase ```\nm```\n from 1 to however many data samples you think is reasonable. Plotting that as you vary ```\nm```\n is one way of determining how many data samples is reasonable, and planning your data collection accordingly.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation mathematical proof [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 9 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI am trying to understand what exactly Big O notation is. I understand it in the literal and practical sense by going through the answers of similar questions in SO. But what the answers don't explain and i fail to understand is the mathematical basis behind it. \nThe formal definition states that \n\n```\n  A function T(n) is the Big-O notation of f(n), if and only if there exist two constants c, n0 > 0 such that\n\n   T(n) <= c * f(n)  for all n >= n0. \n```\n\n\nI understand intuitively to some extent that this equation tries to calculate the upper bound in terms of some function which has higher slope in the sense, something in the higher end to the function f(n). I know my understanding is vague. So can someone please explain the mathmatical basis/representations of Big-O notation. \n    ", "Answer": "\r\nFirst let me just say that if g(n) is a function of n, then O(g(n)) is the set of all functions f(n) such that there exist constants c,N>0 such that f(n) <= cg(n) for all n >= N.\n\nIf one analyzes an algorithm exactly, then one comes up with a function of the input size n, say f(n), which might be some complicated annoying polynomial expression in n (or a complicated expression involving an exponential).\n\nFor example, one might find that an algorithm, given an input size of n, has exactly f(n) = 2n^2 +3n + 1 instructions.\nBut we don't really care about the 3n + 1 part. If n gets very large, the n^2 term of f will dominate the lower-order terms. For instance, for n=100 we already find that 2*100^2 is a lot more significant than 3*100 + 1.\n\nSo, to make this idea more rigorous, we'd like to say that \"f(n) grows at worst like n^2\". In mathematical notation: f(n) is an element of O(n^2). Now as you've stated in your question, to actually prove that f(n) is an element of O(n^2), we need to find constants c and N such that for all n >= N we have f(n) <= cn^2. So if we try c=3, then we get the inequality f(n) <= 3*n^2, and if you play around algebraically then you'll find that for all n >= 5 this holds true. So our c=3 and our N=5. Indeed, f(n) grows at worst like n^2.\n\nNotice however the words \"at worst like\". It would be wrong to say \"f(n) grows like ...\", instead we say \"f(n) grows at worst like...\".\n\nTo give you another example, consider our f(n) = 2n^2 + 3n + 1 again. My claim is that f(n) is not an element of O(n). To actually show that this is true, we need to show that for all c,N>0 there exists an n>=N such that f(n) > cn. Well, f(n) > cn is true if and only if 2n^2 + (3-c)n + 1 > 0, which is true for for sufficiently large n>=N (no matter what N is), of which I'll let you work out the details. This shows that f(n) grows at a worse rate than linear.\n\nYet another example with our f(n) = 2n^2 + 3n + 1: It can be shown that f(n) is an element of O(n^3). Let's do that, shall we? We need to find c,N>0 such that f(n) <= cn^3 for all n>=N. Well, try c=1; then after some algebra you can find that this is true for N=4. This warrants the \"f(n) grows at worst like ...\" sentiment; if you can show that your function f  is in some big-O, then there might a \"better big-O\" for which it is part of.\n\nAs a final exercise, show that O(1) is a proper subset of O(n), which is a proper subset of O(n^2), which is a proper subset of O(n^3), which is a proper subset of O(n^4), ...\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Meaning of Big O notation\r\n                \r\nOur teacher gave us the following definition of Big O notation:\n\n```\nO(f(n)): A function g(n) is in O(f(n)) (“big O of f(n)”) if there exist\nconstants c > 0 and N such that |g(n)| ≤ c |f(n)| for all n > N.\n```\n\n\nI'm trying to tease apart the various components of this definition. First of all, I'm confused by what it means for g(n) to be in O(f(n)). What does in mean?\n\nNext, I'm confused by the overall second portion of the statement. Why does saying that the absolute value of g(n) less than or equal f(n) for all n > N mean anything about Big O Notation?\n\nMy general intuition for what Big O Notation means is that it is a way to describe the runtime of an algorithm. For example, if bubble sort runs in O(n^2) in the worst case, this means that it takes the time of n^2 operations (in this case comparisons) to complete the algorithm. I don't see how this intuition follows from the above definition.\n    ", "Answer": "\r\n\n  First of all, I'm confused by what it means for g(n) to be in O(f(n)). What does in mean?\n\n\nIn this formulation, ```\nO(f(n))```\n is a set of functions.  Thus ```\nO(N)```\n is the set of all functions that are (in simple terms) proportional to N as N tends to infinity.\n\nThe word \"in\" means ... \"is a member of the set\".\n\n\n  Why does saying that the absolute value of g(n) less than or equal f(n) for all n > N mean anything about Big O Notation?\n\n\nIt is the definition.  And besides you have neglected the ```\nc```\n term in your synopsis, and that is an important part of the definition.\n\n\n  My general intuition for what Big O Notation means is that it is a way to describe the runtime of an algorithm. For example, if bubble sort runs in O(n^2) in the worst case, this means that it takes the time of n^2 operations (in this case comparisons) to complete the algorithm. I don't see how this intuition follows from the above definition.\n\n\nYour intuition is incorrect in two respects.\n\n\nFirstly, the real definition of ```\nO(N^2)```\n is NOT that it takes ```\nN^2```\n operations.  it is that it takes proportional to ```\nN^2```\n operations.  That's where the ```\nc```\n comes into it.\nSecondly, it is only proportional to ```\nN^2```\n for large enough values of ```\nN```\n.  Big O notation is not about what happens for small ```\nN```\n.  It is about what happens when the problem size scales up.\n\n\nAlso, as a a comment notes \"proportional\" is not quite the right phraseology here.  It might be more correct to say \"tends towards proportional\" ... but in reality there isn't a simple english description of what is going on here.  The real definition is the mathematical one.\n\nIf you now reread the definition in the light of that, you should see that it fits just nicely.\n\n\n\n(Note that the definitions of Big O, and related measures of complexity can also be expressed in calculus terminology; i.e. using \"limits\".  However, generally speaking the things we are talking about are quantized; i.e. an integer number instructions, an integer number bytes of storage, etc.  Calculus is really about functions involving real numbers.  Hence, you could argue that the formulation above is preferable.  OTOH, a real mathematician would probably see bus-sized holes in this argumentation.)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for triangular numbers?\r\n                \r\nWhat's the correct big O notation for an algorithm that runs in triangular time? Here's an example:\n\n```\nfunc(x):\n  for i in 0..x\n    for j in 0..i\n      do_something(i, j)\n```\n\n\nMy first instinct is ```\nO(n²)```\n, but I'm not entirely sure.\n    ", "Answer": "\r\nYes, N*(N+1)/2, when you drop the constants and lower-order terms, leaves you with N-squared.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Confused with Big O Notation\r\n                \r\nSo I get that the first for loop runs O(n) times, then inside that it runs 3 times, then 3 times again. How do I express this at big O notation though? Then do the 2 print statements matter? How do I add them to my big-o expression? Thanks, really confused and appreciate any help.\n\n```\n    for (int x = 0; x < n; x++) {\n            for (int j = 0; j < 3; j++) {\n                for (int k = 0; k < 3; k++) {\n                    printf(\"%d\", arr[x]);\n                   }\n                printf(\"\\n\");\n            }\n    }\n```\n\n    ", "Answer": "\r\n```\nO(n)```\n is linear time, so any ```\nk * O(n)```\n where ```\nk```\n is a constant (like in your example) is also linear time and is just expressed as ```\nO(n)```\n. Your example has ```\nO(n)```\n time complexity.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Confusion with determining Big-O notation?\r\n                \r\nSo, I really don't get Big O notation. I have been tasked with determining \"O value\" for this code segment.  \n\n```\nfor (int count =1; count < n; count++) // Runs n times, so linear, or O(N)\n    { \n        int count2 = 1;        // Declares an integer, so constant, O(1)\n\n        while (count2 < count) // Here's where I get confused. I recognize that it is a nested loop, but does that make it O(N^2)?\n            {\n                count2 = count2 * 2;   // I would expect this to be constant as well, O(N)\n            }\n    }\n```\n\n    ", "Answer": "\r\n```\nO(f(n))=g(n)\n```\n\n\nThis implies that for some value ```\nk```\n, ```\nf(n)>g(n)```\n where ```\nn>k```\n. This gives the upper bound for the function ```\ng(n)```\n. \n\nWhen you are asked to find ```\nBig O```\n for some code,\n\n1) Try to count the number of computations being performed in terms of ```\nn```\n and thus getting ```\ng(n)```\n.\n\n2) Now try estimating the upper bound function of ```\ng(n)```\n. That will be your answer.\n\nLets apply this procedure to your code.\n\nLets count the number of computations made. The statements ```\ndeclaring```\n and ```\nmultiply by 2```\n take ```\nO(1)```\n time. But these are executed repeatedly. We need to find how many times they are executed.\n\nThe outer loop executes for ```\nn```\n times. Hence the first statement executes for ```\nn```\n times. Now the number of times inner loop gets executed depends on value of ```\nn```\n. For a given value of ```\nn```\n it executes for ```\nlogn```\n times.\n\nNow lets count the total number of computations performed,\n\n```\nlog(1) + log(2) + log(3) +.... log(n) + n\n```\n\n\nNote that the last ```\nn```\n is for the first statement. Simplifying the  above series we get:\n\n```\n= log(1*2*3*...n) + n\n\n= log(n!) + n\n```\n\n\nWe have\n\n```\ng(n)=log(n!) + n\n```\n\n\nLets guess the upper bound for ```\nlog(n!)```\n.\n\nSince, \n\n```\n1.2.3.4...n < n.n.n...(n times)\n```\n\n\nHence,\n\n```\nlog(n!) < log(n^n) for n>1\n```\n\n\nwhich implies\n\n```\nlog(n!) = O(nlogn).\n```\n\n\nIf you want a formal proof for this, check this out. Since ```\nnlogn```\n increases faster than ```\nn```\n , we therefore have:\n\n```\nO(nlogn + n) = O(nlogn)\n```\n\n\nHence your final answer is ```\nO(nlogn)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation and coding\r\n                \r\nI'm struggling to wrap my head around working out big-o notation from code.\n\nI understand the basic steps i.e.\n\n```\nfor (int i = 0; i < n; i++)```\n would be O(n)\n\nAnd that\n\n```\nfor (int i = 0; i < n; i++)\n    for (int j = 0; j < n; j++) \n```\n\n\nwould be O(n2)\n\nI am struggling to understand where or how to calculate the logarithmic values.\n\ni.e.\n\nWould :\n\n```\nfor (int i = 0; i < n * 2; i++)```\n be O(log n) or O(n log n) or O(log 2n) etc\n\nCan someone please demonstrate in code form as to an example and how the notation was formed.\n\nI have researched and keep getting examples where sorting is concerned and the lists are chopped etc, which makes sense in a form but I don't seem to get how to apply that to code as above.\n\nI am new to the whole coding and big-o notation.\n\nI have am familiar with objects, classes, loops, functions, structs, etc.\nI am busy learning c++ as it is part of my course.\nMy text book does not explain logarithmic big-o calculations very well or pretty much at all.\n    ", "Answer": "\r\nOne could represent the code as a recurrence relation:\n\n```\nT(n) = T(n-1)  + 2 * c, where c = the inner part of the code```\n, \n\nwhich we will do ```\n2 * n```\n times.\n\nGiving us solution like:\n```\nT(n) = 2 c n + c_1, where c_1 a constant```\n\n\nAnd since ```\n2 * c```\n is a constant, and the second term, also a constant drops off, we can write:\n\n```\nO(n)```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Confused with Big O Notation\r\n                \r\nSo I get that the first for loop runs O(n) times, then inside that it runs 3 times, then 3 times again. How do I express this at big O notation though? Then do the 2 print statements matter? How do I add them to my big-o expression? Thanks, really confused and appreciate any help.\n\n```\n    for (int x = 0; x < n; x++) {\n            for (int j = 0; j < 3; j++) {\n                for (int k = 0; k < 3; k++) {\n                    printf(\"%d\", arr[x]);\n                   }\n                printf(\"\\n\");\n            }\n    }\n```\n\n    ", "Answer": "\r\n```\nO(n)```\n is linear time, so any ```\nk * O(n)```\n where ```\nk```\n is a constant (like in your example) is also linear time and is just expressed as ```\nO(n)```\n. Your example has ```\nO(n)```\n time complexity.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of a specific method\r\n                \r\nWhat's the Big O notation of the following method and why?\n\n```\npublic int f(int n) {\n  if (n <= 0) {\n    return 0;\n  }\n  return f(n/2) + n;\n}   \n```\n\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation-Please explain O(log2n)\r\n                \r\nI'm having trouble understanding why this piece of code is O(log 2^n) for its Big O Notation:\n\n```\nfor (int i = n; i>=1; i=i/2){\n    sum = i+j;\n}\n```\n\n\nI thought it would be O(n).\n    ", "Answer": "\r\nThis is ```\nO(log_2 n)```\n. Because it will run until ```\nn```\n becomes 1.\n\nAfter k th step suppose the whole things become 1.\n\nSo ```\nn/2^k = 1```\n\n\n```\nk=log_2 n```\n\n\nThe complexity is ```\nO(log_2 n)```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation for complex loop\r\n                \r\nI can not guess the Big-O notation for this loops.\n```\nfor (int j = 2; j < N; j++) {\n    for (int k = 2*j; k <= N; k += j) {\n        nums[k] = false;\n    }\n}\n```\n\nHow to calculate the algorithm complexity for this loop?\n    ", "Answer": "\r\nThe trick for this is Calculus to replace a sum by an integral that we understand.\n```\nfor y from 2 to N:\n    do roughly N/j things\n```\n\nSo we have a sum of:\n```\nN/2 + N/3 + N/4 + ... + N/N\n    = N (1/2 + 1/3 + 1/4 + ... + 1/N)\n```\n\nAnd that sum can be approximated with the integral from ```\n1```\n to ```\nN```\n of ```\n1/x```\n.  Whose integral is ```\nlog(x)```\n.  So, modulo errors, it requires ```\nO(N log(N))```\n steps.\nI waved my hands vigorously.  But I assure you that the rigorous details can be filled in, and \"replace sum by approximate integral\" has a ```\nO(1)```\n error.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Quick question about big-O notation\r\n                \r\nSo let's say we have a function such as 6wn^2 - 6wn + 6w, would the big-o notation be O(n^2) or O(wn^2)? The problem set question asks me to write this function in Big-O form in terms of w and n, so I think it's the second one but I've never seen a big O notation like that involving 2 terms so I'm a little confused.\n    ", "Answer": "\r\nIf a function f(n) is O(g(n)) for some function g(x), it means that f(n) is bounded above by g(n) asymptotically. Basically this means that for large n values, g(n) will be bigger than f(n).\n\n(More formally, we could say that f(n) is O(g(n)) if and only if there exists some N such that g(n)>f(n) for all n>N)\n\n\n\nNow in your case, let f(n) = 6wn^2 - 6wn + 6w.Then f(n) is both O(n^2) and O(wn^2). This is because both are asymptotic upper bounds for f(n). In fact, f(n) is also O(n^2^2^2^2^2^2^2).\n\nHowever the best answer for you to give would likely be that f(n) is O(wn^2), since that includes w, which is what the question asked for.\n\nNote that in practice we usually remove all coefficients and unimportant powers from g(n). The reason is that you get more information about a function if you present a low upper bound as opposed to a high one. For example, if I tell you that my speedy search algorithm is O(n^(1000!)), I'm not telling you very much at all. On the other hand, if I told you it was O(n^2), I'm giving you more information - but both could be correct. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big o notation and dominate term\r\n                \r\nI have a problem in determining the big O-notation and dominate term for fractions and logs.\n\nWhat is the dominate term and big O for ```\n100n / ( 2n + 1 )```\n? \n\nWhat is the difference between ```\nlog( log( n ) )```\n and ```\nlog( n )```\n and which one is faster? \n    ", "Answer": "\r\nFor each polynomial in the equation you are trying to find the order of, scrap all but the highest order term. \n\nFor your example: ```\n100n / (2n + 1)```\n would become ```\n100n / 2n```\n. Which can then be reduced to ```\n50```\n which is a constant, making it order ```\n1```\n.\n\nWhen looking at ```\nlog```\n equations we care about the fact that log is both monotonically increasing and less than linear in order. ```\nlog```\n being monotonically increasing means that log will always be increasing. ```\nlog```\n being of a lesser order than linear means that when combined as ```\nlog(log(n))```\n it will be grow even slower. \n\nGraph of ```\nln(x)```\n vs. ```\nln(ln(x))```\n\n\nAs an example of a monotonic function that is greater than linear in order, you can look at ```\ne^x```\n. Which gets even faster when you combined it with itself.\n\nGraph of ```\ne^x```\n vs. ```\ne^(e^x)```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation in python\r\n                \r\nDoes anyone know of any good resources to learn big o notation? In particular learning how to walk through some code and being able to see that it would be O(N^2) or O(logN)? Preferably something that can tell me why a code like this is equal to O(N log N)\n\n```\ndef complex(numbers):\n    N = len(numbers)\n    result = 0\n    for i in range(N):\n        j = 1\n        while j < N:\n            result += numbers[i]*numbers[j]\n            j = j*2\n    return result \n```\n\n\nThanks! \n    ", "Answer": "\r\nTo start, let me define to you what O(N log N) is. It means, that the program will run at most N log N operations, i.e. it has a upper bound of ~N log N (where N is the size of the input). \n\nNow here, your N is the size of numbers, or your code:\n\n```\nN = len(numbers)\n```\n\n\nNotice that the first for loop runs from 0 to N-1, for a total of N operations. This is where the first N comes from.\n\n-\n\nThen, where does the log N come from? It is from the while loop.\n\nIn the while loop, you keep multiplying 2 to j until j is greater or equal than N. \n\nThis will be completed when we have executed the loop ~log2(N) times, which describes how many times we have to multiply j by 2 to get to N. For example, log2(8) = 3, because we multiply j by 2 three times to get 8:\n\n```\n#ofmult. j      oldj\n  1      2  2 <- 1 * 2\n  2      4  4 <- 2 * 2\n  3      8  8 <- 4 * 2\n```\n\n\nTo better illustrate this, I have added a print statement in your code, for i and j:\n\n```\ndef complex(numbers):\n    N = len(numbers)\n    result = 0\n    for i in range(N):\n        j = 1\n        while j < N:\n            print(str(i) + \" \" + str(j))\n            result += numbers[i]*numbers[j]\n            j = j*2\n    return result \n```\n\n\nWhen this is run:\n\n```\n>>> complex([2,3,5,1,5,3,7,3])\n```\n\n\nThis is what is outputted:\n\n```\n0 1\n0 2\n0 4\n1 1\n1 2\n1 4\n2 1\n2 2\n2 4\n3 1\n3 2\n3 4\n4 1\n4 2\n4 4\n5 1\n5 2\n5 4\n6 1\n6 2\n6 4\n7 1\n7 2\n7 4\n```\n\n\nNotice how our i goes from 0...7 (N times for a total of O(N) ), and the second part, there are always 3 ( log2(N) ) j-outputs for every i.\nSo, the code is O(N log2 N).\n\nAlso, some good websites I would recommend are:\nhttps://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/\n\nAnd, a video from a lecture series from a Stanford professor:\nhttps://www.youtube.com/watch?v=eNsKNfFUqFo\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Matrix Big O Notation\r\n                \r\nI have a problem in calculating the Big O Notation for the code below...I know that hadn't the size of the matrix been known it would be O(n^3), but since this is a 16 x 16 matrix (i.e. the size is known) does it make it O(1)? \n\n```\n#include<stdio.h>\n#include<time.h>\n#include<stdlib.h>\n#define SIZE 16\nint main()\n{\n        float matrix1 [SIZE][SIZE];\n        float matrix2 [SIZE][SIZE];\n        float result  [SIZE][SIZE];\n\n        srand(time(NULL));\n        int s,j,k=0;\n\n        //Generating and displaying matrix 1\n        printf(\"Matrix 1\\n\");\n\n        for(s=0; s<SIZE;s++)\n        {\n            for(j=0;j<SIZE;j++)\n            {\n                matrix1[s][j] = ((float)rand()/RAND_MAX)*10;\n                printf(\"%.3f\\t\" ,matrix1[s][j]);\n            }\n            printf(\"\\n\");\n        }\n\n       //Generating and displaying matrix 2\n       printf(\"\\n\\nMatrix 2\\n\");\n\n       for(s=0; s<SIZE;s++)\n        {\n            for(j=0;j<SIZE;j++)\n            {\n                matrix2[s][j] = ((float)rand()/RAND_MAX)*10;\n                 printf(\"%.3f\\t\" ,matrix2[s][j]);\n            }\n            printf(\"\\n\");\n        }\n\n        //Generating and displaying Result Matrix\n        printf(\"\\n\\nResult Matrix\\n\");\n\n        for(s=0;s<SIZE;s++)\n        {\n            for(j=0;j<SIZE;j++)\n            {\n                float sum=0.0;\n                for(k=0;k<SIZE;k++)\n                {\n                    sum=sum+(matrix1[s][k]*matrix2[k][j]);\n                }\n                result[s][j]=sum;\n                printf(\"%.3f\\t\" ,result[s][j]);\n            }\n\n            printf(\"\\n\");\n        }\n\n    fflush(stdin);\n    getchar();\n    return 0;\n}\n```\n\n    ", "Answer": "\r\nTime complexity is the time taken by an algorithm to run as a function of the length of the string representing the input\nThe length of the matrix given = SIZE * SIZE * SIZE.\nSo the time complexity = O(SIZE * SIZE * SIZE) (assume SIZE = n)\nTime complexity is O(n^3).\nThe problem here is that SIZE is predefined. Hence it becomes a constant\nWe eliminate constants for taking Big-Oh notations. Hence the time complexity will be T(n) = C^3 = O(1) (where C is a constant)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "big O notation of isset operation\r\n                \r\nHaving an array:\n\n```\n$array = (..., 'name' => 'foo', 'age' => 69, 'address' => 'bar');\n```\n\n\nI'll use the operation:\n\n```\nif(isset($array['address'])) { //Do something... }\n```\n\n\nIs the big O notation would be O(n)? How does the isset() works behind the scenes?\n    ", "Answer": "\r\nAs stated by Mark Baker, it's O(1). However, take note that this is only in the average case.\n\nAs a matter of fact, it is still possible for a search operation in hash maps to take O(n) time due to hash collisions no matter how well you implement a hash map (chaining, probing, etc).\n\nNonetheless, that does not mean that hash tables are bad in practice. In fact, Big O is simply something that is often used to describe the worst-case scenario of an algorithm.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What Big O notation do i have here?\r\n                \r\nSo im having a hard time understanding Big O Notations and are looking for some Examples to understand it better. Now lets look at the following code: \n\n```\n`public static void main(String[] args)` {\n\n            int N = 4; \n           int sum = 0;\n\n            for (int i = 1; i < N; i = i*2)\n            {\n                for (int j = 1; j < N; j++)\n                {\n                    sum++;\n                }\n            }\n\nAm i assuming correctly that the Big O Notation here would be: O(N log(N))? Because the first for loop runs log(n) times and the second one does N times? If not: What would be the correct Big O Notation here? \n\n\nAnd another example: \n\n`public static int f(int N){\n\n            if (N<=1)\n            {\n                return 0;\n            }\n            return (2*f(N/2));\n        }`\n```\n\n\nWhat would the Big O notation be here? Is it O(log N)? \n\nAs you can see im guessing a little bit, so if you would have any advice on how to identify the correct Big O Notation i would be really grateful!\n    ", "Answer": "\r\nYou are correct about the first case, and your reasoning is correct.\n\nIndeed, the complexity is O(logn) in the second case. Here is one way to think about it:\n\nIn every step of the recursion, you divide the number by two, until your reach the base case of one. Thus, the number of times this function is called is the number of times you can divide the number by two until you reach one, which is by definition exactly log(n).\nEach time you call the function you perform O(1) operations, and thus the total complexity is O(logn).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation's definition\r\n                \r\nI really want to know the real definition. I have tried to read a book, but couldn't understand it.\n\nO: Big-O notation worst case.\nΘ: Theta notation average case.\nΩ: Omega notation best case.\n\nWhy does Wikipedia represent the speed of algorithms just in Big-O including its average, best and worst cases? How come they did not replace with those formal keywords?\n    ", "Answer": "\r\nO, Θ and Ω do not represent worst, average and best case ; although they have a similar meaning. \n\nBig-O notation f(n) = O(g(n)) means f grows slower than g for large values of n (\"n > n0\" means \"for large values of n\" in this context). This does not mean that g is the worst case: g could be worse than the worst case (quick sort is also O(n!) for instance). For the more complicated algorithms, there is ongoing research to determine the smallest Big-O for their actual complexity: the original author mostly finds a Big-O upper-bound.\n\nΩ notation means the reverse (f grows faster than g), which means it could be better than the best case (all algorithms are Ω(1) for instance). \n\nThere are many algorithms for which there is no single function g such that the complexity is both O(g) and Ω(g). For instance, insertion sort has a Big-O lower bound of O(n²) (meaning you can't find anything smaller than n²) and an Ω upper bound of Ω(n). \n\nOther algorithms do: merge sort is both O(n log n) and Ω(n log n). When this happens, it is written as Θ(n log n), which means that not all algorithms have a Θ-notation complexity (and notably, algorithms with worst cases or best cases do not have one). \n\nTo get rid of worst cases that carry a very low probability, it's fairly common to examine average-case complexity - replacing the standard \"f\" value on the left-hand side by a different function \"favg\" that only takes into account the most probable outcomes. So, for quick sort, f = O(n²) is the best you can get, but favg = O(n log n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "what is the big O notation for (logn + 3n)\r\n                \r\nwhats is the big O notation of this function  f(x) = logn + 3n  i have ridden big o notation but i am confuse in this function so please help me\n    ", "Answer": "\r\nIt is simply O(n).\n\nWhen you have a composite of multiple parts in big O notation which are added, you have to choose the biggest one. In this case it is O(3n), but there is no need to include constants inside parentheses, so we are left with O(n).\n\nhttps://en.wikipedia.org/wiki/Big_O_notation\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for these algorithm complexities\r\n                \r\nI have a few algorithm complexities that I'm not entirely sure of what the Big O notations are for them.\ni) ((n-1)(n-1) * ... * 2 * 1)/2\nii) 56 + 2n + n^2 + 3n^3\niii) 2n(lg n) + 1001\niv) n^2 * n^3 + 2^n\nI believe  ii) and iii) are pretty straightforward with the Big O of ii) being O(n^3) and the Big O of iii) being O(n log n) but let me know if these are wrong.\nIt's mostly i) and iv) I'm a bit confused on. For i) I assumed it followed the same idea as 1+2+3+4+...+n which has a Big O notation of O(n^2) so that's what I put and for iv) I put O(n^5) but I'm not sure if the 2^n affects the Big O notation in this case, I'm not sure what gets priority here or do I just include them both?\nAny help would be much appreciated, I'm not that experienced in Big O notation so any advice would be really helpful as well.\nThanks in advance\n    ", "Answer": "\r\nSince problem i) is multiplying (not adding) the terms from 1 to n, that should be O(n!).\nYou're right on ii) n^3 is the dominant term, so it's O(n^3), and on iii) both constants 2 and 1001 can be ignored leaving you with O(n log n).\nOn iv) you were right to combine the first two terms to get n^5, but even that will eventually be surpassed by the 2^n term, so the answer is O(2^n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "big O notation exercise [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Big O, how do you calculate/approximate it?\r\n                            \r\n                                (24 answers)\r\n                            \r\n                    \r\n                Closed 5 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI'm working on an exercise about Big O notations and I was wondering if there are any experts here who could help me determine the notation for the following code. so far, I am assuming that it is O(N^2). because a for loop is called in another loop. What do you guys think?\n\n```\npublic static Double average(Integer[] values) {\n    Integer sum = 0;\n    for (int i = 0; i < values.length; i++) {\n        sum += values[i];\n    }\n    return sum / values.length;\n}\npublic static IDeque < Integer > slidingAvg(\n        Stack < Integer > values, int width\n) {\n    IDeque < Integer > window = new ArrayDeque < > (width);\n    IDeque < Double > averages = new ArrayDeque < > (values.size());\n    for (int i = 0; i < width; i++) {\n        window.pushFirst(0);\n    }\n    for (int value: values) {\n        window.pullLast();\n        window.pushFirst(value);\n        Integer[] roll = window.toArray(new Integer[0]);\n        Double average = average(roll);\n        averages.push(average);\n    }\n    return averages;\n}\n```\n\n    ", "Answer": "\r\n\n  O(N^2). because a for loop is called in another loop\n\n\nAlmost :) Only, the two loops are not executed the same number of times.\n\nBoth ```\nwindow.toArray(new Integer[0])```\n and ```\naverage(roll)```\n have complexity ```\nO(width)```\n and they are each executed ```\nvalues.length```\n times.\n\nThe overall complexity is thus ```\nO(values.length * width)```\n, which can either become ```\nO(values.length^2)```\n or ```\nO(values.length)```\n depending on how ```\nwidth```\n compares to ```\nvalues.length```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Simplifying Big-o notation\r\n                \r\nI know we can eliminate the lower function in our Big-O notation if it is in addition.\n\nSomething like O(4^n + n^2) will be simplified to O(4^n)\n\nHowever, if it is something like O(4^n * n^2) or O(n * 3^n), how do they get simplified, now that the functions are in multiplication? \n\nPlease do help me understand\n    ", "Answer": "\r\nYou don't simplify the products.\n\nSimplification in the sums occurs because\n\n```\nf(x) + g(x) = f(x) (1 + g(x) / f(x))\n```\n\n\nand for large ```\nx```\n, ```\ng / f```\n becomes insignificant in front of ```\n1```\n.\n\nFor the products ```\nf(x).g(x)```\n, there is no similar rule.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "big O notation of this code\r\n                \r\n```\nint ara(int dizi[], int ilk, int son, int deger) { \n      int indeks;    \n      if ( ilk > son ) \n        return 0; \n\n      indeks = (ilk + son) / 2; \n      if ( dizi[indeks] < deger ) \n        return ara(dizi, indeks+1, son, deger); \n       else if ( dizi[indeks] > deger ) \n        return ara(dizi, ilk, indeks-1, deger); \n       else  \n       return 1; \n} \n\nfor (i=1; i<2*n; i++)   { \n    printf(\"Bir sayi giriniz: \"); \n    scanf(\"%d\", &sayi); \n    sonuc = ara(matrix, 0, n-1, sayi); \n    if ( sonuc == 1 ) \n      printf(\"Found!\\n\"); \n     else \n      printf(\"Not Found!\\n\");  \n}\n```\n\n\nwhat can be the big-O notation of this code? my guess is N*(2^(logN))\n\nI have assigned my hw already! this is just my pre-curiosity!\n    ", "Answer": "\r\n```\nara```\n is a recursive implementation of binary search.  That is O(log n).\n\nIt is called 2n-1 times.  Multiplying the two terms, the program as a whole is O(n log n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Calculate big O notation for prime numbers\r\n                \r\n```\nimport java.util.Scanner;\n/**\nPrints all prime numbers less than a given value\n*/\npublic class PrimeNumberMethod {\n  public static void main(String[] args) {\n    Scanner input = new Scanner(System.in);\n    System.out.println(\"Enter a value for n: \");\n    int n = input.nextInt();\n    System.out.println(\"The prime numbers in the first \" + n + \n     \" numbers are \");\n    printPrimeNumbers(n);\n  }\n\n  public static void printPrimeNumbers(int n) {\n    final int NUMBER_OF_PRIMES_PER_LINE = 10; // Display 10 per line\n    int count = 0; // Count the number of prime numbers\n    int number = 2; // A number to be tested for primeness\n    int runcount = 0;\n    // Repeatedly find prime numbers\n    while (number < n) {\n      runcount++;\n      if (isPrime(number)) {\n        count++; // Increase the count\n\n        if (count % NUMBER_OF_PRIMES_PER_LINE == 0) {\n          // Print the number and advance to the new line\n          System.out.printf(\"%-5s\\n\", number);\n        }\n        else\n          System.out.printf(\"%-5s\", number);\n      }\n\n      // Check if the next number is prime\n      number++;\n    }\n    System.out.println(\"\\n\"+runcount);\n  }\n\n  /** Check whether number is prime */\n  public static boolean isPrime(int number) {\n    for (int divisor = 2; divisor <= number / 2; divisor++) {\n      if (number % divisor == 0) { // If true, number is not prime\n        return false; // number is not a prime\n      }\n    }\n\n    return true; // number is prime\n  }\n}\n```\n\nMy friend and I have been trying to calculate the big O notation for a while now but we're lost. We know the notation of the outer loop is O(n) but what is the inner loop? And what's the big O notation for the program overall, we think it's O(n) because that's the largest loop in the algorithm.\n    ", "Answer": "\r\nFor each round of the loop you check if the number is prime with your isPrime function, which is O(n). Doing that for every round will make your overall complexity O(n^2).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Determining the BIg O notation of this loop\r\n                \r\n```\nfor(i=0;i<n;i+=2) {\n  for(j=1;j<=n;j*=2) {\n      printf(“%d,%d\\n”,i,j);\n    }\n}\n```\n\n\nWhat would be the Big O notation of this loop?\n    ", "Answer": "\r\nThe outer loop will do ```\nn/2```\n iterations, and each inner loop will do ```\nlg_2(n)```\n iterations.\n\nThe overall running time should be ```\nO(n*lgn)```\n (here I use ```\nlg```\n to represent log base 2).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation for two simple recursive functions\r\n                \r\nI have two recursive functions in Python and simply just want to know the Big O Notation for them.  What is the Big O for each these?\n\n```\ndef cost(n):\n    if n == 0:\n        return 1\n    else:\n        return cost(n-1) + cost(n-1)\n\ndef cost(n):\n    if n == 0:\n        return 1\n    else:\n        return 2*cost(n-1)\n```\n\n    ", "Answer": "\r\nLet's use recurrence relations to solve this!  The first function's runtime can be described recursively as\n\n\n  T(0) = 1\n  \n  T(n + 1) = 2T(n) + 1\n\n\nThat is, the base case takes one time unit to complete, and otherwise we make two recursive calls to smaller instances of the problem and do some amount of setup and cleanup work.  Expanding out some terms in this recurrence, we get\n\n\nT(0) = 1\nT(1) = 2T(0) + 1 = 2 + 1 = 3\nT(2) = 2T(1) + 1 = 2 × 3 + 1 = 7\nT(3) = 2T(2) + 1 = 2 × 7 + 1 = 15\n\n\nThis series 1, 3, 7, 15, ... might look familiar, since it's 21 - 1, 22 - 1, 23 - 1, etc.  More generally, we can prove that\n\n\n  T(n) = 2n+1 - 1\n\n\nWe can do this by induction.  As our base case, T(0) = 1 = 21 - 1, so the claim holds for n = 0.  Now assume that for some n that T(n) = 2n+1 - 1.  Then we have that\n\n\n  T(n + 1) = 2T(n) + 1 = 2(2n+1 - 1) + 1 = 2n+2 - 2 + 1 = 2n+2 - 1\n\n\nAnd we're done!  Since this recurrence works out to 2n+1 - 1 = 2(2n) - 1, we have that the runtime is Θ(2n).\n\nThe second function's runtime can be described recursively as\n\n\n  T(0) = 1\n  \n  T(n + 1) = T(n) + 1\n\n\nExpanding out some terms:\n\n\nT(0) = 1\nT(1) = T(0) + 1 = 1 + 1 = 2\nT(2) = T(1) + 1 = 2 + 1 = 3\nT(3) = T(2) + 1 = 3 + 1 = 4\n\n\nThis gives 1, 2, 3, 4, ..., so more generally we might guess that\n\n\n  T(n) = n + 1\n\n\nWe can prove this inductively again.  As a base case, if n = 0, then T(0) = 1 = 0 + 1.  For the inductive step, assume that for some n that T(n) = n + 1.  Then\n\n\n  T(n + 1) = T(n) + 1 = n + 1 + 1 = n + 2\n\n\nAnd we're done!  Since the runtime is n + 1, the runtime is Θ(n).\n\nHope this helps!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation regarding logarithms\r\n                \r\nI got asked an interview question that wanted me to discern the Big-O notation of several logarithmic functions. The functions were as follows:\n\nf(x) = log5(x)\n\nf(x) = log(x5)\n\nf(x) = log(6*log x)\n\nf(x) = log(log x)\n\nI was told that the Big-O for the first and second are not equivalent and the third and fourth are not equivalent after mistakenly guessing the opposite. Can anyone explain why they are not equivalent and what their Big-O are then?\n    ", "Answer": "\r\n\nlog5 x is the same as writing log log log log log x, which is a very slow-growing function of x.\nThis is equivalent to 5 log x (rewriting exponentiation inside the log as multiplication outside), which is equivalent to log x.\nThis is equivalent to log 6 + log log x, which is equivalent to log log x.\nThis is just log log x.\n\n\nSo you have O(log log log log log x), O(log x), O(log log x) and O(log log x), three distinct Big-O classes.\n\nIf your interviewer said 3 and 4 were different, either he was mistaken or you've misremembered the question (happens all the time).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of nested loops\r\n                \r\n```\nfunction compressBoxesTwice(box1, box2) {\n  box1.forEach(function (boxes) {\n    console.log(box1);\n    box2.forEach(function (boxes) {\n      console.log(box2);\n    })\n  })\n}\n\ncompressBoxesTwice();\n```\n\nWhat will be the Big O notation for the above function?\nnested loops are working on different inputs\nI read that if nested llops are working on same input then Big O will be O(n*n). But in my case nested are working on different inputs.\n    ", "Answer": "\r\nIf the outer loop loops through n elements and inner loop loops through m elements, the Big O notation is O(n*m) or O(nm)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "The big O notation\r\n                \r\nI am reading about Big O notation. In the book I have, there is an example in which the complexity of n2 is in class of O(n3). That doesn't seem logical to me because n3 depends on n and it isn't just a plain constant multiplier that we can \"get rid of.\"\n\nPlease explain to me why those two are of the same complexity. I can't find an answer on this forum or any other.\n    ", "Answer": "\r\nBig O determines an upper bound for large values of n. O(n3) is larger than O(n2) and so an n2 program is still O(n3). It's also O(n4), O(*n5), ..., O(ninfinity).\n\nThe reverse is not true, however. An n^3 program is not O(n2). Rather it would be Omega(n2), as Omega determines a lower bound (how much work we have to do at least).\n\nBig O says nothing of this upper bound being \"tight\", it just needs to be higher than the actual complexity. So while an n*n complexity program is bounded by O(n3), that's not a very tight bound. O(n2) is tighter and more informative.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Matrix multiplication Big O notation\r\n                \r\nI have a question regarding best case scenario for this piece of code and worst case scenario in Big O notation. From my point of view, it should be O (n^3) for both cases but some people disagree.\n\n```\npublic int [][] multiply (int [][] A, int \n [][] B, int n) {   \nint [][] C = new int[n][n]; - 1 ( ignored )\nfor(int i=0; i<n; i++) {        - n \n  for(int j=0; j<n; j++) {  - n \n     if(A[i][j ]!=0) {  - 1 ( ignored )\n         for (int k=0; k<n; k++) {   - n\n              C[i][k] += A[i][j]*B[j][k]; - \n        }\n      }\n     }\n   }\n return C; \n```\n\n\n} \n    ", "Answer": "\r\nIt is true that matrix multiplication takes O(n^3) time to run in average and worst cases. For 2 matrices of dimensions m x n and n x p respectively, there is going to be a total of mnp (or n^3 for simplicity) calculations, one for each entry in the resultant matrix. As for the best case, it depends on what your program does when it sees that at least one of the matrices is a zero matrix (all entries in the matrix are 0). If it can spot a zero matrix, then you can short circuit the program. In this case, the running time is going to be O(n^2) (you just scan a couple of n x n matrices at most), which is the best that can be achieved for matrix multiplication. If this optimization is not available, the program will run in O(n^3) in any case. Either way, you can still say that the best case is O(n^3) because of the definition of Big-O notation, but that is not of interest to most people. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Calculating Big O Notation with Recursion\r\n                \r\nI have try to understand the Big O Notation worst case runtime.\nBut I still don't quite understand it.\n\nThis is some code that I wrote recently:\n\n```\ndef g(n):\n    if n==0:\n        return 1\n    elif n==1:\n        return 2\n    else:\n        n_div=n//2\n        n_mod=n%2\n        return g(n_div)*g(n_mod)\n```\n\n\nSo I hope that I'm at least right that: \n\n```\ndef g(n):\n    if n==0:\n        return 1\n```\n\n\nand:\n\n```\nelif n==1:\n    return 2\n```\n\n\nare O(1), so constant. \n\nBut what about the ```\nelse```\n part.\n\nIs it O(n) because it depends on the ```\nn```\n that I choose?\n\nCan anyone explain what that Big O complexity of the ```\nelse```\n part is?\n    ", "Answer": "\r\nWell you've noticed that you can separate your function into 3 individual cases and already identified that the first 2 are O(1). The third is slightly more tricky but you can separate that into two parts as well.\n\nThe recursion clearly occurs from:\n\n```\ng(n//2)*g(n%2)\n```\n\n\nWe can immediately see that ```\nn%2```\n will evaluate either to 0 or 1, which will resolve one of the first 2 cases again, so we can disregard that. Leaving us with ```\ng(n//2)```\n. By rewriting this as a printing loop and examining the output you'll notice something:\n\n```\n>>> n = 37\n>>> while n > 1:\n...     n //= 2\n...     print(n)\n...\n18\n9\n4\n2\n1\n```\n\n\nAs you can see the terms decrease by half each time, and the same occurs in the recursion. This is logarithmic.\n\nAs a result the worst case for this function is O(logn).\n\nYou can learn more about what logn actually means in Big-O notation by looking at this question.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the Big-O Notation of this code? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and  cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened,  visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nWhat is the Big-O Notation of this code ?\n\n```\nfor(int i=0; i<10; ++i) \n    for(int a=0; a<n; ++a){\n        cout << \"*\";\n        cout << endl;\n    }\n```\n\n    ", "Answer": "\r\nThis one is ```\nO(n)```\n: the 10 of the outer loop is just a constant.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Shorten a big O notation\r\n                \r\nI am working for a project of my class and would like some check/help to see if my shortening of Big O notation is right: \n\n```\nn*O(log(n)) + n * O(log((n)) = 2n*O(log(n)) = n*O(log(n))\nn*O(1) + n * O(n) = n*O(n)\n```\n\n\nIs my shortening correct? and are these can be further shortened?\n\nI would really appreciate any help.\n    ", "Answer": "\r\nSince n is O(n), the first one is O(nlogn) and the second one is O(n^2). \n\nThe proof for n being O(n) can be done using the definition of O(n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation Summation confusion\r\n                \r\nI wish to use Big O Notation to convince others about code enhancement - both for efficiency and readability. But I am not sure whether i am wrong.\n\nFrom Big O Notation, I understand that O(n) + O(n) = O(n) (approximate). Meaning, the constant in front is negligible.\n\nHowever, taking an example, let say a for-loop\n\n```\n//Case 1: One For-Loop - O(n)\nfor(var i=0;i<n;i++)\n{\n   doA(i);\n   doB(i);\n}\n```\n\n\nFor better readability, i prefer writing in this way:\n\n```\n//Case 2: Two For-Loop - O(n) + O(n)\nfunction doA(){\n   for(var i=0;i<n;i++){\n     //do logic A\n   }\n}\n\nfunction doB(){\n   for(var i=0;i<n;i++){\n      //do Logic B\n   }\n}\n```\n\n\nSuch that, I can straightaway calling\n\n```\ndoA();\ndoB();\n```\n\n\n...without the for-loop outside.\n\nNow the problem is, i cannot convince myself, IF that for-loop actually takes 10 seconds. Then O(n) + O(n) is actually 20 seconds. How can we say that O(n) + O(n) is approximate-able to O(n)\n    ", "Answer": "\r\n\n  How can we say that O(n) + O(n) is approximate-able to O(n)\n\n\nFor your (university) concerns, there are four types of complexity:\n\n\nlogarithmical ( O(log(n)) )\nlinear ( O(n) )                 \npolynomial ( O(n^k, k elemOf {1,2,3,...}) ) \nexponential ( O(n^n) )         \n\n\nYour question refers to the second case:  \n\nIn Infinity ```\nO(n) + O(n)```\n is ```\nO(n)```\n, because both are linear.\nSo you can think of it as\n\n```\nlinear + linear = linear\n```\n\n\nLinear means that if you double the amount of input, the processing time doubles.  \n\nIn the other, slower, cases the amount of processing time will be \n\n\nn^2, n^3, ... in the case of polynomial complexity (Doubling the amount of input results in input^2, input^3 or input^4 processing time) \nn^n (which is really big/slow) ... in the case of exponential complexity (Multiplying the amount of input by itself results in a processing time multiplied by itself, e.g. 3*3 = 9, 4*4 = 16, 5*5 = 25...).\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Unable to udnerstand Big O notation examples\r\n                \r\nHow they are calculating Big O notation? \n\nBig O notation examples:\n\n\nQuestion:\nFind upper bound for f(n) = n⁴ + 100n² + 50\n\nSolution:\nn⁴ + 100n² + 50 <= 2n⁴, for all n >= 11\nn⁴ + 100n² + 50 = O(n⁴) with c = 2 and n0 = 11\n\nI am unable to understand - how they are calculating Big O notation?\nHow come c = 2 and n0 = 11\nQuestion:\nFind upper bound for f(n) = 2n³ - 2n²\n\nSolution:\n2n³ - 2n² <= 2n³ , for all n >= 1\n2n³ - 2n² = O(2n³) with c = 2 and n0 =1  \n\nHow come n >= 1?\nQuestion:\nf(n) = n\n\nSolution :\nn <= n , for all n >= 1\nN = O(n) with c = 1 and n0 = 1\n\n    ", "Answer": "\r\nQuestion 1:\n\nDefinition of big O notation is (note that the wikipedia definition uses ```\nx0```\n and ```\nM```\n, there is no real difference, just different signs for the same things).\n\nf(n) is said to be in O(g(n)) if there are constants ```\nn0```\n and ```\nc```\n such that for all ```\nn>n0```\n - ```\nf(n) <= c*g(n)```\n.\n\nIn your example, since ```\nn^4+100n^2+50 <= 2n^4```\n for all ```\nn>11```\n, it is basically same as saying the above with ```\nc=2,n0=11```\n - thus, by definition of big O notation, you could say ```\nn^4+100n^2+50```\n is in ```\nO(n^4)```\n\n\nQuestion 2:\n\nIn here, ```\nn>=1```\n because the claim is true for all positive values of ```\nn```\n - note that ```\n2n^3-2n^2```\n will always be smaller than ```\n2n^3```\n - for all positive values of ```\nn```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to prove Big O notation\r\n                \r\nIn my algorithm class we are discussing big O notation and I am stuck proving this example problem:\n\nProve ```\nf(n) = 3n lg n + 10n + lg n + 20 = O(n lg n)```\n\n\nDetails will be appreciated.\n    ", "Answer": "\r\nAll you need to prove is that for some M and X0:\nM n lg n >= 3n lg n +10n + lg n + 20 for all n greater than X0\n\n4 is pretty easy for M\n\nI'm sure you can compute some x0 for which the above inequality holds and then easily show that it remains true for all n greater than X0\n\nIt helps to simplify the above after substituting in the 4 to\n(n-1)lg n >= 10n + 20  \n\nOnce any n is big enough, it should be clear that lg n > 1, so any increase in n beyond that increase the right by 1 and the left by more than 1.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the big deal about Big-O notation in computer science?\r\n                \r\nHow would Big-O notation help in my day-to-day C# programming?  Is it just an academic exercise?\n    ", "Answer": "\r\nBig-O tells you the complexity of an algorithm in terms of the size of its inputs.  This is essential if you want to know how algorithms will scale.  If you're designing a big website and you have a lot of users, the time it takes you to handle those requests is important.  If you have lots of data and you want to store it in a structure, you need to know how to do that efficiently if you're going to write something that doesn't take a million years to run.\n\nIt's not that Big-O notation itself will help you.  It's that if you understand Big-O notation, you understand the worst-case complexity of algorithms.  Essentially, Big-O gives you a high-level sense of which algorithms are fast, which are slow, and what the tradeoffs are.  I don't see how you can understand the performance implications of anything in, say, the .NET collections library if you don't understand this.\n\nI won't go into more detail here, since this question has been asked many times, but suffice it to say that this is something you should understand.  Here's a fairly highly voted previous Big-O question to get you started.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Computational complexity in Big-O notation\r\n                \r\nHow can I specify computational complexity of an algorithm in Big-O notation whose execution follows the following pattern, according to input sizes? \n\n```\nInput size: 4\nNumber of execution steps: 4 + 3 + 2 + 1 = 10\n\nInput size: 5\nNumber of execution steps: 5 + 4 + 3 + 2 + 1 = 15\n\nInput size: 6\nNumber of execution steps: 6 + 5 + 4 + 3 + 2 + 1 = 21\n```\n\n    ", "Answer": "\r\nTechnically, you have not given enough information to determine the complexity. On the information so far, it could take 21 steps for all input sizes greater than 5. In that case, it would be O(1) regardless of the behavior for sizes 4 and 5. Complexity is about limiting behavior for large input sizes, not about how something behaves for three extremely small sizes.\n\nIf the number of steps for size n is, in general, the sum of the numbers from 1 through n then the formula for the number of steps is indeed n(n+1)/2 and it is O(n^2).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is there a thing such as a Big-O-Notation of O(0)?\r\n                \r\nI was wondering whether it is even possible as a concept to have a Big-O-Notation of O(0) (in a very specific scenario).\nImagine I have a list with values and I want to sort it with Bubblesort. Suppose, however, that the list is already sorted. If I'm not mistaken, this would have a Big-O-Notation of O(n), where n is the number of elements.\nNow, I want to express the Big-O-Notation of the swaps I've had to make to get the list sorted. In this very specific scenario, no swaps were made. So, would I go about it by saying that the swaps' Big-O-Notation is O(0) or is the minimum I can have O(1), and why?\n    ", "Answer": "\r\nThe simplest complexity is ```\nO(1)```\n, constant time (or space, or whatever else you may be measuring, including number of swaps). That's constant, even if the value is zero.\nIn any case, sort functions are not O(1) because, at a minimum, you have to check it's sorted even if you don't make any swaps. Your particular measurement of swap count may give you O(1) but I'd be interested in the reasoning behind that being considered a useful metric :-)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Simplify Big O notation\r\n                \r\nI apologize in advance for my poor math skills...\n\nI'm trying to understand how the math behind Big O Notation works.  I understand from this that ```\n2n^2 = O(n^3)```\n and have proved that ```\nn = O(n^2)```\n, but I also seem to have proved that ```\nn^3 = O(n^2)```\n which doesn't make sense and I'm pretty sure is wrong.  Here's how I'm \"proving\" this:\n\n```\nn^3 = O(n^2)\nn^3 <= c*n^2\nn <= c     #n^2 cancels out\n1 <= c/n\nc = 1; n0 = 1\n```\n\n\nWhat am I doing wrong?\n    ", "Answer": "\r\n```\n1 <= c/n```\n does not hold for all ```\nn > n0```\n, for example, for n=2 (with your n0=1,c=1), you get:\n\n```\n1 <= 1/2\n```\n\n\nand this is a false statement.\n\nThe key in big O notation is you need to prove that for ALL ```\nn > n0```\n, the equation ```\nf(n) <= C*g(n)```\n holds (for some ```\nC,n0```\n) , in order to show ```\nf(n)```\n is in ```\nO(g(n))```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O summary for Java Collections Framework implementations? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 6 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI may be teaching a \"Java crash-course\" soon.  While it is probably safe to assume that the audience members will know Big-O notation, it is probably not safe to assume that they will know what the order of the various operations on various collection implementations is.\n\nI could take time to generate a summary matrix myself, but if it's already out there in the public domain somewhere, I'd sure like to reuse it (with proper credit, of course.)\n\nAnyone have any pointers?\n    ", "Answer": "\r\nThe book Java Generics and Collections has this information (pages: 188, 211, 222, 240).\n\nList implementations:\n\n```\n                      get  add  contains next remove(0) iterator.remove\nArrayList             O(1) O(1) O(n)     O(1) O(n)      O(n)\nLinkedList            O(n) O(1) O(n)     O(1) O(1)      O(1)\nCopyOnWrite-ArrayList O(1) O(n) O(n)     O(1) O(n)      O(n)\n```\n\n\nSet implementations:\n\n```\n                      add      contains next     notes\nHashSet               O(1)     O(1)     O(h/n)   h is the table capacity\nLinkedHashSet         O(1)     O(1)     O(1) \nCopyOnWriteArraySet   O(n)     O(n)     O(1) \nEnumSet               O(1)     O(1)     O(1) \nTreeSet               O(log n) O(log n) O(log n)\nConcurrentSkipListSet O(log n) O(log n) O(1)\n```\n\n\nMap implementations:\n\n```\n                      get      containsKey next     Notes\nHashMap               O(1)     O(1)        O(h/n)   h is the table capacity\nLinkedHashMap         O(1)     O(1)        O(1) \nIdentityHashMap       O(1)     O(1)        O(h/n)   h is the table capacity \nEnumMap               O(1)     O(1)        O(1) \nTreeMap               O(log n) O(log n)    O(log n) \nConcurrentHashMap     O(1)     O(1)        O(h/n)   h is the table capacity \nConcurrentSkipListMap O(log n) O(log n)    O(1)\n```\n\n\nQueue implementations:\n\n```\n                      offer    peek poll     size\nPriorityQueue         O(log n) O(1) O(log n) O(1)\nConcurrentLinkedQueue O(1)     O(1) O(1)     O(n)\nArrayBlockingQueue    O(1)     O(1) O(1)     O(1)\nLinkedBlockingQueue   O(1)     O(1) O(1)     O(1)\nPriorityBlockingQueue O(log n) O(1) O(log n) O(1)\nDelayQueue            O(log n) O(1) O(log n) O(1)\nLinkedList            O(1)     O(1) O(1)     O(1)\nArrayDeque            O(1)     O(1) O(1)     O(1)\nLinkedBlockingDeque   O(1)     O(1) O(1)     O(1)\n```\n\n\nThe bottom of the javadoc for the java.util package contains some good links:\n\n\nCollections Overview has a nice summary table.\nAnnotated Outline lists all of the implementations on one page.\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "determine Big-O Notation for functions [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Big O, how do you calculate/approximate it?\r\n                            \r\n                                (24 answers)\r\n                            \r\n                    \r\n                Closed 6 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nIn my course of algorithms we talk about time complexity with Big O Notation. And I am always confused when I try to calculate the Big O. I know for example when a function then it can be O(n) or O(n²). But I don't know the logical background and how to get this solution for each function.\n\n```\nint func1(int n){ \n   for (int i=1; i<n; i=i*2) \n   printf(\"i = %d\", i); return i; \n}\n\nint func2(int a, int b) {\n   int result=1; \n   while (b>0) {\n      result = result*a; \n      b = b-1; \n   }\n   return result;\n}\n\nvoid func3(int n) {\n   for (int i=0; i<pow(2,n); i++) \n      printf(\"%d\", i);  \n}\n```\n\n    ", "Answer": "\r\nBig O notation is talking about worst case, how long will the function run?\n\nIf you have Func2, with b = 1, six instructions run. Assignment, Comparison, Assignment, Assignment, Comparison, Return. When b = 2, 9 instructions are run. We can tell right away that the number of instructions is 3+(3*n). When talking about how long the function will take, we ignore constants both added to our variable, and multiplied by our variable. Because ultimately, when b=1000, that's the overriding factor of how long the function takes to run. And because it increases Linearly with respect to b, the function is said to be O(n) time. (Big O notation always uses 'n' for its variable.)\n\nThe first function increases even slower with respect to n, it increases Logarithmically, only adding more computation for each power of 2 that n passes. (More computation when n = 2, 4, 8, 16, 32, etc)\n\n(Assuming pow(2,n) = n squared; sometimes pow(2,n) = 2 to the power of n)\nThe third function runs the loop once when n = 1, and 4 times when n = 2, we can see that we're talking about n^2 number of iterations through the loop. This is Exponentially expanding, so it's O(n^2) time.\n\nThe way to analyze things for Big O notation is to identify how many instructions run based on n = 1, 2, 3, 4... n Make that a function and identify whether it's Constant (doesn't change based on n), Linear (changes directly proportional to n), Logarithmic (changes based on log(n)), Exponential (changes based on a power of n) or otherwise. (It's been a while, but I remember an n log n type and one that talked about when n is a power that some other exponent is raised to, but memory is hazy on those.)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What would the big O notation for this function?\r\n                \r\nWhat would be the worst time complexity big O notation for the following pseudocode? (assuming the function call is an O(1)) I'm very new to big O notation so I'm unsure of an answer but I was thinking O(log(n)) because the while loop parameters multiplied by 2 each time or would that just be O(loglog(n))? Or am I wrong on both counts? Any input/help is appreciated, I'm trying to grasp the concept of big O notation for worst time complexity which I just started learning. Thanks! \n\n```\ni ← 1\nwhile(i<n)\n    doSomething(...)\n    i ← i * 2\ndone\n```\n\n    ", "Answer": "\r\nIf ```\ni```\n is doubling every time, then the number of times the loop will execute is the number of times you can double ```\ni```\n before reaching ```\nn```\n. Or to write it mathematically, if ```\nx```\n is the number of times the loop will execute we have ```\n2^x <= n```\n. Solving for ```\nx```\n gives  ```\nx <= log_2(n)```\n. Therefore the number of times the loop will execute is ```\nO(log(n))```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for .forEach with .find\r\n                \r\nWhat is the big O notation for the following code?\n```\n    _data.validatorInfo.result.validators.forEach((_validator) => {\n        let del = {\n            delegation: _data.delegations.result.delegation_responses.find(\n                res => res.delegation.validator_address === _validator.operator_address\n            ),\n            validator: _validator,\n            rewards: _data.totalRewards.result.rewards.find(\n                re => re.validator_address === _validator.operator_address\n            )\n        }\n        result.delegations.push(del);\n    })\n```\n\nSince it has a ```\n.forEach```\n and two ```\n.find()```\n operations nested, can I assume it is O(N^3)?\n    ", "Answer": "\r\nYou go through all ```\nvalidators```\n (V), for each of them going through ```\ndelegation_responses```\n (P) and ```\nrewards```\n (W) until a certain element is found, which on average means to go through half of the array .\nThis would give you ```\nV * (P + W)/2```\n. Big O ignores constants factors (sets them to 1), so that gives you ```\nO(V * (P + W))```\n. And at that point, you can start arguing:\n\nIf P and W are very small compared to V (like orders of magnitude), you would argue that they behave like constants and only slightly scale the driving factor V, and you deal with effectively linear complexity ```\nO(N)```\n, where N is the number of validators.\n\nIf V is very small (again, orders of magnitude) compared to P and/or W, you would get ```\nO(N)```\n or ```\nO(N + M)```\n, where N and M are number of  responses/rewards, again arguing that V behaves like a constant.\n\nIf they are all about the same size (or you cannot make assumptions about their size), you get ```\nO(N * 2N)```\n, which is ```\nO(N²)```\n.\n\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation estimate\r\n                \r\nNot sure if this is the right place for this kind of question but here it goes. Given the following code, how many basic operations are there, and how many times is each one performed. What is the big O notation for this running time. This is in MATLAB, if it matters.\n\n```\ntotal = 0;\n\nfor i = 1:n\n    for j = 1:n\n        total = total + j;\n    end\nend\n```\n\n\nMy thinking is that, for each n, the j = 1:n loop runs once. Within the j = 1:n loop there are n calculations. So for the j = 1:n loop its n^2. This runs n times within the i = 1:n loop, so the total amount of calcultions is n^3, and the big O noation is O(N^3). Is this correct?\n    ", "Answer": "\r\nThe short answer is:\n\n```\nO(n^2)\n```\n\n\nThe long (and simplified) answer is:\n\nThe big \"O\" refers to the complexity of an algorithm (in this case, your code). Your question asks \"how many\" loops or operations are performed, but the \"O\" notation gives a relative idea of the complexity of an algorithm, thus not an absolute quantity. This would totally be impractical, the idea of the O notation is to generalise a measure of the complexity so that algorithms can be compared relatively to the other, without worrying too much about how many assignments, loops, and so on are performed.\n\nThat being said, there are specific guidelines on how to compute the complexity of an algorithm. Generally:\n\n\nLoops are of complexity O(\"n\"), not matter how many iterations they perform (remember, this is an abstract measure).\nOperations such as assignments, additions etc are generally approximated to O(1) (complexity of 1) because the time they take to be performed is negligible.\n\n\nThere are specific rules for ```\nif then else```\n operations, but it would make things more complicated and I invite you to read some introduction material on performing algorithm complexity analysis.\n\nAlso, be careful, the \"n\" is not that used in your code, it is a special notation used to denote a \"generic\" linear complexity.\n\nMeasuring the complexity of an algorithm is a recursive operation. You start with the basic operations and move up to loops etc. So, here is a detailed (I purposely detail too much so you get an idea of how it works, but in practice you don't have to go in that level of detail):\n\nYou start of with the first instruction:\n\n```\nO(total = 0;) = O(1)\n```\n\n\nbecause it is an assignment.\n\nThen:\n\n```\nO(total = total + j;) = O(total + j) + O(total = x)\n```\n\n\nwhere ```\nx```\n is the result of ```\ntotal + j```\n.\n\n```\n                      = O(1) + O(1)\n```\n\n\nThese are basic operations, thus they have a complexity of 1.\n\n```\n                      = O(1)\n```\n\n\nBecause \"O\" is a \"greatness\" indicator that considers any sum of constants as 1.\n\nNow coming to the loop:\n\n```\nO(\n    for i = 1:n // O(n)\n        for j = 1:n // O(n)\n            total = total + j; // O(1)\n        end\n    end\n)\n=\nO(\n    n * ( \n        n * (\n            1\n        )\n)\n= O(n * n * 1)\n= O(n^2)\n```\n\n\nIf you had two loops in a row (for ... ; for .... ;), the complexity would not be O(2n), but O(n), because again, O generalises.\n\nHope that helps :)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big o notation and recursive functions\r\n                \r\nI am trying to learn Big-O notation but i have difficulties in calculating time complexity of recursive functions. \n\nCan you help me to understand the time complexity of following example?\n\n```\npublic int recursiveFunction(int n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    return Math.max(recursiveFunction(rand(n)) + 2,recursiveFunction(n - 1));\n}\n\npublic int rand(int n) {\n    return new Random().nextInt(n - 1);\n}\n```\n\n\nThanks.\n    ", "Answer": "\r\nThe time will depend on what ```\nrand(n)```\n returns, but if you take the worst-case, this will be ```\nn-2```\n. So the code simplifies to:\n\n```\npublic int recursiveFunction(int n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    return Math.max(recursiveFunction(n - 2) + 2,recursiveFunction(n - 1));\n}\n```\n\n\nwhich has an asymptotic upper bound equal to that of:\n\n```\npublic int recursiveFunction(int n) {\n    if (n == 0) {\n        return 0;\n    }\n\n    recursiveFunction(n-1);\n    recursiveFunction(n-1);\n\n    return 0;\n}\n```\n\n\nwhich is a recursion with a depth of ```\nn```\n and a branch factor of ```\n2```\n, so O(2^n) time-complexity.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big(O) notation\r\n                \r\n```\ndef largest_34(a:list):\n\n    a.sort()\n    m1, m2, m3, m4 = a[3], a[2], a[1], a[0]\n\n    for i in a[4:]:\n        if i > m1:\n            m4 = m3\n            m3 = m2\n            m2 = m1\n            m1 = i           \n\n    return m3+m4\n```\n\n\n2\n\n```\ndef largest_third(a:list):\n\n    n = len(a)//3\n    a.sort()\n    a.reverse()\n    sum_n = 0\n\n    for i in range(n):\n        sum_n += a[i]\n\n    return sum_n\n```\n\n\n3\n\n```\ndef third_at_least(a:list):\n    L2 = []\n    a.sort()\n    MatchNum = (len(a)//3 + 1)\n    Return_Val_in_List = []\n    for i in range(len(a)):\n        if (i == len(a)-1):\n            CountNum = (a[i], a.count(a[i]))\n            L2.append(CountNum)     \n\n        elif a[i] != a[i+1] and i != (len(a)-1):\n            CountNum = (a[i], a.count(a[i]))\n            L2.append(CountNum)\n\n    for j in range(len(L2)):\n        if L2[j][1] >= MatchNum:\n                Return_Val_in_List.append(L2[j][0])\n\n    if Return_Val_in_List == []:\n        return None\n    else:\n        return Return_Val_in_List\n```\n\n\nI'm learning big-O notation. I'm hoping to get the correct answers for the codes above. So, for the first question, I think it's ```\nnlog(n)```\n since it has ```\na.sort()```\n, which is ```\nnlog(n)```\n and the for loop, which is ```\nn```\n. My answer for this questions is ```\nn+nlog(n)```\n. Since such small numbers are abandoned, I would say it's ```\nnlog(n)```\n\n\nFor question2, I would say it's also ```\nn+log(n)```\n (```\nnlog(n)```\n from ```\na.sort```\n and ```\nn```\n from for loop). Therefore, the final answer is same as Q1. ```\nnlog(n)```\n\n\nFor question3, I would say it's ```\nnlog(n)+2n```\n (```\nnlog(n)```\n from ```\na.sort```\n and ```\n2n```\n from for loops). \n\nCan I take this logic? If not, could anyone possibly explain how to analyse those code?\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Determining/calculating Big O notation\r\n                \r\nSo, I understand that Big O is essentially the time coplexity of a function/algorithm, however my textbook was so brief on the matter, it is difficult to resolves seemingly simple questions like this. \n\n\n  Determine big O notation for function ```\nf(n) = 1000n + 0.1n^2 + n ln ⁡x```\n\n\n\nMy attempt/guess is ```\nO(n^2(n Ln x))```\n, but wouldn't the x be considered a constant and therefor omitted?\nWhat is the best way to solve questions of this type?\n\nThanks in advance. \n    ", "Answer": "\r\nI think the right answer is ```\nO(n^2)```\n. \n\nSee, you have a sum ```\nf(n) = a(n) + b(n) + c(n)```\n and the only thing that matters is the \"largest\" (quickest growing) summand. Let's compare:\n\n\n```\n1000n```\n is linear in ```\nn```\n\n```\n0.1n^2```\n is cubic quadratic in ```\nn```\n\n```\nn ln(x)```\n is linear in ```\nn```\n (whatever ```\nx```\n is, ```\nf```\n doesn't depend on it, so ```\nln(x)```\n is as constant as ```\n42```\n or any other fixed quantity)\n\n\nWhat's the largest one? Right - the cubic quadratic term ```\nn^2```\n, hence the answer: \"```\nf(n)```\n  is in ```\nO(n^2)```\n\".\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Java Stack array - Big O notation\r\n                \r\nWhat are the big o notation for this code below. I still couldn't grasp the concept fully. \nI am supposed to get a thought from experienced coders to give a summary for the big o performance based on this code. \n\n```\nimport java.util.*;\nimport java.util.InputMismatchException;\nimport javax.swing.*;\npublic class MyStack {\n\n   private int maxSize;\n   private long[] stackArray;\n   private int top;\n   public MyStack(int s) {\n      maxSize = s;\n      stackArray = new long[maxSize];\n      top = -1;\n   }\n   public void push(long j) {\n      stackArray[++top] = j;\n      System.out.println(\"Push onto stack\");\n   }\n   public long pop() {\n      return stackArray[top--];\n   }\n   public long peek() {\n      return stackArray[top];\n   }\n   public boolean isEmpty() {\n      return (top == -1);\n   }\n   public boolean isFull() {\n      return (top == maxSize - 1);\n   }\n   public static void main(String[] args) {\n        Scanner num = new Scanner(System.in);\n        int input =0;\n        int x;\n        MyStack theStack = new MyStack(5); \n    for(x=0; x<5; x++)\n    {\n\n\n        System.out.println(\"\\nEnter a number(Push): \");\n        input = num.nextInt();\n        theStack.push(input);\n\n\n\n      } \n\n\n\n     System.out.print(\"The first element on the top is the top of the stack\");\n     System.out.println(\"\");\n      while (!theStack.isEmpty()) {\n         long value = theStack.pop();\n         System.out.print(value);\n         System.out.println(\" Pop\");\n\n      }\n\n\n      System.out.println(\"\");\n\n   }\n}\n```\n\n    ", "Answer": "\r\nBig O performance varies by the operation that you're attempting. Look at your \"isEmpty()\" method. It always just looks at the value of top, so that's constant, or O(1). I don't see other methods in your class (except main(), which we'll look at in a minute) that have any dependency on the number of elements in the array, they all just work with top.\n\nmain() just asks for 5 values, then prints them out. If it asked for 50, it would take ten times longer (assuming that user input remained relatively constant). So main is O(n) where n is the number of elements in the array.\n\nIf you were looking for a particular number in the array, you'd likely have to examine each one in turn, so O(n).\n\nAnd if you were doing something more complicated where you looked at each element and then did some operation or comparison with each other element (e.g. with a nested for loop) you'd end up with O(n^2).\n\nHope that helps your thought process.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation in Java [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and  cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened,  visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nHey guys can someone explain the big o notation for each of the following examples? Thanks in advance\nReading the element at index 28 in an array. Is this O(1)?\nComparing two ArrayList objects to determine if they contain the same elements (disregarding order, and without sorting). Is this O(n^2)?\nSearching for a specific target value in an unsorted array. Is this O(n)\n    ", "Answer": "\r\n1- Because array elements can be accessed randomly (no need to move from one cell to another like linked list) so you can basically say array[28] which is O(1)\n\n2- comparing two array list can done in a way which will lead to a a lower big-O But with sorting. \n\nYou can sort each arraylist using mergesort or quicksort O(nlg(n)) then compare the two sorted lists in O(n). the result is O(nlgn). \n\nBut another algorithm (without sorting) would iterate over each element in one array (n). And then checks whether the element is another array (n) (and marks it to handle duplicates properly). This latter algorithm is O(n^2).\n\n3- Yes it's. You have to go through the whole list one by one till you find the desired element. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Example of O(n!)?\r\n                \r\nWhat is an example (in code) of a ```\nO(n!)```\n function?  It should take appropriate number of operations to run in reference to ```\nn```\n; that is, I'm asking about time complexity.\n    ", "Answer": "\r\nThere you go. This is probably the most trivial example of a function that runs in ```\nO(n!)```\n time (where ```\nn```\n is the argument to the function):\n\n```\nvoid nFacRuntimeFunc(int n) {\n  for(int i=0; i<n; i++) {\n    nFacRuntimeFunc(n-1);\n  }\n}\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for Ruby methods?\r\n                \r\nHow can I find the complexity of a Ruby method? \n\nFor example length? If I look at the source code, I see this:\n\n```\n               static VALUE\nrb_ary_length(VALUE ary)\n{\n    long len = RARRAY_LEN(ary);\n    return LONG2NUM(len);\n}\n```\n\n\nBut I don't know how to read that in order to find the Big O notation.  \n    ", "Answer": "\r\nThere is no maintained list of theoretical complexities of Ruby methods. Of interest to you is ```\nminitest/benchmark```\n, which is used like this:\n\n```\nrequire 'minitest/autorun'\nrequire 'minitest/benchmark'\n\nclass TestFoobar < Minitest::Benchmark\n  def setup\n    @foo_arrays = ( 1 .. 10_000 ).map { |n| [ 42 ] * n }\n    # Because default benchmarking range is [1, 10, 100, 1_000, 10_000]\n  end\n\n  def bench_foo_size\n    assert_performance_constant 0.99 do |n| @foo_arrays[ n ].size end\n  end\nend\n```\n\n\nIf you run the above test, you can actually see that the performance of ```\nArray#size```\n method is constant. If you change ```\n#bench_foo_size```\n to:\n\n```\ndef bench_foo_size\n  assert_performance_linear 0.99 do |n| @foo_arrays[ n ].size end\nend\n```\n\n\nThe test will actually fail, because ```\nArray#size```\n is not linear, but sublinear. ```\nminitest/benchmark```\n is flexible and you can apply it to your own code as well as to built-in assets.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation finding c and n0\r\n                \r\nI've just been introduced to Big-O notation and I've been given some questions. However I'm confused as to how to determine the value of ```\nn0```\n.\nI have to show that ```\n3n^3  +20n^2 + 5```\n is O(n^3). So far I have:\n\n```\n3n^3 + 20n^2 + 5 <= cn^3\n\n(3 - c)n^3 + 20n^2 + 5 <= 0\n\n5 <= n^3(c - 3) - 20n^2\n\n5 <= n^2(n(c - 3) - 20)\n```\n\n\nI just don't know what to do from here to find n0 and c. Would someone mind explaining?\n    ", "Answer": "\r\n```\n3n^3 + 20n^2 + 5 <= cn^3\n=> 20n^2 + 5 <= cn^3 - 3n^3\n=> 20n^2 + 5 <= n^3(c - 3)\n=> 20n^2/n^3 + 5/n^3 <= n^3(c - 3)/n^3\n=> 20/n + 5/n^3 <= c - 3\n=> c >= 20/n + 5/n^3 + 3\n```\n\n\nDepending on where you want the greater than condition to begin, you can now choose n0 and find the value.\n\nFor example, for n0 = 1:\n\n```\nc >= 20/1 + 5/1 + 3 which yields c >= 28\n```\n\n\nIt's worth noting that by the definition of Big-O notation, it's not required that the bound actually be this tight. Since this is a simple function, you could just guess-and-check it (for example, pick 100 for c and note that the condition is indeed true asymptotically).\n\nFor example:\n\n```\n3n^3 + 20n^2 + 5 <= (5 * 10^40) * n^3 for all n >= 1\n```\n\n\nThat inequality holding true is enough to prove that f(n) is O(n^3).\n\n\n\nTo offer a better proof, it actually needs to be shown that two constants, ```\nc```\n and ```\nn0```\n exist such that ```\nf(n) <= cg(n) for all n > n0```\n.\n\nUsing our c = 28, this is very easy to do:\n\n```\n3n^3 + 20n^2 + 5 <= 28n^3\n20n^2 + 5 <= 28n^3 - 3n^3\n20n^2 + 5 <= 25n^3\n20/n + 5/n^3 <= 25\n\nWhen n = 1: 20 + 5 <= 25 or 25 <= 25\nFor any n > 1, 20/n + 5/n^3 < 25, thus for all n > 1 this holds true.\n\nThus 3n^3 + 20n^2 + 5 <= 28n^3 is true for all n >= 1\n```\n\n\n(That's a pretty badly done 'proof' but hopefully the idea shows.)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation of function\r\n                \r\nThe following function takes a 1D array A of size n as a parameter and returns a 2D array M of size n x n. M stores average values. These average values are calculated from array A between 2 iterative variables (i an j) if i <= j using the formula M[i][j]=(A[i] +...+A[j])/( j-i+1). For example if i=1, and j=3. Then M[0][3] will store the value calculated from M[1][3] = (A[1] + A[2] + A[3])/3-1+1\n\n```\n public float[][] averageArray(float[] A){\n        int n = A.length;\n        float sum;\n        float[][] M = new float[n][n];\n\n        for(int j = 0; j<n;j++){\n            for(int i = 0; i<n; i++){\n\n                if(i<=j) {\n                    sum = 0;\n                    for (int k = i; k <= j; k++) {\n                        sum += A[k];\n\n                    }\n                    M[i][j] = sum / (j - i + 1);\n                }else{\n                    M[i][j] = 0;\n                }\n\n                System.out.println(\"[\"+ i +\"]\" + \"[\"+ j +\"]: \"+ M[i][j]);\n            }\n\n        }\n        return M;\n    }\n```\n\n\nI'm confused about the big-o notation of this function. I know that the first two for loops produce a big-O of O(n^2) but because the third loop is conditional, I'm not sure how to put that into big-O notation. From tests I found that the number of times the 3rd  loop is executed increases with the j value (e.x if j = 3 then the loop executes 3 times, if j = 5 then the loop will execute 5 times).\n    ", "Answer": "\r\ntl;dr when you're not sure how it factors into the big-o runtime, create a summation for the loop, and then convert that summation into a function of ```\nn```\n\n\nSometimes you'll encounter questions like \"how many times did the line inside the innermost loop execute in terms of n. It's not quite the runtime analysis, but here's an analysis nonetheless. It will help you get a better picture of the big-o runtime. \n\nIt might be helpful to put a counter inside the inner loop and see how many times it gets executed. \n\nIt also might be helpful to draw a grid and color the squares where ```\ni <= j```\n and ```\nj```\n is the row index (since it's the first loop's variable) and ```\ni```\n is the column index). When you do this, you'll see that all the colored squares split the square grid into 2 triangles down the diagonal from top-left to bottom right. Anything that falls directly on the line still counts (because you said ```\n<=```\n rather than ```\n<```\n). The colored squares will be the bottom/left triangle.\n\nThis is just a depiction of where the innermost loop will actually do something. \n\nThe outer 2 loops will now iterate over each location in the grid. We'll call this the current location. A line of code in the inner loop will now execute once for each colored square above the current location in that column of the grid (and once for the current location if it's colored) each time a new location is determined by the outer 2 loops. \n\nAfter visualizing this, you can more easily see how to count the number of times this will execute. The first column of the grid has n colored squares. The first time this column will be counted will be when the top left square is chosen(j=0, i=0). The second time will be when (j=1, i=0). SO, let's fill in a grid where the value at each location is the number of times each cell is counted. It will be like this:\n\n```\n[n,  0 ,  0,  0, ... ]\n[n-1, n-1, 0, 0, ... ]\n[n-2, n-2, n-2, 0, ...]\n```\n\n\nYou can see the picture now. Adding up everything in this grid will now tell you how many times your inner-most loop executed. \n\n\nFirst row has 1 n\nSecond row has 2 (n-1)'s\nThird row has 3 (n-2)'s\n\n\nYou can see the pattern of (n-j) * (j+1) as the total for each row. \n\nSum over the rows to get the total. \n\nYou end up with a sum like so:\n\n```\nfor(int i = 0; i < n; i++)\n    sum += (n-i)*(i+1);\nreturn sum;\n```\n\n\n\n\nThat's just for the number of times that the inner-most loop executed. Now for the times the inner-most loop did not get executed. This part is much easier. It's simply the number of non-colored squares in the grid from earlier. \n\nBecause it's an n by n grid, n2/2 would seem like the right answer. BUT the main diagonal squares are all colored. n2/2 already counts half of that line, so we have to take out the other half: n/2.\n\nSo, the total number of executions would be the ```\nfor```\n loop sum above, plus half the square of n (non-colored squares), minus half of n (because you just added half of the diagonal that was already colored in the previous plus term).\n\nThis ends up looking like \n\nThe meaning of the first 2 terms is the number of times that the inner-most for-loop did NOT execute. \n\nWhen I run this code, the following is my results:\n\n\nn=10\n\n\ninner-loop executions: 220\ntotal executions: 265\n220 + 102/2 - 10/2 (220 + 50 - 5 = 265)\n\nn=100\n\n\ninner-loop executions: 171700\ntotal executions: 176650\n\nn=1000\n\n\ninner-loop executions: 167167000\ntotal executions: 167666500\n\nn=10000\n\n\ninner-loop executions: 166716670000\ntotal executions: 166766665000\n\nn=100000\n\n\ninner-loop executions: 166671666700000\ntotal executions: 166676666650000\n\nn=100000000 (did it just for the lolz, you can already see the pattern)\n\n\ninner-loop executions: 166666671666666700000000\ntotal executions: 166666676666666650000000\n\n\n\nAND FOR MY FINAL REVEAL, SINCE YOU'VE READ THIS FAR\nThis is an O(n3) function. \n\nI won't get into it too much, but the summation for the number of times that the inner-most for-loop executes simplifies down to \n\nAdding in the 2 terms that count the number of times the inner-most loop did NOT execute, group the like-terms for easier simplification and you'll end up with this:\n\n\n\nYou can use these last 2 formulas to verify the data that I've shared above. You can also empirically verify by adding counters into your loops and running them to see how many times they execute and compare that with the values given by these formulas(for values as large as those I provided you'll need to use ```\nBigInteger```\ns or some other arbitrarily large number format). If you don't trust me, or your computer, you can also try throwing some of this information at an online tool which solves equations, such as wolfram alpha. Lastly, if this is from an exam question, you can take this to your professor and showcase the nature of this problem and the exact number of ```\nfor-loop```\n executions given ```\nn```\n is the length of the array ```\nA```\n. If all this is wrong, I've at the least shown that it's true for powers of 10. Hope this helps in some way. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation for string matching algo\r\n                \r\nWhat would the big O notation of the function foo be?\n\n```\nint foo(char *s1, char *s2)\n{\n   int c=0, s, p, found;\n   for (s=0; s1[s] != '\\0'; s++)\n   {\n      for (p=0, found=0; s2[p] != '\\0'; p++)\n      {\n         if (s2[p] == s1[s])\n         {\n            found = 1;\n            break;\n         }\n      }\n      if (!found) c++;\n   }\n   return c;\n}\n```\n\n\nWhat is the efficiency of the function foo?\n\na) O(n!)\n\nb) O(n^2)\n\nc) O(n lg(base2) n )\n\nd) O(n)\n\nI would have said O(MN)...?\n    ", "Answer": "\r\nIt is ```\nO(n²)```\n where n = max(length(s1),length(s2)) (which can be determined in less than quadratic time - see below). Let's take a look at a textbook definition:\n\n\n  f(n) ∈ O(g(n)) if a positive real number c and positive integer N exist such that f(n) <= c g(n) for all n >= N\n\n\nBy this definition we see that n represents a number - in this case that number is the length of the string passed in. However, there is an apparent discrepancy, since this definition provides only for a single variable function ```\nf(n)```\n and here we clearly pass in 2 strings with independent lengths. So we search for a multivariable definition for Big O. However, as demonstrated by Howell in \"On Asymptotic Notation with Multiple Variables\":\n\n\n  \"it is impossible to define big-O notation for multi-variable functions in a way that implies all of these [commonly-assumed] properties.\"\n\n\nThere is actually a formal definition for Big O with multiple variables however this requires extra constraints beyond single variable Big O be met, and is beyond the scope of most (if not all) algorithms courses. For typical algorithm analysis we can effectively reduce our function to a single variable by bounding all variables to a limiting variable ```\nn```\n. In this case the variables (specifically, length(s1) and length(s2)) are clearly independent, but it is possible to bound them:\n\nMethod 1\n\n```\nLet x1 = length(s1)\nLet x2 = length(s2)\n```\n\n\nThe worst case scenario for this function occurs when there are no matches, therefore we perform x1 * x2 iterations.\n\nBecause multiplication is commutative, the worst case scenario foo(s1,s2) == the worst case scenario of foo(s2,s1). We can therefore assume, without loss of generality, that x1 >= x2. (This is because, if x1 < x2 we could get the same result by passing the arguments in the reverse order).\n\nMethod 2 (in case you don't like the first method)\n\nFor the worst case scenario (in which s1 and s2 contain no common characters), we can determine length(s1) and length(s2) prior to iterating through the loops (in .NET and Java, determining the length of a string is O(1) - but in this case it is O(n)), assigning the greater to x1 and the lesser to x2. Here it is clear that x1 >= x2.\n\nFor this scenario, we will see that the extra calculations to determine x1 and x2 make this O(n² + 2n) We use the following simplification rule which can be found here to simplify to O(n²):\n\n\n  If f(x) is a sum of several terms, the one with the largest growth rate is kept, and all others omitted.\n\n\nConclusion\n\nfor ```\nn = x1```\n (our limiting variable), such that ```\nx1 >= x2```\n, the worst case scenario is ```\nx1 = x2```\n.\nTherefore:  ```\nf(x1) ∈ O(n²)```\n\n\nExtra Hint\n\nFor all homework problems posted to SO related to Big O notation, if the answer is not one of:\n\n```\nO(1)\nO(log log n)\nO(log n)\nO(n^c), 0<c<1\nO(n)\nO(n log n) = O(log n!)\nO(n^2)\nO(n^c)\nO(c^n)\nO(n!)\n```\n\n\nThen the question is probably better off being posted to https://math.stackexchange.com/\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for n/2 in while-loop\r\n                \r\nI'm new to data structure and the likes of it.\nI would like to ask a question, how do we determine the Big-O notation value of this process:\n```\nwhile(n%2==0){\n   console.log(2);\n   n=n/2;\n}\n```\n\nWhat is the Big-O notation? Thanks before.\n    ", "Answer": "\r\nIf ```\nn```\n odd then the loop is not executed. If ```\nn```\n is even then it takes  ```\nlog2n```\n (i.e., log of base 2) iterations until the loop stops. It is ```\nlog2n```\n  because ```\nn```\n gets decrement to half each loop iterations (```\ni.e.,```\n ```\nn=n/2;```\n).\nAssuming that ```\nconsole.log(2);```\n takes ```\nc```\n time the overall complexity would be ```\nO(logn)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation and Time Complexity Confusion\r\n                \r\nOur teacher gave us this group homework about time complexity analysis.\n\nAn algorithm takes 100 seconds to process 5000 data, if it takes 2400 to process 400000 data, what is the time complexity approximation of this algorithm? (In big-O Notation)\n\nWe've made this python script to approximate the big-O notation.\n```\nimport math\ndef O(n):\n    return (n**(1/4))*(math.log(n, 2)**5)\n\nprint(O(400000)/O(5000)) # Prints 23.82907726248897 this is the best approximation we've got\n```\n\nMoreover, we've discussed really far, about some of these notation, are these the same?\n\nO(2^n) and O(3^n)\nO(n^k) and O(n^(k+1))\nO(n^(1/2)) and O(n^(1/3))\n\nThank you in advance.\n    ", "Answer": "\r\nI think your approximation for your group homework is good enough. Nice work! But people seldom analyze their programs in a black-box manner (count time and approximate) since people always know the algorithm and implementation. So don't take the homework too seriously ;)\nAnd the following notation pairs, as you mentioned, are indeed different:\n\nO(2^n) and O(3^n)\nO(n^k) and O(n^(k+1))\nO(n^(1/2)) and O(n^(1/3))\n\nEach pair of notations has a non-negligible gap between each other.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "what is the big O notation for this pseudo-script\r\n                \r\nI am studying big-O notation and I want to find the big O notation of a Math problem I solved from Project Euler.\n\n```\ntotal\n\nfor x (0..9){\n    for y (0..9){\n        for z(0..9){\n            if(some_condition == true){\n                total = total + permute(x,y,z)\n            }\n        }\n    } }\nprint total\n```\n\n\nmy guess is O(N^3) because of the 3 loops but I am not sure \n    ", "Answer": "\r\nThe complexity is = ```\nO(10*10*10*C(permute))```\n = ```\nO(1000*C(permute))```\n = ```\nO(k*C(permute))```\n,\nwhere ```\nC(permute)```\n is the complexity of the function ```\ncompute(x,y,z)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation (exponential functions)\r\n                \r\nWhat is the big O notation of 2^(log(n!)+2) + 3^n\nI can simplify it to <=2^(log(n!)) +3^n\nAnd now I really don't know where to start or how to figure out the big O,\nBecause on the first term we have a smaller base but bigger exponent,on the other we have a bigger base but a smaller exponent\n    ", "Answer": "\r\nI think the answer to this question is O(3^n), because the one with the bigger base will grow substantially faster (considering these are exponential functions) than the function with the smaller base.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation of multiple statements\r\n                \r\nSuppose there are multiple functions with certain big o notations, anything O(N), O(N^2), etc.\nIf you have a code fragment such as.\n\n```\n f1(x);\n f2(x);\n f3(x);\n```\n\n\nAre all the big O notations added together or multiplied? Any explanation as to why either would be correct - addition or multiplication? \n    ", "Answer": "\r\nNeither.  You would take the maximum.\n\nCalling the larger piece of code ```\ng```\n...  If for example O(```\nf2```\n) >= O(```\nf1```\n) and O(```\nf2```\n) >= O(```\nf3```\n), then the complexity of ```\ng```\n is <= 3 * O(```\nf2```\n) = O(```\nf2```\n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation Arrays vs. Linked List insertions\r\n                \r\nBig O Notation Arrays vs. Linked List insertions:\n\nAccording to academic literature for arrays it is constant O(1) and for Linked Lists it is linear O(n).\n\nAn array only takes one multiplication and addition.\n\nA linked list which is not laid out in contiguous memory requires traversal.\n\nThis question is, does O(1) and O(n) accurately describe indexing/search costs for arrays and linked lists respectively?\n    ", "Answer": "\r\n```\nO(1)```\n accurately describes inserting at the end of the array. However, if you're inserting into the middle of an array, you have to shift all the elements after that element, so the complexity for insertion in that case is ```\nO(n)```\n for arrays. End appending also discounts the case where you'd have to resize an array if it's full.\n\nFor linked list, you have to traverse the list to do middle insertions, so that's ```\nO(n)```\n. You don't have to shift elements down though.\n\nThere's a nice chart on wikipedia with this: http://en.wikipedia.org/wiki/Linked_list#Linked_lists_vs._dynamic_arrays\n\n```\n                          Linked list   Array   Dynamic array   Balanced tree\n\nIndexing                          Θ(n)   Θ(1)       Θ(1)             Θ(log n)\nInsert/delete at beginning        Θ(1)   N/A        Θ(n)             Θ(log n)\nInsert/delete at end              Θ(1)   N/A        Θ(1) amortized   Θ(log n)\nInsert/delete in middle     search time \n                                + Θ(1)   N/A        Θ(n)             Θ(log n)\nWasted space (average)            Θ(n)    0         Θ(n)[2]          Θ(n)\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is there a master list of the Big-O notation for everything?\r\n                \r\nIs there a master list of the Big-O notation for everything? Data structures, algorithms, operations performed on each, average-case, worst-case, etc.\n    ", "Answer": "\r\nDictionary of Algorithms and Data Structures is a fairly comprehensive list, and includes complexity (Big-O) in the algorithms' descriptions.  If you need more information, it'll be in one of the linked references, and there's always Wikipedia as a fallback.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation: Definition\r\n                \r\nI've been watching MIT lectures for the algorithms course and the definition for the Big O notation says\nf(n) = O(g(n)) such that for some constants c and n0\n0 < f(n) < c.g(n) for all n>n0\n\nThen the instructor proceeded to give an example,\n2n2=O(n3)\n\nNow I get that Big O gives the upper bound on the function but I am confused as to what exactly does the function f(n) correspond to here? What is its significance? As per my understanding goes, g(n) is the function representing the algorithm we are trying to analyse, but what is the purpose of f(n) or as in the example 2n2?\n\nNeed some clarification on this, I've been stuck here for hours. \n    ", "Answer": "\r\nIn the formal definition of big-O notation, the functions f(n) and g(n) are placeholders for other functions, the same way that, say, in the quadratic formula, the letters a, b, and c are placeholders for the actual coefficients in the quadratic equation.\n\nIn your example, the instructor was talking about how 2n2 = O(n3). You have a formal definition that talks about what it means, in general, for f(n) = O(g(n)) to be true. So let's pattern-match that against the math above. It looks like f(n) is the thing on the left and g(n) is the thing on the right, so in this example f(n) = 2n2 and g(n) = n3.\n\nThe previous paragraph gives a superficial explanation of what f(n) and g(n) are by just looking at one example, but it's better to talk about what they really mean. Mathematically, f(n) and g(n) really can be any functions you'd like, but typically when you're using big-O notation in the context of the analysis of algorithms, you'll usually let f(n) be the true amount of work done by the algorithm in question (or its runtime, or its space usage, or really just about anything else) and will pick g(n) to be some \"nice\" function that's easier to reason about. For example, it might be the case that some function you're analyzing has a true runtime, as a function of n, as 16n3 - 2n2 - 9n + 137. That would be your function f(n). Since the whole point behind big-O notation is to be able to (mathematically rigorously and safely) discard constant factors and low-order terms, we'll try to pick a g(n) that grows at the same rate as f(n) but is easier to reason about - say, g(n) = n3. So now we can try to determine whether f(n) = O(g(n)) by seeing whether we can find the constants c and n0 talked about in the formal definition of big-O notation.\n\nSo to recap:\n\n\nf(n) and g(n) in the definition given are just placeholders for other functions.\nIn practical usage, f(n) will be the true runtime of the algorithm in question, and g(n) will be something a lot simpler that grows at the same rate.\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation - python function\r\n                \r\nWhat is the Big O notation of this function (as a function of n = len(lst)):\n```\ndef foo(lst):\n    jump = 1\n    total = 0\n    while jump < len(lst):\n        for i in range(0, len(lst), jump):\n            total += i\n        jump = jump * 2\n    return total\n```\n\nI thought it was O(n*log(n)) but it's not, trying to understand why it's actually O(n)...\nI'm kinda new in this so if you could also explain how you got to the answer it'll be the best! thanks\n    ", "Answer": "\r\nYou keep doubling your step: 1, 2, 4, 8, ...\nSo those ranges ```\nrange(0, n, step)```\n that you go through have sizes n, n/2, n/4, n/8, ... (well, approximately - rounded to ints).\nThe sum of those is 2n (approximately). Which is O(n).\nIf you didn't know that sum yet, yet, it's easy to see: You start with 0. Then n gets you halfway to 2n. Adding n/2 gets you to 1.5n, again halfway to 2n. Adding n/4 gets you to 1.75, again halfway to 2n. And so on. You only get closer and closer to 2n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation for cellular automaton algorithm\r\n                \r\nProblem\n\nI am new to Big-O notation, attempting to find the Big-O notation for a cellular automaton algorithm.\n\nAlgorithm\n\nThe algorithm I am using is as follows:\n\n```\nAi-1 = Ai;\nRow_Rand = Random(1,ny);\nCol_Rand = Random(1,nx);\nFor r = 1:ny\n    For c = 1:nx\n        IF Col_Rand(c) == 1\n            Ai (Row_Rand(r),Col_Rand(c))= Boundary_Rules(bi2dec(Ai-1(Row_Rand(r),Col_Rand(c)), Ai-1(Row_Rand(r),Col_Rand(c)),, Ai-1(Row_Rand(r),Col_Rand(c)+1)));\n        ELSEIF Col_Rand(c) == nx\n            Ai (Row_Rand(r),Col_Rand(c))= Boundary_Rules(bi2dec(Ai-1(Row_Rand(r),Col_Rand(c)-1), Ai-1(Row_Rand(r),Col_Rand(c)),, Ai-1(Row_Rand(r),Col_Rand(c))));\n        ELSE\n            Ai (Row_Rand(r),Col_Rand(c))= Rules(bi2dec(Ai-1(Row_Rand(r),Col_Rand(c)-1), Ai-1(Row_Rand(r),Col_Rand(c)), Ai-1(Row_Rand(r),Col_Rand(c)+1)));\n        END\n    END\nEND\n```\n\n\nAssuming:\n\n\nThe matrices \"Ai\", \"Boundary_Rules\" and \"Rules\" are already defined in memory.\nThe variables \"ny\" and \"nx\" are already defined in memory and correspond to the number of pixels in an image in both the y and x directions respectively.\nThe function \"Random\" is the Modern “Fisher-Yates Shuffle” function with O(N), where N is the size of the shuffle space.\nThe function \"bi2dec\" turns the binary values stored in Ai-1 into a decimal value for array referencing.\n\n\nWould I be correct in saying that the Big-O notation for this algorithm is O(ny * nx) = O(N) (where N = ny * nx, the total number of pixels in an image).\n\nAlso, if this algorithm was then nested inside a ```\nfor```\n loop which runs for a designated number of iterations (Nit) would the Big-O notation then become O(Nit*N) where Nit would become the dominant factor if it is greater than N?\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the big O notation for this algorithm?\r\n                \r\nWhat is the big O notation for this algorithm?\n\n```\ni<-0\nk<-0\nwhile (i<=n)\n{\n  for (j<-i to n**2)\n   {\n       k<-k+1\n   }\n   i<-i*2\n } \n```\n\n\nPossible Answers:\n\na. O(logn)\n\nb. O(n) \n\nc. O(nlogn) \n\nd. None of the answers\n    ", "Answer": "\r\nAs ```\ni```\n is multiplied by ```\n2```\n each time in a ```\nwhile```\n, Hence the ```\nwhile```\n loop will be run ```\nlog(n)```\n times. And inner ```\nfor```\n loop will be run in ```\nO(n^2)```\n as ```\ni```\n is at most ```\nn```\n. Hence, the time complexity of the code in ```\nO```\n notation is ```\nO(n^2 log(n))```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the big O notation for this algorithm?\r\n                \r\nWhat is the big O notation for this algorithm?\n\n```\ni<-0\nk<-0\nwhile (i<=n)\n{\n  for (j<-i to n**2)\n   {\n       k<-k+1\n   }\n   i<-i*2\n } \n```\n\n\nPossible Answers:\n\na. O(logn)\n\nb. O(n) \n\nc. O(nlogn) \n\nd. None of the answers\n    ", "Answer": "\r\nAs ```\ni```\n is multiplied by ```\n2```\n each time in a ```\nwhile```\n, Hence the ```\nwhile```\n loop will be run ```\nlog(n)```\n times. And inner ```\nfor```\n loop will be run in ```\nO(n^2)```\n as ```\ni```\n is at most ```\nn```\n. Hence, the time complexity of the code in ```\nO```\n notation is ```\nO(n^2 log(n))```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "time complexity of function in terms of Big-O notation?\r\n                \r\n1) Althoug i have studied about the big O notation i couldn't understand how we calculate the the time complexity of this function in terms of Big-O notation. Can you explain in detail.\n\n2) For recursive function; why we call len-2 while using  recursive function ? \n\n```\nbool isPalindrome( char *s, int len) {\n             if (len <= 1) {\n             return true;\n        }\n             else\n            return ((s[0] == s[len-1]) && isPalindrome(s+1,len-2));\n        }\n```\n\n\nWhat is the time complexity of this function in terms of Big-O notation?\n\n```\nT(0) = 1 // base case\nT(1) = 1 // base case\nT(n) = 1 + T(n-2)// general case\nT(n-2)=1+T(n-4)\nT(n) = 2 + T(n-4)\nT(n) = 3 + T(n-6)\nT(n) = k + T(n-2k) ... n-2k = 1  k= (n-1)/2\nT(n) = (n-1)/2 + T(1)  O(n)\n```\n\n    ", "Answer": "\r\nYou call the recursive function with len-2 because in each execution you remove 2 characters from the word(the first and last). Hence len-2.\nT(n) = 1 + T(n-2) = 1 + 1 + T(n-4) = 1 + 1 + 1 + T(n-6) = n/2 + T(1) = O(n)\nA function g(n) is O(f(n)) if there exists a constant c and a number n0 so that for n>n0 \ng(n) < c*f(n).\nThe big-O notation is just an upper limit, so that function is O(n) but also O(n^2) and so on.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "what is order of complexity in Big O notation?\r\n                \r\nQuestion\n\nHi I am trying to understand what order of complexity in terms of Big O notation is. I have read many articles and am yet to find anything explaining exactly 'order of complexity', even on the useful descriptions of Big O on here.\n\nWhat I already understand about big O\n\nThe part which I already understand. about Big O notation is that we are measuring the time and space complexity of an algorithm in terms of the growth of input size n. I also understand that certain sorting methods have best, worst and average scenarios for Big O such as O(n) ,O(n^2) etc and the n is input size (number of elements to be sorted).\n\nAny simple definitions or examples would be greatly appreciated thanks.   \n    ", "Answer": "\r\nBig-O analysis is a form of runtime analysis that measures the efficiency of an algorithm in terms of the time it takes for the algorithm to run as a function of the input size. It’s not a formal bench- mark, just a simple way to classify algorithms by relative efficiency when dealing with very large input sizes.\n\nUpdate:\nThe fastest-possible running time for any runtime analysis is O(1), commonly referred to as constant running time.An algorithm with constant running time always takes the same amount of time\nto execute, regardless of the input size.This is the ideal run time for an algorithm, but it’s rarely achievable.\nThe performance of most algorithms depends on n, the size of the input.The algorithms can be classified as follows from best-to-worse performance:\n\nO(log n) — An algorithm is said to be logarithmic if its running time increases logarithmically in proportion to the input size.\n\nO(n) — A linear algorithm’s running time increases in direct proportion to the input size. \n\nO(n log n) — A superlinear algorithm is midway between a linear algorithm and a polynomial algorithm.\n\nO(n^c) — A polynomial algorithm grows quickly based on the size of the input.\n\nO(c^n) — An exponential algorithm grows even faster than a polynomial algorithm.\n\nO(n!) — A factorial algorithm grows the fastest and becomes quickly unusable for even small values of n.\n\nThe run times of different orders of algorithms separate rapidly as n gets larger.Consider the run time for each of these algorithm classes with\n\n```\n   n = 10:\n   log 10 = 1\n   10 = 10\n   10 log 10 = 10\n   10^2 = 100\n   2^10= 1,024\n   10! = 3,628,800\n   Now double it to n = 20:\n   log 20 = 1.30\n   20 = 20\n   20 log 20= 26.02 \n   20^2 = 400\n   2^20 = 1,048,576 \n   20! = 2.43×1018\n```\n\n\nFinding an algorithm that works in superlinear time or better can make a huge difference in how well an application performs.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Definition of Big O notation\r\n                \r\nI am an experienced programmer who understands the Big O notation (constant, linear, logN, NlogN, polynomial etc). I can analyse algorithms and find their complexities. But I am not a Computer Science Engineer. Everything is mostly self-taught.\nToday, I was looking up Sedgewick's book, chapter on analysis of Algorithms. This is how he defines the Big O algorithm\n\nA function g(N) is said to be O(f(N)) if there exist constants\nco and No such that g(N) is less than\ncof(N) for all N > No\n\nI can't understand this at all. What do I need to know to understand this?\n    ", "Answer": "\r\nIn very simple terms, it means that your function takes 25 * n + 17 CPU cycles, or anything of this form, it can be said to be O(n). We only care about the fastest-growing part of the equation (that's the meaning of the N > N0 bit: for sufficiently-large values of N, we can say 25*n + 17 < 26n), and we do not actually care about any constant factors either (because we are dealing with an abstract concept like \"complexity\", not \"CPU cycles\" or \"execution time\", all of which are impractical to measure and dependent on your implementation anyway).\nThis allows us to simplify stuff like O(n^3 + 7*n^2 + 5 log n + 17) to simply O(n^3), which is a lot shorter and still conveys all we need to know, because for large n, all the other parts will be negligible.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of just a return statement?\r\n                \r\nAm I right in saying that the time complexity in big O notation would just be O(1)?\n\n```\npublic boolean size() {\n        return (size == 0);\n    }\n```\n\n    ", "Answer": "\r\n\n  Am I right in saying that the time complexity in big O notation would just be O(1)?\n\n\nNo.\n\nThis is so common a misconception among students/pupils that I can only constantly repeat this:\n\nBig-O notation is meant to give the complexity of something, with respect to a certain measure, over another number:\n\nFor example, saying:\n\n\n  \"The algorithm for in-place FFT has a space requirement of O(n), with n being the number of FFT bins\"\n\n\nsays something about how much the FFT will need in memory, observed for different lengths of the FFT.\n\nSo, you don't specify \n\n\nWhat is the thing you're actually observing? Is it the time between calling and returning from your method? Is it the comparison alone? Is \"time\" measured in Java bytecode instructions, or real machine cycles?\nWhat do you vary? The number of calls to your method? The variable ```\nsize```\n?\nWhat is it that you actually want to know?\n\n\nI'd like to stress 3.: Computer science students often think that they know how something will behave if they just know the theoretical time complexity of an algorithm. In reality, these numbers tend to mean nothing. And I mean that. A single fetching of a variable that is not in the CPU cache can take the time of 100-10000 additions in the CPU. Calling a method just to see whether something is 0 will take a few dozen instructions if directly compiled, and might take a lot more if you're using something that is (semi-)interpreted like Java; however, in Java, the next time you call that same method, it might already be there as precompiled machine code... \n\nThen, if your compiler is very smart, it might not only inline the function, eliminating the stack save/restore and call/return instructions, but possibly even merging the result into whatever instructions you were conditioning on that return value, which in essence means that this function, in an extreme case, might not take a single cycle to execute.\n\nSo, no matter how you put this, you can not say \"time complexity in big O of something that is a language specific feature\" without saying what you vary, and exactly what your platform is.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Do scalars matter in big-O notation?\r\n                \r\nAre scalars included in big-O notation, or is O(2n) actually the same as O(n) because scalars are not taken into account?  If so, why is this?\n    ", "Answer": "\r\nBig-O notation ignores constant factors (scalars) because of its definition:\n\n\n  f(n) = O(g(n)) iff there is a natural number n0 and real number c such that for any natural number n > n0, |f(n)| ≤ |cg(n)|\n\n\nSo now suppose that f(n) = O(k × g(n)).  This means that there is some natural number n0 and real number c such that for any n > n0, we have that |f(n)| ≤ |c × k × g(n)|.\n\nWe'll use this to show that f(n) = O(g(n)).  To do this, choose n0 as your natural number and c × k as your real number.  Then for any n > n0, we have that |f(n)| ≤ |(c × k) × g(n)|, so f(n) = O(g(n)).\n\nHope this helps!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "how to turn this loops to big-o notation\r\n                \r\nThe question:\nFor the pseudo-code given below with T, being the or instruction period to run the i-th line, provide total execution time in big-O notation.\n\n```\n// get a positive integer from input\nif n > 10\n  print \"this might take a while\"\nfor k=1 to n\n  for j = 1 to k\n    print k*j\nprint \"Done!\"\n```\n\n\nActually I know what that code does but I can't understand how to type this in big-O notation?\n\nEDIT: loop as php\n    ", "Answer": "\r\n```\nfor k=1 to n\n  for j = 1 to k\n    print k*j\n```\n\n\nThe outer loop will iterate n times, that part is easy.  Since it does no work other than running the inner loop we can ignore it for the purposes of Big O calculation.  The inner loop will iterate ```\n1 + 2 + 3 + 4 ... + n```\n times which is a triangular number or ```\n(n*(n+1))/2```\n.  Big O notation ignores constants, so that can be simplified to ```\nO(n*n)```\n or ```\nO(n^2)```\n.\n\nIt's worth noting that the worst, best and average case for this algorithm are all the same.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to find Big O Notation of this function?\r\n                \r\n```\nfor ( i =0; i < n ; i ++) {\n    if ( i %2==0) {\n      for ( j =0; j < i ; j ++) {\n         System . out . println ( \" Hi \" );\n  }\n }\n}\n```\n\n\nHow would I find the big O  notation of this code?\n    ", "Answer": "\r\nYou figure out how many times you need to do each operation in terms of n operations and remove the constants.\n\nThe outermost loop, ```\nfor ( i =0; i < n ; i ++) {```\n changes the value of i which is used in the innermost loop.\n\nThe if statement ```\nif ( i %2==0) {```\n means the inner loop will run n by a constant of 1/2 so you can ignore it, it is just a constant.\n\nIgnoring the if statement, the final inner loop, ```\nfor ( j =0; j < i ; j ++) {```\n runs 1+2+3+4+...+n times which equals n(n+1)/2 times. Ignoring the constants, that's n^2.\n\nO(n^2)\n\nResources: https://en.wikipedia.org/wiki/1_%2B_2_%2B_3_%2B_4_%2B_%E2%8B%AF and Big O, what is the complexity of summing a series of n numbers?\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the big-o notation for this algorithm? [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Computing Time T(n) and Big-O with an infinite loop \r\n                            \r\n                                (3 answers)\r\n                            \r\n                    \r\n                Closed 4 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nWhat will be the big-o notation for the algorithm that consist of multiplication of N in the loop.\n\n```\nvoid testing(int n) {\n    for(int i =0; i<n;i++) {\n        n=n*2;\n        System.out.println(\"hi\"+n);\n    }\n}\n```\n\n    ", "Answer": "\r\nI'll try to be as rigorous as possible for my answer. \n\nEDIT : forgot to say, we assume every operation like comparison, assignment and multiplication have complexity of O(1)\n\nIn short, this algorithm does not terminate in most of the cases, so complexity is not defined for it. \nComplexity is some kind of a upper bound for the cost C of an algorithm, stating O(n) complexity means C <= k x n, k > 0. Non terminating algorithm has a cost which is infinite, and inf > inf is undefined. \n\nThen, let's look at why your algorithm is non-terminating : \n\neach iteration, we continue if i < n. Yet, each iteration n is multiplied by 2. We can see a relation between the value of i and n when checking for the condition of the loop : n = n0x2^i, with n0 being the initial value of n. \nTherefore, your algorithm will only be terminating when n0 <= 0, and when this case occurs, it will not enter the loop once.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Alternatives to the Big-O notation?\r\n                \r\nGood afternoon all,\n\nWe say that a hashtable has O(1) lookup (provided that we have the key), whereas a linked list has O(1) lookup for the next node (provided that we have a reference to the current node).\n\nHowever, due to how the Big-O notation works, it is not very useful in expressing (or differentiating) the cost of an algorithm x, vs the cost of an algorithm x + m.\n\nFor example, even though we label both the hashtable's lookup and the linked list's lookup as O(1), these two O(1)s boil down to a very different number of steps indeed, \n\nThe linked list's lookup is fixed at x number of steps. However, the hashtable's lookup is variable. The cost of the hashtable's lookup depends on the cost of the hashing function, so the number of steps required for the hashtable's lookup is: x &plus; m,\n\n\nwhere x is a fixed number\nand m is an unknown variable value\n\n\nIn other words, even though we call both operations O(1), the cost of the hashtable's lookup is a magnitude higher than the cost of the linked list's lookup.\n\nThe Big-O notation is specifically about the size of the input data collection. This does have its advantages, but it has its disadvantages as well, as can be seen when we collapse and normalize all non-n variables into 1. We cannot see the m variable (the hashing function) inside it anymore. \n\nBesides the Big-O notation, Is there another (established) notation we can use for expressing  the fixed-cost O(1) which means x operations and the variable-cost O(1) which means x + m (m, the hashing function) number of operations?\n    ", "Answer": "\r\n\n  literal O(1) which means exactly 1 operation\n\n\nExcept it doesn't. The big O-Notation concerns relative comparision of complexity in relation to an input. If the algorithm does take a constant amount of steps, completely independent of the size of your input, than the exact amount of steps doesn't matter.\n\nTake a look at the (informal) definition of O(n):\n\n\n\nIt means: There is a certain ```\nk```\n so that for each ```\nn```\n the function ```\nf```\n is smaller than the function ```\ng```\n.\n\nIn the case above, the hashtable lookup and linked list lookup would be ```\nf```\n, and ```\ng```\n would be ```\ng(n) = 1```\n. For each case, you are able to find a ```\nk```\n that ```\nf(n) <= g(n) * k```\n.\n\nNow, this ```\nk```\n doesn't need to fixed, it can vary depending on platform, implementation, specific hardware. The only interesting point is that it exists. That's why both hashtable lookup and linked list node lookup are O(1): Both have a constant complexity, regardless of input. And when evaluating algorithms, that's what interesting, not the physical steps.\n\nSpecifically concerning the Hashtable lookup\n\nYes, the hash function does take a variable amount of operations (depending on implementation). However, it doesn't take a variable amount of operation depending on the size of the input. Big O-Nation is specifically about the size of the input data collection. A hash function takes a single element. For the evaluation of an algorithm it doesn't matter wether a certain function takes 10, 20, 50 or 100 operations, if the number of operations doesn't increase with the input size, it is O(1). There is no way to distinguish this in big O-Notation, as this isn't what big O-Notation is about.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Java Big O notation in algorithm\r\n                \r\ni am comfused with this big O notation problem. This code does not look like O(n) but for loop turns number of word time so it is basically not more than 20\nso, if we say Length (line.split()) is constant c, can we say O(c.n)  = O(n) ?\n\n```\nwhile (this.scannerMainText.hasNextLine()){\n        String line = this.scannerMainText.nextLine();\n        for (String word : line.split(\"[.!,\\\" ]+\")) {\n           some statements\n         }\n     }\n```\n\n    ", "Answer": "\r\nYes, the amount of times a loop is run has a negligible impact on the duration, therefore it is usually not mentioned in the Big-O notation. \n\nFor example, if a O(n) loop is run 20 times, you would think the notation would be O(20n), but because the impact is so small it is not mentioned it the Big-O notation and therefore O(20n) = O(n). Same goes for O(20n²) = O(n²) etc..\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for nested loops [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        How can I find the time complexity of an algorithm?\r\n                            \r\n                                (10 answers)\r\n                            \r\n                    \r\n                Closed 5 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI am just learning the Big O notation and wanted to ask how it works for nested loops.\n\nIs it true that in the case of \n\n```\nfor (int i = 0; i < N; i++){\n    for (int j = 0; j < N; j++){\n       do something;\n    }\n}\n```\n\n\nIt would be O(N squared), while\n\n```\nfor (int i = 0; i < 1000; i++){\n    for (int j = 0; j < N; j++){\n        do something;\n    }\n}\n```\n\n\nIt would be O(N) because the first loop has a constant? Or would it still be O(N squared)? Thank you\n    ", "Answer": "\r\nYour first statement is correct.\nN can be very large and O(n) takes it into account.\n\nso first code is O(N^2)\nwhile second is O(1000*N) => still O(N)\n\nBIG O notation does not include constants\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How can I derive the big O notation?\r\n                \r\nI'm learning about big O notation and I need help trying to understand how the big O notation for the following program is derived:\n```\nstatic void myMethod(int[] arr, in t)\n{\n  for (int i = 0; i < n - 1; i++)\n  {\n    int variable = nextMethod(inputArray, i, n - 1);\n    int temp = inputArray[i];\n    inputArray[i] = inputArray[variable];\n    inputArray[variable] = temp;\n  }\n}\n\nstatic int nextMethod(int[] inputArray, int first, int last)\n{\n  int maximum = inputArray[first];\n  int indexOfMaximum = first;\n  for (int index = last; index > first; index--)\n  {\n    if (inputArray[index] > maximum)\n    {\n      maximum = inputArray[index];\n      indexOfMaximum = index;\n    }\n  }\n  return indexOfMaximum;\n}\n```\n\nThe answer I have been given is:\n2 (n-1) + 3(n*(n-1))/2 + 3(n -1)\n= O(n) + O(n^2) + O(n)\n= O(n^2). But I'm not sure how it is being derived... would appreciate any guidance! :)\n    ", "Answer": "\r\nStep 1)\nIn myMethod, the for-loop is rotated by n-1.\nStep 2)\nIn the nextMethod function, the first goes from last to last, but in the worst case scenario, this will be n.\nTherefore, it is done as much as (n-1)*n. (Programming Operations)\nIf you make this one axis, you will have a time complexity of n^2.\nSummary:\njust did Repeat 2 times.\nIt must be considered in the worst case by case.\nIf you have any questions, please leave comments.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation searching algorithm ?\r\n                \r\nI want to implement at least two different solutions to find N-th largest element with O(N*log(N)) average time complexity in Big-O notation, where N is the number of elements in the list ? \n\nwhich searching algorithm should i use in my program in java to find to n-th largest element with regards to O(N*log(N)) ? \n    ", "Answer": "\r\nActually the problem you are facing can be solve in linear time using the partitioning that is part of the ```\nquick sort```\n algorithm(Have a look here). If you really need and ```\nO(N*log(N))```\n algorithm than most efficient sorting algorithms will do - for instance quick sort, merge sort, heap sort.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How can I derive the big O notation?\r\n                \r\nI'm learning about big O notation and I need help trying to understand how the big O notation for the following program is derived:\n```\nstatic void myMethod(int[] arr, in t)\n{\n  for (int i = 0; i < n - 1; i++)\n  {\n    int variable = nextMethod(inputArray, i, n - 1);\n    int temp = inputArray[i];\n    inputArray[i] = inputArray[variable];\n    inputArray[variable] = temp;\n  }\n}\n\nstatic int nextMethod(int[] inputArray, int first, int last)\n{\n  int maximum = inputArray[first];\n  int indexOfMaximum = first;\n  for (int index = last; index > first; index--)\n  {\n    if (inputArray[index] > maximum)\n    {\n      maximum = inputArray[index];\n      indexOfMaximum = index;\n    }\n  }\n  return indexOfMaximum;\n}\n```\n\nThe answer I have been given is:\n2 (n-1) + 3(n*(n-1))/2 + 3(n -1)\n= O(n) + O(n^2) + O(n)\n= O(n^2). But I'm not sure how it is being derived... would appreciate any guidance! :)\n    ", "Answer": "\r\nStep 1)\nIn myMethod, the for-loop is rotated by n-1.\nStep 2)\nIn the nextMethod function, the first goes from last to last, but in the worst case scenario, this will be n.\nTherefore, it is done as much as (n-1)*n. (Programming Operations)\nIf you make this one axis, you will have a time complexity of n^2.\nSummary:\njust did Repeat 2 times.\nIt must be considered in the worst case by case.\nIf you have any questions, please leave comments.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation with non integer power\r\n                \r\nWhy this Big O Notation statement is considered true?\n```\nn^4 + 10000n^4.5 = O(0.0001 ∗ n^5)```\n\nIf the power of the ```\nn```\n is not an integer, do we need to round it up?\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation searching algorithm ?\r\n                \r\nI want to implement at least two different solutions to find N-th largest element with O(N*log(N)) average time complexity in Big-O notation, where N is the number of elements in the list ? \n\nwhich searching algorithm should i use in my program in java to find to n-th largest element with regards to O(N*log(N)) ? \n    ", "Answer": "\r\nActually the problem you are facing can be solve in linear time using the partitioning that is part of the ```\nquick sort```\n algorithm(Have a look here). If you really need and ```\nO(N*log(N))```\n algorithm than most efficient sorting algorithms will do - for instance quick sort, merge sort, heap sort.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O complexity notation\r\n                \r\nI have a question regarding big O notation.\nIf g(n)=O(f(n)) and h(n)=O(f(n)) is g(n)=O(h(n))?\n\nIs this allways true, sometimes true or allways false?\nThanks\n    ", "Answer": "\r\nIn words: if ```\ng```\n is bounded by ```\nf```\n and ```\nh```\n is bounded by ```\nf```\n, is ```\ng```\n bounded by ```\nh```\n?\n\nFrom this we can see that the conclusion doesn't follow from the premises. You can construct a counterexample by choosing ```\nf```\n, ```\ng```\n, and ```\nh```\n in such a way that the premises hold but the conclusion does not.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation With Separated Procedures\r\n                \r\nToday marks the first day I begin to study Algorithms and Algorithm Analysis. More specifically asymptotic analysis. But before I dive in I have one simple question that needs clarification that I cant seem to find anywhere else. Given the code snippet below what would the algorithm complexity be in Big O notation?\n```\n// Linear Computation: O(n)\n    ....\n\n//Merge Sort: O(n log(n))\n    ....\n\n//Merge Sort: O(n log(n))\n    ....\n\n//Nested for loops iterating n times each: O(n^2)\n    ....\n```\n\nMy assumption would be Big O: O(n) + (2 * O(n log (n) )) + O(n^2) but by definition of Big O do we simplify this further? Would we just call this program O(n^2) considering it is the worse of the three and can be upper bounded by a constant c * n^2?\n    ", "Answer": "\r\nWhen calculating time complexities, we compute it in terms of BIG-O. Now as our programs are huge it is not possible to compute the total of all complexities and on the other hand, there is no sense of doing it because in any expression consisting of big and small terms, if we change big terms then there will significant change in value but if we change small-term then there is no significant change. For eg take value  10000021   if we change leading one to 2, now our value will be 20000021 (huge change), now change ending 1 to 2, now our value will be 10000022 (little change).  Similarly, when the program contains n^2  it is considered instead of o(n) or O(logn) . Change in n^2 is not considered. Therefore we consider n^2.\nOrder -- n! , 2^n , n^R ..... , n^3 , n^2 , nlogn , n , logn\nConsider the maximum which is present in the program.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the big O notation of the following sentence\r\n                \r\nWhat is the big O notation of the following function:\n```\n                             n^2 + n log n2^n\n```\n\n    ", "Answer": "\r\nWe can use some identities on the expression you provided:\n      𝑛2 + 𝑛log(𝑛2𝑛)\nis:\n      𝑛2 + 𝑛[log𝑛 + log(2𝑛)]\nis:\n      𝑛2 + 𝑛[log𝑛 + 𝑛log2]\nNow in terms of asymptotic complexity, O(log𝑛 + 𝑛log2) = O(𝑛), so then the big O for the whole expression is:\n      O(𝑛2 + 𝑛2) = O(𝑛2)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big o notation -- while loops\r\n                \r\nwhat would the big o notation be for this program?\n\n```\nx = int(input(\"Enter a stride:\"))\nstart = [0, 1]\nend = int(input(\"Enter an endpoint:\")) \n\ndef xbonacci(signature, n, x): # returns all numbers in xbonacci under n\n    seq = list(signature[:n])\n    while sum(seq[-x:]) <= n:\n        seq.append(sum(seq[-x:]))\n    return seq\n\nprint(xbonacci(start, end, x))\n```\n\n\nI understand runtime for iterating through a list but in this application there will never be a set number of iterations. it always returns the sequence up to a given number but the length of the sequence changes based on x.\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation of a Binary Tree\r\n                \r\nMy textbook is saying that the Big O Notation for finding a Node in a Binary Tree is O(log2N), if ```\nN = 1```\n then log2N would be 0, which is impossible? Would this just be rounded up to ```\n1```\n or is there more to this?\n    ", "Answer": "\r\nBig-O notation is meant to describe how an algorithm's execution time (or memory consumption, or...) scales when the amount of data (or whatever ```\nN```\n describes) increases towards infinity. It's not meant to provide a precise runtime when given specific values of ```\nN```\n. With low values of ```\nN```\n, constant factors tend to dominate anyway. In this case, all you're meant to derive is that this specific algorithm's execution time scales logarithmically.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation: Nested Loop\r\n                \r\n```\n     sum = 0;\n      for (i = 0; i < m; i++)\n        for (j = 0; j < m*m; j++)\n           sum++;\n```\n\nBeen trying to figure out the time complexity of this algorithm with Big-O Notation. \nI've only found out that  1- first loop counts as O(m) and 2- Second loop counts as O(m^2). But I have a single problem. Does this nested loop counts as O(m^3) or do I have to take the fastest growing function as a basis and say that this loop counts as O(m^2) ?\n    ", "Answer": "\r\nYou have to count how many times ```\nsum++```\n will effectively be performed, and not speculate about the growth of the functions.\nEvery time the inner loop is called, it executes ```\nsum++```\n exactly m² times.\nAnd the inner loop is executed exactly m times by the outer loop, hence in total m³ ```\nsum++```\n.\nNeedless to say, O(m³).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation of 8^log2(n)\r\n                \r\nI'm confused about what the big-O notation of ```\n8^(log2(n))```\n would be.\n\nWould you just be able to change it to ```\nO(8^n)```\n since the ```\nlog2```\n would somewhat act as constant that just reduces the value of n? or would it be something else?\n\nIn a somewhat similar case what would be the big-O for ```\nlog2(n^n)```\n. would this just be ```\nO(log2(n))```\n?\n    ", "Answer": "\r\nWe'll need the following properties of logs and exponents to understand this one:\n\n\nbLogb(x) = x\n\nk . Logb(x) = Logb(xk)\n\n(xa)b = xa.b\n\n\n\n\nFor the first problem:\n\n8Log2(n)\n= (23)Log2(n) (because 8 = 23)\n= 23 . Log2(n) (using Prop#3)\n= 2Log2(n3) (using Prop#2)\n= n3 (using Prop#1)\n= O(n3)\n\n\nFor the second problem:\n\nLog2(nn)\n= n . Log2(n) (using Prop#2)\n= O(n Log(n))\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation with Multiplicative Incremented For Loop\r\n                \r\nI have a question about the Big O notation of this code:\n\n```\n    public static void printItA(int n){\n    for(int i=0; i< n; i++){\n        for (int j = 1; j<n; j*=2){\n            System.out.println(\"Something\");\n        }\n      }\n    }\n```\n\n\nIt seems to me like the big O should be O(n^2), but the j*=2 part of the for loop confuses that for me. Can anyone clear this up for me?\n    ", "Answer": "\r\nTake an example of N = 16\n\nYour outer loop will iterate 16 times.  ```\nO(N)```\n i.e. linearly with your input. \n\nYour inner loop will iterate 4 (1, 2, 4, 8, 16) times for each of outer loop. Your inner loop reduces the work by 2 on each iteration. ```\nO(log n)```\n\n\nSo final runtime complexity ```\nO(n * log(n))```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Find the running time in Big O notation\r\n                \r\n```\n1)  for (i = 1; i < n; i++) {                         > n\n2)      SmallPos = i;                                 > n-1\n3)      Smallest = Array[SmallPos];                   > n-1\n4)      for (j = i+1; j <= n; j++)                    >  n*(n+1 -i-1)??\n5)          if (Array[j] < Smallest) {                > n*(n+1 -i-1 +1)  ?? \n6)              SmallPos = j;                         > n*(n+1 -i-1 +1)  ?? \n7)              Smallest  = Array[SmallPos]           > n*(n+1 -i-1 +1)  ??     \n            }                                         \n8)      Array[SmallPos] = Array[i];                   > n-1\n   9)                Array[i] = Smallest;             > n-1\n                                     }\n```\n\n\ni know the big O notation is n^2 ( my bad its not n^3) \n\ni am not sure between  line 4-7 anyone care to help out? \nim not sure how to get the out put for the second loop since j = i +1 as i changes so does j \n\nalso for line 4 the ans suppose to be n(n+1)/2 -1 i want to know why as i can never get that\n\ni am not really solving for the big O i am trying to do the steps that gets to big O as constant and variables are excuded in big O notations. \n    ", "Answer": "\r\nI would say this is O(n^2) (although as Fred points out above, O(n^2) is a subset of O(n^3), so it's not wrong to say that it's O(n^3)).\n\nNote that it's generally not necessary to compute the number of executions of every single line; as Big-O notation discards low-order terms, it's sufficient to focus only on the most-executed section (which will typically be inside the innermost loop).\n\nSo in your case, none of the loops are affected by the values in ```\nArray```\n, so we can safely ignore all that.  The innermost loop runs ```\n(n-1) + (n-2) + (n-3) + ...```\n times; this is an arithmetic series, and so has a term in n^2.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation of a certain algorithm?\r\n                \r\nI have the following algorithm that determines the greatest common divisor of two numbers x and y. I need to find the big o notation that describes this algorithm and explain why, but I have no idea how to do this. \n\nCould someone please look at my code and explain what type of big oh notation it would be?\n\n```\n     public void question1(int x, int y){\n            ArrayList divisorx = new ArrayList(); //the list of divisors of number x\n            ArrayList divisory = new ArrayList();//divisors of number y\n            ArrayList answerSet = new ArrayList();//the common divisors from both   \n            //divisorx and divisory\n\n            for(int i=1; i<=x; i++){//this loop finds the divisors of number x and \n                                    //adds them to divisorx\n                    double remainder = x%i;\n                    if(remainder==0){\n                        //i is a divisor\n                        divisorx.add(i);\n                    }\n            }\n            for(int i2=1; i2<=y; i2++){//this loop finds the divisors of number y \n                                       //and adds them to divisory\n                    double remainder2 = y%i2;\n                    if(remainder2==0){\n                        //i2 is a divisor\n                        divisory.add(i2);\n                    }\n            }\n      int xsize = divisorx.size();\n      int ysize = divisory.size();\n\n            for(int i=0; i<xsize; i++){//this massive loop compares each element of \n        //divisorx to those of divisory to find common divisors. It adds those common\n        //divisors to the arraylist answerSet\n               for(int j=0; j<ysize; j++){\n                   if(divisorx.get(i)==divisory.get(j)){\n                       //common divisor has been found\n                       //add it to an answer array\n\n                       answerSet.add(divisorx.get(i));\n\n                   }\n                }\n            }\n    Collections.sort(answerSet);//sorts the answerSet from smallest to greatest\n\n    Object gcd = answerSet.get(answerSet.size()-1);//get the last element of the\n                                                   //arraylist, which is the gcd       \n    System.out.print(\"Your Answer: \"+gcd);//print out the greatest common divisor\n }\n```\n\n    ", "Answer": "\r\nFirst two loops have cost O(X) and O(Y), respectively.\n\nNumber of divisors of N is O(sqrt(N)) (see comments), so xsize and ysize are O(sqrt(X)) and O(sqrt(Y)).\n\nYour last loop therefore has cost O(sqrt(X).sqrt(Y)).\n\n```\nanswerSet```\n has size O(min(sqrt(X),sqrt(Y))) since it is the intersection of ```\ndivisorx```\n and ```\ndivisory```\n.\n\nYou perform a sort on answerSet, which is O(min(sqrt(X),sqrt(Y)) log(min(sqrt(X),sqrt(Y)))\n\nAll of those are O(X+Y), so total complexity is O(X+Y).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How can I express this in Big O notation?\r\n                \r\nHow can I express ```\n2(3(logn-1))```\n in Big O notation?\nIs it ```\n2n```\n or ```\n2log n```\n?\n    ", "Answer": "\r\nThis is one case where the basis of the logarithm matters. So lets say, the basis of your logarithm is ```\na```\n. You can change it to base 3 by\n\n\nlogan = log₃n / log₃a\n\n\nNow you can simplify the exponent\n\n\n3logan - 1 = 3log₃n / log₃a - 1 = n1/log₃a / 3\n\n\nSo in total you get\n\n\n2n1/log₃a / 3 = 2n1/log₃a / 3 ⋅ 21/3 ∈ O(2n1/log₃a)\n\n\nIf ```\na = 3```\n the complexity would be ```\nO(2ⁿ)```\n. If ```\na = 2```\n, the complexity would be ```\nO(2nc)```\n, with\n```\nc = 1/log₃2 ≈ 1.5850```\n.\n\nNotice: ```\n2nc = 2(nc) ≠ (2n)c = 2cn```\n. So you cannot simplify the complexity more.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for g(n) > h(n)\r\n                \r\nI've got this function:\n\n```\nf(n) = g(n) + h(n)\ng(n) > h(n)\n```\n\n\nIs this result always correct for Big O notation? ```\nO(g(n))```\n\n\nThank you.\n    ", "Answer": "\r\nYes, it is correct, because ```\ng(n) + h(n) < g(n) + g(n) <= 2*g(n)```\n, so you found a constant ```\nC=2```\n such that ```\nf(n) <= C*g(n)```\n (for large enough values of ```\nn```\n), and by definition of big O, it means ```\nf(n)```\n is in ```\nO(g(n))```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big - O notation linear and binary search [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs details or clarity. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Add details and clarify the problem by editing this post.\r\n                \r\n                    \r\n                        Closed 7 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nIn terms of Big - O notation the linear search is a x^n, but what is the binary search? I am not 100% that the linear search is correct.\n    ", "Answer": "\r\nLinear search means that you will have to iterate through the list of elements until you find the element that you were looking for.\n\nFor instance, if you have a list with elements ```\n[1, 3, 5, 7, 9, 11]```\n and you are looking for 11 you will start by the first element, then the second element, and so on, which in this case will take 6 iterations.\n\nGenerally, we could say that in the worst case you will have to traverse the whole list; so it will take n iterations, where n is the number of elements on the list.\n\nSo we say that the linear search algorithm is ```\nO(n)```\n.\n\nIn the case of binary search, you start on the middle element of the list:\n\n\nCase 1: the number we are searching is the same as the number on the middle element: we are done!\nCase 2: the number we are searching is smaller: we will only search the elements that precedes the middle element.\nCase 3: the number we are searching is bigger: we will only search on the subsequent elements.\n\n\nIn our example, the number we are searching is 11 and the middle element is 5; since 11 > 5, we will only search on the sublist of the elements bigger than 5, namely ```\n[7, 9, 11]```\n.\n\nNow, we will keep doing the same until we find the element that we are searching, in this case it takes only three iterations to get to the last element.\n\nIn general this approach takes ```\nlog(n)```\n iterations; therefore, the algorithm is ```\nO(log(n))```\n.\n\nNote that the latter only works for sorted lists.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Find big O notation with Rabin-Miller implementation\r\n                \r\nI’ve got a question about trying to decide the big O notation for a Rabin-Miller algorithm I have implemented. \n\nWhen generating a \n\n```\n 512 bits prime, I get a runtime of   22 seconds,\n1024 takes                           237 seconds,\n2048 takes                          2942 seconds.\n```\n\n\nHow can I determine the big O notation for these values? It seems to me that the runtime increases by roughly 10 times everytime the bitsize increases by 2. Does that mean that it’s ```\nO(10n)```\n? \n    ", "Answer": "\r\nYou have too few points to estimate the function (and ```\nO(function)```\n as well) from the experiment.\n\n```\n   x | f(x)\n-----------\n 512 |   22\n1024 |  237\n2048 | 2942\n```\n\n\nIf we test ```\nO(n)```\n (```\nO(10n)```\n is in fact ```\nO(n)```\n) as ```\nf(x) = Ax + B```\n guess with a help of Least Squares method, we'll get a good fit\n\n```\nA =     2.0\nB = -1330.5\nR =     0.964   (Correlation)\n```\n\n\nHowever, many alternative functions have better support\n\n```\nf(x) = Ax**4 + B  with correlation R = 0.99990 <- actual best fit\nf(x) = Ax**3 + B         -/-       R = 0.99937 <- expected\nf(x) = Ax**2 + B         -/-       R = 0.99232\n```\n\n\nyou want more points to find out the right function: when having three values only ```\nAx**4 + B```\n (which corresponds to ```\nO(x**4)```\n) is the leader so far, but we can't reject the expected complexity which is ```\nAx**3 + B```\n. \n\nFinally, we can guess (we can't conclude with three points only) that the implementation is suboptimal: ```\nO(x**4)```\n instead of expected ```\nO(x**3)```\n\n\nIf our guess ```\nO(x**4)```\n is correct one, than we might expect that doubling ```\nx```\n: ```\nx -> 2 * x```\n we increase the time ```\n16-fold```\n (```\n2**4 == 16```\n) \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation? Would that be O(n)?\r\n                \r\nWhat would be the Big O and Big Omega notation? \n\n```\nint temp = 0\nfor (i =0; i < 100000000; i++)\n         for (j = 0; j < N; j++)\n                   temp = 0\n                   while(temp < j)\n                   {\n                           temp++;\n                   }\n```\n\n    ", "Answer": "\r\nThat thing is O(n^2), the innermost while runs from 0 to ```\nj```\n, who's upper bound is ```\nN```\n. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Discrete Math Big O Notation\r\n                \r\nI'm studying for my discrete math class and I'm starting to grasp the idea of big O notations a little better and was successful in proofing a few question using the definition of f(x) is O(g(x)).\n\nHow do I proof that question is not big O using the definition. Please provide step by step answer as you would on a test to receive full marks and please explain why you did each step in simple terms!\n\nHere are 2 example questions: \n\n1) 1 is not big O(1/x)\n\n2) e^x is not O(x^5) Big O\n    ", "Answer": "\r\n\n  1) 1 is not big O(1/x)\n\n\nTo show that ```\n1```\n is not O(```\n1/x```\n), we must show that for any constant ```\nc```\n, there is no ```\nx_0```\n such that ```\n1 <= c*1/x```\n for all ```\nx >= x_0```\n. Suppose 1 is big O of 1/x. We take ```\nc = c_0 > 0```\n, a constant. Then we must have ```\n1 <= c_0*1/x```\n for ```\nx >= x_0```\n. Assuming ```\nx > 0```\n, we can solve and get ```\nx <= c_0```\n. This cannot be true for all ```\nx >= x_0```\n (it fails for the first number that is greater than or equal to ```\nmax(x_0, c_0)```\n), so our assumption was wrong and the first is not big O of the second.\n\n\n  2) e^x is not O(x^5) Big O\n\n\nTo show that ```\ne^x```\n is not O(```\nx^5```\n), we must show that for any constant ```\nc```\n, there is no ```\nx_0```\n such that for all ```\nx >= x_0```\n, ```\ne^x <= c*x^5```\n. Suppose ```\ne^x```\n is big O of ```\nx^5```\n. We take ```\nc = c_0 > 0```\n, a constant. Then we must have ```\ne^x <= c_0*x^5```\n for ```\nx >= x_0```\n. We can rearrange this to obtain ```\ne^x / x^5 <= c_0```\n for all ```\nx >= x_0```\n. However, the limit of ```\ne^x / x^5```\n as ```\nx -> +inf```\n tends to ```\n+inf```\n; we can see this by iterating l'Hopital's rule:\n\n```\n        e^x            e^x                  e^x\nlim     ---  = lim     ---  = ... = lim     --- = +inf\nx->+inf x^5    x->+inf 5x^4         x->+inf 120\n```\n\n\nThis is a contradiction since there is no constant ```\nc_0```\n greater than or equal to infinity. Therefore, our assumption was wrong and the first is not big O of the second.\n\nNote: I write ```\nlim = +inf```\n to mean something like \"the value of the expression grows without bound\".\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation with function composition\r\n                \r\nI'm trying to determine the Big O notation of the following function\n\n```\npublic static int f10(int n)\n{\n    return f1(n*n*n)\n}\n```\n\n\nWhere f1() is given by:\n\n```\npublic static int f1(int n)\n{\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        for (int j = 0; j < n; j++)\n            sum++;\n        for (int j = 0; j < n; j++)\n            sum++;\n        for (int j = 0; j < n; j++)\n            sum++;\n    }\n    return sum;\n}\n```\n\n\nI can see that O(f1) is O(n^2), but when we call f10, does this become O(n^6) because the size of n is being cubed before calling f1?\n\nI understand the complexity of f1 is not changing from it's own perspective, but is it from f10's perspective of 'n'? \n    ", "Answer": "\r\nLet's analyse ```\nf1()```\n:\n\n```\nfor (int i = 0; i < n; i++) -> O(n)\n\nfor (int j = 0; j < n; j++)\nfor (int j = 0; j < n; j++)\nfor (int j = 0; j < n; j++) -> O(n)\n\nfor (n-times) { O(n) } -> O(n^2)\n```\n\n\nSo ```\nf1()```\n is O(n^2). It's just two nested loops. But because ```\nf1()```\n is called with n^3, make ```\nf10()```\n indeed O(n^6).\n\nHowever, the above complexity order is theoretical. In practice it may depend on how you call ```\nf10()```\n and/or what optimizations the compiler makes. A smart C compiler could replace ```\nf1()```\n with a simple and O(1) arithmetic expression.\n\nThen, having ```\nf1()```\n reduced to an expression, the compiler could replace calls like ```\nf10(42)```\n with the result, doing all calculations at compile-time.\n\nDo you see what I mean; how would you simplify ```\nf1()```\n to a simple O(1) expression?\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big o notation from formula\r\n                \r\nWhen you have a formula like this : 3n2 + 10n.log(n) + 1000n + 4.log(n) + 9999.\n\nDo you pick the fastest growing function for the big-o-notation? In this case big  O of n2. \n\nCan someone help me understand what this is asking? Suppose you have a computer that requires 1 minute to solve problem instances of size n = 1,000. Suppose you buy a new computer that runs 1,000 times faster than the old one. What instance sizes can be run in 1 minute, assuming the following time complexities T(n) for our algorithm?          \n\n\nT(n) = n\nT(n) = n3\nT(n) = 10n\n\n    ", "Answer": "\r\nLet's say that T(n) is the time required to perform n operations (op).\n\nWe know that T(1000) = 1' in the old machine and T(1000) = 1'/1000 in the new one (because the new one is 1000 faster.)\n\na) T(n) = n\n\nThis means 1000 op in 1' in the old machine. In the new machine (1000 times faster) is 1000 op in 1'/1000. So 1000000 op in 1' in the new machine, and the answer is 1 million. In other words, the new machine will compute 1000000 operations in 1' (this is 1000 times more operations per minute.)\n\nb) T(n) = n^3\n\nHere we have 1000^3 op in 1' (old machine) and 1000^3 op in 1'/1000 (new machine). So, in the new machine: 1000^3 1000 op in 1', or 1000^3 10^3 op in 1', i.e., 10000^3 op in 1', and the answer is ten thousand.\n\nc) T(n) = 10n\n\nWe have 10 * 1000 op in 1' (old), 10 * 1000 op in 1'/1000 (new). So, 10 * 1000000 op in 1', and the answer is again 1 million.\n\nAs we can see the answers for T(n) = n and T(n) = 10 n are the same. In fact, they would also be equal to the answer for T(n) = C n, no matter the value of C > 0. To see this, we only have to replace 10 with C above:\n\nC 1000 op in 1' (old), C 1000 op in 1'/1000 (new) or C 1000000 in 1', and the answer is 1000000.\n\nThis is why we talk about O(n), O(n^2), O(nˆ3), etc., regardless of the constant hidden inside the O.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation of a method that calls a method of O(n)\r\n                \r\n```\npublic boolean equal(MyLinkedList a, MyLinkedList b) {\n    String instanceA = encode(a);\n    String instanceB = encode(b);\n    return instanceA.equals(instanceB);\n}\n```\n\nI was working on a project and looked at my code and realized I didn't know the time complexity rules of calling methods.\nWhat would be the big-O notation of my equal method if the encode(String value) is of O(n)? Would it take into consideration the big-o notation of O(n) or would it be a simple O(1)?\n    ", "Answer": "\r\nIt would be O(n) in the worst case. If you think about it, it boils down to estimating the time complexity of testing for equality, the encodings do not add additional complexity to the problem.\nThe complexity of verifying that two strings are equal is most of the times < O(n). For example, the ```\nequals```\n method could simply compare the length of the strings before actually comparing the contents of the strings, which has constant complexity. If the strings are of equal length, then the ```\nequals```\n method could traverse the whole strings, thus O(n).\nIn CS stackexchange, the subject is discussed in more detail. Check it out here: https://cs.stackexchange.com/questions/127899/what-is-the-expected-time-complexity-of-checking-equality-of-two-arbitrary-strin\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation for a recursive function\r\n                \r\nThis question has been asked a few times before, but I'm still having trouble finding the answer for this problem because it is more complex than the simple examples provided in the other questions. \n\nI'm trying to find the big O notation for:\n\n```\nfloat foo(int start, int end, int s)\n{\n  if (start == end) {\n    return start*start;\n  } else {\n    int iSegment = (end - start + 1) / s;\n    int iCurrResult = 0;\n    for (int i=0; i<=s; i++)\n      iCurrResult += foo (start + i*iSegment, start + (i+1) * iSegment - 1, s);\n    return iCurrResult*iCurrResult;\n  }\n}\n```\n\n    ", "Answer": "\r\nEach time the function is called, it invokes itself recursively ```\ns+1```\n times, with a range of size ```\nn/s```\n (```\nn```\n is the number of elements).\nThis gives you the complexity function:\nT(n) = (s+1)T(n/s) + 1\nAccording to Wolphram Alpha, this function is in ```\nO((s+1)^(logn/logs)) = O((s+1)^log(n-s))```\n\nNote that this is in ```\nOmega(n)```\n, since ```\nn=s^log_s(n)=s^log(n)/log(s)```\n, and it is easy to see that this function is strictly (and asymptotically) bigger than ```\ns^(logn/logs)```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation for if statements?\r\n                \r\nI was wondering what the Big O notation for this would be. I know the for loop is O(n). I wasn't sure if the if statements were O(n log n). If so, doesn't that make the run time complexity (n)*((n log n)^3). Or would it be ((n^2)(log^3n)) ? Also I know storage in an array is O(n) and was wondering if calling elements in a the same array is O(n) or had a different run tim complexity. (Written in Java eclipse)\n\n```\n    for (i=0;i<numberOfProblems;i++){                       \n    String string1= ap.nextString(\"P or NP?\");\n    if(string1=P){\n        pOrNPValue[i]=0;\n    }else{\n        pOrNPValue[i]=1;\n\n\n        String string2 = ap.nextString(\"Best Case Run Time?\");\n\n        if(string2==ok){\n            bestCaseValue[i]=0;\n        }else if(string2=oLogLogN){\n            bestCaseValue[i]=1;\n        } else if(string2=oLogN){\n            bestCaseValue[i]=2;\n        }else if(string2=oNC){\n            bestCaseValue[i]=3;\n        }else if(string2=oN){\n            bestCaseValue[i]=4;\n        }else if(string2=oNLogStarN){\n            bestCaseValue[i]=5;\n        }else if(string2=oNLogN){\n            bestCaseValue[i]=6;\n        }else if(string2=oNK){\n            bestCaseValue[i]=7;\n        }else if(string2=oCN){\n            bestCaseValue[i]=8;\n        }else if(string2=oNFactorial){\n            bestCaseValue[i]=9;\n        }\n\n        String string3 = ap.nextString(\"Average Case Run Time?\");\n\n        if(string3=ok){\n            averageCaseValue[i]=0;\n        }else if(string3=oLogLogN){\n            averageCaseValue[i]=1;\n        } else if(string3=oLogN){\n            averageCaseValue[i]=2;\n        }else if(string3=oNC){\n            averageCaseValue[i]=3;\n        }else if(string3=oN){\n            averageCaseValue[i]=4;\n        }else if(string3=oNLogStarN){\n            averageCaseValue[i]=5;\n        }else if(string3=oLogLogN){\n            averageCaseValue[i]=6;\n        }else if(string3=oNK){\n            averageCaseValue[i]=7;\n        }else if(string3=oCN){\n            averageCaseValue[i]=8;\n        }else if(string3=oNFactorial){\n            averageCaseValue[i]=9;\n        }\n\n        String string4 = ap.nextString(\"Worst Case Run Time?\");\n\n        if(string4=ok){\n            worstCaseValue[i]=0;\n        }else if(string4=oLogLogN){\n            worstCaseValue[i]=1;\n        } else if(string4=oLogN){\n            worstCaseValue[i]=2;\n        }else if(string4=oNC){\n            worstCaseValue[i]=3;\n        }else if(string4=oN){\n            worstCaseValue[i]=4;\n        }else if(string3=oNLogStarN){\n            worstCaseValue[i]=5;\n        }else if(string4=oLogLogN){\n            worstCaseValue[i]=6;\n        }else if(string4=oNK){\n            worstCaseValue[i]=7;\n        }else if(string4=oCN){\n            worstCaseValue[i]=8;\n        }else if(string4=oNFactorial){\n            worstCaseValue[i]=9;\n```\n\n    ", "Answer": "\r\nString comparisons take time proportional to the length of the longer of the two strings (you only need to compare characters up to that point in the worst-case).  Since all of the string comparisons that are being made here are against constant strings, each comparison individually takes time O(1).  Since there's only a fixed number of comparisons, each of which does O(1) work if true (array accesses take time O(1) regardless of the index), the total time required for all the comparisons is O(1) per iteration.  Thus the total amount of work done is O(n): there are O(n) loop iterations, and each of them does O(1) work.\n\n(Technically speaking, you need to account for the work done reading characters from the user, which could be unbounded because the user could just hold down a key indefinitely.  I'll ignore that for now by assuming there's a fixed limit to the total number of characters the user can type at each prompt.)\n\nIn general, comparisons themselves take O(1) time and the real question is how much work is required to evaluate the boolean expression.\n\nHope this helps!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Understanding the Big O Notation O(2^N)\r\n                \r\nI am trying to understand how the following recursive function for calculating the Fibonacci series falls under the notation O(2^N). \n\n```\nint fibo(int num)\n{\n    if (num <= 1) return num;\n    return fibonacci(num - 2) + fibonacci(num - 1);\n}\n```\n\n\nFor example, if we consider the finding the Fibonacci series for the number \"5\", the fibo method is invoked 15 times. How do we say that it falls under the notation O(2^N)?\n\n```\n                             fibo(5)\n                        --------------------\n                        /                  \\\n                  fibo(3)                   fibo(4)\n                ------------              -------------  \n                  /       \\               /           \\\n            fibo(2)        fibo(1)   fibo(3)           fib0o(2)\n      ----------------             ----------          -------------\n       /            \\              /        \\           /         \\\n   fibo(1)          fibo(1)  fibo(2)        fibo(1)  fibo(1)      fibo(0)\n                            ---------\n                            /       \\\n                       fibo(1)       fibo(0)\n```\n\n\nI know my question is trivial. Please consider me as a novice, trying to learn Big-O notation.\n    ", "Answer": "\r\nBig Oh provides you a upper bound for the runtime of your algorithm. That is, \nyou must read fib(n) in O(2^n) as stating that your algorithm perform at most 2^n steps to return a result. Sometimes, upper bounds are not that precise (this is the case). You also can say that fib is in O(n!), that is another upper bound (a really bad one).\n\nFor stating the precise runtime of your algorithm you have to use Theta notation, in this case fib is Theta(Phi^n) where Phi is the golden ratio.You can prove this by induction.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Calculating Big O notation for stack problem using pointer\r\n                \r\nHi can anyone help me to count the algorithmic complexity of this code using Big O notation? I'm not too understand in using Big O because so there are many pointers in this code. I just know some of the code. Like cout is O(1). The rest i don't understand. I'm just a beginner at programming. Please help me to count the big o notation. Thanks.\n\n```\n#include <iostream>\n#include <string>\n#include <stdlib.h>\n\nusing namespace std;\n\nclass Book {\n    int year, page;\n    unsigned long long int code;\n    string language, name, title;\n    Book *head, *next, *prev, *link;\n\npublic:\n    Book (string & name, string & title, unsigned long long int code, string & language, int year, int page) {\n        head = NULL;\n        this->name = name;\n        this->title = title;\n        this->language = language;\n        this->code = code;\n        this->year = year;\n        this->page = page;\n    }\n\n    ~Book (void) {\n        delete head;\n    }\n\n    void display (void);\n    void add (void);\n    void dellete (void);\n};\n\nvoid Book::add(void) {\n    string name, title, language;\n    int year, page;\n    unsigned long long int code;\n    cout << \"Add a book...\";\n    cout << endl << \"Author\\t\\t:\", cin >> name;\n    cout << \"Title\\t\\t:\", cin >> title;\n    cout << \"ISBN(13 digits)\\t:\", cin >> code;\n    cout << \"Language\\t:\", cin >> language;\n    cout << \"Year\\t\\t:\", cin >> year;\n    cout << \"Pages\\t\\t:\", cin >> page;\n\n    Book* p = new Book(name, title, code, language, year, page);\n    p->next = head;\n    head = p;\n}\n\nvoid Book::dellete(void) {\n    string name, title, language;\n    int year, page;\n    unsigned long long int code;\n    Book* p, *prev, *next;\n\n    if(head==NULL) {\n        cout << \"There is no book in the stack\\n\";\n    } else if(head->next==NULL) {\n        p = head;\n        head = NULL;\n        free(p);\n        cout << \"All book has been taken. Now the stack is empty\\n\";\n    } else{\n        p = head;\n        head = p->next;\n        free(p);\n        cout << \"A book has been taken\\n\";\n    }\n}\n\nvoid Book::display(void) {\n    Book* p = head;\n    cout << \"Displaying book(s)...\\n\";\n    while (p) {\n        cout << \"----------------------------- \\n\";\n        cout << \"Author\\t\\t:\" << p->name << endl;\n        cout << \"Title\\t\\t:\" << p->title << endl;\n        cout << \"ISBN\\t\\t:\" << p->code << endl;\n        cout << \"Language\\t:\" << p->language << endl;\n        cout << \"Year\\t\\t:\" << p->year << endl;\n        cout << \"Pages\\t\\t:\" << p->page << endl;\n        cout << endl;\n        p = p->next;\n    }\n}\n\nint main (int argc, char const** argv) {\n    string blank = \"\";\n    Book* B = new Book(blank, blank, 0, blank, 0, 0);\n    int opt;\n    for (;;) {\n        cout << \"----------------------------- \\n\";\n        cout << \"1) Add a book.\\n\";\n        cout << \"2) Show all books.\\n\";\n        cout << \"3) Take a book\\n\";\n        cout << \"4) Exit. \\n\";\n        cout << \"Don't use space but use underscore...\\n\\n\";\n\n        cout << \"Options:\", cin >> opt;\n        switch (opt) {\n            case 1:\n                B->add();\n                break;\n            case 2:\n                B->display();\n                break;\n            case 3:\n                B->dellete();\n                break;\n            case 4:\n                exit(0);\n            default:\n                continue;\n        }\n    }\n    return 0;\n}\n```\n\n    ", "Answer": "\r\nThe O-Notation classifies an algorithm in how complex it gets (in means of e.g. runtime or memory usage) dependent on the size of the problem. So O(1) means no matter how big the problem is, the algorithm doesn't grow in complexity, no matter how big this constant cost is.\n\nLet's view some of your program parts.\n\nDelete\n\nThis has a runtime complexity of O(1), because no matter how big the stack of Books is, it is always nearly the same amount of operations you do for deleting the top of the book stack. The only difference is between 0, 1 and 2 books, but if you grow the stack to infinity the amount of operations doesn't grow, and that is what matters.\n\nAdd\n\nIt is difficult to measure here. Since this method only adds 1 book at a time it is O(1), because no matter how many books there are already (it's the only variable size) you need always the same amount of operations. It would be more interesting if you would allow adding multiple books at once.\n\nDisplay\n\nSo display prints out all the books on the stack. If you grow the amount of books the number of operations also arises. Now the question is in what manner? In this case doubling the amount of books doubles the amount of instructions. This is a linear growth and thus the complexity class is O(n).\n\n\n\nIt can be helpful to view the loop count. One loop over the problem size often does mean O(n). If you have two nested loops (over the problem size) you often have O(n²) and so on. \n\nTo your question what the endless loop in your main function is, well it depends on what you define as your problem size, I don't know if it makes sense to measure it here.\n\nIf you define the number of user actions in total as problem size it gets complicated. If we let out the display part and only allow add and delete, it is O(n), because everything would be constant (since add and delete are O(1) and the other things are independent instructions like cout) and they occur in a loop based on the problem size n (the number of user actions). If you take display into account, it isn't that simple, because display has an O(m) complexity (m = Number of books) and this is highly dependant on the actual user input which was given before. I don't know what would be the complexity there.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation changing the power\r\n                \r\nI want to Express each of the following functions using Big-O notation.\na(n) = 2n + 3n^2 + nlog(n)\nb(n) = 5nlog(n) + 10n^3 + n^2\nfor a(n) I assumed that the answer would be O(n^2) However apparently it is O(n^3)\nthis is the same for b(n) where I assumed the notation would be O(n^3) however it is O(n^4). Is it a rule to round up the power when writing the notation? Why would this be the case? Isn't the notation supposed to take the upper-bound?\n    ", "Answer": "\r\nYou are right, a(n) = O(n2) and b(n) = O(n3).\nHowever, notice that a(n) is also O(n3) and indeed O(n1000). Usually though, we want to express the tightest bound we can find.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation for simple validation\r\n                \r\nI need to derive the big-O notation of this validation program. Its job is to accept product entries of this type: 'jacket,8,12,18,16,6', validates it, sort the sizes, sort the entry into a list by alphabetical order and print the new list after each entry.\n\nThe Big-O notation is based on the worst-case scenario, which is when a program takes the longest time to execute for a particular input. Taking ‘parseData’ method, the worst-case is when the input is fully valid, therefore none of the exceptions are passed and all the method is executed. The product name has 15 characters and 5 sizes are entered. In this scenario, this method and the other smaller validation methods below will always take the same period to execute this worst case event. This gives them O(1) complexity as indicated in the comments.\n\nIn the main, there is; - validation - O(n)\n                       - sorting - O(nlogn)\n                       - printing - O(n^2)\n\nWould this boil down to O(n^2)? Or does it depend on the number of entries considered?\n    ", "Answer": "\r\nfor asymptotic analysis it would boil down to O(n^2), as it grows much faster than O(n logn) and O(n). but be aware that this is just an asymptotic upper bound, that means it might be not really tight, also as you say, its worst case and not average or expected case.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Complexity of arithmetic mean in big O notation\r\n                \r\nWhat is the complexity of calculating the arithmetic mean in big O notation? Please also add a short explanation how you get to the result.\n    ", "Answer": "\r\nAssuming, you are referring to the mean here: https://en.wikipedia.org/wiki/Arithmetic_mean\n\nIt is always going to be O(N). Reason? Simply put, calculating a mean includes doing a summation of all the elements in your set and the further dividing it by N. Now if you take simplest approach and do as stated then you have to iterate through each element in the set. \n\nNow, let's assume that you are being a little smarter about it and doing a divide and conquer on that meaning you divided the set into smaller sets, and then claculated mean and then did a mean of the ```\ncalculated means```\n. In this case it could be O(N/2) + O(N/2) + O(LogN)[which is the cost to calculate mean of the means] = O(N + LogN). In this case, doing the simple mean would be cheaper O(N).\n\nHopefully that makes sense.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation, Complexity\r\n                \r\nSo we're just beginning Big O notation, and we have a question asking this:\n\nWhat is the worst time complexity for the following loop, if ```\nsomeWork```\n has complexity of ```\nO(i)```\n, noting that this means that ```\ni```\n is the loop counter, so the steps of someWork increases every time the counter does:\n\n```\nwhile(i < n)\n    someWork(...)\n    i <-- i + 2\n```\n\n\nThis is obviously written in pseudocode, and I've gotten the easier questions, so I don't believe I have a problem understanding the question, it's this specific one that I'm stuck on. Can I get some help, please?\n\nThanks so much in advance!\n    ", "Answer": "\r\nGiven that ```\nsomeWork()```\n is dependent on ```\ni```\n, and ```\ni```\n is, on average, roughly ```\nn/2```\n over all the outer loop iterations, the time complexity is therefore ```\nO(n2)```\n.\n\nThat's because the outer loop depends on ```\nn```\n and ```\nsomeWork()```\n (an \"inner loop\" of some description) also depends on ```\nn```\n.\n\nThe reasoning behind ```\nsomeWork()```\n being ```\nO(n)```\n is as follows.\n\nLet's say ```\nn```\n is 8. That means ```\ni```\n takes the values ```\n{0, 2, 4, 6}```\n, an average of ```\n12 / 4 == 3```\n.\n\nNow let's say ```\nn```\n is 16. That means ```\ni```\n takes the values ```\n{0, 2, 4, 6, 8, 10, 12, 14}```\n, an average of ```\n56 / 8 == 7```\n.\n\nNow let's say ```\nn```\n is 32. That means ```\ni```\n takes the values ```\n{0, 2, 4, ..., 28, 30}```\n, an average of ```\n240 / 16 == 15```\n.\n\nIf you continue, you will find that the number of operations performed by ```\nsomeWork()```\n is always ```\nn / 2 - 1```\n hence ```\nO(n)```\n.\n\nThat, in combination with the loop itself being ```\nO(n)```\n, gives you the ```\nO(n2)```\n complexity.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation, Complexity\r\n                \r\nSo we're just beginning Big O notation, and we have a question asking this:\n\nWhat is the worst time complexity for the following loop, if ```\nsomeWork```\n has complexity of ```\nO(i)```\n, noting that this means that ```\ni```\n is the loop counter, so the steps of someWork increases every time the counter does:\n\n```\nwhile(i < n)\n    someWork(...)\n    i <-- i + 2\n```\n\n\nThis is obviously written in pseudocode, and I've gotten the easier questions, so I don't believe I have a problem understanding the question, it's this specific one that I'm stuck on. Can I get some help, please?\n\nThanks so much in advance!\n    ", "Answer": "\r\nGiven that ```\nsomeWork()```\n is dependent on ```\ni```\n, and ```\ni```\n is, on average, roughly ```\nn/2```\n over all the outer loop iterations, the time complexity is therefore ```\nO(n2)```\n.\n\nThat's because the outer loop depends on ```\nn```\n and ```\nsomeWork()```\n (an \"inner loop\" of some description) also depends on ```\nn```\n.\n\nThe reasoning behind ```\nsomeWork()```\n being ```\nO(n)```\n is as follows.\n\nLet's say ```\nn```\n is 8. That means ```\ni```\n takes the values ```\n{0, 2, 4, 6}```\n, an average of ```\n12 / 4 == 3```\n.\n\nNow let's say ```\nn```\n is 16. That means ```\ni```\n takes the values ```\n{0, 2, 4, 6, 8, 10, 12, 14}```\n, an average of ```\n56 / 8 == 7```\n.\n\nNow let's say ```\nn```\n is 32. That means ```\ni```\n takes the values ```\n{0, 2, 4, ..., 28, 30}```\n, an average of ```\n240 / 16 == 15```\n.\n\nIf you continue, you will find that the number of operations performed by ```\nsomeWork()```\n is always ```\nn / 2 - 1```\n hence ```\nO(n)```\n.\n\nThat, in combination with the loop itself being ```\nO(n)```\n, gives you the ```\nO(n2)```\n complexity.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Nested Loops in Big-O Notation?\r\n                \r\nMaybe I'm mistaken in my understanding of Big-O notation (it has been a while since I've taken a course on algorithms) but the following has never made too much sense to me:\n\nThis would be considered O(n^2):\n\n```\nfor (int i = 0; i < num_1; i++)\n{\n    for (int j = 0; j < num_2; j++) \n    {\n        cout << i << \" \" << j << endl;\n    }\n}\n```\n\n\nThis would be considered O(n):\n\n```\nfor (int z = 0; z < num_3; z++) { cout << z << endl; }\n```\n\n\nMy issue is when it comes to practical terms.  Lets assume that ```\nnum_1 = 10; num_2 = 20; num_3 = 1000;```\n.  In this case the first example, an O(n^2), would run considerably less iterations of it's interior than the O(n) second example.\n\nIn more general terms: when ```\nnum_3 > num_1 * num_2```\n then the O(n^2) snippet does less than the O(n) snippet.  In real world applications, these two snippets may be doing two very separate tasks where there are functional bounds on ```\nnum_1```\n, ```\nnum_2```\n, and ```\nnum_3```\n are considerably different.  The nested ```\nnum_1```\n and ```\nnum_2```\n may be looping variable values between 0 and 255 but ```\nnum_3```\n may frequent values above a million.\n\nWhy should/would a coder trust an algorithm or snippet based on its Big-O notation when it doesn't take into consideration the practical or operational variable boundaries?\n    ", "Answer": "\r\nSaying that something is in ```\nO(n^2)```\n only makes sense if it is clear what `n´ is supposed to be. Usually it refers to the size of the input (or if the input is a number, it just refers to that number), but in your code, it's not clear what the input is.\n\n```\nfor (int i = 0; i < num_1; i++)\n{\n    for (int j = 0; j < num_2; j++) \n    {\n        cout << i << \" \" << j << endl;\n    }\n}\n```\n\n\nNormally one would say that the above snippet's running time is in ```\nO(num_1 * num_2)```\n. If ```\nnum_1```\n and ```\nnum_2```\n are both constants, this means it is in ```\nO(1)```\n. If both ```\nnum_1```\n and ```\nnum_2```\n are linearly proportional to the size of your program's input (```\nn```\n), it is indeed ```\nO(n^2)```\n. If both ```\nnum_1```\n and ```\nnum_2```\n are proportional to the square of the size of the input, it is in ```\nO(n^4)```\n.\n\nBottom line: it depends entirely on what ```\nnum_1```\n and ```\nnum_2```\n are and how and depending on what factors they grow.\n\n```\nfor (int z = 0; z < num_3; z++) { cout << z << endl; }\n```\n\n\nNow this code is in ```\nO(num_3)```\n. To say what this is in terms of ```\nn```\n would again require us to know how ```\nnum_3```\n is related to ```\nn```\n.\n\nIf all of ```\nnum_1```\n, ```\nnum_2```\n and ```\nnum_3```\n are linearly proportional to ```\nn```\n, then you can indeed say that the first snippet runs in ```\nO(n^2)```\n time and the second in ```\nO(n)```\n. However in that case it is not possible for ```\nnum_3```\n to be greater than ```\nnum_1 * num_2```\n for sufficiently large ```\nn```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for three nested for loops with if else statements\r\n                \r\nHere is the algorithm:\n\n```\nfor j= 1 to m do   \n //assign values \nend for\n\nfor t = 1 to T do  \n  for j = 1 to m do\n      if t < T then\n        for d = 1 to t do\n          // perform some operation\n        end for\n      else\n        for d = 1 to t do\n          // Different operations\n        end for  \n  end for\n\n for j = 1 to m do  \n  for i = 1 to m\n     // doing some operations  \n  end for \n end for\n\n end for\n```\n\n\nI am calculating Big O notation for the above loops. My understanding says that big O notation would be O(Tm(T+m)). Could someone shed some light whether its right or wrong?\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation of T(sqrtn) + 5\r\n                \r\nI face a question: T(N) = T(sqrt(N)) + 5.\n\nI am wondering can I solve it in this way?\n\nT(N) = O(sqrt(N)) + O(5)\n\nSince O(5) = O(1) is a constant, we can ignore it.\n\nSo the big O notation of T(N) is O(N^(1/2)).\n\nOr can I just say its notation is O(N) as there is no big difference between O(N) and O(sqrt(N)).\n\nThank you!\n    ", "Answer": "\r\n(For neatness, let's replace the 5 with a constant ```\nc```\n)\n\nSubstituting this function into itself multiple times, we can spot a pattern emerging:\n\n\n\nWhen do we stop iterating? When the stopping condition is met. Take this to be ```\nn = 2```\n (not 1 as is usually the case, since the argument is asymptotic to ```\nn = 1```\n):\n\n\n\nSo the final cost of this function is:\n\n\n\nNote that the constant ```\nc```\n (= 5) does not matter in terms of asymptotic complexity. (And also that the result is not simply ```\nlog n```\n but ```\nlog log n```\n)\n\n\n\nEDIT: if you were to choose a different stopping condition ```\nn = a, a > 1```\n, then the above step would become:\n\n\n\nWhich only differs by a constant from the original result.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation - Doubts about the loop\r\n                \r\nRecently I started reviewing about Big O notation and got a very simple problem.\nIn fact I'm a little bit confused and if someone can give me a detail of that, will be great.\n\nLooking the pseudocode below:\n\n```\nBoolean: ContainsDuplicates(Integer: array[])\n    // Loop over all of the array's items except the last one.\n    For i = 0 To <largest index> - 1\n        // Loop over the items after item i.\n        For j = i + 1 To <largest index> N\n            // See if these two items are duplicates.\n            If (array[i] == array[j]) Then Return True\n        Next j\n    Next i\n\n    // If we get to this point, there are no duplicates.\n    Return False\nEnd ContainsDuplicates\n```\n\n\nI'd like to understand which Big O represent the loop below since the initial value from j is the i + 1:\n\n\n  For j = i + 1 To  N\n\n\nThanks\n    ", "Answer": "\r\n\nfirst loop: 1 To N\nsecond loop: 2 To N\nthird loop: 3 To N\n...\nbefore last loop: N-2 To N\nlast loop: N-1 to N\n\n\nDo you see any pattern?\nIt's like doing 1+2+3+...+(N-1)+N\n\nThe formulae to achieve this is (N+1)(N)/2\n\nIn Big O notation, this is equivalent to N²\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation's Pgfplots Representation\r\n                \r\nI am looking for graphs of big-o notation terms below;\n\nO(1) is a constant time complexity\nO(n) is a linear time complexity\nO(log_2(n)) is a logarithmic time complexity (I mean log basis 2)\nO(n log_2(n))\nO(n^2) is a quadratic time complexity\nO(n^3) cubic time complexity\nO(2^n) exponential time complexity\n\nCould you help me to represent these big O notations using pgfplots?\nI have tried this code snipped for log_2(n). To tell the truth, I'm not so sure.\n```\n\n\\documentclass[border=50pt]{standalone}\n\\usepackage{pgfplots}\n\n\\begin{document}\n\n\\begin{tikzpicture}[trim axis left]\n\\begin{axis}\n[\n    axis lines = left,\n    xlabel = \\(n\\),\n    ylabel = {\\(\\log_{2}(n)\\)},\n]\n[domain=0:10,\n  samples=1000,\n  enlarge x limits=false,\n  grid=both,\n  no markers,\n  axis equal,\n  legend pos=outer north east,\nlegend style={draw=none},]\n\\addplot +[thick] {ln(x)/ln(2)};\n\\addlegendentry{$\\log_{2}(n)$};\n\\end{axis}\n\\end{tikzpicture}\n\n\\end{document}\n```\n\n\n    ", "Answer": "\r\nYou were almost there ... The only missing parts were other functions and proper axes configuration. Here is my solution. You might need to think about the ranges. I am not entirely sure they are correct but did not bother to check. I changed a way you to make the graph itself centred; I think  ```\noverlay```\n works better.\n\n```\n\\documentclass[margin=50pt]{standalone}\n\\usepackage{pgfplots}\n\n\\pgfplotsset{width=12cm,compat=1.18}\n\n\n\\begin{document}\n\n\\begin{tikzpicture}\n  \\begin{axis}\n    [\n    axis lines = left,\n    xmin=1, xmax=8.1, ymin=1, ymax=9,\n    domain=0:8, samples=100, no markers, thick, grid=both,\n    xlabel = \\(n\\), ylabel = {\\(f(n)\\)},\n    label style = {overlay},       % Has he same effect as\n    ticklabel style = {overlay},   % trim axis [left|right]\n    legend entries = {\n      $1$,\n      $\\log_{5}n$,\n      $\\log_{2}n$,\n      $n$,\n      $n\\log_{2}n$,\n      $2^{n}$,\n      $n^{2}$,\n      $n^{3}$,\n    },\n    every axis/.style = {font=\\footnotesize},\n    label style = {font=\\normalsize},\n    ]\n    \\addplot+ {1};\n    \\addplot+ {1 + ln(x)/ln(5)};\n    \\addplot+ {1 + ln(x)/ln(2)};\n    \\addplot+ {x};\n    \\addplot+ {1 + x*ln(x)/ln(2)};\n    \\addplot+ {2^x-1};\n    \\addplot+ {x^2};\n    \\addplot+ {x^3};\n  \\end{axis}\n\\end{tikzpicture}\n\n\\end{document}\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation representation of a simple algorithm\r\n                \r\nHow can I represent its complexity with Big-O notation? I am little bit confused since the second for loop changes according to the index of the outer loop. Is it still O(n^2)? or less complex? Thanks in advance\n\n```\nfor (int k = 0; k<arr.length; k++){\n      for (m = k; m<arr.length; m++){\n           //do something\n      }\n}\n```\n\n    ", "Answer": "\r\nYour estimation comes from progression formula:\n\n\n\nand, thus, is ```\nO(n^2)```\n. Why your case is progression? Because it's ```\nn + (n-1) + ... + 1```\n summation for your loops.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Problems with Big O Notation\r\n                \r\nI created two dictionaries, one using doubly linked lists and another using binary search trees, both with some basic functions. I want to estimate the Big O notation for both dictionaries and compare them alongside the native Python dictionary, but I'm facing some issues.\nI know that the native dictionary is supposed to be O(1), the list dictionary O(n), and the tree dictionary O(log(n)), but that's not what I'm finding when I generate dictionaries of all three types randomly. What could be the issue? When I try to plot it, it looks like it's kinda alright, but analyzing the times in a list, they are randomly increasing.\nThe code:\n```\nclass dict_linear:\n class Node:\n    def __init__(self, key, value, prev=None, next=None):\n        self.key = key\n        self.value = value\n        self.prev = prev\n        self.next = next\n\n def __init__(self):\n    self.firstNode = None\n\n def search(self, key):\n    curr = self.firstNode\n    while curr is not None:\n        if curr.key == key:\n            return curr\n        curr = curr.next\n    return None\n\n def __setitem__(self, key, value):\n    curr = self.search(key)\n    if curr is not None:\n        curr.value = value\n    else:\n        new_node = self.Node(key, value)\n        if self.firstNode is None:\n            self.firstNode = new_node\n        else:\n            new_node.next = self.firstNode\n            self.firstNode.prev = new_node\n            self.firstNode = new_node\n\n def __getitem__(self, key):\n    curr = self.search(key)\n    if curr is None:\n        raise KeyError(f\"There was an error with the key '{key}'\")\n    return curr.value\n\nclass dict_binary:\n class Node:\n    def __init__(self, key, value, left=None, right=None):\n        self.key = key\n        self.value = value\n        self.left = left\n        self.right = right\n\n def __init__(self):\n    self.root = None\n\n def search(self, curr, key):\n    if curr is None or curr.key == key:\n        return curr\n    elif key < curr.key:\n        return self.search(curr.left, key)\n    else:\n        return self.search(curr.right, key)   \n\n def insert(self, curr, key, value):\n    if key == curr.key:\n        curr.value = value\n    elif key < curr.key:\n        if curr.left is None:\n            curr.left = self.Node(key, value)\n        else:\n            self.insert(curr.left, key, value)\n    elif key > curr.key:\n        if curr.right is None:\n            curr.right = self.Node(key, value)\n        else:\n            self.insert(curr.right, key, value)\n\n def __setitem__(self, key, value):\n    if self.root is None:\n        self.root = self.Node(key, value)\n    else:\n        self.insert(self.root, key, value)\n\n def __getitem__(self, key):\n    curr = self.search(self.root, key)\n    if curr is None:\n        raise KeyError(f\"There was an error with the key '{key}'\")\n    return curr.value\n```\n\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "big o notation for nested for loop\r\n                \r\nHow would I find the big o notation for the nested for loop of the following code?\n\n```\nint sum = 0;\nfor(int i = 1; i < N; i *= 2)\n   for(int j =0; j <i; j++)\n     sum++;\n```\n\n\nI believe the outer loop is log(n) and the inner loop is N, so isn't the answer n*log(n)? If my answer is correct, can we assume that j\n    ", "Answer": "\r\nYour answer would be correct if the inner loop ran from zero to ```\nN```\n. However, it runs from zero to ```\ni```\n, which in turn runs as consecutive powers of two.\n\nTherefore, the number of times the inner block ```\nsum++```\n would get executed could be computed as a sum of the form\n\n```\n1+2+4+8+16+...\n```\n\n\nwith log2N terms.\n\nThis is a geometric series. Computing the sum of its first log2N terms gives the correct answer is O(N).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the big O notation and how do I calculate it?\r\n                \r\nCalculate big o of function. How do I calculate the big O notation in this function?\n\nExample: \n\r\n\r\n```\nfunction fun1(int n)\r\n{\r\n  int s = 0;\r\n  for(int i = 0; s < n; i++)\r\n  {\r\n    s += i;\r\n    for(var j = s; j < n; j++)\r\n      {\r\n        console.log(j);\r\n      }\r\n  }\r\n  return s;\r\n}```\n\r\n\r\n\r\n\n    ", "Answer": "\r\nRoughly speaking, consider the ```\ni```\n-th iteration of the outer loop. After execution of the loop's body,\n\n```\ns = 1 + 2 + ... +  i-1 + i\n```\n\n\nwhich is equal to ```\ni*(i+1)/2 = (i²+i)/2```\n by an identity by Gauss. The maximum value for ```\ni```\n such that this expression is smaller than ```\nn```\n is can be obtained by elementary calculation as follows. If we require\n\n```\n(i²+i)/2 <= n\n```\n\n\nwhich means\n\n```\ni²+i-2n <= 0\n```\n\n\nwe can use the formula for reduced quadratic equation to obtain\n\n```\ni <= -1/2 + sqrt(1/4+2n)\n```\n\n\nwhich is in ```\nO(n^{1/2})```\n. In each iteration of the outer loop, the inner loop takes ```\nn-s```\n iterations, which very roughly can be estimated by ```\nn```\n (but this is very imprecise, I believe the overall analysis can be made more precise). In total, this yields a bound of ```\nO(n^{1/2}*n)=O(n^{3/2})```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O-Notation on my permutation function coded scala\r\n                \r\nBelow code is implementation of permuting elements in list, coded by scala\n\n```\ndef permute(list:List[Int]):List[List[Int]] =\n  if(list.isEmpty) {\n    Nil\n  } else {\n    var result: List[List[Int]] = Nil\n\n    def recursion(plist: List[Int], rlist: List[Int]): Unit = {\n      if (rlist.isEmpty)\n        result ++= plist :: Nil\n\n      if (plist != list.reverse)\n        for (i <- rlist.indices) {\n          val x = rlist(i)\n          val xDropped = drop(x, rlist)\n          val newPList = plist ++ (x :: Nil)\n          recursion(plist ++ (x :: Nil), xDropped)\n        }\n    }\n\n    recursion(Nil, list)\n    result\n  }\n\ndef drop(x:Int, list:List[Int]) = list.filter(_ != x)\nval result = permute(xs)\nresult.foreach(println)\nprintln(result.length)\n```\n\n\nand console will show as below.\n\nconsole result\nMy concern is what is big-o notation on this function,\nand how to calculate it?\n\nI'd hope you to show detail description of the method that results big-o notation\n\nThanks, have a nice day.\n    ", "Answer": "\r\nWell, your algorithm is far too complicated because of a lot of suboptimal steps. The Scala API is overwhelming if you first look at it and come from a imperative language, so this is understandable, but it makes your algorithm hard to analyse.\n\nWhat is clear from the problem, is, that your algorithm has to be exponential because it enumerates exponentially many permutations.\n\nThe dominating factor for your complexity is the recursive call that you do for each element in your input for a list one element smaller. So your recursion depth is the number of elements in your input and the number of recursive calls in each step is as well that number. What we get as complexity is therefore O(n^n) = O(2^(n*ld(n)))\n\nNow this would be your complexity if you returned your results in an optimal way, but you are using ```\nresult ++= plist :: Nil```\n. First, this is not functional style, which, without good reason, should be avoided. But if you do it, you should use a mutable Structure (e.g. ListBuffer) or at least prepend to a list. Here, you append to a list which is linear in its length. So the whole thing is quadratic in the number of elements you append to the list which is exponential. So what you get as complexity is in fact\n\nO((2^(n*ld(n)))^2) = O(2^(2*n*ld(n)))\n\nAnd constants in an exponent do make a difference even in O-notation.\n\nReplacing ```\nresult ++= plist :: Nil```\n by ```\nresult = plist :: result```\n gives your algorithm optimal asymptotic complexity and makes it considerably faster.\n\nThere are however a few other issues with your algorithm that do not influence the asymptotic complexity:\n\n\nThere is no need to iterate over the indices with ```\ni <- rlist.indices```\n, you can directly iterate over the elements ```\nx <- rlist```\n. This is also much faster as index-base access to a List takes linear time.\nYour method drop filters the list to remove one element. This can be achieved much simpler and faster if you use a set instead of a list where you can directly and efficiently remove elements.\nYou append to your plist by appending a list: ```\nplist ++ (x :: Nil)```\n but you could just prepend to the list, which is less convoluted and faster: ```\nx :: plist```\n. Also, there is a :+ operator that appends just one element. You do not have to put an element into a list just to append it.\n\n\nHere is a cleaned-up version of your code:\n\n```\ndef permute(list:List[Int]):List[List[Int]] =\n  if(list.isEmpty) {\n    Nil\n  } else {\n    var result: List[List[Int]] = Nil\n\n    def recursion(plist: List[Int], rlist: List[Int]): Unit = {\n      if (rlist.isEmpty)\n        result = plist :: result\n\n      if (plist != list.reverse)\n        for (x <- rlist) {\n          val xDropped = drop(x, rlist)\n          val newPList = x :: plist\n          recursion(newPList, xDropped)\n        }\n    }\n\n    recursion(Nil, list)\n    result\n  }\n```\n\n\nThis is now quite fast.\n\nWithout thinking about performance I would have done it like this:\n\n```\ndef perm[T](elements: Set[T]): List[List[T]] =\n  if(elements.isEmpty)\n    List(Nil)\n  else\n    for {\n      element <- elements.toList\n      rest <- perm(elements - element)\n    } yield\n      element :: rest\n```\n\n\nThis is much cleaner and simpler and almost as fast as the optimized version of your code. Actually, I was surprised that your cleaned-up version is faster, but there are two reasons: firstly, we have to iterate over all elements over the set anyway, so being able to remove a single element is not much of an advantage, and then iteration over a list is faster than over a set. Secondly, my algorithm iterates over the result set and transforms its elements, that seems to be somewhat slower than building up the complete results.\n\nThis version finally is as fast or a little bit faster than your cleaned-up version:\n\n```\ndef perm[T](elements: List[T], result: List[T]): List[List[T]] =\n  if(elements.isEmpty)\n    List(result)\n  else\n    for {\n      element <- elements\n      res <- perm(elements.filter(_!=element), element :: result)\n    } yield res\ndef perm[T](elements: List[T]): List[List[T]] = perm(elements, Nil)\n```\n\n\nProbably there is an even faster, imperative way to do it, but this should be \"good enough\". The version in the standard library of Scala seems to be a lot slower than this.\n\nAnd then there is of course an even faster and shorter functional version using an insert operation on lists:\n\n```\ndef insert[T](elem: T, list: List[T]) =\n  (0 to list.size).map{\n     pos => list.take(pos) ++ (elem :: list.drop(pos))\n  }\n```\n\n\nSo now we can simply do this:\n\n```\ndef perm[T](elements: List[T]) =\n  elements.foldLeft(List(List[T]())){\n    (results, element) => results.flatMap{insert(element,_)}\n  }\n```\n\n\nSo following the direct approach \"permute n elements by permuting n-1 and inserting the next element at every position\" turns out to be the shortest and fastest...\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What does it mean to find big o notation for memory\r\n                \r\nI have a question, what does it mean to find the big-o order of the memory required by an algorithm?\n\nLike what's the difference between that and the big o operations?\n\nE.g\n\na question asks\nGiven the following pseudo-code, with an initialized two dimensional array A, with both dimensions of size n:\n\n```\nfor  i <- 1  to  n  do\n       for  j <- 1  to  n-i  do\n                        A[i][j]=  i + j\n```\n\n\nWouldn't the big o notation for memory just be n^2 and the computations also be n^2?\n    ", "Answer": "\r\nBig-Oh is about how something grows according to something else (technically the limit on how something grows).  The most common introductory usage is for the something to be how fast an algorithm runs according to the size of inputs.  \n\nThere is nothing that says you can't have the something be how much memory is used according to the size of the input.  \n\nIn your example, since there is a bucket in the array for everything in ```\ni```\n and ```\nj```\n, the space requirements grow as ```\nO(i*j)```\n, which is ```\nO(n^2)```\n\n\nBut if your algorithm was instead keeping track of the largest sum, and not the sums of every number in each array, the runtime complexity would still be ```\nO(n^2)```\n while the space complexity would be constant, as the algorithm only ever needs to keep track of current i, current j, current max, and the max being tested.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What 's Big O notation on this specific code? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                This question is unlikely to help any future visitors; it is only relevant to a small geographic area, a specific moment in time,  or an extraordinarily narrow situation that is not generally applicable to the worldwide audience of the internet. For help making  this question more broadly applicable, visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI want to know Big O notation and Big Omega notation \n(worst case, best case) of this code \nIt is a sorting algorithm and my guess is that it has a ```\nO(n)```\n and ```\nOmega(n)```\n\n:\n\n```\npublic static void swap(int[] A, int i,int j){\n    int temp = 0;\n    temp = A[i];\n    A[i] = A[j];\n    A[j] = temp;\n}\n\npublic static int[] MyAlgorithm(int[] A, int n){\n\n    boolean done = true;\n    int j =0;\n    while(j<=(n-2)){\n        if(A[j]>A[j+1]){\n            swap(A,j,j+1);\n            done = false;\n        }\n        j = j+1;\n    }\n    j = n-1;\n\n    while(j>=1){\n        if(A[j]<A[j-1]){\n            swap(A, j-1,j);\n            done = false;\n        }\n        j = j-1;\n    }\n\n    if(done==false){\n        MyAlgorithm(A,n);\n    }\n\n    return A;\n}\n```\n\n    ", "Answer": "\r\nIt's O(n^2) (for list ```\n[n, n-1, ..., 1]```\n), Omega(n) for ```\n[1, 2, ..., n]```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for a matrix algorithm\r\n                \r\nI have a simple algorithm that prints the two dimensional matrix (m*n, m and n are different numbers):\n\n```\nfor(i=0;i<m;i++)\n    for(j=0;j<n;j++)\n        Console.WriteLine(\"{0}\",A[i,j]);\n```\n\n\nI read that the big O notation for this algorithm is O(n^2);\nCould somebody explain me what is the \"n^2\" in that statement? If this is number of elementary operations, then it should be m*n, not n^2?\n    ", "Answer": "\r\nIn reality it should me m*n. We can assume it to be the number of elementary operations in this case, but the actual definition is its \"the upper bound of the number of elementary operations.\"\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for a matrix algorithm\r\n                \r\nI have a simple algorithm that prints the two dimensional matrix (m*n, m and n are different numbers):\n\n```\nfor(i=0;i<m;i++)\n    for(j=0;j<n;j++)\n        Console.WriteLine(\"{0}\",A[i,j]);\n```\n\n\nI read that the big O notation for this algorithm is O(n^2);\nCould somebody explain me what is the \"n^2\" in that statement? If this is number of elementary operations, then it should be m*n, not n^2?\n    ", "Answer": "\r\nIn reality it should me m*n. We can assume it to be the number of elementary operations in this case, but the actual definition is its \"the upper bound of the number of elementary operations.\"\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Tools for measuring empirical computational complexity of Java codes?\r\n                \r\nI have a few Java codes for which I wish to measure empirical computational complexity . There is a trend-prof tool which takes as input compiled ```\nC/C++```\n programs .\n\nAre there similar tools to the trend-prof that take as an input compiled Java programs ? \n    ", "Answer": "\r\nSonar is commonly used : http://www.sonarsource.org/\n\nRegards.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation with two variables\r\n                \r\nThis originates from writing a program to find the median of two sorted arrays with size ```\nm```\n and ```\nn```\n respectively, with the time complexity of ```\nO(log(m + n))```\n.\n\nI can work out a solution of ```\nO(log(m) + log(n))```\n. Does it meet the time requirement above?\n\nI think it's positive because:\n\n```\nlog(m) + log(n) = log(m*n) <= log((m+n)^2) = 2*log(m+n) = O(log(m+n))```\n\n\nPut another way, there exist ```\nk = 2```\n and ```\nm0 = n0 = 1```\n. For any ```\nm > m0 and n > n0```\n, there is ```\nlog(m*n) <= k*log(m + n)```\n.\n\nIs there a flaw, or am I correct?\n\nMore generally, given constant ```\na```\n, can we say ```\nlog(n^a) = O(log(n))```\n with the same reasoning?\n\n\n\nThanks to David's answer.\nThis is also mentioned by Big-O notation on Wikipedia:\n\n```\n\"We may ignore any powers of n inside of the logarithms. The set O(log n) is exactly the same as O(log(n^c)).\"```\n\n    ", "Answer": "\r\nYes, you're correct on all counts. Log grows slowly enough that the asymptotic class isn't very sensitive to the function inside.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of recursive functions\r\n                \r\nI have been practicing big O notation and I seem to understand it except for when it comes to recursive functions. I can deal with the simple ones (like when it is ```\nO(n)```\n or ```\nO(1)```\n but anything else I usually just get lost. \nBelow are three that are practice problems, I would really appreciate if someone explained the process on how they find the answer. \n\n```\n  public static int method2(int n)\n  {\n    if (n < 1) throw new IllegalArgumentException();\n    if (n == 1)\n      return 2;\n    else\n      return method2(n - 1) * (2 * n);\n  }\n\n\n  public static void method3(int n)\n  {\n    if (n < 1) throw new IllegalArgumentException();\n    if (n == 1)\n      System.out.print(\"1\");\n    else {\n      method3(n - 1);\n      System.out.print(\", \" + n);\n    }\n  }\n\n\n  public static void method4(int n)\n  {\n    if (n < 1) throw new IllegalArgumentException();\n    if (n==1) System.out.print(1);\n    else if (n==2) System.out.print(\"1 1\");\n    else {\n        System.out.print((n+1)/2+ \" \");\n        method4(n-2);\n        System.out.print(\" \"+(n+1)/2);\n    }\n}\n}\n```\n\n    ", "Answer": "\r\nThis examples are quite straightforward. \n\nIn ```\nmethod2```\n, you start with ```\nn```\n, and go down by ```\n1```\n until it comes to ```\n1```\n. so you will have ```\nn```\n calls of ```\nmethod2```\n and that is ```\nO(n)```\n.\n\n```\nmethod3```\n is same as ```\nmethod2```\n, just different operation. You call ```\nmethod3```\n until its 1 by decreasing ```\nn```\n by ```\n1```\n.\n\nIn ```\nmethod4```\n you decrease ```\nn```\n by ```\n2```\n, until its ```\n1```\n or ```\n2```\n, so you will do ```\nn/2```\n steps, which is still O(n).\n\nThis are not quite the best examples to understand the speed of recursive functions. You need to play around with examples that have more than one option for recursion.\n\nAll this options can be transform to for loops with same complexity, so try to think in that way if it can help you. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Tools for measuring empirical computational complexity of Java codes?\r\n                \r\nI have a few Java codes for which I wish to measure empirical computational complexity . There is a trend-prof tool which takes as input compiled ```\nC/C++```\n programs .\n\nAre there similar tools to the trend-prof that take as an input compiled Java programs ? \n    ", "Answer": "\r\nSonar is commonly used : http://www.sonarsource.org/\n\nRegards.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the Big O Notation (Time Complexity) of an array declaration?\r\n                \r\nI was wondering what the time complexity for this line would be in Big O notation.\nAny help would be greatly appreciated!\n```\nint arr[] = new int[10];\n```\n\nSorry if this is a super easy question I'm fairly new to Big O notation.\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "what is complexity of my code in big-o notation\r\n                \r\nThere is a code and i'm curious about the algorithmic complexity of this code in the big-o notation.\n```\ndef listsum(numList):\n   if len(numList) == 1:\n        return numList[0]\n   else:\n        return numList[0] + listsum(numList[1:])\n```\n\n    ", "Answer": "\r\nSlicing an array is linear in the size of the slice, so the recurrence relation is\n```\nT(0) = 1\nT(n) = T(n-1) + n\n```\n\nwhich has the asymptotic solution\n```\nT(n) ∈ Θ(n^2)\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for exponential and logarithmic complexity\r\n                \r\nThere are a lot of questions about big O notation, but I didn't found clear answer for this question.\n\nWe write that:\nO(5n) = O(n)\nand\nO(3n^2 + n + 2) = O(n^2)\n\nCan we write that:\nO(2^(2n)) = O(2^n)?\n\nThe same for logarithmic complexity:\nO(n log(4n)) = O(n log(n))?\n    ", "Answer": "\r\nThe only constants you can remove are additive and multiplicative ones. Meaning O(f(n)) = O(f(n) + C) = O(C × f(n)).\n\n22n = (2n)2. This 2 constant cannot be ignored as it is an exponent. Just as O(n) and O(n2) are different complexity classes, so are O(2n) and O(22n).\n\nOn the other hand, yes, O(n log 4n) = O(n log n). We can use a logarithmic identity to turn the 4 into an multiplicative constant: O(n log 4n) = O(n (log n + log 4)) = O(n log n + (log 4) n) = O(n log n + n) = O(n log n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Specifying Big-O notation in java (algorithms) [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 2 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI am struggling in fully understanding big-o notation, and would appreciate some guidance with the following questions in the image provided.\nI have an answer of O(n^2) for the first question, however i am not fully confident in my answer. Any help would be great thanks.\n\n    ", "Answer": "\r\nIn the first question the complexity is O(log5N), that is, it is ```\nO(log N)```\n, as the number of iterations is limited by a condition ```\n5^N```\n.  Aside comment, in the first question a ```\nproduct```\n is calculated, not a ```\nsum```\n.\nIn the second question the number of iterations is ```\n(N - 5) * (2 * N) * 1000```\n, which is roughly equal to ```\n2000 * N^2```\n, so big-O is ```\nO(N^2)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What does the O in Big-O Notation mean?\r\n                \r\nI‘m trying to wrap my head around the meaning of the Landau-Notation in the context of analysing an algorithm‘s complexity.\nWhat exactly does the O formally mean in Big-O-Notation?\nSo the way I understand it is that O(g(x)) gives a set of functions which grow as rapidly or slower as g(x), meaning, for example in the case of O(n^2):\n\nwhere t(x) could be, for instance, x + 3 or x^2 + 5. Is my understanding correct?\nFurthermore, are the following notations correct?\n\nI saw the following written down by a tutor. What does this mean? How can you use less or equal, if the O-Notation returns a set?\n\nCould I also write something like this?\n\n    ", "Answer": "\r\n\nSo the way I understand it is that O(g(x)) gives a set of functions which grow as rapidly or slower as g(x).\n\nThis explanation of Big-Oh notation is correct.\n\nf(n) = n^2 + 5n - 2,  f(n) is an element of O(n^2)\n\nYes, we can say that. ```\nO(n^2)```\n in plain English, represents \"set of all functions that grow as rapidly as or slower than n^2\". So, ```\nf(n)```\n satisfies that requirement.\n\nO(n) is a subset of O(n^2), O(n^2) is a subset of O(2^n)\n\nThis notation is correct and it comes from the definition. Any function that is in ```\nO(n)```\n, is also in ```\nO(n^2)```\n since growth rate of it is slower than ```\nn^2```\n. ```\n2^n```\n is an exponential time complexity, whereas ```\nn^2```\n is polynomial. You can take limit of ```\nn^2```\n / ```\n2^n```\n as ```\nn```\n goes to infinity and prove that ```\nO(n^2)```\n is a subset of ```\nO(2^n)```\n since ```\n2^n```\n grows bigger.\n\nO(n) <= O(n^2) <= O(2^n)\n\nThis notation is tricky. As explained here, we don't have \"less than or equal to\" for sets. I think tutor meant that time complexity for the functions belonging to the set ```\nO(n)```\n is less than (or equal to) the time complexity for the functions belonging to the set ```\nO(n^2)```\n. Anyways, this notation doesn't really seem familiar, and it's best to avoid such ambiguities in textbooks.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation and recursion with loops\r\n                \r\nWhat is the Big-O notation of the worst case in the following method: \n\n```\n /**\n * @best-case  O(1)\n * @worst-case O(?)\n *\n * {@link NTree#contains(Comparable)}\n */\npublic boolean contains(T elem) {\n\n    if (this.data.compareTo(elem) == 0)\n        return true;\n\n    for(NTree<T> t : children) {\n        if(t != null)\n            return t.contains(elem);    \n    }\n\n\n    return false; \n}\n```\n\n\nThis is an n-ary generic tree, and each tree has a n number of children.\nThe best case happens when the ```\nelem```\nequals the ```\nroot.data```\n. \n\nBut I'm not sure about the worst case where we have to go through every child of the tree.\n    ", "Answer": "\r\nTo quote you at the end there:\n\n\n  ... the worst case where we have to go through every child of the tree.\n\n\nIf you're going through every child in the worst case, that would be ```\nO(n)```\n, where ```\nn```\n is the number of nodes in the tree. \n\nYou can think of it way: if this was a simple linked list, and you had to search through it entirely in the worst case, what would the worst case complexity be? It's the same here. It's just that in this case, each node can have more than one child.\n\nAnd recursion doesn't really play a role here in altering the complexity. It's just the means to loop. It would be the same if you were doing an iterative search using a standard loop construct. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Finding Big O Notation of For loop which is inside an If condition\r\n                \r\n```\nsum = 0;\nfor( i = 1; i < n; ++i )\n  for( j = 1; j < i * i; ++j )\n    if( j % i == 0 )\n      for( k = 0; k < j; ++k )\n        ++sum;\n```\n\n\nHow do I find Big O notation for this code? I'm new to this big o notation thing. So I'll appreciate if someone can explain me it simply with details.. Thank you!\n    ", "Answer": "\r\nBig O is an asymptotic upper bound of a function. So in your case the the for loops take the most time, if the ```\nif```\n condition evaluates always to true, so you can just assume this an get a correct upper bound, which is maybe not tight. But there are a lot of cases where you cannot do better than this.\n\nIn some cases you can try to remove the if while maintaining the number of operations roughly. E.g. in your case you could replace ```\nj = 1```\n by ```\nj = i```\n and ```\n++j```\n by ```\nj += i```\n. This is not to change the algorithm, but only for the complexity analysis to change the way you look at it. You still have to remember that the middle ```\nfor```\n loop takes ```\ni*i```\n steps. Now you have this:\n\n```\nsum = 0;\nfor( i = 1; i < n; ++i )\n  O(i * i) Operations\n  for( j = i; j < i * i; j += i )\n    for( k = 0; k < j; ++k )\n      ++sum;\n```\n\n\nYou also can assume that the ```\nif```\n condition is always false. This way you get a lower bound. In some cases the upper and lower bound match, meaning that the part you hat trouble to analyze is actually irrelevant for the overall complexity.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is Big O(logn) log base e?\r\n                \r\nFor binary search tree type of data structures, I see the Big O notation is typically noted as O(logn).  With a lowercase 'l' in log, does this imply log base e (n)  as described by the natural logarithm?  Sorry for the simple question but I've always had trouble distinguishing between the different implied logarithms.\n    ", "Answer": "\r\nOnce expressed in big-O() notation, both are correct.  However, during the derivation of the O() polynomial, in the case of binary search, only log2 is correct.  I assume  this distinction was the intuitive inspiration for your question to begin with.\n\nAlso, as a matter of my opinion, writing O(log2 N) is better for your example, because it better communicates the derivation of the algorithm's run-time.\n\nIn big-O() notation, constant factors are removed.  Converting from one logarithm base to another involves multiplying by a constant factor.\n\nSo O(log N) is equivalent to O(log2 N) due to a constant factor.\n\nHowever, if you can easily typeset log2 N in your answer, doing so is more pedagogical. In the case of binary tree searching, you are correct that log2 N is introduced during the derivation of the big-O() runtime.  \n\nBefore expressing the result as big-O() notation, the difference is very important.  When deriving the polynomial to be communicated via big-O notation, it would be incorrect for this example to use a logarithm other than log2 N, prior to applying the O()-notation. As soon as the polynomial is used to communicate a worst-case runtime via big-O() notation, it doesn't matter what logarithm is used.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What would be the big O notation for the function?\r\n                \r\nI know that big O notation is a measure of how efficint a function is but I don\\t really get how to get calculate it. \n\n```\ndef method(n)\n   sum = 0\n   for i in range(85)\n       sum += i * n\n   return sum\n```\n\n\nWould the answer be O(f(85)) ?\n    ", "Answer": "\r\nThe complexity of this function is O(1)\n\nin the RAM model basic mathematical functions occur in constant time. The dominate term in this function is \n\n```\nfor i in range(85):\n```\n\n\nsince 85 is a constant the complexity is represented by O(1)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation for a simple function and why\r\n                \r\nWhat would be the big-O notation for a simple function like:\n\n```\ndef function(array, index):\n    return array[index]\n```\n\n\nWould it be linear because the it looks at each cell in the array? or constant? And why?\n    ", "Answer": "\r\nIt depends on the type of object. If ```\narray```\n is a Python list object, it'll be O(1). If it is a linked list, it'll be O(n). If it is a binary tree, it could be O(log n).\n\nIn other words, a function that does nothing but delegate an operation to another object has no clear complexity. It is entirely dependent on the cost of that one operation.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Does Big O notation also represent space complexity?\r\n                \r\nBig O notation represents how long a method takes to run as input gets larger. \n\nDoes it (the formula) also represent space complexity?\n    ", "Answer": "\r\nYes. Big-O notation is used to represent Space Complexity. Big-O notation in Space complexity is similar to that of Time complexity. It represents the \"maximum amount of space used by the algorithm at anytime\".\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What would the big O notation be for a 3x3 multidimensional array in java?\r\n                \r\nI am having trouble figuring out how to calculate the big O notation for a project. What would the big O notation be for a 3x3 grid, or a nxn grid with n being decided by the user.\nAlso, if I output a 3x3 grid multiple times, do I add the big O notations of each array together, or is there just one big O notation for all?\nSorry, I'm a little confused on how big O notation works and how to calculate it, and what parts of code it can be used for.\nI really don't know what to do, this is the code for my multidimentional array.\n```\nString board[][] = new String[3][3];\n    for (int i = 0; i < board.length; i++) {\n\n        for (int j = 0; j < board[0].length; j++) {\n\n            board[i][j] = \"-\";\n        }\n    }\n```\n\n    ", "Answer": "\r\nThe algorithm that you made is initializing a multidimensional array. Although the array is filled by 9 of ```\n\"-\"```\n, the Big O Notation implies the upper bound on the complexity of the algorithm which is the maximum number of operations that the algorithm will perform given the worst-case input.\nSo, the input value that you have given (3x3) should not be considered to get the Big O Notation of your algorithms.\nSince you have two iterations in your algorithms, the maximum iteration would be n*n, so your Big O is ```\nO(n^2)```\n.\nBecause your input data is given, which are 3 and 3, you can estimate the time complexity via ```\nO(n^2)```\n, which is ```\nO(9)```\n.\nFor more information: https://www.geeksforgeeks.org/difference-between-big-oh-big-omega-and-big-theta/\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is Big O(logn) log base e?\r\n                \r\nFor binary search tree type of data structures, I see the Big O notation is typically noted as O(logn).  With a lowercase 'l' in log, does this imply log base e (n)  as described by the natural logarithm?  Sorry for the simple question but I've always had trouble distinguishing between the different implied logarithms.\n    ", "Answer": "\r\nOnce expressed in big-O() notation, both are correct.  However, during the derivation of the O() polynomial, in the case of binary search, only log2 is correct.  I assume  this distinction was the intuitive inspiration for your question to begin with.\n\nAlso, as a matter of my opinion, writing O(log2 N) is better for your example, because it better communicates the derivation of the algorithm's run-time.\n\nIn big-O() notation, constant factors are removed.  Converting from one logarithm base to another involves multiplying by a constant factor.\n\nSo O(log N) is equivalent to O(log2 N) due to a constant factor.\n\nHowever, if you can easily typeset log2 N in your answer, doing so is more pedagogical. In the case of binary tree searching, you are correct that log2 N is introduced during the derivation of the big-O() runtime.  \n\nBefore expressing the result as big-O() notation, the difference is very important.  When deriving the polynomial to be communicated via big-O notation, it would be incorrect for this example to use a logarithm other than log2 N, prior to applying the O()-notation. As soon as the polynomial is used to communicate a worst-case runtime via big-O() notation, it doesn't matter what logarithm is used.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Using Big O Notation, what is the correct label for this algorithm?\r\n                \r\nI am curious. What is the correct way to describe this using Big-O Notation?\n```\nvar prices = [100, 180, 260, 590, 40, 310, 535, 10, 5, 3];\nvar biggest_profit = 0;\n  \nfor (var i = 0; i < prices.length; i++) {\n    var first_price = prices[i];\n  \n    for (var j = i + 1; j <= prices.length; j++) {\n      // do something here\n    }\n}\n  \n```\n\nThis is the bit that throws me off:\n```\nj = i + 1\n```\n\nEvery time we go through ```\ni```\n, the ```\nj```\n becomes shorter and shorter.\nWhat is the correct name for this pattern in Big O Notation?\n    ", "Answer": "\r\nYou can use Sigma notation to calculate the number of visits of the inner loop (\"do something here\")\n\n\n\nWhere ```\n(*)```\n follows from a summation rule made famous by the rumour that Gauss once derived it on-the-spot as a young student.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation of this BinaryHeap code\r\n                \r\nI'm trying to determine the Big O notation of two methods that are based on Jim Mischel's BinaryHeap class.\n\n```\npublic void Add(T item) {\n    int i = _items.Count;\n    _items.Add(item);\n    while (i > 0 && _items[(i - 1) / 2].CompareTo(item) > 0) {\n        _items[i] = _items[(i - 1) / 2];\n        i = (i - 1) / 2;\n    }\n    _items[i] = item;\n}\n\npublic T Remove() {\n    T firstItem = _items[0];\n    T tempItem = _items[_items.Count - 1];\n    _items.RemoveAt(_items.Count - 1);\n    if (_items.Count > 0) {\n        int i = 0;\n        while (i < _items.Count / 2) {\n            int j = (2 * i) + 1;\n            if ((j < _items.Count - 1) && (_items[j].CompareTo(_items[j + 1]) > 0)) ++j;\n            if (_items[j].CompareTo(tempItem) >= 0) break;\n            _items[i] = _items[j];\n            i = j;\n        }\n        _items[i] = tempItem;\n    }\n    return firstItem;\n}\n```\n\n\nFor the ```\nAdd```\n method, I believe it's ```\nO(log(n))```\n since the loop appears to be of the form:\n\n```\nfor(int i = n; i > 0; i=(i-1)/2);\n```\n\n\nAnd for the ```\nRemove```\n method, I believe it's ```\nO(log(log(n))```\n since the loop appears to be of the form: \n\n```\nfor(int i=0; i < n/2; i = (2*i)+1);\n```\n\n\nIs this correct?\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How do i determine complexity using Big-O Notation [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     This question does not appear to be about programming within the scope defined in the help center.\r\n                \r\n                    \r\n                        Closed 7 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nWhat I need is to an explanation on how to determine it, here are few examples and hope you can help me find their complexity using Big-O Notation:\n\n```\nFor each of the following, find the dominant term(s) having the sharpest increase in n and give the time complexity using Big-O notation.  \nConsider that we always have n>m.\n\nExpression Dominant term(s) O(…) \n5+ 0.01n^3 + 25m^3   \n500n +100n^1.5 + 50nlogn   \n0.3n+ 5n^1.5  +2.5n^1.75   \nn^2logn +n(log2m)^2    \nmlog3n +nlog2n   \n50n+5^3 m + 0.01n^2    \n```\n\n    ", "Answer": "\r\nIt's fairly simple.\nAs ```\nn```\n rises to large numbers (towards infinity), some parts of the expression becomes meaningless, so remove them.\nAlso, ```\nO()```\n notation is relativistic, not absolute, meaning there is no scale, so constant factors are meaningless, so remove them.\nExample: ```\n100 + 2*n```\n. At low numbers ```\n100```\n is the main contributor to the result, but as ```\nn```\n increases, it becomes meaningless. Since there is no scale, ```\nn```\n and ```\n2n```\n is the same thing, i.e. a linear curve, so the result is ```\nO(n)```\n.\nOr said another way, you choose the most extreme curve in the expression from this graph:\n\n(source: bigocheatsheet.com)\nLet's take your second example: ```\n500n +100n^1.5 + 50nlogn```\n\n1st part is ```\nO(n)```\n.\n2nd part is ```\nO(n^1.5)```\n.\n3rd part is ```\nO(nlogn)```\n.\nFastest rising curve is ```\nO(n^1.5)```\n, so that is the answer.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Why doesn't the additive constant in Big-O notation matter?\r\n                \r\nIn definition of Big-O notation we care only about ```\nC```\n coefficient:\n\n```\nf(n) ≤ Cg(n) for all n ≥ k\n```\n\n\nWhy don't we care about ```\nA```\n as well:\n\n```\nf(n) ≤ Cg(n) + A for all n ≥ k\n```\n\n    ", "Answer": "\r\nThere are really two cases to consider here. For starters, imagine that your function g(n) has the property that g(n) ≥ 1 for all \"sufficiently large\" choices of n. In that case, if you know that\n\n\n  f(n) ≥ cg(n) + A,\n\n\nthen you also know that\n\n\n  f(n) ≥ cg(n) + Ag(n),\n\n\nso\n\n\n  f(n) ≥ (c + A)g(n).\n\n\nIn other words, if your function g is always at least one, then bounding f(n) by something of the form cg(n) + A is equivalent to bounding it with something of the form c'g(n) for some new constant c'. In that sense, adding some extra flexibility into the definition of big-O notation, at least in this case, wouldn't make a difference.\n\nIn the context of the analysis of algorithms, pretty much every function g(n) you might bound something with will be at least one, and so we can \"munch up\" that extra additive term by choosing a larger multiple of g.\n\nHowever, big-O notation is also used in many cases to bound functions that decrease as n increases. For example, we might say that the probability that some algorithm gives back the right answer is O(1 / n), where the function 1/n drops to 0 as a function of n. In this case, we use big-O notation to talk about how fast the function drops off. If the success probability is O(1 / n2), for example, that's a better guarantee than the earlier O(1 / n) success probability, assuming n gets sufficiently large. In that case, allowing for additive terms in the definition of big-O notation would actually break things. For example, intuitively, the function 1 / n2 drops to 0 faster than the function 1 / n, and using the formal definition of big-O notation you can see this because 1 / n2 ≤ 1 / n for all n ≥ 1. However, with your modified definition of big-O notation, we could also say that 1 / n = O(1 / n2), since\n\n\n  1 / n ≤ 1 / n2 + 1 for all n ≥ 1,\n\n\nwhich is true only because the additive 1 term bounds the 1/n term, not the 1/n2 we might have been initially interested in.\n\nSo the long answer to your question is \"the definition you proposed above is equivalent to the regular definition of big-O if we only restrict ourselves to the case where g(n) doesn't drop to zero as a function of n, and in the case where g(n) does drop the zero as a function of n your new definition isn't particularly useful.\"\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation: For loop with O(1) operation on the inside\r\n                \r\nI'm working on understanding big O notation a little bit better and got stumped on this problem in class.\nIf I had a for loop with a constant number of iterations that simply prints something to the console on each iteration:\n```\nfor(int i=1; i<10; i++)\n{\n   cout << \"Hello!\" << endl; \n}\n```\n\nWhat would the big O notation for this snippet of code be? The line that writes to the console takes 1 unit of time, and I would multiply this by the number of times it would be executed - in this case 10 - so I would get O(10). However, this just reduces to O(1).\nIs this a correct analysis?\n    ", "Answer": "\r\nEven for a loop like this:\n```\nfor (int i = 0; i < 1000000; i++) {\n  cout << i << '\\n';\n}\n```\n\nIt's technically still O(1). I am not going to go through the formal definition of Big O notation here. But in simple words, big O notation measures how fast the running time grows with respect to input. In this case, the theoretical runtime should be the same no matter what the input is. (in fact there isn't even input). So your analysis is correct.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation Time Question\r\n                \r\nWe have 3 functions with big o notations:\n\n```\nFunc A: O(n)\nFunc B: O(n^2)\nFunc C: O(2^n)\n```\n\n\nIf these functions does n proccesses in 10 seconds, how much time need to proccess 2 * n processes for each functions ? Also if you explain why i will be happy.\n\nThank You\n    ", "Answer": "\r\nActually, you really can't tell with only one data point. By way of example, a simplistic answer for the first one would be \"twice as long\", 20 seconds, since O(n) means the time complexity rises directly proportional to the input parameter.\n\nHowever, that fails to take into account that the big-O is usually simplified to show only the highest effect. The actual time taken may well be proportional to ```\nn```\n plus a constant 5 - in other words, there's a constant 5 second set-up time that doesn't depend on ```\nn```\n at all, then half a second per ```\nn```\n after that.\n\nThat would mean the time take would be ```\n15```\n seconds rather than ```\n20```\n. And, for the other cases mentioned it's even worse since O(n2) may actually be proportional to ```\nn^2 + 52n + 7```\n which means you would need three data points, assuming you even know all the components of the equation. It could even be something hideous like:\n\n```\n                  1      12\nn^2 + 52*n + 7 + --- + ------\n                  n    47*n^5\n```\n\n\nwhich would still technically be O(n2).\n\nIf they are simplistic equation (which is likely for homework), then you just need to put together the equations and then plug in 2n wherever you have n, then re-do the equation in terms of the original:\n\n```\nComplexity  Equation     Double N     Time Multiplier\n----------  --------   -------------  ---------------\nO(n)        t = n      t = 2n               2\nO(n^2)      t = n^2    t = (2n)^2\n                         = 4 * n^2          4\nO(2^n)      t = 2^n    t = 2^(2n)\n                         = 2^n * 2^n        2^n\n                                            (i.e., depends on\n                                                  original n)\n```\n\n\nSo., the answers I would have given would have been:\n\n\n```\n(A)```\n 20 seconds;\n```\n(B)```\n 40 seconds; and\n```\n(C)```\n 10 x 2n seconds.\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of \"addition\"\r\n                \r\nI'm learning a course about big O notation on Coursera. I watched a video about the big O of a Fibonacci algorithm (non-recursion method), which is like this:\n```\nOperation                     Runtime\ncreate an array F[0..n]        O(n)\nF[0] <-- 0                     O(1)\nF[1] <-- 1                     O(1)\nfor i from 2 to n:             Loop O(n) times\n  F[i] <-- F[i-1] + F[i-2]     O(n) => I don't understand this line, isn't it O(1)?\nreturn F[n]                    O(1)\nTotal: O(n)+O(1)+O(1)+O(n)*O(n)+O(1) = O(n^2)\n```\n\nI understand every part except  ```\nF[i] <-- F[i-1] + F[i-2]     O(n)```\n => I don't understand this line, isn't it O(1) since it's just a simple addition? Is it the same with ```\nF[i] <-- 1+1```\n?\nThe explanation they give me is:\"But the addition is a bit worse. And normally additions are constant time. But these are large numbers. Remember, the nth Fibonacci number has about n over 5 digits to it, they're very big, and they often won't fit in the machine word.\"\n\"Now if you think about what happens if you add two very big numbers together, how long does that take? Well, you sort of add the tens digit and you carry, and you add the hundreds digit and you carry, and add the thousands digit, you carry and so on and so forth. And you sort of have to do work for each digits place.\nAnd so the amount of work that you do should be proportional to the number of digits. And in this case, the number of digits is proportional to n, so this should take O(n) time to run that line of code\".\nI'm still a bit confusing. Does it mean a large number affects time complexity too? For example ```\na = n+1```\n is O(1) while ```\na = n^50+n^50```\n isn't O(1) anymore?\nVideo link for anyone who needed more information (4:56 to 6:26)\n    ", "Answer": "\r\nBig-O is just a notation for keeping track of orders of magnitude.  But when we apply that in algorithms, we have to remember \"orders of magnitude of WHAT\"?  In this case it is \"time spent\".\nCPUs are set up to execute basic arithmetic on basic arithmetic types in constant time.  For most purposes, we can assume we are dealing with those basic types.\nHowever if ```\nn```\n is a very large positive integer, we can't assume that.  A very large integer will need ```\nO(log(n))```\n bits to represent.  Which, whether we store it as bits, bytes, etc, will need an array of ```\nO(log(n))```\n things to store.  (We would need fewer bytes than bits, but that is just a constant factor.) And when we do a calculation, we have to think about what we will actually do with that array.\nNow suppose that we're trying to calculate ```\nn+m```\n.  We're going to need to generate a result of size ```\nO(log(n+m))```\n, which must take at least that time to allocate.  Luckily the grade school method of long addition where you add digits and keep track of carrying, can be adapted for big integer libraries and is ```\nO(log(n+m))```\n to track.\nSo when you're looking at addition, the log of the size of the answer is what matters.  Since ```\nlog(50^n) = n * log(50)```\n that means that operations with ```\n50^n```\n are at least ```\nO(n)```\n.  (Getting ```\n50^n```\n might take longer...) And it means that calculating ```\nn+1```\n takes time ```\nO(log(n))```\n.\nNow in the case of the Fibonacci sequence, ```\nF(n)```\n is roughly ```\nφ^n```\n where ```\nφ = (1 + sqrt(5))/2```\n so ```\nlog(F(n)) = O(n)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for a js Function to check subset\r\n                \r\ni have write a function that to check subset of an array.\nFor big-o notation, how to explain this function?\nis it just O(n) for this function?\n```\nfunction isSubset(arr1, arr2) {\n  return arr2.filter(function(e) { return arr1.indexOf(e) < 0 })==0;\n}\n\nisSubset(['A','B','C','D','E'],['A','D','Z'])  -> false\n\nisSubset(['A','B','C','D','E'],['A','E','D']) -> true\n\n\n```\n\n    ", "Answer": "\r\nThere are 4 rules for 0(n) classification.\nRule 1: Always worst Case.\nRule 2: Remove Constants.\nRule 3: Different inputs should have different variables.\n```\n  1. (+) for steps in order.\n\n  2. (*) for nested steps.\n```\n\nRule 4: Drop Non-dominant terms.\nSo, the two arrays in your input have different sizes. So, according to rule 3, your time complexity will be ```\n0(n*m)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "data structure performance in Big-O notation\r\n                \r\nIs there a good reference (table or chart) out there somewhere that shows all the time and space complexity in Big-O notation, for all the common operations (add,remove,iterate,etc.) for many of the common data structures (list,array,hash,tree,etc.)?  The more complete the better.\n    ", "Answer": "\r\nThis seems pretty thorough I'd double check the answers it posts though... http://essays.hexapodia.net/datastructures/\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to find Big-O notation of FFT multiplication under a loop\r\n                \r\nThe Big-O notation of FFT Multiplication is O(nlogn). What is Big-O notation of a FFT multiplication under a loop as given in algorithm below? The code is given in matlab and FFTmulti is a function for FFT Multiplication of two polynomials \n\n```\nrG=1;\nrN=1;\nAreaFunc=[1 2 5 2 3 6 7 2 4 5 6];\n\nN=length(AreaFunc);\n\nfor i=1:(N-1)\n    ref_coeff(i) = (AreaFunc(i+1) - AreaFunc(i)) / (AreaFunc(i+1) + AreaFunc(i));\nend\n\nref_coeff=[ref_coeff rN];\nG = (1 + rG) / 2;\nA0 = [1]; B0 = [-rG];\n\nfor i = 1 : length(ref_coeff)\n    G = G * (1 + ref_coeff(i));\n    A1 = [-ref_coeff(i) 0]; B1 = [1 0];\n    An = [0 A0] + FFTmulti(A1,B0);\n    Bn = [0 -ref_coeff(i)*A0] + FFTmulti(B1,B0);\n    A0=An;\n    B0=Bn;\nend\n\nA0 =fliplr(A0);\nnum = zeros(1, (floor(N/2)));\nnum = [num G];\n```\n\n    ", "Answer": "\r\nFFT complexity -for the best known optimization algorithms - is N*log2(N). \n\nIf you call it inside a loop of N, will be N^2 log2(N).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How can I find Big-O notation for my loops?\r\n                \r\nI am having trouble finding out the Big-O notation for this fragment of code.\nI need to find the notation for both for loops.\n\n```\npublic static int fragment(int n)\n{\n  int sum = 0;\n\n  for (int i = n; i >= 1; i /= 2)\n  {\n    for (int j = 1; j <= i; j *= 3)\n    {\n      sum++;\n    }\n  }\n\n  return sum;\n}\n```\n\n    ", "Answer": "\r\nThink of the two loops separately: \n\nFirst lets consider ```\nfor(int i=n; i>=1; i/=2)```\n on each iteration the value of ```\ni```\n will be divided by 2 until it reaches a value less than ```\n1```\n. Therefor the number of iterations N will be equal to the number of times you should divide ```\ni```\n by ```\n2```\n before it gets less than 1. There is a well known function representing this number - ```\nlog(n)```\n.\n\nNow lets consider the inner loop. ```\nfor(int j=1;j<=i; j*=3)```\n. Here you multiply j by 3 until it becomes more than ```\ni```\n. If you think a bit this is exactly the same number of iterations that the following slight modification of the first cycle would do: ```\nfor(int j=i; j>=1; j/=3)```\n. And with exactly the same explanation we have the same function(but with a different base - 3). Problem here is that the number of iterations is depending on i. \n\nSo now we have total complexity being:\n\nlog3(n) + log3(n/2) + log3(n/4) ... + log3(1) =\n\nlog3(n) + log3(n) - log3(2) + log3(n) .... - log3(2log2(n)) = \n\nlog3(n) * log2 (n) - log3(2) - 2 * log3(2) - ... log2(n) * log3(2) =\n\nlog3(n) * log2 (n) - log3(2) * (1 + 2 + ... log2) = \n\nlog3(n) * log2 (n) - log3(2) * (log2(n) * (log2(n) + 1)) / 2 =\n\nlog2 (n) * (log3(n) - log3(2) * (log2(n) + 1) / 2) =\n\nlog2 (n) * (log3(n) -   (log3(n) + log3(2)) / 2) =\n\n(log2 (n) * log3(n)) / 2 - (log2 (n) * log3(2)) / 2\n\nCalculation is bit tricky and I use a few properties of logarithm. However the final conclusion is that the cycles are of the complexity ```\nO(log(n)^2)```\n(remember you can ignore base of a logarithm).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Time complexity in big O notation of algorithms\r\n                \r\nI was given the following time complexities and was supposed to assign them to the correct \"classification\"(big O notation).\nFirst off:\n```\nf(n) = 1000 + log_2(n^(9n))+3*n*log_1000(n^n) -> f in O(n^2*log(n))\n```\n\nSecond:\n```\nf(n) = 2*n^3+5 * 3^n -> f in O(3^n)\n```\n\nAnd last but not least:\n```\nf(n) = 2*n^7+5*e^n -> f in O(e^n)\n```\n\nNow I'm asking for verification since I'm really not sure whether I'm right.\n    ", "Answer": "\r\n```\nf(n) = 1000 + log_2(n^(9n))+3nlog_1000(n^n)\n```\n\nGet rid of constants and log base\n```\n=> O(log(n^(n)) + nlog(n^n))\n```\n\nFrom log rule\n```\nlog(a^b) = b*log(a)\n\n=> O(nlog(n) + n*nlog(n))\n=> O((n^2)*log(n))\n```\n\n\n```\nf(n) = 2*n^3+5 * 3^n\n```\n\nGet rid of constants\n```\n=> O(n^3 + 3^n)\n```\n\nExponential wins\n```\n=> O(3^n) \n```\n\n\n```\nf(n) = 2n^7+5e^n\n```\n\nGet rid of constants\n```\n=> O(n^7 + e^n)\n```\n\nAgain, exponential wins\n```\n=> O(e^n)\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of Formula - P *R Squared\r\n                \r\nI want to calcuate the Big O notation for an area of circle.I wonder what is the time complexity of Pi *r^2? is it O(n) or O(n^2)? Please also check if my way of doing is correct.\n```\n Algorithm\n   Step 1: Start //  f(n)=O(1)  ( Executes 1 time)\n   Step 2: Get an integer input from user for AREA and RADIUS//. f(n)=O(n)=O(1) ( Executes 1 time)\n   Step 3: Print the statement \"Enter the Radius of Circle: //f(n)=O(n)=O(1) ( Executes 1 time)\n    Step 4: Calculate the area of the circle using the formula of  \n    //f(n)=O(n)=?\n    (r - Radius) //f(n)=O(n)=O(1) ( Executes 1 time)\n    \n   Step 5: Print the statement \"Area of Circle:\" //f(n)=O(n)=O(1) ( Executes 1 time)\n    \n   Step 6: Print Area//f(n)=O(n)=O(1) ( Executes 1 time)\n    \n   Step 7: Stop //f(n)=O(n)=O(1) ( Executes 1 time)\n    \n    f(n)=O(n)=?\n```\n\n    ", "Answer": "\r\nThis depends on your model of computation.\nFor most practical purposes its ```\nO(1)```\n, because the runtime does not depend on your input ```\nr```\n. This assumes implicitly that your input is bounded, for example fits into a 32-bit integer.\nMore theoretical you can think about arbitrary big ```\nr```\n. The ```\nN```\n in the big O notation then is normally the length (number of bits) of ```\nr```\n and not the size of the number. Here you cant assume that you can multiply any two numbers in ```\nO(1)```\n but it will depend on the length of the numbers.\nIn this case you would get ```\nO(N^2)```\n with a naive multiplying algorithm, which can be improved to ```\nO(N log N)```\n with more advanced techniques. See https://en.wikipedia.org/wiki/Multiplication_algorithm#Computational_complexity_of_multiplication .\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "big-theta() and big-O() notations\r\n                \r\nwhat will be the big-theta() and big-O() notations. for this\n\n(2^n) - (n x 2^n)\n\nor in another question is n2^n greater than 2^n in time complexity ?\n    ", "Answer": "\r\nYour question does not make sense because your function is not positive for large n. If I get your notation you have (1-n)*(2^n) which is kind of O(-2^n).\nBut, as I said, that is not really correct because the O and Theta abstraction does not work for negative functions. Hence, a minus can only used if the RHS term is smaller for large numbers.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for multiple functions\r\n                \r\nI've a question regarding big O notation when one is using multiple functions.\nLets say I want to find out what the time complexity is for the following pseudo code:\n\n```\nheap sort array of size n\nfor i = 1 to n{\n  retrieve array[i]\n  change value of array[i]\n}\n```\n\n\nI know that using heap sort is O(n log(n)). Since retrieving and changing data in an array is O(1), the loop is of complexity O(n).\nNow my question is: what is the complexity of this code as a whole? Is it just the largest time complexity; O(n log(n)) in this case?\nIf so, what would be the complexity of a function that would look like this:\n\n```\nfor i = 1 to n{\n  // nothing fancy here\n}\nfor y = 1 to n{\n  // nothing fancy here either\n}\n```\n\n\nThanks in advance.\n    ", "Answer": "\r\n```\nfor i = 1 to n{\n  // nothing fancy here\n}  //O(n)\n\nfor y = 1 to n{\n  // nothing fancy here\n}  //O(n)\n```\n\n\nSo together it is ```\nO(n) + O(n) = O(n)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation: O((n-1)!) or O(n!) for T((n-1)!)?\r\n                \r\nI have an algorithm that I have analysed to have a time complexity of ```\n(n-1)!```\n.\n\nTo put this in big-O notation, would I write ```\nO((n-1)!)```\n or ```\nO(n!)```\n?\n    ", "Answer": "\r\nLet's have a look:\n\n\nlimn → ∞ n!/(n-1)! = limn → ∞ n = ∞\n\n\nThus (see the definition) ```\n(n-1)!```\n is in ```\no(n!)```\n (little-o).\n\nSo if your algorithm complexity is in ```\nO((n-1)!)```\n, it is also in ```\nO(n!)```\n, so it is not wrong to write ```\nO(n!)```\n instead of ```\nO((n-1)!)```\n, but ```\nO((n-1)!)```\n is a tighter bound, so you should use this one.\n\nIt is basically the same as writing ```\nπ ≤ 22/7```\n or ```\nπ ≤ 4```\n. Both is right, but ```\nπ ≤ 22/7```\n is closer.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is O(log(n)) the correct Big O notation for this function?\r\n                \r\nI wrote a function to determine if a given number is a prime number. I have the following code\n\n```\ndef prime(n):\n    if n == 1:\n        return n\n    elif n > 1: \n        for i in range(2, (1+floor(sqrt(n)))):\n            if n % i == 0:\n                return False\n        return True\n```\n\n\nI \"calculated\" the Big O notation to be O(log(n)), but I'm not sure if it's correct.\n    ", "Answer": "\r\nModulo is not ```\nO(log(n))```\n, it's just a part of standard integer division and is O(1). You are doing ```\nsqrt(n)```\n iterations, therefore it's ```\nO(sqrt(n))```\n. If modulo actually were O(log(n) your result would be ```\nO(sqrt(n)log(n))```\n not ```\nO(log(n))```\n since it's repeated sqrt(n) times. Floor is definately O(1), sqrt depends on the implementation but I'm pretty sure it will be O(1) in python. \n\nIf you had two sepearate loops: one with ```\nO(sqrt(n))```\n and another with ```\nO(n^2)```\n then the whole algo would be ```\nO(n^2)```\n because ```\nn^2```\n grows much faster than ```\nsqrt(n)```\n and you are adding the two together not multiplying them. You multiply when you have a loop inside another loop, if they follow each other you add them and you can't throw away the slower growing part of the multiplication.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O-Notation, calcuation rule\r\n                \r\nI've got a question about the Big-O-Notation.\nWe have zu proof or show, that the following statemates, are not correct. And i've got no clue how to show it.\n\na) Given a Polynom p with the degree of k >= 1: log(p(n)) ∈ Θ(log(n))\n\nb)  f, g ∈ Θ(h) ⇒ |f − g| ∈ Θ(h), where |f − g| : n → |f(n) − g(n)|\n\nCan anyone of you tell me what is correct an what not an maybe explaine me how can show this?\n    ", "Answer": "\r\n\n  a) Given a Polynom p with the degree of k >= 1: log(p(n)) ∈ Θ(log(n))\n\n\nLet p(n) = a[0] + a[1]n + a[2]n^2 + ... + a[k]n^k with k >= 1. To show Theta we must show O and Omega, or O both ways. To show O, we show that there exists a c such that for n > n0, log(p(n)) <= c log(n). First, we note that a <= b iff e^a <= e^b (since e^x is monotonically increasing). Therefore, log(p(n)) <= c log(n) iff p(n) <= n^c. We may factor from every term of p(n) the highest-order term n^k and the absolute value of the coefficient with largest magnitude (call this a') to get: p(n) = (n^k)(a')(b[0]/n^k + b[1]/n^(k-1) + ... + b[n]). Note that all b[n] < 1 and thus we have p(n) < (n^k)(a')n. But p(n) < (n^k)(a')n = n^(k+1)(a') <= n^c. Let c = k + 1 + z. Then we need n^(k+1)(a') <= n^(k+1)(n^z) which is true if a' <= n^z. As long as n > 1, there exists a z such that n^z > a' for any fixed a': choose z > log_n a'.\n\nShowing the O the other way around is left as an exercise. Hint: n^k <= p(n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "calculating big O notation\r\n                \r\n```\nsum = 0;\nfor (i=0;i<n/2;i++)\n    for (j=i; j<n/4; j++)\n        sum++;\n```\n\n\nWhat is the big O for the above code?\nI calculated the big O but I'm not sure if it's correct.\n\nThis is my answer \n\nthe outer loop will run ```\nn/2```\n times\n\nthe inner loop will run ```\n(n/4 - 1) = ((n-4)/4 )```\n\n\n```\nT(n) = (n/2) * ((n-4)/4 )\n     = (n^2 -4n + 16)/8\n     = 1/8 n^2 - 1/2 n +2\n```\n\n\nso the big O is ```\nO(n^2)```\n is this correct?\n    ", "Answer": "\r\nYes, in big O computations leading coefficients and subleading terms are dropped. \n\nThe formal definition is that ```\nf(x)```\n is ```\nO(g(x))```\n if ```\nf(x) <= M g(x)```\n for ```\nx```\n beyond some ```\nx0```\n and some constant ```\nM```\n. In this case, ```\nM = 1/8```\n and ```\nx0 = 4```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Regarding Big-O Notation for \"reversing the string\" code\r\n                \r\nI wrote a code to reverse a string but I want to calculate the big o notation for my code.\nI guess it is O(m+n). please correct me if this is wrong.   \n\n```\nuse strict;\nuse warnings;\n\nprint \"Please enter you string \\n\";\nchomp (my $string = <STDIN>);\nprint \"Entered string is $string \\n\";\nmy @word = split(\" \",$string);\n\nmy $eachword;\nforeach $eachword(@word){\n    my @each = split(//,$eachword);\n    my $wordlength = scalar(@each);\n    for (my $j=$wordlength-1; $j>=0; $j--) {\n        print $each[$j];\n\n    }\n    print \" \";\n}\n```\n\n\ninput: \n\n```\nhai how are you\n```\n\n\no/p :    \n\n```\niah woh era uoy\n```\n\n    ", "Answer": "\r\nUsing the big o notation you don't care about constants but the complexity assuming values near infinity, so m+n or 2n are within o(n). Let n be your number of words and m be the maximum number of characters which you can assume as constant since n is your real scalability issue. So it's n + c(constant) = o(n). Further reading: Big O notation.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the Big O notation for this function? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                This question is unlikely to help any future visitors; it is only relevant to a small geographic area, a specific moment in time,  or an extraordinarily narrow situation that is not generally applicable to the worldwide audience of the internet. For help making  this question more broadly applicable, visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI have written a function and I need to know the big O notation for it.\nI have tried to slove this myself and I get O(N^2), however I have been told that this is not the correct answer.\n\nCan someone please tell me what the correct notation is and also a step by step explanation of how they came to that answer?\n\nThe function is below.\n\nThanks in advance\n\n```\n    public static string Palindrome(string input) \n    {\n        string current = string.Empty;\n        string longest = string.Empty;\n\n        int left;\n        int center;\n        int right;\n\n\n        if (input == null || input == string.Empty || input.Length == 1)  {   return input;   }\n\n\n        for (center = 1; center < input.Length -1; center++) \n        {\n            left = center - 1;  \n            right = center + 1;\n\n            if (input[left] == input[center])\n            {\n                left--;\n            }\n\n            while (0 <= left && right < input.Length) \n            {\n                if (input[left] != input[right])\n                {\n                    break;\n                }\n\n                current = input.Substring(left, (right - left + 1));\n\n                longest = current.Length > longest.Length ? current : longest;\n\n                left--;  \n                right++;\n            }\n        }\n        return longest;\n    }\n```\n\n    ", "Answer": "\r\nThis is O(n^3) algorithm:\n\nThis part takes O(n^2):\n\n\n  // O(n) times for while loop\n\n\n```\n        while (0 <= left && right < input.Length)   \n        {\n            if (input[left] != input[right])\n            {\n                break;\n            }\n```\n\n\n\n  // taking substring is O(n)\n\n\n```\n            current = input.Substring(left, (right - left + 1)); \n\n            longest = current.Length > longest.Length ? current : longest;\n\n            left--;  \n            right++;\n        }\n```\n\n\nAlso there is an outer O(n), ```\nfor```\n loop, which causes to O(n*n^2).\n\nYou can improve your algorithm by changing this lines:\n\n```\n   current = input.Substring(left, (right - left + 1)); \n   longest = current.Length > longest.Length ? current : longest;\n```\n\n\nto:\n\n```\n   currentLength = right - left + 1;\n   if(currentLength > longest)\n   { \n     longest = current.Length > longest.Length ? current : longest;\n     longestLeft = left;\n     longestRight = right;\n   }\n```\n\n\nand finally return a substring from longestLeft to longestRight. Actually avoid to use ```\nsubstring```\n method too many times.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Finding big-O notation of forumulas [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     This question does not appear to be about a specific programming problem, a software algorithm, or software tools primarily used by programmers. If you believe the question would be on-topic on another Stack Exchange site, you can leave a comment to explain where the question may be able to be answered.\r\n                \r\n                    \r\n                        Closed 6 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI'm trying to see if I'm correct in my work for finding the big-O notation of some formulas. Each of these formulas are the number of operations in some algorithm. These are the formulas:\n\nForumulas\n\na.) n2 + 5n\n\nb.) 3n2 + 5n\n\nc.) (n + 7)(n - 2)\n\nd.) 100n + 5\n\ne.) 5n +3n2\n\nf.) The number of digits in 2n\n\ng.)The number of times that n can be divided by 10 before dropping below 1.0\n\nMy answers:\n\na.) O(n2)\n\nb.) O(n2)\n\nc.) O(n2)\n\nd.) O(n)\n\ne.) O(n2)\n\nf.) O(n)\n\ng.) O(n)\n\nAm I correct on my analysis?\n    ", "Answer": "\r\nLet's go through this one at a time.\n\n\n  a.) n2 + 5. Your answer: O(n2)\n\n\nYep! You're ignoring lower-order terms correctly.\n\n\n  b.) 3n2 + 5n. Your answer: O(n2).\n\n\nYep! Big-O eats constant factors for lunch.\n\n\n  c.) (n + 7)(n - 2). Your answer: O(n2).\n\n\nYep! You could expand this out into n2 + 5n - 14 and from there drop the low-order terms to get O(n2), or you could realize that n + 7 = O(n) and n - 2 = O(n) to see that this is the product of two terms that are each O(n).\n\n\n  d.) 100n + 5. Your answer: O(n).\n\n\nYep! Again, dropping constants and lower-order terms.\n\n\n  e.) 5n + 3n2. Your answer: O(n2).\n\n\nYep! Order is irrelevant; 5n is still a low-order term.\n\n\n  f.) The number of digits in 2n. Your answer: O(n).\n\n\nThis one is technically correct but is not a good bound. Remember that big-O notation gives an upper bound and you are correct that the number n has O(n) digits, but only in the sense that the number of digits of n is asymptotically less than n. To see why this bound isn't very good, let's look at the numbers 10, 100, 1000, 10000, and 100000. These numbers have 2, 3, 4, 5, and 6 digits, respectively. In other words, growing by a factor of ten only grows the number of digits by one. If the O(n) bound you had were tight, then you'd expect that the number of digits would grow by a factor of ten every time you made the number ten times bigger, which isn't accurate.\n\nAs a hint for this one, if a number has d digits, then it's between 10d and 10d+1 - 1. That means the numeric value of a d-digit number is exponential as a function of d. So, if you start with a number of digits, the numeric value is exponentially larger. Try running this backwards. If you have a numeric value that you know is exponentially larger than the number of digits, what does that mean about the number of digits as a function of the numeric value?\n\n\n  f.) The number of times that n can be divided by 10 before dropping below 1.0. Your answer: O(n)\n\n\nThis one is also technically correct but not a very good bound. Let's take the number 100,000, for example. You can divide this by 10 seven times before you drop below 1.0, but giving a bound of O(n) means that you're saying the answers grows linearly as a function of n, so doubling n should double the number of times you can divide by ten... but is that actually the case?\n\nAs a hint, the number of times you can divide a number by ten before it drops below 1.0 is closely related to the number of digits in that number. If you can figure out this problem, you'll figure out part (e), and vice-versa.\n\nGood luck!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation Of Exponential Functions\r\n                \r\nI have noticed that big-O of 1000n or 10n is the same thing as O(n), but big-O of 2^n and 3^n are different: O(2^n) and O(3^n), what I don't get is why can't we ignore the constants in this case (2 or 3) and whether there is any mathematical proof  justifying this?\n    ", "Answer": "\r\nBecause there is no constant value of ```\nk```\n that satisfies the inequality ```\n3^n <= k * 2^n```\n for arbitrarily large ```\nn```\n.  Thus ```\nf(n) = 3^n```\n is not a member of ```\nO(2^n)```\n.\n\nSee http://en.wikipedia.org/wiki/Big_O_notation#Family_of_Bachmann.E2.80.93Landau_notations.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of simple anagram function\r\n                \r\nI've worked up the following code which finds anagrams. I had thought the big O notation for this was ```\nO(n)```\n But was informed by my instructor that I am incorrect. I am confused on why this is not correct however, would anyone be able to offer any advice?\n\n```\n# Define an anagram.\ndef anagram(s1, s2):\n    return sorted(s1) == sorted(s2)\n\n# Main function.\ndef Question1(t, s):\n    # use built in any function to check any anagram of t is substring of s\n    return any(anagram(s[i: i+len(t)], t)\n                 for i in range(len(s)-len(t)+ 1))\n```\n\n\nFunction Call:\n\n```\n# Simple test case.\nprint Question1(\"app\", \"paple\")\n# True\n```\n\n    ", "Answer": "\r\n\n  any anagram of t is substring of s\n\n\nThat's not what your code says. \n\nYou have \"any substring of s is an anagram of t\", which might be equivalent, but it's easier to understand that way. \n\n\n\nAs for complexity, you need to define what you're calling N... Is it ```\nlen(s)-len(t)+ 1```\n? \n\nThe function ```\nany()```\n has complexity N, in that case, yes. \n\nHowever, you've additionally called ```\nanagram```\n over an input of T length, and you seem to have ignored that. \n\n```\nanagram```\n calls ```\nsorted```\n twice. Each call to ```\nsorted```\n is closer to  ```\nO(T * log(T))```\n itself assuming merge sort. You're also performing a list slice, so it could be slightly higher. \n\nLet's say your complexity is somewhere on the order of ```\n(S-T) * 2 * (T * log(T))```\n where T and S are lengths of strings. \n\nThe answer depends on which string of your input is larger.\n\nBest case is that they are the same length because then your range only has one element. \n\nBig O notation is worst case, though, so you need to figure out which conditions generate the most complexity in terms of total operations. For example, what if T > S? Then ```\nlen(s)-len(t)+ 1```\n will be non positive, so does the code run more or less than equal length strings? And what about S < T or S = 0?\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O-notation given probabalistic bounds?\r\n                \r\nI'm trying to estimate the runtime complexity of the following algorithm:\n\n```\nfor i in range(0, len(arr)):\n    for j in range(i+1, len(arr)):\n        if distance(arr[i], arr[j]) > 2:\n            pass\n```\n\n\nThe complexity of the distance function is ```\nmin(len(arg1), len(arg2))```\n. In theory, the maximum length of the arguments can be upto N, but in practice it is usually note more than 20% of N.\n\nFrom this, I can estimate the runtime function as:\n\nf(N) = N*(N/2)*(N*.2)\n\nIs that O(N^2) in the big O notation or O(N^3)? If it's O(n^3), then how does one justify that the runtime is always going to be closer to O(n^2) than to O(n^3) in practice?\n\nThanks\n    ", "Answer": "\r\nYou ask:\n\n\n  If it's O(n^3), then how does one justify that the runtime is always going to be closer to O(n^2) than to O(n^3) in practice?\n\n\nThe answer is that \"closer\" doesn't matter. Only \"bigger than\" and \"smaller than\" matter. \n\nBig O gives an upper bound\n\nIf the runtime complexity of a procedure eventually exceeds c * n^2 for any constant c or larger and a big enough value n, then it cannot possibly be O(n^2).\n\nThat's because the big-O operator doesn't give an estimate; it gives an upper bound. Even a procedure that runs in constant time is still O(n^3). (It's also O(n^2), O(log(n)), O(n!), and so on). That's because it's smaller than all of those runtimes for some constant multiplier c and large values of n.\n\nA concrete example\n\nTo make this concrete, consider the following:\n\n```\n>>> def fa(n):\n...     return n * n * n // 10\n... \n>>> def f2(n):\n...     return n * n\n... \n>>> def f3(n):\n...     return n * n * n\n... \n```\n\n\nFor the above runtimes and small ```\nn```\n, ```\nfa```\n is still less than or equal to ```\nf2```\n:\n\n```\n>>> fa(10), f2(10), f3(10)\n(100, 100, 1000)\n```\n\n\nBut if we multiply ```\nn```\n by 10, ```\nfa```\n exceeds ```\nf2```\n. \n\n```\n>>> fa(100), f2(100), f3(100)\n(100000, 10000, 1000000)\n```\n\n\nAnd it's not hard to see that even if we boost ```\nf2```\n by a constant multiplier ```\nc```\n, we can still find a value of ```\nn```\n such that ```\nfa(n)```\n is larger.\n\n```\n>>> def f2_boost(n, c):\n...     return f2(n) * c\n... \n>>> fa(1000), f2_boost(1000, 10), f3(1000)\n(100000000, 10000000, 1000000000)\n```\n\n\nWhy use a constant multiplier?\n\nYou might still find it confusing that a procedure with a runtime of n^3 * 0.1 falls in the same big-O category as a procedure with a runtime of 1000 * n^3. After all, the absolute difference between these two runtimes is huge! \n\nThis is a bit harder to explain, but it starts to make sense when you remind yourself that big O notation is supposed to describe scaling behavior. Or, to put it another way, big O notation is supposed to describe how the runtime varies when we change the size of the units that we use for our measurements.\n\nLet's take a concrete example: imagine you want to know the height of a building. And suppose someone says \"oh, it's about 300 meters.\" You might feel satisfied by that response; you might not care that it's really 315 meters; 300 is a good enough estimate. But what if, instead, they said \"oh, it's about 300 meters... or was that 300 feet?\" You'd probably feel a lot less satisfied, because 300 meters would be more than three times as high as 300 feet.\n\nIn computer science, we have exactly that problem when measuring time. In fact, it's even worse. Different computers might be vastly faster or slower than others. If we measure time in \"number of calculations performed by a computer,\" then for some computers, we will be measuring time in hundredths of a second, and for other computers, we will be measuring time in billionths of a second. If we want to describe the behavior of the algorithm in a way that isn't skewed by that huge difference, then we need a measurement that is \"scale invariant\" -- that is, a measurement that gives the same answer whether we use hundredths of seconds or billionths of seconds as our units.\n\nBig O notation provides such a measurement. It gives us a way to measure runtime without needing to worry so much about the size of the units we use to measure time. In essence, saying that an algorithm is O(n^2) is saying that for any unit of time equal to or larger than some value c, there is a corresponding value for n such that our procedure will complete before c * n^2 for all larger values of n.\n\nEstimating runtimes\n\nIf you want to talk about estimating runtimes, then you want a measurement called \"big theta.\" Take a look at this answer for details. In short, big O gives an upper bound for arbitrarily large multiplier c; big omega gives a lower bound for arbitrarily large multiplier c; and big theta gives a function that defines both an upper bound and a lower bound, depending on the choice of multiplier c.\n\nIn your case, the big theta value would be O(n^3) because you can choose a constant multiplier c1 such that c1 * n^3 is always larger than n^3 / 10, and you can choose a constant multiplier c2 such that c2 * n^3 is always smaller than n^3 / 10. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of adding and removing elements from Array\r\n                \r\nso I'm quite sure that adding elements to an Array (and doubling the capacity once the maximum is reached) has the Big-O-Notation O(n). Same with removing elements, and halving capacity once only half the capacity or less is filled with elements.\nWhen combining add and remove operations on the same array I think the Big-O-Notation would be O(n) as well.\nSo my question is would there be any worse scenario than O(n)?\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O, how do you calculate/approximate it?\r\n                \r\nMost people with a degree in CS will certainly know what Big O stands for.\nIt helps us to measure how well an algorithm scales. \n\nBut I'm curious, how do you calculate or approximate the complexity of your algorithms?\n    ", "Answer": "\r\nI'll do my best to explain it here on simple terms, but be warned that this topic takes my students a couple of months to finally grasp. You can find more information on the Chapter 2 of the Data Structures and Algorithms in Java book.\n\n\n\nThere is no mechanical procedure that can be used to get the BigOh.\n\nAs a \"cookbook\", to obtain the BigOh from a piece of code you first need to realize that you are creating a math formula to count how many steps of computations get executed given an input of some size.\n\nThe purpose is simple: to compare algorithms from a theoretical point of view, without the need to execute the code. The lesser the number of steps, the faster the algorithm.\n\nFor example, let's say you have this piece of code:\n\n```\nint sum(int* data, int N) {\n    int result = 0;               // 1\n\n    for (int i = 0; i < N; i++) { // 2\n        result += data[i];        // 3\n    }\n\n    return result;                // 4\n}\n```\n\n\nThis function returns the sum of all the elements of the array, and we want to create a formula to count the computational complexity of that function:\n\n```\nNumber_Of_Steps = f(N)\n```\n\n\nSo we have ```\nf(N)```\n, a function to count the number of computational steps. The input of the function is the size of the structure to process. It means that this function is called such as:\n\n```\nNumber_Of_Steps = f(data.length)\n```\n\n\nThe parameter ```\nN```\n takes the ```\ndata.length```\n value. Now we need the actual definition of the function ```\nf()```\n. This is done from the source code, in which each interesting line is numbered from 1 to 4.\n\nThere are many ways to calculate the BigOh. From this point forward we are going to assume that every sentence that doesn't depend on the size of the input data takes a constant ```\nC```\n number computational steps.\n\nWe are going to add the individual number of steps of the function, and neither the local variable declaration nor the return statement depends on the size of the ```\ndata```\n array.\n\nThat means that lines 1 and 4 takes C amount of steps each, and the function is somewhat like this:\n\n```\nf(N) = C + ??? + C\n```\n\n\nThe next part is to define the value of the ```\nfor```\n statement. Remember that we are counting the number of computational steps, meaning that the body of the ```\nfor```\n statement gets executed ```\nN```\n times. That's the same as adding ```\nC```\n, ```\nN```\n times:\n\n```\nf(N) = C + (C + C + ... + C) + C = C + N * C + C\n```\n\n\nThere is no mechanical rule to count how many times the body of the ```\nfor```\n gets executed, you need to count it by looking at what does the code do. To simplify the calculations, we are ignoring the variable initialization, condition and increment parts of the ```\nfor```\n statement.\n\nTo get the actual BigOh we need the Asymptotic analysis of the function. This is roughly done like this:\n\n\nTake away all the constants ```\nC```\n.\nFrom ```\nf()```\n get the polynomium in its ```\nstandard form```\n.\nDivide the terms of the polynomium and sort them by the rate of growth.\nKeep the one that grows bigger when ```\nN```\n approaches ```\ninfinity```\n.\n\n\nOur ```\nf()```\n has two terms:\n\n```\nf(N) = 2 * C * N ^ 0 + 1 * C * N ^ 1\n```\n\n\nTaking away all the ```\nC```\n constants and redundant parts:\n\n```\nf(N) = 1 + N ^ 1\n```\n\n\nSince the last term is the one which grows bigger when ```\nf()```\n approaches infinity (think on limits) this is the BigOh argument, and the ```\nsum()```\n function has a BigOh of:\n\n```\nO(N)\n```\n\n\n\n\nThere are a few tricks to solve some tricky ones: use summations whenever you can.\n\nAs an example, this code can be easily solved using summations:\n\n```\nfor (i = 0; i < 2*n; i += 2) {  // 1\n    for (j=n; j > i; j--) {     // 2\n        foo();                  // 3\n    }\n}\n```\n\n\nThe first thing you needed to be asked is the order of execution of ```\nfoo()```\n. While the usual is to be ```\nO(1)```\n, you need to ask your professors about it. ```\nO(1)```\n means (almost, mostly) constant ```\nC```\n, independent of the size ```\nN```\n.\n\nThe ```\nfor```\n statement on the sentence number one is tricky. While the index ends at ```\n2 * N```\n, the increment is done by two. That means that the first ```\nfor```\n gets executed only ```\nN```\n steps, and we need to divide the count by two.\n\n```\nf(N) = Summation(i from 1 to 2 * N / 2)( ... ) = \n     = Summation(i from 1 to N)( ... )\n```\n\n\nThe sentence number two is even trickier since it depends on the value of ```\ni```\n. Take a look: the index i takes the values: 0, 2, 4, 6, 8, ..., 2 * N, and the second ```\nfor```\n get executed: N times the first one, N - 2 the second, N - 4 the third... up to the N / 2 stage, on which the second ```\nfor```\n never gets executed.\n\nOn formula, that means:\n\n```\nf(N) = Summation(i from 1 to N)( Summation(j = ???)(  ) )\n```\n\n\nAgain, we are counting the number of steps. And by definition, every summation should always start at one, and end at a number bigger-or-equal than one.\n\n```\nf(N) = Summation(i from 1 to N)( Summation(j = 1 to (N - (i - 1) * 2)( C ) )\n```\n\n\n(We are assuming that ```\nfoo()```\n is ```\nO(1)```\n and takes ```\nC```\n steps.)\n\nWe have a problem here: when ```\ni```\n takes the value ```\nN / 2 + 1```\n upwards, the inner Summation ends at a negative number! That's impossible and wrong. We need to split the summation in two, being the pivotal point the moment ```\ni```\n takes ```\nN / 2 + 1```\n.\n\n```\nf(N) = Summation(i from 1 to N / 2)( Summation(j = 1 to (N - (i - 1) * 2)) * ( C ) ) + Summation(i from 1 to N / 2) * ( C )\n```\n\n\nSince the pivotal moment ```\ni > N / 2```\n, the inner ```\nfor```\n won't get executed, and we are assuming a constant C execution complexity on its body.\n\nNow the summations can be simplified using some identity rules:\n\n\nSummation(w from 1 to N)( C ) = N * C\nSummation(w from 1 to N)( A (+/-) B ) = Summation(w from 1 to N)( A ) (+/-) Summation(w from 1 to N)( B )\nSummation(w from 1 to N)( w * C ) = C * Summation(w from 1 to N)( w ) (C is a constant, independent of ```\nw```\n)\nSummation(w from 1 to N)( w ) = (N * (N + 1)) / 2\n\n\nApplying some algebra:\n\n```\nf(N) = Summation(i from 1 to N / 2)( (N - (i - 1) * 2) * ( C ) ) + (N / 2)( C )\n\nf(N) = C * Summation(i from 1 to N / 2)( (N - (i - 1) * 2)) + (N / 2)( C )\n\nf(N) = C * (Summation(i from 1 to N / 2)( N ) - Summation(i from 1 to N / 2)( (i - 1) * 2)) + (N / 2)( C )\n\nf(N) = C * (( N ^ 2 / 2 ) - 2 * Summation(i from 1 to N / 2)( i - 1 )) + (N / 2)( C )\n\n=> Summation(i from 1 to N / 2)( i - 1 ) = Summation(i from 1 to N / 2 - 1)( i )\n\nf(N) = C * (( N ^ 2 / 2 ) - 2 * Summation(i from 1 to N / 2 - 1)( i )) + (N / 2)( C )\n\nf(N) = C * (( N ^ 2 / 2 ) - 2 * ( (N / 2 - 1) * (N / 2 - 1 + 1) / 2) ) + (N / 2)( C )\n\n=> (N / 2 - 1) * (N / 2 - 1 + 1) / 2 = \n\n   (N / 2 - 1) * (N / 2) / 2 = \n\n   ((N ^ 2 / 4) - (N / 2)) / 2 = \n\n   (N ^ 2 / 8) - (N / 4)\n\nf(N) = C * (( N ^ 2 / 2 ) - 2 * ( (N ^ 2 / 8) - (N / 4) )) + (N / 2)( C )\n\nf(N) = C * (( N ^ 2 / 2 ) - ( (N ^ 2 / 4) - (N / 2) )) + (N / 2)( C )\n\nf(N) = C * (( N ^ 2 / 2 ) - (N ^ 2 / 4) + (N / 2)) + (N / 2)( C )\n\nf(N) = C * ( N ^ 2 / 4 ) + C * (N / 2) + C * (N / 2)\n\nf(N) = C * ( N ^ 2 / 4 ) + 2 * C * (N / 2)\n\nf(N) = C * ( N ^ 2 / 4 ) + C * N\n\nf(N) = C * 1/4 * N ^ 2 + C * N\n```\n\n\nAnd the BigOh is:\n\n```\nO(N²)\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Order of magnitude using Big-O notation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Big O, how do you calculate/approximate it?\r\n                            \r\n                                (24 answers)\r\n                            \r\n                    \r\n                Closed 7 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nThis is likely ground that has been covered but I have yet to find an explanation that I am able to understand. It is likely that I will soon feel embarrassed.\n\nFor instance, I am trying to find the order of magnitude using Big-O notation of the following:\n\n```\ncount = 0;\nfor (i = 1; i <= N; i++)\n    count++;\n```\n\n\nWhere do I begin to find what defines the magnitude? I'm relatively bad at mathematics and, even though I've tried a few resources, have yet to find something that can explain the way a piece of code is translated to an algebraic equation. Frankly, I can't even surmise a guess as to what the Big-O efficiency is regarding this loop.\n    ", "Answer": "\r\nThese notations (big O, big omega, theta) simply say how does the algorithm will be \"difficult\" (or complex) asymptotically when things will get bigger and bigger. \n\nFor big O, having two functions: f(x) and g(x) where f(x) = O(g(x)) then you can say that you are able to find one x from which g(x) will be always bigger than f(x). That is why the definition contains \"asymptotically\" because these two functions may have any run at the beginning (for example f(x) > g(x) for few first x) but from the single point, g(x) will get always superior (g(x) >= f(x)). So you are interested in behavior in a long run (not for small numbers only). Sometimes big-O notation is named upper bound because it describes the worst possible scenario (it will never be asymptotically more difficult that this function).\n\nThat is the \"mathematical\" part. When it comes to practice you usually ask: How many times the algorithm will have to process something? How many operations will be done?\n\nFor your simple loop, it is easy because as your N will grow, the complexity of algorithm will grow linearly (as simple linear function), so the complexity is O(N). For N=10 you will have to do 10 operations, for N=100 => 100 operations, for N=1000 => 1000 operations... So the growth is truly linear.\n\nI'll provide few another examples:\n\n```\nfor (int i = 0; i < N; i++) {\n   if (i == randomNumber()) {\n      // do something...\n   }\n}\n```\n\n\nHere it seems that the complexity will be lower because I added the condition to the loop, so we have possible chance the number of \"doing something\" operations will be lower. But we don't know how many times the condition will pass, it may happen it passes every time, so using big-O (the worst case) we again need to say that the complexity is O(N).\n\nAnother example:\n\n```\nfor (int i = 0; i < N; i++) {\n   for (int i = 0; i < N; i++) {\n       // do something\n   }\n}\n```\n\n\nHere as N will be bigger and bigger, the # of operations will grow more rapidly. Having N=10 means that you will have to do 10x10 operations, having N=100 => 100x100 operations, having N=1000 => 1000x1000 operations. You can see the growth is no longer linear it is N x N, so we have O(N x N).\n\nFor the last example I will use idea of full binary tree. Hope you know what binary tree is. So if you have simple reference to the root and you want to traverse it to the left-most leaf (from top to bottom), how many operations will you have to do if the tree has N nodes? The algorithm would be something similar to:\n\n```\nNode actual = root;\nwhile(actual.left != null) {\n   actual = actual.left\n}\n// in actual we have left-most leaf\n```\n\n\nHow many operations (how long loop will execute) will you have to do? Well that depends on the depth of the tree, right? And how is defined depth of full binary tree? It is something like log(N) - with base of logarithm = 2. So here, the complexity will be O(log(N)) - generally we don't care about the base of logarithm, what we care about is the function (linear, quadratic, logaritmic...)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for Array/Stack/Queue\r\n                \r\nI am wondering, why Big O notation is O(1) for Array/Stack/Queue in avg. cases when we are inserting and deleting an element ? \n\nIn my understanding, it is O(1) because interting and deleting an element takes a constant amount of time no matter the amount of data in the set but I am still little bit confused. Any help will be highly appreciated in removing my confusion.\n    ", "Answer": "\r\nO(1) - notation means that the operation is performed in constant time.\n\nO(n) - notation means the operation is performed in linear time, e.g. traversing a list.\n\nArray\n\nWe start with the most obvious one. Array A has a fixed length n, and its elements can be accessed in constant time, by addressing the appropriate location in memory, i.e.\n\n```\nA[i]=10;\n```\n\n\nStack\n\nStack is a Last in first out data structure. We always have a pointer/reference to the top element. So, even if the stack is implemented as a list, where we cannot address a specific element in it in constant time (we have to traverse the list in O(n)), we are accessing the topmost element with pop/peak, to which we have a pointer/reference and is thus accessible in constant time O(1).\n\n```\nStack.pop(); //or peak() perhaps\n```\n\n\nQueue\n\nQueue is a first in first out data structure. As with stack, accessing a specific element of the Queue can be done in linear time O(n), as we need to traverse it. But we usually have a pointer/reference to the first and last element of the queue. Therefore both enqueue and dequeue can be performed in constant time O(1).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of nested sequential loops\r\n                \r\nI have been searching through the forums on big O notation and learned quite a bit.  My problem is pretty specific and I think a unique case will better help me understand big O,  I am ignoring constants. \n\nTo my understanding if a loop goes through all elements than it's O(n). \n\n```\nfor(int i = 0; i < n; i++)\n{   \n\n}\n```\n\n\nIf a loop goes through all of n, inside another loop that goes through all of n, it's multiplied      n * n = n^2\n\n```\nfor(int i = 0; i < n; i++)\n{\n    for(int j = 0; j < n; j++)\n    {\n\n    }\n}\n```\n\n\nLastly if a loop is followed by another loop that goes through all elements it is n + n = 2n\n\n```\nfor(int j = 0; j < n; j++)\n{\n\n}\nfor(int k = 0; k < n; k++)\n{\n\n}\n```\n\n\nMy question directly proceeds these lines of code\n\n```\nfor(int i = 0; i < n; i++)\n{             \n    for(int j = 0; j < n; j++)    \n    {\n\n    }\n    for(int k = 0; k < n; k++)\n    {\n\n    }\n    for(int l = 0; l < n; l++)\n    {\n        for(int m = 0; m < n; m++)\n        {\n\n        }\n    }\n```\n\n\n}\n\nSo based on the rules above I am calculating big O to be n * (n + n + n * n), which is n^3 + 2n^2.  So would that make my big O(n^3) or would my big O be O(n^3 +2n^2).  Am I going about this all wrong?  Or am I somewhere close in the ballpark?  Mainly I'm trying to figure out if these loops would be less than O(n^4).  Thanks in advance.\n    ", "Answer": "\r\nThe big-O notation is used to characterize the asymptotic behavior of an algorithm depending on some value n that indicates the data volume, but independent of any constant, e.g. processor speed.\nIn your example, n^3 grown faster than 2n^2, i.e., for large n, 2n^2 can be neglected compared to n^3. The asymptotic behavior of your nested loops thus have order O(n^3).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Why does Big-O Notation use O(1) instead of O(k)?\r\n                \r\nIf I understand Big-O notation correctly, ```\nk```\n should be a constant time for the efficiency of an algorithm. Why would a constant time be considered ```\nO(1)```\n rather than ```\nO(k)```\n, considering it takes  a variable time? Linear growth ```\n( O(n + k) )```\n uses this variable to shift the time right by a specific amount of time, so why not the same for constant complexity?\n    ", "Answer": "\r\nThere is no such linear growth asymptotic ```\nO(n + k)```\n where ```\nk```\n is a constant. If ```\nk```\n were a constant and you went back to the limit representation of algorithmic growth rates, you'd see that ```\nO(n + k) = O(n)```\n because constants drop out in limits.\n\nYour answer may be ```\nO(n + k)```\n due to a variable ```\nk```\n that is fundamentally independent of the other input set ```\nn```\n. You see this commonly in compares vs moves in sorting algorithm analysis.\n\nTo try to answer your question about why we drop ```\nk```\n in Big-O notation (which I think is taught poorly, leading to all this confusion), one definition (as I recall) of O() is as follows:\n\n\n\n```\nRead: f(n) is in O( g(n) ) iff there exists d and n_0 where for all n > n_0,\n                                         f(n) <= d * g(n)\n```\n\n\nLet's try to apply it to our problem here where k is a constant and thus f(x) = k and g(x) = 1.\n\n\nIs there a ```\nd```\n and ```\nn_0```\n that exist to satisfy these requirements?\n\n\nTrivially, the answer is of course yes. Choose d > ```\nk```\n and for n > 0, the definition holds.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How can I proof a difficult big o notation? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     This question does not appear to be about programming within the scope defined in the help center.\r\n                \r\n                    \r\n                        Closed 5 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI can easily understand how to proof a simple big o notation like n5 + 3n3 ∈ O(n5).\n\nBut how can I proof something more complex like 3n or 2n ∉ O(nk)?\n    ", "Answer": "\r\nUse a proof by contradiction.  \n\nLet's prove that 2n ∉ O(n2).  We assume the opposite, and deduce a contradiction from a consequence.\n\nSo: assumption: there exists M and n0 such that 2n < M n2 for all n >= n0.\n\nLet x be an number such that x > 5, and x > n0 and 2x > 4 M.  Do you agree that such a number must exist?\n\nFinish off the proof by deducing a contradiction based on the inequality that 22x < 4 M x2 by assumption.\n\nNow do the analogous proof for k = 3.  Then do it for k = 4.  Then generalize your result for all k.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How can I apply Big-O notation to this algorithm?\r\n                \r\nI don't know how can I apply Big-O notation to distance problem code. Could you advise me?\n\n```\nfrom math import sqrt\n\na = [4, 2, 8, 5, 6, 2, 6]\nb = [3, 6, 4, 7, 4, 3, 9]\n\nchoose = int(input(\"1.맨하탄 2.유클리드 번호를 입력하시오  \"))\n\ndef euclidean_distance(x,y):\n  return sqrt(sum(pow(q-p,2) for p,q in zip(x,y)))\n\n\ndef manhattan_distance(x,y):\n  return sum(abs(p-q) for p,q in zip(x,y))\n\n\nif 0 < choose < 2 :\n  print(manhattan_distance(a,b))\nelif 1 < choose < 3 :\n  print(euclidean_distance(a,b))\nelse:\n  print(\"Please input 1 or 2\")\n\n```\n\n    ", "Answer": "\r\nThe complexity of your program is one simple loop over the array.\n\nIf you denote the array size as ```\nN```\n, the complexity would be ```\nO(N)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation calculation for nested loop\r\n                \r\n```\nfor ( int i = 1; i < n*n*n; i *= n ) {\n    for ( int j = 0; j < n; j += 2 ) { \n        for ( int k = 1; k < n; k *= 3 ) { \n            cout<<k*n;\n        }\n    }\n}\n```\n\nI am facing an issue with this exercise, where I need to find the big O notation of the following code, but I got ```\nO(n^5)```\n where the first loop is ```\nn^3```\n, 2nd loop n, and the 3rd loop is ```\nn```\n and I am not sure if I am correct or not. Can someone help me please?\n    ", "Answer": "\r\nYour analysis is not correct.\nThe outer loop multiplies i by n each ietration,starting from 1 till n^3.\nTherefore there will be 3 iterations which is O(1).\nThe middle loop increments j by 2 each iteration, starting from 0 till n.\nTherefore there will be n/2 iterations which is O(n).\nThe inner loop multiplies k by 3, from 1 till n.\nTherefore there will be log3(n) iterations, which is O(log(n)).\nIf this step is not clear, see here: Big O confusion: log2(N) vs log3(N).\nThe overall time complexity of the code is a multiplication of all the 3 expressions above (since these are nested loops).\nTherefore is it: O(1) * O(n) * O(log(n)), i.e.: O(n*log(n)).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Does Big O notation apply to a while(true) loop?\r\n                \r\n```\nwhile(true) {\n    System.out.println(\"hola\");\n    break;\n}\n```\n\n\nI have a code snippet here of a ```\nwhile```\n loop with ```\ntrue```\n condition. I was wondering if big O notation can be applied to this code snippet.\n    ", "Answer": "\r\nBig O notation is a tool we can use for analysing algorithms. The way we apply it is by counting the number of steps that the algorithm takes to execute, expressing this number of steps as a function of the input size n, and simplifying the counting/analysis by ignoring constant factors and dominated terms.\n\nFor your algorithm, there is no \"input\" so it is not clear what n should refer to, but the number of steps is also fixed; it prints one string of a fixed length, then the ```\nbreak```\n statement terminates the loop. So its running time is O(1), i.e. constant.\n\nGenerally speaking, the kind of control-flow language constructs you use in the algorithm don't matter (e.g. ```\nwhile```\n vs. ```\nfor```\n, vs. tail recursion), because you can just write an equivalent algorithm which does the same thing with different control-flow constructs. An equivalent algorithm will execute with the same number of steps, so changing the syntax in this way doesn't affect the results of the analysis. The very first algorithm analysis was done by Donald Knuth on flowcharts, which any control-flow graph (including for ```\nwhile(true)```\n / ```\nbreak```\n) can be transformed into.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for a function with x=>0\r\n                \r\nHow should we represent function ```\nf(x)=2x+1```\n with ```\nx->0```\n with big O notation?\n\nWith reasons please.\n\n(I think it should be written as ```\nO(x)```\n as this function seems to grow faster than a constant function while one of my classmate think it should be written as ```\nO(1)```\n.)\n    ", "Answer": "\r\nBig-O notation, in a nutshell, tells you the performance of your algorithm relative to the size of the input. I.e. you can use it to express something like \"on a small input, the algorithm is fast, but with double the input size the algorithm becomes 4 times slower...\".\n\nYour algorithm's performance is constant. It doesn't iterate or do anything which multiples with increased size of the input. Beyond implementation minutiae optimisation, it will always return a result in the same amount of time.\n\nSo it's O(1).\n\nYou cannot express the value of the result using Big-O.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Confused about the big O notation of the code\r\n                \r\nI was trying to find if the given strings are anagrams without using any helper sort function and nested loops.\n\nTherefore I tried using a while loop; however, I am not sure what the big O notation of this code is. Can you please help?\n\n```\ndef anagrams(string1, string2):\n    if len(string1) != len(string2):\n        return False\n    string3 = \"\"\n    x = 0\n    y = 0\n    while x < len(string1) and y < len(string2):\n        element = string1[x]\n        if element == string2[y]:\n            string3 += element\n            x += 1\n            y = 0\n        else:\n            y += 1\n    return string1 == string3\n```\n\n    ", "Answer": "\r\nI believe it will be O(n^2) because your x value tends to increment, but your y value can continue to be reset to zero, therefore being no better than a nested loop.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Calculating Time Complexity and Big-O Notation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Big O, how do you calculate/approximate it?\r\n                            \r\n                                (24 answers)\r\n                            \r\n                    \r\n                Closed 7 months ago.\r\n        \r\n\r\n\r\n    \r\n\r\nWhat is the time complexity of my following code in Big-O notation and what are the steps of calculating it?\n```\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.util.Arrays;\nimport java.util.Scanner;\n\npublic class Main {\n\n    public static void main(String[] args) throws FileNotFoundException {\n        File file = new File(\"C:\\\\Users\\\\yousu\\\\OneDrive\\\\سطح المكتب\\\\textfile\\\\OptimizeBusInput.txt\");\n        Scanner scan = new Scanner(file);\n        while(true) {\n            int n = 0;\n            int d = 0;\n              int r = 0;\n              int outcome = 0;\n            \n            n = scan.nextInt();\n            d = scan.nextInt();\n            r = scan.nextInt();\n\n            if ( n + d + r == 0) break;\n\n            int[] morning = new int[n];\n            int[] afternoon = new int[n];\n\n            for (int i = 0; i < n; i++) {\n                morning[i] = scan.nextInt();\n            }\n            for (int i = 0; i < n; i++) {\n                afternoon[i] = -scan.nextInt();\n            }\n            Arrays.sort(morning);\n            Arrays.sort(afternoon);\n\n            for (int i = 0; i < n; i++) {\n                int sum = morning[i] + (-afternoon[i]) - d;\n                if (sum > 0) outcome += sum * r;\n            }\n            System.out.printf(\"%d\\n\", outcome);\n        }\n```\n\nI have tried calculating the time complexity of every loop and if statement separately but I am not sure how to combine them for the final result. My code follows a Transform & Conquer technique.\n    ", "Answer": "\r\nIgnoring the ```\nwhile (true)```\n loop as well as the user input and analyzing just the part without user interaction, we have two ```\nsort```\n-operations on arrays of size ```\nn```\n (which is known to be ```\nO(n * log(n))```\n and one loop iterating ```\nn```\n times (i.e. ```\nO(n)```\n). Thus, the overall complexity is ```\n2 * n * log(n) + n ∈ O(n * log(n))```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What Big-O notation would this fall under?\r\n                \r\nWhat Big-O notation would this fall under? I know setSearch() and removeAt() are of order O(n) (assume they are either way). I know without the for loop it'd be O(n), for certain, but I get confused how to calculate what it becomes with a for loop thrown into it. I'm not all that great at math... so. Would it be O(n^2)?\n\n```\npublic void removeAll(DataElement clearElement)\n{\n     if(length == 0)\n         System.err.println(\"Cannot delete from an empty list.\");\n     else\n     {\n         for(int i = 0; i < list.length; i++)\n         {      \n            loc = seqSearch(clearElement);\n\n            if(loc != -1)\n            {      \n               removeAt(loc);\n                --i;\n             }\n         } \n     } \n} \n```\n\n    ", "Answer": "\r\nIf removeAt() and seqSearch() are O(n) with respect to the length of the list then yes, this algorithm would be of order O(n^2). This is because within the for loop you call seqSearch every time, with a possibility of calling removeAt(loc). That means for each iteration you are doing either n or 2n operations. Taking the worst case, you have 2n^2 operations which is O(n^2).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Difference between Big-Theta and Big O notation in simple language\r\n                \r\nWhile trying to understand the difference between Theta and O notation I came across the following statement :\n\n```\nThe Theta-notation asymptotically bounds a function from above and below. When\nwe have only an asymptotic upper bound, we use O-notation.\n```\n\n\nBut I do not understand this. The book explains it mathematically, but it's too complex and gets really boring to read when I am really not understanding. \n\nCan anyone explain the difference between the two using simple, yet powerful examples. \n    ", "Answer": "\r\nBig O is giving only upper asymptotic bound, while big Theta is also giving a lower bound.\n\nEverything that is ```\nTheta(f(n))```\n is also ```\nO(f(n))```\n, but not the other way around.\n```\nT(n)```\n is said to be ```\nTheta(f(n))```\n, if it is both ```\nO(f(n))```\n and ```\nOmega(f(n))```\n\n\nFor this reason big-Theta is more informative than big-O notation, so if we can say something is big-Theta, it's usually preferred. However, it is harder to prove something is big Theta, than to prove it is big-O.\n\nFor example, merge sort is both ```\nO(n*log(n))```\n and ```\nTheta(n*log(n))```\n, but it is also O(n2), since n2 is asymptotically \"bigger\" than it. However, it is NOT Theta(n2), Since the algorithm is NOT Omega(n2).\n\n\n\n```\nOmega(n)```\n is asymptotic lower bound. If ```\nT(n)```\n is ```\nOmega(f(n))```\n, it means that from a certain ```\nn0```\n, there is a constant ```\nC1```\n such that ```\nT(n) >= C1 * f(n)```\n. Whereas big-O says there is a constant ```\nC2```\n such that ```\nT(n) <= C2 * f(n))```\n.\n\nAll three (Omega, O, Theta) give only asymptotic information (\"for large input\"):\n\n\nBig O gives upper bound\nBig Omega gives lower bound and\nBig Theta gives both lower and upper bounds\n\n\nNote that this notation is not related to the best, worst and average cases analysis of algorithms. Each one of these can be applied to each analysis.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Find the efficiency in Big-O notation\r\n                \r\nI was having problem with the following question\n\nConsider the following nested loop construct. Categorize its efficiency in terms of the\nvariable n using \"big-o\" notation. Suppose the statements represented by the ellipsis\n(...) require four main memory accesses (each requiring one microsecond) and two\ndisk file accesses (each requiring one millisecond). Express in milliseconds the amount\nof time this construct would require to execute if n were 1000.\n\n```\nx = 1;\ndo\n{\n    y = n;\n    while (y > 0)\n    {\n    ...\n        y--;\n    }\n    x *= 2;\n} while (x < n*n);\n```\n\n    ", "Answer": "\r\nInner loop with y is O(n).\n\nOuter loop runs with x = 1, 2, 2^2, 2^3, ... 2^k < n * n. Hence it runs in O(log(n*n)) which is O(2 * log(n))\n\nHence complexity is O(n * log(n))\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to calculate the big-O notation of this algorithm? [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Why is bubble sort O(n^2)?\r\n                            \r\n                                (6 answers)\r\n                            \r\n                    \r\n                Closed 3 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI'm sure this must be a typical example but I'm very new to calculating O notations from an algorithm. Ideally, the answer to this question wouldn't just give the big-O notation but explain how to find it from looking at the algorithm.\n\n```\n// Apply Newton's law of universal gravitation\n    for (let n = 0; n < sky.length; n++) {\n        let celestial = sky[n]\n\n        for (let m = n + 1; m < sky.length; m++) {\n            let melancholia = sky[m]\n\n            let gravity = // Code to calculate gravity\n\n            // Apply gravity to celestial\n            celestial.applyGravity(gravity)\n\n            // Apply reversed gravity to melancholia\n            gravity.mult(-1)\n            melancholia.applyGravity(gravity\n        }\n\n        celestial.update()\n    }\n```\n\n\nI optimized the code from O(n^2) with ```\nlet m = n + 1```\n instead of ```\nlet m = 0```\n. Now, I recorded O values for certain n values and got a graph looking like:\n\n    ", "Answer": "\r\nLet us suppose that inner calls to ```\napplyGravity```\n and alike are constant time.\nYou already know that:\n\n```\nfor (let n = 0; n < N; n++) {\n    for (let m = 0; m < N; m++) {\n```\n\n\nis O(n^2) because the outer loops N times and for each value the inner loops N times, thus N*N.\n\nNow you have:\n\n```\nfor (let n = 0; n < N; n++) {\n    for (let m = n; m < N; m++) {\n```\n\n\nthe outer loops again N times, but now for each value n the inner loops N-n times. Let look at the generated pairs (n,m):\n\nWhen N=1, you'll have (0,0)\n\nWhen N=2, you'll have (0,0), (0,1), (1,1)\n\nWhen N=3, you'll have (0,0), (0,1), (0,2), (1,1), (1,2), (2,2)\n\nWhen N=4, let us draw it this way:\n\n```\n(0,0) (0,1) (0,2) (0,3)\n      (1,1) (1,2) (1,3)\n            (2,2) (2,3)\n                  (3,3)\n```\n\n\nDon't you see that it will always draw half a square? So something like N^2/2 (surface of a square is N^2), thus in big-O it will be O(n^2), because you can forget any multiplicative constant: N^2 and 1345456*N^2 behave the exactly same.\n\nExact complexity is N+(N-1)+....+1, which is well known to equals to N(N+1)/2 = (N^2+N+1)/2.\n\nIt has the same big-O complexity but not the same exact complexity, that means both behave almost the same as N grows but one can run faster than the other. Remark that as one comment says: the second algorithm executes almost twice as fast as the first one (because (N(N+1)/2) / (N-2)) rouhgly equals to 1/2 as N grows).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to calculate Big-O Notation on the following code\r\n                \r\nI've read the topic:\n\nBig O, how do you calculate/approximate it?\n\nAnd am not sure what the Big-O notation for the following function would be:\n\n```\nstatic int build_nspaces_pattern(const char * const value, char *pattern,\n        size_t sz_pattern) {\n   static char    val_buffer[1025];\n   char           *ptr, *saveptr;\n   size_t         szptrn;\n   ptrdiff_t      offset;\n\n   val_buffer[0] = '\\0';\n   strncat(val_buffer, value, sizeof(val_buffer) - 1);\n   val_buffer[sizeof(val_buffer) - 1] = '\\0';\n\n   pattern[0] = '^'; pattern[1] = '('; pattern[2] = '\\0';\n\n   for ( ptr=strtok_r(val_buffer, \",\", &saveptr);\n         ptr!=NULL;\n         ptr=strtok_r(NULL, \",\", &saveptr)\n       ) {\n      szptrn = sz_pattern - strlen(pattern) - 1;\n\n      if ( sanitize(ptr) != 0 ) {\n         return -1;\n      }\n      strncat(pattern, ptr, szptrn);\n      szptrn -= strlen(ptr);\n      strncat(pattern, \"|\", szptrn);\n   }     \n\n   offset = strlen(pattern);\n   pattern[offset-1] = ')'; pattern[offset] = '$'; pattern[offset+1] = '\\0';\n\n   return 0;\n}\n```\n\n\nSanitize is O(n), but the for loop will run k times (k is the number of commas in the string).\n\nSo, k * O(n) is still O(n), would it be O(n^2), O(k.n) or something else?\n\nThanks.\n    ", "Answer": "\r\nLooks O(n) to me, at a glance.\n\n\n```\nstrtok_r()```\n iterates through the original string = O(n)\n```\nsanitize()```\n you say is O(n), but this is presumably with respect to the length of the token rather than the length of the original string, so multiply token length by number of tokens = O(n)\n```\nstrncat()```\n ends up copying all of the original string with no overlap = O(n)\nyou append a fixed number of characters to the output string (```\n^```\n, ```\n(```\n, ```\n)```\n, ```\n$```\n and a couple of NULLs) = O(1)\nyou append a ```\n|```\n to the string per token = O(n)\n\n\nBut wait!\n\n\nyou call ```\nstrlen()```\n over the output pattern for every iteration of the loop = O(n^2)\n\n\nSo there's your answer.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to calculate Big-O Notation on the following code\r\n                \r\nI've read the topic:\n\nBig O, how do you calculate/approximate it?\n\nAnd am not sure what the Big-O notation for the following function would be:\n\n```\nstatic int build_nspaces_pattern(const char * const value, char *pattern,\n        size_t sz_pattern) {\n   static char    val_buffer[1025];\n   char           *ptr, *saveptr;\n   size_t         szptrn;\n   ptrdiff_t      offset;\n\n   val_buffer[0] = '\\0';\n   strncat(val_buffer, value, sizeof(val_buffer) - 1);\n   val_buffer[sizeof(val_buffer) - 1] = '\\0';\n\n   pattern[0] = '^'; pattern[1] = '('; pattern[2] = '\\0';\n\n   for ( ptr=strtok_r(val_buffer, \",\", &saveptr);\n         ptr!=NULL;\n         ptr=strtok_r(NULL, \",\", &saveptr)\n       ) {\n      szptrn = sz_pattern - strlen(pattern) - 1;\n\n      if ( sanitize(ptr) != 0 ) {\n         return -1;\n      }\n      strncat(pattern, ptr, szptrn);\n      szptrn -= strlen(ptr);\n      strncat(pattern, \"|\", szptrn);\n   }     \n\n   offset = strlen(pattern);\n   pattern[offset-1] = ')'; pattern[offset] = '$'; pattern[offset+1] = '\\0';\n\n   return 0;\n}\n```\n\n\nSanitize is O(n), but the for loop will run k times (k is the number of commas in the string).\n\nSo, k * O(n) is still O(n), would it be O(n^2), O(k.n) or something else?\n\nThanks.\n    ", "Answer": "\r\nLooks O(n) to me, at a glance.\n\n\n```\nstrtok_r()```\n iterates through the original string = O(n)\n```\nsanitize()```\n you say is O(n), but this is presumably with respect to the length of the token rather than the length of the original string, so multiply token length by number of tokens = O(n)\n```\nstrncat()```\n ends up copying all of the original string with no overlap = O(n)\nyou append a fixed number of characters to the output string (```\n^```\n, ```\n(```\n, ```\n)```\n, ```\n$```\n and a couple of NULLs) = O(1)\nyou append a ```\n|```\n to the string per token = O(n)\n\n\nBut wait!\n\n\nyou call ```\nstrlen()```\n over the output pattern for every iteration of the loop = O(n^2)\n\n\nSo there's your answer.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Explanation for Big-O notation comparison in different complexity class\r\n                \r\nWhy Big-O notation can not compare algorithms in the same complexity class. Please explain, I can not find any detailed explanation.\n    ", "Answer": "\r\nSo, ```\nO(n^2)```\n says that this algorithm requires less or equal number of operations to perform. So, when you have algorithm ```\nA```\n which requires ```\nf(n) = 1000n^2 + 2000n + 3000```\n operations and algorithm ```\nB```\n which requires ```\ng(n) = n^2 + 10^20```\n operations. They're both ```\nO(n^2)```\n\nFor small ```\nn```\n the first algorithm will perform better than the second one. And for big ```\nn```\ns second algorithm looks better since it has ```\n1 * n^2```\n, but first has ```\n1000 * n^2```\n.\nAlso, ```\nh(n) = n```\n is also ```\nO(n^2)```\n and ```\nk(n) = 5```\n is ```\nO(n^2)```\n. So, I can say that ```\nk(n)```\n is better than ```\nh(n)```\n because I know how these functions look like.\nConsider the case when I don't know how functions ```\nk(n)```\n and ```\nh(n)```\n look like. The only thing I'm given is ```\nk(n) ~ O(n^2), h(n) ~ O(n^2)```\n. Can I say which function is better? No.\nSummary\nYou can't say which function is better because Big O notation stays for less or equal. And following is true\n```\nO(1) is O(n^2)\nO(n) is O(n^2)\n```\n\nHow to compare functions?\nThere is Big Omega notation which stays for greater or equal, for example ```\nf(n) = n^2 + n + 1```\n, this function is ```\nOmega(n^2)```\n and ```\nOmega(n)```\n and ```\nOmega(1)```\n. When function has complexity equal to some asymptotic, Big Theta is used, so for ```\nf(n)```\n described above we can say that:\n```\nf(n) is O(n^3)\nf(n) is O(n^2)\nf(n) is Omega(n^2)\nf(n) is Omega(n)\nf(n) is Theta(n^2) // this is only one way we can describe f(n) using theta notation\n```\n\nSo, to compare asymptotics of functions you need to use Theta instead of Big O or Omega.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is Big-O Notation also calculated from the functions used?\r\n                \r\nI'm learning about Big-O Notation and algorithms to improve my interview skills, but I don't quite understand how to get the time complexity.\nSuppose I want to sum all the elements of the following list.\n```\nstd::vector<int> myList = {1,2,3,4,5} ;\n```\n\nCase 1:\n```\nint sum = 0;\nfor (int it: myList)\n{\n  sum += it;\n}\n```\n\nCase 2:\n```\nint sum = std::accumulate(std::begin(myList), std::end(myList), 0);\n```\n\nCase 1 is O(N), and case 2 is apparently O(1), but I'm sure those functions do some kind of iteration, so the question is whether Big-O notation is calculated only from of the written code of that block or also of the functions used.\n    ", "Answer": "\r\nIf you talk about big-O, you have to talk in respect of some unit of data being processed.  Both your case 1 and case 2 are O(N) where N is the number of items in the container: the unit is an ```\nint```\n.\nYou tend to want the unit - and N to be the count of - the thing that's likely to grow/vary most in your program.  For example, if you're talking about processing names in phonebooks, then the number of names should be N; even though the length of individual names is also somewhat variable, there's no expected pattern of increasing average name length as your program handles larger phonebooks.\nSimilarly, if your program had to handle an arbitrary number of containers that tended to be roughly the same length, then your unit might be a container, and then you could think of your code - case 1 and case 2 - as being big-O O(1) with respect to the number of containers, because whether there are 0, 1, 10 or a million other containers lying around someone in your program, you're only processing the one - ```\nmyList```\n.  But, any individual ```\naccumulate```\n call is O(N) with respect to any individual container's ```\nint```\ns.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation of two loops nested within loop\r\n                \r\nI have the following code running to check for prime pairs up till a certain condition (p*q<=n) and I'm unsure if the Big-O notation for this would be O(n^2) or O(NlogN):\n\nIn main.cpp:\n\n```\nint main(int argc, char const *argv[]) \n{\n    int n, q;\n    cin >> n;\n\n    for (int p = 0; p * (p + 2) <= n; p++)\n    {\n        q = p+2;\n        if (isPrime(p) && isPrime(q))\n            // output (p,q)\n    }\n    return 0;\n}\n\nbool isPrime(int n)\n{\n    if (n < 2)\n        return false;\n\n    for (int i = 2; i <= n / 2; i++)\n    {\n        if (n % i == 0)\n            return false;\n    }\n    return true;\n}\n```\n\n    ", "Answer": "\r\nYour main for loop will scale as ```\nsqrt(n)```\n.  While the prime function will scale linearly for its input, the input is always roughly ```\nsqrt(n)```\n.  Hence your total run time will scale as ```\nn^.5 * (n^.5 + n^.5) = 2n```\n.  So that's ```\nO(N)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation and Caesar Cipher\r\n                \r\nI have not looked at big O notation since about 1997 so I was hoping for a little sense check.\nImagine a simple algorithm that brute force hacks Caesar Cipher encrypted text. It uses a simple Caesar Cipher algorithm that is nested inside a for 1 to 26 loop, using the loop counter as a key, and outputs all possible results, one of which will be the decrypted plain text.\nWould the complexity of this be O(N) (as opposed to O(N²) ?)\nI'm fairly sure it's O(N). I know nested loops can increase the order of complexity by N² but the number of times it needs to repeat is fixed at 26. If anything the order of complexity could be described as O(N + 26)?\n    ", "Answer": "\r\nThe answer becomes obvious as soon as you explicitly state the meaning of your ```\nN```\n in ```\nO(N)```\n. And the only plausible meaning for ```\nN```\n in your case is the length of the input string.\nBig-O notation is about the asymptotic increase of run time (sometimes also applied to memory consumption) with increasing input size (however you define that \"size\").\nSo, if you define the translation of one character to comsume 1 \"time unit\", then your algorithm will do ```\n26*N```\n translations, plus some (more or less) constant overhead ```\nC```\n, so it's ```\n26*N + C```\n time units. In Big-O notation, we ignore any constant factors and additions, and come to the final result of ```\nO(N)```\n.\nBonus answer:\nIf were were also interested in the algorithm's behaviour with different-sized alphabets (e.g Chinese), with the alphabet's size being ```\nM```\n, you'd get ```\nM*N + C```\n time units, denoted as ```\nO(M*N)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Value of g(n) in Big O Notation\r\n                \r\nAccording to the definition of Big O notation, Let g and f be functions from the set of natural numbers to itself. The function f is said to be O(g), if there is a constant c and a natural n 0 such that f (n) ≤ cg(n) for all n > n0 .\nSo, if f(n) = O(n), can we say that g(n) = n ?\n    ", "Answer": "\r\nFirst of all, your question is not well defined. Taking the question at face value, the answer is no: given ```\nf ∈ O(n)```\n, of course we cannot say that ```\ng(n) = n```\n because you haven't mentioned ```\ng```\n in the antecedent of your hypothesis. You only mentioned ```\ng```\n in the definition of ```\nO```\n, but there is no link between the definition and your hypothesis.\nSo maybe you meant: given ```\nf ∈ O(n)```\n and ```\nf ∈ O(g(n))```\n, does it follow that ```\ng(n) = n```\n? The answer is clearly no.\nSo maybe your question is more about notation: is writing ```\nO(n)```\n equivalent to writing ```\nO(g)```\n where ```\ng(n) = n```\n? This is a more interesting question that I'll attempt to answer in a bit more detail.\nFirst note that the O-notation is used in a way that is sloppy in various aspect which can lead to some confusion. The most obvious sloppiness comes from the fact that people often write ```\nf = O(g)```\n to mean ```\nf ∈ O(g)```\n. The confusion in your case may be at least partly due to a less often noticed sloppiness.\nWhat is usually glossed over is that there are infinitely many different O-operators:\n\nThe O-operator takes a function as argument. In this case the O-operator is (implicitly) parameterized by the arity of the function. To make this explicit, we could write ```\nO_1(g)```\n where ```\ng(x) = x```\n or ```\nO_2(f)```\n where ```\nf(x,y) = x```\n. Note that in this example, ```\nO_1(g)```\n and ```\nO_2(f)```\n do not denote the same set, even though the function bodies are both ```\nx```\n.\n\nThe O-operator takes a function body as argument. In this case the O-operator is (implicitly) parameterized by an ordered sequence of variable names and acts in a way similar to a lambda binding. To make this explicit, we could write ```\nO_n(n)```\n or ```\nO_{n,m}(n)```\n. Note again that these two are not equal. However, these equalities hold: ```\nO_n(n) = O_1(g)```\n and ```\nO_{n,m}(n) = O_2(f)```\n. Note also that the variable names can be alpha-renamed, so ```\nO_n(n) = O_x(x) = O_y(y)```\n.\n\n\n(Nobody usually spells these parameters out explicitly. But it helps to be aware of what is going on implicitly. For example, when people say something like \"the complexity is not ```\nO(E)```\n but ```\nO(V)```\n\" they are implicitly, and perhaps unknowingly, using a dyadic O-operator ```\nO_{E,V}```\n, for otherwise the statement would be meaningless since ```\nO_E(E) = O_V(V)```\n!)\nSo coming back to your question: if you write ```\nO(n)```\n, then you probably mean ```\nO_n(n)```\n, and if you write ```\nO(g)```\n,  you probably mean ```\nO_1(g)```\n. If so, then both are indeed equal. But note that ```\nO_n(n)```\n is also equal to e.g. ```\nO_1(h)```\n where ```\nh(x) = 2x + 1```\n.\nHope you are not even more confused now!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O, Big-Omega, Θ notation\r\n                \r\nWhat is the Big-O, Big-Omega and Theta (Θ) notation for the function,\n5 + 2sin(n)\n    ", "Answer": "\r\nSince 5 + 2sin(n) is bounded by a constant value both above a below, it holds that 5 + 2sin(n) = O(1). It also holds that it is Ω(1). Since it is both, it is also Θ(1).\nYou can show this more formally by finding the values between which your function is bounded, and using the definitions of Big-O and Big-Omega.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the big-O notation for iteration through NSSet and NSDictionary\r\n                \r\nI was wondering what is the big-O notation for iteration through NSSet.\nThe answer for an NSArray is obviously O(n) - but what is the answer for NSSet?\nAlso - I'm assuming the same answer would apply for NSDictionary?\n    ", "Answer": "\r\nYou can get some idea of the computational complexity of Apple's data structures by looking at the comments in the headers of their bridged Core Foundation equivalents (as they are essentially using the same code under the hood).\n\nInterestingly, the time complexity of ```\nCFArray```\n is not actually guaranteed to be O(n):\n\n\n  Computational Complexity\n  \n  The access time for a value in the array is guaranteed to be at\n      worst O(lg N) for any implementation, current and future, but will\n      often be O(1) (constant time). Linear search operations similarly\n      have a worst case complexity of O(N*lg N), though typically the\n      bounds will be tighter, and so on. Insertion or deletion operations\n      will typically be linear in the number of values in the array, but\n      may be O(N*lg N) clearly in the worst case in some implementations.\n      There are no favored positions within the array for performance;\n      that is, it is not necessarily faster to access values with low\n      indices, or to insert or delete values with high indices, or\n      whatever.\n\n\nThese time complexities suggest that ```\nCFArray```\n (and therefore ```\nNSArray```\n) might actually be implemented as a tree (tests show that it might even be switching between multiple underlying data structures).\n\nSimilarly for ```\nCFDictionary```\n, the bounds given have quite a broad range:\n\n\n  Computational Complexity\n  \n  The access time for a value in the dictionary is guaranteed to be at\n      worst O(N) for any implementation, current and future, but will\n      often be O(1) (constant time). Insertion or deletion operations\n      will typically be constant time as well, but are O(N*N) in the\n      worst case in some implementations. Access of values through a key\n      is faster than accessing values directly (if there are any such\n      operations). Dictionaries will tend to use significantly more memory\n      than a array with the same number of values.\n\n\nI was not able to find a similar comment in the Core Foundation headers for ```\nCFSet```\n, but inspection of the source code shows that it's based on ```\nCFBasicHash```\n, which is a hash table, so the time complexity would be as typical for a hash table — O(1) insertion, removal and testing typically, and O(n) in the worst case.\n\nIf you're really interested in finding out exactly how these data structures work, Core Foundation is open source, so you can read the source code on Apple's website.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is it valid to calculate the value of a Big O notation?\r\n                \r\nIs it valid to calculate the value of a given Big O notation? What i mean is, will the number that you get by calculating a given Big O notation always correspond to the exact maximum number of steps that an algorithm has to perform?\n\nAs an example, suppose we have a sorting algorithm with the efficiency of O(n log n), then if we know that the size of N is 8, then could we do:\n8x log2(8) = 24, and so the maximum number of steps required for the algorithm, given that N is 8, will be 24\n    ", "Answer": "\r\nNo, there is no point in this, because\n\na) it is an asymptotic measure, that only describes the growth of the output as the input grows towards infinity\n\nb) it ignores constant offsets and constant multipliers (which completely makes any concrete numbers useless).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation | Time Complexity - nested loop with 'and' keyword\r\n                \r\nI am studying Big-O Notation in Python.\nI have unclear what the time complexity of the following code would be:\n```\nfor elem in list_1:\n    if elem in list_2 and elem in list_3:\n        print(elem)\n```\n\nWhat's your opinion?\n    ", "Answer": "\r\nIf the size of ```\nlist1```\n, ```\nlist2```\n, and ```\nlist3```\n are denoted by ```\nm```\n, ```\nn```\n, and ```\np```\n respectively, the time complexity will be ```\nO(m * (n + p))```\n. Because the outer loop runs ```\nm```\n times, and in each iteration the condition ```\nif```\n can take ```\nO(n + p)```\n comparison to check ```\nelem```\n is contained in the corresponding lists or not.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the Big O notation of a program?\r\n                \r\nI am trying to determine the algorithmic complexity of this program:\n```\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\n \npublic class SuffixArray\n{\n    private String[] text;\n    private int length;\n    private int[] index;\n    private String[] suffix;\n \n    public SuffixArray(String text)\n    {\n        this.text = new String[text.length()]; \n \n        for (int i = 0; i < text.length(); i++)\n        {\n            this.text[i] = text.substring(i, i+1);\n        } \n \n        this.length = text.length();\n        this.index = new int[length];\n        for (int i = 0; i < length; i++)\n        {\n            index[i] = i;\n        } \n \n        suffix = new String[length];\n    }\n \n    public void createSuffixArray()\n    {   \n        for(int index = 0; index < length; index++) \n        {\n            String text = \"\";\n            for (int text_index = index; text_index < length; text_index++)\n            {\n                text+=this.text[text_index];\n            } \n            suffix[index] = text;\n        }\n \n        int back;\n        for (int iteration = 1; iteration < length; iteration++)\n        {\n            String key = suffix[iteration];\n            int keyindex = index[iteration];\n \n            for (back = iteration - 1; back >= 0; back--)\n            {\n                if (suffix[back].compareTo(key) > 0)\n                {\n                    suffix[back + 1] = suffix[back];\n                    index[back + 1] = index[back];\n                }\n                else\n                {\n                    break;\n                }\n            }\n            suffix[ back + 1 ] = key;\n            index[back + 1 ] = keyindex;\n        }\n \n        System.out.println(\"SUFFIX \\t INDEX\");\n        for (int iterate = 0; iterate < length; iterate++)\n        {  \n            System.out.println(suffix[iterate] + \"\\t\" + index[iterate]);\n        }\n    }\n \n \n    public static void main(String...arg)throws IOException\n    {\n        String text = \"\";\n        BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));\n        System.out.println(\"Enter the Text String \");\n        text = reader.readLine();\n \n        SuffixArray suffixarray = new SuffixArray(text);\n        suffixarray.createSuffixArray();\n    } \n}\n```\n\nI have done some research on Wikipedia about the Big-O notation and have run the code with strings of different sizes. Based on the time it takes to run the code with different-size stings, I feel its complexity might be O(n^2).  How can I know for sure?\nAny help would be greatly appreciated.\n    ", "Answer": "\r\nThe complexity of your ```\ncreateSuffixArray```\n method is O(n^2) and you can determine that by examining the 3 looping blocks. Also, since in your question you said you've read the Wikipedia article covering the Big-O notation, then focus on its properties. They are the core of how to apply the right logic and compute the complexity of an algorithm.\nWithin your first block, the innermost loop iterating ```\nlength```\n times is in turn repeated for ```\nlength```\n times, yielding a complexity of O(n^2) due to the ```\nProduct Property```\n of the Big-O notation.\n```\nfor(int index = 0; index < length; index++) \n{\n    String text = \"\";\n    for (int text_index = index; text_index < length; text_index++)\n    {\n        text+=this.text[text_index];\n    } \n    suffix[index] = text;\n}\n```\n\nIn your second block, although the innermost loop does not perform ```\nlength```\n iterations at the beginning, its complexity still tends to O(n), producing again an overall complexity of O(n^2) due to the ```\nProduct Property```\n.\n```\nfor (int iteration = 1; iteration < length; iteration++)\n{\n    String key = suffix[iteration];\n    int keyindex = index[iteration];\n\n    for (back = iteration - 1; back >= 0; back--)\n    {\n        if (suffix[back].compareTo(key) > 0)\n        {\n            suffix[back + 1] = suffix[back];\n            index[back + 1] = index[back];\n        }\n        else\n        {\n            break;\n        }\n    }\n    suffix[ back + 1 ] = key;\n    index[back + 1 ] = keyindex;\n}\n```\n\nYour third and last block simply iterates ```\nlength```\n times, giving us a linear complexity of O(n).\n```\nfor (int iterate = 0; iterate < length; iterate++)\n{  \n    System.out.println(suffix[iterate] + \"\\t\" + index[iterate]);\n}\n```\n\nAt this point to get the method complexity, we need to gather the 3 sub-complexities we've got and apply to them the ```\nSum Property```\n, which will yield O(n^2).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Algorithm complexity and big O notation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        What is the difference between Θ(n) and O(n)?\r\n                            \r\n                                (9 answers)\r\n                            \r\n                    \r\n                Closed 6 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI am taking an online class on algorithms and I had the following quiz. I got it wrong and am trying to understand the reason for the answer.\n\nWhich of the following is O(n^3)?\na) 11n + 151 gn + 100\nb) 1/3 n^2\nc) 25000 n^3\nd) All of the above.\n\nThe correct answer is (d) all of the above. The reason is that Big-O notation provides only the upper bound on the growth rate of function as n gets large.\nI am not sure why the answer is not (c). For example, the upper bound on (b) is less than n^3.\n    ", "Answer": "\r\nThe reason is that formally, big-O notation is an asymptotic upper bound.\n\nSo ```\n1/3*n^2```\n is ```\nO(n^2)```\n, but it is also ```\nO(n^3)```\n and also ```\nO(2^n)```\n.\n\nWhile in every-day conversion about complexity  ```\nO(...)```\n is used as a tight (both upper and lower bound), the theta-notation, or ```\nΘ(...)```\n  is the technically correct term for this.\n\nFor more info see What is the difference between Θ(n) and O(n)?\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Java Big O notation of 3 nested loops of log(n)\r\n                \r\nWhat would the Big O notation be for the following nested loops?\n\n```\n     for (int i = n; i > 0; i = i / 2){\n        for (int j = n; j > 0; j = j / 2){\n           for (int k = n; k > 0; k = k / 2){\n              count++;\n           }\n        }\n     }\n```\n\n\nMy thoughts are:\neach loop is ```\nO(log2(n))```\n so is it as simple as multiply\n\n```\nO(log2(n)) * O(log2(n)) * O(log2(n))  =  O(log2(n)^3)\n```\n\n    ", "Answer": "\r\nYes, that is correct.\n\nOne way to figure out the big-O complexity of nested loops whose bounds do not immediately depend on one another is to work from the inside out.  The innermost loop does O(log n) work.  The second loop runs O(log n) times and does O(log n) work each time, so it does O(log2 n) work.  Finally, the outmost loop runs O(log n) times and does O(log2 n) work on each iteration, so the total work done is O(log3 n).\n\nHope this helps!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to calculate O(log n) in big O notation?\r\n                \r\nI know that ```\nO(log n)```\n refers to an iterative reduction by a fixed ratio of the problem set ```\nN```\n (in big O notation), but how do i actually calculate it to see how many iterations an algorithm with a ```\nlog N```\n complexity would have to preform on the problem set ```\nN```\n before it is done (has one element left)?\n    ", "Answer": "\r\nYou can't. You don't calculate the exact number of iterations with BigO.\n\nYou can \"derive\" BigO when you have exact formula for number of iterations.\n\nBigO just gives information how the number iterations grows with growing N, and only for \"big\" N. \n\nNothing more, nothing less. With this you can draw conclusions how much more operations/time will the algorithm take if you have some sample runs.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the upper found or Big O notation for the given functions\r\n                \r\nI'm trying to learn the upper bound(Big O) notation and I'm really confused with the below 2 types of functions.\n\nf(n) = n^3 + 100n^2 + 5^n\nf(n) = n^3 + 100n^2 + 1.1^n\n\nCan someone please guide me how to find the big o notation of the given 2 functions.\n    ", "Answer": "\r\n```\n5^n```\n and ```\n1.1^n```\n are exponential (growing) functions. They grow asymptotically faster than any polynomial function, which ```\nn^3```\n and ```\nn^2```\n are. (This can be proven e.g. via l'Hôpital.)\nTherefore the functions are ```\nTheta(5^n)```\n and ```\nTheta(1.1^n)```\n respectively. Note that these classes are not the same, since an exponential function with a larger base/rate asymptotically dominates one with a smaller base/rate.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for a copy operation\r\n                \r\nI've come across some code which could definitely be improved, but i'm wondering about the Big-O notation of my improvements.\n\nTheir original code adds a element to an array, and each time it does this it creates a new array of n+1 and copies the old one in like so:\n\n```\npublic MyType GetNewType()\n{\n    MyType[] tempTypes = new MyType[_types.Count + 1];\n    _types.CopyTo(tempTypes, 0);\n    _types = tempTypes;\n\n   _types[types.Count - 1] = new MyType();\n   return _types[types.Count - 1];\n}\n```\n\n\nAs far as I can see this would be a O(n) operation. I therefore rewrote it as follows:\n\n```\nprivate int _currentIndex; //initialized in the constructor\n\npublic MyType GetNewType()\n{\n    if (_types.Length == _currentIndex)\n    {\n        MyType[] tempTypes = new MyType[_types.Length + 10];\n        _types.CopyTo(tempTypes, 0);\n        _types = tempTypes;\n    }\n\n   _types[_currentIndex] = new MyType();\n   _currentIndex++;\n\n   return _types[_currentIndex - 1];\n}\n```\n\n\nWould the result of these changes mean that the function will now run in O(n / 10) as it would only require a copy operation every 10 calls? Or does it not quite work as nicely as that?\n    ", "Answer": "\r\nIn terms of Big-O notation complexity of the ```\n(n/10)```\n would be ```\nO(n)```\n because it does not care such a small constants.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Determining Big O Notation of based on python function\r\n                \r\nI have a school assignment where I need to determine the Big O notations of two functions. The problem is we have had no real courses on Big O, let alone Python. Could someone explain how to determine the big-O, given these functions? Thanks!\n```\n\ndef my_func1(inputs):\n    n = len(inputs)\n    result = 0\n    for i in range(n):\n        j = 1\n        while j < n:\n            result += inputs[i] * inputs[j]\n            j *= 2\n    return result\n\ndef my_func2(inputs):\n    n = len(inputs)\n    for i in range(n - 1):\n        for j in range(n - i - 1):\n            if inputs[j] > inputs[j + 1]:\n                tmp = inputs[j]\n                inputs[j] = inputs[j + 1]\n                inputs[j + 1] = tmp\n\n\n```\n\n    ", "Answer": "\r\nIn order to find the Big O Notation for the above functions, you may have to go through the algorithm analysis of Big O Notation.\nBig-O notation is a metrics used to find algorithm complexity. Basically, Big-O notation signifies the relationship between the input to the algorithm and the steps required to execute the algorithm. It is denoted by a big \"O\" followed by opening and closing parenthesis. Inside the parenthesis, the relationship between the input and the steps taken by the algorithm is presented using \"n\".\nLet's write a simple algorithm in Python that finds the square of the first item in the list and then prints it on the screen.\n```\ndef constant_algo(items):\n    result = items[0] * items[0]\n    print ()\n\nconstant_algo([4, 5, 6, 8])\n```\n\nIn the above script, irrespective of the input size, or the number of items in the input list items, the algorithm performs only 2 steps: Finding the square of the first element and printing the result on the screen. Hence, the complexity remains constant. In Big O Notation, it is O(C).\nSame way, we can simply find the Big O Notations for the algorithms or code by going through this documentation.\nThe Big O Notations for the above two functions is O(n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "what is the time complexity of this program??Big-O-notation for this program?\r\n                \r\nWhat is the time complexity and Big-O notation for this code?\n\n```\ndef rot(a,n):\n    for i in range(n-1):\n        temp=a[i]\n        a[i]=a[i+1]\n        a[i+1]=temp\n    return a\nn=int(input())\n\nx=[1,2,3,4,5,6,7]\n\nfor i in range(n):\n    x=rot(x,7)\n    print(x)\n```\n\n    ", "Answer": "\r\nThe order is O(n)\n\nYou have two nested for loops, which might indicate it's ```\nO(n**2)```\n, but the second for loop does not depend on the variable n. it depends on x and 7, which both do not depend on n.\n\nThe parameter of the function rot is called ```\nn```\n, but its passed value is always  ```\n7```\n, thus not depending on n.\n\nSo if you multiply n by 2 the amount of instructions to execute is about twice as much.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is the Big O notation of nested dependent for loop always n^2\r\n                \r\nIs the Big O notation of nested dependent for loop always O(n^2)?\n    ", "Answer": "\r\nYes, for worst-case complexity. But only if the length /step of the inner loop == length /step of the outer loop.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is there a programmatic way or eclipse plugin to calculate big O notation for java method [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 2 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nIs there a programmatic way or eclipse plugin to calculate big-O notation for  java method ?\n    ", "Answer": "\r\nNo, there isn't such plugin, and if it was, it would be a mere approximation. Namely, even determining whether the program will finish running or not is intractable - see Halting problem.\n\nNow, about the possible approximation. Let's say you have a plugin that tests your program with a small dataset (e.g. ```\nN = 1000```\n) and a medium dataset (e.g. ```\nN = 10000```\n). If your program runs 10 times longer with a medium dataset compared to a small dataset, plugin should conclude that your program is ```\nO(N)```\n, right? Not quite. What about best/average/worst case? For example, quicksort's worst case is ```\nO(N^2)```\n, but it is generally considered as ```\nO(N*logN)```\n sorting algorithm. Therefore, if the plugin hits the special input, it will give a wrong result. What about constants? The program whose running time is ```\nO(N + k*logN)```\n is considered ```\nO(N)```\n, but if a constant ```\nk```\n is large enough compared to ```\nN```\n, plugin would not be able to reach this conclusion, etc.\n\nRegarding your comment:\n\n\n  If anybody tried codility challenges\n  they are evaluation your solution\n  against performance using big O notation , and I'm sure that they are\n  not calculation it manually, that's why I'm asking this question.\n\n\nAuthors of Codility challenges have solutions of their problems with a well-known time complexity (they analyzed it manually). When they measure the running time of your solution for various input and compare it with a running time of their solutions for the same input, they can automatically determine the time complexity of your program (of course, taking into account the programming language you have chosen and certain deviations of the measured time).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Did I do this big-O notation correctly?\r\n                \r\nIt is for a homework assignment and I'm just getting thrown a little bit by the negative sign.\n\nExpress the following in terms of big-O notation. Use the tightest bounds possible. For instance, n5 is technically O(n1000), but this is not as tight as O(n5).\nn2 −500n−2\n\n\nn2 - 500 n - 2\n<= n2 - 500 n\n<= n2 for all n > 0\nwhich is O(n2)\n\n    ", "Answer": "\r\nFor Big O notation what you need to remember is that it only matters for some number x0 and all numbers above that.  Specifically ```\nf(x)= O(g(x))```\n as x approaches infinity if there is some number ```\nM```\n and some real number x0 such that ```\n|f(x)| <= M|g(x)|```\n for all x >= x0. (Source for equations, wikipedia).\n\nBasically, we only need to consider large values of x and you can pick an arbitrarily large value.  So large in fact that ```\nn^2```\n will overshadow a subtraction by ```\n500n```\n.  To be more technical if I pick M to be 2 and x0 to be 100000000000000000. Then the above equation holds.  I'm being lazy and picking an x0 that is extremely large but the equation lets me.  For an M equal to 2 a much smaller value of x0 would work, but again, it doesn't matter.\n\nFinally, your answer of ```\nO(n^2)```\n is correct\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Time complexity (Big-O notation) of Posterior Probability Calculation\r\n                \r\nI got a basic idea of Big-O notation from Big-O notation's definition.\n\nIn my problem, a 2-D surface is divided into uniform M grids. Each grid (m) is assigned with a posterior probability based on A features. \n\nThe posterior probability of m grid is calculated as follows: \n\n\nand the marginal likelihood is given as:\n\n\n\nHere, A features are independent of each other and sigma and mean symbol represent the standard deviation and mean value of each a feature at each grid. I need to calculate the Posterior probability of all M grids. \n\nWhat will be the time complexity of the above operation in terms of Big-O notation?\n\nMy guess is O(M) or O(M+A). Am I correct? I'm expecting an authenticate answer to present at the formal forum. \n\nAlso, what will be the time complexity if M grids are divided into T clusters where every cluster has Q grids (Q << M)  (calculating Posterior Probability only on Q grids out of M grids) ?\n\nThank you very much.\n    ", "Answer": "\r\nDiscrete sum and product \n\n \n\ncan be  understood as loops. If you are happy with floating point approximation most other operators are typically O(1), conditional probability looks like a function call. Just inject constants and variables in your equation and you'll get the expected Big-O, the details of formula are irrelevant. Also be aware that these \"loops\" can often be simplified using mathematical properties.\n\nIf the result is not obvious, please convert your above mathematical formula in actual programming code in a programming language. Computer Science Big-O is never about a formula but about an actual translation of it in programming steps, depending on the implementation the same formula can lead to very different execution complexities. As different as adding integers by actually performing sum O(n) or applying Gauss formula O(1) for instance.\n\nBy the way why are you doing a discrete sum on a discrete domaine N ? Shouldn't it be M ?\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Whats the dominant term in 2^n or n^2 for big O notation\r\n                \r\nI have been looking at Big O notation and have come across an operational count ```\n2^n+n^2```\n. I understand the practice of big O notation is to remove the constants and the low order terms, however I cant figure out which one to make ```\nO(n)```\n. I think it may be ```\n2^n```\n but have had no luck finding anything to suggest this.\n    ", "Answer": "\r\nLook at the growth factor over time. For the first eight values of ```\nn```\n, ```\nO(n^2)```\n works out to:\n\n0, 1, 4, 9, 16, 25, 36, 49...\n\n```\nO(2^n)```\n produces powers of two:\n\n1, 2, 4, 8, 16, 32, 64, 128...\n\nIt should be fairly obvious which one is growing faster.\n\nNote that the general rule applies even with different bases and exponents. ```\nO(1.1^n)```\n may have initially lower work than ```\nO(n^10)```\n for smaller ```\nn```\n, but all exponential growth with an exponent greater than 1 will eventually outpace fixed exponent polynomial growth as ```\nn```\n approaches infinity.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for an if else selection statement with for loops\r\n                \r\n```\nif (A[1][1] == 0)\n  for(i=0; i<n; i++)\n    for (j=0; j<n; j++)\n      A[i][j] = 0;\nelse\n  for (i=0; i<n; i++)\n    A[i][i] = 1;\n```\n\n\nI'm looking for the big O notation for this code.\nI've tried to understand the program but I just couldn't get the hold of it.\n    ", "Answer": "\r\nThe time required to complete the following is proportional to ```\nn```\n, so the following is O(n):\n\n```\nfor (i=0; i<n; i++)\n  A[i][i] = 1;\n```\n\n\nThe time required to complete the following is proportional to ```\nn*n```\n which is n2, so the following is O(n2):\n\n```\nfor(i=0; i<n; i++)\n  for (j=0; j<n; j++)\n    A[i][j] = 0;\n```\n\n\nThe snippet will execute one of these, but not both. As such, the worse case is bound by the worse of those two options, so it's O(n2).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation example claiming 2n^2 = O(n^3)?\r\n                \r\nWhile I was studying time complexity, I found a web page explaining about Big O notation.\n\nWhile I was reading I found the example they used and it got me confused.\n\nIn the example they said something like\n\n```\n2n^{2} = O(n^{3})```\n because for all n bigger than ```\n2(n_{0})```\n, there exists a c(which is 1) that satisfies ```\n0 <= 2n^{2} <= cn^{3}```\n \n\nFirst I thought the Big O should be ```\nO(n^2)```\n. But after reading through few more texts I can see that ```\nn^2```\n is smaller than ```\nn^3```\n so can theoretically say that the big O is ```\nO(n^3)```\n.\nBut is ```\nO(n^2)```\n wrong for the example above? \n    ", "Answer": "\r\nYou're absolutely correct (though you should write out the proof for O(n^2) to convince yourself). Technically the example as written is not wrong, but it's also not a good example. You can think of O(g) as meaning that the function grows as slowly or slower than g. So if a function is O(n^2) it is also O(n^3). There are other variations on Big-O notation (Theta and Omega) which make stronger statements about the asymptotic behavior.\n\nThis thread had some nice information even though the focus of the question is slightly different Difference between Big-O and Little-O Notation.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "When to use big O notation and when to use big Theta notation\r\n                \r\nI understand that Big O is an upper bound and Big Theta is a tight bound when for example we consider the functions f(n)=O(g(n)) or similarly for Big Theta. \nBut how do we know that a particular algorithm will be better represented using the Big theta notation instead of Big O?\n\nFor example, the time complexity of selection sort is given as Big Theta of N^2 rather than Big O of N^2, why?\n    ", "Answer": "\r\nIt's not a question of which is better but rather what do you want to studies.\nIf you want to study the worst case scenario then you can use the upper bound notation. Keep in mind that the tighter the bound the better, but in some cases it's difficult to calculate the tight bound.\nGenerally, when people speak about big-O or big-teta they mean the same think so in Selection sort you can also use big-O notation\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Space and time complexities, in Big O notation, for the JBIG compression algorithms?\r\n                \r\nWhat are the computational complexities of JBIG lossless compression algorithm in Big O notation? \n    ", "Answer": "\r\nSearching the internet, I see that there are a number of variants of JBIG, but this paper suggests O(image_size) or O(image_size+num_symbols) for the Soft Pattern Matching algorithm. Furthermore, this paper states:\n\n\n  The time complexity of the sequential algorithm (JBIG) is O(N).\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation of 1+2+3+...+n\r\n                \r\nI'm currently a CS undergrad enrolled in a data structures course.  During the semester, we learned about big-O notation, and on one assignment, we had to write out the big-O notation of summing the numbers 1+2+3+...+n.  I figured that, in the simplest method, you would be be looping from 1 to n and in each iteration adding i to the sum, so it seemed like this would be O(n) time.\n\nI am also aware that this specific summation can be expressed as (n(n+1))/2 as a more direct way to receive the answer.\n\nMy professor insists that in both cases, the time complexity is O(n^2), and I have been emailing him back and forth hoping to get a better explanation, but he basically sends the same reply every time.\n\nI feel like I must be misunderstanding the purpose of big-O in the first place.  Even when I implement these 2 methods of finding the sum in a program and time the execution, the time of the loop method seems to increase linearly based on the size of n, and in the second method, it takes the same amount of time no matter how large n is because there is no iteration occurring in this case.\n\nCould someone please help me understand why this would still be O(n^2)?\n    ", "Answer": "\r\nYou are computing the order of the wrong value.\n\nAs you indicated in a comment, the question does not ask what's the time complexity of doing the sum; the question asks what is the order of the sum itself. And indeed 1 + \n2 + ... + n is O(n²).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the Big O notation for RSA encryption?\r\n                \r\nI'm doing a task on RSA encryption, and I'm wondering what the big O notation for the entire RSA algorithm is, i.e. Generating keys, encrypting a message, and then decrypting that message, if it is possible to figure that out. How would you go about figuring that out?\nThanks in advance\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of a program (Worst-Case)\r\n                \r\nI have a question regarding complexity theory. If I have a Bubble sort algorithm and I want to find its worst case running time Big O, we can conclude that it is O(n^2). Now, what about If I have a program that executes different operations  like a sorting algorithm, search algorithm, etc. How do I know what is the worst case running time (Big O) of this program in general.\n\nFor example, how having different algorithms within a program with its respective worst case running time Big O notations get to the conclusion of the worst case running time (Big O) of the entire program. Like when a program has the following: O(n^2), O(1), O(n).... Which one of these notations is the one that represents the entire program? \n\nHow would you find the worst-case running time Big O of this program?\n\n```\nimport java.util.*;\npublic class Prog1 {\n   public static void main(String[] args) {\n\n    int first = 0;\n    int last;\n    int middle;\n    int search;\n    int[] array;\n\n    Scanner input = new Scanner(System.in);\n    System.out.println(\"Number of elements\");\n    int n = input.nextInt();\n\n    array = new int[n];\n\n    System.out.println(\"Enter \" + n + \" value \");\n    for (int x = 0; x < n; x++) {\n        array[x] = input.nextInt();\n    }\n\n    System.out.println(\"Value to search\");\n    search = input.nextInt();\n\n    last = n - 1;\n    middle = (first + last) / 2;\n\n    while (first <= last) {\n        if (array[middle] < search)\n            first = middle + 1;\n        else if (array[middle] == search) {\n            System.out.println(\" Number \" + search + \" is in the array\");\n            break;\n        } else\n            last = middle - 1;\n\n        middle = (first + last) / 2;\n    }\n    if (first > last)\n        System.out.println(\" Number \" + search + \" is not in the list.\");\n }\n}\n```\n\n    ", "Answer": "\r\nThe highest one. O(n^2) + O(n) + O(1) = O(n^2) asymptotically talking! \nThis is how you would calculate the complexity of an algorithm though. \nIt doesn't make much sense to talk about program \"complexity\".\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Can someone help with big O notation?\r\n                \r\n```\nvoid printScientificNotation(double value, int powerOfTen)\n{\nif (value >= 1.0  &&  value < 10.0)\n{\n    System.out.println(value + \" x 10^\" + powerOfTen);\n}\nelse if (value < 1.0)\n{\n    printScientificNotation(value * 10, powerOfTen - 1);\n}\nelse     // value >= 10.0\n{\n    printScientificNotation(value / 10, powerOfTen + 1);\n}\n```\n\n\n}\n\nassuming that imputs will not lead to infinite loops\n\nI understand how the method goes but I cannot figure out a way to represent the method. \nFor example, if value was 0.00000009 or 9e-8, the method will call on printScientificNotation(value * 10, powerOfTen - 1); eight times and System.out.println(value + \" x 10^\" + powerOfTen); once. \n\nSo the it is called recursively by the exponent for e. But how do I represent this by big O notation?\n\nThanks!\n    ", "Answer": "\r\nIs this a trick question? That code will recurse infinitely for some of its inputs (for example, value=-1.0, powerOfTen=0), therefore its runtime is not O(f(n)) for any finite function f(n).\n\nEdit: Assuming ```\nvalue > 0.0```\n...\n\nThe run time (or recursion depth, if you prefer to look at it that way) does not depend on the value of ```\npowerOfTen```\n, only on ```\nvalue```\n.  For an intial input ```\nvalue```\n in the range [1.0, 10.0), the run time is constant, so O(1),  For ```\nvalue```\n in [10.0, +infinity), you divide ```\nvalue```\n by 10 for each recursive call until ```\nvalue < 10.0```\n, so the runtime is  O(log10(```\nvalue```\n)).  A similar argument can be made for ```\nvalue```\n in the range (0.0,1.0), but note that log10 ```\nvalue```\n is negative for this case.  So your final answer might involve an absolute value operation.  Then you might consider whether it's necessary to specify the logarithm base in the context of an asymptotic complexity analysis.  Hopefully you can take it from there!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation with Absolute Value?\r\n                \r\nI'm going through some programming interview question books, and I've seen reference to ```\n\"O(|A|)\"```\n time complexity.  I've never seen this notation with the absolute value given.\n\nSome research led me to Big O Cheatsheet that references this notation under the graphs section.  The problem I'm researching is about partitioning an array, which isn't really a graph question (though I risk perhaps showing my ignorance with that statement).\n\nDoes ```\n|A|```\n refer to the magnitude of the array, or otherwise number of elements, i.e. ```\nO(N)```\n?\n    ", "Answer": "\r\nIn set theory notation ```\n|A|```\n is the cardinality of set ```\nA```\n, in other words the number of elements contained in set ```\nA```\n.\n\nFor Reference: http://www.mathsisfun.com/sets/symbols.html\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Confused about big-O notation (specific example)\r\n                \r\nWe did an exercise in class today dealing with big-O notation. Here is one of the problems:\n\n```\nvoid modifyArray(int a[], int size)\n{   \n    int max = a[0];\n    for (int i = 1; i < size / 2; ++i)\n    {\n        if (max < a[i])\n        max = a[i];\n    }\n    for (int j = 1; j <= size * size; ++j)\n    {\n        ++max;\n        cout << max;\n    }\n}\n```\n\n\nMy intuition tells me that f(n) = n/2 + n2 = O(n2) but according to my professor the answer is simply O(n).  Could anyone explain to me why and when we just change what we consider to be the input size?  \n\nI understand that it is not a nested loop -- that is not what is confusing me. I don't understand why for a given input ```\nsize```\n, the second loop is only considered to be O(n). The only way I can make sense of this is if we isolate the second loop and then redefine the input size to simply being n = size^2.  Am I on the right track? \n    ", "Answer": "\r\nIf the code you present is exactly the code your professor is commenting on, then (s)he's wrong.  As written, it outputs each number from 1 to ```\nsize * size```\n, which is definitely O(n^2), as n = size is the sane choice.\n\nYes, you're right to think you could say something like \"O(n) where n is the square of the array size\", but that's complication without purpose.\n\nAs others have said, if the ```\ncout << max```\n is removed, the compiler may optimise out the loop to a single O(1) assignment, meaning the function's other O(n) operation dictates the overall big-O efficiency, but it may not - who said you're even enabling optimisation?  The best way to to describe the big-O efficiency is therefore to say \"if optimisation kicks in then O(n) else O(n^2)\" - it's not useful to assert one or the other then hide your assumptions, and the consequences if they're wrong, in a footnote.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big(O) notation - which one is correct\r\n                \r\nI am trying to learn Big(O) notation. While searching for some articles online, I came across two different articles , A and B\n\nStrictly speaking in terms of loops - it seems that they almost have the same kind of flow. \nFor example \n\n[A]'s code is as follows (its done in JS)\n\n```\nfunction allPairs(arr) {\n    var pairs = [];\n    for (var i = 0; i < arr.length; i++) {\n        for (var j = i + 1; j < arr.length; j++) {\n            pairs.push([arr[i], arr[j]]);\n        }\n    }\n\n    return pairs;\n}\n```\n\n\n[B]'s code is as follows (its done in C)- entire code is here \n\n```\n  for(int i = 0; i < n-1 ; i++) {\n    char min = A[i]; // minimal element seen so far\n    int min_pos = i; // memorize its position\n    // search for min starting from position i+1\n    for(int j = i + 1; j < n; j++) \n      if(A[j] < min) {\n        min = A[j];\n        min_pos = j;\n      }\n    // swap elements at positions i and min_pos\n    A[min_pos] = A[i];\n    A[i] = min;\n  } \n```\n\n\nThe article on site A mentions that time complexity is O(n^2) while the article on site B mentions that its O(1/2·n2).\n\nWhich one is right?\n\nThanks\n    ", "Answer": "\r\nAssuming that O(1/2·n2) means O(1/2·n^2), the two time complexity are equal. Remember that Big(O) notation does not care about constants, so both algorithms are O(n^2).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for non nested loops\r\n                \r\nI have a question in regards to big-o-notation. when you have 2 non-nested loop in a function:1st with the runtime O(N^2), 2ND: with runtime O(N). what will be the run-time of that function? I think it's O(N^2)\n\nPart of my project is to right the run-time for the functions stated in the file\nhttps://drive.google.com/file/d/0Bxt_6d1O-eKnWmdfejMzUjdXelE/view?usp=sharing\n\nI have wrote the run-time for the scenario. If anyone can double check the run-time for me please. \n\nhttps://drive.google.com/file/d/0Bxt_6d1O-eKncHo4c0dSdnMtUWc/view?usp=sharing\n\nthanks \n    ", "Answer": "\r\nWhen you have something that happens after something else, you sum their complexity time. And the leading complexity time is the winner there. so:\n\n```\nO(n^2) + O(n) = O(n^2 + n) = O(n^2)\n```\n\n\nIf you had a loop in the loop you would then multiply them: ```\nO(n^2 * n) = O(n^3)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Determining efficiency (Big O Notation) while using arrays in JavaScript\r\n                \r\nI am struggling to wrap my head around Big O notation and saw the following problem online. I must answer determining whether the following code is O(n), O(n²), O(logn), O(nlogn)\nI have watched several videos but am still failing to understand Big O. Can someone please advise to the answer and their methodology for getting there?\n```\nfunction sortSmallestToLargest(entries):\n    sorted_entries = {};\n\n    while entries is not empty:\n        smallest_entry = entries[0]\n\n        foreach entry in entries:\n            if (entry < smallest_entry):\n                smallest_entry = entry\n\n        sorted_entries.add(smallest_entry)\n        entries.remove(smallest_entry)\n\n    return sorted_entries\n```\n\n    ", "Answer": "\r\nThe algorithm iterates multiple time on 'entries' until it gets empty. first time in the inner loop it iterates n time (assuming entries length is n at start). second time in the inner loop it iterates n-1 time (because one item was removed in the previous iteration). So at the end we have this series of iterations:\nn + (n-1) + (n-2) + ... + 1 = n(n+1)/2 = O(n^2)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to calculate the Big O notation of the following expression?\r\n                \r\nI want to find the Big O notation for the following expression.\n\n\n\nCan it be represented as  [where 1<=l<=alpha, beta is a vector of integers]\n\nCorrect me if I am wrong....\n    ", "Answer": "\r\nAn algorithm for computing that sum would be:\n\n```\nBigInteger sum = BigInteger.ZERO;\nfor (int i = 1; i <= alpha; i++) {\n     sum = sum.add(BigInteger.ONE.shiftLeft(beta[i]));\n}\n```\n\n\nNow the complexity of an N-bit shift or an N-bit add using ```\nBigInteger```\n is ```\nO(N)```\n.\n\nTherefore, the overall complexity of computing the above will be bounded by ```\nalpha * min(beta[i])```\n and ```\nalpha * max(beta[i])```\n.  \n\nFurthermore, the actual complexity will also depend on the order of the ```\nbeta[i]```\n values.\n\nIf the ```\nalpha```\n and ```\nbeta```\n values are small enough that you can use primitive integer arithmetic, then the complexity is ```\nO(alpha)```\n, since both adding and shifting are ```\nO(1)```\n operations.\n\n\n\nOn the other hand, if you want the complexity class for that function, it is going to be something like ```\nO(alpha * 2 ^ (max(beta[i])))```\n.  Note that this is actually a function of a scalar and a vector, and I'm not sure that this is a mathematically sound thing to say.  (What does it mean for a vector to tend to infinity?)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the time complexity of the following codes in Big O notation? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs details or clarity. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Add details and clarify the problem by editing this post.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\n\n```\nT(n) = 8*T(n/2) + n*n```\n\n```\nT(n) = 3*T(n/4) + n```\n\n\n\nI want to calculate the time complexity in Big O notation . What is the answer (without using master theorem)\n    ", "Answer": "\r\nThe master theorem applies to any recurrence of the form ```\nT(n) = a*T(n/b) + n^c```\n. It looks at and compares the two parts of the recurrence:\n\n1) The size of the constant work at this level (c)\n2) The number and size of the recursive calls (a and b)\n\nFrom here, We compare log_b (a) to c. There are three possibilities\n\n\n```\nlog_b (a) > c```\n -> ```\nT(n)```\n is ```\nO(n^log_b (a))```\n\n```\nlog_b (a) < c```\n -> ```\nT(n)```\n is ```\nO(n^c)```\n\n```\nlog_b (a) = c```\n -> ```\nT(n)```\n is ```\nO(n^c log(n))```\n\n\n\nSo for your two examples...\n\n\n```\nT(n) = 8*T(n/2) + n*n```\n, therefore ```\na = 8, b = 2, c = 2```\n, ```\nlog_2 (8) > 2```\n, therefore ```\nT(n)```\n is ```\nO(n^(log_2 (8))```\n = ```\nO(n^3)```\n\n```\nT(n) = 3 * T(n/4) + n```\n, therefore ```\na = 3, b = 4, c = 1```\n, ```\nlog_4 (3) < 1```\n, therefore ```\nT(n)```\n is ```\nO(n^c)```\n = ```\nO(n)```\n\n\n\nA fuller explanation on Wikipedia\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Are these Big-O notations correct for simple modulus and power functions (Python)?\r\n                \r\nSuppose we have the following functions:\n```\ndef modulo(a, b):\n    \"\"\"Take numbers a and b and compute a % b.\"\"\"\n    if b <= 0:\n        return None\n    div = int(a / b)\n    return a - div*b\n\ndef power(a, b):\n    \"\"\"Take number a and non-negative integer b.\n    Compute a**b.\"\"\"\n    if b == 0:\n        return 1\n    else:\n        return a * power(a, b - 1)\n```\n\nFor ```\nmodulo```\n, is the big-O notation ```\nO(a / b)```\n? Because the time it takes to execute depends on  a and b inputs. However, I've seen modulus computations online where the big-O notation is ```\nO(logn)```\n, so not sure if that's applicable here. Can anyone clarify?\nFor ```\npower```\n, would the big-O notation be ```\nO(a^b)```\n because it involves powers?\n    ", "Answer": "\r\nFollowing the previous answered question\nTime complexity is calculated for repetitive tasks. Therefore, if you don't have loops in general, you don't talk about time complexity. You are just applying one thing.\nSo for ```\nmodulo```\n you can 'ignore it' ```\n(O(c)) c is a constant```\n.\nFor the ```\nrecursive function```\n, since we are re-entering in the function each time, until ```\nb==0```\n,  then the complexity is ```\nO(b)```\n similar to ```\nO(n)```\n from the previous question. (```\nlinear```\n)\nIf you google ```\nRecursion```\n it will show you ```\ndid you mean recursion```\n and if you keep clicking it, it keeps taking you to the same page, therefore, recursion is a repetitive task. That is why it you consider it similar to a loop.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the big O notation for this bubble sort?\r\n                \r\nThis bubble sort is part of my code that sorts an array in increasing order and I swap values  with one value between them. I don't know the big O notation of this part of my code. If the length of my array is n, would it be ```\nO(n)```\n or ```\nO(n^2)```\n?\n```\npublic static int Sort(int[] b, int begin) {\n    boolean swapped = false;\n    int count1 = 0;\n    int temporary;\n    while (swapped == false) {\n        swapped = true;\n        for (int i = begin; i < b.length - 1; i+=2) {\n            if (b[i] > b[i + 2]) {\n                temporary = b[i];\n                b[i] = b[i + 2];\n                b[i + 2] = temporary;\n                swapped = false;\n                count1++;\n            }\n        }\n    }\n    return count1;\n}\n```\n\n    ", "Answer": "\r\nTake the worst case: the array is strictly decreasing. Your code will execute the inner for loop ```\nO(n)```\n times, each taking ```\nO(n)```\n, resulting in a total time complexity of ```\nO(n^2)```\n. In general, bubblesort and its derivatives are ```\nO(n^2)```\n.\nHint: Java has Arrays.sort() which uses quicksort for primitives and mergesort for objects, both of which having ```\nO(n log n)```\n runtime which is far superior to ```\nn^2```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Understanding the big O notation [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is off-topic. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\r\n                \r\n                    \r\n                        Closed 10 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nSome standard books on Algorithms produce this:\n\n\n  0 ≤ f(n) ≤ c⋅g(n) for all n > n0\n\n\nWhile defining big-O, can anyone explain to me what this means, using a strong example which can help me to visualize and understand big-O more precisely?\n    ", "Answer": "\r\nAssume you have a function ```\nf(n)```\n and you are trying to classify it - is it a big O of some other function ```\ng(n)```\n.\n\nThe definition basically says that ```\nf(n)```\n is in ```\nO(g(n))```\n if there exists two constants C,N such that\n\n```\nf(n) <= c * g(n) for each n > N\n```\n\n\nNow, let's understand what it means.\n\nStart with the ```\nn>N```\n part - it means, we do not \"care\" for low values of ```\nn```\n, we only care for high values, and if some (final number of) low values do not follow the criteria - we can silently ignore them by choosing ```\nN```\n bigger then them.\n\nHave a look on the following example:\n\n\n\nThough we can see that for low values of n: ```\nn^2 < 10nlog(n)```\n, the second quickly catches up and after ```\nN=10```\n we get that for all ```\nn>10```\n the claim ```\n10nlog(n) < n^2```\n is correct, and thus ```\n10nlog(n)```\n is in ```\nO(n^2)```\n.\nThe constant ```\nc```\n means we can also tolerate some multiple by constant factor, and we can still accept it as desired behavior (useful for example to show that ```\n5*n```\n is ```\nO(n)```\n, because without it we could never find ```\nN```\n such that for each ```\nn > N```\n:  ```\n5n < n```\n, but with the constant ```\nc```\n, we can use c=6 and show ```\n5n < 6n```\n and get that ```\n5n```\n is in ```\nO(n)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation for iteration over steps in list -Python\r\n                \r\nI'm looking to iterate over every third element in my list. But in thinking about Big-O notation, would the Big-O complexity be O(n) where n is the number of elements in the list, or O(n/3) for every third element?\nIn other words, even if I specify that the list should only be iterated over every third element, is Python still looping through the entire list?\nExample code:\n```\ndef function(lst):\n    #iterating over every third list\n    for i in lst[2::3]:\n        pass\n\n```\n\n    ", "Answer": "\r\nWhen using Big-O notation we ignore any scalar multiples out the front of the functions. This is because the algorithm still takes \"linear time\". We do this because Big-O notation considers the behaviour of a algorithm as it scales to large inputs.\nMeaning it doesn't matter if the algorithm is considering every element of the list or every third element the time complexity still scales linearly to the input size. For example if the input size is doubled, it would take twice as long to execute, no matter if you are looking at every element or every third element.\nMathematically we can say this because of the M term in the definition (https://en.wikipedia.org/wiki/Big_O_notation):\n```\nabs(f(x)) <= M * O(f(x)) ```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Sum of big O notation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\n\n  Possible Duplicate:\n  Big O when adding together different routines  \n\n\n\n\nWhat does ```\nO(n) + O(log(n))```\n reduce to? My guess is ```\nO(n)```\n but can not give a rigorous reasoning.\n\nI understand ```\nO(n) + O(1)```\n should reduce to ```\nO(n)```\n since ```\nO(1)```\n is just a constant. \n    ", "Answer": "\r\nWell since ```\nO( f(n) ) + O( g(n) ) = O ( f(n) + g(n) )```\n We are simply trying to calculate an ```\nf(n)```\n such that ```\nf(n) > n + log(n)```\n\n\nSince as n grows sufficiently ```\nlog(n) < n```\n we can say that ```\nf(n) > 2n > n + log(n)```\n\n\nTherefore ```\nO(f(n)) = O(2n) = O(n)```\n\n\nIn a more general sense, ```\nO( f(n) ) + O( g(n) ) = O( f(n) )```\n if ```\nc*f(n)>g(n)```\n for some constant c. Why? Because in this case ```\nf(n)```\n will \"dominate\" our algorithm and dictate its time complexity.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notations - Recursive functions\r\n                \r\nI need to find the complexity of this recursive algorithms, so, i have 3 recursive algorithms and simply just want to know the Big O Notation for them. I think i have the solution for 2 of these algorithms, just wanna check with the community.\n\n```\nint f1(int n)\n{\n    if ( n<= 1)\n        return (1);\n    else \n        return (n *f1(n-1))\n}\n```\n\n\nI think the solution of this is O(n).\n\n```\nint f2 (int n)\n{\n    if(n<=1)\n        return(1);\n    else\n        return(n*f2(n / 2))\n}\n```\n\n\nI think the solution of this is O(Log 2 (n))\n\n```\nint f3 \n{\n    int x, i; \n    if( n <= 1)  \n        return 1;  \n    else \n    {\n        x = f3 (n / 2);           \n        for( i = 1 ; i <= n ; i++)   \n         x++;        \n         return x;  \n    }\n}\n```\n\n\nWhat is the complexity of this recursive algorithm, i don't have the solution for this algorithm, Can you help me?\n    ", "Answer": "\r\nYour first two answer is correct.\nLet's do analysis for your third problem,\nfor each times, n is divides by 2 and we need to add x for n times,\nso the complexity will be\n1*n+1*n/2+1*n/4+.....+1=n(1+1/2+1/4+...)=O(n)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Time complexity of Sieve of Eratosthenes algorithm\r\n                \r\nFrom Wikipedia:\n\n\n  The complexity of the algorithm is\n  ```\nO(n(logn)(loglogn))```\n bit operations.                  \n\n\nHow do you arrive at that?\n\nThat the complexity includes the ```\nloglogn```\n term tells me that there is a ```\nsqrt(n)```\n somewhere.\n\n\n\nSuppose I am running the sieve on the first 100 numbers (```\nn = 100```\n), assuming that marking the numbers as composite takes constant time (array implementation), the number of times we use ```\nmark_composite()```\n would be something like \n\n```\nn/2 + n/3 + n/5 + n/7 + ... + n/97        =      O(n^2)                         \n```\n\n\nAnd to find the next prime number (for example to jump to ```\n7```\n after crossing out all the numbers that are multiples of ```\n5```\n), the number of operations would be ```\nO(n)```\n.\n\nSo, the complexity would be ```\nO(n^3)```\n. Do you agree?\n    ", "Answer": "\r\n\nYour n/2 + n/3 + n/5 + … n/97 is not O(n), because the number of terms is not constant. [Edit after your edit: O(n2) is too loose an upper bound.] A loose upper-bound is n(1+1/2+1/3+1/4+1/5+1/6+…1/n) (sum of reciprocals of all numbers up to n), which is O(n log n): see Harmonic number. A more proper upper-bound is n(1/2 + 1/3 + 1/5 + 1/7 + …), that is sum of reciprocals of primes up to n, which is O(n log log n). (See here or here.)\nThe \"find the next prime number\" bit is only O(n) overall, amortized — you will move ahead to find the next number only n times in total, not per step. So this whole part of the algorithm takes only O(n).\n\n\nSo using these two you get an upper bound of O(n log log n) + O(n) = O(n log log n) arithmetic operations. If you count bit operations, since you're dealing with numbers up to n, they have about log n bits, which is where the factor of log n comes in, giving O(n log n log log n) bit operations.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Sum of big O notation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\n\n  Possible Duplicate:\n  Big O when adding together different routines  \n\n\n\n\nWhat does ```\nO(n) + O(log(n))```\n reduce to? My guess is ```\nO(n)```\n but can not give a rigorous reasoning.\n\nI understand ```\nO(n) + O(1)```\n should reduce to ```\nO(n)```\n since ```\nO(1)```\n is just a constant. \n    ", "Answer": "\r\nWell since ```\nO( f(n) ) + O( g(n) ) = O ( f(n) + g(n) )```\n We are simply trying to calculate an ```\nf(n)```\n such that ```\nf(n) > n + log(n)```\n\n\nSince as n grows sufficiently ```\nlog(n) < n```\n we can say that ```\nf(n) > 2n > n + log(n)```\n\n\nTherefore ```\nO(f(n)) = O(2n) = O(n)```\n\n\nIn a more general sense, ```\nO( f(n) ) + O( g(n) ) = O( f(n) )```\n if ```\nc*f(n)>g(n)```\n for some constant c. Why? Because in this case ```\nf(n)```\n will \"dominate\" our algorithm and dictate its time complexity.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is O(N+N) the same as O(2N) in Big O Notation?\r\n                \r\nIf I have a program with two simple loops and nothing else, giving me O(N+N). With time complexities Big O Notation are we allowed to simplify O(N+N) as O(2N)? If not, do how do both differ in terms of time complexity. I apologize for the simple question but just got into studying these concepts and my study guide keeps using O(N+N) instead of O(2N), which is confusing me.\n    ", "Answer": "\r\nBig O notation is a very well known concept, and so there are tons of material available online.  One Google search away.  Not really sure why would you ask here.\nIn particular, Wikipedia has a very long article: https://en.wikipedia.org/wiki/Big_O_notation\nAnd there is a section on how sums work with this notation: https://en.wikipedia.org/wiki/Big_O_notation#Sum\nOne common misunderstanding is to assume that ```\n=```\n in the notation actually is an equality operation.  While, in fact, it is a set element operation, more commonly written as ```\n∈```\n.\nTechnically, you should be writing ```\nf(x) ∈ O(N)```\n.  Writing ```\nf(x) = O(N)```\n is wrong, as ```\nO(N)```\n is really a set of functions.  It is a set of all the functions that grow as fast as the linear function.\nA sum of two functions that grow as fast as a linear function also grows as fast as a linear function.\n```\nO(f)```\n is a set of functions that grow as fast as ```\nf```\n.\nBoth ```\nO(N+N)```\n, and ```\nO(2N)```\n are the same sets as ```\nO(N)```\n.\nSo, ```\nO(N+N) = O(2N) = O(N)```\n and here, this ```\n=```\n is really the set equality operation.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Sigma Notation\r\n                \r\nWhat is the Big-O of this loop if ```\nsomeWork(..)```\n does exactly ```\ni```\n operations? Algorithm ```\nsomeWork(..)```\n does more work as ```\ni```\n increases. How to represent the solution in sigma notation? \n\n```\ni <--2\nwhile (i < n)\n   someWork (..)\n   i <-- power (i,2)\ndone\n```\n\n\nFirstly, ```\ni <-- power (i,2)```\n is ```\nO(log n)```\n and ```\nsomeWork (..)```\n  seems to be ```\nO(log n)```\n too, because it does more work as ```\ni```\n increases. Multiply both complexities to obtain ```\nO((log n)²)```\n. Confirm?\n    ", "Answer": "\r\nIn every round of the loop the exponent of ```\n2```\n will be doubled. So in the ```\nk```\n-th round the number ```\ni```\n will be ```\n22k```\n. \nThe loop keeps going as long as ```\n22k < n```\n holds, which is equivalent to\n```\nk < log log n```\n. Exactly it is ```\nlog₂ log₂ n```\n, but due to all logarthms are equal exept for a constant factor, I just write ```\nlog log n```\n. \n\nIf ```\nsomeWork()```\n does ```\nO(22k)```\n operations in round ```\nk```\n, you get a total complexity of\n\n\nO( 2 + 22 + 222 + 223 + ... + 22log log n ) \n\n\nThis simplifys to ```\nO(2 ⋅ 22log log n ) = O(n)```\n.\n\nTo see the simplification take a look at the following:\nThe number ```\n2 + 2² + 2⁴ + 2⁸```\n can be written in binary as\n```\n100 010 110```\n. So you can see that\n\n\n2 + 221 + 222 + ... + 22k < 2 ⋅ 22k\n\n\nholds, since it is equal to ```\n100 010 110 < 1 000 000 000```\n.\n\nEdit:\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "violating the given average time complexity in Big-O notation\r\n                \r\nI am trying to Implement a solutions to find k-th largest element in a given integer list with duplicates with ```\nO(N*log(N))```\n average time complexity in Big-O notation, where N is the number of elements in the list.\n\nAs per my understanding Merge-sort has an average time complexity of ```\nO(N*log(N))```\n however in my below code I am actually using an extra for loop along with mergesort algorithm to delete duplicates which is definitely violating my rule of find k-th largest element with ```\nO(N*log(N))```\n. How do I go about it by achieving my task  ```\nO(N*log(N))```\n average time complexity in Big-O notation?\n\n```\npublic class FindLargest {\n    public static void nthLargeNumber(int[] arr, String nthElement) {\n        mergeSort_srt(arr, 0, arr.length - 1);\n        // remove duplicate elements logic\n        int b = 0;\n        for (int i = 1; i < arr.length; i++) {\n            if (arr[b] != arr[i]) {\n                b++;\n                arr[b] = arr[i];\n            }\n        }\n\n        int bbb = Integer.parseInt(nthElement) - 1;\n        // printing second highest number among given list\n        System.out.println(\"Second highest number is::\" + arr[b - bbb]);\n    }\n\n    public static void mergeSort_srt(int array[], int lo, int n) {\n        int low = lo;\n        int high = n;\n        if (low >= high) {\n            return;\n        }\n\n        int middle = (low + high) / 2;\n        mergeSort_srt(array, low, middle);\n        mergeSort_srt(array, middle + 1, high);\n        int end_low = middle;\n        int start_high = middle + 1;\n        while ((lo <= end_low) && (start_high <= high)) {\n            if (array[low] < array[start_high]) {\n                low++;\n            } else {\n                int Temp = array[start_high];\n                for (int k = start_high - 1; k >= low; k--) {\n                    array[k + 1] = array[k];\n                }\n                array[low] = Temp;\n                low++;\n                end_low++;\n                start_high++;\n            }\n        }\n    }\n\n    public static void main(String... str) {\n        String nthElement = \"2\";\n        int[] intArray = { 1, 9, 5, 7, 2, 5 };\n\n        FindLargest.nthLargeNumber(intArray, nthElement);\n    }\n}\n```\n\n    ", "Answer": "\r\nYour only problem here is that you don't understand how to do the time analysis.  If you have one routine which takes O(n) and one which takes O(n*log(n)), running both takes a total of O(n*log(n)).  Thus your code runs in O(n*log(n)) like you want.\n\nTo do things formally, we would note that the definition of O() is as follows:\n f(x) ∈ O(g(x)) if and only if there exists values c > 0 and y such that f(x) < cg(x) whenever x > y.\n\nYour merge sort is in O(n*log(n)) which tells us that its running time is bounded above by c1*n*log(n) when n > y1 for some c1,y1.  Your duplication elimination is in O(n) which tells us that its running time is bounded above by c2*n when n > y2 for some c2 and y2.  Using this, we can know that the total running time of the two is bounded above by c1*n*log(n)+c2*n when n > max(y1,y2).  We know that c1*n*log(n)+c2*n < c1*n*log(n)+c2*n*log(n) because log(n) > 1, and this, of course simplifies to (c1+c2)*n*log(n).  Thus, we can know that the running time of the two together is bounded above by (c1+c2)*n*log(n) when n > max(y1,y2) and thus, using c1+c2 as our c and max(y1,y2) as our y, we know that the running time of the two together is in O(n*log(n)).\n\nInformally, you can just know that faster growing functions always dominate, so if one piece of code is O(n) and the second is O(n^2), the combination is O(n^2).  If one is O(log(n)) and the second is O(n), the combination is O(n).  If one is O(n^20) and the second is O(n^19.99), the combination is O(n^20).  If one is O(n^2000) and the second is O(2^n), the combination is O(2^n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation explanation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Big O, how do you calculate/approximate it?\r\n                            \r\n                                (24 answers)\r\n                            \r\n                    \r\n                Closed 1 year ago.\r\n        \r\n\r\n\r\n    \r\n\r\nwhat is the Big O for below series\n```\n1+2+3+4+....+N\n```\n\nif I've to write a code for the series,  it will be like\n```\npublic void sum(int n){\n int sum =0;\n for(int i=1;i<=n;i++){\n  sum += i;\n }\nprint(sum);\n}\n```\n\nbased on the above code its O(N)\nSomewhere (in a udemy course) I read the order of the series is O(N square). why?\n    ", "Answer": "\r\n\nThis below code has runtime O(N).\n\n```\npublic void sum(int n){\n int sum =0;\n for(int i=1;i<=n;i++){\n  sum=+i;\n }\nprint(sum);\n}\n```\n\nHowever\n\nO(1+2+3+...N) is O(N^2) since O(1+2+3+...N)=O(N(N+1)/2)=O(N^2).\n\nI am guessing you are reading about the second statement and you confuse the two.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation runtime: Cracking the Coding Interview example\r\n                \r\nI mainly just want to confirm my understanding. The following a big-o notation problem from Cracking the Coding Interview. The answer key says runtime is \"O(b) or O(n). The recursive code iterates through b calls, since it subtracts one at each level.\" \n\nSo I understand that the part of the function thats power(a, b-1) is equal to O(b) or O(n). Then would the first \"a\" be constant number in the line \"a * power(a, b-1)\"? \n\nI know that we have to drop constants when we have a big-o of something like O(constant * b), which just becomes O(b). \n\n```\nint power(int a, int b){ \n    if(b < 0)\n       return 0; //error\n    else if(b == 0)\n       return 1; \n    else\n       return a * power(a, b-1)\n }\n```\n\n    ", "Answer": "\r\nYour ```\npower```\n function is just computing the power of some input integer ```\na```\n raised to the ```\nb```\n power.  It does this by simply multiplying ```\na```\n by itself ```\nb```\n times, and then returning that value.  The number of function calls does not really have anything to do with the value of ```\na```\n, but rather only with the value of ```\nb```\n.  So, this function behaves like ```\nO(b)```\n.  We can also rename ```\nb```\n to ```\nn```\n and call this ```\nO(n)```\n, which is what you probably be more likely to see.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation - input size\r\n                \r\nI am reading a blog abt big O notation on topcoder.\nhttps://www.topcoder.com/community/data-science/data-science-tutorials/computational-complexity-section-1/\n\nI have come across the below paragraph\n\n\n  Formal notes on the input size\n  \n  What exactly is this \"input size\" we started to talk about? In the\n  formal  definitions this is the size of the input written in some\n  fixed finite alphabet (with at least 2 \"letters\"). For our needs, we\n  may consider this alphabet to be the numbers 0..255. Then the \"input\n  size\" turns out to be exactly the size of the input file in bytes.\n\n\ncan anyone please explain what does this statement say?\n\n\n  it is the size of the input written in some\n  fixed finite alphabet (with at least 2 \"letters\"). For our needs, we\n  may consider this alphabet to be the numbers 0..255.\n\n    ", "Answer": "\r\nThe statement is about the fundamental representation of information using symbols. The more symbols you use (the bigger the alphabet is), the more information you can represent with less characters although you can represent everything with just two \"letters\", i.e. one bit of information per character. Using the numbers 0..255 is equivalent to using 8 bit, i.e. one byte (2^8=256).\n\nIn computer programming, you normally use bytes but in theoretical computer science bits are used as they have the same capabilities (you just need more of them) and it makes proofs easier to write.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Determining algorithm's runtime using Big-O notation\r\n                \r\nSay there is an algorithm/function that runs as follows:\n\n```\n    function(n)\n       int x = 1\n       int y = 1\n       while( y <= n) {\n          x++\n          y = y + x\n          print(\"something\")\n       }\n```\n\n\nIf I wanted to use Big-O notation to describe its running time, I would say it would be O(logn).\n\nHowever, I say this because as a general rule of thumb, if an algorithm skips over some elements in an element set 'n' (as it does in this case), then the algorithm's running time is likely O(logn).  How would I go about proving this though?  Should I examine the summation of 'y' and its relationship to 'n'? Other approaches?  I appreciate any help.\n    ", "Answer": "\r\nAs I mention in my comment, ```\ny```\n grows quadratically in your function, so the running time is O(sqrt(n)), not O(logn).\n\nIn the case of your simple algorithm, you can put a count in the while loop, to count how many times it runs for different values of n.  This will give you a good starting point for figuring out what you want to prove.\n\nTo actually prove it, just figure out a formula for ```\ny```\n.  You can get a handle on that by calculating ```\ny```\n for small values.  You'll see a pattern.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Why we use big O notation for best and average cases also?\r\n                \r\n\n\nAs we can see best, worst and average case time complexities for different algorithms, then suppose for merge sort, best case should be Ω(n logn) but instead it's given O(n logn). Similarly, for average case it should have been given  Θ(n logn) but there also big O notation is used. And this big O notation is used everywhere in this table, no matter whether it is best case or average case. Please explain me why.\n    ", "Answer": "\r\nIn practice, there are two versions of asymptotic notation in use.\n\n\nFormal, mathematically rigorous asymptotics. If you're working in a mathematical context (for example, you're trying to prove tight bounds on some expression, or you're trying to argue why a certain algorithm doesn't exist), then you absolutely need to choose from O, Ω, o, ω, Θ, etc. properly in the course of making an argument because they have specific, technical meanings. This is why, for example, if you pick up a CS theory paper you'll see a mix of different asymptotic notations tossed around.\nInformal, layperson usage. Most practicing software engineers are interested in big-O notation inasmuch as it relates to overall program efficiency. In this context, big-O notation is used in a way that's not technically mathematically correct but is still a good proxy for what's meant. For example, someone might decide to pick one data structure over another with the justification that \"operations on the first data structure take time O(log n), while operations on the second take time O(n)\" even though such a statement is analogous to saying something like \"Amit is shorter than Pranav, because Amit is at most 2m tall and Pranav is at most 5m tall.\" Although this isn't mathematically correct, in the way that the term is commonly tossed around it's usually clear what's meant.\n\n\nThe challenge with these notations is that if you're expecting a super rigorous, precise, mathematically accurate description of an algorithm's runtime and you get a layperson use of big-O notation, you'll be confused because the literal meaning of what's said might be wrong. Similarly, if you're a software engineer who's used to the layperson version of big-O notation and someone starts tossing around Θ and Ω notation, it can be confusing because you might not be used to seeing it.\n\nI think the \"best\" answer to your question is \"the people making that table probably should be using more precise asymptotic notation, so even though technically what they're doing isn't ideal, it's a relatively common practice to present information this way.\" Since I tend to spend a lot of time in Theoryland, I would personally prefer if they switched to use different asymptotic notation here, but since I also interface with a bunch of software engineers I completely understand why they didn't.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Using Big O notation for counting nodes in a tree\r\n                \r\nHow would I go about solving this problem: Use big O notation to give the number of nodes that can be contained in a tree of depth n\n\nI'm not looking for the exact answer or anything, Just how I would go about to work out the answer? \n    ", "Answer": "\r\nThis was already alluded to in another answer, but specifically there are two relevant quantities in determining the big-O notation for this problem.\n\n\nThe depth of the tree (n)\nThe average number of children that each node has (k)\n\n\nIt's already obvious that if the tree has an infinite depth, then there will be an infinite number of nodes. What big-O notation is really trying to capture here is the rate at which the number of nodes in the tree will grow to infinity.\n\nSince each layer of the tree will have approximately ```\nk```\n times the number of nodes in the previous layer. In the first layer let's say we have some number ```\na```\n nodes, in the second layer then we have ```\nk*a```\n nodes, and in the third layer we have ```\nk*k*a```\n nodes.\n\nOne important piece about complexity is that constants (like a) don't really matter in the grand scheme of things, since ```\n2*infinity```\n is still infinity. Therefore the relevant progression in the previous step through is:\n\n```\n1 -> k -> k*k ->...\n```\n\n\nThe function seems to be of the form, from these few examples, of ```\nk^something```\n, you said not to provide the answer, so I won't directly give the answer, but I think this answer puts your right on the doorstep of it.\n\nGood luck!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "is log(n!) equivalent to nlogn? (Big O notation)\r\n                \r\nIn my book there is a multiple choice question:\n\n\n  What is the big O notation of the following function:\n  n^log(2) +log(n^n) + nlog(n!)\n\n\nI know that log(n!) belongs to O(nlogn), But I read online that they are equivalent. how is log(n!) the same thing as saying nlogn?\nhow is:\nlog(n!) = logn + log(n-1) + ... + log2 + log1 \nequivalent to nlogn?\n    ", "Answer": "\r\nLet ```\nn/2```\n be the quotient of the integer division of ```\nn```\n by ```\n2```\n. We have:\n\n```\nlog(n!) =  log(n) + log(n-1) + ... + log(n/2) + log(n/2 - 1) + ... + log(2) + log(1)\n        >= log(n/2) + log(n/2) + ... + log(n/2) + log(n/2 - 1) + ... + log(2)\n        >= (n/2)log(n/2) + (n/2)log(2)\n        >= (n/2)(log(n) -log(2) + log(2))\n        =  (n/2)log(n)\n```\n\n\nthen\n\n```\nn log(n) <= 2log(n!) = O(log(n!))\n```\n\n\nand ```\nn log(n) = O(log(n!))```\n. Conversely,\n\n```\nlog(n!) <= log(n^n) = n log(n)\n```\n\n\nand ```\nlog(n!) = O(n log(n))```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Finding exact algorithm Efficiency and Big-O notation in nested loops\r\n                \r\nThe efficiency of the algorithm ```\ndoIt```\n can be expressed as O(n) = n^3. Calculate the efficiency of the following program segment exactly. Then calculate the efficiency using the big-O notation. Show calculations.\n\n```\nfor (i = 1; i <= n + 1; i++)\n    for (j = 1; j < n; j++)\n        doIt (...);\n```\n\n\nThanks in advance.\n    ", "Answer": "\r\nThere are two for loops each iterating (almost) n times. So ```\nO(n^2)```\n for them.\n\nA method of ```\nO(n^3)```\n is called (almost) ```\nn^2```\n times, then you will have ```\nn^2 * n^3```\n which will eventually get you to :\n\n```\nO(n^5)```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation explanation nested while loop\r\n                \r\nI was wondering what the big o notation is for the following (java)code:\n\n```\nwhile (n > 0) {\n     while (n > 0){\n        n-- ;\n    }\n } \n```\n\n\nIf I use n = 10 it will do one iteration in the outer loop and 10 inside the inner loop.  So a total of 11 iterations right? \nif I use n = 100 it will de one iteration in the outer loop and 100 inside the inner loop.  So a total of 101 iterations right? \nBut this is the point where I got stuck. Because I think the notation is O(n). Simply because I think the iteration are almost equal to n.\nBut I don't know how to prove it?\n\n\nI am not that much in math so a clear explanation would be appriciated\n    ", "Answer": "\r\nInformally speaking, for positive arguments, the outer loop takes exactly one iteration, as in the inner loop ```\nn```\n is decreased to zero. The inner loop will take exactly ```\nn```\n iterations, so the runtime complexity of the inner loop is ```\nO(n)```\n. In total, although the termination condition of the outer loop syntactically depends on ```\nn```\n, it is in fact independent from ```\nn```\n. The overall complexity can be seen as ```\nO(n+c)```\n where ```\nc```\n is a constant representing the execution of the outer loop. However, ```\nO(n+c)```\n equals ```\nO(n)```\n.\n\nWhat probably puzzles you is that in your terminology, you speak of a number of 101 iterations of a loop, where you in fact refer to two different loops.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation of an algorithm that runs max(n,0) times?\r\n                \r\nI have the following algorithm:\n```\nfor(int i = 1; i < n; i++)\n    for(int j = 0; j < i; j++)\n        if(j % i == 0) System.out.println(i + \" \" + j);\n```\n\nThis will run ```\nmax(n,0)```\n times.\nWould the Big-O notation be ```\nO(n)```\n? If not, what is it and why?\nThank you.\n    ", "Answer": "\r\nYou haven't stated what you are trying to measure with the Big-O notation. Let's assume it's time complexity. Next we have to define what the dependent variable is against which you want to measure the complexity. A reasonable choice here is the absolute value of ```\nn```\n (as opposed to the bit-length), since you are dealing with fixed-length ```\nint```\ns and not arbitrary-length integers.\nYou are right that the ```\nprintln```\n is executed O(n) times, but that's counting how often a certain line is hit, it's not measuring time complexity.\nIt's easy to see that the ```\nif```\n statement is hit ```\nO(n^2)```\n times, so we have already established that the time complexity is bounded from below by ```\nOmega(n^2)```\n. As a commenter has already noted, the if-condition is only true for ```\nj=0```\n, so I suspect that you actually meant to write ```\ni % j```\n instead of ```\nj % i```\n? This matters because the time complexity of the ```\nprintln(i + \" \" + j)```\n-statement is certainly not ```\nO(1)```\n, but ```\nO(log n)```\n (you can't possibly print ```\nx```\n characters in less than ```\nx```\n steps), so at first sight there is a possibility that the overall complexity is strictly worse than ```\nO(n^2)```\n.\nAssuming that you meant to write ```\ni % j```\n we could make the simplifying assumption that the condition is always true, in which case we would obtain the upper bound ```\nO(n^2 log n)```\n, which is strictly worse than ```\nO(n^2)```\n!\nHowever, noting that the number of divisors of ```\nn```\n is bounded by ```\nO(Sqrt(n))```\n, we actually have ```\nO(n^2 + n*Sqrt(n)*log(n))```\n. But since ```\nO(Sqrt(n) * log(n)) < O(n)```\n, this amounts to ```\nO(n^2)```\n.\nYou can dig deeper into number theory to find tighter bounds on the number of divisors, but that doesn't make a difference since the ```\nn^2```\n stays the dominating factor.\nSo the tightest upper bound is indeed ```\nO(n^2)```\n, but it's not as obvious as it seems at first sight.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation complexity for binary multiplcation [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs details or clarity. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Add details and clarify the problem by editing this post.\r\n                \r\n                    \r\n                        Closed 9 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nWhat is the Big O notation for the following operations:\n\nA = 1*(1 0 1 0) + 0*(0 1 0 1)\n\n\n\nThank you OlivierLi. So, please what about this : A = 1*(0 1 1 0 1 0 1 0) + 0*(1 0 0 0 0 1 0 1) + 1 * (1 1 0 0 1 1 0 0) + 0 * (1 0 1 0 1 0 1 0). It is also O(1). As you see it is like one binary bit multiplied by 8 binary bits four times. If it is also O (1), please and kindly, How can I prove that. Thank you with best regards. \n    ", "Answer": "\r\nThis is O(1).\n\nIt doesn't depend on any input at all and is always constant.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation complexity for binary multiplcation [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs details or clarity. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Add details and clarify the problem by editing this post.\r\n                \r\n                    \r\n                        Closed 9 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nWhat is the Big O notation for the following operations:\n\nA = 1*(1 0 1 0) + 0*(0 1 0 1)\n\n\n\nThank you OlivierLi. So, please what about this : A = 1*(0 1 1 0 1 0 1 0) + 0*(1 0 0 0 0 1 0 1) + 1 * (1 1 0 0 1 1 0 0) + 0 * (1 0 1 0 1 0 1 0). It is also O(1). As you see it is like one binary bit multiplied by 8 binary bits four times. If it is also O (1), please and kindly, How can I prove that. Thank you with best regards. \n    ", "Answer": "\r\nThis is O(1).\n\nIt doesn't depend on any input at all and is always constant.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How do you determine the Big-O notation of a while loop?\r\n                \r\nBelow is a binary search function.\n\n```\nint search(int a[], int v, int left, int right)\n{\n    while (right >= left)\n    {\n        int m = (left + right)/2;\n        if (v == a[m]) \n            return m;\n        if (v < a[m])\n            right = m - 1;\n        else left = m + 1;\n    }\n    return -1;\n}\n```\n\n\nHow do I determine the Big-O notation for this function? \n\nIs this search function O(n) since the while loop is dependent on the value of left?\n    ", "Answer": "\r\nIn each step, the range of values halves (roughly) - if you start with ```\nright - left == 100```\n, then at the second step it will be 49, then 24, then 11 etc.\n\nAssuming ```\nn = right - left```\n, the complexity is O(log n). (It's dependent on the values of both right and left.)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Deriving a Big O notation by Algorithm time-complexity analysis\r\n                \r\n```\nint m(int i)                                                                                     \n{\nif (i==1)                                                                                     \n    return i;                                                                                 \nelse                                                                                            \n    return m(i-1) + m(i-1)；                                                     \n        }\n```\n\nI did the following:\nLines 3 and 4 counts for 1 each. I have no idea for calculating the number of times line 5 & 6 will be checked. Since it is recurrsive how can I  to derive the big O notation?\n    ", "Answer": "\r\nYou may understand this best perhaps by just writing out on a piece of paper what the function calls look like for some small input, say ```\ni == 3```\n:\n```\n            m(3)\n       /           \\\n    m(2)           m(2)\n  /      \\       /      \\\nm(1)     m(1)  m(1)     m(1)\n```\n\nIt should be clear that at each level, the number of function calls doubles.  This is exponential behavior, and so the complexity is roughly ```\nO(2^n)```\n, where ```\nn```\n is the input value.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Why this code´s Big-O notation is not log(log(n))\r\n                \r\ntomorrow we have midterm exam and we cant solve the Big O notation of this code:\n```\nfor(int i =1; i<n;i=i*2)\n{\n   for(int j=1; j<i; j = j*2)\n   {\n     cout << \"hello\";\n   }\n} \n```\n\nWe think it can be \"log(log(n))\" but it´s not true.\n    ", "Answer": "\r\nThis loop:\n\n```\nfor(int i =1; i<n;i=i*2)\n```\n\n\nwill execute its body with ```\ni```\n in 1, 2, 4, 8, ..., up to (but not including) ```\ni = n```\n. So it will execute its body O(```\nlog n```\n) times.\nThis loop:\n\n```\n    for(int j=1; j<i; j = j*2)\n```\n\n\nwill execute its body with ```\nj```\n in 1, 2, 4, 8, ..., up to (but not including) ```\ni = j```\n. So it will execute its body O(```\nlog(j)```\n) times. Because ```\nj < n```\n, O(```\nlog j```\n) < O(```\nlog n```\n), so we can also say the ```\nj```\n loop is O(```\nlog n```\n).\nWe've established that the ```\ni```\n loop performs O(```\nlog n```\n) executions of the ```\nj```\n loop, and the ```\nj```\n loop performs O(```\nlog n```\n) executions of its body. Therefore we multiply the factors to get O(```\nlog n```\n) * O(```\nlog n```\n) = O(```\nlog n * log n```\n) = O(```\nlog² n```\n) total executions of the ```\nj```\n loop's body.\nNote that ```\nlog² n```\n is the mathematician's way of writing ```\n(log n)²```\n. It is not the same as ```\nlog log n```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Inclusion of quadratic function in Big O notation\r\n                \r\n\n  We have seen that the Big O notation provides a strict upper bound for\n  f(n). This means that the function f(n) can do better but not worse\n  than the specified value.\n  \n  Examples of functions in O(n3) include: n2.9, n3, n3 + n, 540n3 + 10\n  \n  Examples of functions not in O(n3) include: n3.2, n2, n2 + n, 540n +\n  10, 2n\n  \n  This from the book @Data Structures Using C\", Reema Thareja\n\n\nSo the question is - Why n^2 is not in n^3, as it's less and hence below the upper bound n^3? At the same time n^2.9 is included.\n    ", "Answer": "\r\nThis is a typo or some other kind of mistake. All named functions are in O(n^3) except for n^3.2.\n\nExample 2.1 from the same book even shows that 4n^2 is in O(n^3).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Finding Big O notation when given an array's size range\r\n                \r\nIn one of my CS classes we went over the Big O notation. For a loop like the following the big O would be O(n).\n```\nfor (int i = 0; i < n; i++) { // code for something here}\n```\n\nThis loop is O(n) since n's size is unknown, but what if you knew the size range of n? Say for example you know the passed in array can have a size from 1 to 10 and nothing else. Would the O(n) for the loop above still be O(n), or would it change to be O(1)?\n    ", "Answer": "\r\nBig O is about how the complexity of a problem scales with the size of the input. In your case, the size of the input doesn't change. Because of that, it seems appropriate to call it O(1).\nAn example of when this comes up is when you approximate a computation that requires global information. You may approximate this with local information (only the past K words, or the K nearest nodes in your graph, or your case of a fixed array length) instead of all n items.\nK can be quite large, but in terms of asymptotics it's ignored—because it's fixed. To remind people of this, it's common to still write O(K). Note that for any constant K, the set of functions in O(K) is exactly the same as the set of functions in O(1).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Does anyone actually ever use Big-O notation?\r\n                \r\nWhen during actual development would you care? I understand that it is useful if you wish to compare the speed between two algorithms but is there any other practical use to it during every day development? For example, does anyone ever stop to look at their code and work out the Big-O notation? \n    ", "Answer": "\r\nAmong many things, I write image-processing code, where even a simple function might be applied billions of times per day.\n\nSo, I often include performance info in my code, and where applicable, that includes Big-O information.\n\nBig-O notation is most useful in actual practice for library writers, since library code is always used in cases that were not originally conceived by the author, and such efficiency information can trace down performance problems without time-consuming debugging/profiling.  This is true for internal projects within a company as with publicly-available libraries.  Those who point out that publishers do not normally include Big-O information should be reminded that software just as often does not perform well either.\n\nI see a trend toward less knowledge of algorithms, and more brute-force; this is not about a particular generation of developers, but rather a consequence of how many developers there are - the need for more developers has resulted in most not knowing even the basics of algorithms.  Many don't even know what Big-O notation means, let alone how to apply it, whether choosing a library to use, or writing a library for others to use.  Note that I don't think it is their fault, since there is clearly a focus on getting projects done, whether they are done right or not.  The average life-span of code is decreasing, partially as a consequence of this.\n\nSo, to sum up, I would suggest that Big-O notation is very useful, but not used very often, and its use will decrease even more over time, leaving more developers scratching their heads as to why their code runs so slow.  If you don't use it, don't feel bad, you're in the majority.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "I cannot understand the definition of big O notation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        What is a plain English explanation of \"Big O\" notation?\r\n                            \r\n                                (43 answers)\r\n                            \r\n                    \r\n                Closed 7 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI am taking a Data Structure course this semester and I cannot understand the definition of big O notation.\n\nThe definition says, ```\nf(n) = O(g(n))```\n if there exist positive constant ```\nC```\n and ```\nn0```\n such that\n ```\nf(n)<=C*g(n)```\n for all ```\nn > n0```\n. I understand why ```\nn > n0```\n, it's talking about from the point ```\nn0```\n, ```\nf(n)```\n is always smaller than ```\nc*g(n)```\n. But I cannot understand why ```\nf(n)```\n is compared to ```\nC*g(n)```\n but not just ```\ng(n)```\n.\n\nCan anyone explain please?\n    ", "Answer": "\r\nBecause it allows for a much more succint and useful comparison.\n\nUsing ```\nC```\n makes the definition such that it says\n\n\n  ```\nf```\n grows at roughly (asymptotically) the same speed as ```\ng```\n (or slower)\n\n\nWithout the ```\nC```\n, we'd lose the \"roughly (asymptotically)\" part. Without the ```\nC```\n, we couldn't just say ```\nf = O(n^2)```\n, we'd have to say something like ```\nf = O(1.7 n^2)```\n (unless the factor happened to be 1, of course). That's not particularly useful.\n\nBig O is generally used to talk about algorithm classes, and about ratios. ```\nf = O(n^2)```\n says: \"When you scale ```\nn```\n twice, the computation scales 4 times.\" This would still be true for ```\nO(4 n^2)```\n or ```\nO(1.7 n^2)```\n or even ```\nO(0.01 n^2)```\n.\n\nThe entire point of the Big O notation is to express asymptotic complexity—what trends there are when ```\nn```\n gets large. We don't care whether it takes 4-times as much or half-as much at the same ```\nn```\n; we care how it scales when ```\nn```\n scales, and such scaling is invariant to a multiplicative constant.\n\nNot to mention the fact that fixing the exact constant would be really difficult in specific cases. It's generally easy to show that e.g. an algorithm performs roughly ```\nn```\n operations for each bit of input, and so has ```\nn^2```\n complexity. But it would be quite painful to analyse whether it performs ```\n3 n```\n operations for ```\nn / 2```\n input elements and ```\n2 n```\n operations for the other ```\nn / 2```\n elements, or something else.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of if inside while-loop\r\n                \r\nI have this code and I need to figure out its Big O notation but I'm very unsure about it. My idea was that it should be some large integer times ```\nn```\n because everything but the ```\nwhile```\n-loop should be constant. Hence it would be ```\nO(n)```\n. Is that correct?\n(I know that the code does not make sense. I have changed it so that its actual purpose is no longer recognizable. So c==3, d==4, e==5 and f==6 aren't always true.)\n```\na,b,i = 3,0,0\nmystring = input(\"Input your string\")\nif a == 3:\n    print(a)\nelse:\n    print(\"a is not 3\")\nwhile (b < 10) and (i < 4*len(mystring)):\n    c,d = 3,4\n    if (c==3 and d==4):\n        e,f = 5,6\n        if (e==5 and f==6):\n            print(\"good so far\")\n            b +=1\n        else:\n            i +=1\n    else:\n        i +=1\nif i >= 4*len(mystring):\n    print(\"maximum reached\")\nelse:\n    print(i)\n```\n\n    ", "Answer": "\r\nThe conditions ```\nc == 3 and d == 4```\n and ```\ne == 5 and f == 6```\n are both always true, since the lines immediately before them set these variables to these exact values. So we might as well write it like this, because it does the same thing:\n```\nwhile b < 10 and i < 4 * len(mystring):\n    c, d = 3, 4\n    # (c == 3 and d == 4) is True\n    e, f = 5, 6\n    # (e == 5 and f == 6) is True\n    print(\"good so far\")\n    b += 1\n```\n\nHopefully it is now obvious that this loop iterates at most 10 times, because ```\nb```\n increases by 1 on every iteration and the loop terminates when ```\nb```\n reaches 10 (if not sooner). So the overall time complexity is Θ(1).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Time complexity of this algorithm using Big-O notation\r\n                \r\n\nI have to find the time complexity of the pseudocode I have created and specify it using the Big-O notation. The problem is that I don't know how to calculate it when I have an if-statement inside nested for-loops. \nHere is my pseudocode, what is in brackets is the number of operations: \n```\nAlgorithm largestProduct(A)\n  Input array A\n  Output largest product value of two elements in array A, the values and their indices\n  index1 ← 0                                          (1)\n  index2 ← 0                                          (1)\n  n ← A length                                        (1)\n  max ← 0                                             (1)\n  for i ← 0 to n-1 do                                 (n)\n    for j ← i + 1 to n do                             (n^2)\n      if max < A[ i ] * A[ j ] then                   (?)\n        max ← A[ i ] * A[ j ]\n        index1 ← i\n        index2 ← j\n  return max, A[index1], index1, A[index2], index2\n```\n\nThank you in advance for your help.\n    ", "Answer": "\r\nSince the operations inside the ```\nif```\n statement and its condition do not affect the number of the iterations and they are single operations (constant), then you can consider the ```\nif```\n statement as an ```\nO(1)```\n.\nIt is true that the nested ```\nfor```\n loops do ```\nO(n^2)```\n iterations, therefore, there are constant amount of operations running ```\nO(n^2)```\n times, makes it ```\nO(n^2)*O(1)=O(n^2)```\n overall.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Finding Big O notation\r\n                \r\nI have the following code and I want to find the Big O. I wrote my answers as comments and would like to check my answers for each sentence and the final result.\n\n```\npublic static void findBigO(int [] x, int n)\n{ \n    //1 time\n    for (int i = 0; i < n; i += 2) //n time\n        x[i] += 2; //n+1 time\n\n    int i = 1; //1 time\n    while (i <= n/2) // n time\n    { \n        x[i] += x[i+1]; // n+1 time\n        i++; //n+1 time\n    }\n} //0\n\n//result: 1 + n + n+1 + n + n+1 + n+1  = O(n)\n```\n\n    ", "Answer": "\r\nFirst of all: simple sums and increments are O(1), they are made in constant time so ```\nx[i] += 2;```\n is constant since array indexing is also O(1) the same is true for ```\ni++```\n and the like.\n\nSecond: The complexity of a function is relative to its input size, so in fact this function's time complexity is only pseudo-polynomial\n\nSince n is an integer, the loop takes about n/2 interactions which is linear on the value of n but not on the size of n (4 bytes or log(n)).\n\nSo this algorithm is in fact exponential on the size of n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation runtime\r\n                \r\nI have been given some code to work out big O runtimes on them,  could someone tell me if I am on the right track or not?\n\n```\n//program1\n int i, count = 0, n = 20000;\n\nfor(i = 0; i < n * n; i++)\n{\n    count++;\n}\n```\n\n\nIs that O(n^2)?\n\n```\n//number2\nint i, inner_count = 0, n = 2000000000;\n\n    for(i = 0; i < n; i++)\n    {\n        inner_count++;\n    }\n```\n\n\nis this one O(n)?\n\n```\n//number3\nfor(i = 0; i < n; i++)\n{\n    for(j = 0; j < n; j++)\n    {\n        count++;\n    }\n}\n```\n\n\nO(n^2)?\n\n```\n//number4\nfor(i = 0; i < n; i++)\n{\n    for(j = 0; j < i; j++)\n    {\n        for(k = 0; k < j; k++)\n        {\n            inner_count++;\n        }\n    }\n}\n```\n\n\nIs that O(n^3)?\n\n```\n//number5\nint i, j, inner_count = 0, n = 30000;\n\nfor(i = 0; i < n; i++)\n{\n    for(j = 0; j < i; j++)\n    {\n        inner_count++;\n    }\n}\n```\n\n\nIs that one O(n^3)?\n\n```\n//number6\nint i, j, k, l, pseudo_inner_count = 0, n = 25;\n\nfor(i = 0; i < n; i++)\n{\n    for(j = 0; j < i*i; j++)\n    {\n        for(k = 0; k < i*j; k++)\n        {\n            pseudo_inner_count++;\n            for(l = 0; l < 10; l++);\n        }\n    }\n}\n```\n\n\nVery confused about this one O(n^3)??\n\n```\n//number7\n\nint i, j, k, pseudo_inner_count = 0, n = 16;\n\nfor(i = n; i > 1; i /= 2)\n{\n    for(j = 0; j < n; j++)\n    {\n        pseudo_inner_count++;\n        for(k = 0; k < 50000000; k++);\n     }\n}\n```\n\n\nO(n)? (I get more lost as they get harder)\n\n```\n//number8\nint i, j, pseudo_inner_count = 0, n = 1073741824;\n\nfor(i = n; i > 1; i /= 2)\n{\n    pseudo_inner_count++;\n    for(j = 0; j < 50000000; j++);\n}\n```\n\n\nO(n^2)?\n\none more i didn't see and have absolutely no idea with\n{\n\n```\n    int i, wrong_count = 0, n = 200;\n\n    for(i = 0; i < square(n); i++)\n    {\n        wrong_count++;\n    }\n\n    printf(\"%d %d\\n\", wrong_count, square(wrong_count));\n\n    return 0;\n}\n\nint square(int m)\n{\n    int i, j = 0;\n\n    for(i = 0; i < m * m; i++)\n    {\n        j++;\n    }\n\n    return j;\n}\n```\n\n\nIf anyone could clarify these and help me understand them better I would be very grateful.\n    ", "Answer": "\r\n\nnumber5 is O(N^2)\nnumber6 is O(N^6)\nnumber7 is O(N*logN)\nnumber8 is O(logN)\n\n\nthe rest seems correct.\n\nTry to understand how many operations are done as function of N, all the constant operation, such as\n\n```\nfor (int i = 0; i < 333333333; ++i) { j++; } \n```\n\n\nare in fact O(1)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation Code Algorithm Analysis Homework\r\n                \r\n```\nfor(int i=N; i>0; i=i/2)  \n    irrelevant statement;\n```\n\n\nI am asked to find the complexity class and I am unsure of whether I am supposed to use Big-Omega notation or Big-O? But I am assuming that it is O(N/2) and then O(N) if I drop the constants.\n\n```\nfor (int i=0; i<N; i++)  \n    for (int j = i+1; j<N; j++)  \n        irrelevant statement;\n```\n\n\nFor this one I believe it is O(N) * O(N+1) -> O(N^2 + N) and then O(N^2) after I drop the N?\n    ", "Answer": "\r\nFor the first one, how many more operations get executed if you double N?\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Confused on big O notation\r\n                \r\nAccording to this book, big O means:\n\n\nf(n) = O(g(n)) means c · g(n) is an upper bound on f(n). Thus there exists some constant c such that f(n) is always ≤ c · g(n), for large enough n (i.e. , n ≥ n0 for some constant n0).\n\n\nI have trubble understanding the following big O equation\n\n3n2 − 100n + 6 = O(n2), because I choose c = 3 and 3n2 > 3n2 − 100n + 6; \n\nHow can 3 be a factor? In 3n2 − 100n + 6, if we drop the low order terms -100n and 6, aren't 3n2 and 3.n2 the same? How to solve this equation?\n    ", "Answer": "\r\nI'll take the liberty to slightly paraphrase the question to:\n\n\n  Why do  and  have the same asymptotic complexity.\n\n\nFor that to be true, the definition should be in effect both directions.\n\nFirst:\n\n\nlet \n\n\n\nThen for  the inequality is always satisfied.\n\nThe other way around:\n\n\nlet \n\n\n\nWe have a parabola opened upwards, therefore there is again some  after which the inequality is always satisfied.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to find runtime from Big-O notation?\r\n                \r\nI am having difficulty understanding how to find the exact runtime of a function when only given an input size and the Big-O notation for the function. Could someone explain how to do the following example problem?\nAn algorithm takes 0.5 ms for input size 100. How long will it take for input size 500 if the running time is the following (assume low-order terms are negligible)?\na. linear\nb. O(N logN)\nc. quadratic\nd. cubic\n    ", "Answer": "\r\nShortly put, you treat relative execution time as an appropriate function of relative input size, and you select the 'appropriate function' based on the assumed complexity of the algorithm.\nSpecifically, a linear algorithm's execution time will scale as a linear function of relative input size, a quadratic algorithm's execution time scales quadratically with input size, etc.\nIn this example, you know the algorithm takes 0.5ms to execute on input size 100, and if you want to know how it performs on input size 500, the first question you ask yourself is 'What is the relative change in input size?' (ie 'by what factor did it change?') Answer = new_size/old_size = 500/100 = 5: the input is 5 times larger.\nNext, to determine (estimate) the new execution time, you apply the appropriate function to this scale factor and the original execution time based on the algorithm's assumed complexity. Let's call this function for the new execution time T, denoting the initial time as t_0 and the input scale factor as f. You evaluate T to get the new execution time using the assumed complexity as so:\n```\nLinear:       T(f,t_0) = (f)t_0      = (5)0.5ms       = 0.25ms \nLinearithmic: T(f,t_0) = flog(f)t_0  = (5log(5))0.5ms = 0.58ms (assuming base 2)\nQuadratic:    T(f,t_0) = (f^2)t_0    = (5^2)0.5ms     = 1.25ms\nCubic:        T(f,t_0) = (f^3)t_0    = (5^3)0.5ms     = 62.5ms\n```\n\nYou can generalize this approach to other situations where you want to estimate the execution time of an algorithm on a new input size based on its assumed complexity and a datapoint about its execution time on some input by calculating the new scale factor 'f' and plugging in the known time for t_0.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of if inside while-loop\r\n                \r\nI have this code and I need to figure out its Big O notation but I'm very unsure about it. My idea was that it should be some large integer times ```\nn```\n because everything but the ```\nwhile```\n-loop should be constant. Hence it would be ```\nO(n)```\n. Is that correct?\n(I know that the code does not make sense. I have changed it so that its actual purpose is no longer recognizable. So c==3, d==4, e==5 and f==6 aren't always true.)\n```\na,b,i = 3,0,0\nmystring = input(\"Input your string\")\nif a == 3:\n    print(a)\nelse:\n    print(\"a is not 3\")\nwhile (b < 10) and (i < 4*len(mystring)):\n    c,d = 3,4\n    if (c==3 and d==4):\n        e,f = 5,6\n        if (e==5 and f==6):\n            print(\"good so far\")\n            b +=1\n        else:\n            i +=1\n    else:\n        i +=1\nif i >= 4*len(mystring):\n    print(\"maximum reached\")\nelse:\n    print(i)\n```\n\n    ", "Answer": "\r\nThe conditions ```\nc == 3 and d == 4```\n and ```\ne == 5 and f == 6```\n are both always true, since the lines immediately before them set these variables to these exact values. So we might as well write it like this, because it does the same thing:\n```\nwhile b < 10 and i < 4 * len(mystring):\n    c, d = 3, 4\n    # (c == 3 and d == 4) is True\n    e, f = 5, 6\n    # (e == 5 and f == 6) is True\n    print(\"good so far\")\n    b += 1\n```\n\nHopefully it is now obvious that this loop iterates at most 10 times, because ```\nb```\n increases by 1 on every iteration and the loop terminates when ```\nb```\n reaches 10 (if not sooner). So the overall time complexity is Θ(1).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the complexity of inserting into sorted link list in big-O notation?\r\n                \r\nWhat is the complexity of inserting into sorted link list in big-O notation? Let say I have 5 elements and what is the complexity to insert all of them.\n\nThank you very much\n    ", "Answer": "\r\nThink about what a single insertion into a sorted link list means.  You have a new element that has to go somewhere in the list.\n\n1) You have to find the right place in the list.  This is a linear search.  O(n)\n\n2) Insertion is easy: create new node, fix pointers to the previous and next nodes.  O(1)\n\nIn this case, the O(n) outweighs the O(1) so it's O(n).\n\nThe number of elements doesn't really apply to big-O, since it's all based on orders of magnitude.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "showing that a function is O(something) (big O notation)\r\n                \r\nI have a problem understanding how to solve the big-O-notation equations, such as this one:\n\n```\nf(n) = 10n + 5 and g(n) = n\nShow that f(n) is O(g(n)) \n```\n\n\nTaken from here: http://web.eecs.utk.edu/~booth/311-01/notes/bigOex.html\n\nIt says there that ```\nTo show f(n) is O(g(n)) we must show constants c and k such that f(n) <= cg(n) for all n >=k```\n\n\nI get that: Starting from k, f(n) will not grow faster than g(n) times an arbitrary constant.\n\nWhat I don't get is this:\n\n```\nWe are allowed to choose c and k to be integers we want as long as they \nare positive.  They can be as big as we want, but they can't be functions\nof n.\n```\n\n\nSo in order to solve those equations, I can always choose c and k as long as they meet those requirements? But if I can choose k, then why do they calculate k on that website? \n\n```\nTry c = 15.  Then we need to show: 10n + 5 <= 15n.\n             Solving for n we get: 5 <= 5n  or 1 <= n.\nSo f(n) = 10+5 <= 15g(n) for all n >= 1.  (c = 15, k = 1)\n```\n\n    ", "Answer": "\r\nBasically, they want to show that, beyond a given value of n, c * g(n) is always >= f(n). So if you set k to that value, the inequality holds. It is not necessary to show precisely beyond which point it holds (i.e. the minimum valid value for k given some arbitrary c) -- it just happens to become obvious when solving the inequality. \n\nAs long as you can prove there exists some c, k for which that holds, you can say f(n) is O(g(n)).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Understanding Big-O Notation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        What is a plain English explanation of \"Big O\" notation?\r\n                            \r\n                                (43 answers)\r\n                            \r\n                    \r\n                Closed 8 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI am reading a book \"Beginning Algorithms\" by Simon Harris and James Ross.\n\nIn the early pages, there is a section on Understanding Big-O Notation. I read this section, and re-read this section maybe about a dozen times. I still am not able to wrap my head around a couple of things. I appreciate any help to get rid of my confusion.\n\nThe author / authors state \"The precise number of operations is not actually that important. The complexity of an algorithm is usually defined in terms of the order of magnitude of the number of operations required to perform a function, denoted by a capital O for order of followed by an expression representing some growth relative to the size of the problem denoted by the letter N.\"\n\nThis really made my head hurt, and unfortunately, everything else following this paragraph made no sense to me because this paragraph is supposed to lay the foundation for the next reading.\n\nThe book does not define \"order of magnitude.\" I googled this and the results just told me that order of magnitude are defined in powers of 10. But what does that even mean? Do you take the number of operations and define that number in a power of 10, and that equals the complexity? Also, what is considered the \"size of the problem?\" Is the size of the problem the number of operations? Or is the size of the problem the \"order of magnitude of the number of operations required to perform a function.\"\n\nAny practical examples and a proper explanation of this would really help.\n\nThanks!\n    ", "Answer": "\r\nKeep it simple!\n\nJust think of the Big-O as a way to express the performance of the algorythm. That performance will depend on the number of elements the algorythm is handeling = n.\n\nAn example, when you have to make a sum. You need one statement for the first addition, one statement for the second addition, and so on... So the performance will be linear with the number of elements = O(n).\n\nImagine a sort algorythm, which is very smart, and for each element of handles it automatically shortens the sort for the next element. This will be logarithmic with the number of elements = O(log(n)).\n\nOr a complex formula with parameters, and with each extra parameter, the execution time multiplies. This will be exponential = O(10^n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation Log Base 2 or Log Base 10 [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Is Big O(logn) log base e?\r\n                            \r\n                                (7 answers)\r\n                            \r\n                    \r\n                Closed 9 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nWhen articles/question state that the Big O running time of the algorithm is O(LogN) .\n\nFor example Quicksort has a Big O running time of O (LogN) where the it is Log base 10 but Height of binary tree is O(LogN+1) where it is Log base 2\n\nQuestion\n\n1)I am confused over whether is it Log base 10 or Log base 2 as different articles use different bases for their Logarithm . \n\n2) Does it make a difference if its Log base 2 or Log base 10??\n\n3)Can we assume it mean Log base 10 when we see O(LogN)???\n    ", "Answer": "\r\nI think it does not matter what is the base of the log as the relative complexity is the same irrespective of the base used.\n\nSo you can think of it as  O(log2X) = O(log10X) \n\nAlso to mention that logarithms are related by some constant.\n\n\n\nSo log₁₀(x) = log₂(x) / log₂(10)\n\nSo most of the time we generally disregard constants in complexity analysis, and hence we say that the base doesn't matter.\n\nAlso you may find that the base is considered to be 2 most of the time like in Merge Sort. The tree has a height of ```\nlog₂ n```\n, since the node has two branches.\n\n\n  1)I am confused over whether is it Log base 10 or Log base 2 as\n  different articles use different bases for their Logarithm .\n\n\nSo as explained above this change of base doesn't matter.\n\n\n  2) Does it make a difference if its Log base 2 or Log base 10??\n\n\nNo it does not matter.\n\n\n  3)Can we assume it mean Log base 10 when we see O(LogN)???\n\n\nYes you can assume that provided you know the base conversion rule.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How do I write Big O-notations\r\n                \r\nI don´t really know how to express in big O-notation. I´ve seen several sources talking about this but it only made me more uncertain. When I write in big-O should I just ignore the constants? \n\nexamples:\n\n```\n1. 0.02N³\n2. 4N*log(2^N)\n3. 24Nlog(N)\n4. N²\n5. N*sqrt(N)\n```\n\n\nthis is what I mean with \"ignore the constants\":\n\n```\n1. O(N³)\n2. O( N*log(2^N) )\n3. O( Nlog(N) )\n4. O( N² )\n5. O( N*sqrt(N) )\n```\n\n\nand how fast are ```\nO( N*log(2^N) )```\n and ```\nO( N*sqrt(N) )```\n growing compared to the other examples?\n\nI really appreciate the help so thanks in advance\n    ", "Answer": "\r\nBig O notation is characterizes asymptotic behavior of function. Mathematically ```\nf(x) = O(g(x))```\n when ```\nlim (x->inf) (f(x)/g(x)) = const```\n\n\nLet's get some clarity. There are 5 common notations (Bachmann–Landau notations):\n\n```\n ω (small omega)\n Ω (big omega)\n Θ (theta)\n Ο (big o)\n ο (small o)\n```\n\n\nThey works like mathematical comparison operators:\n\n```\n < (strictly less)\n <= (less or equals)\n = (equals)\n >= (greater or equals)\n > (strictly greater)\n```\n\n\nStrictly saying, big o is just an upper bound so you can't say which function grows faster based just on big-o notation.\n\nFor example, quick sort has worst case complexity = O(n2) but it's also right to say that quick sort has worst case complexity = O(n889).\nIt's just like we can say x < 899 based on knowledge that x < 2.\n\nBecause of the limiting behavior you can ignore constants and less-ordered summands (they are \"dominated\" by the highest order summand) of your functions.\nFor example, if ```\nf(x) = 33*n³ + n² + n + 3544```\n, it's right to say that ```\nf(x) = O(n³)```\n \n(Moreover, it's right to say ```\nf(x) = Θ(n³)```\n which is much more informative (```\nΘ```\n is called a ```\ntight bound```\n)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is the following big-o notation equivalent to each other?\r\n                \r\nO[ ((1/n)*(log2n)2 + 1/√n) * ( √nlog3(log2n) + √nlog2n ) ] = O [ (log n)3/√n ]\n\nIs the above Big O notation equivalent to each other? I expanded the left side out (not shown here) and it seems that  [(log n)3/√n]  is the highest power.\n\nIf they are equivalent to each other, is there a simpler way of finding out why? Because I think expanding the left side out is too much work.\n    ", "Answer": "\r\nThis: ((1/n)*(log2n)2 + 1/√n) can be replaced with just 1/√n, because the rest is much smaller for large n, while ( √nlog3(log2n) + √nlog2n ) becomes √nlog2n for the same reason, so in the end you have 1/√n * √nlog2n, which is just log2n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation when n multiplied by 4 every iteration\r\n                \r\nwhat is the big o notation  of the following: \n\n```\nfor (int i = 1; i < 7; i=*4) \n{\n    print('something ');\n}  \n```\n\n\nas far as i know it should be O(log n) but i heard someone says that n should be multiplied or divided by only 2 not any number else to be log n\nis that correct? \n    ", "Answer": "\r\nYou're right, and what you heard is wrong.\n\nWhen the counter follows a geometric progression up to a limit, the number of iterations is logarithmic with respect to that limit.\n\nChanging the multiplier just changes the base of the logarithm involved, but from a big-O perspective, the base of the logarithm is irrelevant. As long as it's a logarithm in some base, the complexity is O(log N).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to solve Big-O Notation for prime number function?\r\n                \r\nI am trying to understand Big-O Notation. So sorry if I am asking something that is too obvious but I can't seem to wrap my head around this.\n\nI have the following C# code function that I am trying to calculate Big-O Notation for.\n\n```\nfor (i = 2; i < 100; i++)\n     {\n        for (j = 2; j <= (i / j); j++)\n           if ((i % j) == 0) break; // if factor found, not prime\n        if (j > (i / j)) \n           Console.WriteLine(\"{0} is prime\", i);\n     }\n```\n\n\nNow what I got so far is that I think that both the ```\nif```\n clauses are considered constant O(1) and not taken into account for the calculation of this algorithm? And also if I have understood correctly a single for loop\n\n```\nfor(i = 0; i < 100; i++)\n```\n\n\nsince it is a linear function would be O(n) and a nested loop that does not depend on a variable from the surrounding loop \n\n```\nfor(i = 0; i < 100; i++)\n    for(j = 0; j < 100; j++)\n```\n\n\nIs O(n^2)? But how do I calculate a function such as the top one where the second loop is dependent on the first loop and creates a non-linear function? \n\n\n\nI found a definition for linearithmic that says\n\n\n  Linearithmic algorithm scales to huge problems. Whenever N doubles,\n  the running time more (but not much more) than doubles.\n\n\nWhile this seems to be a good description of how this code snippet runs would that mean that it is O(N Log[n]) and if so how could I calculate this?\n    ", "Answer": "\r\n@Jon is close but his analysis is a bit wrong, and the real complexity of your algorithm is ```\nO(n*sqrt(n))```\n.\nThis is based on the fact that for each number ```\ni```\n, the expected number of 'work' you should do in the inner loop is:\n```\n1/2 + 2/3 + 3/4 + ... + (sqrt(i)-1)/sqrt(i) = \n = 1-1/2 + 1-1/3 + ... + 1-1/sqrt(i)\n = sqrt(i) - (1/2 + 1/3 + ... + 1/sqrt(i)\n = sqrt(i) - H_sqrt(i)\n```\n\nsince ```\nH_sqrt(i)```\n (The harmonic number) is in ```\nO(log(sqrt(i)) = O(1/2*log(i)```\n, we can conclude that the complexity is ```\nO(sqrt(i)-log(i)) = O(sqrt(i))```\n, per prime number calculation.\nSince this is done repeatedly per each ```\ni```\n, the total complexity of the problem is ```\nO(sqrt(2) + sqrt(3) + ... + sqrt(n))```\n. According to this forum thread, the sum of squared roots is in ```\nO(n*sqrt(n))```\n, which is \"worse\" than ```\nO(nlogn)```\n.\nThings to notice:\n\nThe first sum is up to sqrt(i) since this is the point where ```\nj > (i / j)```\n.\nThe first sum is ```\n(j-1)/j```\n for each ```\nj```\n, because on average one out of ```\nj```\n elements is getting into the break (1/3 of the elements are dividable by 3, 1/4 by 4,...) this leaves us ```\n(j-1)/j```\n that are not - and this is the expected work we have.\nThe equality ```\nO(log(sqrt(n)) = O(1/2*log(n)```\n comes from ```\nO(log(n^k))=O(k*log(n))=O(log(n))```\n for any constant ```\nk```\n. (in your case k=1/2)\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation for the given code fragments [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Big O, how do you calculate/approximate it?\r\n                            \r\n                                (24 answers)\r\n                            \r\n                    \r\n                Closed 5 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI am looking for some help regarding big O notation. The goal is to give the order of growth for the given code fragments.\n\n```\nint sum = 0\nfor (int k = n; k > 0; k/=2 )\n  for (int i = 0; i < k; i++)\n    sum++;\n```\n\n\nFor this code fragment I got (N logN). The first for loop is logN and the second for loop is N.\n\n```\nint sum = 0\nfor (int i = 1; i < n; i *= 2 )\n  for (int j = 0; j < i; j++)\n    sum++;\n```\n\n\nI had some trouble on this one. The first for loop is logN, however the second for loop is where I get stuck. The second for loop in dependent on the first for loop. I'm not sure how to show that in big N notation. \n\n```\nint sum = 0\nfor (int i = 1; i < n; i *= 2 )\n  for (int j = 0; j < n; j++)\n    sum++;\n```\n\n\nThe first for loop is logN. The second for loop is N. So this is (N)?\n\nI am struggling with this and would appreciate some help. Thank you\n    ", "Answer": "\r\n\nYou are correct on the first code fragment: O(n*log n).\nIn the second code fragment, ```\nj```\n loops up to ```\ni```\n, which can goes as high as ```\nn - 1```\n, so that ```\nj```\n ```\nfor```\n loop is O(n) by itself.  But let's examine what happens.\n\n\nn = 16\ni = 1 The inner loop runs 1 time.\ni = 2 The inner loop runs 2 times.\ni = 4 The inner loop runs 4 times.\ni = 8 The inner loop runs 8 times.\nn = 17\ni = 1 The inner loop runs 1 time.\ni = 2 The inner loop runs 2 times.\ni = 4 The inner loop runs 4 times.\ni = 8 The inner loop runs 8 times.\ni = 16 The inner loop runs 16 times.\n\n\n\nThe counts of the loops is\n\n```\n1 + 2 + 4 + ... + 2^x = 2^(x+1) - 1\n```\n\n\nwhere x is the power of 2 before ```\nn```\n.  This ```\n2^(x+1)```\n could be as high as 2n, so the overall complexity is O(n), dropping the constant \"2\".\n\n\nThe third code fragment is similar to the second code fragment; only that ```\nj```\n goes all the way to ```\nn```\n each time.  The complexity here is O(n*log n).\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation of a function in Java by experiment\r\n                \r\nI need to write a program that determines the Big-O notation of an algorithm in Java. \n\nI don't have access to the algorithms code, so it must be based on experimentation and execution times. I don't know where to start.\n\nCan someone help me?\n\nEdit: The only thing i know about the algorithm is that takes an integer value and doesn't have a return\n    ", "Answer": "\r\nFirstly, you need to be aware that what such a program does it to provided an evidence-based guess as to what complexity class the algorithm belongs to.  It can give the wrong answer.  (Indeed, in complicated cases where the complexity class is unusual, wrong answers are increasingly likely.)\n\nIn short, this is NOT complexity analysis.\n\nThe general approach would be:\n\n\nRun the algorithm multiple times with values of N across the range, measuring the execution times.  Repeat multiple times for each N, to ensure that you are getting consistent measurements.\nTry to fit the experimental results to different kinds of curves; i.e. linear, quadratic, logarithmic.  Note that it is the fit for large values of N that matters.  So when you check for \"goodness of fit\", use a measure that gives increasing weight to the larger data points.\n\n\nThis is intended as a start point. For example, I'm expecting that you will do your own research on issues such as:\n\n\nhow to get reliable execution-time measurements (for Java),\nhow to do curve fitting in a mathematically sound way, and\ndealing with the case where the execution times get too long to measure experimentally for large N.\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is this Big-O Notation Correct for this Simple Sort Function (Python)?\r\n                \r\nSuppose we have the following function:\n```\ndef sort_list(list_of_strings):\n    \"\"\"Take a list of strings, sort the letters in each string,\n    and then sort the full list. The list is modified in place.\"\"\"\n    for i in range(len(list_of_strings)):\n        list_of_strings[i] = ''.join(sorted(list_of_strings[i]))\n    list_of_strings.sort()\n```\n\nWould it be correct to say that this has big-O notation of ```\nO(n)```\n because the length of time it takes for the function to run depends on the length of ```\nlist_of_strings```\n?\n    ", "Answer": "\r\nSorting a list of size N takes O(N * log(N)) time (generally, it depends on the exact input and the algorithm used, but Python's builtin sorting functions don't do worse than that).\nDoing something in a loop multiplies the runtime, and doing things sequentially means adding the runtime together.\nThis means, given ```\nN = len(list_of_strings)```\n and ```\nM = max(len(string) for string in list_of_strings)```\n, the runtime for the loop is ```\nN * O(M * log(M)) = O(N * M * log(M))```\n, and the bit afterwards is ```\nO(N * log(N))```\n, which means the runtime for ```\nsort_list```\n is ```\nO(N * M * log(M) + N * log(N))```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "difference between two codes in big O notation in java [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        What is a plain English explanation of \"Big O\" notation?\r\n                            \r\n                                (43 answers)\r\n                            \r\n                    \r\n                Closed 7 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI was reading about arrays and I programmed the first code below . The teacher did program the second code for searching in an array for a specific number.\nWhat is the difference in ordo notation between the two following codes in java .\nWhich code performs better and what its big O notation.\n\nMy code\n\n```\npublic static void main(String[] args)\n{\n\n   int[] data = { 100, 110, 120, 130, 140, 150 };\n   int index = binarySearch(data, 120);\n   System.out.println(index);\n}\n\nprivate static int binarySearch(int[] data, int i)\n{\n\n   if (data.length == 0)\n   {\n      return -1;\n   }\n\n   for (int k = 0; k < data.length; k++)\n   {\n      if (data[k] == i)\n      {\n         return k;\n      }\n   }\n\n   return -1;\n}   \n```\n\n\n\n\nTeacher's code\n\n```\npublic static void main(String[] args)\n{\n   int[] data = { 100, 110, 120, 130, 140, 150 };\n   int index = binarySearch(data, 120);\n   System.out.println(index);\n}\n\nstatic int binarySearch(int[] keys, int v)\n{\n   int position;\n   int begin = 0, end = keys.length - 1;\n   while (begin <= end)\n   {\n      position = (begin + end) / 2;\n      if (keys[position] == v)\n      {\n         return position;\n      }\n      else if (keys[position] < v)\n      {\n         begin = position + 1;\n      }\n      else\n      {\n         end = position - 1;\n      }\n   }\n   return -1;\n}\n```\n\n    ", "Answer": "\r\n1st Solution is O(n).\n\n2nd Solution is O(log n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Want to make sure that the Big O notation i got is correct [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and  cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened,  visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI have written this code but I have to find the big O notation. I came up with O(n2) but I am not sure if it is correct. Please somebody help. Thanks\n\n```\nint n = array.length;\n\nfor(int i=0;i<array.length;i++){\n  int c = 1;\n  for(int j=i+1;j<array.length;j++)\n    if (array[i]==array[j])\n      c=c+1;\n    if (c>(array.length/2)){\n      return array[i];\n    }\n  }\n  return 0;\n}\n```\n\n    ", "Answer": "\r\nYou are correct, this is an O(n^2) operation because the number of operations you do is proportionate to the square of the number of data elements in your input array.\n\nThe fact that you have a \"bailout\" condition if the number of matches exceeds half of the length of the array also does not change the cardinality of the operation, it's still an N-squared operation since that is the limiting behavior of the function.\n\nOne way you can verify this is through Monte Carlo simulation:  \n\n\nStart with N=2, increase it up to N= say 100\nAdd a second counter inside your inner loop to let you know how many times you hit the inner code block\nCreate a randomized array of length N having only binary (1/0) values\nRun your function and keep track of the number of times you hit the inner loop\nFor each choice of N, run the simulation say 100 times and average your results for each choice of N\nPlot the average iteration count versus N on a log-log scale.  \nA best-fit line having a slope of 2 on the log-log graph indicates that the operation is O(N^2).  A slope of 1 would mean that it's O(N).  A slope of 3 would indicate O(N^3).\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Want to make sure that the Big O notation i got is correct [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and  cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened,  visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI have written this code but I have to find the big O notation. I came up with O(n2) but I am not sure if it is correct. Please somebody help. Thanks\n\n```\nint n = array.length;\n\nfor(int i=0;i<array.length;i++){\n  int c = 1;\n  for(int j=i+1;j<array.length;j++)\n    if (array[i]==array[j])\n      c=c+1;\n    if (c>(array.length/2)){\n      return array[i];\n    }\n  }\n  return 0;\n}\n```\n\n    ", "Answer": "\r\nYou are correct, this is an O(n^2) operation because the number of operations you do is proportionate to the square of the number of data elements in your input array.\n\nThe fact that you have a \"bailout\" condition if the number of matches exceeds half of the length of the array also does not change the cardinality of the operation, it's still an N-squared operation since that is the limiting behavior of the function.\n\nOne way you can verify this is through Monte Carlo simulation:  \n\n\nStart with N=2, increase it up to N= say 100\nAdd a second counter inside your inner loop to let you know how many times you hit the inner code block\nCreate a randomized array of length N having only binary (1/0) values\nRun your function and keep track of the number of times you hit the inner loop\nFor each choice of N, run the simulation say 100 times and average your results for each choice of N\nPlot the average iteration count versus N on a log-log scale.  \nA best-fit line having a slope of 2 on the log-log graph indicates that the operation is O(N^2).  A slope of 1 would mean that it's O(N).  A slope of 3 would indicate O(N^3).\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "About big O notation definition\r\n                \r\nI came across this sentence in a book about algorithms:\n\n\n  O -notation expresses the upper bound of a function within a constant factor\n\n\nWhat does it mean?\n    ", "Answer": "\r\ng(n) another function taking n as a parameter. e.g. g(n) = n; g(n)=nlogn etc. \n\n```\nf(n) = O(g(n))\n```\n\n\nthen there exists constants c and k such that for all n >= k, f(n) <= c*g(n).\n\nIt means, that on a real line, there exists a number k for which there exists a constant c that for each n >= k, f(n) <= c*g(n).\n\nLess formal (less true): f won't grow faster than c times g.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the n in big-O notation?\r\n                \r\nThe question is rather simple, but I just can't find a good enough answer. On the most upvoted SO question regarding the big-O notation, it says that:\n\n\n  For example, sorting algorithms are typically compared based on comparison operations (comparing two nodes to determine their relative ordering).\n\n\nNow let's consider the simple bubble sort algorithm:\n\n```\nfor (int i = arr.length - 1; i > 0; i--) {\n    for (int j = 0; j < i; j++) {\n        if (arr[j] > arr[j+1]) {\n            switchPlaces(...)\n        }\n    }\n}\n```\n\n\nI know that worst case is ```\nO(n²)```\n and best case is ```\nO(n)```\n, but what is ```\nn```\n exactly?  If we attempt to sort an already sorted algorithm (best case), we would end up doing nothing, so why is it still ```\nO(n)```\n? We are looping through 2 for-loops still, so if anything it should be ```\nO(n²)```\n. ```\nn```\n can't be the number of comparison operations, because we still compare all the elements, right?\n    ", "Answer": "\r\nWhen analyzing the Big-O performance of sorting algorithms, ```\nn```\n typically represents the number of elements that you're sorting.\n\nSo, for example, if you're sorting ```\nn```\n items with Bubble Sort, the runtime performance in the worst case will be on the order of O(n2) operations. This is why Bubble Sort is considered to be an extremely poor sorting algorithm, because it doesn't scale well with increasing numbers of elements to sort. As the number of elements to sort increases linearly, the worst case runtime increases quadratically.\n\nHere is an example graph demonstrating how various algorithms scale in terms of worst-case runtime as the problem size N increases. The dark-blue line represents an algorithm that scales linearly, while the magenta/purple line represents a quadratic algorithm.\n\nNotice that for sufficiently large N, the quadratic algorithm eventually takes longer than the linear algorithm to solve the problem.\n\n\n\nGraph taken from http://science.slc.edu/~jmarshall/courses/2002/spring/cs50/BigO/.\n\nSee Also\n\n\nThe formal definition of Big-O.\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for below code snippet\r\n                \r\nI have started studying Data Structures and Algorithms just a few days ago and still trying to grasp the concepts. I was learning about Big-O notations. I understand what O(1)-Constant Time Complexity is and have question.   \n\n```\nvoid Method1(int n) {\n    int a = 10;\n    int b = 20;        \n    int x = a + n;\n    int y = b * n;\n    Console.Writeline(\"{0}{1}\", x, y);\n }\n```\n\n\nThe complexity of  the above code is O(1) for a very large value of n. We are using value of N rather than doing processing N. Will the below method still have same complexity where n and m are very large numbers as inputs.\n\n```\nvoid Method1(int n, int m) {\n        int a = 10;\n        int b = 20;            \n        int x = a + n;\n        int y = b * m;\n        Console.Writeline(\"{0}{1}\", x, y);\n     }\n```\n\n    ", "Answer": "\r\nThis will have the same complexity with respect to the inputs, as you're doing the same operations.  If you simply call your second method with the same value twice, it becomes identical to the first method.\n\nNote, however, that certain implementations will not be O(1) for very large inputs.  That's because \"large integer\" is a data type that expands with the magnitude of the input; those arithmetic operations are O(log N).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Are these Big-O notations correct for simple loop functions (Python)?\r\n                \r\nSuppose we have two functions below:\n```\ndef is_prime(x):\n    \"\"\"Take an integer greater than 1 and check if it is a prime number.\"\"\"\n    for i in range(2, int(x**0.5) + 1):\n        if x % i == 0:\n            return False\n    return True\n\ndef multiply(a, b):\n    \"\"\"Take two integers and compute their product.\"\"\"\n    res = 0\n    for i in range(1, b + 1):\n        res += a\n    return res\n```\n\nIs it correct to say that the Big-O notation for ```\nis_prime```\n is ```\nO(c)```\n and ```\nmultiply```\n is ```\nO(n)```\n?\nI'm a little confused because I thought that loops are at least ```\nO(n)```\n in terms of complexity, but with the ```\nis_prime```\n function, it only takes one input and calculates one result, which I wouldn't think varies with its size? Clarification appreciated to better understand Big-O notation on simple functions.\n    ", "Answer": "\r\n\nIs it correct to say that the Big-O notation for is_prime is O(c) and\nmultiply is O(n)?\n\nNo.\nLet's take the first algorithm.\n```\nfor i in range(2, int(x**0.5) + 1):\n```\n\nis executed for ```\nsqrt(x) - 1```\n times because for loop starts with ```\n2```\n. So complexity is ```\nO(sqrt(x))```\n.\nLet's take the second algorithm.\n```\nfor i in range(1, b + 1):\n```\n\nis executed for ```\nb```\n times. So complexity is ```\nO(b)```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "ratio of running times when N doubled in big O notation\r\n                \r\nI learned that,using Big O notation\n\n```\nO(f(n)) + O(g(n)) -> O(max(f(n),g(n))\n\nO( f(n) )* O( g(n)) -> O( f(n) g(n))\n```\n\n\nbut now, I have this equation for running time T for input size N\n\n```\nT(N) = O(N^2)  // O of N square\n```\n\n\nI need to find the ratio ```\nT(2N) / T(N)```\n\n\nI tried this\n\n```\nT(2N) / T(N) --> O((2N)^2) /O( N^2)  --> 4\n```\n\n\nIs this correct? Or is the above division invalid?\n    ", "Answer": "\r\nI would also say this is incorrect. My intuition was that if ```\nT(N)```\n is ```\nO(N^2)```\n, then ```\nT(2N)/T(N)```\n is ```\nO(1)```\n, consistent with your suggestion that ```\nT(2N)/T(N) = 4```\n. I think however that the intuition is wrong.\n\nConsider a counter-example.\n\nLet ```\nT(N)```\n be 1 if ```\nN```\n is odd, and ```\nN^2```\n if ```\nN```\n is even, as below.\n\n\n\nThis is clearly ```\nO(N^2)```\n, because we can choose a constant ```\np=1```\n, such that ```\nT(N) ≤ pN^2```\n for sufficiently large ```\nN```\n.\n\nWhat is ```\nT(2N)```\n? This is ```\n(2N)^2 = 4N^2```\n, as below, because ```\n2N```\n is always even.\n\n\n\nHence ```\nT(2N)/T(N)```\n is ```\n4N^2/1 = 4N^2```\n when ```\nN```\n is odd, and ```\n4N^2/N^2=4```\n when ```\nN```\n is even, as below.\n\n\n\nClearly ```\nT(2N)/T(N)```\n is not 4. It is, however, ```\nO(N^2)```\n, because we can choose a constant ```\nq=4```\n such that ```\nT(2N)/T(N) ≤ qN^2```\n for sufficiently large ```\nN```\n.\n\nR code for the plots below\n\n```\nx=1:50\nt1=ifelse(x%%2, 1, x^2)\nplot(t1~x,type=\"l\")\n\nx2=2*x\nt2=ifelse(x2%%2, 1, x2^2)\nplot(t2~x,type=\"l\")\n\nratio=t2/t1\nplot(ratio~x,type=\"l\")\n```\n\n\nThis problem is an interesting one and strikes me as belonging in the realm of pure mathematics, i.e. limits of sequences and the like. I am not trained in pure mathematics and I would be nervous of claiming that ```\nT(2N)/T(N)```\n is always ```\nO(N^2)```\n, as it might be possible to invent some rather tortuous counter-examples. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Complexity description valid form using Big O notation\r\n                \r\nAccording to wiki we should use Big O notation in the following manner:\n\n```\nf(n) = O(g(x))```\n\n\nwhere ```\n=```\n is read not as \"equals\" but \"is\" instead.\n\nSo, it means that if algorithm has complexity for example ```\nn^2 + 2n + 5```\n we should note it as:\n\n```\nn^2 + 2n + 5 = O(n^2)```\n\n\nBut in some articles I saw that people note complexity as:\n\n```\nO(n^2 + 2n + 5) = O(n^2)```\n instead \n\nSo is the latter expression is valid form or we can not note it in that way?\n    ", "Answer": "\r\nActually, O(g(x)) is a set and it should be written with set notation, f(x) \\in O(g(x)).\n\nAuthors, usually mention this and say that for simplicity we will use = for this. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to find constant c in big O notation\r\n                \r\nI need to prove that 5n^2-6n is O(n^2). How to prove? I don't know big O notation much!\n\nAs far as I know,In order to prove, we need to find the value of constant C and n0. \nI searched for the answer and I found a solution in internet where they found c=5 where n0>=1. How it comes??? I couldn't figure it out.\nWhenever I try, I get c>= (-1)\n    ", "Answer": "\r\nYou need cn^2 to overtake 5n^2 - 6n after some n. Well, you're in luck, since 5n^2 - 6n > 5n^2 for all positive n, so it's enough for cn^2 to overtake 5n^2 ... which it does, trivially, with c = 5 (or c = 6 if you want a strong inequality), for all integral n >= 1.\n\nMore generally, for a ploynomial function with degree d, the coefficient of n^d is the threshold value. Anything above it will work as a choice for c. Exercise: Prove this!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Which of these two functions is most optimal based on Big O notation\r\n                \r\nI was curious between these two functions, which is most optimal and what Big O notation category would you assign to each and why?\nFunction 1\n```\ndef __getitem__(self, search_key):\n    for key, value in self.items():\n        if key == search_key:\n            return value\n    raise KeyError('key {} not found'.format(search_key))\n```\n\nFunction 2\n```\ndef __getitem__(self, key):\n    if key in self.keys():\n        return self.values()[self.keys().index(key)]\n    else:\n        raise KeyError('key {} not found'.format(key))\n```\n\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Role of lower order terms in big O notation\r\n                \r\nIn big O notation, we always say that we should ignore constant factors for most cases. That is, rather than writing,\n\n3n^2-100n+6\n\nwe are almost always satisfied with\n\nn^2\n\nsince that term is the fastest growing term in the equation.\n\nBut I found many algorithm courses starts comparing functions with many terms\n\n2n^2+120n+5 = big O of n^2\n\nthen finding c and n0 for those long functions, before recommending to ignore low order terms in the end.\n\nMy question is what would I get from trying to understand and annalising these kinds of functions with many terms? Before this month I am comfortable with understanding what O(1), O(n), O(LOG(n)), O(N^3) mean. But am I missing some important concepts if I just rely on this typically used functions? What will I miss if I skipped analysing those long functions?\n    ", "Answer": "\r\nLet's first of all describe what we mean when we say that ```\nf(n) is in O(g(n))```\n:\n\n\n  ... we can say that f(n) is O(g(n)) if we can find a constant c such\n  that f(n) is less than c·g(n) or all n larger than n0, i.e., for all\n  n>n0.\n\n\nIn equation for: we need to find one set of constants (c, n0) that fulfils\n\n```\nf(n) < c · g(n), for all n > n0,                        (+)\n```\n\n\nNow, the result that ```\nf(n) is in O(g(n))```\n is sometimes presented in difference forms, e.g. as ```\nf(n) = O(g(n))```\n or ```\nf(n) ∈ O(g(n))```\n, but the statement is the same. Hence, from your question, the statement ```\n2n^2+120n+5 = big O of n^2```\n is just:\n\n```\nf(n) = 2n^2 + 120n + 5\n\na result after some analysis: f(n) is in O(g(n)), where\n\n    g(n) = n^2\n```\n\n\nOk, with this out of the way, we look at the constant term in the functions we want to analyse asymptotically, and let's look at it educationally, using however, your example.\n\n\n\nAs the result of any big-O analysis is the asymptotic behaviour of a function, in all but some very unusual cases, the constant term has no effect whatsoever on this behaviour. The constant factor can, however, affect how to choose the constant pair (c, n0) used to show that f(n) is in O(g(n)) for some functions f(n) and g(n), i.e., the none-unique constant pair (c, n0) used to show that (+) holds. We can say that the constant term will have no effect of our result of the analysis, but it can affect our derivation of this result.\n\nLets look at your function as well as another related function\n\n```\nf(n) = 2n^2 + 120n + 5                                        (x)\nh(n) = 2n^2 + 120n + 22500                                    (xx)\n```\n\n\nUsing a similar approach as in this thread, for ```\nf(n)```\n, we can show:\n\n```\nlinear term: \n\n    120n < n^2 for n > 120 (verify: 120n = n^2 at n = 120)    (i)\n\nconstant term:\n\n    5 < n^2 for e.g. n > 3 (verify: 3^2 = 9 > 5)              (ii)\n```\n\n\nThis means that if we replace both ```\n120n```\n as well as ```\n5```\n in (x) by ```\nn^2```\n we can state the following inequality result:\n\n```\nGiven that n > 120, we have:\n2n^2 + n^2 + n^2 = 4n^2 > {by (ii)} > 2n^2 + 120n + 5 = f(n)  (iii)\n```\n\n\nFrom (iii), we can choose ```\n(c, n0) = (4, 120)```\n, and (iii) then shows that these constants fulfil (+) for ```\nf(n)```\n with ```\ng(n) = n^2```\n, and hence\n\n```\nresult: f(n) is in O(n^2)\n```\n\n\nNow, for for ```\nh(n)```\n, we analogously have:\n\n```\nlinear term (same as for f(n)) \n\n    120n < n^2 for n > 120 (verify: 120n = n^2 at n = 120)    (I)\n\nconstant term:\n\n    22500 < n^2 for e.g. n > 150 (verify: 150^2 = 22500)      (II)\n```\n\n\nIn this case, we replace ```\n120n```\n as well as ```\n22500```\n in (xx) by ```\nn^2```\n, but we need a larger less than constraint on n for these to hold, namely ```\nn > 150```\n. Hence, we the following holds:\n\n```\nGiven that n > 150, we have:\n2n^2 + n^2 + n^2 = 4n^2 > {by (ii)} > 2n^2 + 120n + 5 = h(n)  (III)\n```\n\n\nIn same way as for ```\nf(n)```\n, we can, here, choose ```\n(c, n0) = (4, 150)```\n, and (III) then shows that these constants fulfil (+) for ```\nh(n)```\n, with ```\ng(n) = n^2```\n, and hence\n\n```\nresult: h(n) is in O(n^2)\n```\n\n\nHence, we have the same result for both functions f(n) and h(n), but we had to use different constants (c,n0) to show these (i.e., somewhat different derivation). Note finally that:\n\n\nNaturally the constants (c,n0) = (4,150) (used for h(n) analysis) are also valid to show that f(n) is in O(n^2), i.e., that (+) holds for f(n) with g(n)=n^2.\nHowever, not the reverse: (c,n0) = (4,120) cannot be used to show that (+) holds for h(n) (with g(n)=n^2).\n\n\nThe core of this discussion is that:\n\n\nAs long as you look at sufficiently large values of ```\nn```\n, you will be able to describe the constant terms in relations as ```\nconstant < dominantTerm(n)```\n, where, in our example, we look at the relation with regard to dominant term ```\nn^2```\n. \nThe asymptotic behaviour of a function will not (in all but some very unusual cases) depend on the constant terms, so we might as well skip looking at them at all. However, for a rigorous proof of the asymptotic behaviour of some function, we need to take into account also the constant terms. \n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of method with While loop containing an if statement\r\n                \r\nI am a little confused as to what the big O notation for a method like such would be\n\n```\npublic void printOut (SinglyLinkedList<Double> myLinked){\nIterator<Double> itr = myLinked.iterator();\nwhile (itr.hasNext()){\n    double d = itr.next(); // unboxing\n    if (d > 5.0)\n    System.out.println (d);\n} // while\n} // method printOut\n```\n\n\nIf there was no if statement I know it'd be 'n' but since the println wouldn't be executing every time it iterates, how do I come up with the notation?\n    ", "Answer": "\r\nThe answer is still ```\nO(n)```\n\n\nIf the loop runs through the list, it is ```\nO(n)```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation and polynomials?\r\n                \r\nSo I have this problem to do and I am not really sure where to start:\n\nUsing the definition of Big-O, prove the following:\n\n\nT(n) = 2n + 3 ∈ O(n)\nT(n) = 5n + 1 ∈ O(n2)\nT(n) = 4n2 + 2n + 3 ∈ O(n2)\n\n\nif anyone can point me in the right direction (you don't necessarily have to give me the exact answers), I would greatly appreciate it.\n    ", "Answer": "\r\nYou can use the same trick to solve all of these problems.  As a hint, use the fact that\n\n\n  If a ≤ b, then for any n ≥ 1, na ≤ nb.\n\n\nAs an example, here's how you could approach the first of these: If n ≥ 1, then 2n + 3 ≤ 2n + 3n = 5n.  Therefore, if you take n0 = 1 and c = 5, you have that for any n ≥ n0 that 2n + 3 ≤ 5n.  Therefore, 2n + 3 = O(n).\n\nTry using a similar approach to solve the other problems.  For the second problem, you might want to use it twice - once to upper-bound 5n + 1 with some linear function, and once more to upper bound that linear function with some quadratic function.\n\nHope this helps!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Confused on Big-O Notation\r\n                \r\nAlright I have a quick question for all of the programmers that are preusing for an easy question.\n\nFor my Computer Science II class we are going over Big-Oh notation and I've got most of it down, but I am still confused on some of the semantics of the examples.\n\nEverything here is written in Java.\n\nMy professor gave us these examples in class, but my luck, I didn't write down the answers.\n\na)\n\n```\nint count = 0; \n    for (int i = 1; i <= n; i++) \n        for (int j = n; j > 1; j-­--­-) \n            for (int k = 1; k < n; k = k + 2) \n                count++;\n```\n\n\nb)\n\n```\nint count = 0; \nfor (int i = 1; i <= n; i++) \n    for (int j = n; j > 1; j-­--­-) \n        for (int k = 1; k < 1000; k = k + 2) \n            count++;\n```\n\n\nc)\n\n```\nint count = 0; \n    for (int i = 1; i <= 1000000; i++) \n        for (int j = i; j > 500; j-­--­-) \n            for (int k = 1; k < 10500; k = k + 2) \n                count++;\n```\n\n\nd)\n\n```\nint count = 0; \nint j = 1; \nfor (int i = 1; i < n; i++) { \n        while (j < n) { \n            j++;\n            count++; \n        } \n        j = 1; \n    }\n```\n\n\ne)\n\n```\nint count = 0; \nint i = n; \nwhile (i > 1) \n{ \n    count++; i = i / 2; \n}\n```\n\n\nAlright so here are my answers/thought processes:\n\na) N * N * (N / 2) = N^3/2, All simplifies to a O(N^3) notation\n\nb) N * N * 500, All simplifies to O(N^2)\n\nc) This is the one that I am majorly confused on you have three for loops, but all iterate the exact number of times. My guess here is O(1), but I have no idea...\n\nd) N * N = N^2, So O(N^2)\n\ne) Divides by half each time, so log(n) = O(log(n)) [both base 2]\n\nSo can anyone check through my thought process and see if I am missing anything? Thank you so much in advance!\n    ", "Answer": "\r\nYeah (C) is O(1) because it's all constants. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for fib recursion\r\n                \r\nWhat is the Big-O run-time of the following function?  Explain.\n\n```\nstatic int fib(int n){\n  if (n <= 2)\n     return 1;\n  else \n     return fib(n-1) + fib(n-2)\n   }\n```\n\n\nAlso how would you re-write the fib(int n) function with a faster Big-O run-time iteratively?\nwould this be the best way with O(n):\n\n```\n public static int fibonacci (int n){\n int previous = -1;\n int result = 1;\n for (int i = 0; i <= n; ++i)\n {\n int sum = result + previous;\n previous = result;\n result = sum;\n }\n return result;\n}\n}\n```\n\n    ", "Answer": "\r\nProof\n\nYou model the time function to calculate ```\nFib(n)```\n as sum of time to calculate ```\nFib(n-1)```\n plus the time to calculate ```\nFib(n-2)```\n plus the time to add them together (```\nO(1)```\n).\n\n```\nT(n<=1) = O(1)```\n\n\n```\nT(n) = T(n-1) + T(n-2) + O(1)```\n\n\nYou solve this recurrence relation (using generating functions, for instance) and you'll end up with the answer.\n\nAlternatively, you can draw the recursion tree, which will have depth ```\nn```\n and intuitively figure out that this function is asymptotically ```\nO(2```\n```\nn```\n```\n)```\n. You can then prove your conjecture by induction.\n\nBase: ```\nn = 1```\n is obvious\n\nAssume ```\nT(n-1) = O(2```\n```\nn-1```\n```\n)```\n, therefore \n\n```\nT(n) = T(n-1) + T(n-2) + O(1)```\n which is equal to\n\n```\nT(n) = O(2```\n```\nn-1```\n```\n) + O(2```\n```\nn-2```\n```\n) + O(1) = O(2```\n```\nn```\n```\n)```\n\n\nIterative version\n\nNote that even this implementation is only suitable for small values of n, since the Fibonacci function grows exponentially and 32-bit signed Java integers can only hold the first 46 Fibonacci numbers\n\n```\nint prev1=0, prev2=1;\nfor(int i=0; i<n; i++) {\n  int savePrev1 = prev1;\n  prev1 = prev2;\n  prev2 = savePrev1 + prev2;\n}\nreturn prev1;\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How can I know big O notation from EXPLAIN (ANALYZE) in PostgreSQL\r\n                \r\nthis is an EXPLAIN (ANALYZE) from PostgreSQL\n\n```\nGroupAggregate  (cost=245.73..292.69 rows=1174 width=46) (actual time=38.850..39.454 rows=148 loops=1)\n  Group Key: location_id, (date_trunc('day'::text, created_at)), payment_method\n  ->  Sort  (cost=245.73..248.66 rows=1174 width=42) (actual time=38.829..38.916 rows=1100 loops=1)\n        Sort Key: location_id, (date_trunc('day'::text, created_at)), payment_method\n        Sort Method: quicksort  Memory: 165kB\n        ->  Seq Scan on payment p  (cost=0.00..185.87 rows=1174 width=42) (actual time=1.905..6.849 rows=1100 loops=1)\n              Filter: ((location_id IS NOT NULL) AND (created_at <= '2019-04-23 00:00:00+00'::timestamp with time zone) AND (created_at >= '2000-04-23 00:00:00+00'::timestamp with time zone) AND ((entity_type)::text = 'RESERVATION'::text) AND ((payment_status)::text = ANY ('{SUCCESS,CAPTURE,REFUNDED}'::text[])))\n              Rows Removed by Filter: 2238\nPlanning Time: 1.895 ms\nExecution Time: 39.727 ms\n```\n\n\nThis is my query code\n\n```\nselect  p.location_id,\n        date_trunc('day', p.created_at) as date,\n        p.payment_method,\n        count(p.id), sum(p.sub_amount)+sum(p.mdr_fee)+sum(p.convenience_fee) as GrossSales,\n        sum(p.sub_amount) netSales\nfrom payment p\nwhere p.location_id is not null\n  and p.entity_type = 'RESERVATION'\n  and p.payment_status in ('SUCCESS','CAPTURE','REFUNDED')\n  and created_at <= '2019-04-23'\n  and created_at >= '2000-04-23'\ngroup by p.location_id, date, p.payment_method\norder by p.location_id asc, date asc, p.payment_method;\n```\n\n\nI don't know how to see the big O notation EXPLAIN (ANALYZE).\n    ", "Answer": "\r\nYou won't see the “big O notation” in ```\nEXPLAIN (ANALYZE)```\n.\n\nIt is not something that you can see in a single data point, it is a description how certain parameters of the input data (e.g., the size of a table) will affect the run time of a certain algorithm. This can be measured by using the same algorithm for different input data, but usually it is determined by analyzing the algorithm.\n\nFor example, the sequential scan has an expense of O(n), where n is the size of the table. That means that it will take roughly twice as long for a table that is twice as big. The cost of a sort is O(n⋅ln(n)), where “ln” is a logarithm, which means that it will be notably more than twice as expensive for a set twice the size.\n\nMeasurements (like ```\nEXPLAIN (ANALYZE)```\n) are not a great way to determine such numbers, because there are always different factors coming in (like caching) that will obscure the numbers. Also, you don't want a guess that the expense might be O(n), you want a mathematical proof.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Run-time Complexity for two algorithms (Big O notation calculation)\r\n                \r\nwhat is the Big O notation for that two algorithms:\n\n```\ndef foo1(n):\n    if n > 1:\n        for i in range(int(n)):\n            foo1(1)\n        foo1(n / 2)\n\ndef foo2(lst1, lst2):\n    i = 1\n    while i < max(len(lst1), len(lst2)):\n            j = 1\n            while j < min(len(lst1), len(lst2)):\n                j *= 2\n            i *= 2\n```\n\n\nI thought that foo1 run time complexity is O(n) because in that case if I see the for loop I can do that:             \n\n\n  T(n) = O(n) + O(n/2) <= c*O(n) (c is const) for all n.\n\n\nis that right ? \n\nand I cant calculate the run time of foo2 can some one help me to know to do that.\n\nthanks...    \n    ", "Answer": "\r\n\nThe number of operations ```\nT(n)```\n is equal to ```\nT(n/2) + n```\n. Applying the Master theorem we get ```\nT(n) = O(n)```\n. In simple terms there are ```\nn + n/2 + n/4 + ... + 1```\n operations that are less than ```\n2*n```\n and is ```\nO(n)```\n.\nThe inner loop does not depend on the outer loop, so we can treat them independently. ```\nT(n) = O(log(maxlen) * log(minlen))```\n.\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation Homework--Code Fragment Algorithm Analysis? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                This question is unlikely to help any future visitors; it is only relevant to a small geographic area, a specific moment in time,  or an extraordinarily narrow situation that is not generally applicable to the worldwide audience of the internet. For help making  this question more broadly applicable, visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nFor homework, I was given the following 8 code fragments to analyze and give a Big-Oh notation for the running time.  Can anybody please tell me if I'm on the right track?  \n\n```\n//Fragment 1\nfor(int i = 0; i < n; i++)\n    sum++;\n```\n\n\nI'm thinking O(N) for fragment 1\n\n```\n//Fragment 2\nfor(int i = 0; i < n; i+=2)\n    sum++;\n```\n\n\nO(N) for fragment 2 as well\n\n```\n//Fragment 3\nfor(int i = 0; i < n; i++)\n    for( int j = 0; j < n; j++)\n        sum++;\n```\n\n\nO(N^2) for fragment 3\n\n```\n//Fragment 4\nfor(int i = 0; i < n; i+=2)\n    sum++;\nfor(int j = 0; j < n; j++)\n    sum++;\n```\n\n\nO(N) for fragment 4\n\n```\n//Fragment 5\nfor(int i = 0; i < n; i++)\n    for( int j = 0; j < n * n; j++)\n        sum++;\n```\n\n\nO(N^2) for fragment 5 but the n * n is throwing me off a bit so I'm not quite sure\n\n```\n//Fragment 6\nfor(int i = 0; i < n; i++)\n    for( int j = 0; j < i; j++)\n        sum++;\n```\n\n\nO(N^2) for fragment 6 as well\n\n```\n//Fragment 7\nfor(int i = 0; i < n; i++)\n    for( int j = 0; j < n * n; j++)\n        for(int k = 0; k < j; k++)\n            sum++;\n```\n\n\nO(N^3) for fragment 7 but once again the n * n is throwing me off\n\n```\n//Fragment 8\nfor(int i = 1; i < n; i = i * 2)\n    sum++;\n```\n\n\nO(N) for fragment 8\n    ", "Answer": "\r\nI think fragment 5 is O(n^3), and similarly fragment 7 is O(n^5)*. It also looks like O(log(n)) for fragment 8.\n\nFor the n * n problems, you have to execute the body of the loop n * n times, so it would be O(n^2), then you compound that with the order of the other code. Fragment 8 actually doubles the counter instead of incrementing it, so the larger the problem, the less additional work you have to do, so it's O(log(n))\n\n*edit: Fragment 7 is O(n^5), not O(n^4) as I previously thought. This is because both j and k go from 1 to n * n. Sorry I didn't catch this earlier.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation (Complexity)\r\n                \r\nWhat is the Big O of this loop?\n-> i understand that the loop itself is going to execute n times. But the task inside of the loop also executes n times right? So would that make this a O(n^2) or do i not combine them and its a simple O(n)? I personally think it is just O(n) because that is how many times the loop is executing but i would just like some clarification as to why or why not it would be? Any explanations would be helpful as i prepare for my midterm.\n\n```\nfor(int i = 0; i < a.length;i++){\n        a[i] = b[i]\n    }\n```\n\n    ", "Answer": "\r\nThink about how many times the code inside the loop executes. Each ```\na[i] = b[i]```\n happens ```\na.length```\n times, which we will call ```\nn```\n. So this is ```\nO(n)```\n. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to express the performance of loop & inner loop in big o notation? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is off-topic. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\r\n                \r\n                    \r\n                        Closed 10 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nFor below loop and inner loop to express the performance in big o notation is : \n\nO(N squared) as its performance is proportional to the square of the size of the input data set.\n\n```\nvar counter = 0\nvar counterval = 0;\nfor ((key, value) <- m2.par){\n        for ((key2, value2) <- m2.par){\n          counter = counter + 1;\n           println(counter)\n           }\n        println(counterval)\n      } \n```\n\n\nIs this correct ?\n    ", "Answer": "\r\nYes, if you consider the size of ```\nm2```\n to be the input size and that increasing ```\ncounter```\n and printing it are both ```\nO(1)```\n (which is a very reasonable assumption).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is big-O notation relevant for concurrent world? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is opinion-based. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.\r\n                \r\n                    \r\n                        Closed 9 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nIn most popular languages like C/C++/C#/Erlang/Java we have threads/processes; there is GPGPU computation market growing. If algorithm requires N data independent steps we get not the same performance as if algorithm would require all steps follow one another. So I wonder if  big-O notation makes sense in concurrent world? And if it does not what is relevant to analyze algorithm performance?\n\nYou can have N or more processors in distributed environment (GPGPU / cluster / FPGA of future where you can get as many cores as you need - concurrent world, not limited to the number of parallel cores)\n    ", "Answer": "\r\nBig-O notation is still relevant.\n\nYou have a constant number of processors (by assumption), thus only a constant number of things can happen concurrently, thus you can only speed up an algorithm by a constant factor, and big-O ignores constant factors.\n\nSo whether you look at the total number of steps, or only consider the number of steps taken by the processor processing the most steps, we end up with exactly the same time complexity, and this still ends up giving us a decent idea of the rate of growth of the time taken in relation to the input size.\n\n\n  ... future where you can get as much cores as you need - concurrent world, not limited to the number of parallel cores.\n\n\nIf we even get to the stage where you can run an algorithm with exponential running time on very large input in seconds, then yes, big-O notation, and the study of algorithms, will probably become much less relevant.\n\nBut considering, for example, that for an ```\nO(n!)```\n algorithm, with ```\nn = 1000```\n (which is pretty small to be honest), it will require in the region of ```\n4x10^2567```\n steps, which is about ```\n4x10^2480```\n times more than the mere ```\n10^87```\n estimated number of atoms in the entire observable universe. In short, big-O notation is unlikely to ever become irrelevant.\n\n\n\nEven on the assumption of an effectively unlimited number of processors, we can still use big-O notation to indicate the steps taken by the processor processing the most steps (which should indicate the running time).\n\nWe could also use it to indicate the number of processors used, if we'd like.\n\nThe bottom line is that big-O notation is to show the rate of growth of a function - a function which could represent just about anything. Just because we typically use it to indicate the number of arithmetic computations, steps, comparisons or similar doesn't mean those are the only things we can use it for.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation calculating constant\r\n                \r\nI have created a jaccard function where i know big O is O(n) how would i estimate the constant for my implementation and computer? \n\n```\ndef jaccard(dict1,dict2):\n   intersection={}\n   for item in dict1.keys():\n       if item in dict2.keys():\n          intersection[item]=min(dict1[item],dict2[item])\n\n   intersectiontot=maketotal(intersection)\n   union = maketotal(dict1)+maketotal(dict2)-intersectiontot\n   return intersectiontot/union\n```\n\n    ", "Answer": "\r\nTo get a good estimate of the constant all you can do is do lots of runs (trying to keep the rest of your system quiet), with lots of different sized data sets, plot all of them on a scatter graph, and see if the points fit somewhere close to a straight line.\n\nThe slope of this line will give you the constant factor.\n\nLarge range of data sizes are required to ensure that you can see reasonable sized changes in execution time. Multiple runs at the same data size will help eliminate variability in measurements caused by other things on the system.\n\nI am suprised your particular function is O(n) - I would have thought it would be O(nm) where n & m are the sizes of your two dictionary (unless one dictionary is always very small compared to the other).\n\nGood luck.\n\n\n\nA far more performant solution (without the multiple dictionary look ups and the repeaeted membership tests might be something like this : \n\n```\n def jaccard(dict1,dict2):\n     sentinel = object()\n     intersection={}\n     for key, value1 in dict1.items():\n         value2 = dict2.get(key, sentinel)\n         if value2 is not sentinel:\n               intersection[item]=min(value1, value2)\n\n     intersectiontot=maketotal(intersection)\n     union = maketotal(dict1)+maketotal(dict2)-intersectiontot\n     return intersectiontot/union\n```\n\n\nThis function does a membership test in dict2 and a lookup at the same time, and removes the need for a separate lookup in dict1 after you have found the keys.\n\nDepending on what maketotal does it might be possible to improve this even further.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Figuring out the algorithmic complexity of my Program in Big-O Notation\r\n                \r\nI have created this program that will do a binary search on a order list the user enters, and outputs the desired value they want to search for in that list.  My problem is I have to find the algorithmic complexity in Big-O Notation for each part of my code then do the Time Complexity of it, however, I am not great at figuring out the Big-O notation.  If possible could you explain how I could do it, etc.  Here's my code and I have tried already to figure the algorithmic complexity  for some lines, if I did something wrong please correct me.\n\n```\n#include<iostream>\n#include<vector>\nusing namespace std;\n\n\nint binarySearch(vector<double> uservector, int, int);\n\n\nint main()\n{\nint size;  // O(1)\nint i;  //O(1)\nint desirednum;  // O(1)\n\ncout << \"How many values do you want to enter: \";  // O(1)\ncin >> size;  // O(1)\nvector<double> uservector(size);\n\nfor (i = 0; i < size; i++)\n{\n    cout << \"Enter a value: \";\n    cin >> uservector[i];\n}\n\ncout << \"What value are you looking for: \";  // O(1)\ncin >> desirednum;  // O(1)\n\nint location = binarySearch(uservector, size, desirednum);  \n\nif( location > -1)\n    cout << \"The value \" << desirednum << \" was found at index \" << location   << endl;  // O(1)\nelse \n    cout << \"The value was not found in the list. \\n\";  // O(1)\n\nsystem(\"PAUSE\");\nreturn 0;\n}\n\nint binarySearch(vector<double> uservector, int size, int value)\n{\nint low, mid, high;\n\nlow = 0;\nhigh = size - 1;\nwhile(low <= high)\n{\n    mid = ((low + high) / 2);\n    if(value == uservector[mid])\n    {\n        return\n            mid;\n    }\n    else if(value > uservector[mid])\n        low = mid + 1;\n    else\n        high = mid - 1;\n}\n    return -1;\n}\n```\n\n    ", "Answer": "\r\nYou've got it right, to some degree. To some other, you don't.\n\n```\nint main()\n{\nint size;  // O(1)\nint i;  //O(1)\nint desirednum;  // O(1)\n```\n\n\nThese aren't instructions, they are declarations. They don't take run time at all.\n\n```\ncout << \"How many values do you want to enter: \";  // O(1)\n```\n\n\nYou shouldn't be counting things that happen prior to the start of your algorithm at all. \n\n```\ncin >> size;  // O(1)\nvector<double> uservector(size);\n\nfor (i = 0; i < size; i++)\n{\n    cout << \"Enter a value: \";\n    cin >> uservector[i];\n}\n\ncout << \"What value are you looking for: \";  // O(1)\ncin >> desirednum;  // O(1)\n```\n\n\nYou shouldn't be counting things that happen prior to the start of your algorithm at all. \n\n```\nint location = binarySearch(uservector, size, desirednum);  \n\nif( location > -1)\n    cout << \"The value \" << desirednum << \" was found at index \" << location   << endl;  // O(1)\nelse \n    cout << \"The value was not found in the list. \\n\";  // O(1)\n\nsystem(\"PAUSE\");\nreturn 0;\n}\n```\n\n\nOk, so here starts the part you actually want to have a complexity for.\n\n```\nint binarySearch(vector<double> uservector, int size, int value)\n{\nint low, mid, high;\n\nlow = 0;\nhigh = size - 1;\nwhile(low <= high)\n{\n    mid = ((low + high) / 2);\n    if(value == uservector[mid])\n    {\n        return\n            mid;\n    }\n    else if(value > uservector[mid])\n        low = mid + 1;\n    else\n        high = mid - 1;\n}\n    return -1;\n}\n```\n\n\nYou're halving the distance to your target each iteration, giving you a log2(N) complexity.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How do I find this function the growth rate based on the big O notation?\r\n                \r\nHow do I find this function the growth rate based on the big O notation?\n\n```\nfor(i=1; i*i<n; i=i+1)\n    for(j=1; j<=i; j=j+1)\n```\n\n    ", "Answer": "\r\nSee the following\n\nwhen ```\ni=1```\n,\ninnerloop runs 1 time\n\noutloop ```\ni=2```\n,\ninnerloop runs 2 times\n\noutloop ```\ni=3```\n,\ninnerloop runs 3 times\n\noutloop ```\ni=4```\n,\ninnerloop runs 4 times\n\n....\n\noutloop ```\ni=sqrt(n)-1```\n,\ninnerloop runs ```\nsqrt(n)-1```\n times, the loop terminates here\n\nLest assume that ```\nn```\n is a perfect square \n\n```\n==> f(n) = 1 + 2 + 3 + 4 + ... + sqrt(n) - 1 = (sqrt(n) - 1 )/2 x ( sqrt(n) - 1 + 1)\n\n==> f(n) = ( (sqrt(n)^2)/2 - sqrt(n)/2 \n\n==> O( sqrt(n)^2)/2 - sqrt(n)/2 ) = O( n - sqrt(n) )\n\n==> O( n - sqrt(n) )\n==> O( n )\n```\n\n\n\nSo it approximately  takes O(n)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the big o notation of this algorithm?\r\n                \r\n```\nfor(int i = 0; i < n; ++i)\n{\n    for(int x = i; x < n; ++x)\n    {\n        // work... \n    }\n}\n```\n\n\nWhat is the big o notation for this type of algorithm? Also, please explain to me how you came up with the solution. \n\nAlso, sorry for the vague title but I didn't know the name of this type of algorithm. \n\nHere is what I tried:\n\nIf n is:\n\n1, there will be 1 work execution. \n\n2, there will be 3 work execution. \n\n3, there will be 6 work execution. \n\n4, there will be 10 work execution. \n\n5, there will be 15 work execution. \n\nPeople in the comment say it is n^2 but the numbers I'm getting don't match the result as 5^2 is 25 and not 15\n    ", "Answer": "\r\nBig O notation is derived from calculating the time complexity. You must take into account the amount of work in which your algorithm is doing.\n\nPlease see below my answer in which I derive Big 0. This is using ```\nLateX```\n which is a nice tool to write equations.\n\nNotes \n\n\nThe giant ```\nE```\n like symbol - is called a Sigma. This is a mathematical symbol that is used in writing up algorithms to annotate a looping function. Think of it as your ```\nfor```\n - the bottom term is like your ```\ni=0```\n and the top term is like your ```\ni < n```\n.\nThe ```\n(n-1)```\n represents the work of the inner loop. - to calculate this, we break the equation into two separate Sigmas - as ```\ni```\n is more complex to derive.\nnotice how the inner loop does not run ```\nn```\n times but ```\nn-i```\n. Also, line (3) - to understand what ```\ni```\n is - we use Summations (Law 6 maybe?).\n\n\nTo get n^2 - we eliminate constants from the equation aswell as terms that do not dominate the growth of the function.\n\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What will be the time complexity of the following nested loopin Big O notation?\r\n                \r\nWas wondering what is the time complexity of the following code:\n\n```\nint i,j,n,p,s=0;\nscanf(\"%d\",&n);\np = pow(3, n);\nfor(i=0; i<p; i++)\n{\n    for(j=0; j<n; j++)\n    {\n        s+=j;\n    }\n    printf(\"%d\",i);\n}\n```\n\n\nIn my opinion, the time complexity will be n*(3^n).\nWhat will it be in Big-O notation, what do you guys think ?\n    ", "Answer": "\r\nYes it is ```\nO((3^n)*n)```\n.Outer loop iterates ```\n3^n```\n times, each time, the inner loop does ```\nO(n)```\n work.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Summing an Array and Big O Notation\r\n                \r\nHow to find an algorithm for calculating the sum value in the array??\n\nIs is Something like this? \n\n```\nAlgorithm Array Sum\nInput: nonnegative integer N, and array A[1],A[2],...,A[N]\nOutput: sum of the N integers in array A\nAlgorith Body:\nj:=1\nsum:=0\nwhile j<N\n      sum := sum + a[J]\n      j:=j+1\n  end while\nend Algorithm Array Sum\n```\n\n\nAnd how I can relate it with the running time of the algorithm by using O-Notation\n\nThis is the past year exam and I need to make revision for my exam.\n\nQuestion\nAn Array A[] holding n integer value is given\n1.Give an algorithm for calculating the sum of all the value in the array\n2.Find the simplest and best O-notation for the running time of the algorithm.\n    ", "Answer": "\r\nThe question is to find the sum of all the values so iterate through each element in the array and add each element to a temporary sum value.\n\n```\ntemp_sum = 0\nfor i in 1 ...array.length\n    temp_sum = temp_sum + array[i]\n```\n\n\nSince you need to go through all the elements in the array, this program depends linearly to the number of elements. If you have 10 elements, iterate through 10 elements, if you have a million you have no choice other than to go through all the million elements and add each of them. Thus the time complexity is Θ(n).\n\nIf you are finding the sum of all the elements and you dont know any thing about the data then you need to look at all the elements at least once. Thus n is the lowerbound. You also need not look at the element more than once. n is also the upper bound. Hence the complexity is Θ(n).\n\nHowever if you know something about the elements..say you get a sequency of n natural numbers, you can do it in constant time with n(n+1)/2. If the data you get are random then you have no choice but do the above linear time algorithm.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What does it mean to drop the constants in terms of big o notation?\r\n                \r\nI'm watching a video on Hackerrank in regards to Big O Notation and while reading materials from the Career Cup, I always hear the term 'drop the constants'.  Please let me know if I'm understanding correctly based on the algorithm below:\n\n```\n    function whyWouldIDoThis(array){\n     max = NULL\n    for each a in array {\n      max = MAX(a, max)\n    }\n    print max\n    for each a in array {\n      for each b in array {\n        print a,b\n      }\n    }\n   }\n```\n\n\nIt was explained that since these are two  separate for loops, then the first for loops is O(n) and the second for loop is O(n^2) and since its all one algorithm then it ultimately becomes O(n^2).  It makes sense that since the term 'n' is the same then they cancel out.  Is this the meaning of 'dropping the contents' in terms of Big O Notation?\n    ", "Answer": "\r\n```\nmax = NULL ----------------- c_1\nfor each a in array {\n  max = MAX(a, max) -------- c_2\n}\nprint max ------------------ c_3\nfor each a in array {\n  for each b in array {\n    print a, b ------------- c_4\n  }\n}\n```\n\n\nAnalyzing the code line-by-line, we have that the total time taken would be\n\n\n  c_1 + c_2 * n + c_3 + c_4 * n * n = c_4 n^2 + c_2 * n + c_1 + c_3\n\n\nNow we only need the dominant term, which is c_4 n^2, and the coefficient c_4 is not needed either. So we have O(n^2).\n\nIn this sense, \"dropping the constant\" is to drop the coefficient. That is, even if your code might be slightly faster than O(n^2), maybe O(n^2 / 2), it does not matter in terms of Big-Oh; it's all O(n^2).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Calculating running time of a program (Big O Notation)\r\n                \r\n```\nvoid foo(float[] array, int start, int end){\n    if((end-start) <=1) return;\n\n    int x = (end-start) / 5;\n    int y = 2*x;\n    int z = 4*x;\n\n    foo(array,start,start+y);\n    for(index = y; index < z; index++){\n        array[index]++;\n    }\n    foo(array, start+z, end);\n}\n```\n\n\nI get that for loop initializations are executed N+1 times, while the inner code is executed N times so that means the time complexity would be O((N+1)*(N)) = O(n^2). But how do I calculate N in this situation? And how do I deal with the recursive call? How can I convert this information into Big O notation?\n\nPlease do not say this is a duplicate question. I understand that there are similar questions with other examples but this example is abstract to me and learning how to solve it would help me in many other situations.\n    ", "Answer": "\r\nLet's say the input to ```\nfoo```\n is:\n\n```\nfoo(array, 0, 1000)\n```\n\n\nSo, first of all, ```\nx```\n becomes 200 which is one fifth of the whole data. Then ```\ny```\n and ```\nz```\n become the second and the fourth quintiles. Note that they are still a fraction of ```\nend - start```\n (1000), not constants.\n\nThe ```\nfor```\n goes:\n\n```\nfor(index = y; index < z; index++)\n```\n\n\nwhich is from ```\ny```\n to ```\nz```\n. ```\ny```\n was 400 (2 * 1000 / 5) and ```\nz```\n was 800 (4 * 1000 / 5). So the number of times the ```\nfor```\n loops is 800 - 400 = 400 = (4 - 2) * 1000 / 5. Perhaps the person who analyzed this for you called it ```\nN```\n, but it's quite absurd. What you do inside the loop is a simple increment which takes constant time (```\nO(1)```\n) and there is no ```\nN```\n involved there!\n\nSo let's name things ourselves. I'll call the 1000 above ```\nN```\n. So basically, the ```\nfor```\n loops 2N/5 times.\n\n\n\nHow about the recursions? The first recursion recurses on ```\nstart```\n to ```\nstart + x```\n (i.e. the first 2/5 of the data) and the second one recurses on ```\nstart + z```\n to ```\nend```\n (i.e. the last 1/5 of the data).\n\nAssuming your function takes ```\nf(N)```\n time in total to calculate whatever it is doing, it can be broken down to:\n\n```\nf(N) = f(2N/5) + 2N/5 + f(N/5)\n```\n\n\nNow all you need is to analyze this recursive function and try to understand what would be the order.\n\n\n\nWhile there are many ways to understand this, such as drawing a tree that shows how ```\nf```\n expands and with what parameters, you can also try to find the easier upper and lower limits of the function. If they are the same, you're in luck:\n\n```\n2N/5 + 2*f(N/5) < f(n) < 2*f(2N/5) + 2N/5\n```\n\n\nBy the master theorem, both limits fall in Case 3, and both limits turn out to be ```\nθ(N)```\n, so your ```\nfoo```\n function is in fact ```\nθ(N)```\n.\n\nAnother way too look at it is the following:\n\n\nThe first recursion touches the first 2N/5 of the data\nThe for loop touches the second 2N/5 of the data\nThe second recursion touches the last N/5 of the data\n\n\nAs you can see, each element of the array is touched once and only once. So the time complexity of the function is ```\nθ(N)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation with special inner loop\r\n                \r\nI would like to have some help regarding analyzing the followings Big O notation.\nCase A:\nThe inner loop only runs once in the worst scenario\n```\n    for (int i = 0; i < nums.length; i++) {\n\n       if (i==nums.length-1 && nums[i] == 3){\n          for (int j = 0; j < nums.length; j++) {\n           print();\n          }\n       }\n    }\n```\n\nThe worst case scenario is an array that finish in 3. Is this O(N) or O(N*N)?\nThe way I analyse this is the following:\n\nThe outer array runs N times= 1(n)\nThe inner loop in the worst case runs just 1 time = 1\n\nSo , n*1 = O(n) , am I ok with the analysis?\nDoes it makes sense \"n*1\" or should be \"n+1\"?\nCase B:\nThe inner loop runs always except if ```\ni```\n is the first element\n```\n   for (int i = 0; i < nums.length; i++) {\n     if (i!=0){\n        for (int j = 0; j < nums.length; j++) {\n          print();\n        }\n     }\n   }\n```\n\nThe way I analyse this is the following  :\n\nThe outer array runs N times= n\nThe inner loop runs N -1 times =n-1\n\nSo , n*(n-1) = O(n*n) , am I ok with the analysis?\nI am very confused with those cases, thanks !\n    ", "Answer": "\r\nCase A:\n\nThe outer array runs N times= 1(n)\nThe inner loop in the worst case runs just 1 time = 1\n\nI think your reasoning here is probably okay, but as it's written this can't be right. ```\nThe outer array runs N times```\n implies that you count ```\n1```\n for every iteration of the loop, but ```\nThe inner loop in the worst case runs just 1 time```\n can't be right (under the same definition), as it will have ```\nn```\n iterations as well (in worst-case, ie. if the last element is ```\n3```\n). I think on the first line you used ```\nruns = number of iterations```\n and on the second line you used ```\nruns = starts/finishes```\n; there's no problem with either, but make sure to be clear and consistent.\nYou've correctly said that ```\nThe worst case scenario is an array that finish in 3```\n, and in this case it runs the outer loop ```\nn```\n times (skipping over the inner loop) then on the final iteration it doesn't skip the inner loop, which runs ```\nn```\n times. This is ```\nO(n+n)```\n = ```\nO(n)```\n.\n\nAnother way to think of it is that the code could be rewritten as:\n```\nfor (int i = 0; i < nums.length; i++) {\n}\n\nif ((nums.length > 0) && (nums[nums.length-1] == 3)){\n    for (int j = 0; j < nums.length; j++) {\n        print();\n    }\n}\n```\n\nWhich we rewrite for worst-case scenario like so:\n```\nfor (int i = 0; i < nums.length; i++) {\n}\n\nfor (int j = 0; j < nums.length; j++) {\n        print();\n}\n```\n\nThen the serial nature of it becomes obvious: ```\nn + n```\n work. The important thing here is that we haven't changed the algorithm in any important way (we haven't effected the worst-case complexity).\nCase B:\nYour logic and math here looks good.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How do I find out the big O notation of each of these algorithms?\r\n                \r\nI've been set a problem where I have a series of questions relating to these two algorithms, however I'm struggling to understand the concept of the big O notation which I feel like is very simple.\n\nEssentially I need to find out what the big O notation is, how long it will take for each algorithm to a solve a problem of one and two million operations.\n\nAlgorithm A:                                                                \n\n```\nSET sum TO 0\nFOR i=1 to size\n   FOR j=1 to 10000\n   sum=sum+1\n```\n\n\nAlgorithm B:\n\n```\nSET sum TO 0\nFOR i=1 to size\n    FOR j=1 to size\n    sum = sum + 1\n```\n\n\nThis is supposing the computer can perform one million operations per second\n    ", "Answer": "\r\nStarting with the last phrase of your post: the time complexity of an algorithm is not dependent on the number of operations a computer can perform per second. It is irrelevant.\n\nI assume the ```\nsum = sum + 1```\n statement belongs inside the inner ```\nFOR```\n loop -- the indentation does not make this clear.\n\nBoth algorithms execute ```\nsum = sum + 1```\n several times. We may consider that one execution of ```\nsum = sum + 1```\n takes constant time.\n\nNow, the difference between the two algorithms is the number of times the inner loop runs. In the first version, that is a constant (10000). This means that one execution of the complete ```\nFOR```\n loop will take constant time. It does not depend on size.\n\nThis means the first algorithm has only one loop that depends on size: it executes the inner loop size times. And so the time complexity is O(size). \n\nIn the second version however, the inner loop also runs size times. So there it clearly depends on size. The number of times that the inner most statement is executed is now size2. So the time complexity is O(size2).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation - Growth Rate\r\n                \r\nI am trying to understand if my reasoning is correct:\n\nIf I am given the following snippet of code and asked to find it's Big O:\n\n```\n for(int i = 3; i < 1000; i++)\n    sum++;\n```\n\n\nI want to say O(n) because we are dealing with one for loop and sum++ which is iterated say n times but then looking at this I realise we are not dealing with n at all as we are given the amount of times this for loop iterates... but in my mind it would be wrong to say that this has a Big O of O(1) because the growth is linear and not constant and depends on the size of this loop (although the loop is 'constant'). Would I be correct in saying that this is O(n)?\n\nAlso, another one that has me thinking around which has a similar setup:\n\n```\n for(int i = 0; i < n * n * n; i++)\n    for(int j = 0; j < i; j++)\n       sum++;\n```\n\n\nNow here again I know that when dealing with a nested loop containing and outer and inner loop we would use the multiplication rule to derive our Big O. Let's assume that the inner loop was in fact j < n then I would say that the Big O of this snippet of code is O(n^4) but as it isn't and we have a the second loop running its iterations off i and not n then would it be correct to say this as a Big Order of O(n^3)?\n\nI think what is throwing me is where 'n' is not appearing and we're given a constant or another variable and all of a sudden I'm assuming n must not be considered for that section of code. However, having said that the other part of my reasoning is telling me that despite not seeing an 'n' I should still treat the code as though there were an n as the growth rate would be the same regardless of the variable?\n    ", "Answer": "\r\nIt works best if you consider the code to always be within a function, where the function's arguments are used to calculate complexity. Thus:\n\n```\n// this is O(1), since it always takes the same time\nvoid doSomething() {\n    for(int i = 3; i < 1000; i++)\n        sum++;\n}\n```\n\n\nAnd\n\n```\n// this is O(n^6), since it only takes one argument\n// and if you plot it, the curve matches t = k * n^6\nvoid doSomethingElse(int n) {\n  for(int i = 0; i < n * n * n; i++)\n     for(int j = 0; j < i; j++)\n        sum++;\n}\n```\n\n\nIn the end, the whole point of big-O is to say what the run-times (or memory-footprints; but if you don't say anything, you are referring to run-times) look like as the problem size increases. It matters not what happens in the inside (although you can use that to estimate complexity) - what really matters is what you would measure outside.\n\nLooking closer at your second snippet, it's O(n^6) because:\n\n\nouter loop runs exactly n^3 times; inner loop runs, on average, n^3 / 2 times.\ntherefore, inner sum runs n^3 * k * n^3 times (with k a constant). In big-O notation, that's O(n^6).\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Can you do addition/multiplication with Big O notations?\r\n                \r\nI'm currently taking an algorithm class, and we're covering Big O notations and such. Last time, we talked about how\n\n```\nO (n^2 + 3n + 5) = O(n^2)\n```\n\n\nAnd I was wondering, if the same rules apply to this:\n\n```\nO(n^2) + O(3n) + O(5) = O(n^2)\n```\n\n\nAlso, do the following notations hold ?\n\n```\nO(n^2) + n\n```\n\n\nor\n\n```\nO(n^2) + Θ (3n+5)\n```\n\n\nThe later n is outside of O, so I'm not sure what it should mean. And in the second notation, I'm adding O and Θ .\n    ", "Answer": "\r\nAt least for practical purposes, the Landau ```\nO(...)```\n can be viewed as a function (hence the appeal of its notation).  This function has properties for standard operations, for example:\n\n```\nO(f(x)) + O(g(x)) = O(f(x) + g(x))\nO(f(x)) * O(g(x)) = O(f(x) * g(x))\nO(k*f(x)) = O(f(x))\n```\n\n\nfor well defined functions ```\nf(x)```\n and ```\ng(x)```\n, and some constant ```\nk```\n.\n\nThus, for your examples, \n\nYes:  ```\nO(n^2) + O(3n) + O(5) = O(n^2)```\n\nand:\n```\nO(n^2) + n = O(n^2) + O(n) = O(n^2)```\n,\n```\nO(n^2) + Θ(3n+5) = O(n^2) + O(3n+5) = O(n^2)```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is Big O notation merit Time Complexity or Rate of Growth?\r\n                \r\nI am studying Data Structures and Algorithms through internet and I learnt that;\n\nTime complexity is the amount of time taken by an algorithm to run, as a function of the length of the input\nBig O notation is a metric for calculating time complexity\nand\nRate of Growth is the rate at which the cost of the algorithm grows as the size of its input grows.\n\nAssume there is an algorithm which has a ```\nf(n) = n^2+3n+1```\n total number of operations.\nThen what would be the Time Complexity and Rate of growth if this algorithm?\nAre both represent by ```\nO(n^2)```\n?\nIt is not clear for me that what is the difference between Time Complexity and Rate of Growth when representing using Big O notation.\nPlease help me to get a clear idea about the terms. It'll be really appreciated.\nThank you in advance!\n    ", "Answer": "\r\nBig O describes both the rate of growth and the time complexity. However, as is mentioned in the comments, the former relates as a derivative. As in the case of O(1), the time is constant so there is no dependent growth rate, but the time complexity is still 1.\nIn your case of an O(n^2) algorithm, the runtime (time complexity) will grow at the expected rate of n^2 (rate of growth).\nAlso in this Quora post of roughly the same question, the answer states the \"runtime depends on the current input size, and the growth rate describes how the runtime will increase as the input size grows.\"\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Analysing complexity/runtime of pseudocode using big-O notation\r\n                \r\nI need a little help with a problem. I just started reading about O-notation but I'm still new when it comes to analysing code. \n\nSo here's the problem:\n\nThe following pseudocode is given, where A is a number field whose elements over the indices 1 to length(A) can be accessed\n\n```\n1: procedure Adder(A)\n2:      for i <- 1 to length(A)\n3:          for j <- length(A) to 1 do \n4:              if i ≠ j then\n5:                 A[i] <- A[i] + A[j]\n```\n\n\nGive the complexity of the following lines of code in big-O notation:\n\n\nlines 4-5\nlines 3-5\nlines 2-5\n\n\nSo for lines 4-5 I thought it should be simply O(1) since it simply just adds 2 elements.\n\nWith the other two I'm really unsure.\n\nFor line 3-5 I think it should be O(n) where n is the number of indices in the number field.\n\nAnd finally for lines 2-5 I would say it's O(n^2) since we now have to loops?\n    ", "Answer": "\r\nThat seems correct to me, although you may want to reformulate some of the justification you have \n\n\n  lines 4-5 I thought it should be simply O(1) since it simply just\n  adds 2 elements.\n\n\nIt is O(1) because no matter what the input is, the algorithm will end up doing either 1 or 2 instructions. It will never grow bigger than 1 or 2\n\n\n  finally for lines 2-5 I would say it's O(n^2) since we now have to\n  loops?\n\n\nIt is O(n^2) because the nested loops are iterating on the sequence you have as input. No matter what happens, if the input is of length N, you will have to make N loops, and N loops inside. So you end up with N * N which is N^2 as you suggested. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for the complexity function of the fourth root of n\r\n                \r\nI am expected to find the Big O notation for the following complexity function: ```\nf(n) = n^(1/4)```\n.\n\nI have come up with a few possible answers.\n\n\nThe more accurate answer would seem to be ```\nO(n^1/4)```\n. However, since it contains a root, it isn't a polynomial, and I've never seen this n'th rooted ```\nn```\n in any textbook or online resource.\nUsing the mathematical definition, I can try to define an upper-bound function with a specified ```\nn```\n limit. I tried plotting ```\nn^(1/4)```\n in red with ```\nlog2 n```\n in blue and ```\nn```\n in green.\n\n\n\n\nThe ```\nlog2 n```\n curve intersects with ```\nn^(1/4)```\n at ```\nn=2.361```\n while ```\nn```\n intersects with ```\nn^(1/4)```\n at ```\nn=1```\n.\n\nGiven the formal mathematical definition, we can come up with two additional Big O notations with different limits.\n\nThe following shows that ```\nO(n)```\n works for ```\nn > 1```\n.\n\n```\nf(n) is O(g(n))\nFind c and n0 so that\nn^(1/4) ≤ cn \nwhere c > 0 and n ≥ n0\nC = 1 and n0 = 1\nf(n) is O(n) for n > 1\n```\n\n\nThis one shows that ```\nO(log2 n)```\n works for ```\nn > 3```\n.\n\n```\nf(n) is O(g(n))\nFind c and n0 so that\nn^(1/4) ≤ clog2 n \nwhere c > 0 and n ≥ n0\nC = 1 and n0 = 3\nf(n) is O(log2 n) for n > 3\n```\n\n\nWhich Big O description of the complexity function would be typically used? Are all 3 \"correct\"? Is it up to interpretation?\n    ", "Answer": "\r\n\nUsing ```\nO(n^1/4)```\n is perfectly fine for big O notation. Here are some examples of fractures in exponents from real life examples\nO(n) is also correct (because big O giving only upper bound), but it is not tight, so ```\nn^1/4```\n is in ```\nO(n)```\n, but not in ```\nTheta(n)```\n\n```\nn^1/4```\n is NOT in ```\nO(log(n))```\n (proof guidelines follows).\n\n\nFor any value ```\nr>0```\n, and for large enough value of ```\nn```\n, ```\nlog(n) < n^r```\n.\nProof:\nHave a look on  ```\nlog(log(n))```\n and ```\nr*log(n)```\n. The first is clearly smaller than the second for large enough values. In big O notation terminology, we can definetly say that the ```\nr*log(n))```\n is NOT in ```\nO(log(log(n))```\n, and ```\nlog(log(n))```\n is(1), so we can say that:\n```\nlog(log(n)) < r*log(n) = log(n^r)     for large enough values of n\n```\n\nNow, exponent each side with base of ```\ne```\n. Note that both left hand and right hand values are positives for large enough ```\nn```\n:\n```\ne^log(log(n)) < e^log(n^r)\nlog(n) < n^r\n```\n\nMoreover, with similar way, we can show that for any constant ```\nc```\n, and for large enough values of ```\nn```\n:\n```\nc*log(n) < n^r\n```\n\nSo, by definition it means ```\nn^r```\n is NOT in ```\nO(log(n))```\n, and your specific case: ```\nn^0.25```\n is NOT in ```\nO(log(n))```\n.\n\nFootnotes:\n(1) If you are still unsure, create a new variable ```\nm=log(n)```\n, is it clear than ```\nr*m```\n is not in ```\nO(log(m))```\n? Proving it is easy, if you want an exercise.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "In Big O notation, how do you factor in calls to other methods?\r\n                \r\nHypothetically, lets say we have these two methods:\n\n```\nvoid example(int p){\n    p += 10;\n    p = exampleTwo(p);\n}\n\nint exampleTwo(int p){\n    int pp = 0;\n    for(int i = 0; i < p; i++){\n        pp++;\n    }\n    return pp;\n}\n```\n\n\nThe method ```\nexampleTwo```\n has a linear runtime. Its run time is O(n).\n\nSo, what is the big O notation of the method ```\nexample```\n, taking into account that it calls ```\nexampleTwo```\n? \n\nI would imagine it is also O(n), but I do not know for sure. \n    ", "Answer": "\r\nFor subroutines, you should multiply the order by the order of the number of the number of times it is called. For example, if a function is called O(n) times, and runs in O(log n) time, the total order is O(n log n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to calculate average asymptotic running time in Big-O notation?\r\n                \r\nTrying to understand Asymptotic notations which I understand is used to describe performance of an algorithm. Am I correct in saying that there is worst, best and average case scenarios? So for example, for the following piece of code, what is the average asymptotic running time in Big-O notation? \n\n```\nfor (int i = 0; i < size; i++)\n{\n    printf(\"%d\\n\", i);\n}\nfor (int i = 0; i < size; i++)\n{\n    for (int j = 0; j < size; j++)\n    {\n        printf(\"%d\\n\", i + j);\n    }\n}\n```\n\n\nWould it be O(n^2)?\n    ", "Answer": "\r\n\n  Am I correct in saying that there is worst, best and average case scenarios?\n\n\nGenerally only worst case and average case are of interest, but you can consider the best case as well if you prefer. \n\n\n  Would it be O(n^2)?\n\n\nWhat you have there is a typical ```\nO(n^2)```\n complexity, yes. Best, average and worst, they're all the same for your particular case. They're only not the same if you can break out of loops early, or conditionally skip entire loops. \n\nEdit: Example of when the average and best case complexity are different:\n\n```\nfor(var i = 0; i < len; ++i) \n  if(arr[i] == needle)\n    return needle;\n```\n\n\nThis is at best a constant operation, ie O(1), if the element is found right away, and at worst an O(n) linear operation -- a linear search specifically. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Why do we ignore co-efficients in Big O notation?\r\n                \r\nWhile searching for answers relating to \"Big O\" notation, I have seen many SO answers such as this, this, or this, but still I have not clearly understood some points.\n\nWhy do we ignore the co-efficients?\n\nFor example this answer says that the final complexity of ```\n2N + 2```\n is ```\nO(N)```\n; we remove the leading co-efficient ```\n2```\n and the final constant ```\n2```\n as well.\n\nRemoving the final constant of ```\n2```\n perhaps understandable.  After all, ```\nN```\n may be very large and so \"forgetting\" the final ```\n2```\n may only change the grand total by a small percentage.\n\nHowever I cannot clearly understand how removing the leading co-efficient does not make difference.  If the leading ```\n2```\n above became a ```\n1```\n or a ```\n3```\n, the percentage change to the grand total would be large.\n\nSimilarly, apparently ```\n2N^3 + 99N^2 + 500```\n is ```\nO(N^3)```\n.  How do we ignore the ```\n99N^2```\n along with the ```\n500```\n?\n    ", "Answer": "\r\nThe purpose of the Big-O notation is to find what is the dominant factor in the asymptotic behavior of a function as the value tends towards the infinity.\nAs we walk through the function domain, some factors become more important than others.\nImagine ```\nf(n) = n^3+n^2```\n. As ```\nn```\n goes to infinity, ```\nn^2```\n becomes less and less relevant when compared with ```\nn^3```\n.\nBut that's just the intuition behind the definition. In practice we ignore some portions of the function because of the formal definition:\n\n```\nf(x) = O(g(x))```\n as ```\nx->infinity```\n\nif and only if there is a positive real ```\nM```\n and a real ```\nx_0```\n such as\n```\n|f(x)| <= M|g(x)|```\n for all ```\nx > x_0```\n.\n\nThat's in wikipedia. What that actually means is that there is a point (after ```\nx_0```\n) after which some multiple of ```\ng(x)```\n dominates ```\nf(x)```\n. That definition acts like a loose upper bound on the value of ```\nf(x)```\n.\nFrom that we can derive many other properties, like ```\nf(x)+K = O(f(x))```\n, ```\nf(x^n+x^n-1)=O(x^n)```\n, etc. It's just a matter of using the definition to prove those.\nIn special, the intuition behind removing the coefficient (```\nK*f(x) = O(f(x))```\n) lies in what we try to measure with computational complexity. Ultimately it's all about time (or any resource, actually). But it's hard to know how much time each operation take. One algorithm may perform ```\n2n```\n operations and the other ```\nn```\n, but the latter may have a large constant time associated with it. So, for this purpose, isn't easy to reason about the difference between ```\nn```\n and ```\n2n```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What's the order of this algorithm in Big-O notation (string permutations)?\r\n                \r\nWhat is the performance in Big-O notation of the following algorithm? \nIt's a function I wrote to print all permutations of a string.\nI know for an input of length n there are n! different permutations.\nCan somebody provide an explanation of the analysis made to reach such conclusions?\n\n```\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n\nvoid permute(char* output, char* input, int used[], int index, int length){\n    int i;\n\n    if (index == length) {\n        printf(\"%s\\n\", output);\n        return;\n    }\n\n    for (i=0; i< length; i++){\n        if (! used[i] ){\n            output[index] = input[i];\n            used[i] = 1;\n            permute(output, input, used, index+1, length);\n            used[i] = 0;\n        }\n    }\n}\n\n\nint main(){\n    char input[] = \"abcd\";\n    char* output;\n    int length = strlen(input);\n    int* used;\n\n    // Allocate space for used array\n    used = (int*) malloc (sizeof (int)* length);\n    memset (used, 0, sizeof (int)* length);\n\n    // Allocate output buffer\n    output = (char*) malloc ( length+1);\n    if (!output) return 1;\n    output[length] = '\\0';\n\n    // First recursive call\n    permute(output, input, used, 0, length);\n\n    free (output);\n\n    return 0;\n}\n```\n\n    ", "Answer": "\r\n\n  I know for an input of length n there are n! different permutations.\n\n\nYou just answered your own question\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Find big O notation of nested loop which have if statement inside it [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Big O, how do you calculate/approximate it?\r\n                            \r\n                                (24 answers)\r\n                            \r\n                    \r\n                Closed 1 year ago.\r\n        \r\n\r\n\r\n    \r\n\r\nHere is a part of an algorithm to rotate an array by 180 degrees.\n```\nSystem.out.println(\"\\nHasil:\");\nfor (i= arr.length-1; i>=0; i--) {\n    System.out.print(\"[\");\n    for (j= arr.length-1; j>=0; j--){\n        System.out.print(arr[i][j]);\n        if (j != 0) {\n            System.out.print(\",\");\n        }\n    }System.out.println(\"]\");\n}\n```\n\nI'm confused how to determine the big o notation that have nested loop with if statement inside it.\n    ", "Answer": "\r\nThe if conditional is not a factor in big O notation of runtime. The length of the array is the variable here (that will be ‘n’) and I see you have a nested loop, 2 loops, that will both traverse the length of that array. That means the runtime will be on the order of n * n, or n squared. O(n squared). It can also be written as O(n^2)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How do I get time of a Python program's execution?\r\n                \r\nI have a command line program in Python that takes a while to finish. I want to know the exact time it takes to finish running.\n\nI've looked at the ```\ntimeit```\n module, but it seems it's only for small snippets of code. I want to time the whole program.\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of sorting methods into graphs\r\n                \r\nI am having allot of trouble understanding big O notation and computational complexity etc. I think all the complex maths I found on the internet is just dazzling me.\n\nI am trying to plot a graph to represent the efficiency of both the insertion sort and shell sort.\n\nI think I understand that the worst case of a shell sort is n^2 and the best case is nlogn. Is this for all shell sorts? How do I represent this in a graph with the relevant axis of time and ?\n\nAny help would be much appreciated, I am very lost.\n\nHere is my code for my shell sort if relevant.\n\n```\nint const size = 5;\nint i, j, increment, temp;\nint array[size]={4,5,2,3,6}, i1=0;\n//split the array into segments between the elements unil we reach beginning of array    \nfor(increment = size/2;increment > 0; increment /= 2)\n{\n    //increment through elements in array for comparison starting point\n    for(i = increment; i<size; i++)\n    {\n        //set temp to last element in array segment\n        temp = array[i];\n        //decrement index by size of gap\n        for(j = i; j >= increment ;j-=increment)\n        {\n            //compare element with element gap length behind\n            if(temp < array[j-increment])\n            {\n                //swap elements if less than gap element\n                array[j] = array[j-increment];\n            }\n            else\n            {\n                //if not break from loop\n                break;\n            }\n        }\n        array[j] = temp;\n    }\n}\n```\n\n    ", "Answer": "\r\n\n  How do I represent this in a graph with the relevant axis of time and ?\n   - N, the size of the input, is your independent variable and is represented in the horizontal axis (x) \n   - Plot N^2 (the worst case) in the vertical axis (y)\n   - Also plot NlogN (the best case) in the vertical axis (y)\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "big O notation for removing an element from a linked list [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Big-O summary for Java Collections Framework implementations? [closed]\r\n                            \r\n                                (4 answers)\r\n                            \r\n                    \r\n                    \r\n                        What are the time complexities of various data structures?\r\n                            \r\n                                (2 answers)\r\n                            \r\n                    \r\n                Closed 6 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI was reading about linked lists. I found that : Removing an desired element from a linked list takes O(n) running time, where n is the number of\nelements in the list. \nhttp://www.cs.mcgill.ca/~dprecup/courses/IntroCS/Exams/comp250-final-2006-solutions.pdf\n\nBut in this webpage I found that deletion an element from a linked list is: O(1).\nhttp://bigocheatsheet.com/\n\nWhich one of the above big O notation is the correct one for deletion from a linked list.\n\nThanks\n    ", "Answer": "\r\nThe time required to remove the item from the linked list depends on how exactly we a going to do this. Here are possibilities:\n\n\nRemove by value. In this case we need to find an item (O(n)) and remove it (O(1)). The total time is O(n).\nRemove by index. In this case we need to traverse list (O(index)) and remove item (O(1)). The total time is O(index). For arbitrary index it is O(n).\nRemove by the reference to the linked list node that contains the item O(1). \n\n\nIn Java the last case is met when you are using ```\nIterator.remove```\n or ```\nListIterator.remove```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is big-O notation a tool to do best, worst, & average case analysis of an algorithm?\r\n                \r\nIs big-O notation a tool to do best, worst, & average case analysis of an algorithm?\nOr is big-O only for worst case analysis, since it is an upper bounding function?\n    ", "Answer": "\r\nIt is Big O, because orders of magnitude are expressed like O(n), O(logN), etc.\n\nThe best, worst, and average cases of an algorithm can all be expressed with Big O notation.\n\nFor an example of this applied to sorting algorithms, see\n\nhttp://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms\n\nNote that an algorithm can be classified according to multiple, independent criteria such as memory use or CPU use.  Often, there is a tradeoff between two or more criteria (e.g. an algorithm that uses little CPU may use quite a bit of memory).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation for simple loop\r\n                \r\nI am just starting out in a data structure class, and the instructors have posted 10 problems and asked the Big O of one of them.  Based off the posts I have read I am assuming that the Big O of this code would be O(1), since the data parameter is a single data element.  However, it does execute multiple times depending on the size of the number so would that make it O(N)?\n\n```\npublic class Main {\n\n    public static void main(String[] args) {\n        f(100000);\n    }\n\n    public static long f (int n) {\n        long sum = 0;\n        for (long i = 2; i < n; i = i * i) {\n            sum += i;\n            System.out.println(sum);\n        }\n        return sum;\n    } // end f\n}\n```\n\n    ", "Answer": "\r\nThis function has a time complexity of O(log(log(n)).\n\n```\ni```\n grows by multiplication in an exponentially growing factor, so that's \"double exponential growth\" (not sure if that's a valid definition), and the complexity is the inverse. You can read more about this class of complexity here.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation in different data structures [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 6 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI was reading about Big O notation in java , I found the following questions, I don't understand the answers of them.\n\n(a) What is the worst-case asymptotic running-time for the best algorithm for finding something in a dictionary implemented with a sorted array?\nO(log n)\n\n(b) What is the best-case asymptotic running-time for the best algorithm for finding something in a dictionary implemented with a sorted array?\nO(1)\n\n(c) What is the worst-case asymptotic running-time for the best algorithm for finding something in a dictionary implemented with a sorted linked list?\nO(n)\n\n(d) What is the best-case asymptotic running-time for the best algorithm for finding something in a dictionary implemented with a sorted linked list?\nO(1)\n\n(e) Given a binary search tree, find which value is the minimum value and delete it. \nO(n) \n\n(f) Given a binary search tree, find which value is the median value, and delete that\nvalue.\nO(n) \n\nCan you explain to me the answers, and what do they mean by that specific algorithm in the first four questions?\n\nThanks\n    ", "Answer": "\r\nBig O is the worst-case scenario. When you want to calculate the big O, you need to assume that everything will be found at the last item, the sky is black, etc.\n\n(a) The best algorithm is binary search, which guesses in the middle of the sorted array and if the needle is larger, then guesses the middle point of the latter elements, otherwise the former elements. This is repeated until the element is found or the subset is trivial. Since you always have two possible decisions, and on each decision the size of the set is halved, the number of steps is repeated the number of times your set can be halved, which is the number which, as a power of 2 would yield the size of the set. This is log(n). The worst-case scenario is that all the steps need to be performed, that is, O(log(n)).\n\n(b) The best case is that the item happens to be exactly at the middle of the set, which happens to be the first guess, therefore the result is O(1).\n\n(c) In a linked list you cannot do a binary search even if it is sorted, since you can only get the next element from a given element. The worst case is that the needle happens to be the very last one. O(n)\n\n(d) The situation is same as above, but the best case is when the very first item happens to be the searched item. O(1)\n\n(e) In a balanced BST the search time would be O(logn) and removal would be O(1). But if we assume the worst, that is, the BST is unbalanced and the root is the maximum, then a search would traverse all items until it reaches the only leaf, the minimum O(n) and removes it O(1).\n\n(f) To find the median in a BST you might need to traverse all nodes, imagine the case when the root has two children and the leftmost child has only right child and the rightmost child has only left child and the tree has a total of two leafs. In this case, the worst case scenario is when you have chosen the wrong direction and traversed the wrong side and then have to traverse to the good direction as well where he very last element is the one you are searching for. O(n)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for (n^2) [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and  cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened,  visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nexample 1\n\n```\nfor (int i = 0; i < n; i++) {\n  for (int j = 0; j < n; j++) {\n        System.out.println(count++);\n   }\n}\n```\n\n\nexample 2  \n\n```\nfor (int i = 0; i < n * n; i++) {            \n     System.out.println(count++);\n} \n```\n\n\nBoth example give me a Big O (n^2). but which ans is the best?\n    ", "Answer": "\r\nTo find the best code, it would be better to compile it and use ```\njavap -c ClassName.class```\n to see the generated bytecode for both methods. I used ```\nfoo```\n method for the former and ```\nbar```\n method for the latter.\n\n```\npublic static void foo(int n) {\n    int count = 0;\n    for (int i = 0; i < n; i++) {\n        for (int j = 0; j < n; j++) {\n            System.out.println(count++);\n        }\n    }\n}\n\npublic static void bar(int n) {\n    int count = 0;\n    for (int i = 0; i < n * n; i++) {\n        System.out.println(count++);\n    }\n}\n```\n\n\nThese are the results:\n\n```\npublic static void foo(int);\n    Code:\n       0: iconst_0      \n       1: istore_1      \n       2: iconst_0      \n       3: istore_2      \n       4: iload_2       \n       5: iload_0       \n       6: if_icmpge     38\n       9: iconst_0      \n      10: istore_3      \n      11: iload_3       \n      12: iload_0       \n      13: if_icmpge     32\n      16: getstatic     #2       // Field java/lang/System.out:Ljava/io/PrintStream;\n      19: iload_1       \n      20: iinc          1, 1\n      23: invokevirtual #3       // Method java/io/PrintStream.println:(I)V\n      26: iinc          3, 1\n      29: goto          11\n      32: iinc          2, 1\n      35: goto          4\n      38: return        \n\npublic static void bar(int);\n    Code:\n       0: iconst_0      \n       1: istore_1      \n       2: iconst_0      \n       3: istore_2      \n       4: iload_2       \n       5: iload_0       \n       6: iload_0       \n       7: imul          \n       8: if_icmpge     27\n      11: getstatic     #2       // Field java/lang/System.out:Ljava/io/PrintStream;\n      14: iload_1       \n      15: iinc          1, 1\n      18: invokevirtual #3       // Method java/io/PrintStream.println:(I)V\n      21: iinc          2, 1\n      24: goto          4\n      27: return        \n```\n\n\nIn the end, the latter implementation seems to be better than the former due to less operations made.\n\nI'm not an expert on this, so if anyone can edit/perform a better analysis, feel free to do it. I wrote this answer since it didn't fit inside a single comment.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation for simple loop\r\n                \r\nI am just starting out in a data structure class, and the instructors have posted 10 problems and asked the Big O of one of them.  Based off the posts I have read I am assuming that the Big O of this code would be O(1), since the data parameter is a single data element.  However, it does execute multiple times depending on the size of the number so would that make it O(N)?\n\n```\npublic class Main {\n\n    public static void main(String[] args) {\n        f(100000);\n    }\n\n    public static long f (int n) {\n        long sum = 0;\n        for (long i = 2; i < n; i = i * i) {\n            sum += i;\n            System.out.println(sum);\n        }\n        return sum;\n    } // end f\n}\n```\n\n    ", "Answer": "\r\nThis function has a time complexity of O(log(log(n)).\n\n```\ni```\n grows by multiplication in an exponentially growing factor, so that's \"double exponential growth\" (not sure if that's a valid definition), and the complexity is the inverse. You can read more about this class of complexity here.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Time and Space Complexity of an Algorithm - Big O Notation\r\n                \r\nI am trying to analyze the Big-O-Notation of a simple algorithm and it has been a while I've worked with it. So I've come with an analysis and trying to figure out if this is correct one according to rules for the following code:\n\n```\npublic int Add()\n{\n  int total = 0; //Step 1\n\n  foreach(var item in list) //Step 2\n  {\n    if(item.value == 1) //Step 3\n    {\n      total += 1; //Step 4\n    }\n  }\n  return total;\n}\n```\n\n\n\nIf you assign a variable or set, in this case the complexity is determined according to the rules of Big O is O(1). So the first phase will be O(1) - This means whatever the input size is, the program will execute for the same time and memory space.\nThe second step comes up with ```\nforeach```\n loop. One thing is pretty clear in the loop. According to the input, the loop iterates or runs. As an example, for input 10, loop iterates 10 times and for 20, 20 times. Totally depends on the input. In accordance with the rules of the Big O, the complexity would be O(n) - n is the number of inputs. So in the above code, the loop iterates depending upon the number of items in the list.\nIn this step, we define a variable that determines a condition check (See Step 3 in the coding). In that case, the complexity is O(1) according to the Big O rule.\nIn the same way, in step 4, there is also no change (See Step 4 in the coding). If the condition check is true, then ```\ntotal```\n variable increments a value by 1. So we write - complexity O(1).\n\n\nSo if the above calculations are perfect, then the final complexity stands as the following:\n\n```\nO(1) + O(n) + O(1) + O(1) or (O(1) + O(n) * O(1) + O(1))\n```\n\n\nI am not sure if this is correct. But I guess, I would expect some clarification on this if this isn't the perfect one. Thanks.\n    ", "Answer": "\r\nBig O notation to describe the asymptotic behavior of functions. Basically, it tells you how fast a function grows or declines\n\nFor example, when analyzing some algorithm, one might find that the time (or the number of steps) it takes to complete a problem of size n is given by\n\n\n  T(n) = 4 n^2 - 2 n + 2\n\n\nIf we ignore constants (which makes sense because those depend on the particular hardware the program is run on) and slower growing terms, we could say \"T(n)\" grows at the order of n^2 \" and write:T(n) = O(n^2)\n\nFor the formal definition, suppose f(x) and g(x) are two functions defined on some subset of the real numbers. We write\n\n\n  f(x) = O(g(x))\n\n\n(or f(x) = O(g(x)) for x -> infinity to be more precise) if and only if there exist constants N and C such that\n\n\n  |f(x)| <= C|g(x)| for all x>N\n\n\nIntuitively, this means that f does not grow faster than g\n\nIf a is some real number, we write\n\n\n  f(x) = O(g(x)) for x->a\n\n\nif and only if there exist constants d > 0 and C such that\n\n\n  |f(x)| <= C|g(x)| for all x with |x-a| < d\n\n\nSo for your case it would be \n\n\n  O(n) as |f(x)| > C|g(x)|\n\n\nReference from http://web.mit.edu/16.070/www/lecture/big_o.pdf\n\n```\nint total = 0;\nfor (int i = n; i < n - 1; i++) { // --> n loop\n    for (int j = 0; j < n; j++) { // --> n loop\n        total = total + 1; // -- 1 time \n    }\n}\n\n}\n```\n\n\nBig O Notation gives an assumption when value is very big outer loop will run n times and inner loop is running n times\n\nAssume n -> 100 than total n^2 10000 run times\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O, Theta, and big Omega notation\r\n                \r\nBased on my understanding, big O is essentially similar to theta notation but can include anything bigger than the given function (e.g. ```\nn^3 = O(n^4), n^3 = O(n^5)```\n, etc.), and big Omega includes anything smaller than the given function (```\nn^3 = Ω(n^2```\n), etc.). \n\nHowever, my professor said the other day that ```\nn^0.79 = Ω(n^0.8)```\n, while he was doing an exercise that involved the master theorem. \n\nWhy/how is this true when ```\nn^0.8```\n is larger than ```\nn^0.79```\n?\n    ", "Answer": "\r\nYou have big O and big Omega backwards.  Big O is everything the \"same\" or smaller than the function.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Does Call by Name or Call by Value have an effect in Big O Notation? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 2 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nWould the way the code is evaluated either though call by value or call by name have an effect in Big O Notation? Why or why not?\n    ", "Answer": "\r\nYes, because a by-name argument is evaluated every time it is accessed.\nFor example, take a method that is ```\nO(n)```\n because it evaluates its argument ```\nn```\n times, and a value that involves a computation that is also ```\nO(n)```\n.\nIf the argument is passed by value then this is ```\nO(n)```\n because the argument is evaluated ```\n1```\n time and accessed ```\nn```\n times.\nIf the argument is passed by name then this is ```\nO(n^2)```\n because the argument is evaluated ```\nn```\n times and each evaluation is ```\nO(n)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big o notation for nested loops with an inner loop that depends on the outer loop\r\n                \r\nI'm just learning big O notation and I'm confused about nested loops:\n```\nfor (int x = 0; x < n; x++)\n   for (int y = 0; y < n; y++)\n      for (int z = 0; z < y; z++)\n         anything();\n```\n\nFrom my understanding the inner loop above executes n(n+1)/2 times, the second loop executes n times and the first loop executes n times. Shouldn't this mean the big O is n x n x n(n+1)/2 = O(n^4)? Why doesn't the second loop get included in the big O formulation?\n    ", "Answer": "\r\nThe question is: how often does ```\nanything()```\n get called, as a function of ```\nn```\n?\n\nthe inner loop above executes n(n+1)/2 times\n\nNo, the inner loop executes y times (per times you enter it), and y averages n/2.\nSo the equation is n * n * n/2 = O(n^3).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation with the use of methods with different parameters and time\r\n                \r\nWhat is the time complexity if faa() takes linear time,\n```\nfor(int j = 0 ; j < n ; j++)\n    faa(k)\n```\n\nor\n```\nfor(int j = 0 ; j < n ;j++)\n    faa(j)\n```\n\nFrom what I learned, we are using Big O Notation and time complexity is found by viewing n so both the loops will be O(n) which would be then multiplied by the next line which would give us the answer to the time complexity for both of the problems. Would the answer simply be O(kn) and O(jn) or am I lost? According to my book it says that faa() takes linear time and when it calls upon a parameter faa(k) Big O is O(k) so would j be the same?\n    ", "Answer": "\r\n\nWith ```\nfaa(k)```\n : the result is ```\nO(n)```\n as the ```\nfaa```\n is done ```\nn```\n times in constant time (does not relate on ```\nj```\n, so ```\nO(kn)```\n goes to ```\nO(n)```\n as k does not change)\nWith ```\nfaa(j)```\n : the result is ```\nO(n^2)```\n as the linear computation is done ```\nn```\n times and each one goes to ```\nj```\n but because ```\nj```\n goes to ```\nn```\n by approximation we say until ```\nn```\n for each time, \n\n\nIt is better understandable to say it's quadratic\n\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Can any one explain time complexity of below code (big O notation)\r\n                \r\nCan anyone explain time complexity of the code below? in Big O notation. Thanks in Advance.\nI was trying to get time complexity for one of my codes. I think its O(logn). But in code we have few delete operations.\n```\nclass Solution:\ndef merge( arr):\n    i=0\n    arr.sort(key=lambda x:x[0],reverse=False)\n    while i<(len(arr)-1):\n\n        if arr[i][1] >= arr[i+1][0]:\n            if arr[i][1]>=arr[i+1][1]:\n                del(arr[i+1])\n                if i>0:\n                    i-=1\n            else:\n                arr[i][1]=arr[i+1][1]\n                del(arr[i+1])\n                if i>0:\n                    i-=1\n            continue\n        i+=1\n    return arr\n\n#driver code:\narr=[[1,3],[2,6],[8,10],[9,11],[12,14]]\nans=Solution.merge(arr)\nprint(ans)\n```\n\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation: What is the order of the algorithm? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs details or clarity. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Add details and clarify the problem by editing this post.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI'm having trouble understanding Big-O Notation. Here is an algorithm I wrote, it is supposed to be an alternative of (C++) Stack's size() function, and I need to determine its running time with the assumption that there are n elements in the stack when it is invoked.\n\n```\nAlgorithm size():\n    Input: none\n    Output: A constant value of the size of an n-element stack.\nLet V be a vector of n type objects.\nLet S be the name of the stack that is being operated on by this function.\nK ← 0                       \nV\nwhile !empty() \n    V.push_back(top())      //Keep track of elements in V\n    S.pop()             //Remove element from stack\n    K ← K + 1           //Count the size of the stack\nreturn K                //Return the size of the stack  \nfor i ← K – 1, i > 0, i-- do            \n    S.push(V[i])            //Retain initial contents of stack\n```\n\n\nPlease correct me where I'm wrong:\n\n\nIn the line K ← 0, I think that is an O(1) operation.\nCreating the vector V is also an O(1) operation.\nthe while loop is an O(n) operation since it runs until it empties a stack containing n contents.\nPushing values back into V is an O(n) operation.\nPopping contents off the stack S is an O(n) operation.\nReturning K is an O(1) operation.\nThe for loop is an O(n) operation.\nPushing contents back into S is an O(n) operation.\n\n    ", "Answer": "\r\nThe push and pop operations are O(1). The for loop that contains the push is O(n) and the push is O(1).\n\nMIT Always puts out good stuff to study http://web.mit.edu/16.070/www/lecture/big_o.pdf.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "(Beginner) Questions about the Big O notation\r\n                \r\nI have some questions related to the Big O notation:\n\n\n\n```\nn^3 =  Big Omega(n^2)\n```\n\n\nThis is true because:\n\n```\nn^3 >= c * n^2 for all n >= n0\n\n-> Lim(n-> infinity) = n^3/n^2 >= c\n```\n\n\nThen I used L'Hospital and got ```\n6n/2 >= c```\n which is true if I for example choose ```\nc```\n as ```\n1```\n and ```\nn0```\n as ```\n1```\n\n\n\n\nAre my thoughts right on this one ?\n\nNow  I got two pairs:\n\n```\nlog n```\n and ```\nn/log n```\n, do they lie in ```\nTheta, O```\n or somewhere else ? Just tell me where they lie, then I can do the proof by myself.\n\n```\nn^(log n)```\n and ```\n2^n```\n follows vice versa\n\n\n\nAnd at last:\n\n```\nf(n) = O(n) -> f(n)^2 = O(n^2)\n\nf(n)g(n) = O(f(n)g(n))\n```\n\n\nThe question is: Are these statements correct ?\n\nI would say yes to the first one, I don't really know why and it seems like there is a hidden trick to this, but I don't really know, could someone help me out here ?\n\nThe second one should be true if ```\ng(n)```\n lies in ```\nO(n)```\n ,but I don't really know here either.\n    ", "Answer": "\r\nSeems like you're right here.  \n\n\n\nAs for ```\nlog(n)```\n and ```\nn/log(n)```\n you can check it by finding the ```\nlim log(n)/(n/log(n))```\n and vice versa.\nUsing the fact that ```\na^b = e^(b*ln(a))```\n:\n```\nn^log(n) = e^(log(n) * log(n)) < e^(n^2) = e^e^(2*log(n)) < (e^e)^(2*n) = O(C^n)```\n, and ```\n2^n```\n is also ```\nBig O(C^n)```\n. \n\n\n\nLet's use the definition and some properties of the Big O(f):  \n\n\n```\nO(f) = f * O(1)```\n \n```\nO(1) * O(1) = O(1)```\n\n\n\nNow we have:\n```\nf(n)^2 = f(n) * f(n) = O(n) * O(n) = n * O(1) * n * O(1) = n^2 * O(1) = O(n^2)```\n.\n```\nf(n)g(n) = f(n)g(n) * O(1) = O(f(n)g(n))```\n.\nSo, yes, it is correct. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "java- Big O Notations\r\n                \r\nSo, I have tried to solve some big o questions and I had some troubles with some of them. I don't quite understand them.\nlike for eg the dominant term of ```\n10MlogM + (N/2) log (N/2) + N/4```\n\nand   ```\nM log (N) + M log (M )```\n. I have trouble understanding big o expressions with 'log' in them. Any help would be appreciated and thanks in advance.\n    ", "Answer": "\r\n\n  I have trouble understanding big o expressions with 'log' in them\n\n\nIt's quite easy to understand Big-O of ```\nlog```\n (or any function) if we draw its graph:\n\n\n\nNow since you are facing an equation having multiple terms (and one of them is ```\nlog```\n), so maybe it would confuse you out. If we plot first term (I assume its ```\nNlogN```\n and discard the constant (10)):\n\n\n\nSimilarly for other term (```\nN/2 * log(N/2)```\n):\n\n\n\nI am sure you will already know, but this graph easily demonstrates that only major term (```\nN logN```\n) defines the Big O (as its just to show the upper bound of function) as I plot the graph of whole function:\n\n\n\nWhat does N log N mean\n\nThis link provides very good explanation with this example:\n\n\n  O(n log n): There was a mix-up at the printer's office, and our phone book had all its pages inserted in a random order. Fix the ordering so that it's correct by looking at the first name on each page and then putting that page in the appropriate spot in a new, empty phone book.\n  \n  For the below examples, we're now at the printer's office. Phone books\n  are waiting to be mailed to each resident or business, and there's a\n  sticker on each phone book identifying where it should be mailed to.\n  Every person or business gets one phone book.\n  \n  O(n log n): We want to personalize the phone book, so we're going to\n  find each person or business's name in their designated copy, then\n  circle their name in the book and write a short thank-you note for\n  their patronage.\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Counting Primitive Operations & Calculating Big-O Notation\r\n                \r\nI've written a piece of java code that when given an array (arrayX), works out the prefix averages of that array and outputs them in another array (arrayA). I am supposed to count the primitive operations and calculate the Big-O notation (which i'm guessing is the overall number of calculations). I have included the java code and what I believe to be the number of primitive operations next to each line, but I am unsure whether I have counted them correctly. Thanks in advance and sorry for my inexperience, i'm finding this hard to grasp :)\n\n```\ndouble [] arrayA = new double [arrayX.length]; *(3 Operations)*\n\n    for (int i = 0; i < arrayX.length; ++i) *(4n + 2 Operations)* \n    {\n        double sum = arrayX[i]; *(3n Operations)*\n\n        for (int j = 0; j < i; ++j) *(4n^2 + 2n Operations)*\n        {\n            sum = sum + arrayX[j]; *(5n^2 Operations)*\n        }\n        arrayA[i] = sum/(i+1); *(6n Operations)*\n    }\n    return arrayA; *(1 Operation)*\n```\n\n\nTotal number of operations: 9n^2 +15n + 6\n    ", "Answer": "\r\nI don't think there's any standard definition of \"what constitutes a primitive operation\".  I'm assuming this is a class assignment; if your instructor has given you detailed information about what operations to count as primitive operations, then go by that, otherwise I don't think he can fault you for any way you count them, as long as you have a reasonable explanation.\n\nRegarding the inner loop:\n\n```\nfor (int j = 0; j < i; ++j)\n```\n\n\nplease note that the total number of times the loop executes is not n2, but rather 0+1+2+...+(n-1) = n(n-1)/2.  So your calculations are probably incorrect there.\n\nBig-O notation is not really \"the total number of calculations\"; roughly speaking, it's a way of estimating how the number of calculations grows when n grows, by saying that the number of calculations is roughly proportional to some function of n.  If the number of calculations is Kn2 for any constant K, we say that the number of calculations is O(n2) regardless of what the constant K is.  Therefore, it doesn't completely matter what you count as primitive operations.  You might get 9n2, someone else who counts different operations may get 7n2 or 3n2, but it doesn't matter--it's all O(n2).  And the lower-degree terms (15n+6) don't count at all, since they grow more slowly than the Kn2 term.  Thus they aren't relevant to determining the appropriate big-O formula.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Asymptotic notation big(O) and Big(Omega)\r\n                \r\n\n  f(n) = 6*2^n  + n^2\n  \n  big(O) = 2^n \n  \n  big(Omega) = 2^n\n\n\nIn above equation both big(O) and big(Omega) has same value. If big (O) is upper bound and big(omega) is lower bound shouldn't big(omega) = n^2. Why the both have same value?\n    ", "Answer": "\r\nIt's true that O and Ω are upper and lower bounds, respectively, but they are more similar to ≤ and ≥ than to < and >. Just like it's possible that, simultaneously a ≥ b and a ≤ b (without contradiction), so can a function be both O and Ω of a different function (in fact, that's one of the ways to define Θ).\n\nHere, for large enough n, \n\n\n6 2n + n2 ≤ 12 2n so 6 2n + n2 grows at most (up to a multiplicative constant)like 2n does (it is O of it).\nConversely, 6 2n + n2 ≥ 0.1 2n so 6 2n + n2 grows at least (up to a multiplicative constant) like 2n does (it is Ω of it).\n\n\nNote that you don't have to use the same multiplicative constants. The conclusion is that 6 2n + n2 = Θ( 2n) \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation and functional programming in javascript\r\n                \r\nI am trying to understand and state the Big O Notation of the following algorithm using ```\nreduce()```\n. My understanding is that reduce is a function applied to an array object. It takes in a callback & an initialValue. The code below is a controller that holds the algorithm:\n\n```\nexport const getPublicID = (req, res) => {\n  const data = req.body.result;\n  if (!data) res.status(422).json({ success: false, message: 'Upload Failed!' });\n  console.time('ARRAY');\n  const insertStuff = data.reduce((array, item) => {\n    array.push({\n      public_id: item.public_id,\n      url: item.url,\n      thumbnail_url: item.thumbnail_url\n    });\n\n    return array;\n  }, []);\n  console.timeEnd('ARRAY');\n\n  Photo.insertMany(insertStuff)\n    .then(\n      img => res.status(201).json({\n      success: true,\n      message: 'Successfully added to database.',\n      cloudinary: img\n     }),\n     err => res.status(422).json({ success: false, message: err })\n    );\n};\n```\n\n\nThe ```\nreq.body.result```\n comes in as an array of objects and through the reduce method I am creating my own array of objects that I then insert in my MongoDB collection.\nReduce is looping through the array so my thought is this is O(n), since the more elements present the more time it will take to iterate over, thus a linear graph. If that is the correct assumption my three questions are how do the following effect my algorithm:\n\n\n```\npush()```\n\n```\ninsertMany()```\n\nthe promise\n\n\nThanks for helping a noob to data structures and algorithms out with understanding the pros and cons of the code, I greatly appreciate it!\n    ", "Answer": "\r\nThe Big O describes asymptotic performance, and more specific it gives the upper bound for time complexity of an algorithm. This means that it doesn't look at how much actual time a function takes, could be 1 ms could be 1 min, just at how efficient your algorithm is.\n\nO(n) means that the script will run in linear time.\nExample of that would be:\n\n```\nfor(int i=0; i<n; ++i) {\n   print(i);\n}\n```\n\n\nNow if you then need to run trough that array again, you'll get a different performance. \n\nO(n^2) = O n squared = Outer loop (i) x outer loop (x)\n\n```\nfor(int i=0; i<n; ++i) {\n    for(int x=0; x<n; ++x) {\n        print(x);\n    }\n}\n```\n\n\nNow looking at what you're doing, you're on the right track with your analysis and you don't have a loop inside of a loop, just sequential loops. \n\nThere's the push(), although it's determined by reduce() rather than push().\nThere's the insertMany() which has the promise as a part of it. It's not an additional loop, just a function that's being executed.\n\nThis means that you have two loops. Some people would say that this gives you O(2n) but others claim there's no such thing, nor would it make a difference.\n\nBottom line: \nLooking at the purpose of Big O, it focuses on growth rate which is still linear, which still gives you O(n). \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation and functional programming in javascript\r\n                \r\nI am trying to understand and state the Big O Notation of the following algorithm using ```\nreduce()```\n. My understanding is that reduce is a function applied to an array object. It takes in a callback & an initialValue. The code below is a controller that holds the algorithm:\n\n```\nexport const getPublicID = (req, res) => {\n  const data = req.body.result;\n  if (!data) res.status(422).json({ success: false, message: 'Upload Failed!' });\n  console.time('ARRAY');\n  const insertStuff = data.reduce((array, item) => {\n    array.push({\n      public_id: item.public_id,\n      url: item.url,\n      thumbnail_url: item.thumbnail_url\n    });\n\n    return array;\n  }, []);\n  console.timeEnd('ARRAY');\n\n  Photo.insertMany(insertStuff)\n    .then(\n      img => res.status(201).json({\n      success: true,\n      message: 'Successfully added to database.',\n      cloudinary: img\n     }),\n     err => res.status(422).json({ success: false, message: err })\n    );\n};\n```\n\n\nThe ```\nreq.body.result```\n comes in as an array of objects and through the reduce method I am creating my own array of objects that I then insert in my MongoDB collection.\nReduce is looping through the array so my thought is this is O(n), since the more elements present the more time it will take to iterate over, thus a linear graph. If that is the correct assumption my three questions are how do the following effect my algorithm:\n\n\n```\npush()```\n\n```\ninsertMany()```\n\nthe promise\n\n\nThanks for helping a noob to data structures and algorithms out with understanding the pros and cons of the code, I greatly appreciate it!\n    ", "Answer": "\r\nThe Big O describes asymptotic performance, and more specific it gives the upper bound for time complexity of an algorithm. This means that it doesn't look at how much actual time a function takes, could be 1 ms could be 1 min, just at how efficient your algorithm is.\n\nO(n) means that the script will run in linear time.\nExample of that would be:\n\n```\nfor(int i=0; i<n; ++i) {\n   print(i);\n}\n```\n\n\nNow if you then need to run trough that array again, you'll get a different performance. \n\nO(n^2) = O n squared = Outer loop (i) x outer loop (x)\n\n```\nfor(int i=0; i<n; ++i) {\n    for(int x=0; x<n; ++x) {\n        print(x);\n    }\n}\n```\n\n\nNow looking at what you're doing, you're on the right track with your analysis and you don't have a loop inside of a loop, just sequential loops. \n\nThere's the push(), although it's determined by reduce() rather than push().\nThere's the insertMany() which has the promise as a part of it. It's not an additional loop, just a function that's being executed.\n\nThis means that you have two loops. Some people would say that this gives you O(2n) but others claim there's no such thing, nor would it make a difference.\n\nBottom line: \nLooking at the purpose of Big O, it focuses on growth rate which is still linear, which still gives you O(n). \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Meaning of average complexity when using Big-O notation\r\n                \r\nWhile answering to this question a debate began in comments about complexity of QuickSort. What I remember from my university time is that QuickSort is ```\nO(n^2)```\n in worst case, ```\nO(n log(n))```\n in average case and ```\nO(n log(n))```\n (but with tighter bound) in best case.\n\nWhat I need is a correct mathematical explanation of the meaning of ```\naverage complexity```\n to explain clearly what it is about to someone who believe the big-O notation can only be used for worst-case.\n\nWhat I remember if that to define average complexity you should consider complexity of algorithm for all possible inputs, count how many degenerating and normal cases. If the number of degenerating cases divided by n tend towards 0 when n get big, then you can speak of average complexity of the overall function for normal cases.\n\nIs this definition right or is definition of average complexity different ? And if it's correct can someone state it more rigorously than I ?\n    ", "Answer": "\r\nYou're right.\n\nBig O (big Theta etc.) is used to measure functions. When you write f=O(g) it doesn't matter what f and g mean. They could be average time complexity, worst time complexity, space complexities, denote distribution of primes etc.\n\nWorst-case complexity is a function that takes size n, and tells you what is maximum number of steps of an algorithm given input of size n.\n\nAverage-case complexity is a function that takes size n, and tells you what is expected number of steps of an algorithm given input of size n.\n\nAs you see worst-case and average-case complexity are functions, so you can use big O to express their growth.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Java/Jsp programming quiz with respect to Big-O notation\r\n                \r\nQuestion: Given: a list of integers (duplicates are allowed); and integer N. Remove the duplicates from the list and find the N-th largest element in the modified list. Implement at least two different solutions to find N-th largest element with O(N*log(N)) average time complexity in Big-O notation, where N is the number of elements in the list.\n\nAccording to my understanding i can use Merge Sort, Heap Sort, Quick sort on the provided integer list with duplicates to find the N-th largest element with O(N*log(N)) average time complexity in Big-O notation. Is that correct ? \n\nAlso, what about duplicates in the list do i just add an extra line of code in the above mentioned algorithm to remove duplicates will that not affect the O(N*log(N)) average time complexity because Merge Sort, Heap Sort, Quick sort will only sort the list not delete duplicates. \n\nI am not looking for any code but just tips and ideas about how to proceed with the question ? I am using Java  also is there any predefined classed/methods in java that i can use to accomplish the task rather than me coding Merge Sort, Heap Sort, Quick sort on my own.\n\nMy aim is to complete the task keeping in mind O(N*log(N)) average time complexity. \n    ", "Answer": "\r\nYou first sort the list and then you remove illiterate over it to find all the duplicates. A second methode might be to loop over all elements and keep a tree structure in which for each element you first check if it's already present in the tree structure O(log(n)) and if is remove it else add it to the tree structure O(log(n)). This makes the whole algorithm O(n(log(n))).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation of Java Nested Loops\r\n                \r\nI am aware of the various rates of Big O such as O(n^2) and O(n), and have no problem determining the Big O value of simple nested for loops such as the following.\n\n```\nfor (int i = 0; i < n; i++)\n    for(int j = 0; j < n; j++)\n   //Simple Statement\n```\n\n\nThe following loop is clearly O(n^2), but how does one go about solving for Big O when the inner nested loop is dependent on the outer loop.\n Example\n\n```\nfor ( int i = 0; i < n; i++)\n    for (int j = n - 1; j >= i; j--)\n    //Simple Statement\n```\n\n\nwould T(n) simply be n*(n-1-i) ?\n\nThanks in advance for the help!\n    ", "Answer": "\r\nIt is still on the order of n^2.  You only care about the most significant term.  The second expression would look something line O(n^2 - an) where a is some sort of coefficient.  All you really care about is the n^2.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Time Complexity and Big O Notation [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 3 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI am stuck on a homework question. The question is as follows. \n\nConsider four programs - A, B, C, and D - that have the following performances.\n\n\nA: O(log n)\nB: O(n)\nC: O(n2)\nC: O(2n)\n\n\nIf each program requires 10 seconds to solve a problem of size 1000, estimate the time required by each program when the size of its problem increases to 2000.\n\nI am pretty sure that O(n) would just double to 20 seconds since we are doubling the size and this would represent a loop in Java that iterates n number of times. Doubling n would double the output. But I am completely lost on numbers 1, 3, and 4. \n\nI am not looking for direct answers to this question, but rather for someone to dumb down the way I can arrive at the answer. Maybe by explaining what each of these Big O notations is actually doing on the back end. If I understood the way that the algorithm is calculated and where all the elements fit into some sort of equation to solve for time, that would be awesome. Thank you in advance.\n\nI have spent weeks combing through the textbook, but it is all written in a very complicated matter that I am having a hard time digesting. Videos online haven't been much help either.\n    ", "Answer": "\r\nLet's have an example (the one that you don't have in your list): O(n^3).\nThe ratio between the sizes of your problems is 2: ```\n2000/1000 = 2```\n. The big-O notation gives you an estimation that if you have a problem of size ```\nn```\n the complexity of the problem of the size ```\n2n```\n would be... ```\n(2n)^3 = 8n^3```\n. That is 8 times higher than the original task.\n\nI hope that would help.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Still sort of confused about Big O notation\r\n                \r\nSo I've been trying to understand Big O notation as well as I can, but there are still some things I'm confused about. So I keep reading that if something is O(n), it usually is referring to the worst-case of an algorithm, but that it doesn't necessarily have to refer to the worst case scenario, which is why we can say the best-case of insertion sort for example is O(n). However, I can't really make sense of what that means. I know that if the worst-case is O(n^2), it means that the function that represents the algorithm in its worst case grows no faster than n^2 (there is an upper bound). But if you have O(n) as the best case, how should I read that as? In the best case, the algorithm grows no faster than n? What I picture is a graph with n as the upper bound, like \n\n\n\nIf the best case scenario of an algorithm is O(n), then n is the upper bound of how fast the operations of the algorithm grow in the best case, so they cannot grow faster than n...but wouldn't that mean that they can grow as fast as O(log n) or O(1), since they are below the upper bound? That wouldn't make sense though, because O(log n) or O(1) is a better scenario than O(n), so O(n) WOULDN'T be the best case? I'm so lost lol\n    ", "Answer": "\r\nBig-O, Big-Θ, Big-Ω are independent from worst-case, average-case, and best-case.\n\nThe notation f(n) = O(g(n)) means f(n) grows no more quickly than some constant multiple of g(n).\nThe notation f(n) = Ω(g(n)) means f(n) grows no more slowly than some constant multiple of g(n).\nThe notation f(n) = Θ(g(n)) means both of the above are true.\n\nNote that f(n) here may represent the best-case, worst-case, or \"average\"-case running time of a program with input size n.\nFurthermore, \"average\" can have many meanings: it can mean the average input or the average input size (\"expected\" time), or it can mean in the long run (amortized time), or both, or something else.\n\nOften, people are interested in the worst-case running time of a program, amortized over the running time of the entire program (so if something costs n initially but only costs 1 time for the next n elements, it averages out to a cost of 2 per element). The most useful thing to measure here is the least upper bound on the worst-case time; so, typically, when you see someone asking for the Big-O of a program, this is what they're looking for.\n\nSimilarly, to prove a problem is inherently difficult, people might try to show that the worst-case (or perhaps average-case) running time is at least a certain amount (for example, exponential).\nYou'd use Big-Ω notation for these, because you're looking for lower bounds on these.\n\nHowever, there is no special relationship between worst-case and Big-O, or best-case and Big-Ω.\nBoth can be used for either, it's just that one of them is more typical than the other.\n\nSo, upper-bounding the best case isn't terribly useful. Yes, if the algorithm always takes O(n) time, then you can say it's O(n) in the best case, as well as on average, as well as the worst case. That's a perfectly fine statement, except the best case is usually very trivial and hence not interesting in itself.\n\nFurthermore, note that f(n) = n = O(n2) -- this is technically correct, because f grows more slowly than n2, but it is not useful because it is not the least upper bound -- there's a very obvious upper bound that's more useful than this one, namely O(n). So yes, you're perfectly welcome to say the best/worst/average-case running time of a program is O(n!). That's mathematically perfectly correct. It's just useless, because when people ask for Big-O they're interested in the least upper bound, not just a random upper bound.\n\nIt's also worth noting that it may simply be insufficient to describe the running-time of a program as f(n). The running time often depends on the input itself, not just its size.  For example, it may be that even queries are trivially easy to answer, whereas odd queries take a long time to answer.\nIn that case, you can't just give f as a function of n -- it would depend on other variables as well.  In the end, remember that this is just a set of mathematical tools; it's your job to figure out how to apply it to your program and to figure out what's an interesting thing to measure. Using tools in a useful manner needs some creativity, and math is no exception.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How do different parameters affect Big O Notation for time complexity\r\n                \r\nFor example, if I have two parameters M and N inside my method and the time complexity turns out to be O(M+N), why do people say O(M+N) instead of O(N), which is simplified from O(2N)? When should we use multiple variables in our Big O Notation? Is there any reasoning behind it? How would different variables passed in for arguments affect the growth rate, why not combine them into one variable for big O? I cannot think of an instance of where different variables parameters passed in can affect run time. Just got into data structures and algorithms, so was wondering.\n    ", "Answer": "\r\nTime complexity represents how the runtime changes w.r.t the change in the input size.\nThe 2 variables passed to the method represent the size of the input and they both are 2 different dimensions.\nIf ```\nM >>> N```\n, then ```\nO(M + N)```\n is equivalent to ```\nO(M)```\n and if ```\nN >>> M```\n, then ```\nO(M + N)```\n is equivalent to ```\nO(N)```\n.\nWe can combine ```\nM```\n and ```\nN```\n only when ```\nM ~ N```\n, i.e., time complexity = ```\nO(2*N)```\n or ```\nO(2*M)```\n which is equivalent to ```\nO(N)```\n or ```\nO(M)```\n.\nOtherwise ```\nO(M + N)```\n is a precise way to represent the time complexity that takes care of all the above mentioned 3 cases on constraints of ```\nM```\n and ```\nN```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What's the Big-O-notation for this algorithm for printing the prime numbers?\r\n                \r\nI am trying to figure out the time complexity of the below problem.\n```\nimport math\ndef prime(n):\n    for i in range(2,n+1):\n        for j in range(2, int(math.sqrt(i))+1):\n            if i%j == 0:\n                break\n        else:\n            print(i)\n\nprime(36)\n```\n\nThis problem prints the prime numbers until 36.\nMy understanding of the above program:\nfor every n the inner loop runs for sqrt(n) times so on until n.\nso the Big-o-Notation is O(n sqrt(n)).\nDoes my understanding is right? Please correct me if I am wrong...\n    ", "Answer": "\r\nTime complexity measures the increase in number or steps (basic operations) as the input scales up:\n```\nO(1)     : constant (hash look-up)\nO(log n) : logarithmic in base 2 (binary search)\nO(n)     : linear (search for an element in unsorted list)\nO(n^2)   : quadratic (bubble sort)\n```\n\nTo determine the exact complexity of an algorithm requires a lot of math and algorithms knowledge. You can find a detailed description of them here: time complexity\nAlso keep in mind that these values are considered for very large values of n, so as a rule of thumb, whenever you see nested for loops, think O(n^2).\nYou can add a ```\nsteps```\n counter inside your inner for loop and record its value for different values of n, then print the relation in a graph. Then you can compare your graph with the graphs of ```\nn```\n, ```\nlog n```\n, ```\nn * sqrt(n)```\n and ```\nn^2```\n to determine exactly where your algorithm is placed.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Given these set of points, what would be the mathematical function for this and the Big(O) notation?\r\n                \r\n```\nX=2, y=1\nX=3, y=3\nX=4, y= 6\nX=5, y= 10\nX=6, y= 15\nX=7, y= 21\nX=8, y=28\n```\n\n\nI know that f(x) = f(x-1) + (x-1)\n\nBut...is that the correct mathematical function? What would Big O notation be?\n    ", "Answer": "\r\nThe correct (or at least, significantly more efficient than recursive) equation would be\n\n```\nf(x) = x * (x - 1) / 2\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "While we drop the constant in big O notation, does it matter in real-life situations?\r\n                \r\nWhile I understand that big O notation simply describes the growth rate of an algorithm, I'm uncertain if there is any difference in efficiency in real life between the following O(n) algorithms.\n\nTo print the value of a node in a linked list k places from the end of the list.\n\nGiven a node:\n\n```\n/* Link list node */\nstruct node\n{\n  int data;\n  struct node* next;\n};\n```\n\n\nSolution 1 O(n)\n\nThis solution iterates over the list twice, once to find the length of the list, and the second time to get to the end of the list - N.\n\n```\nvoid printNthFromLast(struct node* head, int n)\n{\n    int len = 0, i;\n    struct node *temp = head;\n\n\n    // 1) Count the number of nodes in Linked List\n    while (temp != NULL)\n    {\n        temp = temp->next;\n        len++;\n    }\n\n    // Check if value of n is not more than length of the linked list\n    if (len < n)\n      return;\n\n    temp = head;\n\n    // 2) Get the (n-len+1)th node from the begining\n    for (i = 1; i < len-n+1; i++)\n    {\n       temp = temp->next;\n    }\n    printf (\"%d\", temp->data);\n\n    return;\n}\n```\n\n\nSolution 2 O(n)\n\nThis solution only iterates over the list once. The ref_ptr pointer leads, and a second pointer (main_ptr) follows it k places behind. When ref_ptr reaches the end of the list, main_ptr should be pointing at the correct node (k places from the end of the list).\n\n```\nvoid printNthFromLast(struct node *head, int n)\n{\n  struct node *main_ptr = head;\n  struct node *ref_ptr = head;\n\n  int count = 0;\n  if(head != NULL)\n  {\n    while( count < n )\n     {\n        if(ref_ptr == NULL)\n        {\n           return;\n        }\n        ref_ptr = ref_ptr->next;\n        count++;\n     }\n\n     while(ref_ptr != NULL)\n     {\n        main_ptr = main_ptr->next;\n        ref_ptr  = ref_ptr->next;\n     }\n  }\n}\n```\n\n\nThe question is: Even though both solutions are O(n) while leaving big O notation aside, is the second solution more efficient that the first for a very long list as it only iterates over the list once?\n    ", "Answer": "\r\nYes. In the specific example where the same work occurs, a single loop is likely to be more efficient than looping over a set of data twice. But the idea of ```\nO(2n)```\n ~ ```\nO(n)```\n is that 2 ns vs 1 ns may not really matter. Big O works better to show how a piece of code might scale, e.g. if you made the loop ```\nO(n^2)```\n then the difference of ```\nO(n)```\n vs ```\nO(2n)```\n is much less than ```\nO(n)```\n vs ```\nO(n^2)```\n.\n\nIf your linked list contains terrabytes of data, then it might be worth reducing to the single loop iteration. A big O metric, in this case may not be sufficient to describe your worst case; you would be better off timing the code and considering the needs of the application.\n\nAnother example is in embedded software, where 1 ms vs 2 ms could be the difference between a 500 Hz and a 1 kHz control loop.\n\nThe lesson learned is that it depends on the application.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation Python function\r\n                \r\nWhat is the big O of the foo(A) function (where n equals the length of A)?\nAs far as i can tell the foo(4) statement is O(1) for each iteration of the recursion. Also i understand that the running time of the foo(A//8) statement will be logarithmic.\n\nTherefore, will the running time for the program be bigO(log(n)) ?\n\nThis function is used for practicing running times for a test.\n\n```\ndef foo(A):\n    if A <= 6:\n        return 7\n    return foo(A//8) + foo(4)\n```\n\n    ", "Answer": "\r\nYour program could be written as the following recurrsion:\n\n```\nT(n) = T(n/8) + C\n```\n\n\nApplying Master theorem where a=1 and b=8\n\nWe fall into the second case:\n\n```\nn^(log(1) base 8) = n^0 = 1\nC = ϴ(1). \n==> T(n) = O(n^(log(a) base b) * log(n)) = O(n^(log(1) base 8) * log(n))\n         = n^0 * log(n) = 1*log(n) = O(log(n))\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Can you do addition/multiplication with Big O notations?\r\n                \r\nI'm currently taking an algorithm class, and we're covering Big O notations and such. Last time, we talked about how\n\n```\nO (n^2 + 3n + 5) = O(n^2)\n```\n\n\nAnd I was wondering, if the same rules apply to this:\n\n```\nO(n^2) + O(3n) + O(5) = O(n^2)\n```\n\n\nAlso, do the following notations hold ?\n\n```\nO(n^2) + n\n```\n\n\nor\n\n```\nO(n^2) + Θ (3n+5)\n```\n\n\nThe later n is outside of O, so I'm not sure what it should mean. And in the second notation, I'm adding O and Θ .\n    ", "Answer": "\r\nAt least for practical purposes, the Landau ```\nO(...)```\n can be viewed as a function (hence the appeal of its notation).  This function has properties for standard operations, for example:\n\n```\nO(f(x)) + O(g(x)) = O(f(x) + g(x))\nO(f(x)) * O(g(x)) = O(f(x) * g(x))\nO(k*f(x)) = O(f(x))\n```\n\n\nfor well defined functions ```\nf(x)```\n and ```\ng(x)```\n, and some constant ```\nk```\n.\n\nThus, for your examples, \n\nYes:  ```\nO(n^2) + O(3n) + O(5) = O(n^2)```\n\nand:\n```\nO(n^2) + n = O(n^2) + O(n) = O(n^2)```\n,\n```\nO(n^2) + Θ(3n+5) = O(n^2) + O(3n+5) = O(n^2)```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Computational complexity (Big-O notation) of a geometrical weighted centroid among access points\r\n                \r\nI need to compute the computational complexity of the following equations using Big-O notations:\n\n\n\nHere, ```\nm```\n is the total number of access points (perhaps the number of iterations in terms of complexity, ```\ni```\n is individual access point). I learned about Big-O notation form this blog. Moreover, I found a similar question at this link. In the above equation, ```\nd```\n is a distance computed with 4 operations (multiply, subtraction, division, and power). As seen in the above equation, ```\nw```\n is computed with two operations (power and division). ```\nxw```\n and ```\nyw```\nare computed with two operations each (multiplication and division).\nHence, I've figured out the Big-O notation of above algorithm as: \n\n```\n4*[m]+2*[m]+2*[m]+2*[m]\n```\n\n\nIs it correct? Can it be approximated as ```\nO(m)```\n ? \nMoreover, the above algorithm (equations) is combined with next algorithm whose computational complexity is ```\nO(N)```\n, ```\nN```\n being the number of iterations. Here, ```\nN>>m```\n. What will be the final computational complexity in terms of Big-O notation? \n\nThank you.\n\nUPDATE: \n\nThe subscript ```\nw```\n with ```\nx```\n and ```\ny```\n is just a notation. It is not the iteration.  Iteration is only ```\nm```\n. Eg. ```\ni = 1,2,3,4,5,......,m```\n.The two algorithms operate in a pipeline fashion. For eg., at first the algorithm with ```\nm```\n iterations is operated, and the output of this algorithm is fed (as input) to next algorithm with ```\nN```\n iterations. So, when ```\nm```\n iterations (algorithm 1) are completed, it is followed by ```\nN```\n iterations (algorithm 2). My problem is similar to two loops that are not nested and have different iterations where ```\nN>>m```\n. \n\n```\nfor(int i=0; i<m; i++){\n   System.out.println(i);\n}\n\nfor(int j=0; j<N; j++){\n   System.out.println(j);\n}\n```\n\n\nWhat will be the final computational complexity? \n    ", "Answer": "\r\nYes, your sum from ```\ni=1```\n to ```\ni=m```\n takes ```\nO(m)```\n time. All other operations are constant, you dont have any sub-sum in sum or something like this.\n\nAbout your ```\nN```\n value, you did not provide enough information. We have to know how the ```\nN```\n is computed or how it is related to ```\nm```\n.\n\n\n\nAlso you should consider following constraint - can you provide some maximum value (even incredibly) big one that cannot never be reached by the numbers or equations? Usually the operation with numbers are considered constants, because they are made on 32 or 64bit numbers which always take constant time.\n\nHowever if you have some equations with incredible long numbers (like hundreds of characters long or more), the size of the numbers have to be considered in complexity. (You can probably imagine that multiplying two numbers that are milion characters long takes more than doing the same with 2x2)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Why is Big O notation for constant time execution O(1) instead of O(2)?\r\n                \r\nI understand that O(1) indicates an algorithm will take a constant amount of execution time regardless of the input dimensions. I also understand that O(N) indicates a linear increase in execution time proportional to the input dimension size.\n\nHowever, I only know this due to memorizing their definitions. I have no intuition in interpreting O(1) and instead just recall that this means constant time execution. I'm curious how I can understand intuition when reading big O notation.\n\nSo for constant time execution O(1), what does the 1 represent? Why not have it be O(2)? 2 is also a constant that is independent of input size N.\n    ", "Answer": "\r\nThe notation ```\nO(...)```\n means a set of functions. Roughly speaking, ```\nO(f(n))```\n is the set of functions which don't grow asymptotically faster than ```\nf```\n does.\n\nThe constant function ```\nf(n) = 1```\n doesn't grow at all, and neither does the constant function ```\nf(n) = 2```\n, so neither grows asymptotically faster than the other. Also, any other function grows asymptotically faster than ```\n1```\n if and only if it grows asymptotically faster than ```\n2```\n. So a function is in the set ```\nO(1)```\n if and only if it is in the set ```\nO(2)```\n, meaning they are the same set.\n\nThis means you can write ```\nO(2)```\n and it is strictly correct, but it is simpler (and hence conventional) to write ```\nO(1)```\n. You can think of this a bit like solving a maths problem where the answer is a fraction; you are expected to write the answer in its simplest form. Strictly speaking, 6/4 is equal to 3/2, but it is conventional to write 3/2.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Complexity of algorithm in Big O notation\r\n                \r\nConsider the following code snippet:\n\n```\nfor(int index = 1;index < N;index*=2){\n    int counter = 0;\n    while(counter < N){\n       counter++;\n    }\n}\n```\n\n\nDetermine its best - and worst - case runtime in the Big Theta notation as a function of N.\nOptions:\n\na) Best case: O(log(N)) - Worst case: O(N²)\n\nb) Best case: O(N . log(N)) - Worst case: O(N . log(N))\n\nc) Best case: O(N . log(N)) - Worst case: O(N²)\n\nd) Best case: O(N) - Worst case: O(N)\n\nI saw this question in a Java assessment and I really don't know the right answer.\nI answered the option D, but I don't know if it is right.\nCould you help me with that?\n    ", "Answer": "\r\nThis is ```\nO(NlogN)```\n both best and worst case.\n\nThe inner loop repeats itself everytime it starts exactly the same number of times, which is ```\nN```\n times.\n\nThe outer loop repeats itself ```\nlogN```\n times.\n\nSo, if you combine these, the total number of times ```\ncounter```\n is increased is:\n\n```\nT(n) = N + N + ... + N = N*logN\n```\n\n\nWhich gives you total of ```\nO(NlogN)```\n time.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Complexity of algorithm in Big O notation\r\n                \r\nConsider the following code snippet:\n\n```\nfor(int index = 1;index < N;index*=2){\n    int counter = 0;\n    while(counter < N){\n       counter++;\n    }\n}\n```\n\n\nDetermine its best - and worst - case runtime in the Big Theta notation as a function of N.\nOptions:\n\na) Best case: O(log(N)) - Worst case: O(N²)\n\nb) Best case: O(N . log(N)) - Worst case: O(N . log(N))\n\nc) Best case: O(N . log(N)) - Worst case: O(N²)\n\nd) Best case: O(N) - Worst case: O(N)\n\nI saw this question in a Java assessment and I really don't know the right answer.\nI answered the option D, but I don't know if it is right.\nCould you help me with that?\n    ", "Answer": "\r\nThis is ```\nO(NlogN)```\n both best and worst case.\n\nThe inner loop repeats itself everytime it starts exactly the same number of times, which is ```\nN```\n times.\n\nThe outer loop repeats itself ```\nlogN```\n times.\n\nSo, if you combine these, the total number of times ```\ncounter```\n is increased is:\n\n```\nT(n) = N + N + ... + N = N*logN\n```\n\n\nWhich gives you total of ```\nO(NlogN)```\n time.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How would you go about finding the complexity of this algorithm?\r\n                \r\n```\nfunction alg1(n)\n1 a=0\n2 for o=1 to n do\n3     for t=1 to o do\n4         for k=t to o+t do\n5         a=a+1\n6 return(a)\n```\n\n\nIf anyone could guide me to how you would find the worst-case here, and how to get the output a of alg1 as a function of n, I would be very grateful. Thanks!\n    ", "Answer": "\r\nWe can compute the exact number of increments this code executes. First, let's replace \n\n```\nfor k=t to o+t do\n```\n\n\nwith \n\n```\nfor k=1 to o+1 do \n```\n\n\nAfter this change, two inner loops looks like this\n\n```\nfor t=1 to o do\n    for k=1 to o+1 do\n```\n\n\nThe number of iterations of these loops is obviously ```\no*(o+1)```\n. The overall number of iterations can be calculated in the following way: \n\n\nWe can exclude coefficients and lower order terms of the polynomial when using big-O notation. Therefore, the complexity is O(n^3).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation: Encryption Algorithms\r\n                \r\nI am currently completing a dissertation concerning the encryption of data through a variety of cryptographic algorithms.\n\nI have spent much time reading journals and papers but as yet have been unable to find any record of their performance complexity.\n\nWould anyone have an idea of the Big-O complexity of the following algorithms?\n\n\nRSA\nDES\nTriple DES (Which I would expect to be of the same order as DES)\nAES\nBlowfish\n\n\nThank you in advance; if you could provide a link to a reputable and citable source if would be very much appreciated.\n    ", "Answer": "\r\nPartial answer: RSA Laboratories provides this analysis, archived from rsa.com, comparing RSA operations vs. DES. \n\n\n  How fast is the RSA algorithm? \n  \n  An \"RSA operation,\" whether encrypting, decrypting, signing, or\n  verifying is essentially a modular exponentiation. This computation is\n  performed by a series of modular multiplications.\n  \n  In practical applications, it is common to choose a small public\n  exponent for the public key. In fact, entire groups of users can use\n  the same public exponent, each with a different modulus. (There are\n  some restrictions on the prime factors of the modulus when the public\n  exponent is fixed.) This makes encryption faster than decryption and\n  verification faster than signing. With the typical modular\n  exponentiation algorithms used to implement the RSA algorithm,\n  public key operations take O(k^2) steps, private key operations take O(k^3) steps, and key generation takes O(k^4) steps, where k is\n  the number of bits in the modulus. ``Fast multiplication''\n  techniques, such as methods based on the Fast Fourier Transform (FFT),\n  require asymptotically fewer steps. In practice, however, they are not\n  as common due to their greater software complexity and the fact that\n  they may actually be slower for typical key sizes.\n  \n  The speed and efficiency of the many commercially available software\n  and hardware implementations of the RSA algorithm are increasing\n  rapidly; see http://www.rsasecurity.com/ for the latest figures.\n  \n  By comparison, DES (see Section 3.2) and other block ciphers are much\n  faster than the RSA algorithm. DES is generally at least 100 times as\n  fast in software and between 1,000 and 10,000 times as fast in\n  hardware, depending on the implementation. Implementations of the RSA\n  algorithm will probably narrow the gap a bit in coming years, due to\n  high demand, but block ciphers will get faster as well.\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O and Omega Notations\r\n                \r\nI was reading this question Big-O notation's definition.\nBut I have less than 50 reputation to comment, so I hope someone help me.\n\nMy question is about this sentence:\n\n\n  There are many algorithms for which there is no single function g such that the complexity is both O(g) and Ω(g). For instance, insertion sort has a Big-O lower bound of O(n²) (meaning you can't find anything smaller than n²) and an Ω upper bound of Ω(n).\n\n\nfor large n the O(n²) is an upper bound and Ω(n) is a lower bound, or maybe I have misunderstood?\ncould someone help me?\n\n\n    ", "Answer": "\r\n\n  has a Big-O lower bound of O(n²)\n\n\nI don't really agree with the confusing way this was phrased (since big-O is itself an upper bound), but what I'm reading here is the following:\n\nBig-O is an upper bound.\n\nThat is to say, ```\nf(n) ϵ O(g(n))```\n is true if ```\n|f(n)| <= k|g(n)|```\n as ```\nn```\n tends to infinity (by definition).\n\nSo let's say we have a function ```\nf(n) = n2```\n (which is, if we ignore constant factors, the worst-case for insertion sort). We can say ```\nn2 ϵ O(n2)```\n, but we can also say ```\nn2 ϵ O(n3)```\n or ```\nn2 ϵ O(n4)```\n or ```\nn2 ϵ O(n5)```\n or ....\n\nSo the smallest ```\ng(n)```\n we can find is ```\nn2```\n.\n\n\n\nBut the answer you linked to is, as a whole, incorrect - insertion sort itself does not have upper or lower bounds, but rather it has best, average and worst cases, which have upper and lower bounds.\n\nSee the answer I posted there.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Version of Big-O notation that includes parallelism?\r\n                \r\nThis question requires some set up.\n\nhttp://igoro.com/archive/big-oh-in-the-parallel-world/\n\n\"Method 1\" in this link details a notation for describing the effect of parallelism on algorithmic time complexity.  I really like this method of description, but this is the only place I have ever found it.  \n\nMy question is as follows: why is this notation not used more often?\n\nOne explanation is actually given in the method description itself:  \"To multiply two 100×100 matrices, we would need a machine with a 10,000 processors\".  This quote seems to suggest that 10,000 processors is totally unreasonable for anyone to expect.\n\nMy rebuttal:\n\nhttps://en.wikipedia.org/wiki/Manycore_processor\n\nThe largest super computer in the world right now, Taihu Light, contains a total of 40960*256 = 10485760 cores.  We don't all have access to a super computer like Taihu Light, but with cloud computing continuously gaining popularity, I see no reason why 10,000 core processors and beyond won't be widely available for use in the near future.  On top of that, the average GPU has 1000s of cores already.  They're more specialized than CPU cores, but that's besides the point.\n\nSince there are so few examples of this notation, I'll also provide my own example using an equivalent description, which I call big money notation.\n\nSay we have a perfectly balanced tree with a branching factor of b.  All nodes save one contain a boolean value of false, and the last contains a boolean value of true.  We want to find and return this true node.\n\nUsing a breadth first search, the the Big-O runtime to find the true node would be O(|V|+|E|) == O(b^d), where b is the branching factor and d is the depth of the tree.\n\nHowever, if we assume that we have infinitely many processing units, each with finite processing capabilities, the big-money notation is as follows:  $O(b^d -> d).  What this says is that on 1 processor, the algorithm would take O(b^d) time, but as the number of processors increases, the time complexity approaches d, the depth of the tree.  This is because at each false node, we can spawn b more processes to search each of that nodes b children.  Since we have infinite cores, all of these processes can work in parallel.  This is why I call it big money notation.  Time complexity decreases with more processing power, processing power increases as you throw more money at Amazon/Microsoft/Google/YourFavoriteCloudProvider.\n\nI'll reiterate my question again.  Why is notation similar to this not seeing more widespread use?  Thanks!\n    ", "Answer": "\r\nThe complexity class NC deals with problems that can be solved efficiently in a parallel setting. As far as I know, people who work on parallel algorithms do use the traditional O(.) notation and use expressions like \"NC algorithms\", so when you read their papers and see sentences like:\n\n\"We give a O(xxx) NC algorithm for SOMEPROBLEM.\"\n\nyou should interpret this as:\n\n\"We give a parallel algorithm for SOMEPROBLEM which solves it in time O(xxx) using O(yyy) processors.\"\n\n(where xxx and yyy vary of course). \n\nI don't recall seeing something that resembles your notation, but I'm not sure what it would bring to the table. Complexity classes are often partitioned according to some parameter and / or some measure of growth of that parameter (e.g., we have access to O(1) processors, or O(log n) processors, or ...) so as to classify problems with a degree of precision that your notation seems to lack if I interpret it correctly.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Java Big O Notation algorithm\r\n                \r\nIf anyone could help me with finding the Big O' for the code and please explain it to me.\n\n```\n    j = 1; \n    while ( j <= n)\n    {\n      j = j + 2\n    }\n```\n\n\nI know to pick a random n to help see how the loop works out. However, I have my classmates and they say the while(j <= n) executes n/2 times but I just don't see how they came to that answer.\n    ", "Answer": "\r\nWhen you need to find the asymptotic complexities for any algorithm, always look for the highest order of growth.\n\nOrder of growth is the highest power of the polynomial (for a P problem; NP problems are handled differently) for the algorithm. This order will define the complexity.\n\nYou have a a while loop, which is a linear loop. Linear means a power/degree of 1\nIt's linear because if you see the statements executing in the while loop, you can calculate it like this:\n\nTime to execute j=j+2. = C \nNo of times loop runs = any integer N\n\nTherefore, the total cost is C*N\nWhich is polynomial with degree 1\nTherefore the complexity is O(N)\n\nNote: It is true that the loop will run n/2 times. That's because you're incrementing j by 2 in every iteration from 1 to 3,5,7 etc. \nThe advice is NOT to pick numbers to find the complexity. Use the cost*iterations method mentioned above. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big o notation of java function\r\n                \r\nI want to calculate Big o of a function I need to make at school, but after a lot of research I cant calculate it accurately.\n```\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class MainClass {\n\n    public static void main(String args[]) {\n        MainClass main = new MainClass();\n\n        // This test should return false message\n        Boolean test = main.verificationMessage(\"Maxime\", \"eeeeemmiiabcdefodofrjvozomixam\");\n\n        if (test) {\n            System.out.println(\"Message OK\");\n        } else {\n            System.out.println(\"The enemy is at the door\");\n        }\n\n        // This test should return true message\n        test = main.verificationMessage(\"Maxime\", \"axemiMakkoepviivjsobcgdteryid\");\n\n        if (test) {\n            System.out.println(\"Message OK\");\n        } else {\n            System.out.println(\"The enemy is at the door\");\n        }\n        \n        // This test should return false message\n        test = main.verificationMessage(\"Maxime\", \"axemi\");\n\n        if (test) {\n            System.out.println(\"Message OK\");\n        } else {\n            System.out.println(\"The enemy is at the door\");\n        }\n\n    }\n\n    public boolean verificationMessage(String message, String lettersFinded) {\n        //If lettersFinded < message, then we didn't have to do anything, we know it's a false message\n        if(lettersFinded.length() >= message.length()){\n            HashMap<Character, Integer> occurenceMessage = new HashMap<>();\n            HashMap<Character, Integer> occurenceLetters = new HashMap<>();\n\n            makeMaps(message, occurenceMessage);\n            makeMaps(lettersFinded, occurenceLetters);\n\n            for (Map.Entry<Character, Integer> entry : occurenceMessage.entrySet()) {\n                // If a letter of the message map is not present or not found enought times\n                // in lettersFinded, then return false\n                if (occurenceLetters.getOrDefault(entry.getKey(), 0) < entry.getValue()){\n                    return false;\n                }\n            }\n            return true;\n        }\n        return false;\n        \n    }\n\n    // Create a hashmap to define the number of occurences of each character in the two strings\n    public void makeMaps(String s, HashMap<Character, Integer> map) {\n        for (char ch : s.toCharArray()) {\n            if (ch != ' ') {\n                if (map.containsKey(ch)) {\n                    // If char is present in charCountMap, incrementing it's count by 1\n                    map.put(ch, map.get(ch) + 1);\n                } else {\n                    // If char is not present in map putting this char to map with 1 as it's value\n                    map.put(ch, 1);\n                }\n            }\n        }\n    }\n\n}\n```\n\nThis is a simple function that compare two strings and verify that you can make the message with the lettersFinded.\nMakemaps count the occurence of each char of the string and put it in the hashmap.\nFor example :\n\"Tesst\" --> [T,1 ; e,1 ; s,2 ; t,1]\nMy idea is that it was Big o = O(n) where n equals message.length() because I iterate in the hasmap exactly n times, but I think that i'm wrong.\nDoes anybody know how to do it ?\nThank you in advance\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Javascript ES6 computational/time complexity of collections\r\n                \r\nWhat time complexity (in big-O notation) is provided by the ES6 specification for the Keyed Collections (Set, Map, WeakSet, and WeakMap)?\n\nMy expectation, and I expect that of most developers, is that the specifications and implementations would use widely accepted performant algorithms, in which case ```\nSet.prototype.has```\n, ```\nadd```\n and ```\ndelete```\n to all be O(1) in the average case. The same for the ```\nMap```\n and ```\nWeak–```\n equivalents.\n\nIt is not entirely apparent to me whether the time complexity of the implementations was mandated e.g. in ECMAScript 2015 Language Specification - 6th Edition — 23.2 Set Objects.\n\nUnless I misunderstand it (and it's certainly very possible I do), it looks the ECMA spec mandates that the implementations (e.g. ```\nSet.prototype.has```\n) are to use a linear time (O(n)) algorithm. It would strike me as exceedingly surprising that more performant algorithms would not be mandated or even permitted by the spec, and I would be very interested in an explanation for why this is the case.\n    ", "Answer": "\r\nRight from that very paragraph your linked to:\n\n\n  Set objects must be implemented using [mechanisms] that, on average, provide access times that are sublinear on the number of elements in the collection.\n\n\nYou will find the same sentence for Maps, WeakMaps and WeakSets.\n\n\n  It looks the ECMA spec mandates that the implementations (e.g. Set.prototype.has) are to use a linear time (```\nO(n)```\n) algorithm.\n\n\nNo:\n\n\n  The data structures used in this ```\nSet```\n objects specification is only intended to describe the required observable semantics of Set objects. It is not intended to be a viable implementation model.\n\n\nThe observable semantics are mostly related to the predictable iteration order (which still can be implemented efficient and fast). It is indeed expected by the specification that an implementation uses a hash table or something similar with constant access, though trees (with logarithmic access complexity) are allowed as well.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O - O(log(n)) code example\r\n                \r\nLike the Big O notation \"O(1)\" can describe following code: \n\n```\nO(1):\n\n    for (int i = 0; i < 10; i++) {\n        // do stuff \n        a[i] = INT;\n    }\n\nO(n):\n\n    for (int i = 0; i < n; i++) {\n        // do stuff \n        a[i] = INT;\n    }\n\nO(n^2):\n    for (int i = 0; i < n; i++) {\n        for (int j = 0; j < n; j++) {\n            // do stuff\n            a[i][j] = INT;\n        }\n    }\n```\n\n\n\nWhat code can O(log(n)) describe?\n\n\nAnother question:\n\n\nWhat solutions are there for \"Big O problems\" (what to do, when getting a lot of data as an input)?\n\n    ", "Answer": "\r\nClassic example:\n\n```\nwhile (x > 0) {  \n   x /= 2;  \n}  \n```\n\n\nThis will be:\n\n```\nIteration |   x\n----------+-------\n    0     |   x\n    1     |  x/2\n    2     |  x/4\n   ...    |  ...\n   ...    |  ...\n    k     |  x/2^k \n```\n\n\n2k = x → Applying log to both sides → k = log(x)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation for a pairing operation\r\n                \r\nI'm having a hard time wrapping my head around the big-O notation for a pairing operation. The question is pretty simple- Generate all possible pairs for a given list of numbers in an array. \n\nMy first guess is to have a nested for/foreach loop and generate the pairs. This is easy enough and I get that for every n, I analyze n other numbers and this gives me a complexity of n^2.\n\nNow, if I try to optimize this further and say that (1,4) is the same as (4,1), then for a sorted array like 1,2,3,4,5. I only run the pairing operation in a nested for loop this way\n\n```\nfor (i =0; i < count; i++) {\n    for (j = i + 1; j < count - 1; j++)\n    }\n}\n```\n\n\nIn this case, I only run through the array < n^2 times. For a sample size of 7 numbers, i ran through the loop 21 times to generate the pairs. I know that this cannot be a log-n operation and I'm tempted to say that this operation converges to n^2 but I don't remember enough from my math or theory classes to answer this definitively. How do I go about this problem?\n\nContext: I had an interview with a similar question and this was born out of an argument I had with my friend if a pairing operation from a list can ever be better than n^2. \n    ", "Answer": "\r\nYou are correct that you're doing fewer than n2 operations.  The question is how many fewer operations you are doing.\n\nLet's think about how many pairs there are in the array.  If each of the n numbers can be paired with (n - 1) other numbers, the total number of pairs possible is n(n - 1).  Each iteration of the original for loop generates one of these pairs, so the total number of pairs you generate is n2 - n, which is O(n2).\n\nNow, what about if you eliminate duplicate pairs by saying that (1, 4) and (4, 1) are the same?  In this case, note that half of the pairs you generate are going to be extraneous - you'll generate each pair twice.  This means that the number of pairs is (n2 - n) / 2.  This expression is less than n2, but notice that it is still O(n2) because big-O notation ignores constants.\n\nIn other words - you are right that you are generating fewer than n2 pairs, but the total number of pairs you're creating is still O(n2).\n\nMore generally - if you ever decrease the total amount of work that an algorithm does by some constant factor (say, you cut the work in half, or cut the work by a factor of 100), you have not changed the big-O runtime of the algorithm.  Big-O completely ignores constants.  In order to decrease the big-O runtime, you need to decrease the total amount of work by an amount that is more than a constant; say, by a factor of n, log n, etc.\n\nHope this helps!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation for Algorithm\r\n                \r\nI'm busy doing an assignment and I'm struggling with a question. I know I'm not supposed to ask assignment questions outright so I understand if I don't get straight answers. But here goes anyway.\n\nWe must calculate the run time complexity of different algorithms, the one I'm stuck on is this.\n\n```\nfor(int i = 1 ; i < n ; i++)\n    for(int j = 0 ; j < i ; j +=2)\n        sum++;\n```\n\n\nNow with my understanding, my first thought would be less than O(n2), because the nested loop isn't running the full n times, and still the j variable is incrementing by 2 each loop rather than iterating like a normal for loop. Although, when I did some code simulations with N=10, N=100, N=1000, etc. I got the following results when I outputted the sum variable.\n\n```\nN = 10 : 25, \nN = 100 : 2500,\nN = 1000 : 250000,\nN = 10000 : 25000000\n```\n\n\nWhen I look at these results, the O Notations seems like it should be much larger than just O(n).\n\nThe 4 options we have been given in the assignment are : O(1), O(n2), O(n) and O(logn). As I said earlier, I cannot see how it can be as large as O(n2), but the results are pointing to that. So I just think I don't fully understand this, or I'm missing some link.\n\nAny help would be appreciated!\n    ", "Answer": "\r\nBig O notation does not give you the number of operations. It just tells you how fast it will grow with growing input. And this is what you observe.\n\nWhen you increased input ```\nc```\n times, the total number of operations grows ```\nc^2```\n.\n\nIf you calculated (nearly) exact number of operations precisely you would get ```\n(n^2)/4```\n.\n\nOf course you can calculate it with sums, but since I dunno how to use math on SO I will give an \"empirical\" explanation. Simple loop-within-a-loop with the same start and end conditions gives ```\nn^2```\n. Such loop produces a matrix of all possible combinations for  ```\n\"i\"```\n and ```\n\"j\"```\n. So if start is ```\n1```\n and end is ```\nN```\n in both cases you get ```\nN*N```\n combinations (or iterations effectively).\n\nHowever, yours inner loop is for ```\ni < j```\n. This basically makes a triangle out of this square, that is the 1st ```\n0.5```\n factor, and then you skip every other element, this is another ```\n0.5```\n factor; multiplied you get 1/4.\n\nAnd ```\nO(0.25 * n^2) = O(n^2)```\n. Sometimes people like to leave the factor in there because it lets you compare two algorithms with the same complexity. But it does not change the ratio of growth in respect to ```\nn```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the big O notation here and how do I figure it out?\r\n                \r\nThere are 1805 elements in this array.  I am looping through each one of them and outputting the ID and destroying the follower.\nHow do I find the big O notation here?\nWould the better and quicker option be to use followers.destroy_all?\n```\nfollowers.each { |f| puts f.id; f.destroy! }; \n```\n\n    ", "Answer": "\r\nWhen you look at the source code of ```\ndestroy_all```\n then you will notice that it basically does the same as your example. It iterates over all records in the relation and calls ```\ndestroy```\n on each of them. I expect it still to be a bit faster than your version because calling ```\nputs```\n doesn't come for free. But both versions are ```\nO(n)```\n\nIf possible you might want to use ```\ndelete_all```\n instead which doesn't load all records into memory and doesn't call ```\ndestroy```\n on every single element. Instead, it deletes all records in the relation with one single SQL query which makes this method way faster than ```\ndestroy_all```\n. But of course ```\ndelete_all```\n doesn't trigger callbacks and therefore might not work for you.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the Big O notation for the str.replace function in Python?\r\n                \r\nWhat is the big Oh notation for ```\nstr.replace```\n function in Python ?\n\nIs it always O(n) ?\n\n```\nstr = \"this is string example\"\nprint str.replace(\"is\", \"was\")\n```\n\n\n\n```\nthwas was string example\n```\n\n\n    ", "Answer": "\r\nBig O notation is calculated at worst-case scenario, and Python sources for worst case do just 'find next position of substr, replace, and go further'.\nOne replacement does O(n) operations (copying the string).\nOne search, according to http://effbot.org/zone/stringlib.htm, in worst-case scenario does O(n*m) operations. \nAnd since it can be up to n/m replacements, in total it should be surprisingly O(n*n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Wondering the big O notation value of my script?\r\n                \r\nIt's been a little while since I estimated the big O notation on something and I can't seem to get a handle on this one. Basically my script runs through a list of points in the USA with latitude/longitude and finds a set that covers the country if those points are the center of circles with 100 mile radius. So like this:\n\n\nStart looping through the list, index i = 0.\nFind the distance between the ith point in the list and all points that follow it in the list.\nRemove any points that are within 100 miles\nRe-index the array\nIncrease the index by one \nIf i = list length, end, else, loop\n\n    ", "Answer": "\r\nYou algorithm is ```\nO(N^2)```\n, where ```\nn```\n. If I understand your description correctly, it goes something like this:\n\n```\nfor point A in array:\n  for point B in array:\n      d = dist(A,B)\n      //optionally remove from list\n```\n\n\nIn the worst case, every pair of points is more than 100 miles apart, so you end up checking the distance between each pair, hence ```\nO(N^2)```\n.\n\nNote you are doing at most ```\nn + (n-1) + (n-2) + ... + 2 + 1 = (n(n+1))/2```\n distance calculations, which  is still ```\nO(N^2)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation Explanation/Proof\r\n                \r\nI'm still having trouble understanding more complex proofs when determining if one function is big-o of another function, e.g. (f(n) = O(g(n)).\n\nExample:\n\nF(n) http://upurs.us/image/63058.gif\n\nG(n) http://upurs.us/image/63059.gif\n\nI realize that we want the proof to be satisfied under the conditions, S.T f(n) <= C1 * g(n) when n > b. I've watched countless tutorials and can't grasp this concept. In the listed example, how would I go about choosing the constant and using that information to finalize the proof? Thank you.\n    ", "Answer": "\r\nAll you have to do is consider the big terms of ```\nf(n)```\n and ```\ng(n)```\n as ```\nn```\n goes to ```\ninfinity```\n. \n\nNow consider ```\nf(n)```\n:  \n\n\n\nand ```\ng(n)```\n: \n\n\nnow the conclusion:\n \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation : limited input\r\n                \r\nAs an exercise, I am trying to set Monte Carlo Simulation on a chosen ticker symbol.\n```\nfrom numpy.random import randint\nfrom datetime import date\nfrom datetime import timedelta\nimport pandas as pd\nimport yfinance as yf\nfrom math import log\n\n# ticker symbol\nticker_input = \"AAPL\"  # change\n\n# start day + endday for Yahoo Finance API, 5 years of data\nstart_date = date.today()\nend_date = start_date - timedelta(days=1826)\n\n# retrieve data from Yahoo Finance\ndata = yf.download(ticker_input, end_date,start_date)\nyf_data = data.reset_index()\n\n# dataframe : define columns\ndf = pd.DataFrame(columns=['date', \"ln_change\", 'open_price', 'random_num'])\n\nopen_price = []\ndate_historical = []\n\nfor column in yf_data:\n    open_price = yf_data[\"Open\"].values\n    date_historical = yf_data[\"Date\"].values  \n\n# list order: descending\nopen_price[:] = open_price[::-1]\ndate_historical[:] = date_historical[::-1]\n\n# Populate data into dataframe\nfor i in range(0, len(open_price)-1):\n    # date\n    day = date_historical[i]\n    \n    # ln_change\n    lnc = log(open_price[i]/open_price[i+1], 2)\n\n    # random number\n    rnd = randint(1, 1258)\n\n    # op = (open_price[i]) open price\n    df.loc[i] = [day, open_price[i], lnc, rnd]\n```\n\nI was wondering how to calculate Big O if you have e.g. nested loops or exponential complexity but have a limited input like one in my example, maximum input size is 1259 instances of float number. Input size is not going to change.\nHow do you calculate code complexity in that scenario?\n    ", "Answer": "\r\nIt is a matter of points of view. Both ways of seeing it are technically correct. The question is: What information do you wish to convey to the reader?\nConsider the following code:\n```\nquadraticAlgorithm(n) {\n    for (i <- 1...n)\n        for (j <- 1...n)\n            doSomethingConstant();\n}\nquadraticAlgorithm(1000);\n```\n\nThe function is clearly O(n2). And yet the program will always run in the same, constant time, because it just contains one function call with n=1000. It is still perfectly valid to refer to the function as O(n2). And we can refer to the program as O(1).\nBut sometimes the boundaries are not that clear. Then it is up to you to choose if you wish to see it as an algorithm with a time complexity as some function of n, or as a piece of constant code that runs in O(1). The importance is to make it clear to the reader how you define things.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation depend on matrix size only or data type(integer, Boolean) as well?\r\n                \r\nDoes the big O notation depend on the dimension of the matrix only? or it will depend on data type(integer or boolean) as well. \n\nSuppose if I have two matrices of size mat=M x N and I have to multiply them. If the data type of matrix is integer the run time on the computer is different but if the data type is bolean[+1,-1] the run time on the computer is reduced. How I can write big O notation by taking into account data type as well.\n    ", "Answer": "\r\nYou don't take it into account; the big-O is the same in all cases. Big-O is about scaling; how much more work do you do as the size of the input grows, relative to the baseline work?\n\nUnless the algorithm scales to arbitrary bit lengths for each element of the matrix, the difference between boolean and integer would be a constant multiplier, and constant multipliers are ignored in big-O notation. For a choice of 16 vs. 8 vs. 2 vs. 1, it's not really arbitrary growth; 16 bits may take 16 times as long as 1 bit for a 10x10 matrix, but if it also takes 16 times as long as 1 bit for a 20x20, the scaling factor is the same.\n\nNote: On real hardware, there may be more noticeable changes in performance (matrices that stop fitting in a cache line, or in cache at all can change behavior significantly), but big-O is about idealized computing devices, with no such limitations.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation - O(N) Fibonacci Generator\r\n                \r\nJust brushing up on my big O in advance of an interview.\n\nOn pages 53 and 54 of cracking the coding interview (6th ed), in the chapter on big O, you will see example 15, which appears as follows.\n\n```\nvoid allFib(int n){\n    int[] memo = new int[n+1];\n    for(int i=0; i<n; i++){\n        System.out.println(i + \":\" + fib(i,memo));\n    }\n}\n\nint fib(int n, int[] memo){\n    if(n<=0) return 0;\n    else if(n==1) return 1;\n    else if(memo[n]>0) return memo[n];\n\n    memo[n]=fib(n-1,memo)+fib(n-2,memo);\n\n    return memo[n];\n}\n```\n\n\nLong story short, this is a standard approach, which uses memoisation to ensure fib values only have to be calculated once.  But they still have to be calculated once and calculating them is O(2^N) temporal complexity.\n\nThe book is saying that because we can retrieve the fib values in constant time from the memo that the algorithm is O(N).\n\nIt does not explain why it is we can ignore the fact that you do have to derive the values once from an exponential function.  Is amortisation being applied?\n\nI trust the book, but the explanation provided isn't helping me to understand why the temporal complexity is O(N) in this case.\n\nPost Edit:\n\nLet me put this another way.\n\nHow could calling fib n times be O(N)...\n    int[] memo = new int[n+1];\n\n```\nfor(int i=0; i<n; i++){\n    System.out.println(i + \":\" + fib(i,memo));\n}\n```\n\n\n...when calling it once is O(N^2)?\n\n```\nfib(n,memo)\n```\n\n\nFinal Edit:\nThanks all.  I have my answer.  Even the single call to the fib method benefits from memoisation and thus is not O(N^2), it too is O(N).\n    ", "Answer": "\r\nOne way to look at this is that in ```\nmemo[n]=fib(n-1,memo)+fib(n-2,memo)```\n\nwhen ```\nfib(n-1, memo)```\n returns, the value needed by ```\nfib(n-2,memo)```\n is already stored in ```\nmemo```\n ( ```\nif(memo[n]>0) return memo[n]```\n ), this is repeated at every recursion level.  \n\nSo instead of the call graph looking like this (for the naive version):\n\n```\n        *\n       * *\n      * * * * \n  * * * * * * * *\n```\n\n\nIt looks like this\n\n```\n         *\n        * * \n       * *   \n      * *  \n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation for infinity worst case\r\n                \r\nSo I wrote a small program that tries to return an array of random, yet unique, values. Meaning that the values will be random but they will not repeat.\nHere is the code\n```\nconst randArr = (length) => {\n  let a = [];\n  let usedNums = {};\n\n  for (let i = 0; i < length; ) {\n    let randNum = Math.ceil(Math.random() * (length * 1.5));\n    if (!usedNums[randNum]) {\n      a.push(randNum);\n      usedNums[randNum] = randNum;\n      i++;\n    }\n  }\n  \n  return a;\n}; // makes a random array of size \"length\" which contains unique values in a random pattern.\n```\n\nClearly the time complexity and space complexity depends on the input but if you look closely, 'I' is only increased if the number is not already in the newly created array. This means that in the worst case scenario, even though the numbers are somewhat random, you could get the same numbers over and over again which means that this could stretch all the way to infinity.\nYou can tweak the code to make this happen as well\n```\nlet randNum = Math.ceil(Math.random() * (length * 1.5) % 1);```\n Notice how every single time you will get the same number which means that 'I' will never get increased thus the loop will keep running forever.\nDue to the way this works, and the fact that I am new to speaking about programs in terms of time complexities and space complexities, I am unsure what the big O notation for this code would be. Can someone explain please?\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Subtraction is valid in big O-notation?\r\n                \r\nIf we have ```\nf(n)=O(g(n))```\n and ```\nh(n)=O(x(n))```\n, then is it true that ```\nf(n)-h(n)=O(g(n)-x(n))```\n? \n\nI do not think so, because the inequality in the definition of O notation gets reversed while subtracting ```\nx(n)```\n from ```\ng(n)```\n , that is; since ```\nf(n)<=c_1g(n)```\n for some constant ```\nc_1```\n and ```\nh(n)<=c_2x(n)```\n for another constant ```\nc_2```\n, it is not clear whether we can directly subtract the terms. Any hints? Thanks beforehand.\n    ", "Answer": "\r\nYou are correct. ```\nf(n)```\n being ```\nO(g(n))```\n and ```\nh(n)```\n being ```\nO(x(n))```\n does not imply ```\nf(n)-h(n)```\n being ```\nO(g(n)-x(n))```\n.\n\nFor a counter example just take ```\ng(n) = x(n) = 1```\n and ```\nf(n) = 2```\n and ```\nh(n) = 1```\n. This clearly satisfies the assumptions, but ```\nf(n)-h(n) = 1```\n, while ```\ng(n)-x(n) = 0```\n and so the claim doesn't hold.\n\nThe only thing you can say about the difference is that it is ```\nO(max(|h(n)|, |x(n)|))```\n.\n\nNote however, that the same is true for addition if you allow negative values on the functions. In programming-related contexts it is generally assumed that the function only takes positive values and in that case subtraction only makes partially sense.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Worst case time complexity analysis pseudocode\r\n                \r\nCould someone help me out with the time complexity analysis on this pseudocode?\nI'm looking for the worst-case complexity here, and I can't figure out if it's O(n^4), O(n^5) or something else entirely. If you could go into detail into how you solved it exactly, it would be much appreciated.\n\n```\nsum = 0\nfor i = 1 to n do\n   for j = 1 to i*i do\n       if j mod i == 0 then\n          for k = 1 to j do\n              sum = sum + 1\n```\n\n    ", "Answer": "\r\nFirst loop: ```\nO(n)```\n\n\nSecond loop: ```\ni```\n is in average ```\nn/2```\n, you could have an exact formula but it's ```\nO(n²)```\n\n\nThird loop happens ```\ni```\n times inside the second loop, so an average of ```\nn/2```\n times. And it's ```\nO(n²)```\n as well, estimating it.\n\nSo it's ```\nO(n*n²*(1 + 1/n*n²))```\n, I'd say ```\nO(n^4)```\n. The ```\n1/n```\n comes from the fact that the third loop happens roughly ```\n1/n```\n times inside the second one.\n\nIt's all a ballpark estimation, with no rigorous proof, but it should be right. You could confirm it by running code yourself.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Complexity of different operations on different data structures according to the Big-O notation\r\n                \r\nI was reading about big O notation in java programming. I found the following table it shows different big O for different data structures.\n\n\n\nhttp://bigocheatsheet.com/\n\nMy questions are: \n\n\nIf I want to delete an item in an array, is it ```\nO(n^2)```\n? (search and delete)\nIf I want to delete an item in a stack, is it ```\nO(n)```\n?\nWhich one is more effective, is it a single linked list or double single list?\nIn what case is that the insert operation is ```\nO(1)```\n or ```\nO(n)```\n in a hash table?\nIf I want to delete an item in a binary search tree, is it ```\nO(log(n)*log(n))```\n while insert is just ```\nO(log(n))```\n?\n\n\nThanks.\n    ", "Answer": "\r\n\nIf you want to delete an item from an array, first you have to search it (takes ```\nO(n)```\n) and then you have to shift the items to fill the gap (takes ```\nO(n)```\n). So, the effective time complexity is ```\nO(n)```\n.\nIf you want to delete an item from a stack, you can only delete the topmost element (a property of the stack data structure). So, it can be done in ```\nO(1)```\n.\nWhich type of linked list is more effective depends on your requirements. For instance, if you want to save memory, then you might not use doubly-linked lists due to the overhead of maintaining an extra pointer reference. But, if you want to be able to traverse your list in both directions, you have to use a doubly-linked list. Implementation of a doubly-linked list is a bit lengthy as you have to perform more pointer operations, but, many operations (like deleting the last element) can be performed with ease.\nWe prefer to use Hash tables because we can achieve ```\nO(1)```\n insertion and search time. ```\nO(n)```\n insertion time is taken by almost all other data structures. (```\nO(n)```\n class includes ```\nO(log n)```\n, ```\nO(1)```\n, etc.). Suppose we use separate chaining in the hash table where each chain is a sorted linked list. Then, for each insertion, we need to search the linked list to find the right position to insert (just like Insertion Sort), which will take ```\nO(n)```\n worst-case time.\nIf you have to delete an element from a BST, first you have to search it (takes ```\nO(log n)```\n in average case) and then you have to replace the deleted node with its inorder successor or predecessor (takes ```\nO(log n)```\n). So, the effective average-case deletion time is ```\nO(log n)```\n.\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation with nested for loop\r\n                \r\nI have the code below and am trying figure out the big O worst case running time for it. I think that the first loop is O(log N), but I am not sure what the second loop is. I thought maybe it was O(N) but that didn't seem right. Any insights would be very helpful.\n\n```\n    for(int jump = inList.size(); jump > 0; jump/= 2) {\n        for(int i = 0; i < inList.size(); i = ++i * jump) {\n```\n\n    ", "Answer": "\r\nThis is going to be a O(log(n)) algorithm because the outer loop is clearly O(log(n)) and for large enough N the inner loop is going to finish executing in constant (2) iterations because n/2 * (n+1)/4 = (n^2+n)/8 > n for n > 6. For all values greater than 6 the inner for loop always iterates twice, big O deals with large cases (approaching infinity) in which case the inner loop is constant.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "how to calculate time complexity in big O notation of this algorithm\r\n                \r\nI need help finding time complexity of this function in big O notation:\n\n```\nint myfunction(bool exists)\n{\n    int var=0; int k,j,n;\n    if (exists)\n       for(k=1; k<=n; k*=2)\n          for(j=1; j<=k; j++)\n             var++;\n    else\n       for(k=1; k<=n; k*=2)\n          for(j=1; j<=n; j++)\n             var++;\n    return var;\n}\n```\n\n\nfrom what I understood from the book, in cases like this when we have if-else blocks, the overall complexity of the algorithm is the worse case of the both blocks, so I calculated the else block complexity, which does have a complexity of O(log2(n)), correct me if I'm wrong, but I'm having trouble finding out the time complexity of the if block, it seems that it takes less time, but I can't determine how much.\n    ", "Answer": "\r\nA formal answer would be, separating the IF block and the ELSE block:\n\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What makes two functions have fundamentally different efficiency times in Big O notation\r\n                \r\nToday I was posed with the problem of ordering several functions based on efficiency and computing mathematically which functions would have the same Big O notation. Long story short, I ended up getting into a disagreement with my classmate as to whether or not there is a fundamental difference between a function with a run time of 2^n and on with a run time of n^(n/2).\n\nI was taught that in Big O notation, leading coefficients end up being insignificant as n approaches infinity because they are just a vertical scaling of the same parent function, when n is huge, 6*n isn't really THAT different from n, as they both have the same parent growth rate, which makes sense. My argument followed that because this vertical scaling of the function was insignificant because it was simply a child function of the same thing, any constant transformation made would retain the overall parent function, so the child would have the same base growth rate and end up being simplified to the same notation (in this case O(2^n)).\n\nMy classmate made the point that \n\n```\n2^(n/2) = (2^(1/2))^n = sqrt(2)^n\n```\n\n\n....and because 1.414^n is quite smaller than 2^n as n approaches infinity, then it should be noticeably larger.\n\nMy classmate then proposed that two functions have different Big O notations if\n\n```\nlim((f(n)'s efficiency)/(g(n)'s efficiency)) as n->infinity \n    is either infinity (f(n) is bigger), or 0 (g(n) is bigger)\n\nAnd because ((2^n) / (2^(n/2))) = ((2^(n/2) * 2^(n/2)) / (2^(n/2))) = \n    2^(n/2), approaches infinity, they must have a rate of change that is\n    fundamentally different.\n```\n\n\nMy classmate's theory of what makes two algorithms have different Big O notation clearly makes sense for linear vs linear, linear vs quadratic, and just about any other common situation, but then again, so does mine. Something that is a transformed linear function (meaning it is translated and or scaled vertically or horizontally, but not scaled by a negative number, zero, et cetera) will always have a Big O notation of O(n), because it is linear. Any quadratic function will end up being O(n^2) because the constants will become insignificant and only the n^2 term will matter, because it is a transformed quadratic. (works for other stuff too, you get it) Obviously, x^2 is fundamentally different from x^3, because you cannot scale a quadratic to match a cubic function, so they must be different enough to get their own categories in Big-O.\n\nClearly [at least] one of us is thinking about this the wrong way. I mean, either O(2^(n/2)) gets simplified to O(2^n) or it doesn't, right?\n\nSo which one (if either) of us is right, and why is the other wrong, and most importantly, how do we tell if two inefficiencies are fundamentally different in a situation like this?\n\nThanks!\n    ", "Answer": "\r\nYour initial question compares 2^n and n^(n/2). It's easy to see than n^(n/2) is slower because at n=4 they are equal (16), and from that point on n^(n/2) is increasingly larger.\n\n2^(n/2) is smaller than both of these. Effectively, where c is a constant and nc is n * a constant, nc^c < c^nc < nc^nc.\n\nIn the case of 2^n vs 2^(n/2), 2^n has a larger O because no constant can make 2^(n/2) keep up with it as n increases.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the upper bound of function f(n) = n in Big-O notation and why? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     This question does not appear to be about programming within the scope defined in the help center.\r\n                \r\n                    \r\n                        Closed 7 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI was reading the book Algorithm by Karumanchi .In one of the example it is given that for function f(n)= n the big-o notation is O(n^2).But why is that and why isn't it O(n) with c=2 and n0=1.\n    ", "Answer": "\r\nf(n) = O(g(n)) sets an upper limit to the function f(n). But this upper limit need not be tight.\nSo for the function f(n) = n, we can say f(n) = O(n),\nalso f(n) = O(n^2), f(n) = (n^3) and so on. The definition of Big-Oh doesn't say anything about the tighness of the bound. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "About Big O Notation (N*N?)\r\n                \r\nI have a few problems that I'm trying to find the Big O for. The ones that are confusing me are the (N*N) problems:\n\n```\nfor (i=1, sum=0, i <= N; i++) {\n  for (j=1; j <= N*N; j++) {\n     sum++;\n  }\n}\n```\n\n\nI'm guess that it's O(N^3) since (N*N) could be representing two loops.\n\n```\nfor (i=1, sum=0, i <= N; i++) {\n  for (j=1; j <= N*N; j++) {\n     for (k=1; k<=j; k++) {\n       sum++;\n     }\n  }\n}\n```\n\n\nIf so, then this one would be O(N^4)?\n    ", "Answer": "\r\n```\nfor (i=1, sum=0, i <= N; i++) {  // loop over i\n  for (j=1; j <= N*N; j++) {     // loop over j, no dependency\n     sum++;\n  }\n}\n```\n\n\nThe inner loop over ```\nj```\n is independent of ```\ni```\n and has complexity ```\nO(N*N)```\n.\nThe outer loop does this ```\nN```\n times, hence ```\nO(N^3)```\n in total.\n\n```\nfor (i=1, sum=0, i <= N; i++) {  // loop over i\n  for (j=1; j <= N*N; j++) {     // loop over j, no dependency\n     for (k=1; k<=j; k++) {      // loop over k, dependent on j\n       sum++;\n     }\n  }\n}\n```\n\n\nThe loop over ```\nk```\n is dependent on ```\nj```\n. ```\nj```\n loops independently over the integers up to ```\nN*N```\n.\nThe sum ```\n1 + 2 + ... + N * N```\n is equal to ```\n(N * N + 1) * N * N / 2```\n which is ```\nO(N^4)```\n.\nNow again, the loop over ```\ni```\n repeats that ```\nN```\n times, hence total complexity is ```\nO(N^5)```\n.\n\nFor calculating big O, always start from the innermost loop and take care of dependencies!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation using the recursion method [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Big O of Recursive Methods\r\n                            \r\n                                (2 answers)\r\n                            \r\n                    \r\n                Closed 8 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nHow to find the Big O for the following recursive function using the recursive method:\n\n```\nT(n)=(n-1)T(n-1)+(n-1)T(n-2)\n```\n\n    ", "Answer": "\r\nAnyway, I tried to solve this case using the classic recursive relation methodology.\nIt's all about observing if a pattern exists:\n\n\n\nVery expensive algorithm (Enemies of computer science are factorial and exponential orders of growth).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O notation calculation, O(n) * O(log n) = O(n log n)\r\n                \r\nI need to design an algorithm that is able to do some calculations in given O notation. It has been some time since I last calculated with O notation and I am a bit confused on how to add different O notations together. \n\n```\nO(n) * O(log n) = O(n log n)\n\nO(n) + O(n) = O(2n) = O(n)\n\nO(n) * O(log n) + O(n log n) = O(n log n) + O(n log n) = O(n log n)\n```\n\n\nAre these correct? What other rules have I overlooked?\n    ", "Answer": "\r\nThe rule for multiplication is really simple:\n\n```\nO(f) * O(g) = O(f * g)\n```\n\n\nThe sum of two ```\nO```\n terms is harder to calculate if you want it to work for arbitrary functions.\nHowever, if ```\nf ∈ O(g)```\n, then ```\nf + g ∈ O(g)```\n.\n\nTherefore, your calculations are correct, but your original title is not;\n\n```\nO(n) + O(log n) = O(n)\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation - Two arrays removing inner array elements\r\n                \r\nI am a little confused as to what the big O notation for an example like this (this is not a real programming language, just a quick example):\n\n```\nfor each entry in arraylist1\n    for each entry in arraylist2\n       if condition met\n          remove element from arraylist2 \n```\n\n\nEven if this code have two loops I don't think it is O(n^2), since for each entry I can remove n elements, which will reduce the number of iterations drastically. \n\nI usually use this kind of algorithm when I already processed or found the elements on the array for a given criteria but need to continue.\n\nEdit: Now that I think of, the worst case scenario would only remove all elements on the last iteration of the first loop resulting in a \"true\" O(n^2). \n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Logarithms in Computer Science for Big O Notation? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 5 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI have always had this question in my head, and have never been able to connect these two concepts so I am looking for some help in understanding Logarithms in Computer Science with respect to Big-O notation and algorithmic time complexity. I understand logarithms as a math concept as being able to answer the question, \"what number do I need to raise this base to exponentially to get X?\". For example, log2(16) tells us that we need to raise 2 to the 4th power to get 16. I also have a memorization-level understanding that O(log n) algorithms are faster than O(n) and other slower algorithms such as those that are exponential and that an example of an O(log n) algorithm is searching a balanced binary search tree.\n\nMy question is a little hard to state exactly, but I think it boils down to why is searching a balanced BST logarithmic and what makes it logarithmic and how do I relate mathematical logarithms with the CS use of the term? And a follow-up question would be what is the difference between O(n log n) and O(log n)?\n\nI know that is not the clearest question in the world, but if someone could help me connect these two concepts it would clear up a lot of confusion for me and take me past the point of just memorization (which I generally hate).\n    ", "Answer": "\r\nWhen you are calculating Big O notation, you are calculating the complexity of an algorithm as the problem size grows. \n\nFor example, when performing a linear search of a list, the worst possible case is that the element is either in the last index, or not in the list at all, meaning your search will perform N steps, with N being the number of elements in the list. O(N). \n\nAn algorithm that will always take the same amount of steps to complete regardless of problem size is O(1). \n\nLogarithms come into play when you are cutting the problem size as you move through an algorithm. For a BST, you start in the middle of a list. If the element to search for is smaller, you only focus on the first half of the list. If it is larger, you only focus on the second half. After only one step, you just cut your problem size in half. You continue cutting the list in half until you either find the element or can not proceed. (Note that a binary search assumes the list is in order) \n\nLet's consider we are looking for 0 in the list below (A BST is represented as an ordered list):\n[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n\nWe first start in the middle: 7\n0 is less than 7 so we look in the first half of the list: [0,1,2,3,4,5,6] \n\nWe look in the middle of this list: 3\n0 is less than 3 and our working list is now: [0,1,2]\n\nSo we look at 1. 0 is less than 1, so our list is now [0]. \n\nGiven we have a working list of just 1 element, we are at the worst case. We either found the element, or it does not exist in the list. We were able to determine this in just four steps, looking at 7,3,1, and 0. \n\nThe problem size is 16 (number of elements in the list), which we represent as N. \nIn the worst case, we perform 4 comparisons (2^4 = 16 OR Log base 2 of 16 is 4)). \n\nIf we took a look at a problem size of 32, we would perform only 5 comparisons (2^5 = 32 OR Log base 2 of 32 is 5). \n\nTherefor, the Big O for a BST is O(logN) (note that we use a base 2 for logarithms in CS).\n\nFor O(NlogN), the worst case is the problem size times the calculation of it's logarithm. Insertion sort, quick sort, and merge sort are all examples of O(NlogN)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Complexity of algorithms other than asymptotic (Big-O - notation)\r\n                \r\nWhat are the mostly adopted concept, other than asymptotic complexity (Big-O notation), used to evaluate algorithms?\n\nExample:\n\nSupposing I have the following algorithm that calls a function, func, with complexity O(1). Then this algorithm would have complexity O(N1 x N2). But if I knew beforehand that N1 is limited [1,5] then the worst case complexity would be O(5 x N2) which, by definition, is also O(N2).\n\n```\nfor i in range(N1):\n    for j in range(N2):\n        func(i,j)\n```\n\n\nIn case I can come across a different implementation of this algorithm using a function func2, again with complexity O(1), but now using different outer loop range, N3. This algorithm would be expected to be O(N3 x N2). However, if I knew the range of N3 was [10,50], then the worst case complexity would be O(50 x N2) which is again O(N2).\n\n```\nfor i in range(N3):\n    for j in range(N2):\n        func2(i,j)\n```\n\n\nQuestion:\n\nSo, this is a simple to demonstrate that asymptotic notation is useful but maybe is not the most suitable comparison method for some more specific cases. How can I compare these two algorithms? What are the most commonly adopted methods? Is using just the number of iterations required for the algorithm a technically rigorous metric?\n\nAny recommended reference?\n    ", "Answer": "\r\nYour question is very large. Read about algorithm complexity and metrics.\n\nYour problem is that even if you don't use Big-O (there are many others small-o Big-omega, small-omega, theta, etc) you will not be able to easily compare algorithms the way it seems you want! The main reason is that you must first clearly define what you want to measure, and this is not easy. In general, you don't want details. Why? Because we know that we can always linearly accelerate any algorithm (roughly said, specials cases not relevant here). So the multiplicative constant is not relevant.\n\nNow, what you want is (neither worst-case, best-case or average-case) more a prediction function for the running time that is somehow related to an exact complexity. But even there, there are many pitfalls and traps. You may think that two algorithms that have exact complexity of, say, 3n+45 will run as fast on the same platform, but this may be false! If you don't define exactly what you count, then this may be false. One may use 3n multiplications and the other 3n additions (or more subtle mixing of), and running time of multiplication and addition is generally not the same. Worst, as architecture may use pipelines, prediction and others runtime optimizations this may drastically change the running time. Even the environment platform may introduce serious bias, think about virtual memory, caches, various compiler optimizations, etc.\n\nSo, the answer is: it depends on what you want to predict... This is why there are so many complexity measures.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Does declaring a variable count towards big O notation?\r\n                \r\nLets say in Java I declare a new variable:\nint k = 0;\nWill this count towards the O notation, or do I ignore it for the O notation?\n    ", "Answer": "\r\nBig O notation ignores the constants.\n\n```\nint k = 0;\n```\n\n\nis a single constant variable(in terms of memory).\n\nBut following loop would have  ```\nO(n)```\n complexity\n\n```\nfor(int k=0;k<n;k++){}\n```\n\n\nAs it would run ```\nk++```\n ```\nn```\n times.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How can I unfold the recurrence: T(n)=2T((n+2)/3)\r\n                \r\nI'm trying to solve this recurrence, but I don't know how to unfold it.\n\n```\nT(n)=2T((n+2)/3) + 1\n```\n\n\nCan I ignore that \"+2\" and solve it as it was 2T(n/3) + 1?\n\nThis comes from  a from a problem that uses a ```\nV[a..b]```\n array and makes this return:\n\n```\nreturn V(X) + f(V, a, Y) + f(V, Z, b)\n```\n\n\nWhere ```\nY```\n is ```\n(2a+b)/3 and Z is (a+2b)/3```\n\n\nSo: ```\n((b-a+3)/3) = ((n+2)/3)```\n\n    ", "Answer": "\r\nSort of. The rigorous version of this trick is to set ```\nU(n) = T(n+1)```\n and write\n\n```\nU(n) = T(n+1)\n     = 2T((n+1+2)/3) + 1\n     = 2T(n/3 + 1) + 1\n     = 2U(n/3) + 1.\n```\n\n\nThen solve for ```\nU```\n (e.g., ```\nU(n) = O(n^log3(2))```\n) and then you should be able to find an asymptotic expression for ```\nT```\n of the same order.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Figuring out The Big O Notation/Recurrence Relation From My Old Algorithm\r\n                \r\n**Hi all,\nI have a question about recurrence relation/ Big O notation.  I was given a homework assignment that asked me to give the Big O notation of some of my old code/ Algorithms that I came up with for previous homework assignments. Sadly, I have not yet taken a course in Finite Mathematics; so this is new to me. I managed to figure out three of the four Algorithms that I used.  However, I am stuck on the fourth algorithms. This method is a recursive method and is coded in Java. I have, really, spent hours trying to figure this out, and I’ve watched lots of videos and read lots of articles on Big O notation but sadly can’t get it. Any help would be great!!!\nHere is the code: \n\n```\nArrayList<FacebookUser> getRecommendations(FacebookUser e) {\n    FacebookUser rootUser = userCallForList.get(0);\n\n\n    if(rootUser.getFriends().isEmpty() || e.getFriends().isEmpty()){\n        return returnHash();\n    }\n\n    for(FacebookUser hold : e.getFriends()){\n        if( !hold.equals(rootUser) && addHash(hold)){\n            getRecommendations(hold);   \n        }\n    }\n\n    return returnHash();\n}\n```\n\n\nNote About the Code:\nThe method takes a FacebookUser as an argument. The method returns an ArrayList that contains all of the friends of the FacebookUser that is passed into it plus the result of calling the same getRecommendations method on all of that FacebookUser’s friends. It does not add anyone to the list of recommendations if they are already on it and does not add the FacebookUser (E.I rootUser) that is calling it; As that could lead to an infinite loop. I use a HashSet as my collection and then I have to change it back to an ArrayList. \n    ", "Answer": "\r\nIf I'm understanding your code right then this isn't a traditional recurrence because n (the input size) changes on each recursive call but does not necessarily get smaller i.e. it's not a divide and conquer algorithm.\n\nThis statement \n\n```\nfor(FacebookUser hold : e.getFriends()){\n        if( !hold.equals(rootUser) && addHash(hold)){\n            getRecommendations(hold);   \n        }\n    }\n```\n\n\ncalls getRecommendations() for an unspecified number of friends which then calls it for each friend's friends. To me it seems like this method would be called for every friend in the network or every node if you think of it as a graph problem. This implies to me a running time O(k) where k = number of unique nodes reachable from the initial user by any number of edges.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to convert a simple computer algorithm into a mathematical function in order to determine the big o notation?\r\n                \r\nIn my University we are learning Big O Notation. However, one question that I have in light of big o notation is, how do you convert a simple computer algorithm, say for example, a linear searching algorithm, into a mathematical function, say for example 2n^2 + 1?\n\nHere is a simple and non-robust linear searching algorithm that I have written in c++11. Note: I have disregarded all header files (iostream) and function parameters just for simplicity. I will just be using basic operators, loops, and data types in order to show the algorithm.\n\n```\nint array[5] = {1,2,3,4,5};\n// Variable to hold the value we are searching for\nint searchValue;\n// Ask the user to enter a search value\ncout << \"Enter a search value: \";\ncin >> searchValue;\n// Create a loop to traverse through each element of the array and find\n// the search value\nfor (int i = 0; i < 5; i++)\n{\nif (searchValue == array[i])\n{\ncout << \"Search Value Found!\" << endl;\n}\nelse\n// If S.V. not found then print out a message\ncout << \"Sorry... Search Value not found\" << endl;\n```\n\n\nIn conclusion, how do you translate an algorithm into a mathematical function so that we can analyze how efficient an algorithm really is using big o notation? Thanks world.\n    ", "Answer": "\r\nFirst, be aware that it's not always possible to analyze the time complexity of an algorithm, there are some where we do not know their complexity, so we have to rely on experimental data.\n\nAll of the methods imply to count the number of operations done. So first, we have to define the cost of basic operations like assignation, memory allocation, control structures (if, else, for, ...). Some values I will use (working with different models can provide different values):\n\n\nAssignation takes constant time (ex: ```\nint i = 0;```\n)\nBasic operations take constant time (```\n+ - * ∕```\n)\nMemory allocation is proportional to the memory allocated: allocating an array of n elements takes linear time.\nConditions take constant time (```\nif```\n, ```\nelse```\n, ```\nelse if```\n)\nLoops take time proportional to the number of time the code is ran.\n\n\nBasic analysis\n\nThe basic analysis of a piece of code is: count the number of operations for each line. Sum those cost. Done.\n\n```\nint i = 1;\ni = i*2;\nSystem.out.println(i);\n```\n\n\nFor this, there is one operation on line 1, one on line 2 and one on line 3. Those operations are constant: This is O(1).\n\n```\nfor(int i = 0; i < N; i++) {\n    System.out.println(i);\n}\n```\n\n\nFor a loop, count the number of operations inside the loop and multiply by the number of times the loop is ran. There is one operation on the inside which takes constant time. This is ran n times -> Complexity is ```\nn * 1```\n -> O(n).\n\n```\nfor (int i = 0; i < N; i++) {\n    for (int j = i; j < N; j++) {\n        System.out.println(i+j);\n    }\n}\n```\n\n\nThis one is more tricky because the second loop starts its iteration based on ```\ni```\n. Line 3 does 2 operations (addition + print) which take constant time, so it takes constant time. Now, how much time line 3 is ran depends on the value of ```\ni```\n. Enumerate the cases:\n\n\nWhen i = 0, j goes from 0 to N so line 3 is ran N times.\nWhen i = 1, j goes from 1 to N so line 3 is ran N-1 times.\n...\n\n\nNow, summing all this we have to evaluate ```\nN + N-1 + N-2 + ... + 2 + 1```\n. The result of the sum is ```\nN*(N+1)/2```\n which is quadratic, so complexity is O(n^2).\n\nAnd that's how it works for many cases: count the number of operations, sum all of them, get the result.\n\nAmortized time\n\nAn important notion in complexity theory is amortized time. Let's take this example: running ```\noperation()```\n n times:\n\n```\nfor (int i = 0; i < N; i++) {\n    operation();\n}\n```\n\n\nIf one says that ```\noperation```\n takes amortized constant time, it means that running n operations took linear time, even though one particular operation may have taken linear time.\n\nImagine you have an empty array of 1000 elements. Now, insert 1000 elements into it. Easy as pie, every insertion took constant time. And now, insert another element. For that, you have to create a new array (bigger), copy the data from the old array into the new one, and insert the element 1001. The 1000 first insertions took constant time, the last one took linear time. In this case, we say that all insertions took amortized constant time because the cost of that last insertion was amortized by the others.\n\nMake assumptions\n\nIn some other cases, getting the number of operations require to make hypothesises. A perfect example for this is insertion sort, because it is simple and it's running time depends of how is the data ordered.\n\nFirst, we have to make some more assumptions. Sorting involves two elementary operations, that is comparing two elements and swapping two elements. Here I will consider both of them to take constant time. Here is the algorithm where we want to sort array a:\n\n```\nfor (int i = 0; i < a.length; i++) {\n    int j = i;\n    while (j > 0 && a[j] < a[j-1]) {\n        swap(a, i, j);\n        j--;\n    }\n}\n```\n\n\nFirst loop is easy. No matter what happens inside, it will run n times. So the running time of the algorithm is at least linear. Now, to evaluate the second loop we have to make assumptions about how the array is ordered. Usually, we try to define the best-case, worst-case and average case running time.\n\nBest-case: We do never enter the ```\nwhile```\n loop. Is this possible ? Yes. If ```\na```\n is a sorted array, then ```\na[j] > a[j-1]```\n no matter what ```\nj```\n is. Thus, we never enter the second loop. So, what operations are done in this case is the assignation on line 2 and the evaluation of the condition on line 3. Both take constant time. Because of the first loop, those operations are ran ```\nn```\n times. Then in the best case, insertion sort is linear.\n\nWorst-case: We leave the while loop only when we reach the beginning of the array. That is, we swap every element all the way to the 0 index, for every element in the array. It corresponds to an array sorted in reverse order. In this case, we end up with the first element being swapped 0 times, element 2 is swapped 1 times, element 3 is swapped 2 times, etc up to element n being swapped n-1 times. We already know the result of this: worst-case insertion is quadratic.\n\nAverage case: For the average case, we assume the items are randomly distributed inside the array. If you're interested in the maths, it involves probabilities and you can find the proof in many places. Result is quadratic.\n\nConclusion\n\nThose were basics about analyzing the time complexity of an algorithm. The cases were easy, but there are some algorithms which aren't as nice. For example, you can look at the complexity of the pairing heap data structure which is much more complex.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Order the following big O notation, from the fastest running time to slowest\r\n                \r\nOrder the following big O notation, from the fastest running time to slowest running time.\n\n\n1000\n2^n\nn ln⁡ n\n2n^2\nn\n\n\nMy attempt/guess is \n\n\n2^n, \n2n^2,\nn ln⁡ n, \n1000\n\n\nAm I even close?\nTime complexity is a very confusing topic. Please point me in the right direction. \n    ", "Answer": "\r\nHere's an image I found on the web:\n\n\n\nYou should keep in mind that\n\n\n```\n1000```\n is a constant, i.e. it is in ```\nO(1)```\n.\nif a function ```\nf(x)```\n is in ```\nO(...)```\n, then any scaled version ```\nk*f(x)```\n obtained by multiplication with a constant ```\nk```\n is in the same class ```\nO(...)```\n.\nas suggested by @Gassa, inserting a large number for ```\nn```\n is in many cases \"good enough\" to understand which of two classes is \"faster\" (i.e. `ln(10000) < sqrt(10000)\" => O(ln(n)) is lower than O(sqrt(n)))\n\n\nHope that helps!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Expected syntax for Big O notation\r\n                \r\nare there a limited amount of basic O Notations, considering you are meant to 'distil' them down to their most important part? \n\nO(n^2):\n\nO(n):\n\nO(1):\n\nO(log n) logarithmic\n\nO(n!) factorial\n\nO(na) polynomial\n\nOr are you expected to work out variations such as O(n^4) etc... and if so, is that the only exception? the power of X one?\n    ", "Answer": "\r\nGenerally, you distill Big-O notation (and related Bachman-Landau notations like Big-Theta and Big-Omega) down to the operation of the fastest-growing N term. So, you remove/simplify lesser terms (N2 + N == O(N2)) and nonvariable coefficients of the term (O(4N2) == O(N2)), but NOT powers or exponent bases (O(34N) == O(3N)). You also don't strip variable coefficients; NlogN is NlogN, NOT logN or N.\n\nSo, you will normally only see numbers in a Big-Oh notation if if the complexity is polynomial (power of N) or exponential (Nth power of a base). The most common Big-Oh notations are much as you show, with the addition of NlogN (VERY common).\n\nHowever, if you are differentiating between two algorithms of equal general complexity, you MAY add lesser terms and/or coefficients back in to demonstrate the relative difference; an algorithm that performs linearly but has double the instructions of another might be described as O(2N) when comparing it with the other O(N) algorithm. However, taken individually, both algorithms are linear (O(N)).\n\nSome Big-O notations are not algebraic, and may involve multiple variables in therir simplest general-case form. The counting sort, for instance, is complexity O(Max(N,M)), where N is the number of elements in the list, and M is the range of those elements. Often it is possible to reduce this in specific cases by defining M in terms of N and thus reducing to a single variable (if the list in question is of the first N squares, M = N2-1), but in the general case both variables are independent and significant. BucketSort's complexity is officially O(N), but really it's more like O(NlogM) where M is the maximum value of the list of N elements. M is usually considered insignificant, but that depends on the values you normally sort (sorting 5 values each in the billions will require more loops to compare each power of 10 than traversals through the list to put them in the buckets) and on the radix used (RadixSort is a base-2 BucketSort; again, sorting values with a greater log2 value will require more loops than traversals).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What do the vertical bars around a variable mean in Big O notation? [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Big O Notation with Absolute Value?\r\n                            \r\n                                (2 answers)\r\n                            \r\n                    \r\n                Closed 6 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI know in mathematics vertical bars can mean absolute value. \nExample ```\n|n|```\n.\n\nI have seen it used in Big O notation as well. For example take a look at the worst case performance of breadth first search: ```\nO(|V| + |E|)```\n.\n\nWhat do the vertical bars mean here? How is the above example different from: ```\nO(V + E)```\n?\n\nIf they mean absolute value as well, how could we have negative number of vertices or edges? \n    ", "Answer": "\r\nIt means Cardinality. That means the number of elements in the enclosed set of objects.\n\n```\n|V| = number of objects in V.\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "time complexities (Big-O notation) of the following running times expressed as a function of the input size N\r\n                \r\nGive the time complexities (Big-O notation) of the\nfollowing running times expressed as a function of\nthe input size N\n\n```\na) N^12 + 25N^10 + 8\nb) N + 3logN + 12n√n\nc) 12NlogN + 15N2logN\n```\n\n    ", "Answer": "\r\na) ```\nN^12```\n is the dominant term - ```\nO(N^12)```\n\n\nb) ```\nN√N```\n is the dominant term - ```\nO(N√N)```\n. For a proof of why ```\nlog n```\n is smaller than any power term, see this page\n\nc) ```\nN^2 > N```\n so ```\nN^2 log N```\n is the dominant term - ```\nO(N^2 log N)```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is there a general name for the type of complexity notated by Big O?\r\n                \r\nBig O describes the concepts of time complexity and space complexity, but is there a more general higher-level category which describes the domain of the complexity described by Big O notation?\n\nFor example, if someone brought up the topic of complexity in an application, I might ask \"do you mean cyclomatic complexity or Big O complexity?\". However, Big O is a notation for describing complexity, not an actual type of complexity.\n\nAre the concepts of time complexity and space complexity distinct to the point where they are not grouped into a more formal general category? If these concepts are commonly grouped into a more general category then what is the name of that category?\n    ", "Answer": "\r\nOne important property of statements that involve Big-O notation is that they apply only to asymptotic complexity. In that it is different from cyclomatic complexity or other software metrics.\n\nOther aspects that could differentiate it from software metrics are that it applies to algorithms, therefore you could refer to it as algoritmic complexity or theoretical complexity (of an algorithm).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Speed of for loop: beyond Big-O notations\r\n                \r\nThis question has no concern with Big-O notations.\n\nThat's actually what surprises me. Consider the code snippet:\n\n```\nfor(int i = 0; i < 13; i++){\n        long beg = System.currentTimeMillis();\n\n        int k, l;\n\n        for(l = 0; l < 1000000; l++){\n            for(int j = 0; j < 65535; j++) {\n                if(pixels[j] == pixels[l])\n                    k = j;\n                    //j = 70000;\n            }\n        }\n\n        System.out.println(System.currentTimeMillis() - beg + \" ms\");\n        System.out.println();\n    }\n```\n\n\nOutput is:\n\n10 ms\n\n30 ms\n\n10 ms\n\n10 ms\n\n10 ms\n\n10 ms\n\n10 ms\n\n10 ms\n\n10 ms\n\n10 ms\n\n10 ms\n\nNow, consider the code snippet:\n\n```\nfor(int i = 0; i < 13; i++){\n        long beg = System.currentTimeMillis();\n\n        int k, l;\n\n        for(l = 0; l < 1000000; l++){\n            for(int j = 0; j < 65535; j++) {\n                if(pixels[j] == pixels[l])\n                    //k = j;\n                    j = 70000;\n            }\n        }\n\n        System.out.println(System.currentTimeMillis() - beg + \" ms\");\n        System.out.println();\n    }\n```\n\n\nOutput is: \"164596 ms\" and so on. Why?\n    ", "Answer": "\r\nYou are resetting j to 0 every time.\n\nThis means you got an infinite loop.\n\n```\nfor(int j = 0; j < 65535; j++) {\n            //break;\n            j = 0;\n        }\n```\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation Python\r\n                \r\nIs the worst case run time for the function below O(n^2)? Because the while loop is O(n) and the body has lst[1:] which is O(n) so O(n) * O(n) = O(n^2)?\n\n```\ndef fn(lst):\n    ans = 0\n    while lst != []:\n        ans = ans + lst[0]\n        lst = lst[1:]\n    return ans > 100\n```\n\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Graphs for Big O notation\r\n                \r\nI wonder if there's any tool/website where I can plot some run times as graphs. Because, the asymptotic notation is often not what I wanted i.e. Don't want to ignore constants.\nFor example, suppose I have two notations, like:\n1) O = (n * log n).\n2) O = (n * log n * log n)/5.\nIt's obvious that 1st one is asymptotically better. But what I want to see how they perform and at which point the second one starts to become better.\nA graphical notation where I can enter different equations and plot them them to see how they vary would be greatly useful for this purpose. In my search I found this site where they have some plots. I am looking for something similar but I also want to input my equations to plot to analyse the performance for various 'n' values.\n    ", "Answer": "\r\nAs soon as you stop \"ignoring constants\", you're no longer graphing \"Big O\" notation, but just performing a standard XY plot.  As such, any graphing program, even online graphing calculators, would let you display this, just replace \"n\" for \"X\" and you'll get the proper graph.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation For accessing middle element in linked list and binary search?\r\n                \r\nArticle at http://leepoint.net/notes-java/algorithms/big-oh/bigoh.html says that Big O notation For accessing middle element in linked list is O(N) .should not it be O(N/2) .Assume we have 100 elements in linked list So to access the 50th element it has to traverse right from first node to 50th. So number of operation will be around 50.\n\nOne more big notation defined for binary search is log(N). Assume we have 1000 elements. As per log(N) we will be needing the opeartion close to 3. Now lets calculate it manually below will be the pattern\n\n500 th element, 250th, 125,63,32,16,8,4,2  so operation are around 9 which is much larger than 3.\n\nIs there any thing i am missing here?\n    ", "Answer": "\r\nWhat you're is missing that any constant multiples don't matter for Big O. So we have O(N) = O(N/2).\n\nAbout the log part of the question, it is actually log2(N) not log10(N), so in this case log(1000) is actually 9 (when rounding down). Also, as before, O(log2(N)) = O(log10(N)), since the two are just constant multiples of each other. Specifically, log2(N) = log10(N) / log10(2)\n\nThe last thing to consider is that, if several functions are added together, the function of lower degree doesn't matter with Big O. That's because higher degree functions grow more quickly than functions of lower degree. So we find things like O(N3 + N) = O(N3), and O(eN + N2 + 1) = O(eN).\n\nSo that's two things to consider: drop multiples, and drop function of low degree.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation for the permutations of a list of words\r\n                \r\nWhat would be the big O notation of the length of the list of permutations of the characters of a list of words of lenght n?\n\nI just do not know how to express that because it would be like n! for each word where n is the characters of that word but O(n!) is just the complexity of a single word, not a list of n words.\n\nAlso, every word may have different sizes.\n\nAn example:\n\nLet's say that I have the words \"abc\", \"abcd\" and \"abcde\". If I have to create the permutations of each word, I would end up with a list of lists of strings with lengths of 6, 24 and 120 i.e. the permutations of \"abc\" will be in the first list, the permutations of the second word will be in the second and so on and so on.\n\nIf I use an permutation iterator, how much time will it take to generate all those lists?\n    ", "Answer": "\r\nSince BigO represents an upper bound then you can safely say that there are ```\nO(k*n!)```\n strings where ```\nn```\n is the length of the longest input word (since in the worst case all the strings will have the same length then there will be exactly ```\nk*n!```\n permutations, in a better case with variable lengths the number will be ```\n< k*n!```\n but the notation ```\nO(k*n!)```\n still holds). \n\nIf you have some additional information regarding the distribution of those lengths we can try to do a tighter bound :-)\n\n@Edit: as @Aron pointed out this is a pretty (I'm not gonna get very formal here) tight upper bound if you do not have repeating chars. With repeating chars it's still a valid upper bound but a tighter bound would be ```\nO(k*a^n)```\n where ```\na```\n is the size of our alphabet (26 if you are using English, more than 110,000 if you are using Unicode) this will eventually be tighter than ```\nO(k*n!)```\n. This holds because at each of the ```\nn```\n slots in a word you can place one of ```\na```\n characters and notice that with repeating chars ```\na```\n might be smaller (even much smaller) than ```\nn```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation of looping through each character in a string before inserting into a map\r\n                \r\nI'm new to big O notation and have a couple of questions about it in my program.\n\nI have a program that has 2 maps. Before adding to one of the maps I loop through each character and randomly change the casing. \nI just put the string into the other map (no manipulation)\n\nIf the big O of inserting into a map is  O(1), what is it if I am looping through each character before putting it into a map?\nWhat will be the total big O complexity of this program( combining each insertion into the maps)?\n    ", "Answer": "\r\nIf you have a string of size n and you iterate over it, performing an O(1) insertion in the inner loop, then the time complexity is O(n).\n\nTo make this slightly less trivial, assume insertion costs a (where a could be a function of n, a constant, or something else entirely), then the total cost would be O(an+a).  This is because you're doing the insertion in the inner loop n times, then one additional time for the whole string.  In your case, a=1, so we have O(1n+1) = O(n).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Why is the number of edges ignored in the big-O notation\r\n                \r\nI am having a hard time understanding Dijkstra's big O notation exactly. I have a question regarding Dijkstra with an unsorted array.\n\nAs from Wikipedia:\n\n\n  The simplest implementation of the Dijkstra's algorithm stores\n  vertices of set Q in an ordinary linked list or array, and extract\n  minimum from Q is simply a linear search through all vertices in Q. In\n  this case, the running time is O(|E| + |V|^2) = O(|V|^2).\n\n\nI have myself implemented the algorithm in my application, I know how it works.\nI do not understand why ```\nO(|E| + |V|^2) = O(|V|^2)```\n or why the number of edges ```\n|E|```\n is ignored?\n\nConsidering as pseudo code my Dijkstra looks something like:\n\n```\nfor all vertices, current u\n     for each neighbor v of u:\n          .. do stuff\nend for\n```\n\n\nThis is how I explain to myself the ```\nO(|V|^2)```\n, but I do not understand how do they get the ```\n|E|```\n and then remove it?\n    ", "Answer": "\r\nYou can safely assume that ```\n|E| < |V|^2```\n, so you can remove the slowly growing parts which are dominated (standard oh notation stuff...).\n\nExplanation:\n\nIf you had edges between every vertices, that would be still only ```\nE = V*(V-1)/2```\n edges.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "<= vs < when proving big-o notation\r\n                \r\nWe just started learning big-o in class. I understand the general concept that f(x) is big-o of g(x) if there exists two constants c,k such that for all x>k |f(x)|<=c|g(x)|. I had a question whether or not it is required that we include the <= to sign or whether it is just sufficient to put the < sign?\n\nFor example:\nsuppose f(x)=17x+11 and we are to prove that this is O(x^2).\nThen if we take c=28 and x>k=1 we know that 17x+11<=28x^2. So since we know that x will always be greater than 1 this implies that 28x^2 will always be greater than 17x+11. So, do we really need to include the equal sign (<=) or is it okay if we just write (<)?\n\nThanks in advance.\n    ", "Answer": "\r\nFrom |f (x)| ≤ c |g (x)| for some real-valued c, it follows that |f (x)| < (c + e) |g (x)| for all e > 0.\n\nFrom that it follows that there exists c' = (c + e) such that |f (x)| < c' |g (x)|, so you can use < instead of ≤.\n\nMore practically, if you can prove |f (x)| < c |g (x)|, the ≤ part follows trivially.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How can i get the Big O Notations in this while loop?\r\n                \r\nThe computational cost will only consider how many times ```\nc = c+1;```\n is executed.\nI want to represent the Big O notation to use n.\n```\ncount = 0; index = 0; c = 0;\nwhile (index <= n) {\n   count = count + 1;\n   index = index + count;\n   c = c + 1;\n}\n```\n\nI think if the \"iteration of count\" is ```\nk```\n and \"iteration of index\" is ```\nn```\n, then ```\nk(k+1)/2 = n```\n.\nSo, I think O(root(n)) is the answer.\nIs that right solution about this question?\n    ", "Answer": "\r\n\nIs that right solution about this question?\n\nThis is easy to test. The value of ```\nc```\n when your ```\nwhile```\n loop has finished will be the number of times the loop has run (and, thus, the number of times the ```\nc = c + 1;```\n statement is executed). So, let us examine the values of ```\nc```\n, for various ```\nn```\n, and see how they differ from the posited ```\nO(√n)```\n complexity:\n```\n#include <stdio.h>\n#include <math.h>\n\nint main()\n{\n    printf(\"    c   root(n)  ratio\\n\"); // rubric\n    for (int i = 1; i < 10; ++i) {\n        int n = 10000000 * i;\n        int count = 0;\n        int index = 0;\n        int c = 0;\n        while (index < n) {\n            count = count + 1;\n            index = index + count;\n            c = c + 1;\n        }\n        double d = sqrt(n);\n        printf(\"%5d  %8.3lf %8.5lf\\n\", c, d, c / d);\n    }\n    return 0;\n}\n```\n\nOutput:\n```\n    c   root(n)  ratio\n 4472  3162.278  1.41417\n 6325  4472.136  1.41431\n 7746  5477.226  1.41422\n 8944  6324.555  1.41417\n10000  7071.068  1.41421\n10954  7745.967  1.41416\n11832  8366.600  1.41419\n12649  8944.272  1.41420\n13416  9486.833  1.41417\n```\n\nWe can see that, even though there are some 'rounding' errors, the last column appears reasonably constant (and, as it happens, an approximation to √2, which will generally improve as ```\nn```\n becomes larger) – thus, as we ignore constant coefficients in Big-O notation, the complexity is, as you predicted, ```\nO(√n)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "About Big O Notations and Complexity\r\n                \r\nI'm currently learning Big O Notations, but I'm kinda confused with time/iterations calculation according to different complexities.\n\nI made up this problem:\n\n\n  An algorithm that goes through all possible solutions takes 10^(-7) seconds with each test.\n  If the number of solutions are the following functions: logn, n, nlog. n^2, what's the maximum n I can calculate in, for example, less than 1 second?\n\n\nWhat I though about (for case logn) was 10^-7 times logn must take less than 1 second:\n\n```\n10^(-7) * logn < 1 <=> n = 10^(1/10^-7)\n```\n\n\nIncredibly big (If it's not wrong, oh damn --'). But what about n^2 ?\n\n```\n10^(-7) * n^2 < 1 <=> n = square_root(1/10^-7)\n```\n\n\nBut how can the number of solutions in n^2 case be less than the number in n, if the complexity is bigger? This is confusing me...?\n    ", "Answer": "\r\n\"This is bad on so many levels.\"\n\nFirst off, O(f(n)) is not the same as f(n).\n\nComplexity is typically used to represent the time it takes to solve a problem, as a function of the size of the input.\n\nOf course if you can solve more problems in the same amount of time using X vs Y, X will solve more problems in a fixed amount of time than Y.  But since you're not using the term complexity in the right way, it is no surprise you get a (seemingly) paradoxical answer.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the relation between mathematical approximation of Big O notation and pratical one in programming\r\n                \r\nI have a question about asymptotic complexity, specifically Big O notation.\nMathematically speaking: we have a function T(n) it's output :  the amount of time taken by our algorithm.\nfor example T(n) = 1000 + 10n .\nWe choose a simple function F(n) = n , and for some large natural number N and constant C , T(n) <= C.F(n) which is equivalent to that T(n) belonging O(f(n)) ( sets of functions which T one of them dominated by C.F(n) ).\nMy specific question: I didn't get the point of upper bounds , where from a specific input N , all T(n) points are upper bounded by F(n) points .\nWhat's the relation between that mathematical approximation and worst-best case complexity in computer science and Big O notation\n\n    ", "Answer": "\r\nAfter reading more about the topic.\nThis mathematical proof is just an approximation to explain that our function T(n) [ which it's output number of instructions or steps our algorithm spend to do some specific job ] is upper bounded by the equivalent function C.F(n) where C is some specific big constant > 0 . so our upper bound refers to the worst-case complexity , that our algorithm can reach ( and can reach because mathematically T(n) and F(n) intersect in some specific big N > 0 which become a supremum ( upper bound bellong to our function) ) .\nso in resume , it's an approximation to prove that the worst case complexity is an upper bound ( or supremum ) that our algorithm can reach .\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation easy problem with for loops\r\n                \r\nWhat is the Big-O complexity of the following code fragment?\n```\nconst int N = 3;\n\nchar arr[N][N][3];\n\nfor (int i=0; i<N; i++)\n\n        for(int j=0; j<N; j++)\n\n                for(int k=0; k<3; k++)\n\n                {\n\n                        cout<<”Initial Value: “;\n\n                        cin>>arr[ i ][ j ][ k ];\n\n                }\n```\n\nMy answer is O(N^3) although I'm unsure because of the last for loop. Can someone clarify?\n    ", "Answer": "\r\nBig-O complexity of the above code is O(N^3). Even your last loop, has the same value as N i.e. 3.\nSo yes you're right.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Deciding a Big-O notation for an algorithm\r\n                \r\nI have questions for my assignment. \nI need to decide what is the Big-O characterization for this following algorithm:\n\n\n\nI'm guessing the answer for Question 1 is O(n) and Question 2 is O(log n), but I kinda confused \nhow to state the reason. Are my answers correct? And could you explain the reason why the characterization is like that?\n    ", "Answer": "\r\nQuestion 1 : ```\nO(n)```\n because it increments by constant (1).\nfirst loop ```\nO(n)```\n second loop also ```\nO(n)```\n\ntotal   ```\nO(n) + O(n) = O(n)```\n    \n\nQuestion 2 : ```\nO(lg n)```\n it's binary search. \n\nit's ```\nO(lg n)```\n, because problem halves every time. \n\nif the array is size ```\nn```\n at first second is ```\nn/2```\n then ```\nn/4```\n ..... ```\n1```\n.\n\n```\nn/2^i = 1```\n => ```\nn = 2^i```\n => ```\ni = log(n)```\n .\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Is big O notation used to denote something other than the asymptote of a worst case performance?\r\n                \r\nI've come across the notation recently am confused by what I see. My previous understanding is that big O is supposed to be used to denote an upper bound whereas lower or tight bound uses different notations entirely (omega and theta respectively). This understanding however doesn't help me understand what people mean by \"best/worst/average case O(...)\", because it seems like O(...) should always refer to the worst case.\nAs an example, I see claims to the effect that \"hash maps are on average O(1)\". As another, insertion with cuckoo hashing is sometimes said to be \"expected to be O(1)\" because how the probability of kicking out other keys is bounded.\nIn both of these cases, big O notation seems to be used to describe something other than the asymptote of the worst case complexity. For example in the worst case we should expect every insertion into a normal hash map to be a collision; \"average worst case\" doesn't really make sense to me. \"Expected worst case\" seems similarly strange. I seem to be missing something important. What am I misunderstanding?\nI've tried to see if I misunderstood big O notation by looking up e.g. wikipedia, khan academy, and similar resources. They seem to confirm that big O is used to denote worst case scenario only.\n    ", "Answer": "\r\nThis is a common source of confusion when learning complexity analysis, and many resources do a poor job of explaining it. The answer is that the big-O notation is not specific to worst case complexity. big-O, big-Omega and big-Theta are all tools for analyzing mathematical functions, no matter what those functions describe (they don't actually need to describe anything at all).\nIn computer science, it just so happens that we most often apply the big-O notation to functions that describe the worst case runtime complexity of an algorithm - hence the confusion. But as you have discovered, it is also valid to apply it to functions that describe average case, worst case, or anything else. However, in most situations, big-O is usually the most useful notation to apply to the worst case, because the resulting expression also holds for the algorithm's complexity in all cases: if the worst case of some algorithm is O(n2), that means that it can never be worse than that, so the algorithm's complexity in all cases is O(n2): that is, no matter the input, the complexity will be described by some function that does not grow faster than n2.\nEven so, it is perfectly valid to apply the other functions to worst case - for example, an algorithm whose complexity function in the worst case is exactly 3n2 + 4n + 8 can also be said to be Omega(n2) in the worst case. The reason this is not usually done is that it produces a less useful statement, since the statement allows for the possibility that the complexity is arbitrarily high, e.g. 2n. There is, however, a very interesting application of big-Omega to worst case complexity: to express that comparison-based sorting is Omega(n lg n) in the worst case! This means that it is impossible to create a sorting algorithm where the central operation is to compare pairs of numbers and have it always be faster than n lg n - there will always be some inputs where the algorithm will require n lg n time or more. The algorithm might be faster in some cases, so it is not true that the best case for comparison-based sorting is Omega(n lg n), but the worst case is.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Java - Big-O Notation (Bottom-Up or Top-Down Approach) [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and  cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened,  visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nFirst, here is my code for the O(n):\n\n```\nimport java.util.*;\n\npublic class BigO{\n\n  public static void main(String[] args)\n  {\n    Scanner in = new Scanner(System.in);\n    System.out.print(\"Enter a number: \");\n    String userInput = in.nextLine();\n    int mNum = Integer.parseInt(userInput);\n\n    int y = new BigO().linear(mNum);\n    System.out.println(y);\n  }\n\n //O(n) - Linear\n public int linear(int n) {\n  int sum = 0;\n  for (int j = 0; j < n; j++) {\n   sum++;\n   System.out.print(sum + \" \");\n  }\n  return sum;\n }\n```\n\n\nI apologize if this is a dumb question because I haven't done big-O notation for a long time and I want to make sure, but for whatever that I have above, is it a Bottom-Up or Top-Down calculation? If it's neither, how can I approach for either one of them (or both)? Please let me know. Thanks.\n\nUPDATE:\nAlright, nevermind, I've asked some of my friends who are in my class as well as the professor and he written down our problem incorrectly. He corrected it and meant to say we were suppose to use this type of O(n) time algorithm for the recursive fibonacci. Sorry about that lol. \n    ", "Answer": "\r\nThe Big O has nothing to do with top-down/bottom-up.\n\n\nThe first one refers to a way of representing an algorithms complexity and running time, depending on the size of its input.\nThe second refers to the approach taken when breaking a problem up in subproblems in order to reach a solution.\n\n\nAll these are very accessible via a Google search :) .\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation queries\r\n                \r\n```\nint k = n;\nwhile (k > 0)\n{\nfor (int j = 0; j < n; j++)\n{\n System.out.println(“Inside the inner loop”);\n }\nk = k / 2;\n } \n```\n\n\nHi for this question, I came up with two answers, that is \n\nO (N^2 Log N) OR O (n * N/2) = O (N2/2). I'm not sure if there are the same or different? My lecturer also mentioned to chose the upper case. Therefore, for this question, there are one O(N^2) and one O(Log N). So the answer should be O(N^2) according to my lecturer? please help. Thanks.\n    ", "Answer": "\r\nThe answer is ```\nO(N * log N)```\n\n\nSince you divide ```\nK```\n in halfes you get ```\nO(log N)```\n for outer loop. And since in each iteration you iterate ```\nN```\n times then it will be ```\nO(N * log N)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Looking for clarification on Hashing and BST functions and Big O notation\r\n                \r\nSo I am trying to understand the data types and Big O notation of some functions for a BST and Hashing. \n\nSo first off, how are BSTs and Hashing stored? Are BSTs usually arrays, or are they linked lists because they have to point to their left and right leaves?\nWhat about Hashing? I've had the most trouble finding clear information regarding Hashing in terms of computation-based searching. I understand that Hashing is best implemented with an array of chains. Is this for faster searching or to decrease overhead on creating the allocated data type?  \n\nThis following question might be just bad interpretation on my part, but what makes a traversal function different from a search function in BSTs, Hashing, and STL containers?\nIs traversal Big O(N) for BSTS because you're actually visiting each node/data member, whereas search() can reduce its time by eliminating half the searching field?\n\nAnd somewhat related, why is it that in the STL, list.insert() and list.erase() have a Big O(1) whereas the vector and deque counterparts are O(N)? \n\nLastly, why would a vector.push_back() be O(N)? I thought the function could be done something along the lines of this like O(1), but I've come across text saying it is O(N):\n\n```\nvector<int> vic(2,3);\nvector<int>::const iterator IT = vic.end();\n\n//wanna insert 4 to the end using push_back\n\nIT++;\n(*IT) = 4;\n```\n\n\nhopefully this works. I'm a bit tired but I would love any explanations why something similar to that wouldn't be efficient or plausible. Thanks\n    ", "Answer": "\r\nBST's (Ordered Binary Trees) are a series of nodes where a parent node points to its two children, which in turn point to their max-two children, etc.  They're traversed in O(n) time because traversal visits every node.  Lookups take O(log n) time.  Inserts take O(1) time because internally they don't need to a bunch of existing nodes; just allocate some memory and re-aim the pointers. :)\n\nHashes (unordered_map) use a hashing algorithm to assign elements to buckets.  Usually buckets contain a linked list so that hash collisions just result in several elements in the same bucket.  Traversal will again be O(n), as expected.  Lookups and inserts will be amortized O(1).  Amortized means that on average, O(1), though an individual insert might result in a rehashing (redistribution of buckets to minimize collisions).  But over time the average complexity is O(1).  Note, however, that big-O notation doesn't really deal with the \"constant\" aspect; only order of growth.  The constant overhead in the hashing algorithms can be high enough that for some data-sets the O(log n) binary trees outperform the hashes.  Nevertheless, the hash's advantage is that its operations are constant time-complexity.\n\nSearch functions take advantage (in the case of binary trees) of the notion of \"order\"; a search through a BST has the same characteristics as a basic binary search over an ordered array.  O(log n) growth.  Hashes don't really \"search\".  They compute the bucket, and then quickly run through the collisions to find the target.  That's why lookups are constant time.\n\nAs for insert and erase; in array-based sequence containers, all elements that come after the target have to be bumped over to the right.  Move semantics in C++11 can improve upon the performance, but the operation is still O(n).  For linked sequence containers (list, forward_list, trees), insertion and erasing just means fiddling with some pointers internally.  It's a constant-time process.\n\npush_back() will be O(1) until you exceed the existing allocated capacity of the vector.  Once the capacity is exceeded, a new allocation takes place to produce a container that is large enough to accept more elements.  All the elements need to then be moved into the larger memory region, which is an O(n) process.  I believe Move Semantics can help here as well, but it's still going to be O(n).  Vectors and strings are implemented such that as they allocate space for a growing data set, they allocate more than they need, in anticipation of additional growth.  This is an efficiency safeguard; it means that the typical push_back() won't trigger a new allocation and move of the entire data set into a larger container.  But eventually after enough push_backs, the limit will be reached, and the vector's elements will be copied into a larger container, which again has some extra headroom left over for more efficient push_backs.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Simplifying big O notation\r\n                \r\nHey i have done some practice questions simplifying bigO expressions, can someone check which ones are right and which ones aren't? \n\nI'm quite new to this\n\n\n\ni) O(n^3)\n\nii) O(n^3)\n\niii) O(log(n))\n\niv) O(n^5/2) ?????\n\nv)  O(n^2)\n\n\n\ni) O(n^2)\n\nii) O(n^2)\n\niii) O(3^n)\n\niv) O(2^n)\n\nv) O(n^3)\n\n\n\ni) O(n)\n\nii) O(n)\n\niii) O(n^2)\n\niv) O(n^2)\n    ", "Answer": "\r\nIn the first group, your (ii) is wrong, it should be O(n2.3*log(n)).\nThe rest in that group are correct.\n\nIn the second group,\n\n(ii) should be O(x),\n\n(iv) should be O(n*2n), and \n\n(v) should be O(n2).  \n\nThe rest in this group are correct, except that I'm not sure if you're supposed to keep the original ```\nx```\n or ```\nm```\n, or else replace them with ```\nn```\n as you did.\n\nIn the third group, all of your answers are correct.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation Algorithm\r\n                \r\nI am learning Algorithm recently and know that there are usually some good algorithm existed already that we don't need to write our own. I think the problem I am facing in a question paper.\nI have a question in my past paper that if a function is O(n) then can it be O(n^2) ?\ncan we say that if a function is O(n) then it's also O(n^2)???\n    ", "Answer": "\r\nBig O is an upper bound.  So, yes, n is in O(n^2), but not vice-versa.  Also, both n and n^2 are in O(n^3).  \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Simplifying big O notation\r\n                \r\nHey i have done some practice questions simplifying bigO expressions, can someone check which ones are right and which ones aren't? \n\nI'm quite new to this\n\n\n\ni) O(n^3)\n\nii) O(n^3)\n\niii) O(log(n))\n\niv) O(n^5/2) ?????\n\nv)  O(n^2)\n\n\n\ni) O(n^2)\n\nii) O(n^2)\n\niii) O(3^n)\n\niv) O(2^n)\n\nv) O(n^3)\n\n\n\ni) O(n)\n\nii) O(n)\n\niii) O(n^2)\n\niv) O(n^2)\n    ", "Answer": "\r\nIn the first group, your (ii) is wrong, it should be O(n2.3*log(n)).\nThe rest in that group are correct.\n\nIn the second group,\n\n(ii) should be O(x),\n\n(iv) should be O(n*2n), and \n\n(v) should be O(n2).  \n\nThe rest in this group are correct, except that I'm not sure if you're supposed to keep the original ```\nx```\n or ```\nm```\n, or else replace them with ```\nn```\n as you did.\n\nIn the third group, all of your answers are correct.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the Big-O Notation for this code?\r\n                \r\nI am having trouble deciding between N^2 and NlogN as the Big O? Whats throwing me off is the third nested for loop from k <=j. How do I reconcile this?\n\n```\nint Max_Subsequence_Sum( const int A[], const int N )\n{\n  int This_Sum = 0, Max_Sum = 0;\n  for (int i=0; i<N; i++)\n  {\n    for (int j=i; j<N; j++)\n    {\n      This_Sum = 0;\n      for (int k=i; k<=j; k++)\n      {\n        This_Sum += A[k];\n      }\n      if (This_Sum > Max_Sum)\n      {\n        Max_Sum = This_Sum;\n      }\n    }\n  }\n  return Max_Sum;\n}\n```\n\n    ", "Answer": "\r\nThis can be done with estimation or analysis.  Looking at the inner most loop there are ```\nj-i```\n operations inside the second loop.  To get the total number of operations one would sum to get :\n\n```\n(1+N)(2 N + N^2) / 6\n```\n\n\nMaking the algorithm O(N^3).  To estimate one can see that there are three loops which at some point have O(N) calls thus it's O(N^3).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation Advice\r\n                \r\nWhich of these functions is not O(n2)?\nA. O(n2log2n)\nB. O(log2(log2n)\nC. O(nlog2n)\nD. (log2n)2\nI believe, that it is not A, because n2 takes dominance in that one, which would make it a O(n2) as well.\nMy question becomes how do you figure out which one is not a O(n2)? I have not worked much with logarithms, and I am still fuzzy about how to work with them.\nThanks.\n    ", "Answer": "\r\nAnswer is A\nBig-O is an upper bound and in programming represents the worst-case runtime. For example f(n) = n^2 means f(n) is O(n^2) and O(n^3) and O(n^100*logn). So O(n^2) is also O(n^3) but O(n^3) is not O(n^2)\nIn the case of your question, the only answer that is > n^2 is A. Graphing the functions can help you visualize it. As you can see A (red) is the only one increasing faster than n^2 (black)\n\nThis article may help you as well\nhttps://en.wikipedia.org/wiki/Big_O_notation and go to Orders of common functions\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation, when can we drop constants legally?\r\n                \r\nI know in Big O Notation we only consider the highest order, leading polynomial term because we are basically placing this theoretic worst case bound on compute-time complexity but sometimes I get confused on when we can legally drop/consider terms as constants. For example if our equation ran in \n\nO((n^3)/3) --> we pull out the \"1/3\" fraction, treat it as a constant, drop it, then say our algo runs in O(n^3) time.\n\nWhat about in the case of O((n^3)/((log(n))^2))? In this case could we pull out the 1/((log(n)^2)) term, treat it as a constant, drop it, and then ultimately conclude our algorithm is O(n^3). It does not look like we can, but what differentiates this case from the above case? both can be treated as constants because their values are relatively small in comparison to the leading polynomial term in the numerator but in the second case, the denominator term really brings down the worst case bound (convergence) as n values get larger and larger.\n    ", "Answer": "\r\nAt this point, it starts to be a good idea to go back and look at the formal definition for big O notation. Essentially, when we say that ```\nf(x) is O(g(x))```\n we mean that there exists a constant factor ```\na```\n and a starting input ```\nn0```\n such that for all ```\nx >= n0```\n then ```\nf(x) <= a*g(x)```\n.\n\nFor a concrete example, to prove that ```\n3*x^2 + 17```\n is ```\nO(n^2)```\n we can use ```\na = 20```\n and ```\nn0 = 1```\n.\n\nFrom this definition it becomes easy to see why the constant factors get dropped off - its just a matter of adjusting the ```\na```\n to compensate. As for your ```\n1/log(n)```\n question, if we have ```\nf(x)  is O(g(x))```\n and ```\ng(x) is O(h(x))```\n then we also have ```\nf(x) is O(h(x))```\n. So yes, ```\n10*n^3/log(n) + x is O(n^3)```\n but that is not a tighter upper bound and it is a weaker statement than saying that ```\n10*n^3/log(n) +  x is O(n^3/log(n))```\n. For a tight bounds you would want to use big-Theta notation instead.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "How to calculate the Big O notation of following equations? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\n\nThis is a recursive formula:\n\n\nE(r,n) = (E(r-1,n-1)-E(r,n-1))^2 + E(r,n-1)  r<=n\n\nPlease show the process of getting the Big O as well. \n\n\nsummation from i=0 to r, (r-i)(n choose i)(p^(n-i) * (1-p)^i)\n\n\nI think calculating the Big O to this equation equals to calculating Big O of:\n\nsummation from i=0 to r, (n chose i)\n\nIs this correct?\n\nPlease show the process of getting the Big O as well.\n    ", "Answer": "\r\nAssming that I am understanding the problem correctly and you want the time complexity of a program that does each of those things:\n\n\nWhat are the base cases for that recursion? Let's assume that E(0,0) is the base case, E(r,0) is a linear recursion till E(0,0) and r > n. Imagine that we form a recursion tree. We would get that each level of would have the call the function with the n from the previous level minus one, since the function always call itself with n-1. Now analysing the tree and separating it we get:\n\n\nA triple recursion until n = 0;\nAfter n = 0, we get a branches of linear recursion (a is dependent on n and r)\n\n\n\nAnalising the triple recursion, we can find it's complexity by various methods (generating functions, master's, etc) but lets use a simpler method. Let's assume that each node uses one time unit. We know that each level has 3 times the nodes of the level before, that the first level has one node and that the tree has n levels, and that the total time is the summation of all nodes. So we get that \nTotalTripleTime = Summ(0, n) of 3^k\nThis can be given by the closed formula for a geometric progression so:\n(1-3^n+1)/-2 which is O(3^n)\n\nThe amount of is a little harder to calculate due to the complex recursion and I dont have alot of time, maybe I'll try it later. \n\n\nAssuming that r is smaller than n, the we simply have that O(r * O(combinatations)) Assuming that calculating the a simple combinatorial using the closed formula is O(n), we get O(n*r).\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation- Finding the Complexity\r\n                \r\nI'm working on a quiz- I got all right but the following snippet. Help me understand which notation this would be?\nMy selections are: O(n^2), O(n^3), O(nlogn), and O(n+n^2)\nSnippet is:\n```\n   for (int i = 0; i < n; i++) { \n     System.out.println(\"hello\");\n   } \n   for (int i = 0; i < n; i++) { \n     for (int j = 0; j < i; j++) { // NOTE j < i here. \n       System.out.println(\"hello\");\n     }\n  } \n```\n\n    ", "Answer": "\r\nThe first loop is easy: the body runs exactly ```\nn```\n times, so it's ```\nO(n)```\n.\nThe second one requires a bit more thinking. It's clear that the outer loop runs ```\nn```\n times, but how often is the inner loop body executed? The first time through the outer loop, we have ```\ni == 0```\n, so the inner loop body is executed 0 times (```\nj < i```\n is false even for ```\nj == 0```\n). The second time, ```\ni == 1```\n so the inner loop body is executed 1 times. In general, it's executed ```\ni```\n times, for ```\ni = 0, ..., n-1```\n.\nSo the total number of executions of the inner loop body is, let's call it ```\nS```\n, is ```\nS = 0 + 1 + ... + n-1```\n. Now there's a nice trick to make that into a closed form equation. Write it down once, and then once more in reverse:\n```\n S =  0  +  1  + ... + n-2 + n-1\n S = n-1 + n-2 + ... +  1  +  0\n```\n\nThen add the two equations together:\n```\n S  =  0  +  1  + ... + n-2 + n-1\n S  = n-1 + n-2 + ... +  1  +  0\n------------------------------- +\n2*S = n-1 + n-1 + ... + n-1 + n-1\n```\n\nSo ```\n2*S```\n is equal to ```\nn```\n times ```\nn-1```\n. From this, we easily find ```\nS = n * (n-1) / 2```\n. This can be rewritten as ```\nS = ½*n^2 - ½*n```\n. In the big-O form, only the highest order term survives, and the constant ```\n½```\n does not matter, so this is ```\nO(n^2)```\n.\nA more \"handwavy\" way to get the same result is: the inner loop runs, on average, ```\nn/2```\n times (give or take one), and the outer loop runs ```\nn```\n times, again giving ```\nO(½*n*n) = O(n^2)```\n.\nCombined with the ```\nO(n)```\n of the first loop, which becomes asymptotically irrelevant compared to the ```\nO(n^2)```\n, the expected answer is probably ```\nO(n^2)```\n.\nBut note that technically ```\nO(n+n^2)```\n is also correct because ```\nO(n + n^2) = O(n^2)```\n. Again, the lower order term ```\nn```\n does not matter asymptotically. Even ```\nO(n^3)```\n is technically correct because ```\nn^3```\n dominates ```\nn^2```\n; however, the complexity is neither ```\nΩ(n^3)```\n nor ```\nθ(n^3)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation - How do you describe a 2D array of different lengths?\r\n                \r\nI'm having trouble understanding what the Big O notation would be for nested data structures like 2D arrays or an object of arrays.\nThe Scenario\nSum all of the values in all of the arrays.\nWhat I know\nFor example, my understanding is the below data structure would be ```\nO(n^2)```\n since both dimensions are the same length (3x3).\n```\n// O(n^2)\n\nconst arr = [\n  [1,2,3],\n  [1,2,3],\n  [1,2,3],\n]\n```\n\nThis would be ```\nO(n * m)```\n where ```\nn```\n is the length of the first dimension (2), and ```\nm```\n is the length of the second dimension (3).\n```\n// O(n * m)\n\nconst arr = [\n  [1,2,3],\n  [1,2,3],\n]\n```\n\nWhat I don't know\nBut what about a data structure where the second dimension arrays are different lengths?\n```\n// Maybe O(n * m)?\n\nconst arr = [\n  [1,2,3],\n  [1,2,3,4,5,6],\n  [1,2]\n]\n\nlet sum = 0\n\nfor (let nums of arr) {\n  for (let num of nums) {\n    arrSum += num\n  }\n}\n\n// sum = 30\n```\n\nOr an object where the values are arrays?\n```\n// Maybe O(n * m)?\n\nconst obj = {\n  a: [1,2,3],\n  b: [1,2,3,4,5,6],\n  c: [1,2]\n}\n\nlet sum = 0\n\nfor (let nums of Object.values(obj)) {\n  for (let num of nums) {\n    sum += num\n  }\n}\n\n// sum = 30\n```\n\nI am getting stumped on how this would be represented in Big O notation. My thought is you would still go with ```\nO(n * m)```\n where ```\nn```\n is the length of the first dimension (or the number keys), and ```\nm```\n represents the longest array in the second dimension (or key value).\nAm I thinking about this correctly?\n    ", "Answer": "\r\nNo need to make this more complicated than it is. ```\nn```\n and ```\nm```\n are both simply variables. What they describe is entirely up to you. Actually a proper choice for what you base the runtime-complexity of your algorithm on can be of considerable importance.\nSo for the case of an array of arrays with different lengths, you can simply pick ```\nn```\n as the sum of the length of all \"inner\" arrays. For the case with the object, the problem can be reduced to the first problem. Minor nitpick: you depend on the internal behavior of ```\nObject.values```\n; I've simply assumed it's in ```\nO(n)```\n.\nSo in general: pick the most accurate description of the problem-size, not the structure. Whether you're summing all elements of an ```\n256 x 256```\n or an ```\n2 x 32768```\n array won't make any difference in terms of runtime-complexity.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O/Big-Oh Notation\r\n                \r\nI am attempting to calculate the Big-O of the following algorithm but I am confused and require some assistance:\n\n```\nAlgorithm 1. DFS(G,n)\nInput: G- the graph\n       n- the current node\n1) Visit(n)\n2) Mark(n)\n3) For every edge nm (from n to m) in G do\n4)     If m is not marked then\n5)         Dfs(G,m)\n6)     End If\n7) End For\nOutput: Depends on the purpose of the search...\n```\n\n\nI won't even begin to say what I (incorrectly) calculated the solution to be.  Can anybody please help me and explain this to me?\n\nThank you.\n\nEDIT:  Apparently my calculation of ```\nO(n+m)```\n is correct...can somebody verify this?\n\nEDIT 2:  Or is it ```\nO(|n|+|m|)```\n?\n    ", "Answer": "\r\nIts cost is O(n + e ) where n is the number of nodes and e the number of edges.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Where can I read about time complexity (Big-O notation) of MySQL queries? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 6 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI am interested in complexities (Big O notation) of MySQL queries like \"SELECT something WHERE id=1\", for example. I am talking about not only one example but some documentaiton where I can read all this stuff how MySQL implements these queries. Is there one? I'll really appreciate if you share this one.\n    ", "Answer": "\r\nI am going to take as your question:  What is the time complexity of the following query in MySQL?\n\n```\nSELECT something\nFROM t\nWHERE id = 1;\n```\n\n\nThere are basically two possibilities:  O(log(n)) and O(n).  If the database has an index on ```\nid```\n, then the look-up is O(log(n)).  Because indexes are stored in memory and table sizes are usually under, say, a trillion rows, this is often approximated as O(1).  The \"constant\" component of reading the index and setting up the query dominates the time for perusing the index.\n\nThe other possibility is O(n).  This means that every row needs to be read to find the one with the right id.  You don't have a ```\nlimit```\n at all, so every row does need to be read, even if the first row matches the condition.\n\nYou can imagine that if such a simple query has three potential \"right\" answers, that more complex queries are even more difficult to analyze.  If you really want to understand performance, you need to learn about complexity theory, the algorithms that databases implement, and the effects of in-memory and out-of-memory data on the algorithms.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Calculate Big O Notation\r\n                \r\nI currently have the following pseudo code, and I am trying to figure out why the answer to the question is O(n).\n\n```\nsum = 0;\nfor (i = 0; i < n; i++) do\n    for (j = n/3;j < 2*n; j+= n/3) do\n        sum++;\n```\n\n\nI thought the answer would be O(n^2) since the first for loop would run 'n' times and the second for loop has += n/3, giving it another (n divided by something times), which would just simplify to O(n^2). Could somebody explain why it is O(n)?\n    ", "Answer": "\r\nThis is because the second loop runs in constant amount of operations (does not depend on ```\nn```\n). From ```\nn/3```\n to ```\n2n```\n with a step ```\nn/3```\n which is similar to from ```\n1/3```\n to ```\n2```\n with a step ```\n1/3```\n.\n\nThis will run 5-6 times for reasonable ```\nn```\n (not 0) (the number is not important and depends on how do you calculate ```\n/```\n) \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "The most efficient method using the Big O notation to avoid multiple forEach loops\r\n                \r\nI have created these two objects : \n\n```\nstruct Person {\n    let name: String\n    let genderId: Int\n\n}\n\nstruct Gender {\n    let id: Int\n    let sex: String\n}\n```\n\n\nand create these arrays : \n\n```\nlet persons = [\n    Person(name: \"Steve\", genderId: 1),\n    Person(name: \"Kate\", genderId: 2),\n    Person(name: \"Mark\", genderId: 1),\n    Person(name: \"Pam\", genderId: 2)\n\n]\n\nlet genders = [\n    Gender(id: 1, sex: \"girl\"),\n    Gender(id: 2, sex: \"boy\"),\n]\n```\n\n\nWhat should be the most efficient algorithm using the big O notation in order to create an object array based on this new object : \n\n```\nstruct PersonWithGenderString {\n    let name: String\n    let genderString: String\n}\n```\n\n\nI already wrote this method using two forEach loops but I am looking for way which iterate the less time possible : \n\n```\npersons.forEach { (person) in\n    genders.forEach { (gender) in\n        if person.genderId == gender.id {\n            let personWithGenderString = PersonWithGenderString(name: person.name, genderString: gender.sex)\n            print(personWithGenderString)\n        }\n\n    }\n}\n```\n\n    ", "Answer": "\r\nIf you store the ```\nGender```\n objects in order by ```\nGender.id```\n in the ```\ngenders```\n array, which it appears you do, you can access them directly, instead of using the inner loop:\n\n```\npersons.forEach { (person) in\n    let genderIndex = person.genderId - 1\n    if genderIndex < 0 || genderIndex > genders.size {\n       // handle this error (array out of bounds)\n    }\n    let personWithGenderString = PersonWithGenderString(name: person.name, genderString: genders[genderIndex].sex)\n    print(personWithGenderString)\n}\n```\n\n\nThis would be O(1) to convert any particular ```\nPerson```\n to ```\nPersonWithGenderString```\n and O(n) to convert n ```\nPerson```\ns to n ```\nPersonWithGenderString```\ns\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Homework - Big O Notation and Calculating Time\r\n                \r\nlet's say I have an algorithm that takes 0.5ms for n = 20, and I want to figure out how long it takes if n = 40 for O(n^2).\n\nTo my understanding, the procedure is as follows:\n\nt = 0.5 * (40^2/20^2)\n\nBut why is this? I don't understand the mechanics behind this. I know that Big O is an upper bound, and for any n, its an output of some number of instructions. But calculating the time isn't making sense.\n    ", "Answer": "\r\n\n  let's say I have an algorithm that takes 0.5ms for n = 20, and I want to figure out how long it takes if n = 40 for O(n^2).\n\n\nUnfortunately, you can infer about the behavior for n = 40 almost nothing.\n\nThe statement that the algorithm is O(n2) means that there exists some constant c > 0 for which, for large enough n, the running time is not larger than c n2. Consequently, you do not know if 20, 40, or 4,000,000 falls into \"large enough\" n, and even if it does, you can only know that it is bounded from above by something.\n\n\n  But calculating the time isn't making sense.\n\n\nThat's a logical conclusion, unfortunately.\n\n\n\nEdit\n\nThanks to the excellent comment by Anmol Singh Jaggi (many thanks!), here is the figure to which he linked that illustrates the problem \n\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation complexity in three cases\r\n                \r\nDo following algorithms run in O(n) time?\n\n1\n\n```\ns=0\nfor(i=0; i<n; i++)\n{\n    for (j=0; j<n; j++)\n    {\n        s=s+i*j;\n    }\n\n    s=s+1\n}\n```\n\n\nThis is a O(n^2) since here performance directly proportional to the square of the size of the input data set N\n\n2\n\n```\n s=0\n    for(i=0; i<n; i++)\n    { if (i>20)\n        for (j=0; j<n; j++)\n        {\n            s=s+i*j;\n        }\n\n        s=s+1\n    }\n```\n\n\n3\n\n```\n s=0\n    for(i=0; i<n; i++)\n    { if(i<4)\n        for (j=0; j<n; j++)\n        {\n            s=s+i*j;\n        }\n\n        s=s+1\n    }\n```\n\n\nCan you please explain how the if statement affects O(n)? In both cases (#2 and #3) first loop is O(n) and the second loop is going to run if N > 20 or N < 4 respectively. But how this affects complexity? Will these are still be O(n^2) with ```\nif i > 20```\n does 20^2 operations less and ```\nif i < 4```\n 4^2 less? Also that Big O assumes that N is going to infinity? \n    ", "Answer": "\r\n2\n\nStill ```\nO(N^2)```\n. The loop runs a total of\n\n20 + (N - 20) * N iterations (and each iteration is constant) ==> O (N^2) \n\n3\n\n```\nO(N)```\n. The loop runs a total of \n\nN * 4 + (N - 4) iterations (and each iteration is constant) ==> O(N)\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Asymptotic growth (Big o notation)\r\n                \r\nWhat I am trying to do is to sort the following functions:\n\nn, n^3, nlogn, n/logn, n/log^2n, sqrt(n), sqrt(n^3)\n\nin increasing order of asymptotic growth.\n\nWhat I did is,\n\nn/logn, n/log^2n, sqrt(n), n, sqrt(n^3), nlogn, n^3.\n\n1) Is my answer correct?\n\n2) I know about the time complexity of the basic functions such as n, nlogn, n^2, but I am really confused on the functions like, n/nlogn, sqrt(n^3).\nHow should I figure out which one is faster or slower? Is there any way to do this with mathematical calculations?\n\n3) Are the big O time complexity and asymptotic growth different thing?\n\nI would be really appreciated if anyone blows up my confusion... Thanks!\n    ", "Answer": "\r\nAn important result we need here is:\n\n\n  ```\nlog n```\n grows more slowly than ```\nn^a```\n for any strictly positive number ```\na > 0```\n.\n\n\nFor a proof of the above, see here.\n\nIf we re-write ```\nsqrt(n^3)```\n as ```\nn^1.5```\n, we can see than ```\nn log n```\n grows more slowly (divide both by ```\nn```\n and use the result above).\n\nSimilarly, ```\nn / log n```\n grows more quickly than any ```\nn^b```\n where ```\nb < 1```\n; again this is directly from the result above. Note that it is however slower than ```\nn```\n by a factor of ```\nlog n```\n; same for ```\nn / log^2 n```\n.\n\nCombining the above, we find the increasing order to be:\n\n\n```\nsqrt(n)```\n\n```\nn / log^2 n```\n\n```\nn / log n```\n\n```\nn```\n\n```\nn log n```\n\n```\nsqrt(n^3)```\n\n```\nn^3```\n\n\n\nSo I'm afraid to say you got only a few of the orderings right.\n\n\n\nEDIT: to answer your other questions:\n\n\nIf you take the limit of ```\nf(n) / g(n)```\n as ```\nn -> infinity```\n, then it can be said that ```\nf(n)```\n is asymptotically greater than ```\ng(n)```\n if this limit is infinite, and lesser if the limit is zero. This comes directly from the definition of big-O.\nbig-O is a method of classifying asymptotic growth, typically as the parameter approaches infinity.\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Algorithm Big O Notation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 12 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\n\n  Possible Duplicate:\n  If f(n) = O(g(n)) , then is exp(f(n)) = O(exp(g(n)))  \n\n\n\n\nI stumbled upon this question in the Cormen book.\n\nIf f(n) is O (g(n)) then 2^f(n) is also O (2^g(n)). Is this true? I was trying to prove it using limit rules but totally stuck. My instincts are saying it is false but how can we deduce that?\n\nThanks\n    ", "Answer": "\r\nNo it's not.\n\n```\nf(n) = 2n```\n is ```\nO(n)```\n, but ```\ne^(2n)```\n is ```\nO((e^2)^n)```\n, which is obviously slower than ```\nO(e^n)```\n because of the larger base.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation (The complexity) of the following code?\r\n                \r\nI just wondering what is the big o for the code below:\n\nI am thinking O(n). What do you guys think? Thank you for your help!\n\n```\nfor( w = Length ; w >= 0 ; w = w / 2 ){\n   for( i = Length ; i >= 0 ; --i ){\n      if(  randomNumber() == 4  )\n         return\n   }\n}\n```\n\n    ", "Answer": "\r\nSince you are asking for the ```\nBig O```\n notation, which is the worst case time complexity, the answer is:\n\n```\nO(n^x)```\n , where x is the denominator used in the outer-for loop.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Algorithm Big O Notation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 12 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\n\n  Possible Duplicate:\n  If f(n) = O(g(n)) , then is exp(f(n)) = O(exp(g(n)))  \n\n\n\n\nI stumbled upon this question in the Cormen book.\n\nIf f(n) is O (g(n)) then 2^f(n) is also O (2^g(n)). Is this true? I was trying to prove it using limit rules but totally stuck. My instincts are saying it is false but how can we deduce that?\n\nThanks\n    ", "Answer": "\r\nNo it's not.\n\n```\nf(n) = 2n```\n is ```\nO(n)```\n, but ```\ne^(2n)```\n is ```\nO((e^2)^n)```\n, which is obviously slower than ```\nO(e^n)```\n because of the larger base.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What's the big-O notation for this algorithm?\r\n                \r\nI'm currently trying to understand dynamic programming, and I found an interesting problem : \"Given a chess board of nxn squares and a starting position(xs,ys), find the shortest (as in no. of moves) path a knight can take to an end position(xe,ye)\". This is how my solution would sound like :\n\n```\nInitialize the matrix representing the chess board (except the \"square\" xs,ys) with infinity.\nThe first value in a queue is the square xs,ys.\nwhile(the queue is not empty){\n         all the squares available from the first square of the queue (respecting the rules of chess) get \"refreshed\"\n         if (i modified the distance value for a \"square\")\n                    add the recently modified square to the queue\n}\n```\n\n\nCan someone please help me find out what's the computing-time O value for this function? I (kind of) understand big-O, but I just can't put a value for this particular function.\n    ", "Answer": "\r\nBecause you are using a queue, the order that you process the squares is going to be in order of minimum distance.  This means that you will only ever modify the distance value for a square once, and therefore the time will be O(n^2), since there are n^2 squares.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Time complexity of mandelbrot set in term of big O notation\r\n                \r\nI'm trying to find the time complexity of a simple implementation of mandelbrot set. with following code\n\n```\nint main(){\n    int rows, columns, iterations;\n    rows = 22;\n    columns = 72;\n    iterations = 28;\n\n    char matrix[max_rows][max_columns];\n\n    for(int r = 0; r < rows; ++r){\n        for(int c = 0; c < columns; ++c){\n            complex<float> z;\n            int itr = 0;\n            while(abs(z) < 2 && ++itr < iterations)\n                z = pow(z, 2) + decltype(z)((float)c * 2 / columns - 1.5,\n                    (float)r * 2 / rows - 1);\n            matrix[r][c]=(itr== iterations ? '*' : '.');\n        }\n    }\n```\n\n\nNow looking at above code i made some estimation for time complexity in terms of big O notation and want to know if it is correct or not\n\nSo we are creating a 2d array traversing it through nested loops and and at each element we are performing an operation and setting a value of that element, if we take n as input size we can say that greater the input the greater will be the complexity, so the time complexity for rowsxcolumns would be O(rxc) and then again we are traversing it for printout, so what would be the time complexity? is it O(rxc)+O(rxc) ? does the function itself have some effect on time complexity when we are doing multiplication and subtraction on rows and columns? If yes then how?\n    ", "Answer": "\r\nAlmost, given ```\nr```\n rows, ```\nc```\n columns and ```\ni```\n iterations then the running time is ```\nO(r*c*i)```\n. This should be trivial to see if ```\nabs(z)<2```\n is not there. But with this extra condition its not clear how many times will the inner ```\nwhile```\n loop run in total. Yes, it will be less than ```\nr*c*i```\n times, so ```\nO(r*c*i)```\n is still the upper bound. But perhaps we might do better. Given that for any ```\nr,c```\n you compute Mandelbrot set over the same domain with varying resolution then the while loop will run ```\nk*r*c*i```\n times for some constant ```\nk```\n which is somewhere between area-of-Mandelbrot-set-over-area-of-the-domain and 1 --> Running time of the code is ```\nΘ(r*c*i)```\n and ```\nO(r*c*i)```\n cannot be improved.\n\nHad you computed the set over ```\n[-c,c]x[-r,r]```\n domain with fixed resolution then for any ```\n|z|>2```\n the ```\nabs(z)<2```\n breaks after first iteration. Then ```\nO(r*c*i)```\n would not be tight bound and this condition (as all loop conditions) should be considered if you want accurate estimation.\n\nPlease don't use ```\nmalloc```\n,  ```\nstd::vector```\n is safer. \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What does the big-O notation mean? [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 11 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\n\n  Possible Duplicate:\n  Plain English explanation of Big O  \n\n\n\n\nI need to figure out ```\nO(n)```\n of the following:\n\n```\nf(n) = 10n^2 + 10n + 20\n```\n\n\nAll I can come up with is ```\n50```\n, and I am just too embarrassed to state how I came up with that.\n\nCan someone explain what it means and how I should calculate it for ```\nf(n)```\n above?\n    ", "Answer": "\r\nBig-O notation is to do with complexity analysis. A function is ```\nO(g(n))```\n if (for all except some ```\nn```\n values) it is upper-bounded by some constant multiple of ```\ng(n)```\n as ```\nn```\n tends to infinity. More formally:\n\n```\nf(n)```\n is in ```\nO(g(n))```\n iff there exist constants ```\nn0```\n and ```\nc```\n such that for all ```\nn >= n0```\n, ```\nf(n) <= c.g(n)```\n\n\nIn this case, ```\nf(n) = 10n^2 + 10n + 20```\n, so ```\nf(n)```\n is in ```\nO(n^2)```\n, ```\nO(n^3)```\n, ```\nO(n^4)```\n, etc. The tightest upper bound is ```\nO(n^2)```\n.\n\nIn layman's terms, what this means is that ```\nf(n)```\n grows no worse than quadratically as ```\nn```\n tends to infinity.\n\nThere's a corresponding Big-Omega notation which can be used to lower-bound functions in a similar manner. In this case, ```\nf(n)```\n is also ```\nOmega(n^2)```\n: that is, it grows no better than quadratically as ```\nn```\n tends to infinity.\n\nFinally, there's a ```\nBig-Theta```\n notation which combines the two, i.e. iff ```\nf(n)```\n is in ```\nO(g(n))```\n and ```\nf(n)```\n is in ```\nOmega(g(n))```\n then ```\nf(n)```\n is in ```\nTheta(g(n))```\n. In this case, ```\nf(n)```\n is in ```\nTheta(n^2)```\n: that is, it grows exactly quadratically as ```\nn```\n tends to infinity.\n\n--> The point of all this is that as ```\nn```\n gets big, the linear (```\n10n```\n) and constant (```\n20```\n) terms become essentially irrelevant, as the value of the function is far more affected by the quadratic term. <--\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation algorithms\r\n                \r\n\nI have this problem and I don't understand how the answer for quadratic/logarithm and Exponential order is 40, 15 & 100. I know the formulas for these are O(n2), O(log(n)), O(2n) but not able to correlate the answers with these formulas. What is n in this problem? Is it the size of the data set (20) or is it 2? Please let me know your thoughts.\n    ", "Answer": "\r\n```\nn```\n refers to the size of the input; 10/20 in this case. But the important part is not the actual value of ```\nn```\n but the ratio between the previous and current ```\nn```\n (in this case, doubling the input).\n\nIf the algorithm is O(1), doubling the size of the input doesn't change the result. (10 => 10)\nIf the algorithm is O(n), doubling the input doubles the output. (10 => 20)\nIf the algorithm is O(n^2), doubling the input quadruples the output. (10 => 40)\nIf the algorithm is O(log(n)), doubling the input multiplies the output by log(2). (10 => 15)\nIf the algorithm is O(k^n), doubling the input is like raising the output to the 2nd power. (10 => 100).\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big Oh Notation O((log n)^k) = O(log n)?\r\n                \r\nIn big-O notation is ```\nO((log n)^k) = O(log n)```\n, where ```\nk```\n is some constant (e.g. the number of logarithmic for loops), true?\n\nI was told by my professor that this statement was true, however he said it will be proved later in the course. I was wondering if any of you could demonstrate its validity or have a link where I could confirm if it is true.\n    ", "Answer": "\r\n(1) It is true that O(log(n^k)) = O(log n).\n\n(2) It is false that O(log^k(n)) (also written O((log n)^k)) = O(log n).\n\nObservation: (1) has been proven by nmjohn.\n\nExercise: prove (2). (Hint: f(n) = log^2 n is O(log^2 n). Is it O(log n)? What is a sufficiently large constant c such that, for all n greater than n0, c log n > log^2 n?)\n\nEDIT:\n\nOn a related note, anybody who finds this question helpful and/or interesting should go show some love for the new \"Computer Science\" StackExchange site. Here's a link. Go make this new place a reality!\n\nhttp://area51.stackexchange.com/proposals/35636/computer-science-non-programming?referrer=rpnXA1_2BNYzXN85c5ibxQ2\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation constants\r\n                \r\nGiven the following functions f(n) and g(n), is f in O(g(n)) or is f in Θ(g(n)),\nor both? If true, specify a constant c and point n0, if false, briefly specify why.\n\n(a) f(n) = 2n , g(n) = 2^2n\n\n(b) f(n) = n!, g(n) = 2n\n\nI do understand for (a), f(n) = O(g(n)) because g(n) upper bounds f(n)\nand for (b), g(n) = O(f(n)) because of dominance relativity on the fact that n! > 2^n..\n\nI have done some research but could not find much on how to calculate the constants c and n0 for this type of questions. thanks for the reply :)\n    ", "Answer": "\r\na)  f(n) = 2n , g(n) = 2^(2n)\n\n(I added parenthesis.)\n\n```\nf(n) = O(g(n)) iff | f(n) | <= C * | g(n) | for some C>0 and all n > n0\n\n2n <= 1*(2^(2n)) for n>1\n```\n\n\nTherefore 2n = O(2^(2n)). My constants are ```\nC=1```\n and ```\nn0 = 1```\n. But others work too.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Understanding Big O notation - Cracking the Coding Interview\r\n                \r\nI need help understanding how the author got the answer of problem 11 in the Big O chapter. \n\nThe problem goes like this:\n\n\n  The following code prints all strings of length k where the characters are in sorted order. It does this by generating all strings of length k and then checking if each is sorted. What is its runtime?\n\n\n```\npublic static int numChars = 26;\n\npublic static void printSortedStrings(int remaining) {\n    printSortedStrings(remaining, \"\");\n}\n\npublic static void printSortedStrings(int remaining, String prefix) {\n    if (remaining == 0) {\n        if (isInOrder(prefix)) {\n            System.out.println(prefix); // Printing the string\n        }\n    } else {\n        for (int i = 0; i < numChars; i++) {\n            char c = ithLetter(i);\n            printSortedStrings(remaining - 1, prefix + c);\n        }\n    }\n}\n\npublic static boolean isInOrder(String s) {\n    for (int i = 1; i < s.length(); i++) {\n        int prev = ithLetter(s.charAt(i - 1));\n        int curr = ithLetter(s.charAt(i));\n        if (prev > curr) {\n            return false;\n        }\n    }\n    return true;\n}\n\npublic static char ithLetter(int i) {\n    return (char) (((int) 'a') + i);\n}\n\npublic static void main(String[] args) {\n    printSortedStrings(2);\n}\n```\n\n\nBook answer:\n\n\n  O(kck), where k is the length of the string and c is the number of characters in the alphabet. It takes O(ck) time to generate each string. Then, we need to check that each of these is sorted, which takes O(k) time.\n\n\nNotice that printing the string is not taken into account in the answer above, but I've seen the opposite in other problems. \n\nWhen do you take into account printing the string in the runtime? \n\nWould this be the correct answer O(k2ck)?\n\nAlso, any advice on being able to quickly tell that there's an exponential part in the runtime of this code would be greatly appreciated. :)\n    ", "Answer": "\r\nIn short, no. The correct answer is O(kck) as in the book.\nAfter you went over a string to check if its characters were ordered, which would take O(k), printing it would add only O(k) - which does not change your complexity.\n\nSuppose testing whether a string is ordered takes ```\na*k```\n operations, and printing it takes ```\nb*k```\n. Then the total number of operations for each string is at most ```\n(a+b)*k```\n which is still O(k).\n\nEdit: Regarding the second part of your question, going over all words with some fixed length will result in an exponential runtime complexity, since there are ck such words where ```\nc```\n is the size of the alphabet and ```\nk```\n is the length of the word.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "JS: Optimal flattening function big O notation\r\n                \r\nI am working through creating a flatten function on my own and I came up with this:\n```\nfunction flatten(newArr, result = []) {\n   for (let x = 0; x < newArr.length; x++) {\n      if (Array.isArray(newArr[x])) {\n         flatten(newArr[x], result) \n      } else {\n         result.push(newArr[x])\n      }\n   }\n     return result\n}\n```\n\nAre there any pitfalls here that I am missing?  I assume the big O would be O(n) since this is a linear loop that depends on the numb\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What are the Big O Notation for these for loops?\r\n                \r\nI am finishing up an assignment for my class, this particular section involves giving an \"analysis\" of the running time for several for loops. The instructor specified that he wants a Big Oh Notation answer.\n\nI am building on the foundation that total running time is based on:\n\n```\n1)The cost of executing each statement\n2)The frequency of execution of each statement\n3)The idea of working from \"inside and out\", specifically starting from inner loops.\n```\n\n\nI know that:\n\n```\ntotal = 0;\n\n   for (int i = 0; i < n;i++){\n    total++;\n}\n```\n\n\nAnswer: O(N) its running linear.\n\n```\nfor (int i = 0; i < n;++i)\n     for (int j = 0; j < n;++j)\n     total++;\n```\n\n\nAnswer: O(N^2), we only care about how large N grows.\n\nI am confused on \n\n```\nfor (int i = 0;i < n;++i)\n     for (j=0;j < n * n;++j)\n     ++total;\n```\n\n\nand\n\n```\nfor ( i = 0;i < n;++i)\n     for (j = 0; j < i;++j)\n     ++total;\n```\n\n\nAnd last but not least, I am assuming from my textbook that all Triple nested loops are running at N^3 time?\n    ", "Answer": "\r\nYou can analyze your algorithms using Sigma notation, to count/expand the number of iterations run by the inner for loop of your algorithms:\n\n\n\nWhere ```\nT_a```\n covers\n\n```\nfor (i = 0; i < n; ++i)\n    for (j = 0; j < n * n; ++j)\n        ++total;\n```\n\n\nand ```\nT_b```\n:\n\n```\nfor (i = 0; i < n; ++i)\n    for (j = 0; j < i; ++j)\n        ++total;\n```\n\n\n\n\nFinally, a note on your question:\n\n\n  \"And last but not least, I am assuming from my textbook that all\n  Triple nested loops are running at ```\nN^3```\n time?\"\n\n\nThis is not true: it depends on how the iterate is increased as well as bounded in the signature of each loop. Compare e.g. with the inner loop in ```\nT_a```\n above (bounded by ```\nn^2```\n, but simply increased by ```\n1```\n in each iteration) or e.g. the algorithm analyzed in this answer, or, for a slightly trickier case, the single loop analyzed in this answer.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Fibonacci: Time Complexity and Big O Notation\r\n                \r\nI have seen several solution of Fibonacci from different tutorials site and one thing that I noticed is that they have the same way of solving the problem through recursive function. I tested the recursive function and it takes 77 seconds before I get the 40th item in the list so I tried to make a function without dealing recursive function through a for loop and it only takes less than a second. Did I made it right? What is the O notation of my function?\n\n```\nfrom time import time\n\n\ndef my_fibo(n):\n    temp = [0, 1]\n    for _ in range(n):\n        temp.append(temp[-1] + temp[-2])\n    return temp[n]\n\n\nstart = time()\nprint(my_fibo(40), f'Time: {time() - start}')\n# 102334155 Time: 0.0\n```\n\n\nvs\n\n```\nfrom time import time\ndef recur_fibo(n):\n    if n <= 1:\n        return n\n    else:\n        return recur_fibo(n - 1) + recur_fibo(n - 2)\n\n\nstart = time()\nprint(recur_fibo(40), f'Time: {time() - start}')\n# 102334155 Time: 77.78924512863159\n```\n\n    ", "Answer": "\r\nWhat you have done is an example of the time-space tradeoff. \n\nIn the first (iterative) example, you have an O(n) time algorithm that also takes O(n) space. In this example, you store values so that you do not need to recompute them.\n\nIn the second (recursive) example, you have an O(2^n) time (See Computational complexity of Fibonacci Sequence for further details) algorithm that takes up significant space on the stack as well.\n\nIn practice, the latter recursive example is a 'naive' approach at handling the Fibonacci sequence and the version where the previous values are stored is significantly faster.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation Confusion\r\n                \r\nOrdering from smallest to largest and wondering where I have made errors? Equivalents are on the same line. I'm really confused about where O(sqrt(n)) would fall on the list?\n\n```\n1. O(log n)\n2. O(n)\n3. O(2^2 n)\n4. O(2n log n)\n5. O(n log n)\n6. O(n log n^2)\n7. O(sqrt(n))\n8. O(n^1.5) \n9. O(n^2)   O(2n^2)  O(n^2 log n)\n10. O(n^3)\n11. O(k^2) O(2^n)\n```\n\n    ", "Answer": "\r\nThis might help you, in better understanding the order of complexites.\n\n\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "big O-notation of a tree traversal method\r\n                \r\nWhat is the big O runtime complexity of a method which for every node in a tree-structure (in a preorder traversal) has to touch all ancestor nodes, too? O(n * avg(tree-height))? That is the runtime complexity of our method/function is in O(n * avg(tree-height))? (in the average case).\n\nMaybe avg(tree-height) could be defined as (min + max) / 2, but hm\n    ", "Answer": "\r\nI may have misunderstood, but if you're saying you need to go through all nodes (n) and then touch each anscestor per node (worst case: log n in a balanced binary tree) then I would think it would be ```\nO(n*Log(n))```\n.  Constants do not apply to big O notation, but (as pointed out by @interjay below) averages could be.\n\nAssuming an unbalanced tree, your worst case for touching all ancestors is approximately ```\nn/2```\n, which is just n in big O.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Exponents in big-O notation\r\n                \r\nIs 3n = O(2n)? how about (3/2)n = O(2n)? Can you explain the answers?\n\nI got false for the first since, 3n grows faster then 2n no matter what constant C you multiply 2n by. And same for the second?\n\nHow about log(3n) = O(log (2n) )? I think we can't determine this because we don't know the base of the log.\n    ", "Answer": "\r\nLet's actually prove a stronger result: for any constants r0 and r1 where 1 ≤ r0 < r1, it is true that r0n = O(r1n) and it is false that r1n = r0n.  This proves your result as a special case, since 1 < 3/2 < 2.\n\nTo prove the first part, we'll show that r0n = O(r1n).  To do this, we'll use the definition of big-O and find values of n0 and c such that for any n > n0, we have that\n\n\n  r0n ≤ c r1n\n\n\nWe can choose n = n0 and can choose c = 1.  The above inequality then holds, so by definition we have that r0n = O(r1n).\n\nTo prove the second part, we'll show that r1n ≠ O(r0n).  To do this, we'll proceed by contradiction.  Assume for the sake of contradiction that there exists a choice of c and n0 such that for any n > n0, we have that\n\n\n  r1n ≤ c r0n\n\n\nTake the log of both sides to get\n\n\n  n log r1 ≤ log (c r0n)\n  \n  n log r1 ≤ log c + n log r0\n  \n  n (log r1 - log r0) ≤ log c\n  \n  n log(r1 / r0) ≤ log c\n  \n  n ≤ log c / (log(r1 / r0))\n\n\nBut now we're in trouble, since this statement should hold for any choice of n.  However, if we pick any choice of n greater than log c / (log(r1 / r0)), the statement becomes false.\n\nWe have reached a contradiction, so our assumption must have been wrong.  Thus if 1 < r0 < r1, we have that r1n ≠ O(r0n).\n\nHope this helps!\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the big O notation of the Bailey–Borwein–Plouffe algorithm (n-th hexadecimal digit of Pi)?\r\n                \r\nIf I want to compute the n-th hexadecimal digit of Pi with\nhttp://en.wikipedia.org/wiki/Bailey-Borwein-Plouffe_formula\n\nwhat is the big O notation \nhttp://en.wikipedia.org/wiki/Big_O_notation\n\nfor the Bailey–Borwein–Plouffe algorithm?\n    ", "Answer": "\r\nIn the paper the upper bound of ```\nO(n log^3(n))```\n bit complexity is presented for calculations of the digits of ```\nlog(2)```\n. Since they present a more generic formula later on, which also covers pi, I would think that it won't differ much from the above upper bound. But I haven't verified that.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big -O notation\r\n                \r\nHey i have a question.\n say ```\nt(n) = O(n log(n))```\n and u know that this is true.\n\nand then your given these statements and told to say whether they must be true or false. ```\nt(n) = n^4```\n and ```\nt(n) = O(N^4)```\n    \n\nThe statement ```\nt(n) = n^4```\n is false while the statement ```\nt(n) = O(N^4)```\n  is true. Why?\n    ", "Answer": "\r\nYou have to remember that when you write ```\nt(n) = O(n log(n))```\n and ```\nt(n) = O(N^4)```\n, what it actually means is that ```\nt(n)```\n is in ```\nO(...)```\n, not that it's equal to it (as ```\nO(...)```\n is a set of functions and a function can not be equal to a set of functions). However when you write ```\nf(n) = n^4```\n, that means that ```\nf(n)```\n is equal to ```\nn^4```\n.\n\nNow if ```\nf(n)```\n is in ```\nO(n log n)```\n, it is also in ```\nO(n^4)```\n because ```\nO(n^4)```\n is a superset of ```\nO(n log n)```\n. However it can not be equal to ```\nn^4```\n, because ```\nn^4```\n is not in ```\nO(n log n)```\n.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Fibonacci: Time Complexity and Big O Notation\r\n                \r\nI have seen several solution of Fibonacci from different tutorials site and one thing that I noticed is that they have the same way of solving the problem through recursive function. I tested the recursive function and it takes 77 seconds before I get the 40th item in the list so I tried to make a function without dealing recursive function through a for loop and it only takes less than a second. Did I made it right? What is the O notation of my function?\n\n```\nfrom time import time\n\n\ndef my_fibo(n):\n    temp = [0, 1]\n    for _ in range(n):\n        temp.append(temp[-1] + temp[-2])\n    return temp[n]\n\n\nstart = time()\nprint(my_fibo(40), f'Time: {time() - start}')\n# 102334155 Time: 0.0\n```\n\n\nvs\n\n```\nfrom time import time\ndef recur_fibo(n):\n    if n <= 1:\n        return n\n    else:\n        return recur_fibo(n - 1) + recur_fibo(n - 2)\n\n\nstart = time()\nprint(recur_fibo(40), f'Time: {time() - start}')\n# 102334155 Time: 77.78924512863159\n```\n\n    ", "Answer": "\r\nWhat you have done is an example of the time-space tradeoff. \n\nIn the first (iterative) example, you have an O(n) time algorithm that also takes O(n) space. In this example, you store values so that you do not need to recompute them.\n\nIn the second (recursive) example, you have an O(2^n) time (See Computational complexity of Fibonacci Sequence for further details) algorithm that takes up significant space on the stack as well.\n\nIn practice, the latter recursive example is a 'naive' approach at handling the Fibonacci sequence and the version where the previous values are stored is significantly faster.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the Big O notation of for loops located that are nested through methods?\r\n                \r\nI'm unsure of what the Big O notation would be for the following code (Algorithm name: Remove Shift):\n```\nMethod(main)\n    for i=1 to n \n        if isDuplicate (i)\n            remove (i)\n        endif\n    endfor\nreturn n\n\nMethod: isDuplicate (i)\n    for j=1 to n\n        if A[j] = A[i]\n            return true\n        endif\n    endfor\n    return false\n\nMethod: remove (i)\n// Removes and performs a left-shift.\n    for j=n-1 to i\n        A[j] = A[j+1]\n    endfor\n    n = n -1\n```\n\nSo, I know for the runtime of our first loop in main is ```\nO(n)```\n. The duplicate method is yielding an ```\nO(n)```\n for the worst case. To remove, there is remove and performing the left-shifting, so for this purpose obviously, we have to check whole elements making it ```\nO(n)```\n as well. Would that make the runtime of Remove shift, ```\nO(n)```\n or ```\nO(n^3)```\n? I'm confused.\n    ", "Answer": "\r\nI think it is n^3\nin the first loop in main you are visiting all numbers from 1 to n this is O(n)\ninside it the if condition have function  isDuplicate (i) it is also take O(n) now we have O(n^2) inside the if we have function remove it takes O(n) for every element\nnow we have O(n^3) in the worst case and O(n^2) if the function isDuplicate (i) return false....\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O,theta and omega notation\r\n                \r\nI am really confused what big O,big theta and big omega represent: best case, worst case and average case or upper bound and lower bound. \n\nIf the answer is upper bound and lower bound, then whose upper bound and lower bound? For example let's consider an algorithm. Then does it have three different expressions or rate of growths for best case, lower case and average case and for every case can we find it's big O, theta and omega. \n\nLastly, we know merge sort via divide and conquer algorithm has a rate of growth or time complexity of n*log(n), then is it the rate of growth of best case or worst case and how do we relate big O, theta and omega to this. please can you explain via a hypothetical expression.\n    ", "Answer": "\r\nThe notations are all about asymptotic growing. If they explain the worst or the average case depends only on what you say it should express.\n\nE.g. quicksort is a randomized algorithm for sorting. Lets say we use it deterministic and always choose the first element in the list as pivot. Then there exists an input of length ```\nn```\n (for all ```\nn```\n) such that the worst case is ```\nO(n²)```\n. But on random lists the average case is ```\nO(n log n)```\n.\n\nSo here I used the big O for average and worst case.\n\nBasically this notation is for simplification. If you have an algorithm that does exactly ```\n5n³-4n²-3logn```\n steps you can simply write ```\nO(n³)```\n and get rid of all the crap after ```\nn³```\n and also forget about the constants.\n\nYou can use big O to get rid of all monomials except for the one with the biggest exponent and all constant factors (constant means they don't grow, but 10100 is also constant)\n\nAt the end you get with ```\nO(f(n))```\n a set of functions, that all have the upper bound ```\nf(n)```\n (this means ```\ng(n)```\n is in ```\nO(f(n))```\n, if you can find a constant number  ```\nc```\n such that ```\ng(n) ≤ c⋅f(n)```\n)\n\nTo make it a little easier:\nI have explained that big O means an upper bound but not strict. so ```\nn³```\n is in ```\nO(n³)```\n, but also ```\nn²```\n.\nSo you can think about big O as a \"lower equal\".\n\nThe same way you can do with the others.\n\nLittle o is a strict lower: ```\nn²```\n is in ```\no(n³)```\n, but ```\nn³```\n is not.\nBig Omega is a \"greater equal\": ```\nn³```\n is in ```\nΩ(n³)```\n and also ```\nn⁴```\n.\nThe little omega is a strict \"greater\": ```\nn³```\n is not in ```\nω(n³)```\n but ```\nn⁴```\n is.\nAnd the big Theta is something like \"equal\" so n³ is in ```\nΘ(n³)```\n, but neither ```\nn²```\n nor ```\nn⁴```\n is.\n\nI hope this helps a little.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the Big O notation of this method?\r\n                \r\nI have run across this method in our code base and wonder what the Big O is.  The method takes a flat list and creates a tree, assigning the Parent and Children values as it goes.\n\n```\nprivate void AddChildren(Group group, IEnumerable<Group> groups)\n{\n    foreach (var g in groups)\n    {\n        if (g.ParentId == group.Id)\n        {\n            g.Parent = group;\n            group.Children.Add(g);\n            AddChildren(g, groups);\n        }\n    }\n}\n```\n\n\nIt's been a while since I have done Big O outside of identifying straight forward n^2 (or worse) methods, but my take on it goes like this:\n\n\nWe are iterating every node in the list, giving us n\nWe are using a conditional to process a subset of the items being iterated.  There can be multiple matches here and don't know how to express that number, or how it modifies the recursive call to AddChildren\nWe have some simple assignments, and I don't know if that warrants a +1 modifier\nWe are recursing but it's not for every item in the enclosing iteration\n\n\nJust tossing something out there so I can see if I was in the ballpark:\n\nn + (x * n) \n\nwhere x is the number of matches in the if loop. \n\nAny thoughts on what this actually is would be great, thanks.\n    ", "Answer": "\r\nObserve that the recursive function is only called once per parent-child relationship. In a tree structure with n nodes, there are n - 1 such relationships, so ```\nAddChildren()```\n is called n times (including the initial call). In each call, the work performed by the method itself (excluding the recursive call) is O(n) due to the iteration. Hence, O(n^2) in total.\n\nYou can improve the complexity to O(n) by putting all groups in a hashmap first and traverse the list once, looking up each parent node in the hashmap.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "how to determine the Big-O notation of this equation?\r\n                \r\n0.0001*2^n + 10 * n^2 + 10000 * log(n) + 10000000\n\ndo we look at the highest exponent power?\n\nThe big O of this is O(n^2)? Am i right?\n    ", "Answer": "\r\n\n  The big O of this is O(n^2)? Am i right?\n\n\nNo, ```\n2^n```\n will be dominating over everything else, despite its tiny coefficient.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Prove Big O notation\r\n                \r\nI am a beginner and taking an online algorithm course, and when I refer a book, I found out the following questions.\n\n\nGiven that f(x) = 2x^2 + 5x +3 and g(x) = 2x^3 +x -100, prove f(x) = O(g(x))\n\n```\nLet c = 4 and x = 5, \n2x^2 + 5x +3 <= c(2x^3 +x -100)  \nI tried to solve the left side as follows. \nif x >=1, then x <=x^3, 5x <= 5x^3 ... \n2x^2 + 5x +3 => 2x^3 + 5x^3 +3x^3\n             => 10x^3\n```\n\n\nMy question is can I ignore the -100 of g(x) and compare it with 10x^3 with some value of c?\nIs it correct to say the run time of the below algorithm is O(n(logn))?\n\n```\nint foo(int n){\n    int i,j,k=0; \n    for(i = n/2; i <= n; i++){\n        for(j = 2; j<= n; j = j * 2){\n            k += n/2; \n        } \n    } \n    return k; \n}\n```\n\n\n\nThanks for any explanation.\n    ", "Answer": "", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big-O Notation Q\r\n                \r\nI have a few True/False questions I need to answer, I don't know how to do it. Any help would be appreciated!\n\n(a) True or false: Suppose that you develop an algorithm whose run-time is given by f(n) = 5f(n/2) + 3 for n = 2,4,8,16,32,..., and f(1) = 7. Then the run-time of your algorithm is O(n3), but it is not O(n2).\n\n(b) True or false: f (n) = n3n satisfies the recurrence relation f (n) = 6f (n − 1) − 9f(n−2).\n    ", "Answer": "\r\nFor part (a) you can apply the master theorem. The non recursive overhead is O(1), the 'critical' function n^{log_b(a)} is n^{\\log_2(5)} = n^{2.321928}. So the algorithm runs in θ(n^{2.321928}), meaning it is not O(n^2). But, it is O(n^3).\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O notation complexity\r\n                \r\nConsider an operation like binary multiplication between one array with size (1*4) and matrix with size (4*8), as you know we obtain as the output an array of size (1*8). \n\nIs the complexity of this operation O(1) ?\n    ", "Answer": "\r\nThe sizes of the matrices are specified in your question.\n\nSince the size of the problem never changes the complexity will be O(1).\n\nThe complexity would be O(n) when you would need to treat your input data exactly once.\n\nFor example, multiplying every number in an array of length n by 2 is of complexity O(n).\n\nNow as far as your problem is concerned, if the size of your matrices was variable then the run time would be O(n^3), if we assume a naive implementation.\n\nSee Wikipedia for more details.  \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "what is the variable 'C' refers to in Big O or Omega notation\r\n                \r\nIn Big O or Omega notation, I understand that n refers to the input to the program. But what is the variable C refers to?\n    ", "Answer": "\r\nWhile it's hard to answer this question without knowing where you saw a ```\nC```\n in discussion of big O notation, I suspect it was used to represent a constant of some kind.\nFor instance, you can use ```\nC```\n in translating a statement using Big-O notation to a statement using predicate logic terminology:\n\n```\nf(x) = O(g(x))```\n means:\nThere exist positive real numbers ```\nC```\n and ```\nx0```\n, such that for all ```\nx >= x0```\n, ```\nf(x) <= C * g(x)```\n\n\nThe choice of ```\nC```\n for the name of the constant multiple here is completely arbitrary. ```\nC```\n is probably popular simply because it's the first letter of \"constant\". At most, it's a convention.\nYou could use some other letter and the meaning would be the same. The Wikipedia page on the topic (at the time I'm writing this) uses ```\nM```\n in most of its equations (though ```\nC```\n sneaks into a few of them further down the page). It's entirely possible you saw ```\nC```\n in one description of big-O notation, but then read some other description of it that didn't use ```\nC```\n at all.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Big O Notation of an expression\r\n                \r\nIf I have an algorithm that takes 4n^2 + 7n moves to accomplish, what is its O? \nO(4n^2)?\nO(n^2)?\n\nI know that 7n is cut off, but I don't know if I should keep the n^2 coefficient or not.\n\nThanks\n    ", "Answer": "\r\nYou should drop any coefficients because the question is really asking \"on the order of\", which tries to characterize it as linear, exponential, logarithmic, etc...   That is, when n is very large, the coefficient is of little importance.  \n\nThis also explains why you drop the +7n, because when n is very large, that term has relatively little significance to the final answer.  If you are familiar with calculus, you might say lim n->inf(4*n^2+7n) ~= lim n->inf(4*n^2) ~= lim n->inf(n^2)\n\nYou can also think about this in a graphical sense...   that is, if you graph the function 4n^2 + 7n for larger and larger values of n, a mathematician might say \"it looks like n^2\".  Granted, it would have to be a fairly liberal mathematician, as this isn't a rigorous statement, but that's basically what O(...) is trying to convey.  \n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "What is the Big O notation of a loop in loop\r\n                \r\nI have this code and cannot understand the Big-O of this... Thanks\n```\nfor(i = 0; i<n; i++){\n  for(j = i; j<n; j++){\n    if (arr[j]%2!=0){\n       if (minodd > arr[j]){\n       }\n    }\n  }\n}\n```\n\n    ", "Answer": "\r\nOne of the best ways to approach this problem is to break it down into smaller parts.\nFirst, lets look at your inner loop:\n```\nfor(j = i; j<n; j++){\n    if (arr[j]%2!=0){    // O(1)\n       if (minodd > arr[j]){ // O(1)\n       }\n    }\n  }\n```\n\nThe if-statements are O(1) or constant time so we can ignore those and we get just the inner for loop:\n```\nfor(j = i; j<n; j++){\n... // O(1) + O(1)\n}\n```\n\nSince the worst case scenario is it loops n times we have ```\nO(n) + O(1) + O(1)```\n which can be simplified to ```\nO(n)```\n which is called linear time.\nNext, lets zoom out and replace the inner loop with our new info:\n```\nfor(i = 0; i<n; i++){\n  for(j = i; j<n; j++){\n    if (arr[j]%2!=0){\n       if (minodd > arr[j]){\n       }\n    }\n  }\n}\n```\n\nbecomes:\n```\nfor(i = 0; i<n; i++){\n    O(n)\n}\n```\n\nSince we know the outside for loop will cycle n times in the worst case, and the inside for loop will cycle n times in the worst case: We get O(n x n) or ```\nO(n²)```\n which is also know as polynomial time.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
{"Question": "Find tightest big-O\r\n                \r\nI'm trying to learn and understand how to find tightest big O notation. Here I need to find tightest big-O notation for these algorithms, and  I did the calculating for the running time.\n\nNow I need to prove or find the tightest big-O notation but I'm not sure where should I start.\n\n1) ```\n2 n^2+ 2 n +2= O(n^2)```\n\n\n2) ```\n6 n log n +4n +2 =O (n log n)```\n\n\n3) ```\n6 X1000 n+ 4n +2 = O(n)```\n\n\nNot really sure how to solve this part from question. How I make sure my equation is tightest big-O?\n\nAny help or suggestions would be greatly appreciated, thanks!\n    ", "Answer": "\r\nSimply said : You take the one that is \"highest\" term and remove all constant multipliers.\n\nThe reasoning behind this is that as ```\nn```\n grows, the highest term will contribute the most to the total time. So rest will become negligible for big enough ```\nn```\n. And removing the constant multipliers is in definition of time complexity.\n    ", "Knowledge_point": "Big O Notation", "Tag": "算法分析"}
