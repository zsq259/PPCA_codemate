{"Question": "Edit Distance in Python\r\n                \r\nI'm programming a spellcheck program in Python. I have a list of valid words (the dictionary) and I need to output a list of words from this dictionary that have an edit distance of 2 from a given invalid word. \n\nI know I need to start by generating a list with an edit distance of one from the invalid word(and then run that again on all the generated words). I have three methods, inserts(...), deletions(...) and changes(...) that should output a list of words with an edit distance of 1, where inserts outputs all valid words with one more letter than the given word, deletions outputs all valid words with one less letter, and changes outputs all valid words with one different letter.\n\nI've checked a bunch of places but I can't seem to find an algorithm that describes this process. All the ideas I've come up with involve looping through the dictionary list multiple times, which would be extremely time consuming. If anyone could offer some insight, I'd be extremely grateful.\n    ", "Answer": "\r\nThe thing you are looking at is called an edit distance and here is a nice explanation on wiki. There are a lot of ways how to define a distance between the two words and the one that you want is called Levenshtein distance and here is a DP (dynamic programming) implementation in python.\n```\ndef levenshteinDistance(s1, s2):\n    if len(s1) > len(s2):\n        s1, s2 = s2, s1\n\n    distances = range(len(s1) + 1)\n    for i2, c2 in enumerate(s2):\n        distances_ = [i2+1]\n        for i1, c1 in enumerate(s1):\n            if c1 == c2:\n                distances_.append(distances[i1])\n            else:\n                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n        distances = distances_\n    return distances[-1]\n```\n\nAnd a couple of more implementations are here.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Normalizing the edit distance\r\n                \r\nI have a question that can we normalize the levenshtein edit distance by dividing the e.d value by the length of the two strings?\nI am asking this because, if we compare two strings of unequal length, the difference between the lengths of the two will be counted as well. \nfor eg:\ned('has a', 'has a ball') = 4 and ed('has a', 'has a ball the is round') = 15. \nif we increase the length of the string, the edit distance will increase even though they are similar.\nTherefore, I can not set a value, what a good edit distance value should be.\n    ", "Answer": "\r\nYes, normalizing the edit distance is one way to put the differences between strings on a single scale from \"identical\" to \"nothing in common\".\n\nA few things to consider:\n\n\nWhether or not the normalized distance is a better measure of similarity between strings depends on the application. If the question is \"how likely is this word to be a misspelling of that word?\", normalization is a way to go. If it's \"how much has this document changed since the last version?\", the raw edit distance may be a better option.\nIf you want the result to be in the range ```\n[0, 1]```\n, you need to divide the distance by the maximum possible distance between two strings of given lengths. That is, ```\nlength(str1)+length(str2)```\n for the LCS distance and ```\nmax(length(str1), length(str2))```\n for the Levenshtein distance.\nThe normalized distance is not a metric, as it violates the triangle inequality.\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein Edit Distance is not calculating edit distance\r\n                \r\nI am trying to get my Levenshtein Edit Distance algorithm working but for some reason, the number of edits is coming out incorrect. I can't see where my mistake is and I was wondering if someone see's what I am doing incorrectly.\n\n\n  Input\n\n\n```\n5\nATCGTT\nAGTTAC\nACGAAT\nCCGTAAAT\nTTACGACCAGT\n```\n\n\n\n  expected output\n\n\n```\nStrand A: ATCGTT--\nStrand B: A--GTTAC\nEdit Distance: 4\n\nStrand A: ATCG-TT\nStrand B: A-CGAAT\nEdit Distance: 3\n\nStrand A: ATCGT---T\nStrand B: -CCGTAAAT\nEdit Distance: 5\n\nStrand A: AT-CG----TT\nStrand B: TTACGACCAGT\nEdit Distance: 7\n\nStrand A: AGTTAC\nStrand B: ACGAAT\nEdit Distance: 4\n\nStrand A: -AGT-TAC\nStrand B: CCGTAAAT\nEdit Distance: 5\n\nStrand A: --A-G-TTA-C\nStrand B: TTACGACCAGT\nEdit Distance: 8\n\nStrand A: ACG--AAT\nStrand B: CCGTAAAT\nEdit Distance: 3\n\nStrand A: --ACGA--A-T\nStrand B: TTACGACCAGT\nEdit Distance: 5\n\nStrand A: --CCG-TAAAT\nStrand B: TTACGACCAGT\nEdit Distance: 7\n```\n\n\n\n  my output\n\n\n```\nStrand A: ATCGT-\nStrand B: AGTTAC\nEdit Distance: 5\n\nStrand A: ATC-T-\nStrand B: ACGAAT\nEdit Distance: 5\n\nStrand A: ATC-T-\nStrand B: CCGTAAAT\nEdit Distance: 5\n\nStrand A: A-C-T-\nStrand B: TTACGACCAGT\nEdit Distance: 10\n\nStrand A: AGTTAC\nStrand B: ACGAAT\nEdit Distance: 5\n\nStrand A: AG-TAC\nStrand B: CCGTAAAT\nEdit Distance: 6\n\nStrand A: A--T-C\nStrand B: TTACGACCAGT\nEdit Distance: 7\n\nStrand A: AC-AAT\nStrand B: CCGTAAAT\nEdit Distance: 7\n\nStrand A: AC---T\nStrand B: TTACGACCAGT\nEdit Distance: 8\n\nStrand A: CC-TAAAT\nStrand B: TTACGACCAGT\nEdit Distance: 8\n```\n\n\n\n  findEditDistance\n\n\n```\nvoid EditDistance::findEditDistance()\n{\n    int upperValue, leftValue, diagonalValue;\n\n    for (int i = 0; i < mLengthX; ++i)\n    {\n        table[i][0].stringLength = i;\n    }\n\n    for (int i = 0; i < mLengthY; ++i)\n    {\n        table[0][i].stringLength = i;\n    }\n\n    for (int i = 1; i < mLengthX; ++i)\n    {\n        for (int j = 1; j < mLengthY; ++j)\n        {\n            if (mStringX[i] == mStringY[j])\n            {\n                table[i][j].direction = DIAGONAL;\n                table[i][j].stringLength = table[i - 1][j -1].stringLength;\n            }\n            else\n            {\n                upperValue = table[i - 1][j].stringLength;\n                leftValue = table[i][j - 1].stringLength;\n                diagonalValue = table[i - 1][j - 1].stringLength;\n\n                if (upperValue < leftValue)\n                {\n                    if (upperValue < diagonalValue)\n                    {\n                        //upper is the lowest\n                        table[i][j].stringLength = table[i - 1][j].stringLength + 1;\n                        table[i][j].direction = UP;\n                    }\n                    else\n                    {\n                        //diagonal is lowest\n                        table[i][j].stringLength = table[i - 1][j -1].stringLength + 1;\n                        table[i][j].direction = DIAGONAL;\n                    }\n                }\n                else if (leftValue < diagonalValue)\n                {\n                    //left is lowest\n                    table[i][j].stringLength = table[i][j - 1].stringLength + 1;\n                    table[i][j].direction = LEFT;\n                }\n                else\n                {\n                    //diagonal is lowest\n                    table[i][j].stringLength = table[i - 1][j -1].stringLength + 1;\n                    table[i][j].direction = DIAGONAL;\n                }\n            }\n        }\n    }   \n}\n```\n\n\n\n  getDistance\n\n\n```\nvoid EditDistance::getDistance()\n{\n    int i = mStringX.length() - 1;\n    int j = mStringY.length() - 1;\n\n    numEdits = 0;\n\n    updateStrands (i, j);\n}\n```\n\n\n\n  updateStrands\n\n\n```\nvoid EditDistance::updateStrands (int i, int j)\n{\n    if (i == 0 || j == 0)\n    {\n        return;\n    }\n\n    if (table[i][j].direction == DIAGONAL)\n    {\n        ++numEdits;\n        updateStrands (i - 1, j - 1);\n    }\n    else if (table[i][j].direction == UP)\n    {\n        mStringY[j] = '-';\n        ++numEdits;\n        updateStrands (i - 1, j);\n    }\n    else\n    {\n        mStringX[i] = '-';\n        ++numEdits;\n        updateStrands (i, j - 1);\n    }\n}\n```\n\n    ", "Answer": "\r\nThe problem with the edit distance is in your ```\nupdateStrands```\n. It counts diagonal moves as 1, when in fact a diagonal move can have a distance of 1 (substitution) or 0 (match). You could fix this in ```\nupdateStrands```\n, but there's really no need to do the calculation there at all, when the number is already in the lower-right corner of ```\ntable```\n.\n\nIf you want the correct \"strands\" (e.g. \"ATCGTT--\" and \"A--GTTAC\"), you will have to make corrections in ```\nupdateStrands```\n (you change elements of the strings where you should insert), ```\ngetDistance```\n (you start in the wrong place) and ```\nfindEditDistance```\n (you neglect to assign values to ```\ndirection```\n along the upper and left edges when you set ```\nstringLength```\n to ```\ni```\n).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Discriminate edit distance\r\n                \r\nThe levenshtein edit distance cares only about how many edits are done and not on what exactly they are, so the following two pairs will have the same edit distance.\n```\n(\"A P Moller - Maersk A\", \"A.P. Moller - Maersk A/S Class A\")\n(\"A P Moller - Maersk A\", \"A.P. Moller - Maersk A/S Class B\")\n```\n\nAre there any algorithms or libraries that can distinguish between these two pairs?\n    ", "Answer": "\r\nYou can use cosine similarity to find the similarity between to text and it produce different similarity between these two text\n```\nimport math\nimport re\nfrom collections import Counter\n\nWORD = re.compile(r\"\\w+\")\n\n\ndef get_cosine(vec1, vec2):\n    intersection = set(vec1.keys()) & set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator\n\n\ndef text_to_vector(text):\n    words = WORD.findall(text)\n    return Counter(words)\n\n```\n\n```\nx =(\"A P Moller - Maersk A\", \"A.P. Moller - Maersk A/S Class A\")\ny =(\"A P Moller - Maersk A\", \"A.P. Moller - Maersk A/S Class B\")\n```\n\n```\ncosine = get_cosine(text_to_vector(x[0]), text_to_vector(x[1]))\n\nprint(\"Cosine1:\", cosine)\n\ncosine1 = get_cosine(text_to_vector(y[0]), text_to_vector(y[1]))\n\nprint(\"Cosine2:\", cosine1)\n```\n\nOutput:\n```\nCosine1: 0.9091372900969896\nCosine2: 0.8366600265340756\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Shortest distance between a point and a line segment\r\n                \r\nI need a basic function to find the shortest distance between a point and a line segment.  Feel free to write the solution in any language you want; I can translate it into what I'm using (Javascript).\n\nEDIT: My line segment is defined by two endpoints. So my line segment ```\nAB```\n is defined by the two points ```\nA (x1,y1)```\n and ```\nB (x2,y2)```\n.  I'm trying to find the distance between this line segment and a point ```\nC (x3,y3)```\n.  My geometry skills are rusty, so the examples I've seen are confusing, I'm sorry to admit.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance with swaps\r\n                \r\nEdit distance finds the number of insertion, deletion or substitutions required to one string to another. I want to to also include swaps in this algorithm. For example \"apple\" and \"appel\" should give a edit distance of 1.\n    ", "Answer": "\r\nThe edit distance that you are defining is called the Damerau–Levenshtein distance. You can find possible implementations on the Wikipedia page.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance Algorithm\r\n                \r\nI have a dictionary of 'n' words given and there are 'm' Queries to respond to. I want to output the number of words in dictionary which are edit distance 1 or 2. I want to optimize the result set given that n and m are roughly 3000. \n\nEdit added from answer below:\n\nI will try to word it differently.\n\nInitially there are 'n' words given as a set of Dictionary words. Next 'm' words are given which are query words and for each query word, I need to find if the word already exists in Dictionary (Edit Distance '0') or the total count of words in dictionary which are at edit distance 1 or 2 from the dictionary words.\n\nI hope the Question is now Clear.\n\nWell, it times out if the Time Complexity is (m*n)n.The naive use of DP Edit Distance Algorithm times out. Even Calculating the Diagonal Elements of 2k+1 times out where k is the threshold here k=3 in above case.\n    ", "Answer": "\r\nYou want to use the Levenshtein distance between two words, but I assume you know that since that's what the question's tags say.\n\nYou would have to iterate through your List (assumption) and compare every word in the list with the current query you're executing. You could build a BK-tree to limit your search space, but that sounds like an overkill if you only have ~3000 words.\n\n```\nvar upperLimit = 2;\nvar allWords = GetAllWords();\nvar matchingWords = allWords\n        .Where(word => Levenshtein(query, word) <= upperLimit)\n        .ToList();\n```\n\n\nAdded after edit of original question\n\nFinding cases where distance=0 would be easy Contains-queries if you have a case insensitive dictionary. Those cases where distance <= 2 would require a complete scan of the search space, 3000 comparisons per query word. Assuming an equal amount of query words would result in 9 million comparisons.\n\nYou mention that it times out, so I presume you have a timeout configured? Could your speed be due to a poor, or slow, implementation of the Levenshtein calculation?\n\n\n(source: itu.edu.tr)\nAbove graph is stolen from CLiki: bk-tree\n\nAs seen, using bk-tree with an edit distance <= 2 would only visit about 1% of the search space, but that's assuming that you have a very large input data, in their case up to a half million words. I would assume similar numbers in your case, but such a low amount of inputs wouldnt cause much trouble even if stored in a List/Dictionary.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Explanation of normalized edit distance formula\r\n                \r\nBased on this paper:\nIEEE TRANSACTIONS ON PAITERN ANALYSIS : Computation of Normalized Edit Distance and Applications In this paper Normalized Edit Distance as followed:\n\n\n  Given two strings X and Y over a finite alphabet, the normalized edit\n  distance between X and Y, d( X , Y ) is defined as the minimum of W( P\n  ) / L ( P )w, here P is an editing path between X and Y , W ( P ) is\n  the sum of the weights of the elementary edit operations of P, and\n  L(P) is the number of these operations (length of P).\n\n\n\n\nCan i safely translate the normalized edit distance algorithm explained above as this:       \n\n```\nnormalized edit distance = \nlevenshtein(query 1, query 2)/max(length(query 1), length(query 2))\n```\n\n    ", "Answer": "\r\nYou are probably misunderstanding the metric. There are two issues:\n\n\nThe normalization step is to divide ```\nW(P)```\n which is the weight of the edit procedure over ```\nL(P)```\n, which is the length of the edit procedure, not over the max length of the strings as you did;\nAlso, the paper showed that (Example 3.1) normalized edit distance cannot be simply computed with levenshtein distance. You probably need to implement their algorithm.\n\n\nAn explanation of Example 3.1 (c):\n\nFrom ```\naaab```\n to ```\nabbb```\n, the paper used the following transformations:\n\n\nmatch ```\na```\n with ```\na```\n;\nskip ```\na```\n in the first string;\nskip ```\na```\n in the first string;\nskip ```\nb```\n in the second string;\nskip ```\nb```\n in the second string;\nmatch the final ```\nb```\ns.\n\n\nThese are 6 operations which is why ```\nL(P)```\n is 6; from the matrix in (a), matching has cost 0, skipping has cost 2, thus we have total cost of ```\n0 + 2 + 2 + 2 + 2 + 0 = 8```\n, which is exactly ```\nW(P)```\n, and ```\nW(P) / L(P) = 1.33```\n. Similar results can be obtained for (b), which I'll left to you as exercise :-)\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Selective edit distance\r\n                \r\nI have data like \n\n```\nMega Mall\nMega Malls\nL & T Gate 6\nL & T Gate 5\nL & T Gate 2\nMegas Mall\nMega Mwll\n```\n\n\nNow the thing is I want to clean it up. I took the edit distance approach using edit distance 1 and Mega Mall case is handled. The short coming is it deletes L & T Gate 5,2 also[ I am keeping the first entry]. Is there any way I can handle this, not deleting these cases and handling typos, etc.\n    ", "Answer": "\r\nYes, you can use a weighted form of edit distance, without really changing the algorithm or its time or space complexity.  Instead of counting any substitution, insertion or deletion as 1, count it as a higher number when the character (or either of the characters, for a substitution) involved is a digit.\n\nIt's even possible to weight specific positions in the string differently.  E.g. you might decide that every letter immediately following 1 or more digits should be considered more important (since e.g. the address 123B is very different from 123).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance with accents\r\n                \r\nAre there some edit-distance in python that take account of the accent.\nWhere for exemple hold the following property\n\n```\nd('ab', 'ac') > d('àb', 'ab') > 0\n```\n\n    ", "Answer": "\r\nWith the Levenshtein module:\n\n```\nIn [1]: import unicodedata, string\n\nIn [2]: from Levenshtein import distance\n\nIn [3]: def remove_accents(data):\n   ...:     return ''.join(x for x in unicodedata.normalize('NFKD', data)\n   ...:                             if x in string.ascii_letters).lower()\n\nIn [4]: def norm_dist(s1, s2):\n   ...:     norm1, norm2 = remove_accents(s1), remove_accents(s2)\n   ...:     d1, d2 = distance(s1, s2), distance(norm1, norm2)\n   ...:     return (d1+d2)/2.\n\nIn [5]: norm_dist(u'ab', u'ac')\nOut[5]: 1.0\n\nIn [6]: norm_dist(u'àb', u'ab')\nOut[6]: 0.5\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance between two graphs\r\n                \r\nI'm just wondering if, like for strings where we have the Levenshtein distance (or edit distance) between two strings, is there something similar for graphs?\n\nI mean, a scalar measure that identifies the number of atomic operations (node and edges insertion/deletion) to transform a graph ```\nG1```\n to a graph ```\nG2```\n.\n    ", "Answer": "\r\nI think graph edit distance is the measure that you were looking for.\n\nGraph edit distance measures the minimum number of graph edit operations to transform one graph to another, and the allowed graph edit operations includes:\n\n\nInsert/delete an isolated vertex\nInsert/delete an edge\nChange the label of a vertex/edge (if labeled graphs)\n\n\nHowever, computing the graph edit distance between two graphs is NP-hard. The most efficient algorithm for computing this is an A*-based algorithm, and there are other sub-optimal algorithms.  \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance in NodeSQlite in Arabic\r\n                \r\nHow can I check the edit distance between Arabic words in Node SQlite?\n\nFor example, to check the edit distance between ```\nرِحْلَة```\n and ```\nرحلة```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Equivalence of Edit Distance and Alignment Distance\r\n                \r\n\n\n(from: https://math.mit.edu/classes/18.417/Slides/alignment.pdf)\nThe slide on the 11th page talks about how the Edit Distance and the Alignment Distance are equivalent.\nI understand how to prove that the Edit Distance will always be less than or equal to the corresponding Alignment Distance. We can always construct a sequence of edits from a given alignments that gives the same result. Since the Edit Distance sequence is the shortest possible, the corresponding sequence is at least as long.\nI don't understand how to prove the reverse inequality. Can anyone provide the proof? Apparently one needs to use the triangle inequality to prove it.\nOn a similar note, are there any resources that treat this topic rigorously? For example, I want proofs of why these two distances satisfy the axioms of metric spaces.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is it always true that the edit distance of two strings is equal to the edit distance of their substrings?\r\n                \r\nSuppose we have two strings:\n\nccttgg\ngacgct\n\nThe edit distance of these two strings is 6.\nPossible substrings are:\n\ncctt--\ngacg--\n\nTheir edit distance is 4.\nThe remaining parts to equal the original two strings are:\n\n----gg\n----ct\n\nand their edit distance is 2.\nSo 4+2=6, that is the original edit distance.\nIs this type of assumption always correct?\nIf it's not, is there a way to compute the edit distance between two strings using the edit distance of their substrings?\n\nEdit: to be clearer my definition of edit distance is the Levenshtein distance with a cost of 1 for insertion, deletion and replace if the characters are not the same and 0 if the characters are equal.\nI'm not considering the Damerau distance with transpositions.\n    ", "Answer": "\r\nNo\nCounterexample\nConsider the strings:\n\naba\nbab\n\nThey have an edit distance of 2 by deleting an \"a\" from the front and adding a \"b\" to the end.\nIf these are broken into substrings such as\n\nab, a\nba, b\n\nthen the first substrings have an edit distance of 2 and the second substrings have an edit distance of 1 for a total of 3.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is there a sparse edit distance algorithm?\r\n                \r\nSay you have two strings of length 100,000 containing zeros and ones. You can compute their edit distance in roughly 10^10 operations. \n\nIf each string only has 100 ones and the rest are zeros then I can represent each string using 100 integers saying where the ones are. \n\n\n  Is there a much faster algorithm to compute the edit distance using\n  this sparse representation?  Even better would be an algorithm that also uses 100^2 space instead of 10^10 space.\n\n\nTo give something to test on, consider these two strings with 10 ones each. The integers say where the ones are in each string.\n\n```\n[9959, 10271, 12571, 21699, 29220, 39972, 70600, 72783, 81449, 83262]\n\n[9958, 10270, 12570, 29221, 34480, 37952, 39973, 83263, 88129, 94336]\n```\n\n\n\n\nIn algorithmic terms, if we have two sparse binary strings of length ```\nn```\n each represented by ```\nk```\n integers each, does there exist an ```\nO(k^2)```\n time edit distance algorithm?\n    ", "Answer": "\r\nOf course! There are so few possible operations with so many 0s. I mean, the answer is at most 200.\n\nTake a look at\n\n```\n10001010000000001\nvs       ||||||\n10111010100000010\n```\n\n\nLook at all the zeroes with pipes. Does it matter which one out of those you end up deleting? Nope. That's the key.\n\n\n\nSolution 1\n\nLet's consider the normal n*m solution:\n\n```\ndp(int i, int j) {\n    // memo & base case\n    if( str1[i-1] == str1[j-1] ) {\n        return dp(i-1, j-1);\n    }\n    return 1 + min( dp(i-1, j), dp(i-1, j-1), dp(i, j-1) );\n}\n```\n\n\nIf almost every single character was a 0, what would hog the most amount of time?\n\n```\nif( str1[i-1] == str1[j-1] ) { // They will be equal so many times, (99900)^2 times!\n    return dp(i-1, j-1);\n}\n```\n\n\nI could imagine that trickling down for tens of thousands of recursions. All you actually need logic for are the ~200 critical points. You can ignore the rest. A simple modification would be\n\n```\nif( str1[i-1] == str1[j-1] ) {\n    if( str1[i-1] == 1 )\n        return dp(i-1, j-1); // Already hit a critical point\n\n    // rightmost location of a 1 in str1 or str2, that is <= i-1\n    best = binarySearch(CriticalPoints, i-1);\n    return dp(best + 1, best + 1); // Use that critical point\n    // Important! best+1 because we still want to compute the answer at best\n    // Without it, we would skip over in a case where str1[best] is 1, and str2[best] is 0.\n}\n```\n\n\nCriticalPoints would be the array containing the index of every 1 in either str1 or str2. Make sure that it's sorted before you binary search. Keep in mind those gochya's. My logic was: Okay I need to make sure to calculate the answer at the index ```\nbest```\n itself, so let's go with ```\nbest + 1```\n as the parameter. But, if ```\nbest == i - 1```\n, we get stuck in a loop. I'll handle that with a quick ```\nstr1[i-1] == 1```\n check. Done, phew.\n\nYou can do a quick check for correctness by noting that at worst case you will hit all 200*100000 combinations of i and j that make critical points, and when those critical points call ```\nmin(a, b, c)```\n, it only makes three recursive function calls. If any of those functions are critical points, then it's part of those 200*100000 we already counted and we can ignore it. If it's not, then in O(log(200)) it falls into a single call on another critical point (Now, it's something we know is part of the 200*100000 we already counted). Thus, each critical point takes at worst ```\n3*log(200)```\n time excluding calls to other critical points. Similarly, the very first function call will fall into a critical point in ```\nlog(200)```\n time. Thus, we have an upper bound of O(200*100000*3*log(200) + log(200)).\n\nAlso, make sure your memo table is a hashmap, not an array. 10^10 memory will not fit on your computer.\n\n\n\nSolution 2\n\nYou know the answer is at most 200, so just prevent yourself from computing more than that many operations deep.\n\n```\ndp(int i, int j) { // O(100000 * 205), sounds good to me.\n    if( abs(i - j) > 205 )\n        return 205; // The answer in this case is at least 205, so it's irrelevant to calculating the answer because when min is called, it wont be smallest.\n    // memo & base case\n    if( str1[i-1] == str1[j-1] ) {\n        return dp(i-1, j-1);\n    }\n    return 1 + min( dp(i-1, j), dp(i-1, j-1), dp(i, j-1) );\n}\n```\n\n\nThis one is very simple, but I leave it for solution two because this solution seems to have come out from thin air, as opposed to analyzing the problem and figuring out where the slow part is and how to optimize it. Keep this in your toolbox though, since you should be coding this solution.\n\n\n\nSolution 3\n\nJust like Solution 2, we could have done it like this:\n\n```\ndp(int i, int j, int threshold = 205) {\n    if( threshold == 0 )\n        return 205;\n    // memo & base case\n    if( str1[i-1] == str1[j-1] ) {\n        return dp(i-1, j-1);\n    }\n    return 1 + min( dp(i-1, j, threshold - 1), dp(i-1, j-1, threshold - 1), dp(i, j-1, threshold - 1) );\n}\n```\n\n\nYou might be worried about dp(i-1, j-1) trickling down, but the threshold keeps i and j close together so it calculates a subset of Solution 2. This is because the threshold gets decremented every time i and j get farther apart. ```\ndp(i-1, j-1, threshold)```\n would make it identical to Solution 2 (Thus, this one is slightly faster).\n\n\n\nSpace\n\nThese solutions will give you the answer very quickly, but if you want a space-optimizing solution as well, it would be easy to replace ```\nstr1[i]```\n with ```\n(i in Str1CriticalPoints) ? 1 : 0```\n, using a hashmap. This would give a final solution that is still very fast (Though will be 10x slower), and also avoids keeping the long strings in memory (To the point where it could run on an Arduino). I don't think this is necessary though.\n\nNote that the original solution does not use 10^10 space. You mention \"even better, less than 10^10 space\", with an implication that 10^10 space would be acceptable. Unfortunately, even with enough RAM, iterating though that space takes 10^10 time, which is definitely not acceptable. None of my solutions use 10^10 space: only 2 * 10^5 to hold the strings - which can be avoided as discussed above. 10^10 Bytes it 10 GB for reference.\n\n\n\nEDIT: As maniek notes, you only need to check ```\nabs(i - j) > 105```\n, as the remaining 100 insertions needed to equate ```\ni```\n and ```\nj```\n will pull the number of operations above 200.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "string edit distance algorithm confusion\r\n                \r\nHere are the edit distance definition. My question is whether edit distance from word 1 to word 2, is always the same as edit distance from word 2 to word 1, and why? Thanks.\n\nGiven two words word1 and word2, edit distance is the minimum number of steps required to convert word1 to word2.\n(each operation is counted as 1 step.)\n\nThere are 3 operations permitted on a word:\n\na) Insert a character\nb) Delete a character\nc) Replace a character\n\nregards,\nLin\n    ", "Answer": "\r\nIt is always the same: the process to come from w1 to w2 can be run backwards in the same number of steps.\n\nFor each step a) there is a corresponding step b) and vice versa. Each step c) can be undone by another step c).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Maintaining headers in edit distance\r\n                \r\nI am running edit distance using ```\nstringdist```\n. The output replaces the input with a numbered list instead of the actual string being compared. This is currently what I have:\n\n```\nlibrary(stringdist)\n\na <- c(\"foo\", \"bar\", \"bear\", \"boat\", method = \"lv\")\nstringdistmatrix(a)\n\n1 2 3\n2 3    \n3 4 1  \n4 3 2 2\n```\n\n\nI would like the output to look like the following so that I can see where the edit distance comes from.\n\n```\nfoo bar bear\nbar 3    \nbear 4 1  \nboat 3 2 2\n```\n\n    ", "Answer": "\r\nThere is the ```\nuseNames```\n parameter you can specify:\n\n```\nstringdistmatrix(a, useNames = TRUE)\n\n#     foo bar bear\n#bar    3         \n#bear   4   1     \n#boat   3   2    2\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Efficiently calculate edit distance between two strings\r\n                \r\nI have a string S of length 1000 and a query string Q of length 100. I want to calculate the edit distance of query string Q with every sub-string of string S of length 100. One naive way to do is calculate dynamically edit distance of every sub-string independently  i.e. ```\nedDist(q,s[0:100])```\n, ```\nedDist(q,s[1:101])```\n, ```\nedDist(q,s[2:102])```\n.......  ```\nedDist(q,s[900:1000])```\n .     \n\n```\ndef edDist(x, y):\n\"\"\" Calculate edit distance between sequences x and y using\n    matrix dynamic programming.  Return distance. \"\"\"\nD = zeros((len(x)+1, len(y)+1), dtype=int)\nD[0, 1:] = range(1, len(y)+1)\nD[1:, 0] = range(1, len(x)+1)\nfor i in range(1, len(x)+1):\n    for j in range(1, len(y)+1):\n        delt = 1 if x[i-1] != y[j-1] else 0\n        D[i, j] = min(D[i-1, j-1]+delt, D[i-1, j]+1, D[i, j-1]+1)\nreturn D[len(x), len(y)]\n```\n\n\nCan somebody suggest an alternate approach to calculate edit distance efficiently. My take on this is that we know the ```\nedDist(q,s[900:1000])```\n. Can we somehow use this knowledge to calculate ```\nedDist[(q,s[899:999])]```\n...since there we have a difference of 1 character only and then proceed backward to ```\nedDist[(q,s[1:100])]```\n using the previously calculated edit Distance ?  \n    ", "Answer": "\r\nImproving Space Complexity\n\nOne way to make your Levenshtein distance algorithm more efficient is to reduce the amount of memory required for your calculation.\n\nTo use an entire matrix, that requires you to utilize ```\nO(n * m)```\n memory, where ```\nn```\n represents the length of the first string and ```\nm```\n the second string.\n\nIf you think about it, the only parts of the matrix we really care about are the last two columns that we're checking - the previous column and the current column.\n\nKnowing this, we can pretend we have a matrix, but only really ever create these two columns; writing over the data when we need to update them.\n\nAll we need here is two arrays of size ```\nn + 1```\n:\n\n```\nvar column_crawler_0 = new Array(n + 1);\nvar column_crawler_1 = new Array(n + 1);\n```\n\n\nInitialize the values of these pseudo columns:\n\n```\nfor (let i = 0; i < n + 1; ++i) {\n  column_crawler_0[i] = i;\n  column_crawler_1[i] = 0;\n}\n```\n\n\nAnd then go through your normal algorithm, but just make sure that you're updating these arrays with the new values as we go along:\n\n```\nfor (let j = 1; j < m + 1; ++j) {\n  column_crawler_1[0] = j;\n  for (let i = 1; i < n + 1; ++i) {\n    // Perform normal Levenshtein calculation method, updating current column\n    let cost = a[i-1] === b[j-1] ? 0 : 1;\n    column_crawler_1[i] = MIN(column_crawler_1[i - 1] + 1, column_crawler_0[i] + 1, column_crawler_0[i - 1] + cost);\n  }\n\n  // Copy current column into previous before we move on\n  column_crawler_1.map((e, i) => {\n    column_crawler_0[i] = e;\n  });\n}\n\nreturn column_crawler_1.pop()\n```\n\n\nIf you want to analyze this approach further, I wrote a small open sourced library using this specific technique, so feel free to check it out if you're curious.\n\nImproving Time Complexity\n\nThere's no non-trivial way to improve a Levenshtein distance algorithm to perform faster than ```\nO(n^2)```\n. There are a few complicated approached, one using VP-Tree data structures. There are a few good sources if you're curious to read about them here and here, and these approaches can reach up to an asymptotical speed of ```\nO(nlgn)```\n.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Weighted unordered string edit distance\r\n                \r\nI need an efficient way of calculating the minimum edit distance between two unordered collections of symbols. Like in the Levenshtein distance, which only works for sequences, I require insertions, deletions, and substitutions with different per-symbol costs. I'm also interested in recovering the edit script. \n\nSince what I'm trying to accomplish is very similar to calculating string edit distance, I figured it might be called unordered string edit distance or maybe just set edit distance. However, Google doesn't turn up anything with those search terms, so I'm interested to learn if the problem is known by another name?\n\nTo clarify, the problem would be solved by \n\n```\ndef unordered_edit_distance(target, source):\n    return min(edit_distance(target, source_perm) \n               for source_perm in permuations(source))\n```\n\n\nSo for instance, the ```\nunordered_edit_distance('abc', 'cba')```\n would be ```\n0```\n, whereas ```\nedit_distance('abc', 'cba')```\n is ```\n2```\n.  Unfortunately, the number of permutations grows large very quickly and is not practical even for moderately sized inputs.\n\nEDIT Make it clearer that operations are associated with different costs.\n    ", "Answer": "\r\nSort them (not necessary), then remove items which are same (and in equal numbers!) in both sets.\nThen if the sets are equal in size, you need that numer of substitutions; if one is greater, then you also need some insertions or deletions. Anyway you need the number of operations equal the size of the greater set remaining after the first phase.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Word-level edit distance of a sentence\r\n                \r\nIs there an algorithm that lets you find the word-level edit distance between 2 sentences?\nFor eg., \"A Big Fat Dog\" and \"The Big House with the Fat Dog\" have 1 substitute, 3 insertions\n    ", "Answer": "\r\nIn general, this is called the sequence alignment problem. Actually it does not matter what entities you align - bits, characters, words, or DNA bases - as long as the algorithm works for one type of items it will work for everything else. What matters is whether you want global or local alignment.\n\nGlobal alignment, which attempt to align every residue in every sequence, is most useful when the sequences are similar and of roughly equal size. A general global alignment technique is the Needleman-Wunsch algorithm algorithm, which is based on dynamic programming. When people talk about Levinstain distance they usually mean global alignment. The algorithm is so straightforward, that several people discovered it independently, and sometimes you may come across Wagner-Fischer algorithm which is essentially the same thing, but is mentioned more often in the context of edit distance between two strings of characters.\n\nLocal alignment is more useful for dissimilar sequences that are suspected to contain regions of similarity or similar sequence motifs within their larger sequence context. The  Smith-Waterman algorithm is a general local alignment method also based on dynamic programming. It is quite rarely used in natural language processing, and more often - in bioinformatics.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Python edit distance\r\n                \r\nI am a molecular biologist using Biopython to analyze mutations in genes and my problem is this:\n\nI have a file containing many different sequences (millions), most of which are duplicates. I need to find the duplicates and discard them, keeping one copy of each unique sequence. I was planning on using the module editdist to calculate the edit distance between them all to determine which ones the duplicates are, but editdist can only work with 2 strings, not files.\n\nAnyone know how I can use that module with files instead of strings?\n    ", "Answer": "\r\nAssuming your file consists solely of sequences arranged one sequence per line, I would suggest the following:\n\n```\nseq_file = open(#your file)\n\nsequences = [seq for seq in seq_file]\n\nuniques = list(set(sequences))\n```\n\n\nAssuming you have the memory for it. How many millions?\n\nETA:\n\nWas reading the comments above (but don't have comment privs) - assuming the sequence IDs are the same for any duplicates, this will work. If duplicate sequences can different sequence IDs, then would to know which comes first and what is between them in the file.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance Matrix\r\n                \r\nI'm trying to build a program that takes two strings and fills in the edit distance matrix for them. The thing that is tripping me up is, for the second string input, it is skipping over the second input. I've tried clearing the buffer with getch(), but it didn't work. I've also tried switching over to scanf(), but that resulted in some crashes as well. Help please!\n\nCode:\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n\nint min(int a, int b, int c){\n    if(a > b && a > c)\n        return a;\n    else if(b > a && b > c)\n        return b;\n    else\n        return c;\n}\n\nint main(){\n\n    // allocate size for strings\n    int i, j;\n    char *input1 = (char*)malloc(sizeof(char)*100);\n    char *input2 = (char*)malloc(sizeof(char)*100);\n\n    // ask for input\n    printf(\"Enter the first string: \");\n    fgets(input1, sizeof(input1), stdin);\n    printf(\"\\nEnter the second string: \");\n    fgets(input2, sizeof(input2), stdin);\n\n    // make matrix\n    int len1 = sizeof(input1), len2 = sizeof(input2);\n    int c[len1 + 1][len2 + 1];\n\n    // set up input 2 length\n    for(i = 0; i < len2 + 1; i++){\n        c[0][i] = i;\n    }\n\n    // set up input 1 length\n    for(i = 0; i < len1 + 1; i++){\n        c[i][0] = i;\n    }\n\n    // fill in the rest of the matrix\n    for(i = 1; i < len1; i++){\n        for(j = 1; j < len2; j++){\n            if(input1[i] == input2[j]) // if the first letters are equal make the diagonal equal to the last\n                c[i][j] = c[i - 1][j - 1];\n            else\n                c[i][j] = 1 + min(c[i - 1][j - 1], c[i - 1][j], c[i][j - 1]);\n        }\n    }\n\n    // print the matrix\n    printf(\"\\n\");\n    for(j = 0; j < len2; j++){\n        for(i = 0; i < len1; i++){\n            printf(\"|  %d\", c[i][j]);\n        }\n        printf(\"\\n\");\n    }\n\n    return 1;\n}\n```\n\n    ", "Answer": "\r\nStick with ```\nfgets```\n.\n\nAs others have pointed out, use ```\nchar input1[100]```\n instead of ```\nchar *input1 = malloc(...)```\n\n\nBut, even with that change, which makes the ```\nsizeof```\n inside of the ```\nfgets```\n correct, using ```\nsizeof```\n when setting up ```\nlen1```\n and ```\nlen2```\n is wrong. You'll be processing an entire buffer of 100, even if their are only 10 valid characters in it (i.e. the remaining ones are undefined/random).\n\nWhat you [probably] want is ```\nstrlen```\n [and a newline strip] to get the actual useful lengths.\n\nHere's the modified code [please pardon the gratuitous style cleanup]:\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\nint\nmin(int a, int b, int c)\n{\n    if (a > b && a > c)\n        return a;\n    if (b > a && b > c)\n        return b;\n    return c;\n}\n\nint\nmain(void)\n{\n\n    // allocate size for strings\n    int i;\n    int j;\n    char input1[100];\n    char input2[100];\n\n    // ask for input\n    printf(\"Enter the first string: \");\n    fgets(input1, sizeof(input1), stdin);\n    int len1 = strlen(input1);\n    if (input1[len1 - 1] == '\\n') {\n        input1[len1 - 1] = 0;\n        --len1;\n    }\n\n    printf(\"\\nEnter the second string: \");\n    fgets(input2, sizeof(input2), stdin);\n    int len2 = strlen(input2);\n    if (input2[len2 - 1] == '\\n') {\n        input2[len2 - 1] = 0;\n        --len2;\n    }\n\n    // make matrix\n    int c[len1 + 1][len2 + 1];\n\n    // set up input 2 length\n    for (i = 0; i < len2 + 1; i++) {\n        c[0][i] = i;\n    }\n\n    // set up input 1 length\n    for (i = 0; i < len1 + 1; i++) {\n        c[i][0] = i;\n    }\n\n    // fill in the rest of the matrix\n    for (i = 1; i < len1; i++) {\n        for (j = 1; j < len2; j++) {\n            // if the 1st letters are equal make the diagonal equal to the last\n            if (input1[i] == input2[j])\n                c[i][j] = c[i - 1][j - 1];\n            else\n                c[i][j] = 1 + min(c[i - 1][j - 1], c[i - 1][j], c[i][j - 1]);\n        }\n    }\n\n    // print the matrix\n    printf(\"\\n\");\n    for (j = 0; j < len2; j++) {\n        for (i = 0; i < len1; i++) {\n            printf(\"|  %d\", c[i][j]);\n        }\n        printf(\"\\n\");\n    }\n\n    return 1;\n}\n```\n\n\n\n\nUPDATE:\n\n\n  Okay sweet I see what you mean! The reason I was trying to use malloc though was to avoid making the matrix that I had to print a size of 100x100 blank spaces.\n\n\nWith either the fixed size ```\ninput1```\n or the ```\nmalloc```\ned one, ```\nfgets```\n will only fill it to the input size entered [clipped to the second argument, if necessary]. But, it does not pad/fill the remainder of the buffer with anything (e.g. spaces on the right). What it does do is add an EOS [end-of-string] character [which is a binary 0x00] after the last char read from input [which is usually the newline].\n\nThus, if the input string is: ```\nabcdef\\n```\n, the length [obtainable from ```\nstrlen```\n] is 7, input[7] will be 0x00, and ```\ninput1[8]```\n through ```\ninput1[99]```\n will have undefined/random/unpredictable values and not spaces.\n\nSince a newline char isn't terribly useful, it is often stripped out before further processing. For example, it isn't terribly relevant when computing edit distance for a small phrase.\n\n\n  Does using strlen() only count the number of chars inside the array, or does it include all the blank spaces too?\n\n\nAs I mentioned above, ```\nfgets```\n does not pad the string at the end, so, not to worry. It will do what you want/expect.\n\n```\nstrlen```\n only counts chars up to [but not including the EOS terminator character (i.e.) zero]. If some of these chars happen to be spaces, they will be counted by ```\nstrlen```\n--which is what we want.\n\nConsider computing the edit distance between any two of the following phrases:\n\n```\nquick brown fox jumped over the lazy dogs\nthe quick brown fox jumped over lazy dogs\nquick brown fox jumps over the lazy dog\n```\n\n\nIn each case, we want ```\nstrlen```\n to include the [internal/embedded] spaces in the length calculation. That's because it is perfectly valid to compute the edit distance of phrases.\n\n\n\nThere is a valid usage for ```\nmalloc```\n: when the amount of data is too big to fit on the stack. Most systems have a default limit (e.g. under linux, it's 8 MB).\n\nSuppose we were computing the edit distance for two book chapters [read from files], we'd have (e.g.):\n\n```\nchar input1[50000];\nchar input2[50000];\n```\n\n\nThe above would fit, but the ```\nc```\n matrix would cause a stack overflow:\n\n```\nint c[50000][50000];\n```\n\n\nbecause the size of this would be ```\n50000 * 50000 * 4```\n which is approx 9.3 GB.\n\nSo, to fit all this data, we'd need to allocate it on the heap. While it is possible to do a ```\nmalloc```\n for ```\nc```\n and maintain the 2D matrix access, we'd have to create a function and pass off the pointer to ```\nc```\n to it.\n\nSo, here's a modified version that takes input of arbitrarily large sizes:\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <errno.h>\n#include <string.h>\n\n#define sysfault(_fmt...) \\\n    do { \\\n        fprintf(stderr,_fmt); \\\n        exit(1); \\\n    } while (0)\n\n#define C(y,x)      c[((y) * (len2 + 1)) + (x)]\n\nlong\nmin(long a, long b, long c)\n{\n    if (a > b && a > c)\n        return a;\n    if (b > a && b > c)\n        return b;\n    return c;\n}\n\nchar *\ninput(const char *prompt,long *lenp,const char *file)\n{\n    FILE *fp;\n    char *lhs;\n    int chr;\n    long siz;\n    long len;\n\n    if (file != NULL)\n        fp = fopen(file,\"r\");\n    else {\n        fp = stdin;\n        printf(\"Enter %s string: \",prompt);\n        fflush(stdout);\n    }\n\n    lhs = NULL;\n    siz = 0;\n    len = 0;\n\n    while (1) {\n        chr = fgetc(fp);\n        if (chr == EOF)\n            break;\n\n        if ((chr == '\\n') && (file == NULL))\n            break;\n\n        // grow the character array\n        if ((len + 1) >= siz) {\n            siz += 100;\n            lhs = realloc(lhs,siz);\n            if (lhs == NULL)\n                sysfault(\"input: realloc failure -- %s\\n\",strerror(errno));\n        }\n\n        lhs[len] = chr;\n        len += 1;\n    }\n\n    if (file != NULL)\n        fclose(fp);\n\n    if (lhs == NULL)\n        sysfault(\"input: premature EOF\\n\");\n\n    // add the EOS\n    lhs[len] = 0;\n\n    // return the length to the caller\n    *lenp = len;\n\n    return lhs;\n}\n\nint\nmain(int argc,char **argv)\n{\n    long i;\n    long j;\n    char *input1;\n    long len1;\n    char *input2;\n    long len2;\n    long *c;\n\n    --argc;\n    ++argv;\n\n    switch (argc) {\n    case 2:\n        input1 = input(\"first\",&len1,argv[0]);\n        input2 = input(\"second\",&len2,argv[1]);\n        break;\n    default:\n        input1 = input(\"first\",&len1,NULL);\n        input2 = input(\"second\",&len2,NULL);\n        break;\n    }\n\n    // make matrix\n    c = malloc(sizeof(*c) * (len1 + 1) * (len2 + 1));\n    if (c == NULL)\n        sysfault(\"main: malloc failure -- %s\\n\",strerror(errno));\n\n    // set up input 2 length\n    for (i = 0; i < len2 + 1; i++) {\n        C(0,i) = i;\n    }\n\n    // set up input 1 length\n    for (i = 0; i < len1 + 1; i++) {\n        C(i,0) = i;\n    }\n\n    // fill in the rest of the matrix\n    for (i = 1; i < len1; i++) {\n        for (j = 1; j < len2; j++) {\n            // if the 1st letters are equal make the diagonal equal to the last\n            if (input1[i] == input2[j])\n                C(i,j) = C(i - 1,j - 1);\n            else\n                C(i,j) = 1 + min(C(i - 1,j - 1), C(i - 1,j), C(i,j - 1));\n        }\n    }\n\n    // print the matrix\n    printf(\"\\n\");\n    for (j = 0; j < len2; j++) {\n        for (i = 0; i < len1; i++) {\n            printf(\"|  %ld\", C(i,j));\n        }\n        printf(\"\\n\");\n    }\n\n    return 1;\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Algorithm to find edit distance to all substrings\r\n                \r\nGiven 2 strings ```\ns```\n and ```\nt```\n. I need to find for each substring in ```\ns```\n edit distance(Levenshtein distance) to ```\nt```\n. Actually I need to know for each ```\ni```\n position in ```\ns```\n what is the minimum edit distance for all substrings started at position ```\ni```\n.\n\nFor example:\n\n```\nt = \"ab\"    \ns = \"sdabcb\"\n```\n\n\nAnd I need to get something like:\n\n```\n{2,1,0,2,2}```\n\n\nExplanation:\n\n```\n1st position:\ndistance(\"ab\", \"sd\") = 4 ( 2*subst )\ndistance(\"ab\", \"sda\") = 3( 2*delete + insert )\ndistance(\"ab\", \"sdab\") = 2 ( 2 * delete)\ndistance(\"ab\", \"sdabc\") = 3 ( 3 * delete)\ndistance(\"ab\", \"sdabcb\") = 4 ( 4 * delete)\nSo, minimum is 2\n\n2nd position:\ndistance(\"ab\", \"da\") = 2 (delete + insert)\ndistance(\"ab\", \"dab\") = 1 (delete)\ndistance(\"ab\", \"dabc\") = 2 (2*delete)\n....\nSo, minimum is 1\n\n3th position:\ndistance(\"ab\", \"ab\") = 0\n...\nminimum is 0\n```\n\n\nand so on.\n\nI can use brute force algorithm to solve this task, of course. But is there faster algorithm? \n\nThanks for help.\n    ", "Answer": "\r\nTo find substrings in a given string is very easy.\nYou take the normal Levenshtein algorithm and modify it slightly.\n\nFIRST:\nInstead of filling the first row of the matrix with 0,1,2,3,4,5,...\nyou fill it entirely with zeros. (green rectangle)\n\nSECOND:\nThen you run the algorithm.\n\nTHIRD:\nInstead of returning the last cell of the last row you search for the smallest value in the last row and return it. (red rectangle)\n\nExample:\nneedle: \"aba\", haystack: \"c abba c\" --> result = 1 (converting abba -> aba)\n\n\n\nI tested it and it works.\n\nThis is much faster than your suggestion of stepping character by character through the string as you do in your question. You only create the matrix once.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance - With memoization\r\n                \r\nI'm trying to do the edit distance problem, but cache the results so I don't repeat calls.  It worked before I tried to store subproblems in a map, but now it stops working.  For the call I make, comparing \"thou shalt not\" and \"you should not\", it returns 1.  Obviously incorrect, but why? \n\n```\nusing namespace std;\nint counter = 0;\n\nint match(char c1, char c2){\n  c1 == c2 ? 0 : 1;\n}\n\nint edit_distance(string s1, string s2,map<pair<string,string>, int>& memo){\n  if(memo[make_pair(s1,s2)])\n    return memo[make_pair(s1,s2)];\n  int i = s1.size();\n  int j = s2.size();\n\n  if(s1.empty())\n    return memo[make_pair(s1,s2)] = 1 + j;\n  if(s2.empty())\n    return memo[make_pair(s1,s2)] = 1 + i;\n\n  int opt[3];\n\n  opt[0] = edit_distance(s1.substr(1), s2.substr(1),memo) + match(s1[i-1],s2[j-1]);\n  opt[1] = edit_distance(s1.substr(1), s2,memo) + 1;\n  opt[2] = edit_distance(s1, s2.substr(1),memo) + 1;\n\n  int min = opt[0];\n  for(int i = 1; i < 3; i++){\n    if(opt[i] < min)\n      min = opt[i];\n  }\n  memo[ make_pair(s1,s2) ] = min;\n  return min;\n}\n\nint edit_distance_driver(string s1, string s2){\n  map<pair<string,string>,int> memo;\n  return edit_distance(s1, s2, memo);\n}\n\nint main(){\n  cout << edit_distance_driver(\"thou shalt not\",\"you should not\") << endl;\n}\n```\n\n    ", "Answer": "\r\nThe problem is here:\n\n```\nopt[0] = edit_distance(s1.substr(1), s2.substr(1),memo) + match(s1[i-1],s2[j-1]);\n```\n\n\nYou recurse without the first characters, but you check the last characters.\n\nYou should instead check the first characters, so it should be:\n\n```\nopt[0] = edit_distance(s1.substr(1), s2.substr(1),memo) + match(s1[0],s2[0]);\n```\n\n\nAnd obviously ```\nmatch```\n should return something:\n\n```\nint match(char c1, char c2){\n  return c1 == c2 ? 0 : 1;\n}\n```\n\n\nThen your code prints 6 for those strings.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance between two regular expression\r\n                \r\nI faced this question in an interview:\n\nGiven two regular expression, compute the edit distance between them. The edit distance being defined as the smallest edit distance between any two strings generated by the two regular expressions respectively.\n\nFormally, we are looking for ```\nd(L1,L2) = min { d(x,y) | x from L1, y from L2 }```\n, where ```\nL1```\n and ```\nL2```\n are the languages generated by the two regular expressions.\n\nI was not able to solve it during interviews. Even now I don't have any clue how to solve it. Any ideas?\n\nI think this is same as http://www.spoj.com/problems/AMR10B/\n    ", "Answer": "\r\nThere's finite state machines that represent the two languages. Let's say the first language has states S[1], S[2], ..., S[N1] and transitions c: S[i]->S[j] (meaning state i goes to state j under input character c), and T[1], T[2], ... T[N2] for the second language (with its own set of transitions).\n\nNow, you can construct the weighted multi-graph with nodes being pairs of states, and edges between pairs (S[i1], T[i2]) -> (S[j1], T[j2]) if any of these four cases hold:\n\n\nThere's c: S[i1] -> S[j1] and i2 = j2. This has weight 1\nThere's c: T[i2] -> T[j2] and i1 = j1. This has weight 1\nThere's c: S[i1] -> S[j1] and c: T[i2] -> T[j2]. This has weight 0\nThere's c: S[i1] -> S[j1] and d: T[i2] -> T[j2]. This has weight 1\n\n\nThen, finding the lowest weight path from the pair of start states to any pair of accepting states gives you the minimal edit distance.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance program\r\n                \r\nI'm trying to write a program that calculates the edit distance between any two words in the English language.  The edit distance is defined as the minimum number of steps it takes to go from one word to another by changing just one letter at a time, where each intermediate word is contained in some standard list of English words.  I will prompt the user for two words.  So for example, if the user enters: \"trail\" and \"crawl\" I want the output to be:\n\n```\nFirst word:  trail\nLast word: crawl\nThe edit distance is 2 with the following chain: trail -> trawl -> crawl\n```\n\n\n(so it also includes the path as well)\n\n```\nimport java.util.Scanner;\nimport java.io.*;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.util.HashMap;\nimport java.util.Set;\nimport java.util.HashSet;\n\npublic class EditDistance {\n public static void main (String[] args) {\n    try {\n        inputDictionary();\n    }\n    catch (FileNotFoundException e) {\n    System.out.println(\"Error: FileNotFoundException\");\n    }\n    promptUser();\n}\n\nstatic ArrayList<String> dictionary;\n\n    //input dictionary\npublic static void inputDictionary() throws FileNotFoundException {\n    dictionary = new ArrayList<String>();\n    Scanner scan = new Scanner(new File (\"dict.txt\"));\n    while (scan.hasNext()) {\n        String word = scan.next();\n        dictionary.add(word);\n    }\n    scan.close();\n}\n\npublic static HashSet<String> changeOneLetter(String Word) {\n    HashSet<String> newWords = new HashSet<String>();\n    char alphabet[] = \"abcdefghijklmnopqrstuvwxyz\".toCharArray();\n        //take each letter and change it to a different letter\n        for (int i = 0; i < Word.length(); i++) { //cycles through each letter of the word\n            char wordArray[] = Word.toCharArray();\n            for (int j = 0; j < 26; j++) { //swaps in each letter of the alphabet\n                wordArray[i] = alphabet[j];\n                //check if it's not the same as previous word\n                String newWord = new String(wordArray);\n                if (!newWord.equals(Word)) {\n                    // if it's in the dictionary add the word to the set\n                    for (String word1: dictionary) {\n                        if (newWord.equals(word1)) {\n                            newWords.add(newWord);\n                        }\n                    }\n                }\n            }\n        }\n    return newWords;\n}\n\nstatic Map<String, Set<String>> eachWordChange;\nstatic String firstWord;\nstatic String secondWord;\n\n    //prompts the user for two words\npublic static void promptUser() {\n    System.out.println(\"Enter the first word: \");\n    Scanner scan = new Scanner(System.in);\n    firstWord = scan.next();\n    for (String word1: dictionary) {\n        if (firstWord.equals(word1)) {\n            System.out.println(\"Enter the second word: \");\n            secondWord = scan.next();\n            for (String word2: dictionary) {\n                if (secondWord.equals(word2)) {\n                    calculateEdits();\n                }\n            }\n        }\n    }\n    scan.close();\n}\n\n   //this is where I'm having trouble\npublic static void calculateEdits() {\n    String oldWord = secondWord;\n    Map <String, String> findPath = new HashMap<String, String>();\n    HashSet<String> oneLevel = changeOneLetter(oldWord);\n    System.out.println(\"This is the first level: \" + oneLevel);\n    for (String word: oneLevel) {\n        if (!firstWord.equals(word)) {\n\n            findPath.put(word, secondWord);\n            System.out.println(\"Next Levels: \" + changeOneLetter(word));\n    }\n    System.out.println(\"Path of first level: \" + findPath);\n}\n```\n\n\n}\n\nRight now I just have println's to show me what's going on, so right now it is showing me:\n\n```\nThis is the first level: [trawl, brawl, drawl]\nNext Levels: [brawl, trail, crawl, drawl]\nNext Levels: [trawl, crawl, drawl]\nNext Levels: [trawl, brawl, crawl, drawn]\nPath of first level: {trawl=crawl, brawl=crawl, drawl=crawl}\n```\n\n\nRight now I have the first level, and can generate the second level from each of the first levels, but I want to be able to store those next levels, and then check if the first word (=trail) occurs there, and if it doesn't I'll have to create a map and redo the process until the first word appears.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance recursive algorithm -- Skiena\r\n                \r\nI'm reading The Algorithm Design Manual by Steven Skiena, and I'm on the dynamic programming chapter.  He has some example code for edit distance and uses some functions which are explained neither in the book nor on the internet.  So I'm wondering \n\na) how does this algorithm work?\n\nb) what do the functions indel and match do?\n\n```\n#define MATCH     0       /* enumerated type symbol for match */\n#define INSERT    1       /* enumerated type symbol for insert */\n#define DELETE    2       /* enumerated type symbol for delete */\n\nint string_compare(char *s, char *t, int i, int j)\n{\n        int k;                  /* counter */\n        int opt[3];             /* cost of the three options */\n        int lowest_cost;        /* lowest cost */\n\n        if (i == 0) return(j * indel(' '));\n        if (j == 0) return(i * indel(' '));\n\n        opt[MATCH] = string_compare(s,t,i-1,j-1) + match(s[i],t[j]);\n        opt[INSERT] = string_compare(s,t,i,j-1) + indel(t[j]);\n        opt[DELETE] = string_compare(s,t,i-1,j) + indel(s[i]);\n\n        lowest_cost = opt[MATCH];\n        for (k=INSERT; k<=DELETE; k++)\n                if (opt[k] < lowest_cost) lowest_cost = opt[k];\n\n        return( lowest_cost );\n}\n```\n\n    ", "Answer": "\r\nOn page 287 in the book:\n\n```\nint match(char c, char d)\n{\n  if (c == d) return(0); \n  else return(1); \n}\n\nint indel(char c)\n{\n  return(1);\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Faster edit distance algorithm [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nProblem: I know the trivial edit distance DP formulation and computation in O(mn) for 2 strings of size n and m respectively. But I recently came to know that if we only need to calculate the minimum value of edit distance f and it is bounded |f|<=s, then we can calculate it in O(min(m,n) + s^2) or O(s*min(m,n)) [wikipedia] time.\n\nPlease explain the dp formulation behind it if this is DP based or explain the algorithm .   \n\nLook at the ```\nimproved algorithm```\n section of the\nlink: http://en.wikipedia.org/wiki/Edit_distance .\n\none more link about improved UKKONEN'S algorithm http://www.berghel.net/publications/asm/asm.php\n\nThanks in advance.\n    ", "Answer": "\r\nYou can calculate edit distance in O(min(n, m) * s) time use next simple idea:\n\nConsider the i-th string in DP-table. \n\nSo, if we know, that answer <= s, then we are intersted in cells with coordinates (i, i - s), (i, i - s + 1), ... ,(i, i + s). Because in other cells answer strictly greater than s. \n\nFor example, suppose we know, that edit distance between \"abacaba\" and \"baadba\" less than 3.\n\n\n\nSo, we can skip red cells, because they have value more than s.\n\nAsymptotic of the algorithm O(min(n, m) * s) because we calculate s cells to the left and right of the main diagonal.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How do I calculate tree edit distance? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 3 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI need to calculate the edit distance between trees.  This paper describes an algorithm, but I can't make heads or tails out of it.  Could you describe an applicable algorithm in a more approachable way? Pseudocode or code would both be helpful.\n    ", "Answer": "\r\nThis Python library implements the Zhang-Shasha algorithm correctly: Zhang-Shasha: Tree edit distance in Python\nIt began as a direct port of the Java source listed in the currently accepted answer (the one with the tarball link), but that implementation is not correct and is nearly impossible to run at all.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Calculate edit distance percentage\r\n                \r\nI am attempting to get a percentage of an edit distance from a group of sequences. So far this is what I have:\n\n```\nlibrary(stringdist)\n\nsequence <- c(\"CA--------W----------------------EKDRRTEAF---F------\",\n   \"CA--------W----------------------EKDRRTEAF---F------\", \n   \"CA--------S-------------------SLVFGQGDNIQY---F------\", \n   \"RA--------S-------------------SLIYSP----LH---F------\")\n\nedit_dist <- stringdistmatrix(sequence)\n#0 \n#13 13 \n#14 14 11\n\nlen <- stri_length(gsub('-', '', sequence))\n#13    13    16    12\n```\n\n\nAs each line of ```\nlen```\n is equivalent to each line of ```\nsequence```\n, when comparing the two lines I would like to use the largest ```\nlen```\n to get the percentage. So as when having an edit distance between the second and third sequence it would use the length of 16 rather than 13 to get a percentage.\n\nI know this code is wrong, but it is generally the idea I am going for:\n\n```\nfor (i in len) {\n  num1 <- len[i]\n  for (j in len){\n    num2 <- len[j] \n    if (num2 > num1){\n        num <- num2\n        }else{\n          num <- num1\n        }\n    }\n    edit_dist/num\n}\n```\n\n\nThe answer should look something similar to that below:\n\n\n0\n.8125  .8125\n1.0769  1.0769  .6875\n\n    ", "Answer": "\r\nYou can construct a suitable matrix of the max length with ```\nouter```\n and ```\npmax```\n, which you can then coerce to ```\ndist```\n class (like ```\nedit_dist```\n) so you can divide:\n\n```\nedit_dist <- stringdistmatrix(sequence)\nn <- nchar(gsub('-', '', sequence))\n\nedit_dist / as.dist(outer(n, n, pmax))\n##          1        2        3\n## 2 0.000000                  \n## 3 0.812500 0.812500         \n## 4 1.076923 1.076923 0.687500\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein distance in python giving only 1 as edit distance\r\n                \r\nI have a python program to read two lists (one with errors and other with correct data). Every element in my list with error needs to be compared with every element in my correct list. After comparing i get all the edit distance between every compared pair. now i can find the least edit distance for the given error data and thru get my correct data.\n\nI am trying to use levenshtein distance to calculate the edit distance but its returning all edit distance as 1 even which is wrong.\n\nThis means the code for calculating levenshtein distance is not correct. I am struggling to find a fix for this. HELP!\n\nMy Code\n\n```\nimport csv\n\ndef lev(a, b):\n    if not a: return len(b)\n    if not b: return len(a)\n    return min(lev(a[1:], b[1:])+(a[0] != b[0]), lev(a[1:], b)+1, lev(a, b[1:])+1)\n\nif __name__ == \"__main__\":\n\n    with open(\"all_correct_promo.csv\",\"rb\") as file1:\n        reader1 = csv.reader(file1)\n        correctPromoList = list(reader1)\n        #print correctPromoList\n\n    with open(\"all_extracted_promo.csv\",\"rb\") as file2:\n        reader2 = csv.reader(file2)\n        extractedPromoList = list(reader2)\n        #print extractedPromoList\n\n    incorrectPromo = []\n    count = 0\n    for extracted in extractedPromoList:\n        if(extracted not in correctPromoList):\n            incorrectPromo.append(extracted)\n        else:\n            count = count + 1\n    #print incorrectPromo\n\n    for promos in incorrectPromo:\n        for correctPromo in correctPromoList:\n            distance = lev(promos,correctPromo)\n            print promos, correctPromo , distance\n```\n\n    ", "Answer": "\r\nThere is a Python package available that implements the levenshtein distance : python-levenshtein\n\nTo install it:\n\n```\npip install python-levenshtein\n```\n\n\nTo use it:\n\n```\n>>> import Levenshtein\n>>> string1 = 'dsfjksdjs'\n>>> string2 = 'dsfiksjsd'\n>>> print Levenshtein.distance(string1, string2)\n3\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance - dynamic approach\r\n                \r\nI have to code edit distance with dynamic programming approach, and I don't understand why, with my code, for the strings \"short\" and \"ports\", my output is 2 when I should have 3 :\n```\n#include <iostream>\n#include <string>\n#include <algorithm>\n#include <vector>\n\nusing std::string;\nusing std::vector;\n\nint edit_distance(const string &str1, const string &str2) {\n    if(str1 == str2) return 0;\n    int n = str1.length() +1;\n    int m = str2.length() +1;\n    vector<vector<int>> D(n); // Matrix that will contain all the calculation\n    for(int i = 0; i<n; ++i){ // Resizing of the matrix\n        D[i].resize(m);\n    }\n    for(int i = 0; i < n; ++i) //Initialisation\n        D[i][0] = i;\n    for(int j = 0; j < m; ++j) //Initialisation\n        D[0][j] = j;\n    for(int i = 1; i < n; ++i){\n        for(int j = 1; j < m; ++j){\n            if(str1[i] != str2[j]) // if str1[i] != str2[j], then take the minimum of the case of deletion, insertion, mismatch\n                D[i][j] = std::min({D[i-1][j]+1, D[i][j-1]+1, D[i-1][j-1]+1});\n            else if(str1[i] == str2[j]) // if str1[i] == str2[j], then take the minimum of the case of deletion, insertion, match\n                D[i][j] = std::min({D[i-1][j]+1, D[i][j-1]+1, D[i-1][j-1]});\n            }\n        }\n    return D[n-1][m-1]; //retourner le résultat voulu\n}\n```\n\n    ", "Answer": "\r\nHad the same issue.\nThe strings indices didn't match the distance matrix indices.\nEven though you added one to ```\nstr1```\n & ```\nstr2```\n, the first char in each of the string will start from 0 and not from 1.\nYou can prepend a matching char to both strings to fix that (make sure you remove ```\nconst```\n in the function's parameters)\n```\n#include <iostream>\n#include <string>\n#include <algorithm>\n#include <vector>\n\nusing std::string;\nusing std::vector;\n\nint edit_distance(string& str1, string& str2) { // remove const\n    if (str1 == str2) return 0;\n\n    // adding a match prefix to start from 0\n    str1.insert(0, \" \"); \n    str2.insert(0, \" \");\n\n    // the strings length is correspondly incremented\n    int n = str1.length();\n    int m = str2.length();\n\n    vector<vector<int>> D(n); // Matrix that will contain all the calculation\n    for (int i = 0; i < n; ++i) { // Resizing of the matrix\n        D[i].resize(m);\n    }\n    for (int i = 0; i < n; ++i) //Initialisation\n        D[i][0] = i;\n    for (int j = 0; j < m; ++j) //Initialisation\n        D[0][j] = j;\n    for (int i = 1; i < n; ++i) {\n        for (int j = 1; j < m; ++j) {\n            if (str1[i] != str2[j]) // if str1[i] != str2[j], then take the minimum of the case of deletion, insertion, mismatch\n                D[i][j] = std::min({ D[i - 1][j] + 1, D[i][j - 1] + 1, D[i - 1][j - 1] + 1 });\n            else if (str1[i] == str2[j]) // if str1[i] == str2[j], then take the minimum of the case of deletion, insertion, match\n                D[i][j] = std::min({ D[i - 1][j] + 1, D[i][j - 1] + 1, D[i - 1][j - 1] });\n        }\n    }\n    return D[n - 1][m - 1]; //retourner le résultat voulu\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein edit distance Python\r\n                \r\nThis piece of code returns the Levenshtein edit distance of 2 terms.\nHow can i make this so that insertion and deletion only costs 0.5 instead of 1 ? substitution should still costs 1.\n\n```\ndef substCost(x,y):\n   if x == y: \n      return 0\n   else: \n      return 1\n\ndef  levenshtein(target, source):\n   i = len(target); j = len(source)\n   if i == 0:  \n      return j\n   elif j == 0: \n      return i\n\n   return(min(levenshtein(target[:i-1],source)+1,\n          levenshtein(target, source[:j-1])+1,\n          levenshtein(target[:i-1], source[:j-1])+substCost(source[j-1],target[i-1])))\n```\n\n    ", "Answer": "\r\nThere are two places you need to account for the reduced cost of adding or removing a vowel. They are the ```\nreturn j```\n and ```\nreturn i```\n lines in the base cases of your function, and the ```\n+1```\ns in the ```\nmin```\n call after the first two recursive calls.\n\nWe need to change each of those to use a \"ternary\" expression: ```\n0.5 if ch in 'aeiou' else 1```\n in place of assuming a cost of ```\n1```\n per character added or removed.\n\nFor the base cases, we can replace the return values with ```\nsum```\n calls on a generator expression that includes the ternary expression:\n\n```\nif i == 0:  \n    return sum(0.5 if ch in 'aeiou' else 1 for ch in source)\nelif j == 0: \n    return sum(0.5 if ch in 'aeiou' else 1 for ch in target)\n```\n\n\nFor later cases, we can replace the ```\n+1```\n with the ternary expression itself (with an index rather than the ```\nch```\n iteration variable):\n\n```\nreturn min(levenshtein(target[:i-1],source) + (0.5 if target[-1] in 'aeiou' else 1),\n           levenshtein(target, source[:j-1]) + (0.5 if source[-1] in 'aeiou' else 1),\n           levenshtein(target[:i-1], source[:j-1])+substCost(source[j-1],target[i-1]))\n```\n\n\nIf you wanted to generalize this, you could move the ternary expression out into its own function, named something like ```\naddCost```\n, and call it from the code in the ```\nlevenshtein```\n function.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Confused about the edit distance\r\n                \r\nUsing the edit distance I have to find how many edits between two strings, which I have already done in my code below, but the part im stuck on is printing the 2d array the output is suppose to look like this:\n\n\n```\nint editdistance(char *s, int ls, char *t, int lt)\n{\n    int a, b, c;\n    if (!ls) return lt;\n\n    if (!lt) return ls;\n\n    if (s[ls] == t[ls])\n            return editdistance(s, ls - 1, t, lt - 1);\n    a = editdistance(s, ls - 1, t, lt - 1);\n    b = editdistance(s, ls,     t, lt - 1);\n    c = editdistance(s, ls - 1, t, lt    );\n\n    if (a > b) a = b;\n    if (a > c) a = c;\n\n    return a + 1;\n}\n\nint main()\n{\nchar s1[100];\nchar s2[100];\nprintf(\"first: \\n\");\nscanf(\"%s\",s1);\nprintf(\"second: \\n\");\nscanf(\"%s\",s2);\n\nprintf(\"edit distance: %d\\n\", editdistance(s1, strlen(s1), s2, strlen(s2)));\n\n    return 0;\n}\n```\n\n    ", "Answer": "\r\nIf i am right, you are looking for Levenshtein distance. this is an algorithm in Dynamic programming.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "neo4j edit distance search\r\n                \r\nI am running neo4j 3.0.4 and want do a search on the node property using edit distance of 1. I searched the documentation and couldn't find anything, the closest I found was regex search. Any help would be appreciated.\n    ", "Answer": "\r\nYou can use a manual Lucene index, e.g. via the APOC procedure library.\n\nInstallation of the library, see: https://github.com/neo4j-contrib/neo4j-apoc-procedures\n\nDocumentation: https://neo4j-contrib.github.io/neo4j-apoc-procedures/#_full_text_search\n\n```\nCALL apoc.index.search(\"locations\", \"Address.address:Paris~\") YIELD node AS addr\nMATCH (addr)<-[:HAS_ADDRESS]-(company:Company)\nRETURN company LIMIT 50\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Pyomo Examples about Graph Edit Distance\r\n                \r\nI want to modify the weight for the node substitute. Therefore, the default function in networkx cannot work. Graph edit distance is an optimization problem, are there any tutorials about using Pyomo to solve the graph edit distance?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein edit distance and different sets of edits\r\n                \r\nI was just going over some questions but I got stuck at a Levenshtein edit distance question.\n\nSo the first part of the question was:\n\n\n  What is the Levenshtein edit distance between the strings ```\nSTRONGEST```\n and ```\nTRAINERS```\n?\n\n\nWhich I calculated as ```\n6```\n. But the next question I could not get is\n\n\n  Let ```\nd```\n be the edit distance found in part (so ```\n6```\n). How many different sets of ```\nd```\n ‘edits’ (insertions, deletions, or substitutions) are there that will change the string ```\nSTRONGEST```\n into the string ```\nTRAINERS```\n?\n\n\nCould anyone explain how I could find how many different sets exists here and how you arrived with the solution? \n    ", "Answer": "\r\nIf you have used memoization table approach for the first problem, just go to the right-bottom corner of table (where you get minimum edit distance) and trace back all possible paths of minimum edits. All these paths will give you different sets of edits.  For reference on how to trace back, you could see this solution to problem of printing LCS of two strings.\n\nYou could also refer to my comment on the above mentioned page.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Calculating graph edit distance\r\n                \r\nWhat is the difference between g.graph_edit_distance() and g.optimize_graph_edit_distance (python), in terms of what is being calculated? Also, are there other ways to get graph edit distance if the size of graphs makes g.graph_edit_distance() too complex to calculate?\n    ", "Answer": "\r\nThis is in the documentation for these functions. Write ```\nhelp(func_name)```\n in Python to get the documentation. ```\noptimize_graph_edit_distance```\n\n\nReturns consecutive approximations of GED (graph edit distance) between graphs G1 and G2.\n\nWhereas ```\ngraph_edit_distance```\n\n\nReturns GED (graph edit distance) between graphs G1 and G2.\n\nSo the difference is that one is approximating.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance Data Structure\r\n                \r\nI am new to data structure want to know the flow of this diagram as mentioned ,it's for calculating minimum edit distance between two string ,in the graph i understood that String 1 is of three length and String 2 is also of three length , so tutorial shown graph from eD(3,3) then why the graph split again in eD(3,2),eD(2,3),eD(2,2) for the 2 level of recursion . What it signifies ? Please need detail explanation . Why we can't split level 2 ,like this eD(3,2),eD(2,3). \n\nI am following this Url : https://www.geeksforgeeks.org/dynamic-programming-set-5-edit-distance/\n\nenter image description here\n    ", "Answer": "\r\nOk, so basically in case of Edit Distance, we are trying to either insert, update or delete an element. So, our basic approach is to try all three of these available operations at each point and check which case gives the best result.\n\nSpecific to the case that you are trying to understand, the following is the scenario:\n\n```\neD(3,3) = eD(2,2) if str1[3] == str2[3] # Without incuring any cost, we find edit distance of the remaining strings.\n          else l + min(del, ins, rep)\n```\n\n\nwhere\n\n```\ndel = eD(2,3)```\n, Deleted the last character of str1, and finding the edit distance of the remaining strings.\n\n```\nins = eD(3,2)```\n, Inserted the last element of str2 in str1, thus now we are finding the edit distance between the remaining strings. E.g. ```\n'adc'```\n and ```\n'axf'```\n, if we add ```\n'f'```\n to the first string, it will become ```\n'adcf'```\n. Thus now, the last characters of both strings is same, I have to eventually find the edit distance between ```\n'adc'```\n and ```\n'ax'```\n, Thus it becomes ```\neD(3,2)```\n.\n\n```\nrep = eD(2,2)```\n, Replaced the last element of str1 with the last element of str2, and finding the edit distance of the remaining strings. E.g. ```\n'abc'```\n and ```\nadf```\n, if we replace the last character of first string with the last character of second string, we will get ```\n'abf'```\n and ```\n'adf'```\n, as now the last characters of both the strings are same, eventually we are finding the edit distance between ```\n'ab'```\n and ```\n'ad'```\n. Thus, ```\neD(2,2)```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Pyomo example for graph edit distance\r\n                \r\nAre there any Pyomo examples for graph edit distance between graph1 and graph2?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance Similarity in Lucene/Solr\r\n                \r\nAnyone know if there is any Edit Distance similarity implementation, like Levenshtein in Lucene/Solr? Thanks\n    ", "Answer": "\r\nYes, fuzzy queries and fuzzy term enumeration use Levenshtein edit distance.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Token-based edit distance in Python?\r\n                \r\nI'm familiar with python's ```\nnltk.metrics.distance```\n module, which is commonly used to compute edit distance of two string.\n\nI am interested in a function which computes such distance but not char-wise as normally but token-wise. By that I mean that you can replace/add/delete whole tokens only (instead of chars).\n\nExample of regular edit distance and my desired tokenized version:\n\n```\n> char_dist(\"aa bbbb cc\",\n            \"aa b cc\")\n3                              # add 'b' character three-times\n\n> token_dist(\"aa bbbb cc\",\n             \"aa b cc\")\n1                              # replace 'bbbb' token with 'b' token\n```\n\n\nIs there already some function, that can compute ```\ntoken_dist```\n in python? I'd rather use something already implemented and tested than writing my own piece of code. Thanks for tips.\n    ", "Answer": "\r\nNLTK's ```\nedit_distance```\n appears to work just as well with lists as with strings:\n\n```\nnltk.edit_distance(\"aa bbbb cc\", \"aa b cc\")\n> 3\nnltk.edit_distance(\"aa bbbb cc\".split(), \"aa b cc\".split())\n> 1\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Approximate Edit distance tree - Exact Edit path\r\n                \r\nI've been looking for an algorithm to efficiently compute an edit path between two trees, a path that does not have to correspond to shortest edit distance but preferably a relatively short one.\n\nThe case is that I have two directory trees, consisting of directories and files, and want to compute a sequence of deletes, inserts and renames that will transform one to the other.\n\nI have tried searching both stackoverflow and the wild web but all I find is algorithms for computing shortest edit distance, but they all have high scaling factors.\n\nSo my question is, is there any more efficient way then for example \"Zhang and Shasha\" when I don't need the optimum distance?\n\nKind regards\n    ", "Answer": "\r\nThere is the Klein algorithm that performs slightly better than \"Zhang and Sasha\", however it remains of very high complexity in both space and time for practical purpose.\n\nThere is an algorithm here that is in fact an heuristic, since the authors misused the term approximation.\n It reduces the problem to a series of maximum weighted cliques for which it exists several approximation and heuristics, even a greedy approach could here perform reasonably well.\n\nWhat is true for graphs is true for trees, you could therefore use a graph kernel convolution approach.\n\nIf you are looking for an off the shelf implementation (of an unspeficied algorithm, I woudl guess Zhang or Klein), you can check here\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "MALLET: How to implement crf based edit distance?\r\n                \r\nI'm attempting to track down an edit distance algorithm that is supposedly implemented in MALLET.\nI want to use the CRF edit distance algorithm as described here (by Andrew McCallum et al). The authors confirm its Mallet inclusion here in the FST class:\n\nThe model has been implemented as part of the\nfinite-state transducer classes in Mallet.\n\nHowever, neither the FST tutorial page - which shows an NER tagger - or the test cases are of any help, at least that I can see.\nQuestions:\n\nWhere can I find the implementation of the edit distance CRF algorithm in Mallet?\nHow can I use it?\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance string and list\r\n                \r\nI am trying to write a python edit distance, the code I wrote is only comparing two words, but how about if I want to compare a word and a list of words?\n\nHere's my code:\n\n```\ndef fdistance(S1,S2):\n    a = len(S1)\n    b = len(S2)\n    fdn = {} # Global dict\n    for x in range(a+1):\n        fdn[x,0] = x\n    for y in range(b+1):\n        fdn[0,y] = y\n\n    for x in range(1,a+1):\n        for y in range(1,b+1):\n            if S1[x-1] == S2[y-1]: \n                c = 0 \n            else:\n                c = 1                 \n            fdn[x,y] = min(fdn[x,y-1]+1, fdn[x-1,y]+1, fdn[x-1,y-1]+c)\n    return fdn[x,y]\n```\n\n\nBut it only can print the distance between string and string. My question is if S2 is a list, so how to compare a string and a list?\n    ", "Answer": "\r\nYou have a function to compare two words.\n\nTo compare a word against a list of words:\n\n```\n>>> words = ['halo', 'hallo', 'help']\n>>> [fdistance('hello', word) for word in words]\n[2, 1, 2]\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to group sentences by edit distance?\r\n                \r\nI have a large set (36k sentence) of sentences (text list) and their POS tags (POS list), and I'd like to group/cluster the elements in the POS list using edit distance/Levenshtein:\n\n(e.g Sentx POS tags= [CC DT VBZ RB JJ], Senty POS tags= [CC DT VBZ RB JJ] ) are in cluster edit distance =0, \n\nwhile ([CC DT VBZ RB JJ], [CC DT VB RB JJ]) are in cluster edit distance =1. \n\nI understand how the clustering algorithms work but I am confused how to approach such a problem in python and how to store the clusters in data structures so that I can retrieve them easily.\n\nI tried to create a matrix (measuring the distance of each sentence with all the sentences in the corpus) but it takes very long to be processed.\n    ", "Answer": "\r\nThere is only a limited set of POS tags.\n\nRather than using edit distance, compute a POS-POS similarity matrix just once. You may even want to edit this matrixes desired, e.g. to make two POS tags effectively the same, or to increase the difference of two tags.\n\nStore that in a numpy array, convert all your vectors to indexes, and then compute similarities using that lookup table. For performance reasons, use numpy where possible, and write the performance critical code in cython because the Python interpreter is very slow.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Percentage edit distance from array\r\n                \r\nI am attempting to get a percentage of an edit distance from a group of sequences. So far this is what I have:\n\n```\n#!/usr/bin/perl -w\nuse strict;\nuse Text::Levenshtein qw(distance);\n\nmy @sequence = qw(CA--------W----------------------EKDRRTEAF---F------ \nCA--------W----------------------EKDRRTEAF---F------ \nCA--------S-------------------SLVFGQGDNIQY---F------  \nRA--------S-------------------SLIYSP----LH---F------);\n\n\nforeach my $list (@sequence){\n    my @distance = distance($list, @sequence);\n    my @length = $list =~ tr/[A-Z]///;\n}\n```\n\n\nI am able to get the edit distance with ```\n@distance```\n and the length of each sequence, based on the letters with ```\n@length```\n. If printed the results are as follows: \n\n```\n@distance```\n\n\n```\n0 0 13 14\n0 0 13 14\n13 13 0 11 \n14 14 11 0\n```\n\n\n```\n@length```\n\n\n```\n13\n13\n16\n12\n```\n\n\nAs each line of ```\n@length```\n is equivalent to each line of ```\n@sequence```\n, when comparing the two lines I would like to use the largest ```\n@length```\n to get the percentage. So as when having an edit distance between the second and third sequence it would use the length of 16 rather than 13 to get a percentage. What I think needs to happen is to call only two elements of the ```\n@length```\n array and pick the larger one to then put into a percentage, possibly using an ```\nif```\n statement.\n\nI know this code is wrong, but it is generally the idea I am going for:\n\n```\nforeach my  $list (@sequence){\n        my @distance = distance($list, @sequence);      \n        my @length = $list =~ tr/[A-Z]//;                # / syntax hilite fix\n\n        foreach my $item(@distance){\n                foreach @length {\n                        my $num1 = if $length[0] >= $length[1];\n                                 print \"$item/$num1\\n\";\n                        else my $num2 = $length[1] >= $length[0];\n                                print \"$item/$num2\\n\";\n                }\n        }\n}\n```\n\n\nThe answer should look something similar to that below:\n\n\n0  0 .8125  1.0769\n0  0  .8125  1.0769\n.8125  .8125  0  .6875\n1.0769  1.0769  .6875  0\n\n    ", "Answer": "\r\nTry this. To summarize: We compute the edit distances between pairs of \nstrings. For each pair we want to determine the fraction of the distance and the maximum number of characters (```\nA-Z```\n). The maximum number of characters is taken to be the maximum for the two items in the pair.\n\n```\nuse strict;\nuse warnings;\n\nuse Text::Levenshtein qw(distance);\n\nmy @sequence = qw(\n        CA--------W----------------------EKDRRTEAF---F------\n        CA--------W----------------------EKDRRTEAF---F------\n        CA--------S-------------------SLVFGQGDNIQY---F------\n        RA--------S-------------------SLIYSP----LH---F------\n);\n\nmy @length = map { tr/[A-Z]// } @sequence;\n\nfor my $i (0..$#sequence) {\n    my $list = $sequence[$i];\n    my @distance = distance($list, @sequence);\n    my $num1 = $length[$i];\n    for my $j (0..$#distance) {\n        my $item = $distance[$j];\n        my $num2 = $length[$j];\n        my $num = ( $num2 > $num1 ) ? $num2 : $num1;\n        printf \"%.4f \", $item/$num;\n    }\n    print \"\\n\";\n}\n```\n\n\nOutput:\n\n```\n0.0000 0.0000 0.8125 1.0769 \n0.0000 0.0000 0.8125 1.0769 \n0.8125 0.8125 0.0000 0.6875 \n1.0769 1.0769 0.6875 0.0000 \n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Similarity matrix for weighted edit distance\r\n                \r\nI wanted to implement a modification of the basic edit distance algorithm. That is, the weighted edit distance. (Context: Spelling errors while trying to create a search engine)\n\nFor example, the cost of substituting s by a would be lesser than substituting s by, say, p.\n\nThe algorithm for this using DP would require a simple change, i.e., \n\n```\nd[i, j] := minimum(d[i-1, j] + 1,                         // deletion\n                         d[i, j-1] + 1,                   // insertion\n                         d[i-1, j-1] + substitutionCost)  // substitution\n```\n\n\nI looked, but I could not find such a matrix anywhere, that would give me the appropriate substitutionCost for all pairs of letters. I mean, I want the costs to be based on the distance between letters on the keyboard. Has nobody explicitly defined such a matrix yet?  \n    ", "Answer": "\r\nI have written a c++ code that should work, i have also made the assumption that the keys are placed symmetrically: \n\n```\n#include<bits/stdc++.h>\n\nusing namespace std;\n\nstring s[3];\nint mat[35][35];\n\nint main() {\n    s[0] = \"qwertyuiop\";\n    s[1] = \"asdfghjkl;\";\n    s[2] = \"zxcvbnm,./\";\n\n    for(int i = 0;i < 10;i++){\n        for(int j = 0;j < 3;j++){\n            for(int k = 0;k < 10;k++){\n                for(int l = 0;l < 3;l++){\n                    if(j == 1 && i > 8) continue;if(l == 1 && k > 8) continue;\n                    if(j == 2 && i > 6) continue;if(l == 2 && k > 6) continue;\n                    int st1 = s[j][i] - 'a';\n                    int st2 = s[l][k] - 'a';\n                    mat[st1][st2] = abs(j-l) + abs(i-k);\n                }\n            }\n        }\n    }\n    for(int i = 0;i < 26;i++){\n        for(int j = 0;j < 26;j++){\n            cout << (char)(i+'a') << \" \" << (char)(j+'a') << \" \" << mat[i][j] << endl;\n        }\n    }\n\nreturn 0;\n}\n```\n\n\nLink to output on Ideone : http://ideone.com/xq7kKp\n\nHere ```\nmat[i][j]```\n contains the distance between keys.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Minimal edit distance between two circular lists?\r\n                \r\nGiven two circular lists, is there an efficient way to compute the optimal alignment between the two lists? For example, given circular lists:\n\n```\na b b c\nb c a\n```\n\n\nthe optimal alignment is \n\n```\na b b c\na b _ c\n```\n\n\nbecause this alignment has the smallest edit distance (note: this optimal alignment is not and need not be unique).\n\nOne way to do this is to compute the edit distance between the first list and each cyclic permutation of the second list, taking the minimum edit distance as the optimal alignment. Is there a more efficient way to do so?\n    ", "Answer": "\r\nsuppose S1=\"abbc\", S2=\"bca\"\n\nnow let S2'=strcat(S2,S2)=\"bcabca\", then compute edit distance between S1 and S2', you will get\n\n```\n--abbc-\nbcab-ca\n```\n\n\njust a hint, more details should be considered\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Minimum Edit Distance Reconstruction\r\n                \r\nI know there are similar answer to this on stack, as well as online, but I feel I'm missing something. Given the code below, we need to reconstruct the sequence of events that led to the resulting minimum edit distance. For the code below, we need to write a function that outputs:\n\n```\nEqual, L, L\nDelete, E\nEqual, A, A\nSubstitute, D, S\nInsert, T\n```\n\n\nEDIT: CODE IS UPDATED WITH MY (PARTIALLY CORRECT) SOLUTION\n\nHere is the code, with my partial solution. It works for example I was given (\"lead\" -> \"last\"), but doesn't work for the example below (\"hint\" -> \"isnt\"). I suspect this is because the first character is equal, which is throwing off my code. Any tips or pointers in the right direction would be great!\n\n```\ndef printMatrix(M):\n        for row in M:\n                print row\n        print\n\ndef med(s, t):  \n        k = len(s) + 1\n        l = len(t) + 1\n\n        M = [[0 for i in range(k)] for j in range(l)]\n        MTrace = [[\"\" for i in range(k)] for j in range(l)]\n\n        M[0][0] = 0\n\n\n        for i in xrange(0, k):\n                M[i][0] = i\n                MTrace[i][0] = s[i-1]\n\n        for j in xrange(0, l):\n                M[0][j] = j\n                MTrace[0][j] = t[j-1]\n\n        MTrace[0][0] = \"DONE\"\n\n        for i in xrange(1, k):\n                for j in xrange(1, l):\n\n                        sub = 1\n                        sub_op = \"sub\"\n                        if s[i-1] == t[j-1]:\n                                # equality\n                                sub = 0\n                                sub_op = \"eq\"\n\n\n                        # deletion\n                        min_value = M[i-1][j] + 1\n                        op = \"del\"\n                        if min_value > M[i][j-1] + 1:\n                                # insertion\n                                min_value = M[i][j-1] + 1\n                                op = \"ins\"\n                        if min_value > M[i-1][j-1] + sub:\n                                # substitution\n                                min_value = M[i-1][j-1] + sub\n                                op = sub_op\n\n\n                        M[i][j] = min_value\n                        MTrace[i][j] = op                        \n\n        print \"final Matrix\"\n        printMatrix(M)\n        printMatrix(MTrace)\n\n############ MY PARTIAL SOLUTION\n\n        def array_append(array,x,y):\n            ops_string = MTrace[x][y]\n            if ops_string == 'ins':\n                array.append((\"Insert\",MTrace[0][y]))\n            elif ops_string == 'sub':\n                array.append((\"Substitute\",MTrace[x][0],MTrace[0][y]))\n            elif ops_string == 'eq':\n                array.append((\"Equal\",MTrace[x][0],MTrace[0][y]))\n            elif ops_string == 'del':\n                array.append((\"Delete\",MTrace[x][0]))\n\n\n        i = len(s)\n        j = len(t)\n\n        ops_array = []\n        base = M[i][j]\n        array_append(ops_array,i,j)\n\n\n        while MTrace[i][j] != \"DONE\":\n            base = M[i][j]\n            local_min = min(M[i][j-1],M[i-1][j],M[i-1][j-1])\n            if base == local_min:\n                i = i - 1\n                j = j - 1\n                array_append(ops_array,i,j)\n            elif M[i][j-1] < M[i-1][j]:\n                j = j -1\n                array_append(ops_array,i,j)\n            elif M[i-1][j] < M[i][j-1]:\n                i = i - 1\n                array_append(ops_array,i,j)\n            else:\n                i = i - 1\n                j = j - 1\n                array_append(ops_array,i,j)\n\n        print ops_array\n#########\n\n        return M[k-1][l-1]      \n\nprint med('lead', 'last')\n```\n\n    ", "Answer": "\r\nIt's my opinion that understanding the algorithm more deeply is important in this case. Rather than giving you some pseudocode, I'll walk you through the essential steps of the algorithm, and show you how the data you want is \"encoded\" in the final matrix that results.  Of course, if you don't need to roll your own algorithm, then you should obviously just use someone else's, as MattH suggests!\n\nThe Big Picture\n\nThis looks to me like an implementation of the Wagner-Fischer algorithm. The basic idea is to calculate the distances between \"nearby\" prefixes, take the minimum, and then calculate the distance for the current pair of strings from that. So for example, say you have two strings ```\n'i'```\n and ```\n'h'```\n. Let's lay them out along the vertical and horizontal axes of a matrix, like so:\n\n```\n  _ h\n_ 0 1\ni 1 1\n```\n\n\nHere, ```\n'_'```\n denotes an empty string, and each cell in the matrix corresponds to an edit sequence that takes an input (```\n''```\n or ```\n'i'```\n) to an output (```\n''```\n or ```\n'h'```\n). \n\nThe distance from the empty string to any string of length L is L, (requiring L insertions). The distance from any string of length L to the empty string is also L (requiring L deletions). That covers the values in the first row and column, which simply increment. \n\nFrom there, you can calculate the value of any location by taking the minimum from among the upper, left, and upper-left values, and adding one, or, if the letter is the same at that point in the string, taking the upper-left value unchanged. For the value at ```\n(1, 1)```\n in the table above, the minimum is ```\n0```\n at ```\n(0, 0)```\n, so the value at ```\n(1, 1)```\n is ```\n1```\n, and that's the minimum edit distance from ```\n'i'```\n to ```\n'h'```\n (one substitution). So in general, the minimum edit distance is always in the lower right corner of the matrix. \n\nNow let's do another, comparing ```\nis```\n to ```\nhi```\n. Here again, each cell in the matrix corresponds to an edit sequence that takes an input (```\n''```\n, ```\n'i'```\n, or ```\n'is'```\n) to an output (```\n''```\n, ```\n'h'```\n, or ```\n'hi'```\n). \n\n```\n  _ h i\n_ 0 1 2\ni 1 1 #\ns 2 # #\n```\n\n\nWe begin by enlarging the matrix, using ```\n#```\n as a placeholder for values we don't know yet, and extending the first row and column by incrementing. Having done so, we can begin calculating results for positions marked ```\n#```\n above. Let's start at ```\n(2, 1)```\n (in (row, column), i.e. row-major notation). Among the upper, upper-left, and left values, the minimum is ```\n1```\n. The corresponding letters in the table are different -- ```\ns```\n and ```\nh```\n -- so we add one to that minimum value to get ```\n2```\n, and carry on.\n\n```\n  _ h i\n_ 0 1 2\ni 1 1 #\ns 2 2 #\n```\n\n\nLet's move on to the value at ```\n(1, 2)```\n. Now things go a little differently because the corresponding letters in the table are the same -- they're both ```\ni```\n. This means we have the option of taking the value in the upper-left cell without adding one. The guiding intuition here is that we don't have to increase the count because the same letter is being added to both strings at this position. And since the lengths of both strings have increased by one, we move diagonally.\n\n```\n  _ h i\n_ 0 1 2\ni 1 1 1\ns 2 2 #\n```\n\n\nWith the last empty cell, things go back to normal. The corresponding letters are ```\ns```\n and ```\ni```\n, and so we again take the minimum value and add one, to get ```\n2```\n:\n\n```\n  _ h i\n_ 0 1 2\ni 1 1 1\ns 2 2 2\n```\n\n\nHere's the table we get if we continue this process for two longer words that start with ```\nis```\n and ```\nhi```\n -- ```\nisnt```\n (ignoring punctuation) and ```\nhint```\n:\n\n```\n  _ h i n t\n_ 0 1 2 3 4\ni 1 1 1 2 3\ns 2 2 2 2 3\nn 3 3 3 2 3\nt 4 4 4 3 2\n```\n\n\nThis matrix is slightly more complex, but the final minimum edit distance here is still just ```\n2```\n, because the last two letters of these two strings are the same. Convenient! \n\nRecreating the Sequence of Edits\n\nSo how can we extract the types of edits from this table? The key is to realize that movement on the table corresponds to particular types of edits. So for example, a rightward movement from ```\n(0, 0)```\n to ```\n(0, 1)```\n takes us from ```\n_ -> _```\n, requiring no edits, to ```\n_ -> h```\n, requiring one edit, an insertion. Likewise, a downward movement from ```\n(0, 0)```\n to ```\n(1, 0)```\n takes us from ```\n_ -> _```\n, requiring no edits, to ```\ni -> _```\n, requiring one edit, a deletion. And finally, a diagonal movement from ```\n(0, 0)```\n to ```\n(1, 1)```\n takes us from ```\n_ -> _```\n, requiring no edits, to ```\ni -> h```\n, requiring one edit, a substitution.\n\nSo now all we have to do is reverse our steps, tracing local minima from among the upper, left, and upper-left cells back to the origin, ```\n(0, 0)```\n, keeping in mind that if the current value is the same as the minimum, then we must go to the upper-left cell, since that's the only kind of movement that doesn't increment the edit distance. \n\nHere is a detailed description of the steps you could take to do so. Starting from the lower-right corner of the completed matrix, repeat the following until you reach the upper-left corner: \n\n\nLook at the neighboring cell to the upper left. If it doesn't exist, go to step 3. If the cell does exist, note the value stored there.\nIs the value in the upper-left cell equal to the value in the current cell? If so, then do the following:\n\n\nRecord an empty operation (i.e. ```\nEqual```\n). No edit was required in this case because the characters at this location are the same.\nUpdate the current cell, moving up and left.\nReturn to step 1.\n\nThere are many branches here:\n\n\nIf there is no cell to the left and no cell above, then you are in the upper-left corner, and the algorithm has completed. \nIf there is no cell to the left, go to step 4. (This will continue in a loop until you reach the upper-left corner.)\nIf there is no cell above, go to step 5. (This will continue in a loop until you reach the upper-left corner.)\nOtherwise, do a three-way comparison between the cell to the left, the cell to the upper left, and the cell above. Pick the one with the smallest value. If there are multiple candidates, you can pick one at random; they are all valid at this stage. (They correspond to different edit paths with the same total edit distance.)\n\n\nIf you picked the cell above, go to step 4.\nIf you picked the cell to the left, go to step 5.\nIf you picked the cell to the upper left, go to step 6.\n\n\nYou are moving up. Do the following:\n\n\nRecord a deletion of the input character at the current cell.\nUpdate the current cell, moving up.\nReturn to step 1.\n\nYou are moving left. Do the following:\n\n\nRecord an insertion of the output character at the current cell.\nUpdate the current cell, moving left.\nReturn to step 1.\n\nYou are moving diagonally. Do the following:\n\n\nRecord a substitution of the input character at the current cell in place of the output character at the current cell.\nUpdate the current cell, moving up and left.\nReturn to step 1. \n\n\n\nPutting it Together\n\nIn the example above, there are two possible paths: \n\n```\n(4, 4) -> (3, 3) -> (2, 2) -> (1, 2) -> (0, 1) -> (0, 0)\n```\n\n\nand\n\n```\n(4, 4) -> (3, 3) -> (2, 2) -> (1, 1) -> (0, 0)\n```\n\n\nReversing them, we get\n\n```\n(0, 0) -> (0, 1) -> (1, 2) -> (2, 2) -> (3, 3) -> (4, 4)\n```\n\n\nand\n\n```\n(0, 0) -> (1, 1) -> (2, 2) -> (3, 3) -> (4, 4)\n```\n\n\nSo for the first version, our first operation is a movement to the right, i.e. an insertion. The letter inserted is ```\nh```\n, since we're moving from ```\nisnt```\n to ```\nhint```\n. (This corresponds to ```\nInsert, h```\n in your verbose output.) Our next operation is a diagonal movement, i.e. either a substitution, or a no-op. In this case, it's a no-op because the edit distance is the same at both locations (i.e. the letter is the same). So ```\nEqual, i, i```\n. Then a downward movement, corresponding to a deletion. The letter deleted is ```\ns```\n, since again, we're moving from ```\nisnt```\n to ```\nhint```\n. (In general, the letter to insert comes from the output string, while the letter to delete comes from the input string.) So that's ```\nDelete, s```\n. Then two diagonal movements with no change in value: ```\nEqual, n, n```\n and ```\nEqual, t, t```\n. \n\nThe result:\n\n```\nInsert, h\nEqual, i, i\nDelete, s\nEqual, n, n\nEqual, t, t\n```\n\n\nPerforming these instructions on ```\nisnt```\n:\n\n```\nisnt   (No change)\nhisnt  (Insertion)\nhisnt  (No change)\nhint   (Deletion)\nhint   (No change)\nhint   (No change)\n```\n\n\nFor a total edit distance of 2. \n\nI'll leave the second minimum path as an exercise. Keep in mind that both paths are completely equivalent; they may be different, but they will result in the same minimum edit distance of 2, and so are entirely interchangeable. At any point as you work backwards through the matrix, if you see two different possible local minima, you may take either one, and the final result is guaranteed to be correct \n\nOnce you grok all this, it shouldn't be hard to code at all. The key, in cases like this, is to deeply understand the algorithm first. Once you've done that, coding it up is a cinch.\n\nAccumulation vs. Reconstruction\n\nAs a final note, you might chose to accumulate the edits as you populate the matrix. In that case, each cell in your matrix could be a tuple: ```\n(2, ('ins', 'eq', 'del', 'eq', 'eq'))```\n. You would increment the length, and append the operation corresponding to a movement from the minimal previous state. That does away with the backtracking, and so decreases the complexity of the code; but it takes up extra memory. If you do this, the final edit sequence will appear along with the final edit distance in the lower right corner of the matrix.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance: Ignore start/end [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 5 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI am looking for an algorithm that does edit distance, but which will ignore start+end in the one string and white space:\n\n```\nedit(\"four\",\"foor\") = 1\nedit(\"four\",\"noise fo or blur\") = 1\n```\n\n\nIs there an existing algorithm for that? Maybe even a Perl or a Python Library?\n    ", "Answer": "\r\nThe code to do this is simple in concept. It's your idea of what you'd like to ignore that you can add on your own:\n\n```\n#!perl\nuse v5.22;\nuse feature qw(signatures);\nno warnings qw(experimental::signatures);\n\nuse Text::Levenshtein qw(distance);\n\nsay edit( \"four\", \"foor\" );\nsay edit( \"four\", \"noise fo or blur\" );\n\nsub edit ( $start, $target ) {\n    # transform strings to ignore what you want\n    # ...\n    distance( $start, $target )\n    }\n```\n\n\nMaybe you want to check all substrings of the same length:\n\n```\nuse v5.22;\nuse feature qw(signatures);\nno warnings qw(experimental::signatures);\n\nuse Text::Levenshtein qw(distance);\n\nsay edit( \"four\", \"foar\" );\nsay edit( \"four\", \"noise fo or blur\" );\n\nsub edit ( $start, $target ) {\n    my $start_length = length $start;\n    $target =~ s/\\s+//g;\n    my @all_n_chars = map {\n        substr $target, $_, 4\n        } 0 .. ( length($target) - $start_length );\n\n    my $closest;\n    my $closest_distance = $start_length + 1;\n    foreach ( @all_n_chars ) {\n        my $distance = distance( $start, $_ );\n        if( $distance < $closest_distance ) {\n            $closest = $_;\n            $closest_distance = $distance;\n            say \"closest: $closest Distance: $distance\";\n            last if $distance == 0;\n            }\n        }\n\n    return $closest_distance;\n    }\n```\n\n\nThis very simpleminded implementation finds what you want. However, realize that other random strings might accidentally have an edit distance that is lower.\n\n```\nclosest: foar Distance: 1\n1\nclosest: nois Distance: 3\nclosest: foor Distance: 1\n1\n```\n\n\nYou could extend this to remember the true starting positions of each string so you can find it again in the original, but this should be enough to send you on your way. If you wanted to use Python, I think the program might look very similar.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Calculating the complexity of Levenshtein Edit Distance\r\n                \r\nI have been looking at this simple python implementation of Levenshtein Edit Distance for all day now.\n\n```\ndef lev(a, b):\n    \"\"\"Recursively calculate the Levenshtein edit distance between two strings, a and b.\n    Returns the edit distance.\n    \"\"\"\n    if(\"\" == a):\n        return len(b)   # returns if a is an empty string\n    if(\"\" == b):\n        return len(a)   # returns if b is an empty string\n    return min(lev(a[:-1], b[:-1])+(a[-1] != b[-1]), lev(a[:-1], b)+1, lev(a, b[:-1])+1)\n```\n\n\nFrom: http://www.clear.rice.edu/comp130/12spring/editdist/\n\nI know it has an exponential complexity, but how would I proceed to calculate that complexity from scratch?\n\nI have been searching all over the internet but haven't found any explainations only statements that it is exponential.\n\nThanks.\n    ", "Answer": "\r\n\nDraw the call tree (which you apparently have already done).\n\nAbstract from the call tree. For arbitrary n, determine the depth d of the tree as a function of n.\nAlso, determine how many branches/children there are per node, on average, as n approaches infinity; that's called the average branching factor b.\n\nRealize that visiting every node in a tree of depth d with average branching factor b takes at least on the order of b ^ d operations. Write that figure in terms of n and you have a lower bound on complexity in terms of the input size.\n\n\nMore specifically: you keep recursing until you hit an empty string, taking one character off each time. If we call the lengths of the strings m and n, then the depth of the tree is min(m, n). At every node in the call tree except the leaves, you recurse exactly three times, so in the limit the average branching factor is 3. That gives us a call tree of Θ(3^min(m, n)) nodes. The worst case occurs when m = n, so we can call that Θ(3^n).\nThis is still only a lower bound on the complexity. For the full picture, you should also take into account the amount of work done between recursive calls. In this naive code, that's actually linear time because ```\na[:-1]```\n has to copy (at Θ(n) cost) almost all of ```\na```\n, giving Θ(n 3^n) total complexity.*\n[* I once caught a CS professor using Python's slicing in a binary search, which as a result ran in time Θ(n lg n).]\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance variation\r\n                \r\nGoing through \"Introduction to algorithms\" by \"Cormen et al \", I came across a very popular question of dynamic programming named as \"Edit Distance\". While reading it and devising its solution I got troubled with a possible variation of the same.  \nSuppose the question is as:\nWe are given two strings and operations: copy, insert, delete, match, twiddle, kill.\nAlso as input we are given the number of transformations (say: n).\nFind if it is possible to transform first string to second in exactly \"n\" transformations .\nNote: The number of transformations given may be greater than the minimum number of transformations.\n\nThanks in advance.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance w/ operational weights in Python\r\n                \r\nI am learning about edit distance for the first time and have only been coding for a few months. I'm trying to modify the algorithm such that the different editing operations carry different weights as follows: insertion weighs 20, deletion weighs 20 and replacement weighs 5.\nI have been able to implement the basic code that calculates minimum edit distance if all operations were equal in weight (levenshtein distance). But how would one implement it if they are different as stated above? This is what I have at the moment:\n```\nstr1=\"algorithms\"\nstr2=\"alligator\"\nm=len(str1)\nn=len(str2)\n\ndef editdistance(str1, str2, m, n):\n  table=[[0 for x in range(n+1)] for x in range(m+1)]\n  \n  for i in range(m+1):\n    for j in range(n+1):\n\n      if i==0:\n        table[i][j]=j\n\n      elif j==0:\n        table[i][j]=i\n\n      elif str1[i-1]==str2[j-1]:\n        table[i][j]=table[i-1][j-1]\n\n      else:\n         table[i][j] = min(20+table[i][j-1], 20+table[i-1][j], 5+table[i-1][j-1])\n        \n\n  return table[m][n]\n\nprint(editdistance(str1, str2, m, n)) \n```\n\nOutput is 46, which is obviously wrong as the answer should be a multiple of 5. What am I missing here? Any help would be greatly appreciated.\n    ", "Answer": "\r\nYou have the base costs for when ```\ni = 0```\n and ```\nj = 0```\n to be ```\nj```\n and ```\ni```\n respectively, which are not multiples of 5. Then you should be multiplying them by ```\n20```\n since not using the letters is essentially the same as deleting or inserting them for the purposes of edit distance.\nSo you should try something like this:\n```\nstr1=\"algorithms\"\nstr2=\"alligator\"\nm=len(str1)\nn=len(str2)\n\ndef editdistance(str1, str2, m, n):\n  table=[[0 for x in range(n+1)] for x in range(m+1)]\n  \n  for i in range(m+1):\n    for j in range(n+1):\n\n      if i==0:\n        table[i][j]=j*20\n\n      elif j==0:\n        table[i][j]=i*20\n\n      elif str1[i-1]==str2[j-1]:\n        table[i][j]=table[i-1][j-1]\n\n      else:\n         table[i][j] = min(20+table[i][j-1], 20+table[i-1][j], 5+table[i-1][j-1])\n        \n\n  return table[m][n]\n\nprint(editdistance(str1, str2, m, n)) \n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance specifc implementation for number . PYTHON\r\n                \r\nI got a question on code chef asking the modified version of edit distance , where it is edit distance between 2 numbers , where only deletion operation is allowed and deletion cost = deleted number .\n\nSo I tried this to implement on python . But getting \n\n```\nTypeError: 'function' object has no attribute '__getitem__' \n```\n\n\nI am getting this error in the last line i.e return line in EditDisRec function\n\nHere is the code\n\n```\ndef ISTHERE(x,y):\n    a=len(str(y))\n    b=y\n    for i in range(0,a):\n        if b%10 == x:\n            return(True)\n            break\n        else:\n            b=b/10\n    return(False)            \n\n\ndef sum_digits(N):\n    n=int(N)\n    s = 0\n    while n:\n        s += n % 10\n        n //= 10\n    return s\ndef delt(x,y):\n    if int(x)==int(y):\n        return 0\n    else:\n        return int(x)+int(y)\n\ndef EditDistRec(S,T):\n    if S==0:\n        return sum_digits(T)\n    elif T==0:\n        return sum_digits(S)\n    elif (S==1 or S==2 or S==3 or S==4 or S==5 or S==6 or S==7 or S==8 or S==9 ):\n        if ISTHERE(S,T)==True :\n            return sum_digits(T) - S \n        elif ISTHERE(S,T)==False:\n            return sum_digits(T)\n    elif (T==1 or T==2 or T==3 or T==4 or T==5 or T==6 or T==7 or T==8 or T==9 ):\n        if ISTHERE(T,S)==True :\n            return sum_digits(S) - T \n        elif ISTHERE(T,S)==False:\n            return sum_digits(S) \n\n\n    return min(EditDistRec(S/10,T/10) + delt[S%10,T%10],(EditDistRec(S ,T/10) + int(T%10)),(EditDistRec(S/10,T ) + int(S%10)))\n\n\nprint(EditDistRec(7315,713))\n```\n\n    ", "Answer": "\r\nyou have typo here  ```\ndelt[S%10,T%10]```\n \n\nThis means that you cannot index them like a list because getitem is the method which handles this operation.```\ndelt```\n is a function, not a list \n\nCorrected: \n\n```\nreturn min(EditDistRec(S/10,T/10) + delt(S%10,T%10),(EditDistRec(S ,T/10) + int(T%10)),(EditDistRec(S/10,T ) + int(S%10)))\n```\n\n\nOutput:\n\n```\nsh-4.3$ python main.py                                                             \n7\nsh-4.3$\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "python numpy pairwise edit-distance\r\n                \r\nSo, I have a numpy array of strings, and I want to calculate the pairwise edit-distance between each pair of elements using this function: scipy.spatial.distance.pdist from http://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.spatial.distance.pdist.html\n\nA sample  of my array is as follows: \n\n```\n >>> d[0:10]\n array(['TTTTT', 'ATTTT', 'CTTTT', 'GTTTT', 'TATTT', 'AATTT', 'CATTT',\n   'GATTT', 'TCTTT', 'ACTTT'], \n  dtype='|S5')\n```\n\n\nHowever, since it doesn't have the 'editdistance' option, therefore, I want to give a customized distance function. I tried this and I faced the following error:\n\n```\n >>> import editdist\n >>> import scipy\n >>> import scipy.spatial\n >>> scipy.spatial.distance.pdist(d[0:10], lambda u,v: editdist.distance(u,v))\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/epd-7.3.2/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 1150, in pdist\n    [X] = _copy_arrays_if_base_present([_convert_to_double(X)])\n  File \"/usr/local/epd-7.3.2/lib/python2.7/site-packages/scipy/spatial/distance.py\", line 153, in _convert_to_double\n    X = np.double(X)\nValueError: could not convert string to float: TTTTT\n```\n\n    ", "Answer": "\r\nIf you really must use ```\npdist```\n, you first need to convert your strings to numeric format. If you know that all strings will be the same length, you can do this rather easily:\n\n```\nnumeric_d = d.view(np.uint8).reshape((len(d),-1))\n```\n\n\nThis simply views your array of strings as a long array of ```\nuint8```\n bytes, then reshapes it such that each original string is on a row by itself. In your example, this would look like:\n\n```\nIn [18]: d.view(np.uint8).reshape((len(d),-1))\nOut[18]:\narray([[84, 84, 84, 84, 84],\n       [65, 84, 84, 84, 84],\n       [67, 84, 84, 84, 84],\n       [71, 84, 84, 84, 84],\n       [84, 65, 84, 84, 84],\n       [65, 65, 84, 84, 84],\n       [67, 65, 84, 84, 84],\n       [71, 65, 84, 84, 84],\n       [84, 67, 84, 84, 84],\n       [65, 67, 84, 84, 84]], dtype=uint8)\n```\n\n\nThen, you can use ```\npdist```\n as you normally would. Just make sure that your ```\neditdist```\n function is expecting arrays of integers, rather than strings. You could quickly convert your new inputs by calling ```\n.tostring()```\n:\n\n```\ndef editdist(x, y):\n  s1 = x.tostring()\n  s2 = y.tostring()\n  ... rest of function as before ...\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Suggestive Spelling Checker based on edit distance and lcs?\r\n                \r\nHow can i implement a simple spell checker that accepts a misspelled word and an edit distance threshold then produce a list of suggested correct words .\nThat is wanted to be achieved using an algorithm that\n1-use both edit distance and longest common subsequence\n2-don't calculate the edit distance for each word in dictionary??????? \n    ", "Answer": "\r\nSo you have a dictionary of words and you want to use edit distance to calculate the closest match to a given word.\n\nSome suggestions to shortcut the process of checking all possible options:\n\n\nCache the closest matches to words when you do the calculations.  If someone enters \"speling\" and your top matches are \"spelling\", \"spewing\", and \"spilling\", save these matches with their calculated distances and threshold.  Next time you see \"speling\", you can just retrieve any results where the threshold <= the new threshold.\nWith levenshtein distance, when calculating, you can discard any words where the length difference is greater than the threshold.  You should be able to shortcut that process.  Of course if you want common subsequences, this falls over.\nModify the levenshtein distance calculator to break as soon as the threshold is reached. You'll still be starting to check a lot of words that don't match, but you'll do less work by breaking early.\n\n\nIn case you're still after a levenshtein distance algorithm, have a look at this example. It's pretty fast.\n\nhttp://dotnetperls.com/levenshtein\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein edit distance with separator for multi-character units\r\n                \r\nI have searched through R functions ```\nadist```\n, ```\nagrep```\n, ```\nmatch```\n, and ```\nstringdist```\n, but have not found a method to calculate edit distance with a separator.\n\nExisting edit distance: \n\n```\n“that” & ”fat” = 2  i.e., adist(\"that\",\"fat\")\n```\n\n\nDesired function would use a separator to denote multi-character units: \n\n```\n“th.a.t” & ”f.a.t” = 1\n```\n\n    ", "Answer": "\r\nLevenshtein distance is easy to implement, so just calculate it. Here's a quick no guarantee version of the Wagner-Fischer algorithm (see Wikipedia)\n\n```\nvecLeven <- function(s, t) {\n  d <- matrix(0, nrow = length(s) + 1, ncol=length(t) + 1)\n  d[, 1] <- (1:nrow(d)) - 1\n  d[1,] <- (1:ncol(d))-1\n  for (i in 1:length(s))  {\n    for (j in 1:length(t)) {\n      d[i+1, j+1] <- min(\n        d[i, j+1] + 1, # deletion\n        d[i+1, j] + 1, # insertion\n        d[i, j] + if (s[i] == t[j]) 0 else 1 # substitution\n      )\n    }\n  }\n\n    d[nrow(d), ncol(d)]\n}\n\nsepLeven <- function(s, t, sep=\".\") {\n  mapply(vecLeven, \n         strsplit(s, sep, fixed=TRUE), \n         strsplit(t, sep, fixed=TRUE))\n}\n\nsepLeven(c(\"th.a.t\", \"t.e.s.t\"), c(\"f.a.t\", \"f.e.t\"), sep=\".\")\n# output: [1] 1 2\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Quickly check large database for edit-distance similarity\r\n                \r\nI have a database of ```\n350,000```\n strings with an average length of about ```\n500```\n. The strings are not made up of words, they are an essentially random assortment of characters.\n\nI need to make sure no two of the strings are too similar, where similarity is defined as edit distance divided by avg length of string. The division is because smaller edit distances are more acceptable for smaller strings. It is fine if a different metric is used for performance reasons, but edit distance is the preferred baseline metric.\n\nNaively, we calculate edit distance with runtime ```\nO(a*b)```\n, where ```\na,b```\n are the length of the two strings. We do this for all ```\nn^2```\n pairs, which gives an overall runtime of ```\nO(n^2*a*b)```\n, clearly too large with ```\nn=350,000, a,b=500```\n.\n\nThe database is in the form of a Python list read from a csv file. I'd like to process it in a Pythonic way, if possible.\n\nHow can this be sped up? I'm not sure how long the naive algorithm will take to finish (on the order of weeks) but it ideally should take less than a day to run.\n    ", "Answer": "\r\nI wrote a very brief prototype of a simple locality sensitive hashing algorithm in python. However there are a few caveats and you may want to optimize some pieces as well. I'll mention them when we see them.\n\nAssume all your strings are stored in ```\nstrings```\n.\n\n```\nimport random\nfrom collections import Counter\n\nMAX_LENGTH = 500\nSAMPLING_LENGTH = 10\n\ndef bit_sampling(string, indices):\n    return ''.join([string[i] if i<len(string) else ' ' for i in indices])\n\nindices = random.sample(range(MAX_LENGTH),SAMPLING_LENGTH)\nhashes = [bit_sampling(string, indices) for string in strings]\n\ncounter = Counter(hashes)\nmost_common, count = counter.most_common()[0]\nwhile count > 1:\n    dup_indices = [i for i, x in enumerate(hashes) if x == most_common]\n    # You can use dup_indices to check the edit distance for original groups here.\n    counter.pop(most_common)\n    most_common, count = counter.most_common()[0]\n```\n\n\nFirst of all, this is a slight variant of bit sampling that works best for  the general hamming distance. Ideally if all your string are of the same length, this can give a theoretical probability bound for the hamming distance. When the hamming distance between two string is small, it is very unlikely that they will have different hash. This can be specified by the parameter ```\nSAMPLING_LENGTH```\n. A larger ```\nSAMPLING_LENGTH```\n will make it more likely to hash similar string to different hash but also reduce the probability of hashing not very similar string to the same hash. For hamming distance, you can calculate this trade-off easily.\n\nRun this snippet multiple times can increase your confident on no similar strings since each time you will sample different places.\n\nTo accommodate your purpose to compare different length strings, one possible approach is to left padding space on shorter strings and make copies of them.\n\nThough all of the operation in this snippet are linear (O(n)), it may still consume significant memory and running time and it might be possible to reduce a constant factor.\n\nYou might also want to consider using more complicated locality sensitive hashing algorithm such as surveyed here: https://arxiv.org/pdf/1408.2927.pdf\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Tools to compute graph edit distance (GED) [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 3 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI read a lot of theory on computing graph edit distance (GED), or other graph similarity measures (such as http://goo.gl/gmDMgA) but I'm failing to find tools to accomplish such computations.\n\nIs there a programming library or softwares that computes graph edit distances, or, once again, any other graph similarity measures, between two graphs?\n    ", "Answer": "\r\nThere are at least three possibilities for software to compute graph edit distance:\n\n\n  GEDEVO, is a software tool for solving the network alignment problem.\n  GEDEVO stands for Graph Edit Distance + EVOlution and it utilizes the\n  evolutionary computing strategies for solving the so-called Graph Edit\n  Distance problem.\n  \n  graph mapping distance matrix generator is parallel Java code which generates a graph mapping distance matrix. It is related to frequent subgraph mining based on the REAFUM algorithm.\n  \n  GRAPH EDIT DISTANCE : A NEW BINARY LINEAR FORMULATION from the paper New binary linear programming formulation to compute the graph edit distance\n  \n  ** You can find other Python, Java, and C++ implementations by searching sourceForge and gitHub. \n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Weighted Minimum Edit Distance\r\n                \r\nI am trying to learn about minimum edit distance. It is all good, except (check this image) I don't know how to find del[x(i)] and ins[y(j)] value. I know how to find the sub[x(i),y(j)] from this confusion matrix, but I really don't know how to get the delete and insert values, even after reading some slide and watching some videos.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Error in Dynamic Edit distance\r\n                \r\nGoal of this program is to calculate for each word of file 'correctme.txt' the minimum edit distance inside 'dictionary.txt'.\nI don't know why but my code goes to core dump (Segmentation Fault) after first for (inside). Function 'dynamic_edit_distance' is correct because i've already tested it.\nSame as for 'read_quote' and 'read_dictionary'.\nWhy this error?\n\n```\n#define QUOTE_SIZE 100\n#define DICTIONARY_SIZE 1000000\n\nint main(){\n  long i, j, min, a;\n  char** quote = read_quote(\"correctme.txt\", QUOTE_SIZE);\n  char** dictionary = read_dictionary(\"dictionary.txt\", DICTIONARY_SIZE);\n\n\n  for(i = 0; i < QUOTE_SIZE; i++){\n    min = QUOTE_SIZE;\n    for(j = 0; j < DICTIONARY_SIZE; j++){\n      dynamic_edit_distance(quote[i], dictionary[j]);\n      min = a < min ? a : min;\n    }\n    printf(\"%s\\tMinimum distance: %ld\", quote[i], min);\n  }\n\n  //free memory\n\n  for(i = 0; i < DICTIONARY_SIZE; i++)\n    free(dictionary[i]);\n  free(dictionary);\n  for(i = 0; i < QUOTE_SIZE; i++)\n    free(quote[i]);\n  free(quote);\n}\n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance such as Levenshtein taking into account proximity on keyboard\r\n                \r\nIs there an edit distance such as Levenshtein which takes into account distance for substitutions?\n\nFor example, if we would consider if words are equal, ```\ntypo```\n and ```\ntylo```\n are really close (```\np```\n and ```\nl```\n are physically close on the keyboard), while ```\ntypo```\n and ```\ntyqo```\n are far apart. I'd like to allocate a smaller distance to more likely typos.\n\nThere must be a metric that takes this kind of promixity into account?\n    ", "Answer": "\r\nthe kind of distance you ask is not included in levenshtein - but you should use a helper like euclidean or manhattan distance, to get the result.my simple assumption is, q (in english qwerty layout) is cartesian (y=0; x=0)\nso, w will be (y=0; x=1) and so on. whole list here\n\n```\nkeyboard_cartesian= {\n                     'q': {'y': 0, 'x': 0},\n                     'w': {'y': 0, 'x': 1},\n                     'e': {'y': 0, 'x': 2},   \n                     'r': {'y': 0, 'x': 3},    \n                      # ...\n                     'a': {'y': 1, 'x': 0}, \n                      #...\n                     'z': {'y': 2, 'x': 0},\n                     'x' : {'x':1, 'y':2},\n                      #   \n                     }\n```\n\n\nassume, word qaz has a meaning. \nlevenshtein distance between ```\nqaz```\n  and with both of ```\nwaz```\n and ```\neaz```\n is 1. to check out which misspell is more likely, take the differences (here (q,w) and (q,e)) and calculate euclidean distance\n\n```\n>>> from math import *\n>>> def euclidean_distance(a,b):\n...     X = (keyboard_cartesian[a]['x']-keyboard_cartesian[b]['x'])**2\n...     Y = (keyboard_cartesian[a]['y']-keyboard_cartesian[b]['y'])**2\n...     return sqrt(X+Y)\n... \n>>> euclidean_distance('q', 'w')\n1.0 \n>>> euclidean_distance('q', 'e')\n2.0\n```\n\n\nthis means misspell of qaz as waz is more likley than qaz as eaz.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to get an edit-distance between two commits?\r\n                \r\nI'm looking for a way to compute a good edit distance between the contents of any two commits.\n\nThe best I've found is to derive something from the output of\n\n```\ngit diff <commit-ish> <commit-ish> --numstat\n```\n\n\n...but anything I can come up using this method would be a very crude proxy for edit distance.\n\nIs there anything better?\n    ", "Answer": "\r\nI think your best bet here is to use an outside tool for calculating Levenshtein distance. For example Perl's ```\nText::Levenshtein```\n module.\n\nFor example, somewhat hackily:\n\n```\n#!/bin/sh\n\nCOMMIT_ONE=$1\nCOMMIT_TWO=$2\n\nFILES_AFFECTED=$(git diff $COMMIT_ONE $COMMIT_TWO --numstat | awk '{ print $3 }')\n\nTOTAL_LEV_DIST=0\nfor FILE in $FILES_AFFECTED; do\n\n    CONTENTS_ONE=$(git show $COMMIT_ONE:$FILE)\n    CONTENTS_TWO=$(git show $COMMIT_TWO:$FILE)\n\n    LEV_DIST=$(perl -MText::Levenshtein -e 'my ($str1, $str2) = @ARGV; print Text::Levenshtein::distance($str1, $str2);' \"$CONTENTS_ONE\" \"$CONTENTS_TWO\")\n\n    TOTAL_LEV_DIST=$(($TOTAL_LEV_DIST + $LEV_DIST))\n\ndone\n\necho $TOTAL_LEV_DIST\n```\n\n\nWhich seems to do the trick:\n\n```\n$ git diff HEAD HEAD~3 --numstat\n0       5       Changes\n1       3       dist.ini\n$ ./lev_dist_git_commits.sh HEAD HEAD~3\n230\n$ ./lev_dist_git_commits.sh HEAD HEAD\n0\n```\n\n\nNote: You can install ```\nText::Levenshtein::XS```\n for a speed boost if you have a C compiler and if speed is important. On my computer that reduced the time from 1.5s to 0.05s.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Customizing fuzzywuzzy string matching to edit distance <= 1\r\n                \r\nI am new in algorithms and my question may be silly, but how can I specify the edit distance in fuzzywuzzy library? I want edit distance <= 1 between two strings.\n\n```\nfrom fuzzywuzzy import fuzz\n\n\nfuzz.ratio('Apple', 'Aple')\n```\n\n\nI tried to look at source code, but I didn't understand how to customize\n    ", "Answer": "\r\nAre you restricted in any way to use the ```\nfuzzywuzzy```\n library? The ```\nratio```\n function does not have parameters to set the edit distance threshold. You could of course alter the source code, however this is something you generally avoid as it is hard to maintain (for example when ```\nfuzzywuzzy```\n gets an update).\n\nYou could simply implement the ```\nratio```\n function yourself, for example based on ```\nhttps://github.com/miohtama/python-Levenshtein```\n or ```\nhttps://github.com/aflc/editdistance```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Block edit distance with Swapping only\r\n                \r\nSuppose I have distinct alphabets ```\n∑={a1,a2,...,an}```\n. I also have two permutations of these alphabets, let's call them ```\nA,B```\n. How can I find the Edit distance between ```\nA```\n and ```\nB```\n with block edit operations allowed?\n\nTo make it clearer, an example would be ```\n∑={a,b,c,d}```\n. Two possible permutations are ```\nA=abcd```\n,```\nB=dabc```\n. The edit distance here would be 1 because we can swap the block ```\nabc```\n with ```\nd```\n to get one string to the other.\n\nClearly, there won't be any deletions/insertions in this form of the problem, it will purely be swaps since the two strings are permutations of the same letters.\n\nNow I know that the original edit-block problem with all operations is NP-Hard, however, would the restriction of only block swaps perhaps make this solvable in polynomial time? Most text I've read don't address this version and instead address variations of the original problem. Any help would be appreciated.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance or metaphone?\r\n                \r\nI am working on online review data, that is full of \"Internet lingo\". I want to do some lexicon analysis on the words. Long story short I want a spell checker that can take into account the language used in internet. After some research I, I found out 2 approaches:\n\n\nText Brew which is a modified version of edit distance.\nMetaphone, which uses sound based approach.\n\n\nPS. I will be parsing the data to clean some of the net lingo like \"lol\", \"lmao\" etc. My only concern is wrongly spelled words and I am working on Java.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance between two pandas columns\r\n                \r\nI have a pandas DataFrame consisting of two columns of strings. I would like to create a third column containing the Edit Distance of the two columns.\n\n```\nfrom nltk.metrics import edit_distance    \ndf['edit'] = edit_distance(df['column1'], df['column2'])\n```\n\n\nFor some reason this seems to go to some sort of infinite loop in the sense that it remains unresponsive for quite some time and then I have to terminate it manually. \n\nAny suggestions are welcome.\n    ", "Answer": "\r\nThe nltk's ```\nedit_distance```\n function is for comparing pairs of strings. If you want to compute the edit distance between corresponding pairs of strings, ```\napply```\n it separately to each row's strings like this:\n\n```\nresults = df.apply(lambda x: edit_distance(x[\"column1\"], x[\"column2\"]), axis=1)\n```\n\n\nOr like this (probably a little more efficient), to avoid including the irrelevant columns of the dataframe:\n\n```\nresults = df.loc[:, [\"column1\", \"column2\"]].apply(lambda x: edit_distance(*x), axis=1)\n```\n\n\nTo add the results to your dataframe, you'd use it like this:\n\n```\ndf[\"distance\"] = df.loc[:, [\"column1\",\"column2\"]].apply(lambda x: edit_distance(*x), axis=1)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to find edit distance between two s-expressions?\r\n                \r\nWould such an edit distance be a good measure of expression similarity? What if we wanted a semantic difference of two s-expressions? Can such an edit distance be used for s-expression compression?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to optimize edit distance code?\r\n                \r\nHow to optimize this edit distance code i.e. finding the number of bits changed between 2 values! e.g. word1 = '010000001000011111101000001001000110001' \n             word2 = '010000001000011111101000001011111111111' \n\nWhen i tried to run on Hadoop it takes ages to complete?\n\nHow to reduce the for loop and comparsions ?\n\n```\n#!/usr/bin/python\n\nimport os, re, string, sys\n\nfrom numpy import zeros\n\ndef calculateDistance(word1, word2):\n\n    x = zeros( (len(word1)+1, len(word2)+1) )\n\n    for i in range(0,len(word1)+1):\n\n        x[i,0] = i\n\n    for i in range(0,len(word2)+1):\n\n        x[0,i] = i\n\n    for j in range(1,len(word2)+1):\n\n        for i in range(1,len(word1)+1):\n\n            if word1[i-1] == word2[j-1]:\n\n                x[i,j] = x[i-1,j-1]\n\n            else:\n\n                minimum = x[i-1, j] + 1\n\n                if minimum > x[i, j-1] + 1:\n\n                    minimum = x[i, j-1] + 1\n\n                if minimum > x[i-1, j-1] + 1:\n\n                    minimum = x[i-1, j-1] + 1\n\n                x[i,j] = minimum\n\n    return x[len(word1), len(word2)]\n```\n\n    ", "Answer": "\r\nI looked for a bit counting algorithm online, and I found this page, which has several good algorithms. My favorite there is a one-line function which claims to work for Python 2.6 / 3.0:\n\n```\nreturn sum( b == '1' for b in bin(word1 ^ word2)[2:] )\n```\n\n\nI don't have Python, so I can't test, but if this one doesn't work, try one of the others. The key is to count the number of 1's in the bitwise XOR of your two words, because there will be a 1 for each difference.\n\nYou are calculating the Hamming distance, right?\n\nEDIT: I'm trying to understand your algorithm, and the way you're manipulating the inputs, it looks like they are actually arrays, and not just binary numbers. So I would expect that your code should look more like:\n\n```\nreturn sum( a != b for a, b in zip(word1, word2) )\n```\n\n\nEDIT2: I've figured out what your code does, and it's not the Hamming distance at all! It's actually the Levenshtein distance, which counts how many additions, deletions, or substitutions are needed to turn one string into another (the Hamming distance only counts substitutions, and so is only suitable for equal length strings of digits). Looking at the Wikipedia page, your algorithm is more or less a straight port of the pseudocode they have there. As they point out, the time and space complexity of a comparison of strings of length m and n is O(mn), which is pretty bad. They have a few suggestions of optimizations depending on your needs, but I don't know what you use this function for, so I can't say what would be best for you. If the Hamming distance is good enough for you, the code above should suffice (time complexity O(n)), but it gives different results on some sets of strings, even if they are of equal length, like '0101010101' and '1010101010', which have Hamming distance 10 (flip all bits) and Levenshtein distance 2 (remove the first 0 and add it at the end)\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "NZEC in Edit distance code\r\n                \r\nSPOJ is giving NZEC exception in my solution for the problem ==> http://www.spoj.com/problems/EDIST/\n\nMy solution is ==>\n\n```\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Scanner;\n\nclass EDIST {\n\n    public static int trans(String a, String b, int i, int j, Map<String, Integer> m)\n    {\n        if(m.containsKey(i+\" \"+j))\n            return m.get(i+\" \"+j);\n        if(i==a.length())\n            m.put(i+\" \"+j, b.length()-j);                   \n        else if(j==b.length())\n            m.put(i+\" \"+j, a.length()-i);\n        else if(a.charAt(i)==b.charAt(j))\n            m.put(i+\" \"+j, trans(a,b,i+1,j+1, m));\n        else \n            m.put(i+\" \"+j, 1+ Math.min(trans(a,b,i,j+1, m),Math.min(trans(a,b,i+1,j+1, m), trans(a,b,i+1,j, m)))); //I, R and D;\n        return m.get(i+\" \"+j); \n    }\n    public static void main(String[] args) {\n        Scanner in = new Scanner(System.in);\n        int t = in.nextInt();\n        while(t--!=0)\n        {\n            String a = in.next();\n            String b = in.next();\n            Map<String, Integer> map = new  HashMap<String, Integer>();\n            System.out.println(trans(a,b,0,0,map));\n        }\n        in.close();\n    }\n\n}\n```\n\n\ngeeksforgeeks(http://www.geeksforgeeks.org/dynamic-programming-set-5-edit-distance/) and my logic is same but still i am getting NZEC error.\nI am using HashMap for storing intermediate results.I have tested my code for large input but still, i do not why am i getting NZEC on spoj.\n    ", "Answer": "\r\nApart from getting uncaught exception, spoj also gives NZEC error if you forgot to remove package yourpackagename; from your code before submitting in spoj (although the code you added here doesn't contain this line but I still recommend you to verify this in your actual submission).\n\nI was getting NZEC error in different question for this reason.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Designing an algorithm to calculate the edit distance between two strings\r\n                \r\nPlease consider the following question:\n\nThe edit distance of two strings ```\ns```\n and ```\nt```\n is the minimum number of single character operations (insert, delete, substitution) needed to convert ```\ns```\n into ```\nt```\n. Let ```\nm```\n and ```\nn```\n be the length of strings ```\ns```\n and ```\nt```\n.\n\nDesign an ```\nO(nm)```\n time and ```\nO(nm)```\n space algorithm to calculate the edit distance between ```\ns```\n and ```\nt```\n.\n\nMy thoughts:\n\nIsn't it easier to just compare two strings one character at a time:\n\n```\nL = maximum(length(s), length(t))\nfor i in L:\n   if i > length(s):\n      distance += length(t) - i\n      break\n   if i > length(t):\n      distance += length(s) - i\n      break\n   if s[i] != t[i]:\n      distance += 1\n```\n\n\nIf I am wrong, then am I supposed to use the edit distance algorithm table? Is so, how do I design an ```\nO(nm)```\n time and ```\nO(nm)```\n space algorithm?\n    ", "Answer": "\r\nConsider the strings ```\nabcd```\n  and ```\nbcd```\n. They differ for one deletion, but your approach would count them as distance 4.\n\nWhat you want to do is find the Longest Common Subsequence. This is a well known problem  and you can google up a lot of code examples about it, with one solution being in fact O (NM).\n\nFor example, for strings ```\nabcdqef```\n and ```\nxybcdzzzef```\n the LCS is ```\nbcdqef```\n. consider the subsequence in the two strings: \n\n```\na-bcd-q-ef\nxy-bcd-zzz-ef\n```\n\n\nYou can transform ```\na```\n into ```\nxy```\n with one modification and one insertion, and ```\nq```\n into ```\nzzz```\n with one modification and two insertion. If you think about it, the number of operations required (i.e. distance) is the number of characters in the longest string not belonging to the LCS.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Computing Edit Distance (feed_dict error)\r\n                \r\nI've written some code in Tensorflow to compute the edit-distance between one string and a set of strings.  I can't figure out the error.\n\n```\nimport tensorflow as tf\nsess = tf.Session()\n\n# Create input data\ntest_string = ['foo']\nref_strings = ['food', 'bar']\n\ndef create_sparse_vec(word_list):\n    num_words = len(word_list)\n    indices = [[xi, 0, yi] for xi,x in enumerate(word_list) for yi,y in enumerate(x)]\n    chars = list(''.join(word_list))\n    return(tf.SparseTensor(indices, chars, [num_words,1,1]))\n\n\ntest_string_sparse = create_sparse_vec(test_string*len(ref_strings))\nref_string_sparse = create_sparse_vec(ref_strings)\n\nsess.run(tf.edit_distance(test_string_sparse, ref_string_sparse, normalize=True))\n```\n\n\nThis code works and when run, it produces the output:\n\n```\narray([[ 0.25],\n       [ 1.  ]], dtype=float32)\n```\n\n\nBut when I attempt to do this by feeding the sparse tensors in through sparse placeholders, I get an error.\n\n```\ntest_input = tf.sparse_placeholder(dtype=tf.string)\nref_input = tf.sparse_placeholder(dtype=tf.string)\n\nedit_distances = tf.edit_distance(test_input, ref_input, normalize=True)\n\nfeed_dict = {test_input: test_string_sparse,\n             ref_input: ref_string_sparse}\n\nsess.run(edit_distances, feed_dict=feed_dict)\n```\n\n\nHere is the error traceback:\n\n```\nTraceback (most recent call last):\n\n  File \"<ipython-input-29-4e06de0b7af3>\", line 1, in <module>\n    sess.run(edit_distances, feed_dict=feed_dict)\n\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 372, in run\nrun_metadata_ptr)\n\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 597, in _run\n    for subfeed, subfeed_val in _feed_fn(feed, feed_val):\n\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 558, in _feed_fn\n    return feed_fn(feed, feed_val)\n\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 268, in <lambda>\n    [feed.indices, feed.values, feed.shape], feed_val)),\n\nTypeError: zip argument #2 must support iteration\n```\n\n\nAny idea what is going on here?\n    ", "Answer": "\r\nTL;DR: For the return type of ```\ncreate_sparse_vec()```\n, use ```\ntf.SparseTensorValue```\n instead of ```\ntf.SparseTensor```\n.\n\nThe problem here comes from the return type of ```\ncreate_sparse_vec()```\n, which is ```\ntf.SparseTensor```\n, and is not understood as a feed value in the call to ```\nsess.run()```\n.\n\nWhen you feed a (dense) ```\ntf.Tensor```\n, the expected value type is a NumPy array (or certain objects that can be converted to an array). When you feed a ```\ntf.SparseTensor```\n, the expected value type is a ```\ntf.SparseTensorValue```\n, which is similar to a ```\ntf.SparseTensor```\n but its ```\nindices```\n, ```\nvalues```\n, and ```\nshape```\n properties are NumPy arrays (or certain objects that can be converted to arrays, like the lists in your example.\n\nThe following code should work:\n\n```\ndef create_sparse_vec(word_list):\n    num_words = len(word_list)\n    indices = [[xi, 0, yi] for xi,x in enumerate(word_list) for yi,y in enumerate(x)]\n    chars = list(''.join(word_list))\n    return tf.SparseTensorValue(indices, chars, [num_words,1,1])\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Worst case time complexity of edit distance?\r\n                \r\nI am trying to calculate the worst case scenario time complexity for finding the edit distance from ```\nT```\n test words to ```\nD```\n dictionary words, where all words have a length ```\nMAX_LEN```\n.\n    ", "Answer": "\r\nWorst time complexity can be exponential, ```\nO(3^MAX_LEN)```\n when using a naive recursive solution. The worst-case happens when none of the characters of two strings match.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to group strings that are one-edit distance\r\n                \r\nI have a list of strings such as:\n\n```\narr1 = [\"ABC\", \"ABD\", \"ABCD\", \"ABCE\", \"ACCE\", \"AB\"]\n```\n\n\nI want to group these strings into sublists, such that each sublist will only contain the strings that are x-edit-distance away. For example, 1-edit-distance away strings can be found by replacing one letter with something else. So for the list above, I want to produce:\n\n```\narr2 = [[\"ABC\", \"ABD\"], [\"ABCD\", \"ABCE\", \"ACCE\"], [\"AB\"]]\n```\n\n\nIs there an algorithm in the literature to solve this problem? What is an efficient way to solve this?\n\nEdit: The edit-distance I define is a bit different in the sense that: only replacement of x letters (if x=1, only 1 letter can be different) is allowed, no addition or deletion can be done. \n    ", "Answer": "\r\nThe algorithm implied by your examples may not end up being the algorithm you were looking for, but it's certainly possible:\n\n```\neditdist = lambda a, b: sum(0 if c1 == c2 else 1 for (c1, c2) in zip(a, b))\na = [\"ABC\", \"ABD\", \"ABCD\", \"ABCE\", \"ACCE\", \"AB\"]\na = list(reversed(a))\nret = []\nwhile a:\n    s = a.pop()\n    for sublist in ret:\n        if len(sublist[-1]) == len(s) and editdist(sublist[-1], s) == 1:\n            sublist.append(s)\n            s = None\n            break\n    if s: ret.append([s])\nprint ret\n```\n\n\nThe code assumes you want the result in your question: sequences of strings such that each string in the sequence is one edit distance away from the string before it and after it.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Key Error: 0 in Python with edit distance\r\n                \r\nCould someone please let me know why I am getting a \"Key Error: 0\" message with reference to line 23 for the following code? I am trying to implement an edit distance function, returning the cost and the last operation. Thanks!\n\n```\nfrom enum import Enum\n\n\nclass Operation(Enum):\n    \"\"\"Operations\"\"\"\n\n    DELETED = 1\n    INSERTED = 2\n    SUBSTITUTED = 3\n\n    def __str__(self):\n        return str(self.name.lower())\n\n\ndef distances(a, b):\n    \"\"\"Calculate edit distance from a to b\"\"\"\n\n    # edit distance\n    x = len(a) + 1\n    y = len(b) + 1\n    cost = {}\n    for i in range(0, x):\n        cost[i][0] = i\n    for j in range(0, y):\n        cost[0][j] = j\n    for i in range(1, x):\n        for j in range(1, y):\n            if a[i] == b[j]:\n                sub_cost = 0\n            else:\n                sub_cost = 1\n            cost[i][j] = min(cost[i - 1][j] + 1, cost[i][j - 1] + 1, cost[i - 1][j - 1] + sub_cost)\n\n            # final operation\n            if cost[i - 1][j] + 1 == min(cost[i - 1][j] + 1, cost[i][j - 1] + 1, cost[i - 1][j - 1] + sub_cost):\n                last_operation = Operation.DELETED\n            if cost[i][j - 1] + 1 == min(cost[i - 1][j] + 1, cost[i][j - 1] + 1, cost[i - 1][j - 1] + sub_cost):\n                last_operation = Operation.INSERTED\n            else:\n                last_operation = Operation.SUBSTITUTED\n\n\n    return cost[x][y], last_operation\n```\n\n    ", "Answer": "\r\nThe issue is that when you run ```\ncost[i][0] = i```\n on an empty dictionary, you are trying assign a value to a sub-dictionary, but since you have not initialized any values in your dictionary yet, there is nothing to access, hence the 'Key Error'. You must initialize the sub-dictionary before you can add a key/value to it\n\n```\nfor i in range(0, x):\n    cost[i] = {}\n    cost[i][0] = i\n```\n\n\nOr you could use defaultdict to set a default value of sub-items in your dictionary.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Using Data.Memocombinators to implement edit distance algorithm\r\n                \r\nLets say I wanted to implement the usual dynamic programming algorithm for Levensthein distance (edit distance). It is quite easy to come up with the recursion:\n\n```\neditDistance [] ys = length ys\neditDistance xs [] = length xs\neditDistance (x:xs) (y:ys) \n  | x == y = editDistance xs ys\n  | otherwise = minimum [\n      1 + editDistance xs (y:ys),\n      1 + editDistance (x:xs) ys,\n      1 + editDistance xs ys]\n```\n\n\nThis suffers from exponential running time, so it is necessary to memoize the function. I want to do so by using Data.Memocombinators, and I have tried several ways. Here is my current attempt:\n\n```\nimport qualified Data.MemoCombinators as M\n\nmemLength str = \n  M.wrap \n    (\\i -> drop i str) \n    (\\xs -> length str - length xs)\n    (M.arrayRange (0,length str))\n\nelm xs ys = (M.memo2 memListx memListy editDistance) xs ys\n  where\n    memListx = memLength xs\n    memListy = memLength ys\n```\n\n\nHowever, the memoization seems not to have any effect, although I would expect any memoization to have a noticeable improvement to running time since it would at least be polynomial. What is wrong with my implementation? How can I get an ok running time while preserving as much as possible of the high level definition of edit distance? \n    ", "Answer": "\r\nIf the code you posted is actually what you're doing, then you're doing it wrong! :-).  If you are going to memoize a recursive function, you need to have the calls to the recursive version call back into the memoized version.  So eg.:\n\n```\neditDistance (x:xs) (y:ys)\n  | x == y = elm xs ys\n  | ...\n```\n\n\nOtherwise you do the full recursive computation and only store the final result.  You need to store intermediate results.\n\nThere is another issue here.  The memo table for elm should not depend on its arguments (ideally you shouldn't even mention the arguments, so you don't depend on the compiler being smart enough to figure out the dependencies). If the memo table depends on the arguments, then you have to build a new table for each different argument, and we need to share a table for all arguments.  You could try something dumb like memoizing on the whole structure of the arguments:\n\n```\nelm = M.memo2 (M.list M.char) (M.list M.char)\n```\n\n\nSee if that speeds it up (incorporating the former trick).  Then you can move on to trying to use just the lengths instead of the entire list structure for an extra boost.\n\nHope that helped.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Generate regular expression for given string and edit distance\r\n                \r\nI have the problem that I want to match all strings in the database having a certain edit distance to a given string.\n\nMy idea was to generate a regular expression that would match all strings with edit distance ```\nd```\n to string ```\ns```\n.\n\nSo for example I want to generate a regex ```\nr```\n for ```\nd = 1```\n and ```\ns = 'abc'```\n in the form of: ```\nr = 'abc|.abc|.bc|a.c|ab.|abc.'```\n and so on. But I'm not sure if this is very efficient or are there already some good algorithms to that problem? I want to consider even character swaps in the edit distance. so ```\n'acb'```\n should also be part of ```\nr```\n. I want to realise it in PHP and then make an SQL query: ```\nSELECT * FROM table WHERE name RLIKE TheRegularExpression```\n.\n\nIs it a good way to make it like that? Or what would you recommend?\n    ", "Answer": "\r\nYou can store a Levenshtein function in Mysql. After that you can simply do the search like this:\n\n```\nmysql_qery(\"SELECT `term` FROM `words` WHERE levenshtein('$word', `term`) BETWEEN 0 AND '$d'\");\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to find the edit distance / levenshtein distance between a string and a language?\r\n                \r\nEdit distance of a string x from a language S is the edit distance of x from the 'closest' string y ∈ S. Given a string x∈{0,1,...,9,(,),+,-,*,/} * , I want to find an efficient algorithm that calculates the edit distance between x and the language of simple arithmetic expressions. I think I have to use dynamic-programming as in the classic edit-distance problem, but in this case I don't know the second string to compare to the first one. Should I use somehow the language limitations? Construct a syntax diagram or syntax tree? Thank you. \n\nP.S.: The language S of simple arithmetic expressions is defined as: \n\n\nevery string that is an example of the regular expression {1|2|...|9}{0|1|2|...|9}* | 0 belongs to S \nif x ∈ S then (x) ∈ S\nif x,y ∈ S then x+y, x-y, x*y, x/y ∈ S\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to modify Levenshteins Edit Distance to count \"adjacent letter exchanges\" as 1 edit\r\n                \r\nI'm playing around with Levenshteins Edit Distance algorithm, and I want to extend this to count transpositions -- that is, exchanges of adjacent letters -- as 1 edit. The unmodified algorithm counts insertions, deletes or substitutions needed to reach a certain string from another. For instance, the edit distance from \"KITTEN\" to \"SITTING\" is 3. Here's the explanation from Wikipedia:\n\n\nkitten → sitten (substitution of 'k' with 's')\nsitten → sittin (substitution of 'e' with 'i')\nsittin → sitting (insert 'g' at the end).\n\n\nFollowing the same method, the edit distance from \"CHIAR\" to \"CHAIR\" is 2:\n\n\nCHIAR → CHAR (delete 'I')\nCHAR → CHAIR (insert 'I')\n\n\nI would like to count this as \"1 edit\", since I only exchange two adjacent letters. How would I go about to do this? \n    ", "Answer": "\r\nYou need one more case in the algorithm from Wikipedia:\n\n```\nif s[i] = t[j] then \n  d[i, j] := d[i-1, j-1]\nelse if i > 0 and j > 0 and s[i] = t[j - 1] and s[i - 1] = t[j] then\n  d[i, j] := minimum\n             (\n               d[i-2, j-2] + 1 // transpose\n               d[i-1, j] + 1,  // deletion\n               d[i, j-1] + 1,  // insertion\n               d[i-1, j-1] + 1 // substitution\n             )\nelse\n  d[i, j] := minimum\n             (\n               d[i-1, j] + 1,  // deletion\n               d[i, j-1] + 1,  // insertion\n               d[i-1, j-1] + 1 // substitution\n             )\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Explanation of ukkonen's algorithm for edit distance\r\n                \r\nI was searching for an algorithm that performs better than normal O(nm) edit distance algorithm, and read that it has O(nd) worst case time complexity but couldn't find any proper explanation for it. Can someone please explain how the algorithm works? \n    ", "Answer": "\r\nThe best explanation of Ukkonen's algorithm is Ukkonen's suffix tree algorithm in plain English? Hope that helps, its not easy one to understand.\n\nBasically Ukkonen's algo allows you to construct a suffix tree quickly. You would then have to traverse the tree to calculate the edit distance.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Reducing time for minimum edit distance in python?\r\n                \r\nI am trying to create a list with the edit distances between each word in a set of documents, ranging from 10k-42k words. If my idea of edit distance is correct, I would end up with a distance for each word compared to every single other word. So if I had a corpus of 10k words each word would have 9,999 distances associated with it. Is there any way to optimize this or possibly a completely different approach as the run time is unreasonably long.\nI am using JW Distance and I have the following code:\n```\nru_distances = []\n# ru_final is a list of tokenized words\nfor i, j in enumerate(ru_final[:-1]):\n    a = j\n    b = ru_final[i + 1]\n    dist = distance.get_jaro_distance(a, b, winkler=True, scaling=0.1)\n    ru_distances.append((a, b, dist))\n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance with varying dictionaries\r\n                \r\nMy question is similar to Algorithm to transform one word to another through valid words\n\nBut with is a major difference. I have one fixed word say \"JAMES\" and varying dictionaries as i/p. Ofcourse, I can't preprocess dictionary now.\n\nSo I have to find the minimum cost for processing \"JAMES\" to \"JOHNY\" with different dictionaries as input.\n\nIs there anyway I could preprocess the word \"JAMES\" so that I need to perform minimum number of edit distance calculations at run time? What do you guys suggest?\n    ", "Answer": "\r\nI am assuming the rules are similar to the question you cited i.e. only single edit is allowed at a time, and each intermediate word should be present in the given dictionary.\n\n\nCreate single edit versions of source string to destination string. for eg:\nJOMES\nJAHES\nJAMNS\nJAMEY\n\n\nRecurse for each of these words and keep creating nodes for every unique word.  Just create edges if the node is already is created.  This completes the preprocessing.\n\nNow given a dictionary, find all the first level words in dictionary.  For all matches, further find all the second level words and so on, till you reach destination word.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "graph edit distance (GED) and (non)-isomorphic graphs\r\n                \r\nIs it necessary for graph edit distance (GED) between two graphs G1 and G2 that the graphs (G1, G2) are isomorphic?\n    ", "Answer": "\r\nCertainly not. Edit distance is defined for any pair of graphs.\n\nIsomorphic graphs are just a very special case: if G1 and G2 are isomorphic, than their edit distance is 0 directly from definition.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to combine TF-IDF with edit distance or Jaro-winkler distance\r\n                \r\nI am looking for ways to improve the accuracy of TF-IDF weighing scheme in string matching (similarity). The main issue is that TF-IDF is sensitive to typographical errors in stings, and most large datasets tend to have typos.\nI realised variants of edit distance (character-based similarity metrics---levienshtein, affine-gas, Jaro and Jaro-winkler) are suitable for computing similarity between strings where there are typographical errors, but not suitable when words are out of order in strings.\n\nHence I would like to use edit distance correcting ability to enhance the accuracy of TF-IDF.\n\nAny ideas on how to address this challenge will be highly appreciated. \n\nThanks in advance. \n    ", "Answer": "\r\nThere is a paper published by CMU researchers in 2003 and they have explained how to combine TFIDF with Jaro-Winkler:\nhttps://www.cs.cmu.edu/~pradeepr/papers/ijcai03.pdf\n\nTheir Java code is also available on sourceforge as secondString project:\nhttps://sourceforge.net/projects/secondstring/\n\nHere is a link to Javadocs:\nhttp://secondstring.sourceforge.net/javadoc/\n\nThe secondString project page:\nhttp://secondstring.sourceforge.net/\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Numpy implementation of Edit Distance Algorithm\r\n                \r\nI am new to numpy(and python) and working on making the edit distance Algorithm with numpy. This is my code so far. I have an error for the first line after the else: . The error says: \"index 3 is out of bounds for axis 0 with size 2\". I'm very confused as to what this means. I also have a question about how I'm initializing my T array - am I doing it in the best way? It took me a while to figure out how to make an empty 2d array that are 1 column and 1 row wide. Any guidance would be super helpful! (I also need to figure out how to make the lines within the if statement in terms of numpy)\nmore information about edit-distance function:\n[![implementing this in terms of numpy functions][2]][2]\n[2]: https://i.stack.imgur.com/IeJnI.png\n```\n    i = len(string1) - 1\n    j = len(string2) - 1\n    T = np.array([[1,1],[1,1]])\n\n    if string1[i] == string2[j]:\n      T[i][j] = T[i-1][j-1]\n\n    else:\n      T[i][j] = 1 + min(T[i-1][j], T[i-1][j-1], T[i][j-1])\n      \n\nassert(edit_distance('hello', 'relevant') == 6)\nassert(edit_distance('elephant', 'relevant') == 3)\nassert(edit_distance('statistics', 'mathematics') == 6)\nassert(edit_distance('numpy', 'alexa') == 5)```\n\n\n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance for n-grams?\r\n                \r\nI want to know if we can find the edit distance (or something like that) between n-grams?\n\nI'm creating a topic model for which I'm receiving topics of the following form:\n\n```\nTopic #5:\n[u'hour block', u'block hour', u'package hour', u'hour block hour']\nTopic #16:\n[u'schedule block', u'block schedule', u'able schedule block', u'open schedule']\n```\n\n\nSo like for the above example in topic #5 the first two bigrams are same except words in reverse order, I want to find such ngrams (like by finding some sort of a weight between each ngram in that topic) and group them under one (say represent ```\n'hour block'```\n and ```\n'block hour'```\n by ```\n'hour block'```\n only)\n\nHow do I do that?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Calculating edit distance on successive rows of a `Spark Dataframe\r\n                \r\nI have a data frame as follows:\n\n```\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\n\n// some data...\nval df = Seq(\n  (1, \"AA\", \"BB\", (\"AA\", \"BB\")),\n  (2, \"AA\", \"BB\", (\"AA\", \"BB\")),\n  (3, \"AB\", \"BB\", (\"AB\", \"BB\"))\n).toDF(\"id\",\"name\", \"surname\", \"array\")\n\ndf.show()\n```\n\n\nand i am looking to calculate the edit distance between the 'array' column in successive row. As an example i want to calculate the edit distance between the 'array' entity in column 1 (\"AA\", \"BB\") and the the 'array' entity in column 2 (\"AA\", \"BB\"). Here is the edit distance function i am using:\n\n```\ndef editDist2[A](a: Iterable[A], b: Iterable[A]): Int = {\n  val startRow = (0 to b.size).toList\n  a.foldLeft(startRow) { (prevRow, aElem) =>\n    (prevRow.zip(prevRow.tail).zip(b)).scanLeft(prevRow.head + 1) {\n      case (left, ((diag, up), bElem)) => {\n        val aGapScore = up + 1\n        val bGapScore = left + 1\n        val matchScore = diag + (if (aElem == bElem) 0 else 1)\n        List(aGapScore, bGapScore, matchScore).min\n      }\n    }\n  }.last\n}\n```\n\n\nI know i need to create a UDF for this function but can't seem to be able to. If i use the function as is and using Spark Windowing to get at the pervious row:\n\n```\n// creating window - ordered by ID\nval window = Window.orderBy(\"id\")\n\n// using the window with lag function to compare to previous value in each column\ndf.withColumn(\"edit-d\", editDist2(($\"array\"), lag(\"array\", 1).over(window))).show()\n```\n\n\ni get the following error:\n\n```\n<console>:245: error: type mismatch;\n found   : org.apache.spark.sql.ColumnName\n required: Iterable[?]\n       df.withColumn(\"edit-d\", editDist2(($\"array\"), lag(\"array\", 1).over(window))).show()\n```\n\n    ", "Answer": "\r\nI figured out you can use Spark's own levenshtein function for this. This function takes in two string to compare, so it can't be used with the array.\n\n```\n// creating window - ordered by ID\nval window = Window.orderBy(\"id\")\n\n// using the window with lag function to compare to previous value in each column\ndf.withColumn(\"edit-d\", levenshtein(($\"name\"), lag(\"name\", 1).over(window)) + levenshtein(($\"surname\"), lag(\"surname\", 1).over(window))).show()\n```\n\n\ngiving the desired output:\n\n```\n+---+----+-------+--------+------+\n| id|name|surname|   array|edit-d|\n+---+----+-------+--------+------+\n|  1|  AA|     BB|[AA, BB]|  null|\n|  2|  AA|     BB|[AA, BB]|     0|\n|  3|  AB|     BB|[AB, BB]|     1|\n+---+----+-------+--------+------+\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is it possible to infer the edit distance using a unified diff?\r\n                \r\nI have 2 files with contents spanning multiple lines. I'd like to find the edit distance; i.e. how many changes are required to transform A to B assuming only insertions and deletions are possible.\n\n```\n> cat > A                                 \nA\nB\nC\nD\nE\n> cat > B                                 \nA\nB\nD\nD\nF\nE\n> diff -u A B                             \n--- A   2015-05-12 16:09:31.000000000 +0200\n+++ B   2015-05-12 16:09:42.000000000 +0200\n@@ -1,5 +1,6 @@\n A\n B\n-C\n D\n+D\n+F\n E\n```\n\n\nWould it be accurate to say that the total number of ```\n+```\n and ```\n-```\n give me the edit distance?\n    ", "Answer": "\r\nGoing by your definition of edit distance (similiar to \"Longest common subsequence problem\"), you will first need to define what a single change is.\n\n\na single character?\na line?\na file?\n\n\n\n  The longest common subsequence problem is a classic computer science\n  problem, the basis of data comparison programs such as the ```\ndiff```\n\n  utility, and has applications in bioinformatics. It is also widely\n  used by revision control systems such as Git for reconciling multiple\n  changes made to a revision-controlled collection of files.\n\n\nAssuming you want lines to define a change (based on your example), then yes, the total number of ```\n+```\n and ```\n-```\n using the ```\ndiff```\n command would suffice. This is because an update/substitution will show up as both a deletion (```\n-```\n) and an insertion (```\n+```\n).\n\nSee also http://en.wikipedia.org/wiki/Diff_utility#Unified_format\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance solution for Large Strings\r\n                \r\nI'm trying to solve the edit distance problem. the code I've been using is below.\n\n```\n public static int minDistance(String word1, String word2) {\n    int len1 = word1.length();\n    int len2 = word2.length();\n\n    // len1+1, len2+1, because finally return dp[len1][len2]\n    int[][] dp = new int[len1 + 1][len2 + 1];\n\n    for (int i = 0; i <= len1; i++) {\n        dp[i][0] = i;\n    }\n\n    for (int j = 0; j <= len2; j++) {\n        dp[0][j] = j;\n    }\n\n    //iterate though, and check last char\n    for (int i = 0; i < len1; i++) {\n        char c1 = word1.charAt(i);\n        for (int j = 0; j < len2; j++) {\n            char c2 = word2.charAt(j);\n\n            //if last two chars equal\n            if (c1 == c2) {\n                //update dp value for +1 length\n                dp[i + 1][j + 1] = dp[i][j];\n            } else {\n                int replace = dp[i][j] + 1 ;\n                int insert = dp[i][j + 1] + 1  ;\n                int delete = dp[i + 1][j] + 1 ;\n\n\n                int min = replace > insert ? insert : replace;\n                min = delete > min ? min : delete;\n                dp[i + 1][j + 1] = min;\n            }\n        }\n    }\n\n    return dp[len1][len2];\n}\n```\n\n\nIt's a DP approach. The problem it since it use a 2D array we cant solve this problem using above method for large strings. Ex: String length > 100000.\n\nSo Is there anyway to modify this algorithm to overcome that difficulty ?\n\nNOTE: \nThe above code will accurately solve the Edit Distance problem for small strings. (which has length below 1000 or near)\n\nAs you can see in the code it uses a Java 2D Array \"dp[][]\" . So we can't initialize a 2D array for large rows and columns.\n\nEx : If i need to check 2  strings whose lengths are more than 100000\n\n```\nint[][] dp = new int[len1 + 1][len2 + 1];\n```\n\n\nthe above will be \n\n```\nint[][] dp = new int[100000][100000];\n```\n\n\nSo it will give a stackOverflow error.\n\nSo the above program only good for small length Strings. \nWhat I'm asking is , Is there any way to solve this problem for large strings(length > 100000) efficiently in java. \n    ", "Answer": "\r\nFirst of all, there's no problem in allocating a 100k x 100k int array in Java, you just have to do it in the Heap, not the Stack (and on a machine with around 80GB of memory :))\n\nSecondly, as a (very direct) hint:\n\nNote that in your loop, you are only ever using 2 rows at a time - row ```\ni```\n and row ```\ni+1```\n. In fact, you calculate row ```\ni+1```\n from row ```\ni```\n. Once you get ```\ni+1```\n you don't need to store row ```\ni```\n anymore. \n\nThis neat trick allows you to store only 2 rows at the same time, bringing down the space complexity from ```\nn^2```\n to ```\nn```\n. Since you stated that this is not homework (even though you're a CS undergrad by your profile...), I'll trust you to come up with the code yourself.\n\n Come to think of it I recall having this exact problem when I was doing a class in my CS degree...\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Fuzzy string matching using elasticsearchand return the edit distance or similarity score\r\n                \r\nI'm trying to do a match query with fuzzy results and order the results based on edit distance. However elasticsearch returns a relevance score (_score) which is based on frequency and each query. Is there anyway to obtain only the edit distance from elasticsearch. Also will writing my own custom function to calculate the edit distance slow down the search?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance leetcode\r\n                \r\nSo I am doing this question of EDIT DISTANCE and before going to DP approach I am trying to solve this question in recursive manner and I am facing some logical error, please help....\nHere is my code -\n```\nclass Solution {\npublic int minDistance(String word1, String word2) {\n    int n=word1.length();\n    int m=word2.length();\n    if(m<n)\n   return Solve(word1,word2,n,m);\n     else\n                return Solve(word2,word1,m,n);\n}\nprivate int Solve(String word1,String word2,int n,int m){\n     if(n==0||m==0)\n        return Math.abs(n-m);\n    \n    if(word1.charAt(n-1)==word2.charAt(m-1))\n        return 0+Solve(word1,word2,n-1,m-1);\n    \n    else{\n        //insert\n        int insert = 1+Solve(word1,word2,n-1,m);\n        \n        //replace\n        int replace = 1+Solve(word1,word2,n-1,m-1);\n        \n        //delete\n        int delete = 1+Solve(word1,word2,n-1,m);\n        \n        int max1 = Math.min(insert,replace);\n        return Math.min(max1,delete);\n    }\n}\n```\n\n}\nhere I am checking the last element of both the strings if both the characters are equal then simple moving both string to n-1 and m-1 resp.\nElse\nNow I am having 3 cases of insertion , deletion and replace ,and between these 3 I have to find minima.\nIf I am replacing the character then simply I moved the character to n-1 & m-1.\nIf I am inserting the character from my logic I think I should insert the character at the last of smaller length string and move the pointer to n-1 and m\nTo delete the element I think I should delete the element from the larger length String that's why I move pointer to n-1 and m but I think I am making mistake here please help.\nLeetcode is giving me wrong answer for word1 = \"plasma\" and word2 = \"altruism\".\n    ", "Answer": "\r\nThe problem is that the recursive expression for the insert-case is the same as for the delete-case.\nReasoning further, it turns out the one for the insert-case is wrong. In that case we choose to resolve the letter in word2 (at index ```\nm-1```\n) through insertion, so it should not be considered any more during the recursive process. On the other hand the considered letter in word1 could still be matched with another letter in word2, so that letter should still be considered during the recursive process.\nThat means that ```\nm```\n should be decremented, not ```\nn```\n.\nSo change:\n```\nint insert = 1+Solve(word1,word2,n-1,m);\n```\n\nto:\n```\nint insert = 1+Solve(word1,word2,n,m-1);\n```\n\n...and it will work. Then remains to add the memoization for getting a good efficiency.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "use edit distance on arrays in perl\r\n                \r\nI am attempting to compare the edit distance between two arrays. I have tried using Text:Levenshtein.\n\n```\n#!/usr/bin/perl -w\nuse strict;\nuse Text::Levenshtein qw(distance);\n\nmy @words = qw(four foo bar);\nmy @list = qw(foo fear);\nmy @distances = distance(@list, @words);\n\nprint \"@distances\\n\";\n#results: 3 2 0 3\n```\n\n\nI however want the results to appear as follows:\n\n```\n2 0 3\n2 3 2\n```\n\n\nTaking the first element of @list through the array of @words and doing the same through out the rest of the elements of @list.\nI plan on upscaling this to a much larger arrays.\n    ", "Answer": "\r\nI'm not sure to understand exactly what you meant, but I think this is what you expect :\n\n```\n#!/usr/bin/perl -w\nuse strict;\nuse Text::Levenshtein qw(distance);\n\nmy @words = qw(four foo bar);\nmy @list = qw(foo fear);\n\nforeach my $word (@list) {\n   my @distances = distance($word, @words);\n   print \"@distances\\n\";\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Proof of correct of the dynamic programming approach to min edit distance\r\n                \r\nTo calculate min edit distance (the minimum amount of insertions, deletions and substitutions required to transform one word to another), a dynamic programming solution is based on the recurrence relation, where the last character of both string is examined. The details are in https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm.\n\nThe description of this algorithm is everywhere on the Internet when it comes to edit distance, but all of them just asserts its correctness without proof.  By definition of edit distance, you can insert, delete or substitute characters in the middle, not just at the end. Then how do you prove that this recurrence relation actually holds?\n    ", "Answer": "\r\nUse Induction to Prove Recursive Algorithms Correct\n\nFirst, as I said in the comment, you can view dynamic programming as a way to speed up recursion, and the easiest way to prove a recursive algorithm correct is nearly always by induction: Show that it's correct on some small base case(s), and then show that, assuming it is correct for a problem of size n, it is also correct for a problem of size n+1.  This way the proof closely mirrors the recursive structure.\n\nThe usual recursion for this problem breaks the problem \"Find the minimum cost to edit string A into string B\" into the (|A|+1)(|B|+1) subproblems \"Find the minimum cost to edit the first i characters of string A into the first j characters of string B\", for all 0 <= i <= |A| and 0 <= j <= |B|.\n\nChoosing Base Cases\n\nUsually with induction, we can pick a small number of simple base cases (perhaps just one), show that we can easily compute the correct answers for them, and it's obvious how the correctness of all other cases will be implied by the correctness of the base cases, because regardless of what case we start with, there will be just a single \"chain\" of assumptions that needs to be satisfied, and this chain clearly must end at one of our base cases.  However for this particular problem, to show that the algorithm solves the (i, j) subproblem optimally, we need to first assume that it solves the (i-1, j), (i, j-1) and (i-1, j-1) subproblems optimally (since if any of the answers to those subproblems was incorrect, it could easily compute a totally wrong answer for the (i, j) subproblem).  This will require a more complicated induction than usual: instead of a single \"chain\" of assumptions that need to be satisfied, we now have a branching \"tree\" of assumptions, with (up to) 3 children at each point.  We need to pick base cases in such a way that for any (i, j), this entire tree of assumptions eventually \"stops\", i.e., every path in it eventually hits a base case where its assumptions are satisfied.\n\nTo recap: To prove our solution to (i, j) optimal, we must assume we have optimal solutions for (i-1, j), (i, j-1), and (i-1, j-1); to satisfy that assumption for, say, (i-1, j) (that is, to prove our solution to (i-1, j) is optimal), we need to assume we have optimal solutions for (i-2, j), (i-1, j-1) and (i-2, j-1), etc., etc.  How to choose base case(s) that will work?  While traversing down this tree, i.e., in going from proving our solution to the subproblem (i, j) correct to proving our solution to any one of its \"child\" subproblems (i', j') correct, we notice that:\n\n\nAt least one of i' < i or j' < j holds.\nWe never \"skip over\" subproblems -- that is, we never have i-i' >= 2, or j-j' >= 2.\n\n\nBasically, if we take a single step down this tree, at least one of our two \"subproblem co-ordinates\" (i or j) decreases, but never by more than 1.  What that means is that if we keep descending through the tree, then regardless of which particular \"children\" subproblems we pick on the way down, we must eventually hit a subproblem with a 0 for (at least) one of its co-ordinates -- i.e. a (i'', 0) subproblem for some 0 <= i'' <= |A| or a (0, j'') subproblem for some 0 <= j'' <= |B|.  And what that means is that if we make those subproblems our base cases, we can ensure that every path in the induction tree hits a base case where its assumptions are satisfied and can therefore stop.\n\nLuckily, these base cases are indeed easy to compute optimal answers to.  Consider a problem (i, 0): This problem asks for the minimum cost required to change the first i characters of string A into the first 0 characters of string B.  Clearly the best (only!) way to do this is to delete all i characters in the prefix of A, for a cost of i.  Likewise the problem (0, j) asks for the minimum cost required to change the first 0 characters of A into the first j characters of B: just as clearly, the best way to do this is to simply insert all j characters in this prefix of B, for a cost of j.\n\nInductive Step\n\nAll that remains is the inductive step: Showing that we compute the answer to the (i, j) subproblem correctly, under the assumption that we have computed the answers to the (i-1, j), (i, j-1) and (i-1, j-1) subproblems correctly.  The trick is to see that in every possible way of editing the first i characters of A into the first j characters of B, there are actually only 3 possible things that we could do with the last character in each of these prefixes (that is, the i-th character in A and the j-th character in B):\n\n\nPair A[i] with B[j].  Either they match (cost 0) or not (cost 1), but either way, the total cost of this pairing must be that cost (either 0 or 1), plus the smallest possible cost of editing the rest of the prefix of A (A[1 .. i-1]) into the rest of the prefix of B (B[1 .. j-1]) -- which, by assumption, we have already calculated correctly!\nDelete A[i].  This costs 1, so the total cost of doing this must be 1 plus the smallest possible cost of editing the rest of the prefix of A (A[1 .. i-1]) into the entire prefix of B (B[1 .. j]) -- which, by assumption, we have already calculated correctly!\nInsert B[j].  This costs 1, so the total cost of doing this must be 1 plus the smallest possible cost of editing the entire prefix of A (A[1 .. i]) into the rest of the prefix of B (B[1 .. j-1]) -- which, by assumption, we have already calculated correctly!\n\n\nSince those 3 things are the only possible things that we could do, and for each of the 3 we have calculated the overall lowest cost of doing that thing, the overall best thing to do must be the best of the 3 of them.  This proves that we correctly calculate the lowest cost needed to edit the first i characters of A into the first j characters of B, for any i and j -- so in particular, it's true for i = |A| and j = |B|, that is, for editing the complete string A into the complete string B.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "The difference in application between SequenceMatcher in edit distance and that in difflib?\r\n                \r\nI know the implementation of the edit distance algorithm. By dynamic programming, we first fill the first column and first row and then the entries immediately right and below of the filled entries by comparing three paths from the left, above, and left above. While for the Ratcliff/Obershelp algorithm, we first extract the longest common substring out from the two strings, then we do recursive operations for the left side two sub-strings and right side two sub-strings until no characters are left.\nBoth of them can be utilized to calculate the similarity between two strings and transform one string into another using four operations: delete, replace, copy, and insert.\nBut I wonder when to use which between SequenceMatcher in edit distance and that in difflib?\nHere is what I found on the internet that makes me think that this question would also benefit others:\n\nIn the documentation of edit distance it reads that\n\n\nSimilar to the difflib SequenceMatcher, but uses Levenshtein/edit distance.\n\n\nIn this answer to a question on calculating edit distance, an answer on Ratcliff/Obershelp algorithm was provided.\nThere are only a few resources about the Ratcliff/Obershelp algorithm, let alone its comparison to edit distance that I thought is the most well known string alignment algorithm.\n\nSo far as I know, I have the following ideas:\n\nI find that edit distance and the Ratcliff/Obershelp algorithm can both be used for spell checking. But when to use which?\n\nI thought the edit distance is employed to find the minimal edit sequence while the Ratcliff/Obershelp algorithm yields matches that \"look right\" to people. However, 'look right' seems too vague a term, especially in real world applications. What's more, when is the minimum edit sequence a must/preferred?\n\n\nAny suggestions would be highly appreciated, and thanks in advance.\n    ", "Answer": "\r\n\"Looks right to people\" needn't be all that vague. Search the web for discussion of why, e.g., the very widely used ```\ngit```\n source control system added \"patience\" and \"histogram\" differencing algorithms, as options. Variations of \"minimal edit distance\" routinely produce diffs that are jarring to humans, and I'm not going to reproduce examples here that are easily found by searching.\nFrom a formal perspective, Levenshtein is more in line with what a mathematician means by \"distance\". Chiefly, ```\ndifflib```\n's ```\n.ratio()```\n can depend on the order of the arguments passed to it, but Levenshtein is insensitve to order:\n```\n>>> import difflib\n>>> difflib.SequenceMatcher(None, \"tide\", \"diet\").ratio()\n0.25\n>>> difflib.SequenceMatcher(None, \"diet\", \"tide\").ratio()\n0.5\n```\n\nFor the rest, I don't think you're going to get crisp answers. There are many notions of \"similarity\", not just the two you mentioned, and they all have their fans. \"Minimal\" was probably thought to be more important back when disk space and bandwidth were scarce and expensive.\nThe physical realities constraining genetic mutation have made measures that take into account spatial locality much more important in that field - doesn't matter one whit if it's \"minimal\" if it's also physically implausible ;-) Terms to search for: Smith–Waterman, and Needleman–Wunsch.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance alike algorithm in recursive\r\n                \r\nI was given a task to find the \"Edit distance\" between array of binary int e.g: 011011 and such\n\nThere are 3 options: 1. remove bit from the right 2. remove bit from the left, 3. do nothing\n\nNow if we remove a bit the distance is reduced by 1 and if the bits are matching the distance is increased by one.\n\nSo for example: s1=01010101 s2=10101010 we can remove the leftmost bit in s1 (-1) and the rightmost bit in s2 (-1) and get s1=1010101 and s_2=1010101 which is 7-2=5\n\nI am trying to write an algorithm and thought about the following:\n\n```\nfun(s1,s2){\n    if s1[i] == s2[i]\n        score++\n    else\n        return min(fun(s1[n-1],s2),fun(s1,s2[n-1]),fun(s1+1,s2),fun(s1,s2+1))-1\n}\n```\n\n\nHow to proceed from here?\n    ", "Answer": "\r\nYou have four options in total remove leftmost bit from ```\ns1```\n, remove rightmost bit from ```\ns1```\n, remove leftmost bit from ```\ns2```\n, remove rightmost bit from ```\ns2```\n. Try all four oprions and take the minimum. Here is a solution in python using memoization in python.\n\n```\nmemo = {}\ndef fun(s1, s2):\n    if s1 == s2:\n        return 0\n    if (s1, s2) in memo:\n        return memo[s1, s2]\n    r = 1e10 # infinity\n    if len(s1) > 0:\n        # remove left bit from s1\n        r = min(r, 1 + fun(s1[1:], s2))\n        # remove right bit from s1\n        r = min(r, 1 + fun(s1[:-1], s2))\n    if len(s2) > 0:\n        # remove left bit from s2\n        r = min(r, 1 + fun(s1, s2[1:]))\n        # remove right bit from s2\n        r = min(r, 1 + fun(s1, s2[:-1]))\n    memo[s1, s2] = r\n    return r\nfun('01010101', '10101010') # 2    \n```\n\n\nThis can be further optimized by using indices of the substring instead passing the string as argument. Time complexity is ```\nO(n^4)```\n. The fact that you cannot just insert or remove character from anywhere you want actually makes it complexity higher I think. I think it can be reduced though.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Algorithm to find one edit distance words from input word using Levenshtein distance?\r\n                \r\nI have a dictionary which has so much words in it(Approximately 100000). I am taking one word from user which is wrote wrong. For example this word is \"andd\". User always write wrong and with one edit distance. My program scan all the dict and find all 1 edit distance words and according to their usage rates, return true correct spelling. For example it finds and, andy, ande. After that calculate usage rate and return one of them.\n\nHowever my program works very slowly when I take 300 words from user. Therefore I want to change my code. Firstly, I want to create all words with edit distance 1 from given word and check which ones in the dict. If word in the dict, I will calculate usage rate again and return it. \nWith this way, my program don't control edit distance for every word in the dict. \n\nBriefly, I want to create all combinations with edit distance 1 from given input word. Should I add the whole alphabet and try it everywhere, or does it have an algorithm? I couldn't find anything on the internet. Thank you everyone. \n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Find out edit distance between two strings\r\n                \r\nI am calculating edit distance between two dataframe. Both the dataframe consists of ~30L of rows, as the dataframe size is large it is taking lot of time. Is there any way to improve the performance?\n```\nfor i in range(0,len(targets1)):\n    if i % 100 == 0:\n        pct = (i/len(targets1)) * 100\n        print(\"(\" + str(dt.datetime.now()) + \") completed: \" + str(round(pct, 2)) + \"%\")\n    sr1_new=sr1[(sr2==targets2[i]) & (len_sr2>=(len_targets2[i]-10)) & (len_sr2 <=(len_targets2[i]+10))]\n    if len(sr1_new) > 0:\n        ee=sr1_new.str.edit_distance(targets1[i])\n        ee=ee.sort_values()\n        output_final = output_final.append({'Name': targets[i],'Matched_Name': sr[ee.index[0]],'score':ee[ee.index[0]],'score_final':(len(sr[ee.index[0]])+len(targets[i])-ee[ee.index[0]])/(len(sr[ee.index[0]])+len(targets[i]))*100}, ignore_index=True)\n    else:\n        output_final = output_final.append({'Name': targets[i],'Matched_Name': '','Matched_REF': \"0\",'score':0,'score_final':0}, ignore_index=True)\n```\n\n```\ntargets1 = pd.Series(['ABBSHHCH','ABBSAJSJAHDKAJKJ', 'BASJBASJASH', 'KJSAKASJAS', 'KJSAIUBDAKS',\n                'KAJSNDSAX', 'JASANXAJSKJ', 'NASNXHY', 'AIUSSHXBAHSJASHJ'])\n\ntargets2 = pd.Series(['AB','AB', 'BA', 'KJ', 'KJ','KA', 'JA', 'NA', 'AI'])\n\nsr1 = pd.Series(['ABBSHHSJAKX','ABBMNASASJKKLASAHDKAJKJ', 'BASSAMSAJASH', 'KJSMSANMAASJAS', 'KJSSMNASBDAKS',\n                'KASKJADSAX', 'JASAKJKJSKJ', 'NASAKXHY', 'AIUSSANMASSJASHJ','NSAASJNCXA','ABBSASMNKAJKJ', 'ASNASNXJASH', \n                'KJSKJSAKSJAS', 'KJASKJSDAKS', 'KAJSAKJSAX', 'JAKJASXAJSKJ', 'NADADHY', 'AIUSNASSASJASHJ'])\n\nsr2 = pd.Series(['AB','AB','BA','KJ','KJ','KA','JA','NA','AI','NS','AB','AS','KJ','KJ','KA','JA','NA','AI'])\n\nlen_sr2 = pd.Series([11,23,12,14,13,10,11,8,16,10,13,11,12,11,10,12,7,15])\n\nlen_targets2 = pd.Series([8,16,11,10,11,9,11,7,16])\n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Formulate edit distance as matrix multiplication\r\n                \r\nI am computing a weighted edit distance between two strings using a slight modification of the Levenshtein distance where I use context-specific edit operation probabilities. Unlike the standard Levenshtein that only considers the best sequence of edit operations, here, I am interested in a sum of costs of possible edit operation sequences transforming one string into another.\n\nFor two strings ```\ns1```\n and ```\ns2```\n, I have a cost tensor of shape ```\nlen(s1)```\n × ```\nlen(s2)```\n × 3 with costs that correspond to doing deletion, insertion respectively. For simplicity, both strings start with a technical start symbol which stands for an empty string.\n\n```\ndeletion_id = 0\ninsertion_id = 1\nsubstitute_id = 2\n\nalpha = np.zeros((len(s1), len(s2))\nalpha[0, 0] = 1.0\n\nfor t, _ in enumerate(s1):\n    for v, _ in enumerate(s2):\n        if v >= 1:\n            alpha[t, v] += costs[t, v, insertion_id] * alpha[t, v - 1]\n        if t >= 1:\n            alpha[t, v] += costs[t, v, deletion_id] * alpha[t - 1, v]\n        if v >= 1 and t >= 1:\n            alpha[t, v] += costs[t, v, subsitute_id] * alpha[t - 1, v - 1]\n```\n\n\nThe sum of probabilities of all possible ends up in ```\nalpha[len(s1) - 1, len(s2) - 1]```\n.\n\nThe nested for loops very much resemble the definition of matrix multiplication. Vectorization of the computation could speed things up, but I did not figure out how to reformulate this using matrix multiplication. Any ideas?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Distance edit array output\r\n                \r\nI am doing an edit distance with the user input. I am storing my values in array. then the edit distance will compare the user input with my array of strings. I am doing a loop that if the edit distance is more than 2 it will display invalid else valid.\n\nThe only problem I've got is that although the program is working out fine, the output is the result of all the '28' strings that I have in my array. I would like to display only invalid or valid once.\n\nTest is my array of strings and user is - String user - the user input.\n\n\n\n```\nvoid testingLD()\n{\n  for (int i=0; i<test.length; i++)\n  {\n      if(getLevenshteinDistance(test[i],user) > 2)\n      {\n        println (\"Invalid re-input\");\n      }\n      else \n      {\n        println (\"Valid\");\n      }\n  }\n}\n```\n\n    ", "Answer": "\r\nYou have your print line functions inside your loop so they get printed once per iteration.\n\nTry this.\n\n```\nvoid testingLD()\n{\n  boolean isValid = true; // assume true, check each and update\n\n  // begin loop\n  for (int i=0; i<test.length; i++)\n  {\n      if(getLevenshteinDistance(test[i],user) > 2)\n      {\n        isValid = false;\n        break; // don't need to test the rest of the loop if we already found a false\n      }\n  }\n  // end loop\n\n  if(isValid){\n    println(\"Valid\");\n  }else{\n    println(\"Invalid re-input\");\n  }\n}\n```\n\n\nSimilarly you could count the number of valid ```\nint validCount = 0; validCount++```\n and then display stats about how many were valid, the percentage etc. Or keep an array of the invalid strings and display those as the ones that fail etc!\n\nWrap up:\nWhen you want to check an entire collection or array for some condition and output one answer make sure to have your output outside of the loop! \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance algorithm special issue\r\n                \r\nmy first post here.\nI'm writing generic edit distance algorithm for online course. I've tried to test it and don't see problem with it yet the online testing system shows I'm failing one of the tests without saying where the problem is.\nThe long \"if\" clause is for special case when algorithm fails to add \"1\" like for example strings \"aab\" and \"aba\" where I was getting distance of 1 instead of 2.\nSo what I'm trying to do here is find border cases for which my algorithm doesn't work.\nI'm actually surprised because I wrote this following directions from a book and I'm pretty sure algorithm in the book would fail on these special cases as well.\nthis is the code:\n```\ndef editDistance(string1, string2):\n\n    string1 = list(string1)\n    string2 = list(string2)\n\n    m = len(string1)\n    n = len(string2)\n\n    matrix = []\n\n    matrix.append([])\n\n    for i in range(0, m+1):\n        matrix[0].append(i)\n\n    for j in range(1, n+1):\n        matrix.append([j])\n\n    for j in range(1, n+1):\n        for i in range(1, m+1):\n            minimum = min(matrix[j][i-1],matrix[j-1][i],matrix[j-1][i-1])\n\n            if i > 1 and string1[i-1] == string1[i-2] == string2[i-2] != string2[i-1] and minimum == 0:\n                matrix[j].append(1)\n            elif string1[i-1] == string2[j-1]:\n                matrix[j].append(minimum)\n            else:\n                matrix[j].append(minimum + 1)\n\n    #for _ in range(n+1):\n        #print(matrix[_])\n\n    return matrix[n][m]\n\n\nentry1 = input()\nentry2 = input()\n\nprint(editDistance(entry1,entry2))\n```\n\n    ", "Answer": "\r\n```\nEdit Distance```\n carries some ambiguity, as there are several ways to measure it See Wikipedia\nYou'd get a distance of 1 if you are calculating the Damerau-Levenshtein distance because there are 4 string operation each with cost 1 (insertion, deletion, substitution, adjacent transposition).\nPlain old Levenshtein would give you 2 because there are 3 operations (insertion, deletion & substitution).\nYour implementation looks very similar to the pseudo-code here.\nI'm guessing the site you are submitting your attempts to is expecting the Levenshtein\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Variation of Edit distance algorithm that only tracks substitutions and insertions\r\n                \r\nDoes anyone know of edit-distance algorithm that only counts substitutions and insertions. So basically, it would be Levenshtein Distance algorithm without deletions. \n    ", "Answer": "\r\nYou can use almost the same dynamic programming solution that is used for computing normal Levenshtein distance, but without transitions that correspond to deletions.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Normalizing graph edit distance to [0,1] (networkx)\r\n                \r\nI want to have a normalized graph edit distance.\nI'm using this function:\nhttps://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.similarity.graph_edit_distance.html#networkx.algorithms.similarity.graph_edit_distance\n\nI'm trying to understand to graph_edit_distance function in order to be able to normalize it between 0 and 1 but I don't understand it fully.\n\nFor example: \n\n```\ndef compare_graphs(Ga, Gb):\n    draw_graph(Ga)\n    draw_graph(Gb)\n    graph_edit_distance = nx.graph_edit_distance(Ga, Gb, node_match=lambda x,y : x==y)\n    return graph_edit_distance\n\n    compare_graphs(G1, G3)\n```\n\n\nWhy is the graph_edit_distance = 4? \n\n\n\nGraph construction:\nHey\n\n```\ne1 = [(1,2), (2,3)]\ne3 = [(1,3), (3,1)]\nG1 = nx.DiGraph()\nG1.add_edges_from(e1)\nG3 = nx.DiGraph()\nG3.add_edges_from(e3)\n```\n\n\nThe edit distance is measured by:\n\n```\nnx.graph_edit_distance(Ga, Gb, node_match=lambda x,y : x==y)\n```\n\n\nThe difference from graph_edit_distance is that it relates to node indexes. \n\nThis is the output of optimize_edit_paths: \n\n```\nlist(optimize_edit_paths(G1, G2, node_match, edge_match,\n                        node_subst_cost, node_del_cost, node_ins_cost,\n                        edge_subst_cost, edge_del_cost, edge_ins_cost,\n                        upper_bound, True))\nOut[3]: \n[([(1, 1), (2, None), (3, 3)],\n  [((1, 2), None), ((2, 3), None), (None, (1, 3)), (None, (3, 1))],\n  5.0),\n ([(1, 1), (2, 3), (3, None)],\n  [((1, 2), (1, 3)), (None, (3, 1)), ((2, 3), None)],\n  4.0)]\n```\n\n\nI know it should be the minimum sequence of node and edge edit operations transforming graph G1 to graph isomorphic to G2.\n\nWhen I try to count, I get: \n1. Add node 2 to G3, \n2. Cancel e1=(1,3) from G3\n3. Cancel e2=(3,1) from G3\n4. Add e3 = (1,2) to G3\n5. Add e4 = (2,3) to G3\n\ngraph_edit_distance = 5.\n\nWhat am I missing? \n\nOr alternatively, what can I do in order to normalize the distance I receive?\n\nI thought about dividing by |V1| + |V2| + |E1| + |E2|, or dividing by max(|V1| + |E1|, |V2| + |E2|)) but I'm not sure.\n\nThanks in advance. \n    ", "Answer": "\r\nI know its old post but I am currently reading about GED and was willing to answer it for someone looking for it in future.\nGraph edit distance is 4\nReason:\n1 and 3 are connected using an undirected edge. While 1 and 2 are connected using directed edge. Now graph edit path will be :\n\nTurn undirected edge to directed edge\nChange 3 to 2 (substitution)\nAdd an edge to 2\nFinally add a node to that edge\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Editing distance with limit (threshold)\r\n                \r\nI work for a project in Java 8 and I want to compute the editing distance for 2 strings - in a iterative way (so without recursion); the method will be executed many times, so I need to improve it with a given limit (threshold), meaning that if the editing distance from 2 strings is greater than a given number x, the algorithm will be stopped.\nNow, where from should I find a method like this? Is there a java library (or provided from maven) which contain such a method? e.g. in StringUtils?\nFor current implementation I use classic editing distance algorithm.\n```\n    static int min(int x, int y, int z)\n{\n    if (x <= y && x <= z)\n        return x;\n    if (y <= x && y <= z)\n        return y;\n    else\n        return z;\n}\n\nstatic int editDistDP(String str1, String str2, int m,\n                      int n)\n{\n    // Create a table to store results of subproblems\n    int dp[][] = new int[m + 1][n + 1];\n\n    // Fill d[][] in bottom up manner\n    for (int i = 0; i <= m; i++) {\n        for (int j = 0; j <= n; j++) {\n            // If first string is empty, only option is\n            // to insert all characters of second string\n            if (i == 0)\n                dp[i][j] = j; // Min. operations = j\n\n            // If second string is empty, only option is\n            // to remove all characters of second string\n            else if (j == 0)\n                dp[i][j] = i; // Min. operations = i\n\n            // If last characters are same, ignore last\n            // char and recur for remaining string\n            else if (str1.charAt(i - 1)\n                     == str2.charAt(j - 1))\n                dp[i][j] = dp[i - 1][j - 1];\n\n            // If the last character is different,\n            // consider all possibilities and find the\n            // minimum\n            else\n                dp[i][j] = 1\n                           + min(dp[i][j - 1], // Insert\n                                 dp[i - 1][j], // Remove\n                                 dp[i - 1]\n                                   [j - 1]); // Replace\n        }\n    }\n\n    return dp[m][n];\n}\n```\n\nAlso, I find this which use Levenstein distance with a given limit, but I wouldn't want to change to Levenstein  if it ins't necessary\n    ", "Answer": "\r\nI think this by Apache Commons is exactly what you are looking for which has a constructor with threshold for maximum distance.\nPseudocode:\n```\nLevenshteinDistance ld = new LevenshteinDistance(10);\nInteger editDistanceWithThreshold = ld.apply('String1', 'String2')\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Search Lucene with precise edit distances\r\n                \r\nI would like to search a Lucene index with edit distances. For example, say, there is a document with a field FIRST_NAME; I want all documents with first names that are 1 edit distance away from, say, 'john'. \n\nI know that Lucene supports fuzzy searches (FIRST_NAME:john~) and takes a number between 0 and 1 to control the fuzziness. The problem (for me) is this number does not directly translate to an edit distance. And when the values in the documents are short strings (less than 3 characters) the fuzzy search has difficulty finding them. For example if there is a document with FIRST_NAME 'J' and I search for FIRST_NAME:I~0.0 I don't get anything back. \n    ", "Answer": "\r\nIn Lucene's FuzzyQuery, you cannot specify the extact distance. You can specify the value of \"fuzziness\" between 0 and 1 where values closer to 0 indicate broad match and values closer to 1 indicate narrow match. The formula for \"fuzziness\" is as follows. (From Lucene in Action)\n\n\nFrom this formula, you can work back to an approximate fuzziness for given value of distance. So, ```\nStackOverflow```\n is to be matched with ```\nStackUnderflow```\n, which is at a distance of 3, the fuzziness required will be approximately 0.77.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance explanation\r\n                \r\nI have seen a lot of code to solve that but I am not able to see why they are using a matrix to represent the distance between two words. Can any one please explain to me?\n\nHere is a sample code I found: \n\n```\npublic static int minDistance(String word1, String word2)\n{\n    int l1 = word1.length(), l2 = word2.length();\n\n    int[][] d = new int[l1 + 1][l2 + 1];\n\n    // the edit distance between an empty string and the prefixes of\n    // word2\n    for (int i = 0; i < l2 + 1; i++) {\n        d[0][i] = i;\n    }\n\n    // the edit distance between an empty string and the prefixes of\n    // word1\n    for (int j = 0; j < l1 + 1; j++) {\n        d[j][0] = j;\n    }\n\n    for (int i = 1; i < l1 + 1; i++) {\n        for (int j = 1; j < l2 + 1; j++) {\n            if (word1.charAt(i - 1) == word2.charAt(j - 1)) {\n                d[i][j] = d[i - 1][j - 1];\n            } else {\n                d[i][j] = min(1 + d[i][j - 1], 1 + d[i - 1][j],\n                1 + d[i - 1][j - 1]); // min of insertion,\n                // deletion, replacement\n            }\n        }\n    }\n\n    return d[l1][l2];\n}\n```\n\n    ", "Answer": "\r\nYour code is calculating the Levenshtein distance using dynamic programming.  \n\nThe array ```\nd```\n will ultimately contain the solution to various sub-problems, where ```\nd[i][j]```\n is the distance between the first ```\ni```\n letters of the first word and the first ```\nj```\n letters of the second.  There is a relationship between ```\nd[i][j]```\n and the entries ```\nd[i-1][j]```\n, ```\nd[i][j-1]```\n and ```\nd[i-1][j-1]```\n.  The algorithm calculates the entries of the table in such a way that required sub-problems have already been calculated (this is the dynamic programming part).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "What does \"transposition\" mean in edit distance algorithm?\r\n                \r\nI am studying edit distance algorithm but cannot understand transpositions - what does it mean?\n\nhttp://www.nltk.org/_modules/nltk/metrics/distance.html\n\n```\ndef edit_distance(s1, s2, transpositions=False):\n```\n\n\nIt is not clearly explained in documentation.\n\nCan you give some simple example to help me understand?\n    ", "Answer": "\r\nConvert GAOL to GOAL:\nWithout transpositions you must do two edits: delete 'A', insert 'A' after 'O'.\nWith transpositions you must do one edit: transpose 'A' and 'O'\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Word-level edit distance between two sentences in R\r\n                \r\nI am looking for a fast solution in R for determining word-level edit distance between two sentences. More specifically, I want to determine minimal number of additions, substitutions or deletions of words, to transform sentence A to sentence B. For example, if sentence A is \"very nice car\" and sentence B is \"nice red car\", the result should be 2 (1 deletion and 1 addition).\n\nI know that there are existing solutions in R for character-level edit distance (e.g., native ```\nadist()```\n and ```\nstringdist()```\n from package 'stringdist'), but I found none for word-level.\n    ", "Answer": "\r\nHow about\n\n```\nintersect(strsplit(levels(factor(\"very nice car\"[1])),\" \")[[1]],strsplit(levels(factor(\"nice red car\"[1])),\" \")[[1]])```\n\n\n```\n> [1] \"nice\" \"car\"```\n\n\n```\nlength(intersect(strsplit(levels(factor(\"very nice car\"[1])),\" \")[[1]],strsplit(levels(factor(\"nice red car\"[1])),\" \")[[1]]))```\n\n\n```\n> [1] 2```\n\n\nOf course, you can make your own function that even works with a ```\nlist```\n:\n\n```\nmy_function <- function (x, prsep = \" \") \n{\n    if (isTRUE(length(x) != 0) == TRUE && isTRUE(is.na(x)) == \n        FALSE) {\n        if (isTRUE(is.list(x)) == TRUE) {\n            for (i in 1:length(x)) ifelse(isTRUE(length(x[[i]]) != \n                0) == TRUE, x[[i]] <- strsplit(x[[i]], prsep)[[1]], \n                NA)\n            return(x)\n        }\n        else if (isTRUE(is.list(x)) == FALSE) {\n            Lt <- list()\n            for (i in 1:length(x)) Lt[[length(Lt) + 1]] <- strsplit(levels(factor(x[i])), \n                prsep)[[1]]\n            return(Lt[[1]])\n        }\n    }\n    else {\n    x\n    }\n}\n```\n\n\nSo you just need\n\n```\nintersect(my_function(\"very nice car\",\" \"), my_function(\"nice red car\",\" \"))```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance between the words comparing between two Strings\r\n                \r\nI have seen many resources from the internet but couldn't found the exact help. i am trying to figure out the edit distance between the two strings example:\nString a = \"put return between paragraph gioo\";\nString b = \"put hello between line phone gio\";\nhere I am always comparing with String a with the other string so here the edit distance should be 4.\nI have done some code execution its comparing me with the each character in the string.\n\n```\n                           int len1 = row10.length();\n                            int len2 = row01.length();\n                            int[][] dp = new int[len1 + 1][len2 + 1];\n\n                            for (int i = 0; i <= len1; i++) {\n                                dp[i][0] = i;\n                            }\n\n                            for (int j = 0; j <= len2; j++) {\n                                dp[0][j] = j;\n                            }\n\n                            for (int i = 0; i < len1; i++) {\n                                char c1 = row10.charAt(i);\n                                for (int j = 0; j < len2; j++) {\n                                    char c2 = row01.charAt(j);\n                                    if (c1 == c2) {\n                                        dp[i + 1][j + 1] = dp[i][j];\n                                    } else {\n                                        int replace = dp[i][j] + 1;\n                                        int insert = dp[i][j + 1] + 1;\n                                        int delete = dp[i + 1][j] + 1;\n                                        int min = replace > insert ? insert : replace;\n                                        min = delete > min ? min : delete;\n                                        dp[i + 1][j + 1] = min;\n                                    }\n                                }\n                            }\n                            System.out.println(dp[len1][len2]);\n```\n\n    ", "Answer": "\r\nMade a sample function. It doesn't really take into the consideration of corner cases but it works. Also, do think about the case sensitivity of the words.\n\n```\npackage test;\n\npublic class CalcWordDiff {\n\n    public static void main(String[] args) {\n        // TODO Auto-generated method stub\n        String a = \"My name is ABC.\";\n        String b = \"My name xyz.\";\n        System.out.println(\"Edit distance will be : \"+calcDistanceBetweenWords(a,b));\n    }\n\n    public static int calcDistanceBetweenWords(String first, String second)\n    {\n        int res = 0;\n        String[] words_string_first = first.trim().split(\" \"); // By trim, I removed the Whitespaces if they exist\n        String[] words_string_second = second.trim().split(\" \");\n        //Check the length of both the arrays\n        System.out.println(\"Size of arrays first is : \"+words_string_first.length);\n        System.out.println(\"Size of arrays second is : \"+words_string_second.length);\n        int lowerWordSentSize = 0;\n        if(words_string_first.length<=words_string_second.length)\n        {\n            lowerWordSentSize = words_string_first.length;\n        }\n        else\n        {\n            lowerWordSentSize = words_string_second.length;\n        }\n        //Now iterate through the array of lower size\n        for(int i = 0; i< lowerWordSentSize; i++)\n        {\n            if(words_string_first[i].equals(words_string_second[i]))\n            {\n                //Do nothing, it means both the words are same\n            }\n            else\n            {\n                System.out.println(\"Words mismatched at \"+(i+1)+\" th Position.\");\n                res = i; \n            }\n        }\n        return res;\n    }\n\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "A plot showing the edit distance as a line between two cities. The thickness / opacity of the line can be adjusted based on the edit distance score\r\n                \r\nI have calculate the edit disctance between differents language, as shown below.\n\nNow i am trying to plot showing the edit distance as a line between two cities. The thickness / opacity of the line can be adjusted based on the edit distance score. e.g. source\n\nI have done alot of effort to design it in excel  tried map graph\n\nplease someone have idea of it guide me.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Using Damerau-Levenshtein Edit Distance to modify String\r\n                \r\nI have a problem with Damerau-Levenshtein Edit Distance (operations: delete, sostitution, insert, swap)\n\nI need to use Damerau-Levenshtein matrix to change concretely the first String with a list of used operations\n\nexample:\n\n```\nstr1: pair\nstr2: pari\n----\ndistance: 1\nSWAP [2] \"i\": pair -> pari\n```\n\n\nMy code work correctly with matrix generation\n\n```\n public int[][] matrixgen(String str1, String str2, int rows, int cols)\n                return LevenshteinDistanceMatrix\n```\n\n\nI have trouble with the second part, infact in some cases this algorithm dosen't \ncarries out the operations in the correct order\n\n```\npublic int editDistDP(String str1, String str2, int rows, int cols) {\n\n    SString sstr_1 = new SString(str1);\n    SString sstr_2 = new SString(str2);\n\n    int i = rows;\n    int j = cols;\n\n    int[][] dp = matrixgen(sstr_1.to_String(), sstr_2.to_String(), rows, cols);\n\n    while (i > 0 && j > 0) {\n\n        int min;\n        System.out.println(sstr_1.to_String() + \" -> \" + sstr_2.to_String());\n\n\n        if (sstr_1.equals_To(sstr_2)) break;\n\n        if (i > 1 && j > 1) {\n            min = min_v2(dp[i][j - 1], \n                    dp[i - 1][j],  \n                    dp[i - 1][j - 1], \n                    dp[i - 2][j - 2]); //minumum as 4\n        } else {\n            min = min(dp[i][j - 1], \n                    dp[i - 1][j],  \n                    dp[i - 1][j - 1]); //minimun as 3\n        }\n\n        if (min == dp[i][j]) {\n            i--;\n            j--;\n        } \n        else if (min == dp[i - 1][j]) {\n            System.out.print(\"DELETE [\" + i + \"]: \" + sstr_1.to_String().charAt(i - 1));\n            sstr_1.delete(i); //edit sstr_1 and delete in posix i\n            i--;\n        } \n        else if (min == dp[i][j - 1]) {\n\n            System.out.print(\"INSERT [\" + i + \"]: \" + sstr_2.to_String().charAt(j - 1));\n            sstr_1.insert(sstr_2.to_String().charAt(j - 1), i);  //edit sstr_1 and insert sstr_2.to_String().charAt(j - 1) in sstr_1 into posix i           \n            j--;\n        } \n        else if (min == dp[i - 1][j - 1]) {\n\n            System.out.print(\"REPLACE [\" + i + \"]: \" + sstr_2.to_String().charAt(j - 1));\n            sstr_1.replace(i, sstr_2.to_String().charAt(j - 1)); //edit sstr_1 and replace sstr_2.to_String().charAt(j - 1) in sstr_1's posix i       \n            i--;\n            j--;\n        } \n        else if (min == dp[i - 2][j - 2]) {\n            System.out.print(\"SWAP [\" + (i - 2) + \"]: \" + sstr_1.to_String().charAt(i - 2));\n            sstr_1.swap(i - 2); //edit sstr_1 and swap sstr_1's char in posix i-2 with sstr_1's char in posix (i-2)+1\n            i -= 1;\n            j -= 1;\n        }\n    }\n\n    System.out.println(sstr_1.to_String() + \" == \" + sstr_2.to_String());\n\n    return dp[rows][cols];\n}\n```\n\n\nIn cases as \"pair\" -> \"pari\", this algorithm works\n\nIn cases as \"volubile\" -> \"olubiel\", where distance is 2 and we have swap + delete this is the output:\n\n```\nSTART: volubile -> olubiel\nSWAP [6]: \"l\" >> volubiel | olubiel //correct\nvolubiel -> olubiel\nREPLACE [7]: \"e\" >> volubiee | olubiel //ERROR: it has to delete \"v\" not replace\nvolubiee -> olubiel\n```\n\n\nthen return 0\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Understanding edit distance by recursion\r\n                \r\nI am reading section \"8.2.1 Edit distance by recusion\" from Algorithm Design Manual book by Skiena. In this section I could not able to understand below two points.\n\n\nD[i,j-1]+1. This means that there is an extra character in the text to account for,so we do not advance the pattern pointer and pay the cost of an insertion.\nD[i-1,j]+1. This means that there is an extra character in the pattern to remove,so we do not advance the text pointer and pay the cost on a deletion.\n\n\nAbove two points mentioning about calculating insertion and deletion distance. I could not able to understand how this logic works. It seems that for every pair it is assuming insertion and deletion is needed. Am i right? Why 1 is added for every insertion and deletion? please explain how this logic works.\n\nBelow is the algorithm\n\n```\n\n\n    //we will pass two strings and lengths of those strings\n    int string_compare(char *s, char *t, int i, int j)\n    {\n      int k; /* counter */\n      int opt[3]; /* cost of the three options */\n      int lowest_cost; /* lowest cost */\n      if (i == 0) return(j * indel(’ ’));\n      if (j == 0) return(i * indel(’ ’));\n\n      opt[MATCH] = string_compare(s,t,i-1,j-1) + match(s[i],t[j]);\n\n    // *** I could not able to understand below two lines.   ***\n      opt[INSERT] = string_compare(s,t,i,j-1) + indel(t[j]); \n      opt[DELETE] = string_compare(s,t,i-1,j) + indel(s[i]);\n\n\n      return( min(opt[Match],opt[INSERT],opt[DELETE])); //min function will return min of all three values\n    }\n\n    int match(char c, char d)\n    {\n        if (c == d) return(0);\n        else return(MAXLEN);\n    }\n    int indel(char c)\n    {\n        return(1);\n    }\n\n```\n\n\nPlease explain with example.\n\nThis is not a duplicate question. I did research but i could not able to find anything.\n    ", "Answer": "\r\nThis algorithm took me a while to truly wrap my mind around.  Please be aware that I don't have that textbook in front of me, but I'll try to help with what I know.\nExplanation\nLet's say we're evaluating string1 and string2.  I'm going to elaborate on MATCH a little bit as well.\nYour statement, \"It seems that for every pair it is assuming insertion and deletion is needed\" just needs a little clarification.  The algorithm does not necessarily assume insertion and deletion are needed, it just checks all possibilities.  After it checks the results of recursive insert/delete/match calls, it returns the minimum of all 3 -- the best choice of the 3 possible ways to change string1 into string2.\n\"Why 1 is added for every insertion and deletion?\" - You are adding 1 for every change to the string.  So, each level of recursion that requires a change will mean \"add 1\" to the edit distance.  However, if the letters are the same, no change is required, and you add 0.  This is shown in ```\nmatch```\n.```\n*```\n  Each recursive call represents a single change to the string.\nA Goofy Example\nIf you look at the references at the bottom of this post, you can find some well worded, thoughtful explanations about how the algorithm works.  Sometimes that's not what you need.  This is kind of weird, but I occasionally find it helpful if I can personify the code.  With that in mind, I hope this helps.\nLet's take an example, ```\nstring_compare(\"he\", \"her\", 2, 3)```\n.  We are starting the 2nd and 3rd positions (the ends) of each string, respectively.  Here are some vocal expressions of what the function 'says' when it sends off the recursive calls the first time around:\n\nMATCH - I'll handle 'e' and 'r'.  I'll add 1 since they are different, and you just get me the result of 'h' and 'he'.\nINSERT - Let's find out how many changes we need if we insert a character into string1.  We'll of course insert a matching character, so we can really just skip the 'r' in string2.  You evaluate 'he' and 'he', and I'll add 1 to whatever you get.\nDELETE - Let's find out how many changes are necessary if we skip the 'e' in string1.  This is a change, so I'll add 1 to whatever you get when you compare 'h' and 'her'.\n\nThere are so many branches (this is exponential time complexity), that it is difficult to draw out every scenario.  However, you can see that the INSERT dialogue is comparing 'he' and 'he'.  In the following recursions, every possibility will be tested.  However, the MATCH will always be optimal because each character matches and adds 0.\nEach recursive call runs through that conversation.  Compare the current characters and recur, insert a character into string1 and recur, and delete a character from string1 and recur.  In each recursive level, the minimum of these 3 is the path with the least changes.  That will carry up the stack to give you your answer.\nI know it's an odd explanation, but I hope it helps.\nOther than the possible duplicate already provided, there's a pretty solid write up about this algorithm (with code) here.\n```\n*```\nThat being said, I'm honestly not sure why your ```\nmatch```\n function returns ```\nMAXLEN```\n.  I would expect it to return 1 as shown in the possible duplicate link from the comments.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to fit strings using spaces, minimizing edit distance?\r\n                \r\nI'm looking for an algorithm that fits two strings, filling them up with spaces if necessary to minimize edit distance between them:\n\n```\nfit('algorithm', 'lgrthm') == ' lg r thm'\n```\n\n\nThere sure must be some prewritten algorithm for this. Any ideas?\n    ", "Answer": "\r\nYou could do something like the following:\n\n```\ndef fit(target, source):\n    i, j = 0, 0\n    result = []\n    while i < len(source) and j < len(target):\n        if source[i] == target[j]:\n            result.append(source[i])\n            i += 1\n        else:\n            result.append(' ')\n        j += 1\n\n    return ''.join(result)\n\n\ntest = [('algorithm', 'lgrthm'), ('pineapple', 'pine'), ('pineapple', 'apple'), ('pineapple', 'eale'),\n        ('foo', 'fo'), ('stack', 'sak'), ('over', 'or'), ('flow', 'lw')]\n\nfor t, s in test:\n    print(t)\n    print(fit(t, s))\n    print('---')\n```\n\n\nOutput\n\n```\nalgorithm\n lg r thm\n---\npineapple\npine\n---\npineapple\n    apple\n---\npineapple\n   ea  le\n---\nfoo\nfo\n---\nstack\ns a k\n---\nover\no  r\n---\nflow\n l w\n---\n```\n\n\nA perhaps better version, is the following:\n\n```\nfrom collections import deque\n\n\ndef peak(q, default=' '):\n    \"\"\"Perform a safe peak, if the queue is empty return default\"\"\"\n    return q[0] if q else default\n\n\ndef fit(target, source):\n    ds = deque(source)\n    return ''.join([ds.popleft() if peak(ds) == e else ' ' for e in target])\n```\n\n\nIs better in the sense that you do not need to keep track of state variables ```\ni, j```\n like in the previous approach.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "input networkx graph into zss algorithm (tree edit distance)\r\n                \r\nI want to compute the Zhang-Shasha tree-edit distance between 2 trees (```\nzss```\n library). However, my trees are in the form of ```\nnetworkx```\n graphs (they actually represent DOM html trees). The example in the zss documentation shows how to create a tree by hand:\n\n```\nfrom zss import *\nA = (\n    Node(\"f\")\n        .addkid(Node(\"a\")\n            .addkid(Node(\"h\"))\n            .addkid(Node(\"c\")\n                .addkid(Node(\"l\"))))\n        .addkid(Node(\"e\"))\n    )\nzss.simple_distance(A, A) # [0.0]\n```\n\n\nWhich would be the same tree as:\n\n```\nimport networkx as nx\nG=nx.DiGraph()\nG.add_edges_from([('f', 'a'), ('a', 'h'), ('a', 'c'), ('c', 'l'), ('f', 'e')])\n```\n\n\nso I would like to convert tree objects of networkx class into a ```\nzss```\n Node object, then compute the edit distance between 2 trees.\n\nThanks\n\n(and do not hesitate to tell me if you think this is a XY problem)\n    ", "Answer": "\r\nUsing ```\ndfs_tree```\n can definitely help:\n\n```\nimport zss\nimport networkx as nx\n\nG=nx.DiGraph()\nG.add_edges_from([('f', 'a'), ('a', 'h'), ('a', 'c'), ('c', 'l'), ('f', 'e')])\nT = nx.dfs_tree(G, source='f')\nnodes_dict = {}\nfor edge in T.edges():\n    if edge[0] not in nodes_dict:\n        nodes_dict[edge[0]] = zss.Node(edge[0])\n    if edge[1] not in nodes_dict:\n        nodes_dict[edge[1]] = zss.Node(edge[1])\n    nodes_dict[edge[0]].addkid(nodes_dict[edge[1]])\n\nprint(zss.simple_distance(nodes_dict['f'], nodes_dict['f'])) # 0.0\n```\n\n\nIn case we don't know which node is G's root node, but know we have a valid tree, we can get the source node by calling:\n\n```\nsource = [n for (n, d) in G.in_degree() if d == 0][0]\nT = nx.dfs_tree(G, source=source)\n```\n\n\nSince the root is the only node with no incoming nodes, that should work.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Finding the edit distance of two strings with recursion\r\n                \r\nI need to use recursion to find the edit distance of two strings, i.e I give the function two arguments(each a different sring). And the function will find the least amount of changes required to change ```\ns1```\n into ```\ns2```\n. This is what I have so far:\n\n```\ndef edit_distance(s1,s2):\n    split1 = list(s1)\n    split2 = list(s2)\n    count = 0\n    pos = 0\n\n    if split1[pos] == split2[pos]:\n        pos += 1\n    else:\n        pos +=1\n        count += 1\n        edit_distance(s1, s2)\n\nreturn count #This should be the minimum amount required to match the two strings\n```\n\n    ", "Answer": "\r\nI annotated your code to show you the code flow. I hope you understand now why you get the error:\n\n```\ndef edit_distance(s1,s2):\n    split1 = list(s1)    # Split strings into characters\n    split2 = list(s2)\n    count = 0            # This variable is local, it is not shared through calls to the function!\n    pos = 0              # Same\n\n    if split1[pos] == split2[pos]:   # pos is always 0 here!\n        pos += 1                     # pos is incremented anyway, in if but also in else !\n    else:\n        pos +=1                      # See above\n        count += 1                   # count is incremented, now it is 1\n        edit_distance(s1, s2)        # recursive call, but with the identical arguments as before! The next function call will do exactly the same as this one, resulting in infinite recursion!\n\nreturn count    # Wrong indentation here\n```\n\n\nYour function does not do what you want. In case you are talking about Hamming distance, which is not really clear to me still, here is a sample implementation assuming the lengths of both strings are equal:\n\n```\n# Notice that pos is passed between calls and initially set to zero\ndef hamming(s1, s2, pos=0):    \n    # Are we after the last character already?\n    if pos < len(s1):\n        # Return one if the current position differs and add the result for the following positions (starting at pos+1) to that\n        return (s1[pos] != s2[pos]) + hamming(s1, s2, pos+1)\n    else:\n        # If the end is already reached, the remaining distance is 0\n        return 0\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Javascript find edit distance not returning correct value\r\n                \r\nI'm working on a function that computes the edit distance of two strings but according to this only calculator Im getting an incorrect value. Im getting 19 and the calculator is returning 7. Im not sure whats wrong with my program I based it off of The Algorithm Design Manual.\n\n```\nvar recFindEditDistance = function(P, T, i, j) {\n    if (i === undefined || j === undefined) return recFindEditDistance(P, T, P.length - 1, T.length - 1);\n    if (i === 0 && j === 0) return 0;\n    if (i === 0) return j;\n    if (j === 0) return i;\n\n    var sub = recFindEditDistance(P, T, i-1, j-1) + (P[i]===T[j] ? 0 : 1);\n    var del = recFindEditDistance(P, T, i, j-1) + 1;\n    var add = recFindEditDistance(P, T, i-1, j) + 1;\n\n    return Math.max(sub, add, del);\n};\n\nconsole.log(recFindEditDistance('ffadsfsadfasf', 'asdfasdf'));\n```\n\n    ", "Answer": "\r\nLevenshtein distance algorithm at the every position tries to achieve the minimum distance to get to the next position.\n\n\n\nAt the same time, you are trying to get the maximum one :)\n\nSimply change\n\n```\nreturn Math.max(sub, add, del);\n```\n\n\nto\n\n```\nreturn Math.min(sub, add, del);\n```\n\n\nAnd it will work :)\n\nDemo:\n\n\r\n\r\n```\nfunction recFindEditDistance(P, T, i, j) {\r\n    if (i === undefined || j === undefined) return recFindEditDistance(P, T, P.length - 1, T.length - 1);\r\n    if (i === 0 && j === 0) return 0;\r\n    if (i === 0) return j;\r\n    if (j === 0) return i;\r\n\r\n    var sub = recFindEditDistance(P, T, i-1, j-1) + (P[i]===T[j] ? 0 : 1);\r\n    var del = recFindEditDistance(P, T, i, j-1) + 1;\r\n    var add = recFindEditDistance(P, T, i-1, j) + 1;\r\n\r\n    return Math.min(sub, add, del);\r\n};\r\n\r\ndocument.body.innerText = recFindEditDistance('ffadsfsadfasf', 'asdfasdf');```\n\r\n\r\n\r\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "most efficient edit distance to identify misspellings in names?\r\n                \r\nAlgorithms for edit distance give a measure of the distance between two strings.\n\nQuestion: which of these measures would be most relevant to detect two different persons names which are actually the same? (different because of a mispelling). The trick is that it should minimize false positives. Example:\n\nObaama\nObama\n=> should probably be merged\n\nObama\nIbama\n=> should not be merged.\n\nThis is just an oversimple example. Are their programmers and computer scientists who worked out this issue in more detail?\n    ", "Answer": "\r\nI can suggest an information-retrieval technique of doing so, but it requires a large collection of documents in order to work properly.\n\nIndex your data, using the standard IR techniques. Lucene is a good open source library that can help you with it.\n\nOnce you get a name (Obaama for example): retrieve the set of collections the word Obaama appears in. Let this set be ```\nD1```\n.\nNow, for each word ```\nw```\n in D11 search for ```\nObaama AND w```\n (using your IR system). Let the set be ```\nD2```\n.\n\nThe score ```\n|D2|/|D1|```\n is an estimation how much ```\nw```\n is connected to ```\nObaama```\n, and most likely will be close to 1 for ```\nw=Obama```\n2.\nYou can manually label a set of examples and find the value from which words will be expected.\n\nUsing a standard lexicographical similarity technique you can chose to filter out words that are definetly not spelling mistakes (Like ```\nBarack```\n).\n\nAnother solution that is often used requires a query log - find a correlation between searched words, if obaama has correlation with obama in the query log - they are connected.\n\n\n\n1: You can improve performance by first doing the 2nd filter, and check only for candidates who are \"similar enough\" lexicographically.\n\n2: Usually a normalization is also used, because more frequent words are more likely to be in the same documents with any word, regardless of being related or not.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Can not understand the idea/usefulness of edit distance among strings\r\n                \r\nI am reading about the problem of Edit Distance between 2 strings.\nIt can be solved by Dynamic Programming using a formula of Edit Distance. What I can not understand is its usefulness. \nFirst of all how is this any different than knowing the longest common subsequense of 2 strings?\nIf the idea is to pick a string with the smallest edit distance you might as well use the max LCS among the strings.Right?\nAdditionally when we actually code to do the replacement, the code would be similar to the following:  \n\n```\nif(a.length == b.length){  \n   for(int i = 0;i < a.length;i++){  \n          a[i] = b[i];  \n   }  \n}  \nelse{   \n    a = new char[b.length];  \n    for(int i = 0;i < a.length;i++){  \n          a[i] = b[i];  \n    }    \n}  \n```\n\n\nI mean just replace the characters. Is there any difference between doing an assignment and checking if the characters are the same and if not, only then do the assignment at runtime? Aren't both constant time operations?\nWhat am I misunderstanding with this problem?\n    ", "Answer": "\r\nEdit Distance and LCS are related by a simple formula if no substitution is allowed in editing (or if substitution is twice as expensive as insertion or deletion):\n\ned(x,y) = x.length + y.length - 2*lcs(x,y).length\n\nIf substitution is a separate unit-cost operation, then ED can be less than that. This is important in practice since we want a way to create shorter diff files. Not just asymptotically bounded up to a constant factor, but actually smallest possible ones.\n\nEdit shorter diff files are probably not a concern here, they won't be substantially shorter if we do not allow substitution. There are more interesting applications, like ranking correction suggestions in a spell checker (this is based on a comment by @nhahtdh below).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Calculating Minimum Edit Distance for unequal strings python\r\n                \r\nI am trying to implement Minimum Edit Distance with the substitution cost of 2. Following is the code I've so far. It works well for strings of equal length but generates error for the unequal strings. Kindly correct me where i am wrong\n\n```\ndef med(source, target):\n#     if len(x) > len(y):\n#         print(\"insode if\")\n#         source, target = y, x\nprint(len(source), len(target))\ncost = [[0 for inner in range(len(source)+1)] for outer in \nrange(len(target)+1)]\n\nglobal backtrace \nbacktrace = [[0 for inner in range(len(source)+1)] for outer in \nrange(len(target)+1)]\nglobal SUB\nglobal INS\nglobal DEL\n\nfor i in range(0,len(target)+1):\n    cost[i][0] = i\n\nfor j in range(0,len(source)+1):\n    cost[0][j] = j\n\nfor i in range(1,len(target)+1):\n    for j in range(1,len(source)+1):\n        if source[i-1]==target[j-1]:\n            cost[i][j] = cost[i-1][j-1] \n        else:\n            deletion = cost[i-1][j]+1\n            insertion = cost[i][j-1]+1\n            substitution = cost[i-1][j-1]+2\n            cost[i][j] = min(insertion,deletion,substitution)\n\n            if cost[i][j] == substitution:\n                backtrace[i][j] = SUB\n            elif cost[i][j] == insertion:\n                backtrace[i][j] = INS\n            else:\n                backtrace[i][j] = DEL\n\n\nreturn cost[i][j]\n\nmed(\"levenshtein\",\"levels\")\n```\n\n\nThe error i get is:\n\n```\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-26-86bf20ea27c7> in <module>()\n 49     return cost[i][j]\n 50 \n---> 51 med(\"levenshtein\",\"levels\")\n\n<ipython-input-26-86bf20ea27c7> in med(source, target)\n 31     for i in range(1,len(target)+1):\n 32         for j in range(1,len(source)+1):\n---> 33             if source[i-1]==target[j-1]:\n 34                 cost[i][j] = cost[i-1][j-1]\n 35             else:\n\nIndexError: string index out of range\n```\n\n    ", "Answer": "\r\nFor different length strings, ```\ncost```\n and ```\nbacktrace```\n indices doesn't match.\nCan be implemented minimum edit distance with 2 substitution cost by updating only one numpy ```\nm```\n * ```\nn```\n arr with cost at each step.\n\nAs per Algorithm,\nBelow code will do the job.\n```\ndef minimumEditDistance(first, second): \n    \n    #Creating numpy ndarray( initialized with 0 of dimension of size of both strings\n    \n    matrix = np.zeros((len(first)+1,len(second)+1), dtype=np.int)\n    \n    \n    # Cross relation loop through each character of each string with each other and\n    # fill the respective index of matrxi (row,column)\n    \n    for i in range(len(first)+1): \n        for j in range(len(second)+1): \n            \n            #First doing the boundary value analysis, if first or second string is empty so directly adding insertion cost\n            if i == 0:  \n                matrix[i][j] = j  \n            #Second case\n            elif j == 0: \n                matrix[i][j] = i\n            else: \n                matrix[i][j] = min(matrix[i][j-1] + 1,  \n                                   matrix[i-1][j] + 1,        \n                                   matrix[i-1][j-1] + 2 if first[i-1] != second[j-1] else matrix[i-1][j-1] + 0)     \n                                   # Adjusted the cost accordinly, insertion = 1, deletion=1 and substitution=2\n    return matrix[len(first)][len(second)]  # Returning the final\n```\n\nOutput:\n```\n>>>print(minimumEditDistance('levenshtein','levels'))\n7\n>>>print(minimumEditDistance('levenshtein','levenshtein'))\n0\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Best method for Edit Distance and LCS algorithms\r\n                \r\nI'm writing a monografie for college about optimizations in Edit Distance and LCS algorithms. \n\nSearching the internet I found two optimizations The four Russians method and Ukkonen Algorithm.\n\nI have to test it with this SPOJ problem (which requires a better Big O than O(NM) or O(N^2)). My question is simple:\n\nWhich is better in terms of performance?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance library in Java supporting user cost functions\r\n                \r\nThere are quite a few edit distance implementations for Java. However, they mostly implement specifically Levenshtein distance, i.e. one where cost of any replacement is 2, any insertion: 1, and any deletion: 1. Is there an existing library where I can supply a different cost for edits, so that e.g. replacing \"m\" by \"n\" costs less than replacing \"m\" by \"a\"?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "ElasticSearch - score match query by edit distance\r\n                \r\nI'm diving in ElasticSearch, and my supertask is to provide fuzzy search for exact single term with ranking by edit distance between searched term and found term, ignoring tf/idf and other scoring models, basically like this (it hasn't to be exactly like this, e.g. it's not necessary to treat transposition as single edit):\n\n```\nsearch: mark\n\noutput:\n  - mark: 1.0\n  - marx: 0.5\n  - marks: 0.5\n  - marker: 0.25\n  ...\n```\n\n\nIs this possible with ElasticSearch and without tricky scoring scripts? I've tried to do it out-of-the-box, but i've suddenly got ```\nmark-a```\n ranked higher than exact match ```\nmark```\n (and my analyzer did not split ```\nmark-a```\n to tokens ```\nmark```\n and ```\na```\n).\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "javascript text similarity algorithm in percentage based on edit distance [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI already know many edit-distance algorithm implementations in javascript, but I want to calculate the text similarity in percentage based on it. Does anyone know how to implement it?\n    ", "Answer": "\r\nYou have to find the maximum possible distance between a string of length ```\nn```\n and a string of length ```\nm```\n. If for example this maximum distance is ```\nn + m```\n then the percentage will be\n\n```\n100 - 100 * edit_distance(a, b) / (a.length + b.length)\n```\n\n\nIf for example you use Levenshtein distance where each insert, replacement, deletion has cost ```\n1```\n then this maximum possible distance is ```\nmax(n, m)```\n and so the percentage will be\n\n```\n100 - 100 * Levenshtein(a, b) / Math.max(a.length, b.length)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to calculate Edit distance between two string of integers in java?\r\n                \r\nIs there any library to calculate the edit distance between two integer strings in java?\n    ", "Answer": "\r\nIn Apache Commons Text, there is the apache.commons.text.similarity namespace.  There are multiple definitions of \"distance\", but the Levenshtein Distance is a good place to start.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Disambiguation of Names using Edit Distance\r\n                \r\nI have a huge list of company names and a huge list of zipcodes associated with those names. (>100,000). \n\nI have to output similar names (for example, AJAX INC and AJAX are the same company, I have chosen a threshold of 4 characters for edit distance), but only if their corresponding zipcodes match too. \n\nThe trouble is that I can put all these company names in a dictionary, and associate a list of zipcode and other characteristics with that dictionary key. However, then I have to match each pair, and with O(n^2), it takes forever. Is there a faster way to do it?\n    ", "Answer": "\r\nCreate a dictionary keyed by zipcode, with lists of company names as the values. Now you only have to match company names per zipcode, a much smaller search space.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "BigQuery Query with Edit Distance UDF Throws Error\r\n                \r\nI am trying to use compare the content among articles using edit distance algorithm. I create a UDF in the Standard Query. It works fine when number of articles are small (such as 10 articles, total comparison is be 10X10 = 100). \n\nIf the total number of articles is big, like 100 (total comparison is 100 x 100 =  10000). BigQuery UI throws the following error: \n\nError: An internal error occurred and the request could not be completed.\nJob ID: newspapers-142716:US.bquijob_cfbb316_161c359b4ec\n\nI wonder if it is because BigQuery can't handle that much amount of data when calling the UDF. The Edit Distance algorithm is not \"cheap\" operation, could that the reason too?\n\nIs using the UDF for this purpose the right approach? Any other alternatives? \n\nBelow is my UDF code:\n\n\r\n\r\n```\nCREATE TEMPORARY FUNCTION\r\n  editDistance(a STRING, b STRING)\r\n  RETURNS FLOAT64\r\n  LANGUAGE js \r\n  AS \r\n\"\"\"\r\n  if(a.length == 0) return b.length; \r\n  if(b.length == 0) return a.length; \r\n\r\n  var matrix = [];\r\n\r\n  // increment along the first column of each row\r\n  var i;\r\n  for(i = 0; i <= b.length; i++){\r\n    matrix[i] = [i];\r\n  }\r\n\r\n  // increment each column in the first row\r\n  var j;\r\n  for(j = 0; j <= a.length; j++){\r\n    matrix[0][j] = j;\r\n  }\r\n\r\n  // Fill in the rest of the matrix\r\n  for(i = 1; i <= b.length; i++){\r\n    for(j = 1; j <= a.length; j++){\r\n      if(b.charAt(i-1) == a.charAt(j-1)){\r\n        matrix[i][j] = matrix[i-1][j-1];\r\n      } else {\r\n        matrix[i][j] = Math.min(matrix[i-1][j-1] + 1, // substitution\r\n                                Math.min(matrix[i][j-1] + 1, // insertion\r\n                                         matrix[i-1][j] + 1)); // deletion\r\n      }\r\n    }\r\n  }\r\n\r\n  distance = matrix[b.length][a.length]\r\n  \r\n  return distance/Math.max(a.length, b.length)\r\n  \r\n  //return matrix[b.length][a.length];\r\n    \r\n\"\"\"\r\n;```\n\r\n\r\n\r\n\n    ", "Answer": "\r\n\n  I wonder if it is because BigQuery can't handle that much amount of\n  data when calling the UDF.\n\n\nYou are right - the actual error for your job was\n\n\n  Allocation failed - JavaScript heap out of memory\n\n\nI suppose every article is quite long, probably few KBs, so ```\nmatrix```\n that you build inside function is several MBs.\nThis type of ```\nCROSS JOIN```\n processing is probably better suited for DataFlow or some other batch tool. To do it in BigQuery, you will need to break the table into smaller pieces, or to run each article against every other article as a separate query (but this will result in multiple scans, and can be costly).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "edit distance algorithm in Haskell - performance tuning\r\n                \r\nI'm trying to implement the levenshtein distance (or edit distance) in Haskell, but its performance decreases rapidly when the string lenght increases.\n\nI'm still quite new to Haskell, so it would be nice if you could give me some advice on how I could improve the algorithm. I already tried to \"precompute\" values (the inits), but since it didn't change anything I reverted that change.\n\nI know there's already an editDistance implementation on Hackage, but I need it to work on lists of arbitrary tokens, not necessarily strings. Also, I find it a bit complicated, at least compared to my version.\n\nSo, here's the code:\n\n\n-- standard levenshtein distance between two lists\neditDistance      :: Eq a => [a] -> [a] -> Int\neditDistance s1 s2 = editDistance' 1 1 1 s1 s2 \n\n-- weighted levenshtein distance\n-- ins, sub and del are the costs for the various operations\neditDistance'      :: Eq a => Int -> Int -> Int -> [a] -> [a] -> Int\neditDistance' _ _ ins s1 [] = ins * length s1 \neditDistance' _ _ ins [] s2 = ins * length s2 \neditDistance' del sub ins s1 s2  \n    | last s1 == last s2 = editDistance' del sub ins (init s1) (init s2)\n    | otherwise          = minimum [ editDistance' del sub ins s1 (init s2)        + del -- deletion \n                                   , editDistance' del sub ins (init s1) (init s2) + sub -- substitution\n                                   , editDistance' del sub ins (init s1) s2        + ins -- insertion\n                                   ]\n\n\nIt seems to be a correct implementation, at least it gives exactly the same results as this online tool. \n\nThanks in advance for your help! If you need any additional information, please let me know.\n\nGreetings,\nbzn\n    ", "Answer": "\r\nIgnoring that this is a bad algorithm (should be memoizing, I get to that second)...\n\nUse O(1) Primitives and not O(n)\n\nOne problem is you use a whole bunch calls that are O(n) for lists (haskell lists are singly linked lists).  A better data structure would give you O(1) operations, I used Vector:\n\n```\nimport qualified Data.Vector as V\n\n-- standard levenshtein distance between two lists\neditDistance      :: Eq a => [a] -> [a] -> Int\neditDistance s1 s2 = editDistance' 1 1 1 (V.fromList s1) (V.fromList s2)\n\n-- weighted levenshtein distance\n-- ins, sub and del are the costs for the various operations\neditDistance'      :: Eq a => Int -> Int -> Int -> V.Vector a -> V.Vector a -> Int\neditDistance' del sub ins s1 s2\n  | V.null s2 = ins * V.length s1\n  | V.null s1 = ins * V.length s2\n  | V.last s1 == V.last s2 = editDistance' del sub ins (V.init s1) (V.init s2)\n  | otherwise            = minimum [ editDistance' del sub ins s1 (V.init s2)        + del -- deletion \n                                   , editDistance' del sub ins (V.init s1) (V.init s2) + sub -- substitution\n                                   , editDistance' del sub ins (V.init s1) s2        + ins -- insertion\n                                   ]\n```\n\n\nThe operations that are O(n) for lists include init, length, and last (though init is able to be lazy at least).  All these operations are O(1) using Vector.\n\nWhile real benchmarking should use Criterion, a quick and dirty benchmark:\n\n```\nstr2 = replicate 15 'a' ++ replicate 25 'b'\nstr1 = replicate 20 'a' ++ replicate 20 'b'\nmain = print $ editDistance str1 str2\n```\n\n\nshows the vector version takes 0.09 seconds while strings take 1.6 seconds, so we saved about an order of magnitude without even looking at your ```\neditDistance```\n algorithm.\n\nNow what about memoizing results?\n\nThe bigger issue is obviously the need for memoization.  I took this as an opportunity to learn the monad-memo package - my god is that awesome!  For one extra constraint (you need ```\nOrd a```\n), you get a memoization basically for no effort.  The code:\n\n```\nimport qualified Data.Vector as V\nimport Control.Monad.Memo\n\n-- standard levenshtein distance between two lists\neditDistance      :: (Eq a, Ord a) => [a] -> [a] -> Int\neditDistance s1 s2 = startEvalMemo $ editDistance' (1, 1, 1, (V.fromList s1), (V.fromList s2))\n\n-- weighted levenshtein distance\n-- ins, sub and del are the costs for the various operations\neditDistance' :: (MonadMemo (Int, Int, Int, V.Vector a, V.Vector a) Int m, Eq a) => (Int, Int, Int, V.Vector a, V.Vector a) -> m Int\neditDistance' (del, sub, ins, s1, s2)\n  | V.null s2 = return $ ins * V.length s1\n  | V.null s1 = return $ ins * V.length s2\n  | V.last s1 == V.last s2 = memo editDistance' (del, sub, ins, (V.init s1), (V.init s2))\n  | otherwise = do\n        r1 <- memo editDistance' (del, sub, ins, s1, (V.init s2))\n        r2 <- memo editDistance' (del, sub, ins, (V.init s1), (V.init s2))\n        r3 <- memo editDistance' (del, sub, ins, (V.init s1), s2)\n        return $ minimum [ r1 + del -- deletion \n                         , r2 + sub -- substitution\n                         , r3 + ins -- insertion\n                                   ]\n```\n\n\nYou see how the memoization needs a single \"key\" (see the MonadMemo class)?  I packaged all the arguments as a big ugly tuple.  It also needs one \"value\", which is your resulting ```\nInt```\n.  Then it's just plug and play using the \"memo\" function for the values you want to memoize.\n\nFor benchmarking I used a shorter, but larger-distance, string:\n\n```\n$ time ./so  # the memoized vector version\n12\n\nreal    0m0.003s\n\n$ time ./so3  # the non-memoized vector version\n12\n\nreal    1m33.122s\n```\n\n\nDon't even think about running the non-memoized string version, I figure it would take around 15 minutes at a minimum.  As for me, I now love monad-memo - thanks for the package Eduard!\n\nEDIT: The difference between ```\nString```\n and ```\nVector```\n isn't as much in the memoized version, but still grows to a factor of 2 when the distance gets to around 200, so still worth while.\n\nEDIT: Perhaps I should explain why the bigger issue is \"obviously\" memoizing results.  Well, if you look at the heart of the original algorithm:\n\n```\n [ editDistance' ... s1          (V.init s2)  + del \n , editDistance' ... (V.init s1) (V.init s2) + sub\n , editDistance' ... (V.init s1) s2          + ins]\n```\n\n\nIt's quite clear a call of ```\neditDistance' s1 s2```\n results in 3 calls to ```\neditDistance'```\n... each of which call ```\neditDistance'```\n three more times... and three more time... and AHHH!  Exponential explosion!  Luckly most the calls are identical!  for example (using ```\n-->```\n for \"calls\" and ```\neD```\n for ```\neditDistance'```\n):\n\n```\neD s1 s2  --> eD s1 (init s2)             -- The parent\n            , eD (init s1) s2\n            , eD (init s1) (init s2)\neD (init s1) s2 --> eD (init s1) (init s2)         -- The first \"child\"\n                  , eD (init (init s1)) s2\n                  , eD (init (init s1)) (init s2) \neD s1 (init s2) --> eD s1 (init (init s2))\n                  , eD (init s1) (init s2)\n                  , eD (init s1) (init (init s2))\n```\n\n\nJust by considering the parent and two immediate children we can see the call ```\ned (init s1) (init s2)```\n is done three times.  The other child share calls with the parent too and all children share many calls with each other (and their children, cue Monty Python skit).\n\nIt would be a fun, perhaps instructive, exercise to make a ```\nrunMemo```\n like function that returns the number of cached results used.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Efficient edit distance\r\n                \r\nI have a big corpus and I'm trying to find the most similar n-grams in the corpus. For that case, I'm using ```\nget_close matches```\n.\n\nThe problem is that this procedure takes a lot of time. A friend suggests me to convert the n-grams to MD5 and then calculate the distance. I suspect that it will work. Is hashing invariant to hashing? Is distance calculation efficiently running on MD5 that strings?\n\nPost scriptum, what is the most efficient way to calculate the distance between strings (like n-grams) in a large corpus?\n    ", "Answer": "\r\nA promising approach would be metric embedding. In this paper: Convolutional Embedding for Edit Distance the researchers state that the algorithm can accelerate the searching by orders of magnitude. After doing the training metric embedding you can apply the approximate nearest neighbor algorithms to find the k text with the shortest distance.\nHTH.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "anagram string edit distance algorithm/code?\r\n                \r\nThere are two anagram strings S and P. There are two basic operations:\n\n\nSwap two letters that are in neighborhood, e.g, swap \"A\" and \"C\" in BCCAB, cost is 1.\nSwap the first letter and the last letter in the string, cost is 1.\n\n\nQuestion: Design an efficient algorithm that minimize the cost to change S to P.\n\nI tried a greedy algorithm, but I found counter examples and I think it is incorrect. I know famous DP problem edit distance, but I did not get the formula for this one.\n\nAnyone can help? An idea and pseudo code would be great.  \n    ", "Answer": "\r\nI wonder if http://en.wikipedia.org/wiki/A*_search_algorithm would count as efficient? For a heuristic, look for the smallest distance each character has to go, treating the string as a circle, and divide the sum of these distances by two. On the circle, each character needs to participate in enough swaps to move it, one step at a time, to its destination, and each swap affects only two characters, so this heuristic should be a lower bound to the number of swaps required.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "String similarity with Python + Sqlite (Levenshtein distance / edit distance)\r\n                \r\nIs there a string similarity measure available in Python+Sqlite, for example with the ```\nsqlite3```\n module? \n\nExample of use case: \n\n```\nimport sqlite3\nconn = sqlite3.connect(':memory:')\nc = conn.cursor()\nc.execute('CREATE TABLE mytable (id integer, description text)')\nc.execute('INSERT INTO mytable VALUES (1, \"hello world, guys\")')\nc.execute('INSERT INTO mytable VALUES (2, \"hello there everybody\")')\n```\n\n\nThis query should match the row with ID 1, but not the row with ID 2:\n\n```\nc.execute('SELECT * FROM mytable WHERE dist(description, \"He lo wrold gyus\") < 6')\n```\n\n\nHow to do this in Sqlite+Python?\n\nNotes about what I've found so far:\n\n\nThe Levenshtein distance, i.e. the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other, can be useful, but I'm not sure if an official implementation exists in Sqlite (I've seen a few custom implementations, like this one)\nThe Damerau-Levenshtein is the same, except it also allows transposition between 2 adjacent characters; it is also called the Edit distance\nI know it's possible to define a function myself, but implementing such a distance will be non-trivial (doing natural language processing comparison super efficiently for databases is really non-trivial), that's why I wanted to see if Python / Sqlite already features such a tool\nSqlite has FTS (Full Text Seach) features: FTS3, FTS4, FTS5\n\n```\nCREATE VIRTUAL TABLE enrondata1 USING fts3(content TEXT);     /* FTS3 table */\nCREATE TABLE enrondata2(content TEXT);                        /* Ordinary table */\nSELECT count(*) FROM enrondata1 WHERE content MATCH 'linux';  /* 0.03 seconds */\nSELECT count(*) FROM enrondata2 WHERE content LIKE '%linux%'; /* 22.5 seconds */\n```\n\n\nbut I don't find about string comparison with such a \"similarity distance\", FTS's features ```\nMATCH```\n or ```\nNEAR```\n don't seem to have similarity measure with letters changes, etc.\nMoreover this answer shows that:\n\n\n  SQLite's FTS engine is based on tokens - keywords that the search engine tries to match.\n  A variety of tokenizers are available, but they are relatively simple. The \"simple\" tokenizer simply splits up each word and lowercases it: for example, in the string \"The quick brown fox jumps over the lazy dog\", the word \"jumps\" would match, but not \"jump\". The \"porter\" tokenizer is a bit more advanced, stripping the conjugations of words, so that \"jumps\" and \"jumping\" would match, but a typo like \"jmups\" would not.\n\n\nThe latter (the fact that \"jmups\" cannot be found as similar to \"jumps\") makes it unpractical for my use case, sadly.\n\n    ", "Answer": "\r\nHere is a ready-to-use example ```\ntest.py```\n:\n\n```\nimport sqlite3\ndb = sqlite3.connect(':memory:')\ndb.enable_load_extension(True)\ndb.load_extension('./spellfix')                 # for Linux\n#db.load_extension('./spellfix.dll')            # <-- UNCOMMENT HERE FOR WINDOWS\ndb.enable_load_extension(False)\nc = db.cursor()\nc.execute('CREATE TABLE mytable (id integer, description text)')\nc.execute('INSERT INTO mytable VALUES (1, \"hello world, guys\")')\nc.execute('INSERT INTO mytable VALUES (2, \"hello there everybody\")')\nc.execute('SELECT * FROM mytable WHERE editdist3(description, \"hel o wrold guy\") < 600')\nprint c.fetchall()\n# Output: [(1, u'hello world, guys')]\n```\n\n\nImportant note: The distance editdist3 is normalized so that\n\n\n  the value of 100 is used for insertion and deletion and 150 is used for substitution\n\n\n\n\nHere is what to do first on Windows:\n\n\nDownload https://sqlite.org/2016/sqlite-src-3110100.zip, https://sqlite.org/2016/sqlite-amalgamation-3110100.zip and unzip them\nReplace ```\nC:\\Python27\\DLLs\\sqlite3.dll```\n by the new sqlite3.dll from here. If skipping this, you'd get a ```\nsqlite3.OperationalError: The specified procedure could not be found```\n later\nRun:\n\n```\ncall \"C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\vcvarsall.bat\"  \n```\n\n\nor\n\n```\ncall \"C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\vcvarsall.bat\" x64\ncl /I sqlite-amalgamation-3110100/ sqlite-src-3110100/ext/misc/spellfix.c /link /DLL /OUT:spellfix.dll\npython test.py\n```\n\n\n(With MinGW, it would be: ```\ngcc -g -shared spellfix.c -I ~/sqlite-amalgation-3230100/ -o spellfix.dll```\n)\n\n\nHere is how to do it on Linux Debian:\n\n(based on this answer)\n\n```\napt-get -y install unzip build-essential libsqlite3-dev\nwget https://sqlite.org/2016/sqlite-src-3110100.zip\nunzip sqlite-src-3110100.zip\ngcc -shared -fPIC -Wall -Isqlite-src-3110100 sqlite-src-3110100/ext/misc/spellfix.c -o spellfix.so\npython test.py\n```\n\n\nHere is how to do it on Linux Debian with an older Python version:\n\nIf your distribution's Python is a bit old, it will require another method. As ```\nsqlite3```\n module is built-in in Python, it seems not straightforward to upgrade it (```\npip install --upgrade pysqlite```\n would only upgrade the pysqlite module, not the underlying SQLite library). Thus this method works for example if ```\nimport sqlite3; print sqlite3.sqlite_version```\n is 3.8.2:\n\n```\nwget https://www.sqlite.org/src/tarball/27392118/SQLite-27392118.tar.gz\ntar xvfz SQLite-27392118.tar.gz\ncd SQLite-27392118 ; sh configure ; make sqlite3.c ; cd ..\ngcc -g -fPIC -shared SQLite-27392118/ext/misc/spellfix.c -I SQLite-27392118/src/ -o spellfix.so\npython test.py   # [(1, u'hello world, guys')]\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Max edit distance and suggestion based on word frequency\r\n                \r\nI need a spell checker with the following specification:  \n\n\nVery scalable.\nTo be able to set a maximum edit distance for the suggested words.\nTo get suggestion based on provided words frequencies (most common word first).\n\n\n\n\nI took a look at Hunspell:\nI found the parameter MAXDIFF in the man but doesn't seem to work as expected. Maybe I'm using it the wrong way\n\nfile t.aff:  \n\n```\nMAXDIFF 1 \n```\n\n\nfile dico.dic:  \n\n```\n5  \nrouge  \nvert  \nbleu  \nbleue  \norange  \n```\n\n\n-  \n\n```\nNHunspell.Hunspell h = new NHunspell.Hunspell(\"t.aff\", \"dico.dic\");\nList<string> s = h.Suggest(\"bleuue\");\n```\n\n\nreturns the same thing ```\nt.aff```\n being empty or not:\n\n```\nbleue\nbleu\n```\n\n    ", "Answer": "\r\nWe decided to use ```\nApache Solr```\n, which exactly fulfills our needs.\nhttp://wiki.apache.org/solr/SpellCheckComponent#spellcheck\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Custom replacement matrix for edit distance in R\r\n                \r\nI need to compute the edit distance between two strings based on a custom cost function for replacements. For example, I want to specify different cost for replacing 'a' with 'b' than replacing 'a' with 'c'\n\nIs there an R package that allows me to pass a custom cost matrix as an argument? If not, I will have to modify a package for this purpose, then which package do you think is good for implementing this kind of extension?\n\nThanks.\n    ", "Answer": "\r\nI came to the conclusion that there is no such package which is capable of doing what I needed. Therefore, I had to come up with my own solution to this problem by writing an R package (which is not indeed generic -easy to adapt for other applications) capable of doing what I need.\n\nhere it is for the curious -> https://github.com/aanilpala/r-trajectory-editdistance\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Proving the edit distance between graphs with no edges is a metric\r\n                \r\nThe problem is finding the minimum edit distance between two graphs with no edges, considering there might be different costs for adding, removing or replacing vertices. \n\nI was told this distance is a metric, and there is a easy way to prove it. Is it so? How it can be done?\n    ", "Answer": "\r\nI will call the distance between two graphs G and H, d(G, H).\n\n\nFor any graph G, d(G, G) = 0. As long as all the costs are strictly positive, then for any G != H, d(G, H) > 0. So non-negativity is satisfied.\nAs long as the cost of removing a vertex is the same as the cost of adding a vertex, for any two graphs G, H, d(G, H) = d(H, G). So symmetry is satisfied.\nFinally, for any three graphs G, H, K, d(G, K) is at most d(G, H) + d(H, K) because one could edit G to K by first editing it to H.\n\n\nThe above criteria define a metric.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance, with a twist\r\n                \r\nI'm trying to solve something using dynamic programming, but I'm having some trouble. When I work on dynamic programming, I usually determine a recursive algorithm then go from there to my dynamic solution. This time I'm having trouble\n\nThe Problem\n\nSay you have two strings: m and n, such that n.length is greater than m.length, and n does not contain the character '#'. You want the string that turns m into the same length as string n in minimum cost.\n\nCost is defined as SUM(Penalty(m[i],n[i])), where i is in an index of the strings char array. \n\nPenalty is defined as such\n\n```\nprivate static int penalty(char x,char y) {\n    if (x==y) { return 0;}\n    else if (y=='#') { return 4;}\n    else { return 2;}\n}\n```\n\n\nThe only way I can think of is as follows:\n\n[0] If m and n are the same length, return m\n\n[1] Compute cost of inserting a # at any index of m\n\n[2] determine the string that has the minimum of such cost. Let that string be m'\n\n[3] Run the algorithm on m' and n again. \n\nI don't think this is even the optimal recursive algorithm, leading me to believe that I'm not on the right track for a dynamic algorithm. \n\nI've read up on using a m.length x n.length matrix for normal edit distance, but I don't see how I could easily transform that to fit my algorithm. \n\nThoughts on my recursive algorithm and the steps I need to take to reach a dynamic solution? \n    ", "Answer": "\r\nTaking your definitions (python):\n\n```\ndef penalty(x, y):\n    if x == y:\n        return 0\n    if y == '#':\n        return 4\n    return 2\n\ndef cost(n, m):\n    return sum(penalty(a, b) for a, b in zip(n, m))\n```\n\n\nThen you can define the distance reassigning to ```\nm```\n the lowest cost for each ```\n#```\n to be included.\n\n```\ndef distance(n, m):\n    for _ in range(len(n) - len(m)):\n        m = min((m[:i]+'#'+m[i:] for i in range(len(m)+1)), key=lambda s: cost(n, s))\n    return m\n\n>>> distance('hello world', 'heloworld')\n'he#lo#world'\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Finding words in long string within edit distance ignoring whitespace\r\n                \r\nI am looking for a algorithm to efficiently search for words within given edit distance in a query string while ignoring whitespace.\n\nFor e.g. If words on which I need to build an index are:\n\n```\nOHIO, WELL\n```\n\n\nand query String:\n\n```\nHELLO HI THERE H E L L O WORLD WE LC OME\n```\n\n\nFor edit distance 1, I need output:\n\n```\nHELL, O HI T, H E L L, WE LC\n```\n\n\nFor ignoring whitespace part, perhaps we can remove all spaces, but I can't find any algorithm that search text fuzzily in a string without spaces.\n\nI have done lot of research without any success. Please let me know if the question is unclear or need more information.\n    ", "Answer": "\r\n```\npublic static void main(String[] args) {\n    System.out.println(getMatches(List.of(\"OHIO\", \"WELL\"), \"HELLO HI THERE H E L L O WORLD WE LC OME\", 1));\n}\n\nprivate static List<String> getMatches(List<String> words, String query, int editDistance) {\n    return words.stream()\n            .flatMap(w -> getMatches(w, query, editDistance).stream().map(String::trim))\n            .distinct()\n            .collect(Collectors.toList());\n}\n\nprivate static List<String> getMatches(String word, String query, int editDistance) {\n    List<String> matches = new ArrayList<>();\n    for (int i = 0; i < query.length(); i++) {\n        StringBuilder candidate = new StringBuilder();\n        StringBuilder candidateWithoutSpaces = new StringBuilder();\n        populateCandidates(word, query, i, candidate, candidateWithoutSpaces);\n        if (isMatch(candidateWithoutSpaces, word, editDistance)) matches.add(candidate.toString());\n    }\n    return matches;\n}\n\nprivate static boolean isMatch(StringBuilder candidateWithoutSpaces, String word, int editDistance) {\n    if (candidateWithoutSpaces.length() != word.length()) return false;\n    for (int i = 0; i < candidateWithoutSpaces.length(); i++) {\n        if (candidateWithoutSpaces.charAt(i) != word.charAt(i) && --editDistance < 0) return false;\n    }\n    return true;\n}\n\nprivate static void populateCandidates(String word, String query, int i, StringBuilder candidate, StringBuilder candidateWithoutSpaces) {\n    int j = 0;\n    while (candidateWithoutSpaces.length() < word.length() && i + j < query.length()) {\n        char c = query.charAt(i + j);\n        candidate.append(c);\n        if (c != ' ') candidateWithoutSpaces.append(c);\n        j++;\n    }\n}\n```\n\n\nOutput\n\n```\n[O HI T, HELL, H E L L, WE LC]\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How can I sort a string list based on edit distance efficiently?\r\n                \r\nI'm trying to sort a list by edit distance using the levenshtein distance.\n```\ndef suggest(dic, word, distance, maxSugestions=5):\n   list = []\n   for i in range(1, 200):\n       for word1 in sorted(dic):\n           if distance(word1, word) == i:\n               list.append(word1)\n               if len(list) == maxSugestions:\n                   return list\n```\n\nThis is my current function that receives a list of string (this list has about 43 thousand strings), a word that I want to compare, a function that returns the edit distance between two strings and an integer  of maxSugestions that the list should have.\nThis is the current distance funtion:\n```\ndef levDistance(str1, str2):\n   matrix = [[0 for x in range(len(str2) + 1)] for x in range(len(str1) + 1)]\n\n   for i in range(len(str1) + 1):\n       for j in range(len(str2) + 1):\n           if i == 0:\n               matrix[i][j] = j\n           elif j == 0:\n               matrix[i][j] = i\n           elif str1[i-1] == str2[j-1]:\n               matrix[i][j] = matrix[i-1][j-1]\n           else:\n               matrix[i][j] = 1 + min(matrix[i][j-1], matrix[i-1][j], matrix[i-1][j-1])    \n\n   return matrix[len(str1)][len(str2)]\n```\n\nThe current suggest() function works, however I need it to be optimized since it is taking too long and I don't know how I should do that. Any help is grateful. Thank you\n    ", "Answer": "\r\nYou are calculating same distances over each iteration which is a big no no. Instead try to calculate only once and then have number of suggestions as defined by maxSuggestion:\n```\ndef suggest(dic, word, distance, maxSugestions=5):\n   return [i[1] for i in sorted([(distance(word1, word), word1) for word1 in dic])[:maxSuggestion]]\n```\n\nThen comes your implementation! If you still want this to be even faster, better would be to use editdistance library. (Or any other C-based implementation if it matters) instead of a python based implementation. For me it went 20x faster than python implementation. From the original answer:\n\nI used a c-based implementation of calculating levenshtein distance\nmaking use of : editdistance library. On research I found that\nmany such tasks have C-based implementations like\nmatrix-multiplication and search algorithms etc. are readily\navailable. Besides you can always write a module in C and make use of\nit in python. ```\neditdistance.eval('banana', 'bahama')```\n took only ```\n1.71 µs ± 289 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)```\n\nin comparison with my defined function ```\nlevenshteinDistance('banana', 'bahama')```\n which took ```\n34.4 µs ± 4.2 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)```\n That’s a 20x speedup.\n\nP.S. I am not author of the package and found the package using a nominal google search of 'C based implementation of levenshtein distance'\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Fastest dictionary based edit distance\r\n                \r\nI have a dictionary of strings (7 million strings). I have a list queries/strings (30 million strings) i want to run against the dictionary.\n\nI want to get all the dictionary matches having edit distance 1 or 2 for a given query.\n\nAt present i am using solr to do this. I have indexed 7 million strings in solr (2GB index size). I am using strdist() function to get the matches. But this approach is taking 300 to 500 ms on average for each query. It will take 104 days to complete 30 million queries.\n\nI tried solr MMapDirectory approach also but i did not find any performance difference. I tried increasing documentCache, queryResultCache also but it did not improve performance much.\n\nCan i improve this performance to 1 ms using solr or any other approach other than solr ? Is there any better option than solr here ?\n    ", "Answer": "\r\nTry using the fuzzy search support instead. It uses edit distance as its underlying comparison, and supports edit distances up to 2 (which should be enough for your need).\n\nAnother possibility is to create a simpler query that you filter against first, but exactly what that query would look like depends on your input and matching data (for some use cases a phonetic search works fine as the first step).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Lucene ignores / overwrite fuzzy edit distance in QueryParser\r\n                \r\nGiven the following QueryParser with a FuzzySearch term in the query string:\n```\nfun fuzzyquery() {\n    val query = QueryParser(\"term\", GermanAnalyzer()).parse(\"field:search~4\")\n    println(query)\n}\n```\n\nThe resulting Query will actually have this representation:\n```\nfield:search~2\n```\n\nSo, the ```\n~4```\n gets rewritten to ```\n~2```\n. I traced the code down to the following implementation:\nQueryParserBase\n```\nprotected Query newFuzzyQuery(Term term, float minimumSimilarity, int prefixLength) {\n    String text = term.text();\n    int numEdits = FuzzyQuery.floatToEdits(minimumSimilarity, text.codePointCount(0, text.length()));\n    return new FuzzyQuery(term, numEdits, prefixLength);\n}\n```\n\nFuzzyQuery\n```\npublic static int floatToEdits(float minimumSimilarity, int termLen) {\n    if (minimumSimilarity >= 1.0F) {\n        return (int)Math.min(minimumSimilarity, 2.0F);\n    } else {\n        return minimumSimilarity == 0.0F ? 0 : Math.min((int)((1.0D - (double)minimumSimilarity) * (double)termLen), 2);\n    }\n}\n```\n\nAs is clearly visible, any value higher than 2 will always get reset to ```\n2```\n. Why is this and how can I correctly get the fuzzy edit distance I want into the query parser?\n    ", "Answer": "\r\nThis may cross the border into \"not an answer\" - but it is too long for a comment (or a few comments):\nWhy is this?\nThat was a design decision, it would seem. It's mentioned in the documentation here.\n\n\"The value is between 0 and 2\"\n\nThere is an old article here which gives an explanation:\n\n\"Larger differences are far more expensive to compute efficiently and are not processed by Lucene.\".\n\nI don't know how official that is, however.\nMore officially, from the JavaDoc for the ```\nFuzzyQuery```\n class, it states:\n\n\"At most, this query will match terms up to 2 edits. Higher distances (especially with transpositions enabled), are generally not useful and will match a significant amount of the term dictionary.\"\n\nHow can I correctly get the fuzzy edit distance I want into the query parser?\nYou cannot, unless you customize the source code.\nThe best (least worst?) alternative, I think, is probably the one mentioned in the above referenced ```\nFuzzyQuery```\n Javadoc:\n\n\"If you really want this, consider using an n-gram indexing technique (such as the SpellChecker in the suggest module) instead.\"\n\nIn this case, one price to be paid will be a potentially much larger index - and even then, n-grams are not really equivalent to edit distances. I don't know if this would meet your needs.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "comput edit distance between 2 very large strings\r\n                \r\nSCENARIO: Given 2 input strings I need to find minimum number of insertions deletions and substitutions required to convert one string to other. The strings are text from 2 files. The comparison has to be done at word level.\n\nWhat i have done is implemented edit distance algorithm which does the job well using a 2-dimensional array of size (m*n) where sizes of input strings are m and n.\n\nThe PROBLEM i am facing is if the value of m and n becomes large, say more than 16,000 i am getting OutOfMemory exception due to the large size of m*n array. Also i am running into memory fragmentation and LargeObjectHeap issues\n\nQUESTION Looking for a C# code to solve edit distance problem for 2 very large sized strings (each containing more than 20k words) without getting OutOfMemory exception.\n\nMapReduce or DataBase or MemoryMappedFile related solutions not feasible. Only a pure C# code will work. \n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance (Dynamic Programming): Aren't insertion and deletion the same thing?\r\n                \r\nIn looking through the dynamic programming algorithm for computing the minimum edit distance between two strings I am having a hard time grasping one thing. To me it seems like given the two strings ```\ns```\n and ```\nt```\n inserting a character into ```\ns```\n would be the same as deleting a character from ```\nt```\n. Why then do we need to consider these operations separately when computing the edit distance? I always have a hard time computing the indices in the recurrence relation because I can't intuitively understand this part.\n\nI've read through Skiena and some other sources but they all don't explain this part well. This SO link explains the insert and delete operations better than elsewhere in terms of understanding what string is being inserted into or deleted from but I still can't figure out why they aren't one and the same.\n\nEdit: Ok, I didn't do a very good job of detailing the source of my confusion.\n\nThe way Skiena explains computing the minimum edit distance m(i,j) of the first i characters of a string ```\ns```\n and the first j characters of a string ```\nt```\n based on already having computed solutions to the subproblems is as follows. m(i,j) will be the minimum of the following 3 possibilities:\n\n```\nopt[MATCH] = m[i-1][j-1].cost + match(s[i],t[j]);\nopt[INSERT] = m[i][j-1].cost + indel(t[j]);\nopt[DELETE] = m[i-1][j].cost + indel(s[i]);\n```\n\n\nThe way I understand it the 3 operations are all operations on the string ```\ns```\n. An INSERT means you have to insert a character at the end of string ```\ns```\n to get the minimum edit distance. A DELETE means you have to delete the character at the end of string ```\ns```\n to get the minimum edit distance.\n\nGiven ```\ns = \"SU\"```\n and ```\nt = \"SATU\"```\n INSERT and DELETE would be as follows:\n\n```\nInsert:\n SU_\nSATU\n\nDelete:\n   SU\nSATU_\n```\n\n\nMy confusion was that an INSERT into ```\ns```\n is the same as a DELETION from ```\nt```\n. I'm probably confused on something basic but it's not intuitive to me yet.\n\nEdit 2: I think this link kind of clarifies my confusion but I'd love an explanation given my specific questions above.\n    ", "Answer": "\r\nThey aren't the same thing any more than ```\n<```\n and ```\n>```\n are the same thing. There is of course a sort of duality and you are correct to point it out. ```\na < b```\n if and only if ```\nb > a```\n so if you have a good algorithm to test for ```\nb > a```\n then it makes sense to use it when you need to test if ```\na < b```\n. \n\nIt is much easier to directly test if ```\ns```\n can be obtained from ```\nt```\n by deletion rather than to directly test if ```\nt```\n can be obtained from ```\ns```\n by insertion. It would be silly to randomly insert letters to ```\ns```\n and see if you get ```\nt```\n. I can't imagine that any implementation of edit-distance actually does that. Still, it doesn't mean that you can't distinguish between insertion and deletion.\n\nMore abstractly. There is a relation, ```\nR```\n on any set of strings defined by\n\n```\ns R t <=> t can be obtained from s by insertion\n```\n\n\n```\ndeletion```\n is the inverse relation. Closely related, but not the same.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Graph edit distance for connected components in a graph - considering the spatial distance\r\n                \r\nhas anyone ever done or seen something like this?\nI have two disconnected graphs of the same size with the same nodes but different edges. They may contain connected components. I want to compare one connected component \"a\" of graph 1 to one connected component \"b\" of graph 2. But I dont only want to calculate the graph edit distance for these two connected components which gives me the cost of transforming \"a\" to \"b\". In additon to that, I need to consider the spatial distance between the two connected components in their graph to also have the cost for the spatial distance between them. So transformation from \"a\" to \"b\" means to me bringing it into the same \"shape\" with the exact same edges AND bringing it to the same position as b is in its graph.\nExample:\nGraph 1 with connected component \"a\"\nGraph 2 with connected component \"b\"\nSo now I need to add two edges two transform \"a\" to \"b\". But I also need to \"move\" \"a\" 4 steps to the right and one step down.\nAre there any known algorithms for this or any approved ways to store the position of connected components and then compare it?\nThank you my dears!\nMy first idea was to just use some kind of edit distance and leave \"a\" after transformation at its initial position. Then identify the node which is furthest down and then furthest left and identify the corresponding same one in \"b\", then calculate the euclididan distance and give some weight to that. The exact weight is currently not in the focus.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "C - Dynamic Programming - Edit Distance\r\n                \r\n(I have simplified my code after reading advice)\nI am writing a program to take 2 strings and return the smallest editing distance as int.\n```\neg. str1 = ab, str2 = ab ; //distance will be 0. \n(when both char of str1 and str2 are the same, distance will be 0)\n\neg. str1 = abc, str2 = c ; distance will be 2.\n```\n\nIn my code,I have used below strings.\n```\nstr1 = editing\nstr2 = distance\n```\n\ncorrect answer should be 5 (e, s, i&a, g&c, e), but my program returns 6.\nI have tried other strings eg, (short & ports) which gives the answer what it is supposed to be.\nI have tried to debug, but I do not understand why sometimes those 3 options (ans1, ans2, ans3) in the function \"check\" give correct outcome, sometimes they dont.\n```\n ans1 is the result when index of str1 +1\n ans2 is the result when index of str2 +1\n ans3 is the result when index of both str1 and str2 +1\n```\n\nI have read my code many times and I cant find what went wrong.\nplease help me to understand my problems.\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\nint check(const char* str1, const char* str2, int i, int j, int dp[10][10])\n{\n  int steps = 0;\n  while (str1[i] != '\\0' || str2[j] != '\\0')\n  { //while one side has not reach '\\0'\n        while (str1[i] != '\\0' && str2[j] != '\\0')\n        { //while both sides has not reach '\\0'\n            if (dp[i][j] != -1){ return dp[i][j];} //if there is stored answer, return that\n             if (str1[i] == str2[j]) \n            { // if both char are the same, both move forward\n                i++;\n                j++;\n            }\n            else  //if both char are not the same\n            { \n              int ans1 = check(str1, str2, i+1, j, dp);  //ans1 = when str1 move forward\n              int ans2 = check(str1, str2, i, j+1, dp);  //ans2 = when str2 move forward\n              int ans3 = check(str1, str2, i+1, j+1, dp);//ans3 = when both move forward\n\n                //compare the result below to find the smallest steps(distance)\n            \n                if (ans1 <= ans2)\n                { //ans1 is smaller than ans2\n                    if (ans1 <= ans3)\n                    { //ans1 is smallest\n                        dp[i][j] = ans1 + 1; //store the answer to dp array\n                        i++;                        //move forward\n                        steps++;                    //steps +1\n                    }\n                    else //ans3 is smallest\n                    { \n                        dp[i][j] = ans3 + 1;\n                        i++;\n                        j++;\n                        steps++;\n                    }\n                }\n                else //ans2 is smaller than ans1\n                { \n                    if (ans2 <= ans3)\n                    { //ans2 is smallest\n                        dp[i][j] = ans2 + 1;\n                        j++;\n                        steps++;\n                    }\n                    else //ans3 is smallest\n                    { \n                        dp[i][j] = ans3 + 1;\n                        i++;\n                        j++;\n                        steps++;\n                    }\n                }\n            }//else ends here\n        }//while loop ends (both sides are not \\0) \n        \n        //when str1 or str2 reaches the end '\\0'\n        if (str1[i] != '\\0')\n        {\n            i++;\n            steps++;\n        }\n        else if (str2[j] != '\\0')\n        {\n            j++;\n            steps++;\n        }\n  }//while loop ends(one side is not \\0)\n    \n  //printf(\"i=%d\\nj=%d\\nsteps=%d\\n\", i, j, steps);\n  return steps;\n  \n}//function ends here\n\n\nint main() {\n  char str1[10] = \"editing\";\n  char str2[10] = \"distance\";\n    int min_step[10][10]; //with index [i][j]\n  for (int i = 0; i < 10; i++)\n    for (int j = 0; j < 10; j++)\n      min_step[i][j] = -1; \n  //scanf(\"%s %s\", str1, str2);\n  printf(\"%d\\n\", check(str1, str2, 0, 0, min_step));\n  return 0;\n}\n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Single linkage clustering of edit distance matrix with distance threshold stopping criterion\r\n                \r\nI'm trying to assign flat, single-linkage clusters to sequence IDs separated by an edit distance < n, given a square distance matrix. I believe ```\nscipy.cluster.hierarchy.fclusterdata()```\n with ```\ncriterion='distance'```\n may be a way to do this, but it isn't quite returning the clusters I'd expect for this toy example.\n\nSpecifically, in the 4x4 distance matrix example below, I would expect ```\nclusters_50```\n (which uses ```\nt=50```\n) to create 2 clusters, where actually it finds 3. I think the issue is that ```\nfclusterdata()```\n doesn't expect a distance matrix, but ```\nfcluster()```\n doesn't seem to do what I want either.\n\nI've also looked at ```\nsklearn.cluster.AgglomerativeClustering```\n but this requires ```\nn_clusters```\n to be specified, and I want to create as many clusters as needed until the distance threshold I specify has been satisfied.\n\nI see that there is a currently unmerged scikit-learn pull request for this exact feature: https://github.com/scikit-learn/scikit-learn/pull/9069 \n\nCan anyone point me in the right direction? Clustering with an absolute distance threshold criterion seems like a commmon use case.\n\n```\nimport pandas as pd\nfrom scipy.cluster.hierarchy import fclusterdata\n\ncols = ['a', 'b', 'c', 'd']\n\ndf = pd.DataFrame([{'a': 0, 'b': 29467, 'c': 35, 'd': 13},\n                   {'a': 29467, 'b': 0, 'c': 29468, 'd': 29470},\n                   {'a': 35, 'b': 29468, 'c': 0, 'd': 38},\n                   {'a': 13, 'b': 29470, 'c': 38, 'd': 0}],\n                  index=cols)\n\nclusters_20 = fclusterdata(df.values, t=20, criterion='distance')\nclusters_50 = fclusterdata(df.values, t=50, criterion='distance')\nclusters_100 = fclusterdata(df.values, t=100, criterion='distance')\n\nnames_clusters_20 = {n: c for n, c in zip(cols, clusters_20)}\nnames_clusters_50 = {n: c for n, c in zip(cols, clusters_50)}\nnames_clusters_100 = {n: c for n, c in zip(cols, clusters_100)}\n```\n\n\n```\nnames_clusters_20  # Expecting 3 clusters, finds 3\n>>> {'a': 1, 'b': 3, 'c': 2, 'd': 1}\n\nnames_clusters_50  # Expecting 2 clusters, finds 3\n>>> {'a': 1, 'b': 3, 'c': 2, 'd': 1}\n\nnames_clusters_100 # Expecting 2 clusters, finds 2\n>>> {'a': 1, 'b': 2, 'c': 1, 'd': 1}\n```\n\n    ", "Answer": "\r\nYou did not set the metric parameter.\n\nThe default then is ```\nmetric='euclidean'```\n, not precomputed.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Modification to Hamming distance/ Edit Distance\r\n                \r\nI am having trouble modifying the Hamming distance algorithm in order to affect my data in two ways\n\n\nAdd .5 to the Hamming distance if a capital letter is switched for a lower case letter unless it is in the first position.\nExamples include: \"Killer\" and \"killer\" have a distance of 0 \"killer\" and \"KiLler\" have a Hamming distance of .5. \"Funny\" and FAnny\" have a distance of 1.5 (1 for the different letter, additional .5 for the different capitalization). \nMaking it so that b and d (and their capitalized counterparts) are seen as the same thing\n\n\nHere is the code i have found that makes up the basic Hamming program\n\n```\ndef hamming_distance(s1, s2):\n    assert len(s1) == len(s2)\n    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))\n\nif __name__==\"__main__\":\n    a = 'mark'\n    b = 'Make'\n    print hamming_distance(a, b) \n```\n\n\nAny suggestions would be welcomed!\n    ", "Answer": "\r\nHere is a simple solution. For sure it could be optimized for better performance.\n\nNote: I used Python 3, since Python 2 will retire soon.\n\n```\ndef hamming_distance(s1, s2):\n    assert len(s1) == len(s2)\n    # b and d are interchangeable\n    s1 = s1.replace('b', 'd').replace('B', 'D')\n    s2 = s2.replace('b', 'd').replace('B', 'D')\n    # add 1 for each different character\n    hammingdist = sum(ch1 != ch2 for ch1, ch2 in zip(s1.lower(), s2.lower()))\n    # add .5 for each lower/upper case difference (without first letter)\n    for i in range(1, len(s1)):\n        hammingdist += 0.5 * (s1[i] >= 'a' and s1[i] <= 'z' and\\\n                              s2[i] >= 'A' and s2[i] <= 'Z' or\\\n                              s1[i] >= 'A' and s1[i] <= 'Z' and\\\n                              s2[i] >= 'a' and s2[i] <= 'z')\n    return hammingdist\n\ndef print_hamming_distance(s1, s2):\n    print(\"hamming distance between\", s1, \"and\", s2, \"is\",\n          hamming_distance(s1, s2))\n\nif __name__ == \"__main__\":\n    assert hamming_distance('mark', 'Make') == 2\n    assert hamming_distance('Killer', 'killer') == 0\n    assert hamming_distance('killer', 'KiLler') == 0.5\n    assert hamming_distance('bole', 'dole') == 0\n    print(\"all fine\")\n    print_hamming_distance(\"organized\", \"orGanised\")\n    # prints: hamming distance between organized and orGanised is 1.5\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Generating a list of distinct (distant, by edit distance) words by filtering\r\n                \r\nI have a long (> 1000 items) list of words, from which I would like to remove words that are \"too similar\" to other words, until the remaining words are all \"significantly different\".  For example, so that no two words are within an edit distance D.\n\nI do not need a unique solution, and it doesn't have to be exactly optimal, but it should be reasonably quick (in Python) and not discard way too many entries.\n\nHow can I achieve this?  Thanks.\n\nEdit: to be clear, I can google for a python routine that measures edit distance.  The problem is how to do this efficiently, and, perhaps, in some way that finds a \"natural\" value of D.  Maybe by constructing some kind of trie from all words and then pruning?\n    ", "Answer": "\r\nYou can use a ```\nbk-tree```\n, and before each item is added check that it is not within distance D of any others (thanks to @DietrichEpp in the comments for this idea.\n\nYou can use this recipe for a bk-tree (though any similar recipes are easily modified). Simply make two changes: change the line:\n\n```\ndef __init__(self, items, distance, usegc=False):\n```\n\n\nto\n\n```\ndef __init__(self, items, distance, threshold=0, usegc=False):\n```\n\n\nAnd change the line\n\n```\n        if el not in self.nodes: # do not add duplicates\n```\n\n\nto\n\n```\n        if (el not in self.nodes and\n            (threshold == None or len(self.find(el, threshold)) == 0)):\n```\n\n\nThis makes sure there are no duplicates when an item is added. Then, the code to remove duplicates from a list is simply:\n\n```\nfrom Levenshtein import distance\nfrom bktree import BKtree\ndef remove_duplicates(lst, threshold):\n    tr = BKtree(iter(lst), distance, threshold)\n    return tr.nodes.keys()\n```\n\n\nNote that this relies on the python-Levenshtein package for its distance function, which is much faster than the one provided by bk-tree. python-Levenshtein has C-compiled components, but it's worth the installation.\n\n\n\nFinally, I set up a performance test with an increasing number of words (grabbed randomly from ```\n/usr/share/dict/words```\n) and graphed the amount of time each took to run:\n\n```\nimport random\nimport time\nfrom Levenshtein import distance\nfrom bktree import BKtree\n\nwith open(\"/usr/share/dict/words\") as inf:\n    word_list = [l[:-1] for l in inf]\n\ndef remove_duplicates(lst, threshold):\n    tr = BKtree(iter(lst), distance, threshold)\n    return tr.nodes.keys()\n\ndef time_remove_duplicates(n, threshold):\n    \"\"\"Test using n words\"\"\"\n    nwords = random.sample(word_list, n)\n    t = time.time()\n    newlst = remove_duplicates(nwords, threshold)\n    return len(newlst), time.time() - t\n\nns = range(1000, 16000, 2000)\nresults = [time_remove_duplicates(n, 3) for n in ns]\nlengths, timings = zip(*results)\n\nfrom matplotlib import pyplot as plt\n\nplt.plot(ns, timings)\nplt.xlabel(\"Number of strings\")\nplt.ylabel(\"Time (s)\")\nplt.savefig(\"number_vs_time.pdf\")\n```\n\n\n\n\nWithout confirming it mathematically, I don't think it's quadratic, and I think it might actually be ```\nn log n```\n, which would make sense if inserting into a bk-tree is a log time operation. Most notably, it runs pretty quickly with under 5000 strings, which hopefully is the OP's goal (and it's reasonable with 15000, which a traditional for loop solution would not be).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Elasticsearch - Edit distance using fuzzy is inaccurate\r\n                \r\nI am using ES 5.5 and my requirement is to allow upto two edits while matching a field.\n\nIn ES,I have value as 124456788 and query comes in as 123456789\n\n```\n\"fuzzy\": {\n                            \"idkey\": {\n                                \"value\": **\"123456789\"**,\n                                \"fuzziness\": \"20\"\n\n                            }\n                        }\n```\n\n\nTo my knowledge the edit distance is 2 between these two numbers. But it is not matching even with fuzziness property as 20.\n\nI did an explain api call and here is what I am seeing \n\n```\n\"description\": \"no match on required clause (((idkey:012345789)^0.7777778 (idkey:012346789)^0.7777778 (idkey:013456789)^0.7777778 (idkey:023456789)^0.8888889 (idkey:102345678)^0.7777778 (idkey:112345678)^0.7777778 (idkey:113456789)^0.8888889 (idkey:120456589)^0.7777778 (idkey:121345678)^0.7777778 (idkey:122345678)^0.7777778 (idkey:122345679)^0.7777778 (idkey:122456789)^0.8888889 (idkey:123006789)^0.7777778 (idkey:123045678)^0.7777778 (idkey:123096789)^0.7777778 (idkey:123106789)^0.7777778 (idkey:123145678)^0.7777778 (idkey:123146789)^0.7777778 (idkey:123226789)^0.7777778 (idkey:123256789)^0.8888889 (idkey:123345678)^0.7777778 (idkey:123345689)^0.7777778 (idkey:123346789)^0.7777778 (idkey:123406784)^0.7777778 (idkey:123415678)^0.7777778 (idkey:123435678)^0.7777778 (idkey:123446789)^0.8888889 (idkey:123453789)^0.8888889 (idkey:123454789)^0.8888889 (idkey:123455789)^0.8888889 (idkey:123456289)^0.8888889 (idkey:123456489)^0.8888889 (idkey:123456709)^0.8888889 (idkey:123456779)^0.8888889 (idkey:123456780)^0.8888889 (idkey:123456781)^0.8888889 (idkey:123456783)^0.8888889 (idkey:123456785)^0.8888889 (idkey:123456786)^0.8888889 (idkey:123456787)^0.8888889 (idkey:123456889)^0.8888889 (idkey:123457789)^0.8888889 (idkey:123466789)^0.8888889 (idkey:123496789)^0.8888889 (idkey:123556789)^0.8888889 (idkey:126456789)^0.8888889 (idkey:223456789)^0.8888889 (idkey:423456789)^0.8888889 (idkey:623456789)^0.8888889 (idkey:723456789)^0.8888889)^5.0)\",\n```\n\n\nThe value I am expecting to match is 124456788 but ES query is internally not converting it as one of the possible match parameter in fuzzy query. \n\nDo i need to use different ES method to make this work?\n    ", "Answer": "\r\nThis a simple indexing and search.\n\n```\nPUT /myIndex/type1/1\n{\n    \"key\":\"123456789\",\n    \"name\":\"test\"\n}\n\nGET /myIndex/_search\n{\n    \"query\": {\n        \"fuzzy\": {\n            \"key\": {\n                \"value\": \"124456799\",\n                \"fuzziness\": 2\n            }\n        }\n    }\n}\n```\n\n\nIt is always matching with the given key. ```\nfuzziness```\n values 2 or greater is fine.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to find the weighted edit distance between two strings with Agrepl\r\n                \r\nI am having difficult time understanding how the edit distance is calculated in the agrepl function. I am trying to output the distance that agrepl calculates between two strings. I used the \"adist\" and \"stringdist\" function but it gives the total # of insertions/deletions etc, not the weighted distance that goes into the agrepl function.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "nltk edit distance lower than expected for tuple\r\n                \r\nWhile calculating the edit distance nltk does not seem to consider the changes correctly. For e.g.\n\n```\nedit_distance(('paravati', 'selke', 'vital'), ('paravati', 'selke', 'vital'), transpositions=True)\n```\n\n\nThis returns 0 because the string match.\n\n```\nedit_distance(('paravati', 'selke', 'vital'), ('selke', 'paravati', 'vital'), transpositions=True)\n```\n\n\nThis returns 1 because transpositions is enabled. Else 2\n\n```\nedit_distance(('paravati', 'selke', 'vital'), ('belke', 'paravati', 'vital'), transpositions=True)\n```\n\n\nThis returns 2 because 1 point for transpositions and 1 for substitution.\n\n```\nedit_distance(('paravati', 'selke', 'vital'), ('belke', 'zaravati', 'vital'), transpositions=True)\n```\n\n\nBut when I change the spelling of the second word (from p to z) I still get the value of 2. The expected value is more than 2 because there are now 2 words changed.\n\n\n\nIn other words, the distance of these 2 words is 5 and the distance of the tuple that contains exactly same words is 3. This does not seem correct.\n\n```\nfrom nltk.metrics import edit_distance\n\nedit_distance('vital', 'vataldedd', transpositions=True)\n\nedit_distance(('paravati', 'selke', 'vital'), ('selke', 'zaravatiasdf', 'vataldedd'), transpositions=True)\n```\n\n\nExpected distance for this tuple comparison is 5 or more.\n\n\n\nUpdate:\n\nI compared all the strings separately and took the total that returns 10 as expected.\n\n```\ns1, s2 = ('paravati', 'selke', 'vital'), ('selke', 'zaravatiasdf', 'vataldedd')\n\nfinal=list()\nfor i in s1:\n    mylist=list()\n    for k in s2:\n        mylist.append(edit_distance(i, k, transpositions=True))\n    final.append(min(mylist))\n\nsum(final)\n```\n\n\nIs this approach correct or am I missing something?\n    ", "Answer": "\r\nTh```\nedit_distance()```\n function expects strings as input. Simply, concatenate the substrings in each tuple into a single string.\n\n```\n>>> from nltk.metrics import edit_distance\n\n>>> x, y = ('paravati', 'selke', 'vital'), ('belke', 'zaravati', 'vital')\n>>> x, y = \" \".join(x), \" \".join(y)\n\n>>> x\n'paravati selke vital'\n>>> y\n'belke zaravati vital'\n\n>>> edit_distance(x, y)\n13\n\n>>> edit_distance(x, y, transpositions=True)\n13\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Calculate Editing Distance with Levenshtein\r\n                \r\nProblem:\nI've coded a Levenshtein String Editing program that appears to work correctly to me, but apparently yields the wrong answer. I think I misunderstand how the Editing Distance is calculated.\n\nComparison\nfor the strings superca and antigravitational, here's a comparison of the last two rows\n\nMine:\n\n```\n    a n t i g r a v i t  a  t  i  o  n  a  l\nc 6 6 6 6 6 6 6 6 7 8 9  10 11 12 13 14 15 16\na 7 6 7 7 7 7 7 6 7 8 9  9  10 11 12 13 13 14\n```\n\n\nOthers:\n\n```\n    a n t i g r a v i t  a  t  i  o  n  a  l\nc 6 6 6 6 6 6 6 6 7 8 9  10 11 12 13 14 15 16\na 7 6 7 7 7 7 7 6 7 8 9  9  10 11 12 13 14 15\n```\n\n\nIn my calculation, you can see that because the last \"a\" in antigravitational matches the \"a\" in superca so I gave it a value of 13 since the number directly to the left is a 13. Therefore, the cost is 0.\n\nIn other calculations, it appears that people are still adding a cost in these situations. As you can see in the other example, they placed a 14 where I placed the 13 resulting in the final Editing Distance of 15 (as opposed to my 14).\n\nAm I right and everyone else is wrong (unlikely), or am I missing a step?\n    ", "Answer": "\r\nI found my own answer after a few YouTube videos.\n\nMy Mistake\nMy instructions stated \"If the letters you are comparing match, set the cost to 0 and copy the minimum(left, diagonal, above). If they do not match, add 1.\n\nThe Correction\nSo the instructions were slightly inaccurate. In cases where the letters match you should ALWAYS copy the diagonal with no cost, regardless of what the value to the left and the value above are.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Dynamic edit distance\r\n                \r\nI tried to make a dynamic algorithm to calculate the distance between two strings. For example, if I have \"casa\" and \"cara\", the output should be 2 (1 insert + 1 deletion), but I get 1. This is the algorithm:\n\n\r\n\r\n```\npublic static int distance(String word1, String word2) {\r\n    int dp[][] = new int[m+1][n+1];\r\n    for (int i=0; i<=m; i++) {\r\n      for (int j=0; j<=n; j++) {\r\n        if (i==0)\r\n          dp[i][j] = j;  \r\n        else if (j==0)\r\n          dp[i][j] = i; \r\n        else if (str1.charAt(i-1) == str2.charAt(j-1))\r\n          dp[i][j] = dp[i-1][j-1];\r\n        else {\r\n          int rep=dp[i-1][j-1];\r\n          int ins=dp[i][j-1];\r\n          int del=dp[i-1][j];\r\n          dp[i][j] = 1 + Math.min(rep, Math.min(ins, del));\r\n        }\r\n      }\r\n    }\r\n    return dp[m][n];\r\n  }```\n\r\n\r\n\r\n\n\nCould you please find me which code causes the bug?\nWhy the output of this algorithm is 1 instead of 2 using the strings \"casa\" and \"cara\"? \n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Variation of finding edit distance with only insertions and deletions?\r\n                \r\nI need to find the edit distance between a word and its sorted word (ex: apple and aelpp), using only insertions and deletions recursively.\n\nI have found some sources that used insertions, deletions, and substitutions, but I am not sure how to only use insertion and deletion.\n\nThis is the code I found:\n\n```\ndef ld(s, t):\n    if not s: return len(t)\n    if not t: return len(s)\n    if s[0] == t[0]: return ld(s[1:], t[1:])\n    l1 = ld(s, t[1:])\n    l2 = ld(s[1:], t)\n    l3 = ld(s[1:], t[1:])\n    return 1 + min(l1, l2, l3)\n```\n\n\nWhat edits would need to be made to only find the number of insertions and deletions?\n    ", "Answer": "\r\nRemove ```\nl3```\n, which computes substitutions like so\n\n```\ndef ld2(s, t):\n    if not s: return len(t)\n    if not t: return len(s)\n    if s[0] == t[0]: return ld2(s[1:], t[1:])\n    l1 = ld2(s, t[1:])\n    l2 = ld2(s[1:], t)\n    return 1 + min(l1, l2)\n```\n\n\nYou can see that ```\nld('apple', 'applx')```\n is equal to 1, while ```\nld2```\n with the same parameters evaluates to 2.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Sort Solr returned result in spellchecker by edit distance\r\n                \r\nI'm using spell checker on Solr. And I want to sort the returned result by Edit Distance, not by frequency.\n\nFor example I have the term \"anp\". I mistyped into \"apn\" and the result was like the following order:\n\n```\n<str>plan</str>\n<str>anp</str>\n<str>cacc</str>\n```\n\n\nAnd I want the \"anp\" will be first that shown in the result because the distance between \"apn\" and \"anp\" is shorter than \"apn\" and \"plan\".\n\nMy field in this case is multivalue.\n\nUPDATE:\n\nI have changed it to multivalue=false so It can be easier in sorting but It still stay the same. Anyone who knows about the ordering of returned result please help.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is there an edit distance algorithm that takes \"chunk transposition\" into account?\r\n                \r\nI put \"chunk transposition\" in quotes because I don't know whether or what the technical term should be. Just knowing if there is a technical term for the process would be very helpful.\n\nThe Wikipedia article on edit distance gives some good background on the concept.\n\nBy taking \"chunk transposition\" into account, I mean that\n\n```\nTuring, Alan.\n```\n\n\nshould match \n\n```\nAlan Turing\n```\n\n\nmore closely than it matches\n\n```\nTuring Machine\n```\n\n\nI.e. the distance calculation should detect when substrings of the text have simply been moved within the text. This is not the case with the common Levenshtein distance formula.\n\nThe strings will be a few hundred characters long at most -- they are author names or lists of author names which could be in a variety of formats. I'm not doing DNA sequencing (though I suspect people that do will know a bit about this subject).\n    ", "Answer": "\r\nIn the case of your application you should probably think about adapting some algorithms from bioinformatics.\n\nFor example you could firstly unify your strings by making sure, that all separators are spaces or anything else you like, such that you would compare \"Alan Turing\" with \"Turing Alan\". And then split one of the strings and do an exact string matching algorithm ( like the Horspool-Algorithm ) with the pieces against the other string, counting the number of matching substrings.\n\nIf you would like to find matches that are merely similar but not equal, something along the lines of a local alignment might be more suitable since it provides a score that describes the similarity, but the referenced Smith-Waterman-Algorithm is probably a bit overkill for your application and not even the best local alignment algorithm available.\n\nDepending on your programming environment there is a possibility that an implementation is already available. I personally have worked with SeqAn lately, which is a bioinformatics library for C++ and definitely provides the desired functionality.\n\nWell, that was a rather abstract answer, but I hope it points you in the right direction, but sadly it doesn't provide you with a simple formula to solve your problem.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Complexity of edit distance (Levenshtein distance) recursion top down implementation\r\n                \r\nI have been working all day with a problem which I can't seem to get a handle on. The task is to show that a recursive implementation of edit distance has the time complexity Ω(2max(n,m)) where n & m are the length of the words being measured.\n\nThe implementation is comparable to this small python example\n\n```\ndef lev(a, b):\n    if(\"\" == a):\n       return len(b)   # returns if a is an empty string\n    if(\"\" == b):\n        return len(a)   # returns if b is an empty string\n    return min(lev(a[:-1], b[:-1])+(a[-1] != b[-1]), lev(a[:-1], b)+1, lev(a, b[:-1])+1)\n```\n\n\nFrom: http://www.clear.rice.edu/comp130/12spring/editdist/\n\nI have tried drawing trees of the recursion depth for different short words but I cant find the connection between the tree depth and complexity.\n\nRecursion Formula from my calculation\n\n```\nm = length of word1\nn = length of word2\nT(m,n) = T(m-1,n-1) + 1 + T(m-1,n) + T(m,n-1)\nWith the base cases:\nT(0,n) = n\nT(m,0) = m\n```\n\n\nBut I have no idea on how to proceed since each call leads to 3 new calls as the lengths don't reach 0.\n\nI would be grateful for any tips on how I can proceed to show that the lower bound complexity is Ω(2max(n,m)).\n    ", "Answer": "\r\nYour recursion formula:\n\n```\nT(m,n) = T(m-1,n-1) + T(m-1,n) + T(m,n-1) + 1\nT(0,n) = n\nT(m,0) = m\n```\n\n\nis right.\n\nYou can see, that every ```\nT(m,n)```\n splits of into three paths. Due to every node runs in ```\nO(1)```\n we only have to count the nodes.\n\nA shortest path has the length ```\nmin(m,n)```\n, so the tree has at least ```\n3min(m,n)```\n nodes. But there are some path that are longer. You get the longest path by alternately reduce the first and the second string. This path will have the length ```\nm+n-1```\n, so the whole tree has at most ```\n3m+n-1```\n nodes.\n\nLet ```\nm = min(m,n)```\n. The tree contains also at least \n\n\n\ndifferent paths, one for each possible order of reducing ```\nn```\n. \n\nSo ```\nΩ(2max(m,n))```\n and ```\nΩ(3min(m,n))```\n are lower bounds and ```\nO(3m+n-1)```\n is an upper bound.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance problem( using DP) -- Getting wrong answer\r\n                \r\nI was solving a minimum Edit Distance problem where I had to find the minimum number of insertions, deletions, and replacements required to convert one string into another one. I used the bottom-up approach to form an array and then proceed further. But I failed on a test case where I had to convert 'short' to 'ports'( my output was 2, the expected output is 3).\nHere's my source code:\n```\n#include <iostream>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nusing std::string;\nusing std::vector;\nusing std::min;\n\nint min1( int x, int y, int z );\nint edit_distance(const string &str1, const string &str2);\n\nint min1( int x, int y, int z ){\n  return min( min(x,y),z);\n}\n\nint edit_distance(const string &str1, const string &str2) {\n\n  int m = str1.length();\n  int n = str2.length();\n  vector <vector <int> > D ( (m+1) , vector <int> (n+1) ); \n\n  for( size_t i{1} ; i<=m ; ++i ){\n    for( size_t j{1} ; j<= n ; ++j ){\n      if( j==0){\n         D.at(i).at(j) = i;\n      }\n      else if( i==0 ){\n         D.at(i).at(j) = j;\n      }\n      else if(str1[i-1] == str2[j-1]){\n        D.at(i).at(j)= D.at(i-1).at(j-1);\n      }\n      else{\n      D.at(i).at(j) =1+ min1( D.at(i-1).at(j) , D.at(i).at(j-1) , D.at(i-1).at(j-1) );\n      }\n    }\n  }\n\n  return D.at(m).at(n);\n}\nint main() {\n  string str1;\n  string str2;\n  std::cin >> str1 >> str2;\n  std::cout << edit_distance(str1, str2) << std::endl;\n  return 0;\n}\n```\n\nPlease help me with the problem in my code..\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Finding words from Wordnet separated by a fixed Edit Distance from a given word\r\n                \r\nI am writing a spell checker using nltk and wordnet, I have a few wrongly spelt words say \"belive\". What I want to do is find all words from wordnet that are separated by a leveshtein's edit distance of 1 or 2 from this given word. \nDoes nltk provide any methods to accomplish this? How to do this?\n\n\n\nMay be, I put it wrongly. the ```\nedit_distance```\n method takes 2 arguments like ```\nedit_distance(word1,word2)```\n returns the levenshtein's distance between word1 and word2.\nWhat I want is to find edit distance between the word I give with every other word in wordnet.\n    ", "Answer": "\r\nIt does in fact provide an ```\nedit_distance```\n method.  See the docs here\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Fuzzy sentence search in elasticsearch based on edit distance of words\r\n                \r\nFor a given index I have added documents like:\n```\n[\n{\"expression\": \"tell me something about elasticsearch\"},\n{\"expression\": \"this is a new feature for elasticsearch\"},\n{\"expression\": \"tell me something about kibana\"},\n# ... and so on\n]\n```\n\nNow, I want to query elastic search in a such a that for given input expression:\n```\n\"tell me something on elasticsearch\"```\n. It must give out:\n```\n{\"expression\": \"tell me something about elasticsearch\"},\n{\"expression\": \"tell me something about kibana\"}\n```\n\nSince it this case edit distance w.r.t. to ```\nwords```\n (not character level) is less in this case.\nCan we perform such a query on elasticsearch?\n    ", "Answer": "\r\nas per my understanding fuzziness does not allow type phrase/match phrase.\nBut let me share few use cases for you and try if these are helpful.\n\nIf you want to perform search ignoring missing words use slop with match_phrase and not fuzziness(this may work for you)\n```\nGET demo_index/_search\n{\n    \"query\": {\n        \"match_phrase\": {\n            \"field1\": {\n                \"query\": \"tell me something elasticsearch\",\n                \"slop\": 1                                     -----> you can increase it as per your requirement\n            }\n        }\n    }\n}\n```\n\n\nSecondly if you want to perform search on character level changes you can use below queries with fuzziness\n\n\n\nSingle search on different fields\n```\nGET index_name/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"multi_match\": {\n            \"query\": \"enginere\",                   -----> Wrong spelling but still we will get result wherever query match **engineer** keyword. Again you can increase fuzziness.\n            \"fields\": [\n              \"field_name1\",\n              \"field_name2\",\n              ...\n            ],\n            \"fuzziness\": 1,\n            \"slop\": 1                              -----> not compulsory \n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n\nMulti search in different fields\n```\nGET index_name/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"field1\": {\n              \"query\": \"text1\",\n              \"fuzziness\": 1\n            }\n          }\n        },\n        {\n          \"match\": {\n            \"field2\": {\n              \"query\": \"text2\",\n              \"fuzziness\": 2\n            }\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"match\": {\n            \"field3\": \"text3\"\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to visualize mean edit distance in Tensorboard using Keras callback?\r\n                \r\nSo far, I have been experimenting with Tensorflow and Keras. I took a code from image_ocr.py which allowed me to train printed text ocr. I want to see the training progress as it goes and have successfuly visualized the accuracy and loss of the training model. However, from what I have heard OCR RNN does not take accuracy as a validation but using mean edit distance instead to validate the accuracy of the words. In this case, I have been trying to get a variable called mean_ed and mean_norm_ed to be visualized in Tensorboard from class VizCallback. I have tried the method from this link but it still does not work. Can anyone help me with visualizing the mean edit distance variables? Here are the code snippets from my code:\n\n```\nclass VizCallback(keras.callbacks.Callback):\n\ndef __init__(self, run_name, test_func, text_img_gen, num_display_words=6):\n    self.test_func = test_func\n    self.output_dir = os.path.join(\n        OUTPUT_DIR, run_name)\n    self.text_img_gen = text_img_gen\n    self.num_display_words = num_display_words\n    if not os.path.exists(self.output_dir):\n        os.makedirs(self.output_dir)\n\ndef on_train_begin(self, logs={}):\n    self.med = []\n    self.nmed = []\n\ndef show_edit_distance(self, num, logs={}):\n    num_left = num\n    mean_norm_ed = 0.0\n    mean_ed = 0.0\n    while num_left > 0:\n        word_batch = next(self.text_img_gen)[0]\n        num_proc = min(word_batch['the_input'].shape[0], num_left)\n        decoded_res = decode_batch(self.test_func, word_batch['the_input'][0:num_proc])\n        for j in range(num_proc):\n            edit_dist = editdistance.eval(decoded_res[j], word_batch['source_str'][j])\n            mean_ed += float(edit_dist)\n            mean_norm_ed += float(edit_dist) / len(word_batch['source_str'][j])\n        num_left -= num_proc\n    mean_norm_ed = mean_norm_ed / num\n    mean_ed = mean_ed / num\n    #Create scalar summaries for both mean edit distance and normalized mean edit distance\n    tf_med_ph = tf.placeholder(tf.float32,shape=None,name='med_summary')\n    tf_nmed_ph = tf.placeholder(tf.float32,shape=None,name='nmed_summary')\n    tf_med = tf.summary.scalar('med', tf_med_ph)\n    tf_nmed = tf.summary.scalar('nmed', tf_nmed_ph)\n    performance_summaries = tf.summary.merge([tf_med,tf_nmed])\n\n    #Create a session for displaying the summary\n    config = tf.ConfigProto(allow_soft_placement=True)\n    session = tf.InteractiveSession(config=config)\n    summ_writer = tf.summary.FileWriter(os.path.join('summaries','first'), session.graph)\n\n    # Execute the summaries defined above\n    summ = session.run(performance_summaries, feed_dict={tf_med_ph:mean_ed, tf_nmed_ph:mean_norm_ed})\n\n    # Write the obtained summaries to the file, so it can be displayed in the TensorBoard\n    summ_writer.add_summary(summ, epoch)\n\n    session.close()\n    print('\\nOut of %d samples:  Mean edit distance: %.3f Mean normalized edit distance: %0.3f'\n          % (num, mean_ed, mean_norm_ed))\n\ndef on_epoch_end(self, epoch, logs={}):\n    self.model.save_weights(os.path.join(self.output_dir, 'weights%02d.h5' % (epoch)))\n    self.show_edit_distance(256)\n    word_batch = next(self.text_img_gen)[0]\n    res = decode_batch(self.test_func, word_batch['the_input'][0:self.num_display_words])\n    if word_batch['the_input'][0].shape[0] < 256:\n        cols = 2\n    else:\n        cols = 1\n    for i in range(self.num_display_words):\n        plt.subplot(self.num_display_words // cols, cols, i + 1)\n        if K.image_data_format() == 'channels_first':\n            the_input = word_batch['the_input'][i, 0, :, :]\n        else:\n            the_input = word_batch['the_input'][i, :, :, 0]\n        plt.imshow(the_input.T, cmap='Greys_r')\n        plt.xlabel('Truth = \\'%s\\'\\nDecoded = \\'%s\\'' % (word_batch['source_str'][i], res[i]))\n    fig = plt.gcf()\n    fig.set_size_inches(10, 13)\n    plt.savefig(os.path.join(self.output_dir, 'e%02d.png' % (epoch)))\n    plt.close()\n\ndef train(run_name, start_epoch, stop_epoch, img_w):\n# Input Parameters\nimg_h = 64\nwords_per_epoch = 16000\nval_split = 0.2\nval_words = int(words_per_epoch * (val_split))\n\n# Network parameters\nconv_filters = 16\nkernel_size = (3, 3)\npool_size = 2\ntime_dense_size = 32\nrnn_size = 512\nminibatch_size = 32\n\nif K.image_data_format() == 'channels_first':\n    input_shape = (1, img_w, img_h)\nelse:\n    input_shape = (img_w, img_h, 1)\n\nfdir = os.path.dirname(get_file('wordlists.tgz',\n                                origin='http://test.com/wordlist.tgz', untar=True))\n\nimg_gen = TextImageGenerator(monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'),\n                             bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'),\n                             minibatch_size=minibatch_size,\n                             img_w=img_w,\n                             img_h=img_h,\n                             downsample_factor=(pool_size ** 2),\n                             val_split=words_per_epoch - val_words\n                             )\nact = 'relu'\ninput_data = Input(name='the_input', shape=input_shape, dtype='float32')\ninner = Conv2D(conv_filters, kernel_size, padding='same',\n               activation=act, kernel_initializer='he_normal',\n               name='conv1')(input_data)\ninner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner)\ninner = Conv2D(conv_filters, kernel_size, padding='same',\n               activation=act, kernel_initializer='he_normal',\n               name='conv2')(inner)\ninner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner)\n\nconv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters)\ninner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n\n# cuts down input size going into RNN:\ninner = Dense(time_dense_size, activation=act, name='dense1')(inner)\n\n# Two layers of bidirectional GRUs\n# GRU seems to work as well, if not better than LSTM:\n\ngru_1 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru1')(inner)\ngru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(inner)\ngru1_merged = add([gru_1, gru_1b])\ngru_2 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru2')(gru1_merged)\ngru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru2_b')(gru1_merged)\n\n# transforms RNN output to character activations:\ninner = Dense(img_gen.get_output_size(), kernel_initializer='he_normal',\n              name='dense2')(concatenate([gru_2, gru_2b]))\ny_pred = Activation('softmax', name='softmax')(inner)\nModel(inputs=input_data, outputs=y_pred).summary()\n\nlabels = Input(name='the_labels', shape=[img_gen.absolute_max_string_len], dtype='float32')\ninput_length = Input(name='input_length', shape=[1], dtype='int64')\nlabel_length = Input(name='label_length', shape=[1], dtype='int64')\n# Keras doesn't currently support loss funcs with extra parameters\n# so CTC loss is implemented in a lambda layer\nloss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n\n# clipnorm seems to speeds up convergence\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n\nmodel = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n\n#Make tensorboard instance\ninit_op = tf.initialize_all_variables()\n\nsess = tf.Session()\nsess.run(init_op)\ntbname=\"tensorboard-of-{}\".format(int(time.time()))\ntensorboard = keras.callbacks.TensorBoard(\nlog_dir=\"logs/{}\".format(tbname),\nhistogram_freq=0,\nwrite_images=True)\n\n# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\nmodel.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd,\n          metrics=['accuracy'])\nif start_epoch > 0:\n    weight_file = os.path.join(OUTPUT_DIR, os.path.join(run_name, 'weights%02d.h5' % (start_epoch - 1)))\n    model.load_weights(weight_file)\n# captures output of softmax so we can decode the output during visualization\ntest_func = K.function([input_data], [y_pred])\n\nviz_cb = VizCallback(run_name, test_func, img_gen.next_val())\n\nmodel.fit_generator(generator=img_gen.next_train(),\n                    steps_per_epoch=(words_per_epoch - val_words) // minibatch_size,\n                    epochs=stop_epoch,\n                    validation_data=img_gen.next_val(),\n                    validation_steps=val_words // minibatch_size,\n                    callbacks=[tensorboard,viz_cb, img_gen],\n                    initial_epoch=start_epoch)\n```\n\n\nAny help would be much appriciated. Thank you!\n\nP.S. I am using Tensorflow 1.9.0 and Python 3.6.8\n\nUPDATE\nnow it is just a matter of passing the variable performance_summaries from the VizCallbak class towards the metrics in the train function. Any help here?\n    ", "Answer": "\r\nYou could modify ```\nshow_edit_distance```\n to add the summaries every time this function is being called:\n\n```\ndef show_edit_distance(self, num, epoch):\n    ...\n    summary = tf.Summary()\n    summary.value.add(tag='mean_ed', simple_value=mean_ed)\n    summ_writer.add_summary(summary, epoch)\n\n    summary = tf.Summary()\n    summary.value.add(tag='mean_norm_ed', simple_value=mean_norm_ed)\n    summ_writer.add_summary(summary, epoch)\n    ...\n```\n\n\nNote that you will need an extra argument ```\nepoch```\n:\n\n```\ndef on_epoch_end(self, epoch, logs={}):\n    ...\n    self.show_edit_distance(256, epoch)\n    ...\n```\n\n\nThe Tensorboard callback should automatically pick up these summaries, as they're being added to the ```\nGraphKeys.SUMMARIES```\n collection.\n\nNOTE: Unfortunately, I couldn't test the solution. Please let me know if there is something I am missing.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to implement alignment through traceback for Levenshtein edit distance\r\n                \r\nSo I have successfully implemented the Levenshtein (edit minimum distance) algorithm with the help of Wikipedia and this Needleman tutorial, whereby custom, insertion and deletion cost 1, and substitution/replacement cost 2.\nExecutable gist https://gist.github.com/axelmukwena/8696ec4ec72849d3cf384f5d97321407\n```\nimport numpy as np\n\ndef word_edit_distance(x, y):\n    rows = len(x) + 1\n    cols = len(y) + 1\n    distance = np.zeros((rows, cols), dtype=int)\n\n    for i in range(1, rows):\n        for k in range(1, cols):\n            distance[i][0] = i\n            distance[0][k] = k\n\n    for col in range(1, cols):\n        for row in range(1, rows):\n            if x[row - 1] == y[col - 1]:\n                cost = 0\n            else:\n                cost = 2\n            distance[row][col] = min(distance[row - 1][col] + 1,\n                                     distance[row][col - 1] + 1,\n                                     distance[row - 1][col - 1] + cost)\n     \n    print(backtrace(x, y, distance))\n    edit_distance = distance[row][col]\n    return edit_distance, distance\n\n\nresult = word_edit_distance(\"AACGCA\", \"GAGCTA\")\nprint(result[0])\nprint(result[1])\n```\n\n```\n# output\n4\n[[0 1 2 3 4 5 6]\n [1 2 1 2 3 4 5]\n [2 3 2 3 4 5 4]\n [3 4 3 4 3 4 5]\n [4 3 4 3 4 5 6]\n [5 4 5 4 3 4 5]\n [6 5 4 5 4 5 4]]\n```\n\n\nAnd, I somehow also understand how to compute the backtracking, see my attempt below. However, there is a slight error. See Bottom\n\n```\ndef backtrace(first, second, matrix):\n    f = [char for char in first]\n    s = [char for char in second]\n    new_f, new_s = [], []\n    row = len(f)\n    col = len(s)\n    trace = [[row, col]]\n\n    while True:\n        a = matrix[row - 1][col]\n        b = matrix[row - 1][col - 1]\n        c = matrix[row][col - 1]\n\n        which = min(a, b, c)\n\n        if which == matrix[row][col] or which == matrix[row][col] - 2:\n            # when diagonal backtrace substitution or no substitution\n            trace.append([row - 1, col - 1])\n            new_f = [f[row - 1]] + new_f\n            new_s = [s[col - 1]] + new_s\n\n            row, col = row - 1, col - 1\n\n        elif which == matrix[row][col] - 1:\n            # either deletion or insertion, find if minimum is up or left\n            if which == matrix[row - 1][col]:\n                trace.append([row - 1, col])\n                new_f = [f[row - 1]] + new_f\n                new_s = [\"-\"] + new_s\n\n                row, col = row - 1, col\n\n            elif which == matrix[row][col - 1]:\n                trace.append([row, col - 1])\n                new_f = [\"-\"] + new_f\n                new_s = [s[col - 1]] + new_s\n\n                row, col = row, col - 1\n\n        # Exit the loop\n        if row == 0 or col == 0:\n            return trace, new_f, new_s\n```\n\nOutcome\n```\n# trace => [[6, 6], [5, 5], [5, 4], [4, 3], [3, 2], [2, 2], [1, 2], [0, 1]]\n['A', 'A', 'C', 'G', 'C', '-', 'A'], ['A', '-', '-', 'G', 'C', 'T', 'A']\n```\n\nExpected outcome:\n```\n# trace => [[6, 6], [5, 5], [5, 4], [4, 3], [3, 2], [2, 2], [1, 1], [0, 0]]\n['A', 'A', 'C', 'G', 'C', '-', 'A'], ['A', 'A', '-', 'G', 'C', 'T', 'A']\n```\n\n\n\nWhats happening is:\n\n\nDuring finding edit distance,\n\n```\n# cost = 2\ndistance[row - 1][col] + 1 = 2         # orange\ndistance[row][col - 1] + 1 = 4         # yellow\ndistance[row - 1][col - 1] + cost = 2  # green \n```\n\nSo here, both orange and green are candidates. But the ideal candidate is green because ```\nA == A```\n\n\n\nDuring backtracking, the we don't have information about the sequences, just the points in the matrix. So the trace will get the lowest of the three...\n\n```\na = matrix[row - 1][col]      # a = 1\nb = matrix[row - 1][col - 1]  # b = 2\nc = matrix[row][col - 1]      # c = 3\n\nwhich = min(a, b, c)          # which = a instead of b\n```\n\n\n\nAm I even using the correct backtracing algorithm?\n    ", "Answer": "\r\nIt should not be ```\nmin(a,b,c)```\n. You should select the node that minimizes the score of the other node plus cost of operation from the other node to the current one.\n```\nr = matrix[row][col]          # current node\na = matrix[row - 1][col]      # a = 1\nb = matrix[row - 1][col - 1]  # b = 2\nc = matrix[row][col - 1]      # c = 3\n\nif x[row - 1] == y[col - 1]:\n    cost = 0\nelse:\n    cost = 2\n\nif r == a + 1: return a\nif r == b + cost: return b\nif r == c + 1: return c\n```\n\nor in a more compressed form:\n```\nwhich = min(a + 1, b + cost, c + 1)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How do I calculate the Graph Edit Distance with networkx(Python)?\r\n                \r\nI am working with the graph edit distance; According to the definition it is the minimum sum of costs to transform the original graph G1 into a graph that is isomorphic to G2;\nThe graph edit operations typically include:\n\nvertex insertion to introduce a single new labeled vertex to a graph.\nvertex deletion to remove a single (often disconnected) vertex from a graph.\nvertex substitution to change the label (or color) of a given vertex.\nedge insertion to introduce a new colored edge between a pair of vertices.\nedge deletion to remove a single edge between a pair of vertices.\nedge substitution to change the label (or color) of a given edge.\n\nNow I want to use the implementation networkx has - I do not have any edge labels, the node set of G1 and G2 is the same and I do not want a graph isomorphic to G2 but I want G2 itself;\nThis is mainly because G1: 1->2->3 and G2: 3->2->1 are isomorphic to each other but if the nodes represent some events, from a perspective of causality, they are very very different;\nSo in this context, I've been running a test like the following:\n```\nimport networkx as nx\n\nG=nx.DiGraph()\nG.add_node(1)\nG.add_node(2)\nG.add_node(3)\nG.add_edges_from([(1, 2),(2,3)])\n\n\nG2=nx.DiGraph()\nG2.add_node(1)\nG2.add_node(2)\nG2.add_node(3)\nG2.add_edges_from([(3, 2),(2, 1)])\n\n\nnx.graph_edit_distance(G,G2)\n```\n\nBut it returns that the distance is zero which makes sense because the graphs are isomorphic to each other;\nSo I tried to set up node_match but still no luck\n```\nimport networkx as nx\n\ndef nmatch(n1, n2):\n    return n1==n2 \n\nG=nx.DiGraph()\nG.add_node(1)\nG.add_node(2)\nG.add_node(3)\nG.add_edges_from([(1, 2),(2,3)])\n\n\nG2=nx.DiGraph()\nG2.add_node(1)\nG2.add_node(2)\nG2.add_node(3)\nG2.add_edges_from([(3, 2),(2, 1)])\n\n\nnx.graph_edit_distance(G,G2, node_match=nmatch)\n\n```\n\nIf we assume a cost of 1 to delete or add an edge/ vertex, then the edit distance should be 4, because we can:\n\ndelete both edges in G, add the 2 edges from G2\n\nHow would it be suitable to calculate the edit distance not considering isomorphy but really equivalence?\n    ", "Answer": "\r\nIt doesn't seem that you are comparing what you want to. ```\nn1```\n and ```\nn2```\n in nmatch are always ```\n{}```\n. From documentation\n\n(...) That is, the function will receive the node attribute dictionaries for n1 and n2 as inputs.\n\nyou are not comparing the nodes object, but dictionaries associated with them (as any data you need)\nYou can add your custom data to that dictionary when adding nodes, for example:\n```\nimport networkx as nx\n\ndef nmatch(n1, n2):\n    return n1==n2\n\nG=nx.DiGraph()\nG.add_node(1, id=1)\nG.add_node(2, id=2)\nG.add_node(3, id=3)\nG.add_edges_from([(1, 2),(2,3)])\n\n\nG2=nx.DiGraph()\nG2.add_node(1, id=1)\nG2.add_node(2, id=2)\nG2.add_node(3, id=3)\nG2.add_edges_from([(3, 2),(2,1)])\n\n\nnx.graph_edit_distance(G,G2, node_match=nmatch)\n```\n\nreturns 2, as you can do 2 edge substitutions. You could probably increase substitution cost if you wanted result to be 4 (2 insertions, 2 deletions)\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Cluster sequences in a network by their editing distance - in R\r\n                \r\nI have a dataframe my_df with 10,000 different sequences with different lengths (between 13to18) they comprised from different numbers (0-3)\nexample of my data (60 lines) :\n```\nlibrary(stringdist)\nlibrary(igraph)\nlibrary(reshape2)\n\n\nstructure(list(alfa_ch = c(\"2000000232003211\",\"2000000331021\", \"20000003310320011\", \"20000003323331021\", \n                           \"20000003331001\",\"20000003331001\", \"20000003332021\", \"200000100331021\",\n                           \"20000013011001\",\"20000013301021\", \"2000001333331011\", \"20000023231031\",\n                           \"200000233302001\",\"20000023331011\", \"20000023331012\", \"20000023332021\",\n                           \"200000233331021\",\"20000030231011\", \"200000303323331021\", \"200000313301021\",\n                           \"20000032031021\",\"2000003220021\", \"2000003221011\", \"2000003231031\",\n                           \"20000032311001\",\"200000330330021\", \"2000003311211\", \"2000003331001\",\n                           \"2000003331001\",\"2000003331012\", \"20000033321012\", \"200000333231011\",\n                           \"20000033323331021\",\"20000033331021\", \"2000010320011\", \"20000103323331021\",\n                           \"200001113011001\",\"20000113011001\", \"20000120330021\", \"20000123033011\",\n                           \"2000012331131\",\"2000013011001\", \"2000013301021\", \"200001330231011\",\n                           \"2000013323001\",\"20000133231311\", \"20000133301001\", \"200001333331011\",\n                           \"200001333331011\",\"200001333331011\", \"200001333331011\", \"20000200331021\",\n                           \"20000200331021\",\"20000200331131\", \"20000203221011\", \"2000020333133011\",\n                           \"20000212221111\",\"20000213301021\", \"2000021331011\", \"200002223231011\")),\n          row.names = c(1L,3L, 5L, 6L, 7L, 8L, 9L, 10L, 12L, 13L, 14L, 16L, 17L, 18L, 19L,20L, 21L,\n                        23L, 24L, 27L, 29L, 31L, 32L, 33L, 34L, 35L, 38L, 41L,42L, 43L, 46L, 47L, 48L,\n                        49L, 58L, 59L, 60L, 62L, 63L, 64L, 66L,68L, 71L, 72L, 73L, 74L, 75L, 77L, 78L,\n                        79L, 80L, 81L, 82L, 83L,84L, 85L, 89L, 90L, 91L, 95L), class = \"data.frame\")\n```\n\n, my goal is to cluster them by editing distance < 3.\n```\ndist_mtx=as.matrix(stringdistmatrix(my_df$alfa,my_df$alfa,method = \"lv\"))\ndist_mtx[dist_mtx>3]=NA\ndist_mtx[new_test_2==0]=NA\ncolnames(dist_mtx) <- dist_mtx$alfa\nrownames(dist_mtx) <- dist_mtx$alfa\n```\n\nthen created an edge list , while the value represents the editing distance between any 2 sequences:\n```\nedge_list <- unique(melt(dist_mtx,na.rm = TRUE,varnames = c('seq1','seq2'),as.is = T))\nedge_list=edge_list[!is.na(edge_list$value),]\n```\n\nthen created the igraph object :\n```\nigraph_obj <- igraph::graph_from_data_frame(edge_list,directed = F,vertices = dist_mtx$alfa)\n```\n\nthen i tried numerous methods to try and cluster those sequences with louvain method and im still getting clusters which its members have editing distance > 3 , im aware that it might be because of the connected components.\nso my questions are :\n\nis there a way to cluster to sequences together so that in each cluster the members would be with editing distance < 3 ?\nis there a way to recognize the cluster centers (HUBS) , tried hubness.score() and assign vertices according to those centers with consideration of the editing distance ?\n\nthis is my first post ,\ni will appreciate any help\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to check similarity of two Xml trees (Tree Edit Distance in C#)\r\n                \r\nIn a C# application I need to check the output of my algorithm, which is an XML tree against another XML tree to see how they are similar. (node order is important, but the structure (nested nodes), names of nodes are more important). Maybe the number of ```\nadds```\n, ```\nremoves```\n and ```\nmoves```\n that occurs in some \"Tree Edit distance\" algorithms be a good indicator. But the answers are more Java or Python packages. \n\nSo, I tried to use XMLDiffPatch, it works good when the algorithm type is set to ```\nPrecise```\n. However its bad point is that it just generate a ```\nDiffGram```\n file that needs to be analyzed to find the number of operations. Moreover, it is very buggy and generates ```\nOutOfRangeException```\n for some XML trees. I also couldn't find better packages for my purpose for .Net. There are some Xml difference packages but maybe none or few of them are on ```\nTree Edit Distance```\n.\n\nAn example:\n\n```\n<A>\n  <B>\n    <C></C>\n    <D></D>\n    <E>\n       <F>\n       </F>\n    </E>\n  </B>\n</A>\n```\n\n\nTo:\n\n```\n<A>      \n    <C></C>\n    <D></D>\n    <G></G>\n</A>\n```\n\n\nTo convert the first Xml to the second, you need to remove ```\nE```\n and ```\nF```\n (2 costs) then you need to remove ```\nB```\n (but not its sub tree) and to add ```\nG```\n. Then the total costs is 4.\n\nSo, as I know here I shouldn't ask for packages and tools, I ask for a simple algorithm or (tree edit distance algorithm in .Net) to do that. This is my own algorithm to check similarity and ignore minor difference (Having one or few nested nodes) but it is very primary and just for an starting point:\n\n```\npublic int XMLCompare(XmlNode primary, XmlNode secondary)\n{\n    int x = 0;\n    if (secondary == null || primary == null)\n        return 1;\n\n    if (secondary.ChildNodes.Count == 1 && primary.ChildNodes.Count > 1)\n    {\n        x += XMLCompare(primary, secondary.ChildNodes[0]);\n    }\n    else if (secondary.ChildNodes.Count > 1 && primary.ChildNodes.Count == 1)\n    {\n        x += XMLCompare(primary.ChildNodes[0], secondary);\n    }\n    else\n    {\n        if (primary.Name.ToLower() != secondary.Name.ToLower())\n            x = 1;\n        int m = Math.Max(primary.ChildNodes.Count, secondary.ChildNodes.Count);\n        for (int i = 0; i < m  i++)\n        {\n            x += XMLCompare(\n            i < primary.ChildNodes.Count ? primary.ChildNodes[i] : null,  \n            i < secondary.ChildNodes.Count ? secondary.ChildNodes[i] : null);\n\n        }\n    }\n\n    return x;\n}\n```\n\n    ", "Answer": "\r\nMicrosoft has an API for that. check this.\nThis may be old dll reference but just for you information, you need to use something like this.\nC:\\Windows\\assembly\\GAC\\XmlDiffPatch\\1.0.8.28__b03f5f7f11d50a3a\\XmlDiffPatch.dll \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance Dynamic Programing for very large input\r\n                \r\nI am solving the well known Edit Distance Dynamic Programing Problem.Actually the problem is given two strings string1 and string2 and given the cost for deletion,insertion and replacement of the character,I have to convert string1 to string2 in minimum cost.For DP I have to use a two dimensional array.For a small string (size<=10000) my code is working but for a larger input(size>=100000) the compiler says \"array size is too large\". If the problem has to be solved using dynamic programing(for input size=100000) then please tell me how should I handle this error.Here is my code.\n\n```\n#include <iostream>\n#include <cstdio>\n#include <stdlib.h>\n#include <algorithm>\n#include <map>\n#include <queue>\n#include <iomanip>\n#include <string>\n#include <math.h>\n#include <limits>\n#include <map>\n#include <float.h>\n#include <limits.h>\n#include <string.h>\nusing namespace std;\n#define rep(i,a,N) for(int i=a;i<N;++i)\nint DP[10000][10000],delete_cost,add_cost,replace_cost;\nstring first,second;\nint Min(int x,int y,int z){\n    int min=x<y?x:y;\n    min=min<z?min:z;\n    return min;\n}\n\nint Transform(int i,int j){ \n    if(DP[i][j]!=-1){\n        //printf(\"DP is set\\n\");\n        return DP[i][j];\n    }\n    if(i==first.size())\n        return (second.size()-j)*add_cost;\n    if(j==second.size())\n        return (first.size()-i)*delete_cost;\n    if(first.at(i)!=second.at(j)){\n        int add,del,rep;\n        add=Transform(i,j+1)+add_cost;\n        del=Transform(i+1,j)+delete_cost;\n        rep=Transform(i+1,j+1)+replace_cost;\n        return DP[i][j]=Min(add,del,rep);\n    }\n    else\n        return DP[i][j]=Transform(i+1,j+1);\n\n}\n    int main(){\n    int T,a,b,k,ans;\n    scanf(\"%d\",&T);\n\n    while(T--){\n        memset(DP,-1,sizeof(DP));\n        cin>>first;\n        cin>>second;\n        scanf(\"%d %d %d\",&a,&b,&k);\n        add_cost=a;\n        delete_cost=b;\n        replace_cost=k;\n        //ans=Transform(0,0);\n        //if(ans<=k)\n            printf(\"%d\\n\",ans );\n        //else\n        //  printf(\"%d\\n\",-1);\n    }\nreturn 0;\n}\n```\n\n    ", "Answer": "\r\nRight, because your ```\n100000 * 100000```\n array of 32-bit integers takes up 40 gigabytes of memory.\nYou need to use a different algorithm.  If you only need to compute the edit distance up to a certain maximum ```\nk```\n, there is a modified version of the classic algorithm that only uses ```\nO(n * (2k + 1))```\n storage (where ```\nn```\n is the string length) because it only uses the middle ```\n2k + 1```\n diagonals of the dynamic programming matrix.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Tree Edit Distance: How can I get the optimal mapping?\r\n                \r\nI have implemented the algorithm by Zhang and Shasha to calculate the minimal edit distance between two trees. Everything is working fine and I am very pleased with the current running times.\n\nNow I would also like to generate a diff that highlights the changed / deleted / inserted nodes. According to their paper, it is very natural to ask for the mapping that yielded the computed distance and according to the last slides of this presentation it seems that the mapping can be easily extracted from the last forest distance table and the tree distance table. Unfortunately I haven't been able to figure out the exact rules yet.\n\nAny additional description would be highly appreciated. Many thanks!\n    ", "Answer": "\r\nOk, I have finally figured it out on my own. In order to generate the optimal mapping for the nodes of two trees, with m and n nodes respectively, you need to do some backtracking in the forest tables.\n\nFor each field (x, y) starting with (m, n) of the (m, n) forest distance table, do the following: if the minimum was obtained by summing up the field (x', y') and the edit / delete / insert costs, then write down the mapping and go to the field (x', y') of the current forest distance table. On the other hand, if the minimum was obtained by summing up the field (x', y') from the current forest distance table and the field (tx, ty) from the tree distance table, then go to the field (x', y') from the current forest distance table AND to the field (tx, ty) from the forest table corresponding to the tree (tx, ty). You now need to continue the backtracking in both forest tables individually and collect the mappings from both.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance Dynamic Programing for very large input\r\n                \r\nI am solving the well known Edit Distance Dynamic Programing Problem.Actually the problem is given two strings string1 and string2 and given the cost for deletion,insertion and replacement of the character,I have to convert string1 to string2 in minimum cost.For DP I have to use a two dimensional array.For a small string (size<=10000) my code is working but for a larger input(size>=100000) the compiler says \"array size is too large\". If the problem has to be solved using dynamic programing(for input size=100000) then please tell me how should I handle this error.Here is my code.\n\n```\n#include <iostream>\n#include <cstdio>\n#include <stdlib.h>\n#include <algorithm>\n#include <map>\n#include <queue>\n#include <iomanip>\n#include <string>\n#include <math.h>\n#include <limits>\n#include <map>\n#include <float.h>\n#include <limits.h>\n#include <string.h>\nusing namespace std;\n#define rep(i,a,N) for(int i=a;i<N;++i)\nint DP[10000][10000],delete_cost,add_cost,replace_cost;\nstring first,second;\nint Min(int x,int y,int z){\n    int min=x<y?x:y;\n    min=min<z?min:z;\n    return min;\n}\n\nint Transform(int i,int j){ \n    if(DP[i][j]!=-1){\n        //printf(\"DP is set\\n\");\n        return DP[i][j];\n    }\n    if(i==first.size())\n        return (second.size()-j)*add_cost;\n    if(j==second.size())\n        return (first.size()-i)*delete_cost;\n    if(first.at(i)!=second.at(j)){\n        int add,del,rep;\n        add=Transform(i,j+1)+add_cost;\n        del=Transform(i+1,j)+delete_cost;\n        rep=Transform(i+1,j+1)+replace_cost;\n        return DP[i][j]=Min(add,del,rep);\n    }\n    else\n        return DP[i][j]=Transform(i+1,j+1);\n\n}\n    int main(){\n    int T,a,b,k,ans;\n    scanf(\"%d\",&T);\n\n    while(T--){\n        memset(DP,-1,sizeof(DP));\n        cin>>first;\n        cin>>second;\n        scanf(\"%d %d %d\",&a,&b,&k);\n        add_cost=a;\n        delete_cost=b;\n        replace_cost=k;\n        //ans=Transform(0,0);\n        //if(ans<=k)\n            printf(\"%d\\n\",ans );\n        //else\n        //  printf(\"%d\\n\",-1);\n    }\nreturn 0;\n}\n```\n\n    ", "Answer": "\r\nRight, because your ```\n100000 * 100000```\n array of 32-bit integers takes up 40 gigabytes of memory.\nYou need to use a different algorithm.  If you only need to compute the edit distance up to a certain maximum ```\nk```\n, there is a modified version of the classic algorithm that only uses ```\nO(n * (2k + 1))```\n storage (where ```\nn```\n is the string length) because it only uses the middle ```\n2k + 1```\n diagonals of the dynamic programming matrix.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Minimum edit distance of two documents\r\n                \r\nI know how to get the minimum edit distance of two short strings like 'abcde' and 'abfde',but how to do the MED of two documents or essays with spaces , tabs , enters or even two codes?\nFor example:\ntext1:\nThe computer learns from a huge database of four million videos from volunteers and paid-for market\nresearchers in various emotional states and the algorithms are constantly updated and tested against real-world\nscenarios.\nThe next stage is to integrate voice analysis and other measures of physical wellbeing such as heart rate and\nhand gestures.\nand\ntext2:A computer model has been developed that can predict what word you are thinking of. The model may help to\nresolve questions about how the brain processes words and language, and might even lead to techniques for\ndecoding people’s thoughts.\nResearchers led by Tom Mitchell of Carnegie Mellon University in Pittsburgh, Pennsylvania, 'trained' a computer\nmodel to recognize the patterns of brain activity associated with 60 images, each of which represented a\ndifferent noun, such as 'celery' or 'aeroplane'.\nThis is the code I wrote of finding the MED of two strings within 20 characters.\n`\n```\nint med_lev(char S[], char T[]) {\n    int dis_lev[20][20];\n    int n = strlen(S);\n    int m = strlen(T);\n\n    for (int i = 0; i <= n; i++) {\n        dis_lev[i][0] = i;\n    }\n    for (int j = 0; j <= m; j++) {\n        dis_lev[0][j] = j;\n    }\n    for (int i = 1; i <= n; i++) {\n        for (int j = 1; j <= m; j++) {\n            if (S[i - 1] == T[j - 1]) {\n                dis_lev[i][j] = dis_lev[i - 1][j - 1];\n            }\n            else {\n                dis_lev[i][j] = min(dis_lev[i - 1][j - 1] + 2, min(dis_lev[i - 1][j] + 1, dis_lev[i][j - 1] + 1));\n            }\n        }\n    }\n    return dis_lev[n][m];\n}\n```\n\n`\nI've thought out a method that is delete all spaces , tabs , enters and put every word in a single string but the problem is the string might be too long. Is there a better way?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "solr fuzzy search with edit distance above 1\r\n                \r\nEnviornment- java version \"11.0.12\" 2021-07-20 LTS, solr-8.9.0\nI have the following field declaration for my Solr index:\n```\n<field name=\"Field1\" type=\"string\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>\n<field name=\"author\" type=\"text_general\" multiValued=\"false\" indexed=\"true\" stored=\"true\"/>\n<field name=\"Field2\" type=\"string\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/>\n```\n\nField type:\n```\n<fieldType name=\"text_general\" class=\"solr.TextField\" positionIncrementGap=\"100\" multiValued=\"true\">\n    <analyzer type=\"index\">\n      <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n      <filter class=\"solr.LowerCaseFilterFactory\"/>\n    </analyzer>\n    <analyzer type=\"query\">\n      <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n      <filter class=\"solr.LowerCaseFilterFactory\"/>\n    </analyzer>\n  </fieldType>\n```\n\nSolr-core has been created using command : ./solr create -c fuzzyCore\nThe .csv file used to indexed the data is https://drive.google.com/file/d/1z684x2GKsSQWGAdyi6O4uKit4a96iiuh/view\nI understand that \"Lucene supports fuzzy searches based on the Levenshtein Distance, or Edit Distance algorithm. To do a fuzzy search the tilde, \"~\", symbol at the end of a Single word Term is used.\n~ operator is used to run fuzzy searches. We need to add ~ operator after every single term and can also specify distance which is optional after that as below.\"\n```\n{FIELD_NAME:TERM_1~{Edit_Distance}\n```\n\nSince 'KeywordTokenizer' keeps the whole input as a single token and I want each word to be searchable, so 'StandardTokenizer' is used.\nrequest looks like as mentioned below :\n```\n    curl \"http://localhost:8983/solr/fuzzyCore/select\" --data-urlencode \"q=author:beaeb~' AND Field1:(w1 x)\" --data-urlencode \"rows=20\"\n{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":14,\n    \"params\":{\n      \"q\":\"author:beaeb~' AND Field1:(w1 x)\",\n      \"rows\":\"20\"}},\n  \"response\":{\"numFound\":12,\"start\":0,\"numFoundExact\":true,\"docs\":[\n      {\n        \"Field1\":\"x\",\n        \"author\":\"bbaeb\",\n        \"Field2\":\"o\",\n        \"id\":\"f8fbb58d-9e0d-47b2-aa3c-e3920e25a7d1\",\n        \"_version_\":1746912583192936455},\n      {\n        \"Field1\":\"x\",\n        \"author\":\"beabe\",\n        \"Field2\":\"p\",\n        \"id\":\"7d73e7ba-8455-4eb4-818f-1e19b1d35a22\",\n        \"_version_\":1746912583244316680},\n      {\n        \"Field1\":\"x\",\n        \"author\":\"baeeb\",\n        \"Field2\":\"n\",\n        \"id\":\"b4e86fc3-7ecc-407b-b638-88d167a66934\",\n        \"_version_\":1746912583292551181},\n      {\n        \"Field1\":\"x\",\n        \"author\":\"beaea\",\n        \"Field2\":\"o\",\n        \"id\":\"131ad4de-eaa2-47b8-b58b-e690316eed1c\",\n        \"_version_\":1746912583314571267},\n      {\n        \"Field1\":\"x\",\n        \"author\":\"bbaeb\",\n        \"Field2\":\"q\",\n        \"id\":\"d034e66c-a302-4b24-a186-5a2bafecab40\",\n        \"_version_\":1746912583392165900},\n      {\n        \"Field1\":\"x\",\n        \"author\":\"beacb\",\n        \"Field2\":\"n\",\n        \"id\":\"c0ab3e48-2b2d-438d-8cc2-1acfcf6efde8\",\n        \"_version_\":1746912583490732036},\n      {\n        \"Field1\":\"x\",\n        \"author\":\"aeabe\",\n        \"Field2\":\"m\",\n        \"id\":\"4472ec5d-eace-446f-b1d6-c8911be24368\",\n        \"_version_\":1746912583266336776},\n      {\n        \"Field1\":\"x\",\n        \"author\":\"baeab\",\n        \"Field2\":\"q\",\n        \"id\":\"b4c24da3-9199-4eba-a8a3-e30fc17d9167\",\n        \"_version_\":1746912583274725377},\n      {\n        \"Field1\":\"x\",\n        \"author\":\"aeaea\",\n        \"Field2\":\"n\",\n        \"id\":\"bb17bc26-e392-4fed-ae46-bbdd40af0ac0\",\n        \"_version_\":1746912583294648329},\n      {\n        \"Field1\":\"x\",\n        \"author\":\"aeceb\",\n        \"Field2\":\"p\",\n        \"id\":\"5e5cfe21-ff19-464f-8adf-8b5888c418e4\",\n        \"_version_\":1746912583296745472},\n      {\n        \"Field1\":\"x\",\n        \"author\":\"baeab\",\n        \"Field2\":\"p\",\n        \"id\":\"54a3c8e6-137d-47c3-9192-a5ed1904dc55\",\n        \"_version_\":1746912583357562889},\n      {\n        \"Field1\":\"x\",\n        \"author\":\"aeeeb\",\n        \"Field2\":\"m\",\n        \"id\":\"200694a0-6248-49fd-8182-dac79657e045\",\n        \"_version_\":1746912583385874444}]\n  }}\n```\n\n,\nThe above request is not retrieving output as 'author:bebbeb',although there is author:'bebbeb' is present in data with Field1:w1. This can be\nverified with following two commands\n```\ncurl \"http://localhost:8983/solr/fuzzyCore/select\" --data-urlencode \"q=author:beaeb~' AND Field1:w1\"\n{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":4,\n    \"params\":{\n      \"q\":\"author:beaeb~' AND Field1:w1\"}},\n  \"response\":{\"numFound\":0,\"start\":0,\"numFoundExact\":true,\"docs\":[]\n  }}\n```\n\nAlthough output of following command is\n```\ncurl \"http://localhost:8983/solr/fuzzyCore/select\" --data-urlencode \"q=Field1:w1\"\n{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":1,\n    \"params\":{\n      \"q\":\"Field1:w1\"}},\n  \"response\":{\"numFound\":1,\"start\":0,\"numFoundExact\":true,\"docs\":[\n      {\n        \"Field1\":\"w1\",\n        \"author\":\"bebbeb\",\n        \"Field2\":\"p\",\n        \"id\":\"4356dff2-ab93-4bab-a4dc-1797db38240c\",\n        \"_version_\":1746912583504363523}]\n  }}\n```\n\nso I tried to post everything you need to understand my problem. Any ideas? Why author:'bebbeb' is not resulting as output for input:beaeb~\n    ", "Answer": "\r\nAfter debugging Lucene we discovered that there is a parameter called ```\nmaxExpansions```\n set to 50 by default, which could be extended to 1024.\nHowever, looking at the Solr code, we can see that the ```\nFuzzyQuery```\n constructor is only called twice and always uses the default ```\nmaxExpansions```\n value (for performance reasons); this means fuzzy searches take at most the 50 most similar terms and discard the others. That's why when many documents are indexed and most of the terms are similar (as in your case), some documents may not be returned.\nA Solr open-source contribution would be needed to expose this parameter and make the use of this feature more flexible (allowing different values to be set).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit-based distance(matching) with custom character substitution distance\r\n                \r\nI want to match a string with another string from OCR(Optical Character Recognition).\nUsually, OCR-read text are imperfect. In my case, 5's are misrecognized as S and so on.\nSo I am wondering if there's a way to calucate a edit-distance with custom distance.\nFor example, if I want to calculate a distance from ```\n5S00AS```\n to ```\nSSOODS```\n,\nI would want to make a substitution distance for ```\nA```\n to ```\nD```\n large and ```\n5```\n to ```\nS```\n small so that ```\ndistance('5S00AS', 'SSOOAS')```\n is much smaller than ```\ndistance('5S00AS', 'PDDDDA')```\n.\nI think soundex is in the similar vein except that similar sounds have smaller distance. We should have smaller distance for simliar looking spellings.\nI wonder if there is already a function or package for doing this type of distance calculation.\n    ", "Answer": "\r\nYou are looking for Levenshtein distance with a custom scores. See the more general https://en.wikipedia.org/wiki/Needleman–Wunsch_algorithm\nThe nltk lib levenshein implementation doesn't support using custom scores, but in needleman-wunsch implemenations you can usually specify this (it's easy to modify the algorithm to use a matrix of custom distances).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Problems with Networkx optimal_edit_paths (graph edit distance)\r\n                \r\nI'm using Networkx to find the graph edit distance (GED) between two directed acyclic graphs (DAGs) via the ```\nReconcile```\n method shown below, for the purpose of tree reconciliation. I map nodes and edges by their labels, which are (as shown) just their IDs.\n\nIn the test case below, I copy a graph ```\ng1```\n to ```\ng2```\n and add ```\nn```\n new nodes/edges to ```\ng2```\n then remove ```\nm```\n nodes/edges. Even in the case of ```\nn```\n = several hundred, the ```\nReconcile```\n method is very fast when ```\nm```\n = 0. However, my problem is that it becomes very slow if I remove more than 2 or 3 nodes that would otherwise be common to both graphs.\n\n```\nimport networkx as nx\n\nclass MyGraph(nx.DiGraph):\n\n    @classmethod\n    def Reconcile(cls, a1, a2):\n\n    # ---------------------------------------------------------------------\n    # STAGE 1: MAP NODES/EDGES B/T THE TWO GRAPHSS\n\n    # Equality here means having same labels\n\n        def return_eq(item1, item2):\n\n            label1 = item1['label']\n            label2 = item2['label']\n\n            _eq = label1 == label2\n\n            if _eq:\n                print('Mapped ', label1, 'to ', label2)\n\n            return _eq\n\n\n        # ---------------------------------------------------------------------\n        # STAGE 2: FIND EDIT PATHS\n\n        paths, cost_nx = nx.optimal_edit_paths(a1, a2, node_match = return_eq, edge_match = return_eq)\n        paths = paths[0]\n\n        node_edits = [el for el in paths[0] if el[0] != el[1]]\n        edge_edits = [el for el in paths[1] if el[0] != el[1]]\n        cost = len(node_edits) + len(edge_edits)\n\n        print('Node edits: {}\\nEdge edits: {}\\nTotal cost (Networkx): {}\\nTotal cost (no. of edits): {}'.format(\n            node_edits, edge_edits, cost_nx, cost))\n        print('No. of node edits: ', len(node_edits))\n        print('No. of edge edits: ', len(edge_edits))\n\n        return paths, cost_nx, cost, node_edits, edge_edits\n\n#######################################################\n\ng1 = MyGraph()\n\np = 10\nedges = [('root', edge) for edge in range(p)]\ng1.add_edges_from(edges) # Add root and some nodes + edges\n\ng2 = g1.copy()\n\nn = 10\nfor edge in range(n):\n    node = 100*(edge+10)\n    print('Edge added: ', edge)\n    g2.add_edge('root2', node) # Add some more edges + nodes not present in g1\n\nm = 2 # When m > 2, code slows massively\nfor node in list(g1.nodes)[-m:]: # Any will do!\n    print('Node/edge removed:', node)\n    g2.remove_node(node)\n\nfor g in [g1,g2]:\n    for node in g.nodes:\n        g.nodes[node]['label'] = node\n    for edge in g.edges:\n        g.edges[edge]['label'] = edge\n\npaths, cost_nx, cost, node_edits, edge_edits = MyGraph.Reconcile(g1,g2)\n```\n\n\nI may be missing something here, and it may be that the problem is simply a computational one: I realise it's a hard problem and I'm absolutely not suggesting there's a bug in Networkx. However, any help or insight would be much appreciated, for example by preconditioning the graphs in some way before attemping to reconcile them.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Why my code doesn't print the right command of the edit distance?\r\n                \r\nI'm writing an edit distance program that print the right sequenze of command (ADD,DEL,SET).\nthis is my code:\n```\nint start(char *path,char *path2) {\nchar *file1=openFile(path),*file2=openFile(path2);\nlong int dim1=calcLen(path),dim2 =calcLen(path2);\nregister int i, j;\nint *prev = malloc((dim2 + 1) * sizeof(int));\nint *curr = malloc((dim2 + 1) * sizeof(int));\nint *tmp = 0;\nfor(i=0;i<=dim2;i++)prev[i]=i;\n\n//_________________________________________________________________//\nfor (i = 1; i <= dim1; i++) {\n    curr[0] = i;\n    for (j = 1; j <= dim2; j++) {\n\n        if (file1[i - 1] != file2[j - 1]) {\n            int k = minimum(curr[j - 1], prev[j - 1], prev[j]);\n            curr[j] = k + 1;\n            if(k==prev[j-1]){\n                printf(\"SET\\n\");\n            }else if(k==prev[j]){\n                printf(\"ADD\\n\");\n            }else if(k==curr[j-1]){\n                printf(\"DEL\\n\");\n            }\n        } else {\n            curr[j] = prev[j - 1];\n        }\n\n    }\n\n    tmp = prev;\n    prev = curr;\n    curr = tmp;\n\n    memset((void *) curr, 0, sizeof(int) * (dim2 + 1));\n  }\n}\n```\n\nThe two strings are:\n\nhello\ncammelloh\n\nAnd it print me:\n```\nSET\nSET\nSET\nSET\nSET\nSET\nSET\nSET\nSET\nSET\nSET\nSET\nDEL\nDEL\nDEL\nDEL\nSET\nSET\nSET\nSET\nSET\nDEL\nDEL\nSET\nSET\nSET\nSET\nSET\nDEL\nDEL\nSET\nSET\nSET\nSET\nSET\nSET\nADD\nDEL\n\nProcess finished with exit code 0\n```\n\nBut if i a method that calculate the distance between this two strings it prints me:\n```\nEDIT DISTANCE: 5\nTIME: 0.002000\nProcess finished with exit code 0\n```\n\nI realy don't understan how it do that, I think it prints a lot of commands because the second for. But I think that if the algorith is wrong all the program might be wrong and not only the method \"start\".\nThis is calcLen:\n```\nint calcLen(char *path) {\nFILE *fp = fopen(path, \"r\");\nif (fp == NULL) {\n    printf(\"File Not Found!\\n\");\n    return -1;\n}\nfseek(fp, 0L, SEEK_END);\nlong int res = ftell(fp);\nfclose(fp);\nreturn res;\n}\n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "how to configure solr / lucene to perform levenshtein edit distance searching?\r\n                \r\ni have a long list of words that i put into a very simple SOLR / Lucene database. my goal is to find 'similar' words from the list for single-term queries, where 'similarity' is specifically understood as (damerau) levensthein edit distance. i understand SOLR provides such a distance for spelling suggestions.\n\nin my SOLR ```\nschema.xml```\n, i have configured a field type ```\nstring```\n:\n\n```\n<fieldType name=\"string\" class=\"solr.StrField\" sortMissingLast=\"true\" omitNorms=\"true\"/>\n```\n\n\nwhich i use to define a field\n\n```\n<field name='term' type='string' indexed='true' stored='true' required='true'/>\n```\n\n\ni want to search this field and have results returned according to their levenshtein edit distance. however, when i run a query like ```\nwebspace~0.1```\n against SOLR with debugging and explanations on, the report shows that a whole bunch of considerations went into calculating the scores, e.g.:\n\n```\n\"1582\":\"\n1.1353534 = (MATCH) sum of:\n  1.1353534 = (MATCH) weight(term:webpage^0.8148148 in 1581), product of:\n    0.08618848 = queryWeight(term:webpage^0.8148148), product of:\n      0.8148148 = boost\n      13.172914 = idf(docFreq=1, maxDocs=386954)\n      0.008029869 = queryNorm\n    13.172914 = (MATCH) fieldWeight(term:webpage in 1581), product of:\n      1.0 = tf(termFreq(term:webpage)=1)\n      13.172914 = idf(docFreq=1, maxDocs=386954)\n      1.0 = fieldNorm(field=term, doc=1581)\n```\n\n\nclearly, for my application, term frequencies, ```\nidf```\ns and so on are meaningless, as each document only contains a single term. i tried to use the spelling suggestions component, but didn't manage to make it return the actual similarity scores. \n\ncan anybody provide hints how to configure SOLR to perform levensthein / jaro-winkler / n-gram searches with scores returned and without doing additional stuff like ```\ntf```\n, ```\nidf```\n, ```\nboost```\n and so included? is there a bare-bones configuration sample for SOLR somewhere? i find the number of options truly daunting.\n    ", "Answer": "\r\nIf you're using a nightly build, then you can sort results based on levenshtein distance using the strdist function:\n\n```\nq=term:webspace~0.1&sort=strdist(\"webspace\", term, edit) desc\n```\n\n\nMore details here and here\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Distance measure of sequences; string edit\r\n                \r\nI would like to compare two eye-tracking scanpaths.\nThe eye-tracking results in a sequence of labels that the observer look at, for a division of an image into labeled tiles (rectangular regions). We also know from the eye-tracking at what time, and for how long the eye look at tile N.\n\nThe Levenshtein or string edit distance works fine, as long as the timing of the fixations are not taken into account. For example, f user 1 looks at the tiles \"AKPLA\", and user 2 looks at tiles \"ATPLB\" the string edit distance will be 2, but user 2 might look at \"P\" in a much longer time than user 2.  \n\nAny ideas of how to improve the  distance measure to measure timing differences as well?\n(note that the algorithm is not restricted to character strings, it works equally well with arrays of integers).\n    ", "Answer": "\r\nAn eye-tracking scanpaths originally would be a time-series. Transforming your time-series into a string only containing the labels where the person looked at leads to loosing information about time. \n\nThus if you want to take the time into account, you have to either work with the original time-series or take the time into account for your transformation. \n\nFor example: You could for every ten seconds give the laben where the person looked at on average. It could be \"AAAAKPLAA\" as compared to \"AATTTPLBB\". In this case you could use the Edit Distance and it would take into account how long someone looked where.\n\nYou could also simply work on the original time-series of eye-tracking which - as I assume - contains a time-stamp and a position. Then you could use Dynamic Time Warping to estimate the dissimilarity. \n\nAnyhow, this is a very broad question and probably of no relevance for you any longer. If you could post the answer that you found yourself, it would be great. \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to find all strings at a given edit distance from a given string\r\n                \r\nWe all have seen in Google, that if we type a query, and make a typo, Google suggests a saner version of the query (which is correct more often than not). Now how do they do it? One possible way I can think of is find out all other strings at an edit distance of 1 from the given string, and if any on of them returns a string with a higher value 'searched` attribute (might come from back-end DB, where each indexed query term has a weight associated with it based on how frequently that term crops up in queries) than the given string, that string is suggested. If none are found, then strings with an edit distance of 2 are searched, and so on, until, say at 5, the SE decides that may be this string is the one the user is looking for, and returns the corresponding search results.\n\nNow is it possible at all to find strings at a given edit distance from a given string? How efficient would that be for this process? Is there any cool algorithm to do this?\n    ", "Answer": "\r\nThere is interesting article of Peter Norvig \"How to Write a Spelling Corrector\" talking about how \"Do you mean\" might work\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Why the definition of edit Distance algorithm in Stanford NLP course plus 2 not 1\r\n                \r\nI am studying the Stanford NLP course by the following slides:https://web.stanford.edu/class/cs124/lec/med.pdf.\nThe definition of edit Distance algorithm in this slide as following:\n\nInitialization\n\n```\nD(i,0) = i\nD(0,j) = j\n```\n\n\nRecurrence Relation:\n\n```\n For each  i = 1…M\n    For each  j = 1…N\n\n\n       D(i,j)= min  {D(i-1,j) + 1, D(i,j-1) + 1, \n                     D(i-1,j-1) +   2(if X(i) ≠ Y(j) )  \n                                    0(if X(i) = Y(j))}\n```\n\n\nwhy D(i-1,j-1) + 2 not (+1), if X(i) ≠ Y(j). \nI found the definition of edit Distance algorithm in the wikipedia is '+1':https://en.wikipedia.org/wiki/Levenshtein_distance.\nCould you guys explain the difference of these two definition, please. I am a new guy to NLP. Thanks!\n    ", "Answer": "\r\n\n  When editing one string, what is the minimal number of changes you need to do in order to get another string?\n\n\nThis is a general, not specific, definition for editing distance. To get a precise definition, you need to define what a \"change\" is.\n\n\nIf a \"change\" includes \"replacing one letter by another\", you have +1 in your definition\nIf a \"change\" doesn't include \"replacing one letter by another\", you have +2 in your definition\n\n\nExample: how many changes do you need to turn ```\nfeh```\n into ```\nfah```\n?\n\n\nOne change - just replace ```\ne```\n by ```\na```\n\nTwo changes - remove ```\ne```\n; then add ```\na```\n in the same place\n\n\nBoth answers are useful, and lead to two slightly different definitions of editing distance.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "exact matching between two strings - linear edit distance?\r\n                \r\nThis is the problem, given a string with characters from: ```\na-z```\n, ```\n.```\n, ```\n*```\n, and another string with characters from ```\na-z```\n. where ```\n*```\n can delete the character before it, otherwise * is skipped and ```\n.```\n can match any single character. the question is whether the first string can match the second one.\n\nNote: That is the statement of the problem as I found, but in this case the character ```\n*```\n performs the same function that ```\n?```\n in a regular expression. \n\nExample:\n\n```\nisMatch(\"a*\", \"\") = true; //\"a*\" could be \"a\" or an empty string \"\"\nisMatch(\".\", \"\") = false; \nisMatch(\"ab*\", \"a\") = true; \nisMatch(\"a.\", \"ab\") = true; \nisMatch(\"a\", \"a\") = true;\n```\n\n\nI've already solved this problem using a slightly modified edit distance, which I only know a 2D dynamic programming approach. I wonder whether exists a linear solution for this problem, maybe it is solvable without a dp approach?\n    ", "Answer": "\r\nThanks to @RealzSlaw for the advice. My question was looking for a linear solution and it seems to be impossible, simply because of this case(now with regex syntax):\n\n```\nisMatch(\"a?a?b?b?b?a?a?b?a?\",\"abbab\")\n```\n\n\nIt asks whether ```\nabbab```\n is a subsequence of ```\naabbbaaba```\n and It's a NP-hard as far as I know, so linear solution seems to be infeasible. \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Lucene Search based on edit-distance on entire text rather than individual tokens\r\n                \r\nI am using SpanNearQuery with SpanMultiTermQueryWrapper to match my query text with an edit-distance of either 1 or 2 containing more than a word with the documents each containing multiple tokens\nHere I need to specify the edit distance for each individual token in the query text which works pretty well!\nHowever, Is there a way to search the document based on the edit-distance of either 1 or 2 on the entire query-text rather than specifying for each individual tokens?\nFor example, this is the current setup: (Not the exact query-syntax, just for simplicity)\nFor query \"bread basket\" - \"bread~2 : basket~2\", but I am expecting something like \"bread basket~2\".\nIndexing Method:\nI am using StandardAnalyzer to index my multi-termed documents\nBasically, I am looking to do word segmentation. If the input-query is \"the breadbasket\", it should match with the document \"the bread basket\". Let me know if there exists any hack to achieve this.\nAny help would be appreciated. Thanks in advance!\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is there any modified Minimum Edit Distance (Levenshteina Distance ) for incomplete strings?\r\n                \r\nI've sequences builded from 0's and 1's. I want to somehow measure their distance from target string. But target string is incomplete.\n\nExample of data I have, where x is target string, where [0] means the occurance of at least one ```\n'0'```\n : \n\n```\nx =11[0]1111[0]1111111[0]1[0]`, the length of x is fixed and eaquel to length of y.\n\ny1=11110111111000000101010110101010111\n\ny2=01101000011100001101010101101010010\nall y's have the same length\n```\n\n\nit's easy to see that ```\nx```\n could be indeed interpreted as set of strings, but this set could be very large, mayby simply I need to sample from that set and take average of minimum edit distances, but again it's too big computional problem. \n\nI've tried to figure out algo, but I'm stacked, it steps look like this :\nx - target string - fuzzy one,\n\ny - second string - fixed\nCx1, Cy1 - numbers of ones in x and y\nGx1, Gy1 - lists of vectors, length of each list is equal to number of groups of ones in given sequence, \n\nGx1[i] i-th vector,\n\nGx1[i]=(first one in i-th group of ones, length of i-th group of ones)\n\nif lengths of Gx1 and Gy1 are the same then we know how many ones to add or remove from each group, but there's a problem, because I don't know if simple adding and removing gives minimum distance\n    ", "Answer": "\r\nLet (Q, Σ, δ, q0, F) be the target automaton, which accepts a regular language L &subseteq; Σ*, and let w &in; Σ* be the source string. You want to compute minx &in; L d(x, w), where d denotes Levenshtein distance.\n\nMy approach is to generalize the usual dynamic program. Let D be a table indexed by Q × {0, …, |w|}. At the end of the computation, D(q, i) will be\n\nminx : δ(q0, x) = q d(x, w[0…i]),\n\nwhere w[0…i] denotes the length-(i + 1) prefix of w. In other words, D(q, i) is the distance between w[0…i] and the set of strings that leave the automaton in state q. The overall answer is\n\nminq &in; F D(q, |w|),\n\nor the distance between w and the set of strings that leave the automaton in one of the final states, i.e., the language L.\n\n\n\nThe first column of D consists of the entries D(q, 0) for every state q &in; Q. Since for every string x &in; Σ* it holds that d(x, ε) = |x|, the entry D(q, 0) is the length of the shortest path from q0 to q in the graph defined by the transition function δ. Compute these entries by running \"Dijkstra's algorithm\" from q0 (actually just breadth-first search because the edge-lengths are all 1).\n\nSubsequent columns of D are computed from the preceding column. First compute an auxiliary quantity D'(q, i) by minimizing over several possibilities.\n\nExact match For every state r &in; Q such that δ(r, w[i]) = q, include D(r, i - 1).\n\nDeletion Include D(q, i - 1) + 1.\n\nSubstitution For every state r &in; Q and every letter a &in; Σ &setminus; {w[i]} such that δ(r, a) = q, include D(r, i - 1) + 1.\n\nNote that I have left out Insertion. As with the first column, this is because it may be necessary to insert many letters here. To compute the D(i, q)s from the D'(i, q)s, run Dijkstra on an implicit graph with vertices Q ∪ {s} and, for every q &in; Q, edges of length D'(i, q) from the super-source s to q and, for every q &in; Q and a &in; Σ, edges of length 1 from q to δ(q, a). Let D(i, q) be the final distances.\n\n\n\nI believe that this algorithm, if implemented well (with a heap specialized to support Dijkstra with unit lengths), has running time O(|Q| |w| |Σ|), which, for small alphabets Σ, is comparable to the usual Levenshtein DP.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein edit distance algorithm that supports Transposition of two adjacent letters in C#\r\n                \r\ni'm searching for an algorithm for computing Levenshtein edit distance that also supports the case in which two adjacent letters are transposed that is implemented in C#. \n\nfor example the word \"animals\"  and  \"ainmals\" :\nswitching between the letters \"n\" and \"i\" \nwont be scored as two replacements -which will make a big distance -\nbut instead on will be scored as a transpose of two letters -much more less distance-\n\nwhat i reached so far in searching \n\n\ncomputing Lichtenstein distance  but it doesn't contain replacements\nthis question \n\n    ", "Answer": "\r\nSee the implementation on Wikipedia. You can easily adapt the algorithm to include the case for letter swaps. For example:\n\n```\n//bla bla. I'm just copying the code on the Wikipedia.\n d[i, j] := minimum\n                   (\n                     d[i-1, j] + 1,  // a deletion\n                     d[i, j-1] + 1,  // an insertion\n                     d[i-1, j-1] + 1, // a substitution\n                   )\n\n// This single statement is all you need:\nif(s[i-1]==t[j-2] && s[i-2]==t[j-1])\n   d[i,j] := minimum\n                  (\n                      d[i,j],               //cost without swapping \n                      d[i-2,j-2]+something  //cost with swapping. probably something=1 \n                  );\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance Java\r\n                \r\nI wrote this algorithm to calculate the sum of the number of deletion and insert (so, of the edits) to make the first string equals to the second one. But it doesn't work.\n\n```\npublic static int distance (String s1, String s2) {\n    return distance(s1, s2, 0, 0);\n}\n\nprivate static int distance(String s1, String s2, int i, int j) {\n    if (i == s1.length) return j;\n    if (j == s2.length) return i;\n    if (s1.charAt(i) == s2.charAt(j))\n        return distance(s1, s2, i + 1, j + 1);\n    int rep = distance(s1, s2, i + 1, j + 1) + 1;\n    int del = distance(s1, s2, i, j + 1) + 1;\n    int ins = distance(s1, s2, i + 1, j) + 1;\n    return Math.min(del, Math.min(ins, rep));\n}\n```\n\n\nEDIT: Example\nString 1: \"casa\"\nString 2: \"cara\"\nedit_distance=2 (1 deletion + 1 insert)\n\nEDIT2: \nThese are the strings that work: \nString 1: \"casa\", String 2: \"cassa\", edit_distance=1;\nString 1:\"pioppo\", String 2: \"pioppo\", edit_distance=0;\n\nThese are the one which doesn't work:\nString 1: \"casa\", String 2: \"cara\", edit_distance=2; (in my code=0)\nString 1: \"tassa\", String 2: \"passato\", edit_distance=4; (in my code=2)\n    ", "Answer": "\r\nI think that the implementation is almost correct and you missed the stop conditions. They should be:\n\n```\nif (j == s2.length()) {\n    return s1.length() - i;\n}\nif (i == s1.length()) {\n    return s2.length() - j;\n}\n```\n\n\nSo the full implementation should be:\n\n```\nprivate static int distance(String s1, String s2, int i, int j) {\n    if (j == s2.length()) {\n        return s1.length() - i;\n    }\n    if (i == s1.length()) {\n        return s2.length() - j;\n    }\n    if (s1.charAt(i) == s2.charAt(j))\n        return distance(s1, s2, i + 1, j + 1);\n    int rep = distance(s1, s2, i + 1, j + 1) + 2; // since Jim Belushi considers replacement to be worth 2.\n    int del = distance(s1, s2, i, j + 1) + 1;\n    int ins = distance(s1, s2, i + 1, j) + 1;\n    return Math.min(del, Math.min(ins, rep));\n}\n```\n\n\nUpdate\n\nHere  is the result for \"tassa\" and \"passato\":\n\nCode:\n\n```\nprivate static int distance(String s1, String s2, int i, int j) {\n    if (j == s2.length()) {\n        return s1.length() - i;\n    }\n    if (i == s1.length()) {\n        return s2.length() - j;\n    }\n    if (s1.charAt(i) == s2.charAt(j))\n        return distance(s1, s2, i + 1, j + 1);\n    int rep = distance(s1, s2, i + 1, j + 1) + 2;\n    int del = distance(s1, s2, i, j + 1) + 1;\n    int ins = distance(s1, s2, i + 1, j) + 1;\n    return Math.min(del, Math.min(ins, rep));\n}\n\npublic static void main(String[] args) {\n    int dist = distance(\"tassa\", \"passato\", 0, 0);\n    System.out.println(dist);\n}\n```\n\n\nIf you run this you get:\n\n```\n4\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance between all the columns of a pandas dataframe\r\n                \r\nI am interested in calculating the edit distances across all the columns of a given pandas DataFrame. Let's say we have a 3*5 DataFrame - I want to output something like this with the distance scores -  (column*column matrix)\n\n```\n  col1  col2 col3 col4 col5\n```\n\n\ncol1 \n\ncol2\n\ncol3\n\ncol4\n\ncol5\n\nI want each element of a column to match with every element of the other columns. Therefore, for every col1*col2 cell = summation of all the scores of the nested loop of col1 and col2. \n\nI would highly appreciate any help in this regards. Thanks in advance. \n\n\n\nINSPECTION_ID  STRUCTURE_ID  RELOCATE_FID HECO_ID  HECO_ID_TAG_NOT_FOUND  \\\n0           100         95308           NaN   18/29                    0.0\n1           101         95346           NaN  Nov-29                    0.0\n2           102      50008606           NaN   25/29                    0.0\n3           103         95310           NaN  Dec-29                    0.0\n4           104         95286           NaN   17/29                    0.0   \n\nOSMOSE_POLE_ID ALTERNATE_ID STREET_NBR STREET_DIRECTIONAL STREET_NAME  \\\n0             NaN          NaN       1888                NaN   KAIKUNANE\n1             NaN          NaN       1731                NaN   MAKUAHINE\n2             NaN          NaN       1862                NaN   MAKUAHINE\n3             NaN          NaN       1825                NaN   KAIKUNANE\n4             NaN          NaN       1816                NaN   KAIKUNANE   \n\nLikewise, I got a (191795, 58) dataset. My objective is to find the edit distance between each column of the dataset so as to understand the patterns between them if any. \n\nFor instance, I desire INSPECTION_ID  100 to be checked with all the values of column STRUCTURE_ID ans so on. I understand the need of an optimized iterator in this case. Kindly help me throwing some direction to solve this problem. Thanks in advance. \n    ", "Answer": "\r\nVery naive solution (assuming you already have an edit distance function) but might just work for small datasets\n\n```\ndf = # your dataset\ndef edit_distance(s1, s2):\n    # some code\n    # return edit distance of s1, s2\n\n\ndf_distances = []\nfor i, row in df.iterrows():\n    row_distances = []\n    for item in row:\n        for item2 in row:\n              row_distances.append(edit_distance(item, item2))\n    df_distances.append(some_array)\n```\n\n\nI haven't tested this solution so there might be bugs but the general principle should work. If you don't have an edit distance function, you can use this implementation \nhttps://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python or one of the many others freely available\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Substring Matching and Longest Common Subsequence as Variation of Edit Distance Problem -- Skiena\r\n                \r\nIn Algorithm Design Manual, edit distance is solved by the following algorithm\n```\n#define INSERT    1       /* enumerated type symbol for insert */\n#define DELETE    2       /* enumerated type symbol for delete */\n\nint string_compare(char *s, char *t, int i, int j)\n{\n        int k;                  /* counter */\n        int opt[3];             /* cost of the three options */\n        int lowest_cost;        /* lowest cost */\n\n        if (i == 0) return(j * indel(' '));\n        if (j == 0) return(i * indel(' '));\n\n        opt[MATCH] = string_compare(s,t,i-1,j-1) + match(s[i],t[j]);\n        opt[INSERT] = string_compare(s,t,i,j-1) + indel(t[j]);\n        opt[DELETE] = string_compare(s,t,i-1,j) + indel(s[i]);\n\n        lowest_cost = opt[MATCH];\n        for (k=INSERT; k<=DELETE; k++)\n                if (opt[k] < lowest_cost) lowest_cost = opt[k];\n\n        return( lowest_cost );\n}\n```\n\nI understand everything up to this point but am struggling to understand the following section where substring matching and longest common subsequence are solved as variations of the edit distance problem. I believe I kind of understand the intuition behind them, where the least amount of edits means preserving the \"sequences of interest\". In the case of substring matching, it is the substring; in the case of the longest common subsequence, it is that common subsequence. However, I don't understand how exactly each problem is solved.\nFor substring matching, following changes are made:\n```\nrow_init(int i)\n{\n    m[0][i].cost = 0; /* note change */\n    m[0][i].parent = -1; /* note change */\n}\ngoal_cell(char *s, char *t, int *i, int *j)\n{\n    int k; /* counter */\n    *i = strlen(s) - 1;\n    *j = 0;\n    for (k=1; k<strlen(t); k++)\n        if (m[*i][k].cost < m[*i][*j].cost) *j = k;\n    }\n}\n```\n\nFor longest common subsequence, the following change is made:\n```\nint match(char c, char d)\n{\n    if (c == d) return(0);\n    else return(MAXLEN);\n}\n```\n\nWould someone care to explain and help me understand this better?\n    ", "Answer": "\r\nThere is an explanation of the substring matching problem in Section 21.4 of the book.\nExplanation Screenshot -> https://i.stack.imgur.com/N56LP.png\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance algorithm explanation\r\n                \r\nAccording to wikipedia, the definition of the recursive formula which calculates the Levenshtein distance between two strings a and b is the following:\n\n\n\nI don't understand why we don't take into consideration the cases in which we delete ```\na[j]```\n, or we insert ```\nb[i]```\n. Also, correct me if I am wrong, isn't the case of insertion the same as the case of the deletion? I mean, instead of deleting a character from one string, we could insert the same character in the second string, and the opposite. So why not merge the insert/delete operations into one operation with cost equal to ```\nmin{cost_insert, cost_delete}```\n?\n    ", "Answer": "\r\nThis is not done, because you are not allowed to edit both strings. The definition of the edit distance (from wikipedia) is:\n\n\n  the minimum-weight series of edit operations that transforms a into b.\n\n\nSo you are specifically looking for (the weight of) a sequence of operations to execute on the string ```\na```\n to transform it into string ```\nb```\n.\n\nAlso, the edit distance is not necessarily symmetric. If your costs for inserts and deletions are identical the distance is symmetric: ```\nd(a,b) = d(b,a)```\n \n\nConsider the wikipedia example but with different costs:\n\n\ncosts for insertions: w_ins = 1\ncosts for deletions: w_del = 2\ncosts for substitutions: w_sub = 1\n\n\nThe distance of kitten and sitting still is 3,\n\n```\nkitten -> sitten  (substitution k->s, cost 1)\nsitten -> sittin  (substitution e->i, cost 1)\nsittin -> sitting (insertion of g, cost 1)\n=> d(kitten, sitting) = 3\n```\n\n\nbut the distance of sitting and kitten is not:\n\n```\nsitting -> kitting  (substitution s->k, cost 1)\nkitting -> kitteng  (substitution i->e, cost 1)\nkitteng -> kitten (deletion of g, cost 2)\n=> d(kitten, sitting) = 4\n```\n\n\nYou see that ```\nd(kitten, sitting) != d(kitten, sitting)```\n.\n\nOn the other hand if you do use symmetric costs, as the Levenshtein distance (which is an edit distance with unit costs) does, you can assume that ```\nd(a,b) = d(b,a)```\n holds. Then you do not win anything by also considering the inverse cases. What you lose is the information which character has been replaced in which string, what makes it harder to extract the sequence of operations afterwards.\n\nThe Wagner-Fisher algorithm which you are showing in your question can extract this from the DP matrix by backtracking the path with minimal costs. Consider this two edit matrices between to and foo with unit costs:\n\n```\n  t o       f o o\nf 1 2     t 1 2 3\no 2 1     o 2 1 2\no 3 2\n```\n\n\nNote that if you transpose the matrix for ```\nd(to, foo)```\n you get the matrix for ```\nd(foo, to)```\n. Note that by this, an insertion in the first matrix becomes a deletion in the second matrix and vice versa. So this is where this symmetry you are looking for is coming up again.\n\nI hope this helps :)\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Python Edit distance algorithm with dynamic programming and 2D Array - Output is not optimal\r\n                \r\nI have encountered the edit distance (Levenshtein distance) problem. I have looked at other similar stackoverflow questions, and is certain that my question is distinct from them - either from the language used or the approach.\nI have used a 2D array that compares the two strings, and dynamic programming to store previous values. If i and j in the string indices match, it would output 0, as we don't need to do anything; else, the output is 1. It is as shown in the picture below, the orange arrow represents a match.\n(Code below is edited after suggestions from answers)\n```\ndef edit_distance(source, target):\n    n = len(target)+1\n    m = len(source)+1\n\n    # let D denote the 2-dimensional array, m is the column, n is row\n    D = [ [0]*m for _ in range(n)] \n\n    # the loop inside is the target string, we operate this\n    # while the loop outside is the source string\n\n    for j in range(0, m):\n        for i in range(0, n):\n            if target[i-1] == source[j-1]:\n                # match, insertion and deletion, find the path with least move\n                D[i][j] = min(D[i-1][j-1], D[i-1][j]+1, D[i][j-1]+1)\n\n            else:\n                # mismatch, insertion and deletion, find the path with least move\n                D[i][j] = min(D[i-1][j-1]+1, D[i-1][j]+1, D[i][j-1]+1)\n            \n    return D[n-1][m-1]\n\nprint(edit_distance(\"distance\", \"editing\"))\n```\n\n\nHowever, the final output was 8 in my code, while the optimal editing distance between the strings \"editing\" and \"distance\" should be 5, and I am very confused. Could you please help with it from the approach of dynamic programming?\n    ", "Answer": "\r\nYou have 2 mistakes.\nFirst is intialization. You fill everything with 0s, but then when you want to fill in D[1][m] you look in the cell above (where it should be m) and you find a 0.  Make sure the borders are correctly filled in.\nSecond your iterations are off. range(1, n) over 'editing' will give you 'diting'. To fix it N and M by 1 (```\nn=len(target) + 1```\n) and in your comparison use ```\ntarget[i-1] == source[j-1]```\n.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Setting an upper bound for a Levenshtein (edit) distance in python\r\n                \r\nI need to calculate the edit distance between DNA sequences in order to group similar sequences together. If sequences have a distance that is larger than some threshold they are considered different and I don't care what the actual value is, so naturally I want to set an upper bound on the Levenshtein distance and for the function to stop when passing it. The vat majority of comparisons are supposed to end with that result.\n\nI found that there are some fast implementation in cython/C wrapped in python, for example python-Levenshtein and editdistance, but to my surprise nor these nor others I found have an option to set an upper bound, which I thought would be basic. I could use an all python implementation and modify it, but it would be much slower than a C implementation.\n\nDoes anyone know of a python tool that would allow that, or of a way to tweak the implementation so it would stop after some threshold?\n\nThanks\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein type algorithm with numeric vectors\r\n                \r\nI have two vectors with numeric values. Such as\n\n```\nv1 <- c(1, 3, 4, 5, 6, 7, 8)\nv2 <- c(54, 23, 12, 53, 7, 8)\n```\n\n\nI would like to compute the number of insertions, deletions and replacements that I need to turn one vector into the other with certain costs per operation c1 c2 and c3 respectively. I am aware that the function adist on the base package does this for strings but I have no knowledge of the equivalent function with numbers. \n\nI thought about referencing each number with a letter but I have more than 2000 unique numbers so if anybody knows how to get 2000 different characters in R that would also be a solution for me.\n\nThanks for your help.     \n    ", "Answer": "\r\nAn integer vector can be seen as a single string encoded in UTF-32 (in which one Unicode code point is represented as a single 32-bit integer). You may obtain an \"ordinary\" string, just by converting such a vector to UTF-8 with ```\nintToUtf8```\n.\n\n```\nintToUtf8(c(65, 97))\n## [1] \"Aa\"\n```\n\n\nBy the way, ```\nadist```\n does ```\nutf8ToInt```\n (reverse op) by default on its inputs anyway. So internally, it computes the results according to integer vectors. No big hack.\n\nThis is the solution.\n\n```\nadist(intToUtf8(c(1, 3, 4, 5, 6, 7, 8)), intToUtf8(c(54, 23, 12, 53, 7, 8)), counts=TRUE)\n##      [,1]\n## [1,]    5\n## attr(,\"counts\")\n## , , ins\n## \n##      [,1]\n## [1,]    0\n## \n## , , del\n## \n##      [,1]\n## [1,]    1\n## \n## , , sub\n## \n##      [,1]\n## [1,]    4\n## \n## attr(,\"trafos\")\n##      [,1]     \n## [1,] \"SSSSDMM\"\n```\n\n\nThe above code should work if at least all the numbers are strictly greater than 0.\nR treats Unicode code points quite liberally (in fact, too liberally, but in this case you're a winner), even the largest possible integer is accepted:\n\n```\nutf8ToInt(intToUtf8(c(2147483647)))\n## 2147483647\n```\n\n\nIf you have a vector with negative values, you may transform it somehow, e.g. with ```\nx <- x-min(x)+1```\n.\n\nIf you need different costs for insertion, removal, replacement, check out the ```\nadist's```\n ```\ncosts```\n argument. There is also a package called stringdist, which included many other string metrics. The above scheme should also work there.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance between the facet / strip and the plot\r\n                \r\nFor example,\n\n```\nlibrary(ggplot2)\nggplot(mpg, aes(displ, cty)) + geom_point() + facet_grid(cols = vars(drv))\n```\n\n\n\nHow can I change the distance between the strip and the main plot? (For example, create a gap between the strip and the main plot.)\nBut I don't need to change the strip size (different from this edit strip size ggplot2).\n    ", "Answer": "\r\nThere can be multiple solutions to this problem.\n\n```\ngeom_hline```\n\n\nA hacky one is to add a line (probably white, but it depends on your theme) on top of the plot. We can do this using ```\ngeom_hline```\n (or ```\ngeom_vline```\n if your facets are in rows). This creates an illusion of distance.\n\n```\nlibrary(ggplot2)\nggplot(mpg, aes(displ, cty)) +\n  geom_point() +\n  facet_grid(cols = vars(drv)) +\n  # Add white line on top (Inf) of the plot (ie, betweem plot and facet)\n  geom_hline(yintercept = Inf, color = \"white\", size = 4) +\n  labs(title = \"geom_hline\")\n```\n\n\n\n\n```\nstrip.background```\n\n\nAnother solution (as suggested by @atsyplenkov) is to use ```\ntheme(strip.background = ...)```\n. There you can specify color of the border. However, this is not a perfect as it cuts border from all the directions (there might be a way to improve this).\n\n```\nggplot(mpg, aes(displ, cty)) +\n  geom_point() +\n  facet_grid(cols = vars(drv)) +\n  # Increase size of the border\n  theme(strip.background = element_rect(color = \"white\", size = 3)) +\n  labs(title = \"strip.background\")\n```\n\n\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How Can I Optimize This Recursive Edit Distance Function or Its Associated Data Filtration Function?\r\n                \r\nI built a recursive function to calculate the edit distance between two strings, which I need to iterate over thousands of distinct sentences in order to construct several JSON files for an app I'm updating. The edit distance function is giving good results, but I think it could stand to be simplified:\n```\ndef rec_edit_dist(of_str_a: str, of_str_b: str) -> int:\n    if of_str_a == str() or of_str_b == str():\n        return len(of_str_a) if of_str_b == str() else len(of_str_b)\n    if len(of_str_a) < len(of_str_b):\n        short_str, long_str = of_str_a, of_str_b\n    else:\n        short_str, long_str = of_str_b, of_str_a\n    del of_str_a, of_str_b\n    s, e = 0, 1\n    best_match: str = str()\n    for e in range(1, len(short_str) + 1):\n        substr = short_str[s:e]\n        if substr in long_str:\n            if len(best_match) < len(substr):\n                best_match = substr\n        else:\n            s += 1\n    if best_match == str():\n        return len(long_str)\n    short_split = short_str.split(best_match, 1)\n    long_split = long_str.split(best_match, 1)\n    return sum(rec_edit_dist(a, b) for a, b in zip(short_split, long_split))\n\n\ndef filt_by_best_dists(this_str: str,\n                       against_strs: List[str]) -> List[Tuple[str, int]]:\n    ts_len = len(this_str)\n    against_strs.sort(key=lambda i: abs(len(i) - ts_len))\n    best_dist = 1000\n    keeps: List[Tuple[str, int]] = list()\n    for that_str in against_strs:\n        if abs(len(that_str) - ts_len) > best_dist:\n            break\n        that_ed = rec_edit_dist(of_str_a=this_str, of_str_b=that_str)\n        if that_ed > 0 and that_ed < best_dist:\n            best_dist = that_ed\n            keeps += [(that_str, that_ed)]\n    return keeps\n```\n\nEssentially, ```\nfilt_by_best_dists```\n avoids checking overlong and under-long entries as better options get found, and the ```\nrec_edit_dist```\n just weeds out the longest common substrings (as 0 to the edit distance) until either split string is blank, at which point it returns the longer of the two.\nAre there any other tricks that I could employ to get these two functions running as fast as possible? I planned to run this on 600 threads to cover all of the information I need, and the ```\nagainst_strs```\n parameter has up to thousands of strings.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "String edit distance in python\r\n                \r\nI need to check if the string distance (Measure the minimal number of changes - character removal, addition, and transposition) between two strings in python is greater than 1.\nI can implement it on my own, but I bet there are existing packages for that would save me from implementing that on my own. I wasn't able to find any such package I could identify as commonly used. Are there any?\n    ", "Answer": "\r\nThere is a NLTK package which you can use, it uses the Levenshtein edit-distance which should be what you're looking for.\nExample:\n```\nimport nltk\ns1 = \"abc\"\ns2 = \"ebcd\"\nnltk.edit_distance(s1, s2) # output: 2\n```\n\nReference:\nhttps://tedboy.github.io/nlps/generated/generated/nltk.edit_distance.html\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance Algorithm (where changes can be made to entire substrings)\r\n                \r\nAre there examples of algorithms for determining the edit distance between 2 strings when there are extra primitive operations (the standard being insert, delete, transpose & substitute a single character) which operate on a whole substring. Examples of possible extra primitive operations are:\n\n1) The duplicate function - which copies any substring and inserts it where needed\n\n2) The move function - which moves any substring to a new location\n\nUsing these, if d & D are the Levenstein distance but D also includes 1) & 2), d(\"Sheep\", \"SheepBeep\") = 4 (as 4 inserts must be made), but D(\"Sheep\", \"SheepBeep\") = 2 (insert a \"B\" then duplicate the \"eep\"). Similarly, d(\"CarDog\", \"DogCar\") = 6 but D(\"CarDog\", \"DogCar\") = 1 by 2).\n\nAre there (simple) modifications that can be made to the Levenstein Distance algorithm in order to achieve this?\n    ", "Answer": "\r\nHaving only insert, delete and move results in an NP-hard problem. It seems very unlikely that adding duplicate, transpose, and substitute makes it easier again. So polynomial-time approaches like the Levenshtein dynamic programming are very unlikely to work.\n\nSimilar problems have also been considered in Bioinformatics under terms like \"genome rearrangement\" and \"translocation distance\".\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Damerau–Levenshtein distance (Edit Distance with Transposition) c implementation\r\n                \r\nI implemented the Damerau–Levenshtein distance in c++ but it does not give correct o/p for the input (pantera,aorta) the correct o/p is 4 but my code gives 5.....\n\n```\nint  editdist(string s,string t,int n,int m) \n{\n    int d1,d2,d3,cost;\n    int i,j;\n    for(i=0;i<=n;i++) \n    {\n        for(j=0;j<=m;j++)\n        {\n          if(s[i+1]==t[j+1]) \n              cost=0;\n          else\n              cost=1;\n          d1=d[i][j+1]+1;\n          d2=d[i+1][j]+1;\n          d3=d[i][j]+cost;\n          d[i+1][j+1]=minimum(d1,d2,d3);\n          if(i>0 && j>0 && s[i+1]==t[j] && s[i]==t[j+1] )   //transposition\n          {\n              d[i+1][j+1]=min(d[i+1][j+1],d[i-1][j-1]+cost);\n          }\n        }\n    }\n    return d[n+1][m+1]; \n}\n```\n\n\nI don't see any errors. Can someone find a problem with the code?\n    ", "Answer": "\r\nThe algorithm in the post does not compute Damerau-Levenshtein distance. In a wikipedia article this algorithm is defined as the Optimal String Alignment Distance.\n\nA java implementation of DL distance algorithm can be found in another SO post.\n\nTo get the correct values of OSA distance please change the lines marked with ```\n-```\n below with the lines marked with ```\n+```\n\n\n```\n int  editdist(string s,string t,int n,int m) \n {\n     int d1,d2,d3,cost;\n     int i,j;\n     for(i=0;i<=n;i++) \n     {\n         for(j=0;j<=m;j++)\n         {\n-          if(s[i+1]==t[j+1]) \n+          if(s[i+1]==t[j+1]) \n              cost=0;\n           else\n              cost=1;\n           d1=d[i][j+1]+1;\n           d2=d[i+1][j]+1;\n           d3=d[i][j]+cost;\n           d[i+1][j+1]=minimum(d1,d2,d3);\n-          if(i>0 && j>0 && s[i+1]==t[j] && s[i]==t[j+1] )   //transposition\n+          if(i>0 && j>0 && s[i]==t[j-1] && s[i-1]==t[j] )   //transposition\n           {\n               d[i+1][j+1]=min(d[i+1][j+1],d[i-1][j-1]+cost);\n           }\n         }\n     }\n     return d[n+1][m+1]; \n }\n```\n\n\nIt looks as if the code was copied from a program written in a programming language where array indices start at 1 by default. Therefore all references to the elements of the distance array ```\nd```\n were incremented. However the references to the characters within the strings are references to 0-based arrays, therefore they should not be updated.\n\nTo compute the distance the distance array has to be properly initialized:\n\n```\nfor( i = 0; i < n + 1; i++)\n      d[i][0] = i;\nfor( j = 1; j < m + 1; j++)\n      d[0][j] = j;\n```\n\n\nSince you have got the answer 5, you probably have your distance array already initialized correctly.\n\nSince the above algorithm does not compute the DL distance, here is a sketch of a C implementation of the DL algorithm (derived from the SO post with a java impl. derived from an ActionScript impl. in the Wikipedia article).\n\n```\n#define d(i,j) dd[(i) * (m+2) + (j) ]\n#define min(x,y) ((x) < (y) ? (x) : (y))\n#define min3(a,b,c) ((a)< (b) ? min((a),(c)) : min((b),(c)))\n#define min4(a,b,c,d) ((a)< (b) ? min3((a),(c),(d)) : min3((b),(c),(d)))\n\nint dprint(int* dd, int n,int m){\n int i,j;\n for (i=0; i < n+2;i++){\n    for (j=0;j < m+2; j++){\n        printf(\"%02d \",d(i,j));\n    }\n    printf(\"\\n\");\n }\n printf(\"\\n\");\n return 0;\n}\n\nint dldist2(char *s, char* t, int n, int m) {\n    int *dd;\n    int i, j, cost, i1,j1,DB;\n    int INFINITY = n + m;\n    int DA[256 * sizeof(int)];\n\n    memset(DA, 0, sizeof(DA));\n\n    if (!(dd = (int*) malloc((n+2)*(m+2)*sizeof(int)))) {\n      return -1;\n    }\n\n    d(0,0) = INFINITY;\n    for(i = 0; i < n+1; i++) {\n      d(i+1,1) = i ;\n      d(i+1,0) = INFINITY;\n    }\n    for(j = 0; j<m+1; j++) {\n      d(1,j+1) = j ;\n      d(0,j+1) = INFINITY;\n    }      \n    dprint(dd,n,m);\n\n    for(i = 1; i< n+1; i++) {\n      DB = 0;\n      for(j = 1; j< m+1; j++) {\n        i1 = DA[t[j-1]];\n        j1 = DB;\n        cost = ((s[i-1]==t[j-1])?0:1);\n        if(cost==0) DB = j;\n        d(i+1,j+1) =\n          min4(d(i,j)+cost,\n              d(i+1,j) + 1,\n              d(i,j+1)+1, \n              d(i1,j1) + (i-i1-1) + 1 + (j-j1-1));\n      }\n      DA[s[i-1]] = i;\n      dprint(dd,n,m);\n    }\n    cost = d(n+1,m+1);\n    free(dd);\n    return cost;\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Difference in normalization of Levenshtein (edit) distance?\r\n                \r\nIf the Levenshtein distance between two strings, ```\ns```\n and ```\nt```\n is given by ```\nL(s,t)```\n,\nwhat is the difference in the impact on the resulting heuristic of the following two different normalization approaches?\n\n```\nL(s,t) / [length(s) + length(t)]```\n\n\n```\nL(s,t) / max[length(s), length(t)]```\n\n\n```\n(L(s,t)*2) / [length(s) + length(t)]```\n\n\n\nI noticed that normalization approach 2 is recommended by the Levenshtein distance Wikipedia page but no mention is made of approach 1. Are both approaches equally valid? Just wondering if there is some mathematical justification for using one over the other.\nAlso, what is the difference between approach 1 and approach 3?\nWith the following example:\n```\ns = \"Hi, my name is\"\nt = \"Hello, my name is\"\nL(s,t) = 4\nlength(s) = 14 # (includes white space)\nlength(t) = 17 # (includes white space)\n```\n\nThe Levenshtein distance given the three normalization algorithms above are:\n```\n[Approach 1]   4  /(14+17) = 0.129\n[Approach 2]   4  /(17)    = 0.235\n[Approach 3] (4*2)/(14+17) = 0.258\n```\n\n    ", "Answer": "\r\nThe effects of both variants should be nearly the same.\n\nThe second approach covers a range from 0 (strings are equal) to 1 (completely different)...\nwhile the upper range in the first variant depends on the length of the strings: if the lengths are nearly equal the upper bound is 0.5, and increases on larger differences between the lengths.\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How can I generate all variants of a word within 1-edit distance (Levenshtein)? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 7 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI would like to generate all variants of a word within 1-edit distance using Levenshtein distance.\n\nPHP has a function that will take two strings as parameter and will return just the number (int) of insert, replace and delete operations needed to transform str1 into str2. PHP Manual - levenshtein\n\n```\nint levenshtein ( string $str1 , string $str2 )\n```\n\n\nI am looking for a PHP solution to create an algorithm that generates the variants of a given word.\n    ", "Answer": "\r\nThis is pretty easy for distance 1. Generating all possibilities for distances > 1 becomes somewhat more complex. \n\nStart with a word:\n\n```\n$input = 'word';\n```\n\n\nSplit the word into letters and generate a list of replacements.\n\n```\n$letters = str_split($input);\n\n$alphabet = range('a', 'z');\n```\n\n\nDeletions are the easiest, just loop over each position and replace with ```\n''```\n:\n\n```\nforeach ($letters as $i => $letter) {\n    $variants[] = substr_replace($input, '', $i, 1);\n}\n```\n\n\nInsertions and replacements can be done at the same time, because they'll both require a loop over the letters in the input nested inside a loop over the alphabet.\n\n```\nforeach ($alphabet as $variation) {\n    foreach ($letters as $i => $letter) {\n\n        // insertion\n        $variants[] = substr($input, 0, $i) . $variation . substr($input, $i);\n\n        // substitution\n        // (check that the letter is different or you'll get multiple copies of the input)\n        if ($variation != $letter) {\n            $variants[] = substr_replace($input, $variation, $i, 1);\n        }\n    }\n    $variants[] = $input . $variation; // handle insertion at the end\n}\n```\n\n\nYou can check the results to verify the levenshtein distances are correct:\n\n```\nforeach ($variants as $variant) {\n    $result[$variant] = levenshtein($input, $variant);\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is there an easy way to calculate the graph edit distance of two graphs with a brut force algorithm?\r\n                \r\nWithout using the networkx built-in function, is there an easy brut force algorithm that calculates the graph edit distance between ```\nG1```\n and ```\nG2```\n?\nI searched on the Internet, but I could only find hard optimal solutions\n    ", "Answer": "\r\nAccording to Juan Carlos Ramirez reply (who gave a pseudo code of the algorithm), I finally implemented the ugly brutforce algorithm I was expecting. As mentionned in the conversation, it only deals with small graphs as the complexity is exponential. The following Python code uses:\n\n```\nnetworkx```\n  (for graph manipulation)\n```\nalgorithmx```\n  (for graph    2D-rendering)\n\n```\n  from networkx.classes.function import nodes\n  \n  import algorithmx\n  import networkx as nx\n  from algorithmx.networkx import add_graph\n  \n  canvas1 = algorithmx.jupyter_canvas()\n  canvas2 = algorithmx.jupyter_canvas()\n  # Create a directed graph\n  G1 = nx.Graph()\n  G2 = nx.Graph()\n  \n  # G1.add_edges_from([(1, 2), (7, 4), (2, 7),(3, 7)])\n  # G2.add_edges_from([(1, 2), (3, 4), (2, 3), (3, 5)])\n  \n  G1.add_edges_from([(1, 2), (5, 4), (2, 5),(3, 5)])\n  G2.add_edges_from([(1, 2), (3, 4), (2, 3), (3, 1)])\n  \n  \n  \n  def graph_distance(G1, G2):\n    tmp_graphs = []\n    next_graphs = [G1]\n    dist = 0\n    nId = 1000\n  \n    while 1:\n      print(dist)\n      for graph in next_graphs:\n        if nx.is_isomorphic(graph, G2): # Check isomorphism for each built graph\n          return dist\n  \n  \n        new_graph = graph.copy()\n        new_graph.add_node(len(graph.nodes))\n        tmp_graphs.append(new_graph) # Add one vertex (that will be connected to the rest of the graph in the next iterations)\n  \n        graph_copy = graph.copy()\n        for node in graph.nodes : # Add edge\n          for newNeighbour in graph.nodes :\n            if not graph.has_edge(node, newNeighbour):\n              new_graph = graph.copy()\n              new_graph.add_edge(node, newNeighbour)\n              tmp_graphs.append(new_graph)\n        \n        for node in graph.nodes : # Delete edge\n          for newNeighbour in graph.nodes:\n            if graph.has_edge(node, newNeighbour):\n              new_graph = graph.copy()\n              new_graph.remove_edge(node, newNeighbour)\n              tmp_graphs.append(new_graph)\n            \n        for node in graph.nodes : # Delete vetex\n          new_graph = graph.copy()\n          new_graph.remove_node(node)\n          tmp_graphs.append(new_graph)\n  \n      dist+=1\n      next_graphs = tmp_graphs\n      tmp_graphs = []\n      \n      \n   \n        \n  print(\"Graph edit distance is:\",graph_distance(G1, G2))\n  add_graph(canvas1, G1)\n  add_graph(canvas2, G2)\n\n  canvas1\n  # canvas2\n```\n\nUncomment canvas1/canvas2 to display the one you want\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Does the Levenshtein (Edit Distance) algorithm perform faster than O(n*m) in a native graph database?\r\n                \r\nWould the Levenshtein (Edit Distance) have better time complexity in a native graph database such as Neo4j than the current limit of O(n*m)? If so, why?\n    ", "Answer": "\r\nSince the implementations of ```\napoc.text.levenshteinDistance```\n and ```\napoc.text.levenshteinSimilarity```\n simply rely on org.apache.commons.text.similarity.LevenshteinDistance to do the calculation, the APOC library does not introduce any complexity improvements.\n\nIn any case, such a calculation should just compare 2 strings of text and should not in any way rely on the graphical nature of the DB.\n\nAnd finally, it has been proven that the complexity cannot be improved (unless the Strong Exponential Time Hypothesis is wrong).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to read a large file - line by line?\r\n                \r\nI want to iterate over each line of an entire file. One way to do this is by reading the entire file, saving it to a list, then going over the line of interest. This method uses a lot of memory, so I am looking for an alternative.\nMy code so far:\n```\nfor each_line in fileinput.input(input_file):\n    do_something(each_line)\n\n    for each_line_again in fileinput.input(input_file):\n        do_something(each_line_again)\n```\n\nExecuting this code gives an error message: ```\ndevice active```\n.\nAny suggestions?\nThe purpose is to calculate pair-wise string similarity, meaning for each line in file, I want to calculate the Levenshtein distance with every other line.\nNov. 2022 Edit: A related question that was asked 8 months after this question has many useful answers and comments. To get a deeper understanding of python logic, do also read this related question How should I read a file line-by-line in Python?\n    ", "Answer": "\r\nThe correct, fully Pythonic way to read a file is the following:\n\n```\nwith open(...) as f:\n    for line in f:\n        # Do something with 'line'\n```\n\n\nThe ```\nwith```\n statement handles opening and closing the file, including if an exception is raised in the inner block. The ```\nfor line in f```\n treats the file object ```\nf```\n as an iterable, which automatically uses buffered I/O and memory management so you don't have to worry about large files.\n\n\n  There should be one -- and preferably only one -- obvious way to do it.\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Compute Edit distance for a dataframe which has only column and multiple rows in python\r\n                \r\nI have  a dataframe which has one column and more that 2000 rows. How to compute the edit distance between  each rows of the same column.\n\nMy Dataframe looks like this:\n\n```\n  Name\n  John\n  Mrinmayee\n  rituja\n  ritz\n  divya\n  priyanka\n  chetna\n  chetan\n  mansi\n  mansvi\n  mani\n  aliya\n  shelia\n  Dilip\n  Dilipa\n```\n\n\nI need to calculate distance between each and every row ? How can we do this or achieve this.\n\nI have written some code but that doesnot work this .. gives and enndless list of distances  I guess  I am going wrong in for loop. can somebody help please\n\n```\n   import pandas as pd\n   import numpy as np\n   import editdistance\n   data_dist =  pd.read_csv(Data_TestDescription.csv')\n   df = pd.DataFrame(data_dist)\n   levdist =[]\n   for index, row in df.iterrows():\n        levdist = editdistance.eval(row,row)\n        print levdist \n```\n\n    ", "Answer": "\r\nThis is a neat trick I learned courtesy Adirio. You can use ```\nitertools.product```\n, and then calculate edit distance in a loop.\n\n```\nfrom itertools import product\n\ndist = np.empty(df.shape[0]**2, dtype=int) \nfor i, x in enumerate(product(df.Name, repeat=2)): \n    dist[i] = editdistance.eval(*x)\n\ndist_df = pd.DataFrame(dist.reshape(-1, df.shape[0]))\n\ndist_df\n\n    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14\n0    0   8   6   4   5   7   5   5   5   6   4   5   6   5   6\n1    8   0   7   7   7   6   8   8   7   8   7   7   8   8   8\n2    6   7   0   3   4   5   5   6   6   6   6   5   5   5   4\n3    4   7   3   0   4   6   5   5   5   6   4   4   6   4   5\n4    5   7   4   4   0   6   5   5   5   6   5   3   5   4   4\n5    7   6   5   6   6   0   6   6   6   7   6   5   7   7   6\n6    5   8   5   5   5   6   0   2   6   6   5   5   3   6   5\n7    5   8   6   5   5   6   2   0   6   6   5   5   4   6   6\n8    5   7   6   5   5   6   6   6   0   1   1   5   5   5   6\n9    6   8   6   6   6   7   6   6   1   0   2   5   6   6   6\n10   4   7   6   4   5   6   5   5   1   2   0   4   5   4   5\n11   5   7   5   4   3   5   5   5   5   5   4   0   4   4   3\n12   6   8   5   6   5   7   3   4   5   6   5   4   0   4   4\n13   5   8   5   4   4   7   6   6   5   6   4   4   4   0   1\n14   6   8   4   5   4   6   5   6   6   6   5   3   4   1   0\n```\n\n\n```\nnp.empty```\n initialises an empty array, which you then fill up through each call to ```\neditdistance.eval```\n. \n\n\n\nBorrowing from senderle's ```\ncartesian_product```\n, we can achieve some speed gains:\n\n```\ndef cartesian_product(*arrays):\n    la = len(arrays)\n    dtype = np.result_type(*arrays)\n    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n    for i, a in enumerate(np.ix_(*arrays)):\n        arr[...,i] = a\n    return arr.reshape(-1, la)\n\nv = np.apply_along_axis(func1d=lambda x: editdistance.eval(*x), \n           arr=cartesian_product(df.Name, df.Name), axis=1).reshape(-1, df.shape[0])\n\ndist_df = pd.DataFrame(v)\n```\n\n\nAlternatively, you could define a function to compute edit distance and vectorise it:\n\n```\ndef f(x, y):\n    return editdistance.eval(x, y)\n\nv = np.vectorize(f)\n\narr = cartesian_product(df.Name, df.Name).T\narr = v(arr[0, :], arr[1, :])\n\ndist_df = pd.DataFrame(arr.reshape(-1, df.shape[0]))\n```\n\n\nIf you need annotated index and columns, you can just add it when constructing ```\ndist_df```\n:\n\n```\ndist_df = pd.DataFrame(..., index=df.Name, columns=df.Name)\n\ndist_df\n\nName       John  Mrinmayee  rituja  ritz  divya  priyanka  chetna  chetan  \\\nName                                                                        \nJohn          0          8       6     4      5         7       5       5   \nMrinmayee     8          0       7     7      7         6       8       8   \nrituja        6          7       0     3      4         5       5       6   \nritz          4          7       3     0      4         6       5       5   \ndivya         5          7       4     4      0         6       5       5   \npriyanka      7          6       5     6      6         0       6       6   \nchetna        5          8       5     5      5         6       0       2   \nchetan        5          8       6     5      5         6       2       0   \nmansi         5          7       6     5      5         6       6       6   \nmansvi        6          8       6     6      6         7       6       6   \nmani          4          7       6     4      5         6       5       5   \naliya         5          7       5     4      3         5       5       5   \nshelia        6          8       5     6      5         7       3       4   \nDilip         5          8       5     4      4         7       6       6   \nDilipa        6          8       4     5      4         6       5       6   \n\nName       mansi  mansvi  mani  aliya  shelia  Dilip  Dilipa  \nName                                                          \nJohn           5       6     4      5       6      5       6  \nMrinmayee      7       8     7      7       8      8       8  \nrituja         6       6     6      5       5      5       4  \nritz           5       6     4      4       6      4       5  \ndivya          5       6     5      3       5      4       4  \npriyanka       6       7     6      5       7      7       6  \nchetna         6       6     5      5       3      6       5  \nchetan         6       6     5      5       4      6       6  \nmansi          0       1     1      5       5      5       6  \nmansvi         1       0     2      5       6      6       6  \nmani           1       2     0      4       5      4       5  \naliya          5       5     4      0       4      4       3  \nshelia         5       6     5      4       0      4       4  \nDilip          5       6     4      4       4      0       1  \nDilipa         6       6     5      3       4      1       0 \n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to set the default edit distance in fuzzy search from 2 to 1?\r\n                \r\nI am using edismax parser. I have many fields in my schema and fullText is one of them.I am using this query:\n\n```\n http://localhost:8983/solr/simple/select?q=design~+chair~&wt=json&defType=edismax&qf=fullText   \n```\n\n\n(Check the ~ symbol after query words) \n\nNow when i use debugQuery i find out that this internally querying for this:\n\n```\n\"parsedquery_toString\": \"+((fullText:design~2) (fullText:chair~2))\"\n```\n\n\nIs there any way i can change the default edit distance from 2 to 1?\n    ", "Answer": "\r\nYou can supply a parameter to the fuzzy search... so ```\ndesign~1```\n for instance will give you an edit distance of 1\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "how to add new column to data frame in spark by deriving edit distance data frame columns (String)\r\n                \r\nI am new to Scala and Spark. I want to derive a new column from existing columns of data frame by computing edit distance. For example FNAME and LNAME are two columns of data frame, wanted to add new column called NAMESCORE which keeps edit distance of FNAME to LNAME. Kindly please advise with a working or pseudo code.\n\nHere is the link I got some partial answer.\n\nDerive multiple columns from a single column in a Spark DataFrame\n    ", "Answer": "\r\nYou can use UDF:\n\n```\ndef udfToFindEditDistance(col1 :String,col2 :String): String ={\n    //find edit distance b/w col1 and col2 \n  }\n```\n\n\nRegister the udf\n\n```\n val newUDF=udf(udfToFindEditDistance(_:String,_:String)) \n```\n\n\nAdding a new column \n\n```\nval newDf=df.withColumn(\"newColumnName\",newUDF(df(\"FNAME\"),df(\"LNAME\")))\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "how to efficiently check if the Levenshtein edit distance between two string is 1 [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and  cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened,  visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nplease note that it doesn't require to really calculate Levenshtein edit distance. just check it's 1 or not. \n\nThe signature of the method may look like this: \n\n```\nbool Is1EditDistance(string s1, string s2). \n```\n\n\nfor example:\n1. \"abc\" and \"ab\" return true\n2. \"abc\" and \"aebc\" return true\n3. \"abc\" and \"a\" return false.\n\nI've tried recursive approve, but it it not efficient. \n\n\n\nupdate: got answer from a friend:\n\n```\n        for (int i = 0; i < s1.Length && i < s2.Length; i++)\n        {\n            if (s1[i] != s2[i])\n            {\n                return s1.Substring(i + 1) == s2.Substring(i + 1)   //case of change\n                    || s1.Substring(i + 1) == s2.Substring(i)       //case of s1 has extra\n                    || s1.Substring(i) == s2.Substring(i + 1);      //case of s2 has extra\n            }\n        }\n        return Math.Abs(s1.Length - s2.Length) == 1;\n```\n\n    ", "Answer": "\r\nIf you only care if the distance is exactly 1 or not, you can do something like this:\n\n\nIf the difference of the strings' lengths is not 0 or 1, return false.\nIf both strings have length ```\nn```\n, loop ```\ni = 0..n```\n checking ```\ns1[i] == s2[i]```\n for all ```\ni```\n except one.\nIf the strings have length ```\nn```\n and ```\nn+1```\n, let ```\ni```\n be the smallest index where ```\ns1[i] != s2[i]```\n, then loop ```\nj=i..n```\n checking ```\ns1[j] == s2[j+1]```\n for all ```\nj```\n. \n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "More efficient way of finding edit distance over a large array\r\n                \r\nI have a large array of words (300k words) and I want to find the edit distance between each word, so I was just iterating over it and doing running through this version of the levenstein algorithm:\n\n```\nunsigned int edit_distance(const std::string& s1, const std::string& s2)\n{\n    const std::size_t len1 = s1.size(), len2 = s2.size();\n    std::vector<std::vector<unsigned int>> d(len1 + 1, std::vector<unsigned int>(len2 + 1));\nd[0][0] = 0;\nfor (unsigned int i = 1; i <= len1; ++i) d[i][0] = i;\nfor (unsigned int i = 1; i <= len2; ++i) d[0][i] = i;\n\nfor (unsigned int i = 1; i <= len1; ++i)\n    for (unsigned int j = 1; j <= len2; ++j)\n        // note that std::min({arg1, arg2, arg3}) works only in C++11,\n        // for C++98 use std::min(std::min(arg1, arg2), arg3)\n        d[i][j] = std::min({ d[i - 1][j] + 1, d[i][j - 1] + 1, d[i - 1][j - 1] + (s1[i - 1] == s2[j - 1] ? 0 : 1) });\nreturn d[len1][len2];\n}\n```\n\n\nSo what I was wondering is, if there was a more efficient way of doing this, I heard about Levenshtein Autonoma but I wasn't sure if that would be any more efficient. \n\nI would imagine that there you could avoid processing the same thing over and over again by preprocessing something but I have no idea how to actually achieve it (some approximate calculations would be to preprocess everything would be around 10^28 operations so that would not be an improvement)\n    ", "Answer": "\r\nAs stated in his comment, The OP is actually looking for all the pairs with edit distance of less than 2.\n\nGiven an input of n words, a naive approach would be to make n(n-1)/2 comparisons, but less comparison may be required when L in an edit distance which is a metric space for strings. \n\nLevenshtein distance is a metric space, and satisfies the 4 required metric axioms - including the triangle inequality.\n\nEdit:\n\nGiven this, we can use the method proposed by Sergey Brin (Google's co-founder) in his paper Near Neighbor Search in Large Metric Spaces back in 1995, to solve our problem.\n\nQuoting from the paper: Given a metric space (X, d), a data set Y ⊆ X, a query point x ∈ X, and a range r ∈ R, the near neighbors of x are the set of points y ∈ Y, such that d(x, y) ≤ r.\n\nIn this paper, Brin introduced GNAT (Geometric Near-neighbor Access Tree) - a data structure to solve this problem. Brin actually test the performance of his algorithm using the Levenshtein distance (which he calls \"Edit distance\") against two text corpora.\n\nOver the years GNAT become well-known and widely used. Some improvements to GNAT where suggested in Geometric Near-neighbor Access Tree (GNAT) revisited - Fredriksson 2016.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Determining a sequence of edits that produces the Levenshtein distance\r\n                \r\nI am doing some work using Levenshtein (edit) distance using dynamic programming. I think I understand the Wagner-Fischer algorithm to do this efficiently. However, it doesn't look like the algorithm is constructive. If I compute that the edit distance between two strings is, e.g., 10, then I would also like to determine a particular sequence of 10 edits that turns one into the other. Can this be done efficiently too? If so, how?\n    ", "Answer": "\r\nWhile trying to implement Ante's algorithm I got wrong results, which means it is either wrong or I implemented it in a wrong way.  In any case I got it working and here's my more detailed algorithm.  See Wagner-Fischer algorithm for a description of ```\nd```\n.\n\n\nStart at cell ```\nd(m, n)```\n\nCheck cells ```\nd(m - 1, n - 1)```\n, ```\nd(m - 1, n)```\n and ```\nd(m, n - 1)```\n and determine which one contains the smallest value.\n\n\nIf it's ```\nd(m - 1, n - 1)```\n (prefer this if it's a tie) then you have either\n\n\na substitution if ```\nd(m - 1, n - 1) < d(m, n)```\n.  Decrement ```\nm```\n and ```\nn```\n by one.\nno operation if ```\nd(m - 1, n - 1) == d(m, n)```\n.  Decrement ```\nm```\n and ```\nn```\n by one.\n\nIf it's ```\nd(m - 1, n)```\n then you have a deletion.  Decrement ```\nm```\n by one.\nIf it's ```\nd(m, n - 1)```\n then you have an insertion.  Decrement ```\nn```\n by one.\n\n\n\nIf any cell lookup would cause negative indexes, just skip them.  If you arrive at cell ```\n(0, 0)```\n you're done.  You will have produced the list of edits in reverse order.\n\nI wrote an implementation in Python that outputs the exact instructions including the characters and offsets involved in each operations.  It also includes some tests to validate the output and which also demonstrate the format of the output.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Pymongo, How to find edit distance of field value and a input string\r\n                \r\nI want to find the similarity between two sequences, for this i am using edit distance. Since I hav very large data, I am using mongodb database\nMy data is of the form\n\n```\n{\n    \"POS\": \"*\",\n    \"MAPQ\": \"0\",\n    \"`SEQ\": \"AGGGTTAGGGTTAGGGTTAGGGCTAGGGTTAGGGTAAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTATGGTTAGGGTTAGTGGTAGGGTTAGGTTTAGGGG\"\n},\n\n{\n    \"POS\": \"*\",\n    \"MAPQ\": \"0\",\n    \"`SEQ\": \"TAGGGTTAGTGGTAGGGTTAGGTTTAGGGG\"\"\n},\n```\n\n\nHere if I input a seq = \"GATAGAAACCCTAACCCTCTAACCCTAACCCTCTAACCCTAACCCTCTAACCCTAACCCTATAGGGTTAGTGGTAGGGTTAGGTTTAGGGG\"  \n\n```\nfor doc in fastqseq.find({}, {'SEQ':1,'_id':0}):\n            print  doc\n            ed = edit_distance(doc,seq)\n            print ed \n```\n\n\nFor the above code, the output I get\nFile \"/Library/Python/2.7/site-packages/nltk/metrics/distance.py\", line 61, in edit_distance\n    _edit_dist_step(lev, i+1, j+1, s1[i], s2[j])\nKeyError: 0\n\nAny suggestion is appreciated \n\noutput of print doc\n\n```\n{u'SEQ': u'ATCTGATATCCTGGAAAAGCACCCACACCCCCAGGTGAGCATCTGACAGCCTGGAACAGCATCCACAACCCCAGGTGAACATCCGACAGCCTGAAGCAGAA'}\n{u'SEQ': u'TGTTCCAGGCTGTCAGAGGCTCACCTGGGCGGGGGGGGGCTGTTTCAGTCTGTCAGATGCTCCCCTGGGGGGGGGGGTTCTGTTCCAGGCTGCAGGATGCT'}\n{u'SEQ': u'GGGGGTGTGGGTGCTGTTCCAGGCCGTCAGATGCTCACTTGGGGGTGCAGGGTGCTGTTCCAGTCTGTCAGATGCTCACCTGGGGGTGTGGGTTGTGTTCC'}\n```\n\n    ", "Answer": "\r\nYou need to compare one string to another. ```\ndoc```\n is a dictionary, so you can't use it like you are using it.\n\nUntested, but this general idea should work work:\n\n```\ndocs = fastqseq.find({}, {'SEQ':1,'_id':0})\n\nfor d1 in docs:\n  for d2 in docs:\n    distance = edit_distance(d1['SEQ'],d2['SEQ'])\n    // use the distance variable here\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "construct edit matrix for Levenshtein distance\r\n                \r\nTo calculate Levenshtein distance, we always choose to use dynamic programming. For this, we will create an edit distance matrix as shown below:\nenter image description here\nHere is the code:\n```\nwhile True:\n    try:\n        a = input()\n        b = input()\n         \n        board = [[0 for j in range(len(b)+1)] for i in range(len(a)+1)]\n         \n        for i in range(len(a)+1):\n            board[i][0] = i\n        for j in range(len(b)+1):\n            board[0][j] = j\n             \n        for i in range(1, len(a)+1):\n            for j in range(1, len(b)+1):\n                 \n                if a[i-1] == b[j-1]:\n                    d = 0\n                else:\n                    d = 1\n                 \n                board[i][j] = min(board[i-1][j]+1,\n                                  board[i][j-1]+1,\n                                  board[i-1][j-1]+d)\n         \n        print(board[-1][-1])\n         \n    except:\n        break\n```\n\nSo my question is when we construct the matrix, why we need to add 1 to len(a) and len(b). Because as shown in the picture before, only the red part is the valid part in the matrix. So I modified my code:\n```\nwhile True:\n    try:\n        a = input()\n        b = input()\n\n        board = [[0 for j in range(len(b))] for i in range(len(a))]\n\n        for i in range(len(a)):\n            board[i][0] = i\n\n        for j in range(len(b)):\n            board[0][j] = j\n\n        for i in range(1, len(a)):\n            for j in range(1, len(b)):\n                if a[i] == b[j]:\n                    d = 0\n                else:\n                    d = 1\n\n                board[i][j] = min(board[i-1][j]+1,\n                                  board[i][j-1]+1,\n                                  board[i-1][j-1]+d)\n        print(board[-1][-1])\n\n    except:\n        break \n\n```\n\nI test this modified code and it still gives the correct answer in most tests. But when both strings are very long, the result will be 1 less. I am very confused about this. Maybe this question is stupid, but I still hope to be answered, thank you. 🙏\n    ", "Answer": "\r\nThe problem with your solution is that you skip ```\na[0]```\n and ```\nb[0]```\n case and you have to handle that case first. The original solution handles it with ```\na[i-1] == b[j-1]```\n when ```\ni = 1```\n and ```\nj = 1```\n but you don't\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Including backtrace pointers with symbols in a Python output of a DP table for min edit distance algorythm\r\n                \r\nI have a working min edit distance program for two words that also iterates and outputs a DP table to the console, however I wish to add a backtrace to the programme that also prints arrow pointer symbols in the DP table to show the backtrace clearly on the output. I cannot figure out how to output these symbols correctly.\n```\nimport re\nimport time\n\ntime1= time.time() \ndef printTable(table, description):\n\nprint(f'{description}\\n')\ncurrent_row = current_col = 0\ncurrent_row_col = re.search(\"^row ([0-9]+) , col ([0-9]+)$\",description)\nif current_row_col:\n    current_row = int(current_row_col.group(1))\n    current_col= int(current_row_col.group(2))\nrow_counter=0\nfor row in table:\n    row_counter+=1\n    col_counter=0\n    for col in row:\n        col_counter+=1\n        #print(row_counter , row, current_col, col)\n        if (row_counter == current_row) and (col_counter == current_col): \n            formatting = '\\033[1m'+'\\033[91m' #bold + red\n        else: formatting = '\\x1b[0m' #reset fomatting\n        print(formatting + str(col).rjust(10, ' '), end=' ')  # rjust returns a 10-characters long, right justified version of the string\n    print('\\n\\n')\nprint('---------------------------------------------------------------------------------------------------------------')                       \n    \n\n# A DP-based solution for edit distance problem \ndef editDistDP(x,y):\n\nleftarrow = \"←\"\nuparrow = \"↑\"\ndiagarrow = \"\"\n\ndp = [] # Create an empty table to store results of subproblems\n# fill in the table with zeros \nfor row in range(len(x) + 1):\n    dp.append([0]* (len(y) + 1))\n# Alternatively, you can use List Comprehension to initiate the DP table in one line of code\n# dp = [[0 for column in range(len(y) + 1)] for row in range(len(x) + 1)]\n\n# Fill in the base case (easy) subproblems, i.e. the first row and column of the DP table\n\n# first row: base case subproblems for computing the cost of converting \"\" to y\nfor i in range(len(y) + 1):\n    # If x is empty then the only option is to insert all the characters of y\n    # Minimum number of required operations (cost) is i insertions, where i = len(y)\n    dp[0][i] = i\n\n# first column: base case subproblems for computing the cost of converting x to \"\"\nfor i in range(len(x) + 1):\n    # If y is empty then the only option is to delete all the characters of x\n    # Minimum number of required operations (cost) is i deletions, where i = len(x)\n    dp[i][0] = i\n\n\nprintTable(dp,\"DP table after the base case (easy) subproblems are solved\");\n\n# Fill in the rest of the DP table in a BOTTOM-UP manner \nfor i in range(1, len(x) + 1):\n    for j in range(1, len(y) + 1):\n\n        horizontal_or_insertion_cost = (dp[i][j-1] + 1)\n        vertical_or_deletion_cost= dp[i-1][j] + 1\n# Weighted Minimum Edit Distance for sub\n        if x[i-1] != y[j-1] and x[i-1].isnumeric: \n          delta = 3 \n        elif x[i-1] != y[j-1]:\n          delta = 2\n        else: \n          delta = 0\n        diagonal_or_substitution_cost= dp[i-1][j-1] + delta\n\n        minValue = min(horizontal_or_insertion_cost,vertical_or_deletion_cost,diagonal_or_substitution_cost)\n        dp[i][j] = minValue          \n        # printTable(dp,f'row {i+1} , col {j+1}') #UNCOMMENT this line to see how the DP table is filled at each step\n\nprintTable(dp,\"Completed DP table after all the subproblems are solved\")\nreturn dp[-1][-1]\n\n\n\nstr1, str2 = \"intention\", \"execution\" \nprint(f'edit distance between \"{str1}\" and \"{str2}\": {editDistDP(str1, str2)}') \n\ntime2 = time.time() \nexecTime = time2-time1\nexecTime = str(execTime)\nprint(\"--- Executed in: \" + execTime + \" seconds ---\") \n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Create high nr of random sequences with min Edit Distance time efficient\r\n                \r\nI need to create a program/script for the creation of a high numbers of random sequences (20 letter long sequence based on 4 different letters) with a minimum edit distance between all sequences. \"High\" would here be a minimum of 100k sequences, but if possible up to 1 million. \n\nI started with a naive approach of just generating random 20 letter sequences, and for each sequence, calculate the edit distance between the sequence and all other sequences already created and stored. If the new sequence pass my threshold value, store it, otherwise discard. \n\nAs you understand, this scales very badly for higher number of sequences. Up to 10k is reasonably fine, but trying to get 100k this starts to get troublesome. \n\nI really only need to create the sequences once and store the output, so I'm really not that fussy about speed, but making 1 million at this rate today is just no possible. \n\nBeen trying to think of alternatives to speed up the process, like building the sequences is \"blocks\" of minimal ED and then combining, but haven't come up with any solution. \n\nWondering, do anyone have any smart idea/method that could be implemented to create such high number of sequences with minimal ED more time efficient? \n\nCheers,\nJB\n    ", "Answer": "\r\nIt seems, from wikipedia, that edit distance is one of three operations insertion, deletion, substitution; performed on a starting string. Why not systematically generate all strings up to N edits from a starting string then stop  when you reach your limit?\n\nThere would be no need to check for the actual edit distance as they would be correct by generation. For randomness could you generate a number then shuffle them.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is there any algorithm to compute edit distance between two graphs including same nodes?\r\n                \r\nFirst, I know there has been a lot of works to compute the edit distance between two graphs. But most of the GED algorithms are applied in general cases.\n\nNow considering my case, there are two graphs G(V1,E1) and G(V2,E2). Vk is a set of nodes which includes k vertices(k is a constant), and Vk satisfies both Vk⊆V1 and Vk⊆V2. I want to remain the correspondance between these two graphs when computing the edit distance between them.\n\nI am wondering if there any algorithm aim at this situation? If there is not, does any one have any advice for me? thanks a lot\n\nPS\n\nAssume that vi is a node in Vk. What I am concerned about is that vi remains unchanged when G1 is transformed to G2, which means there is no operation on vi(e.g substitute vi in G1 to u in G2, delete vi in G1, insert vi in G2) during the operations sequence which transform G1 to G2.\n    ", "Answer": "\r\nThere is no algorithm to solve your specific problem, because there is no use case and there is no mathematical form. How I would solve this:\n\n1) In the comments you specify Vk unchanged but Ek(1) Ek(2) where Ek(i) are the edges between nodes in Vk for Vi. In that case compute edge add/rem/replace, GED(Vk1, Vk2) ignoring edges out of Vk1/2\n\n2) compute GED(V1-Vk1, V2-Vk2) ignoring the edges between Vi-Vki and Vki. Where V1-Vk1 is the graph V1 after removing all nodes in Vk and all edges linked to Vk\n\n3) compute GED(E(V1-Vk1 <-> Vk1), E(V2-Vk2 <-> Vk2)), that is, compute the GED of replacing \"edges linking V1-Vk1 and Vk1\" with \"edges linking V2-Vk2 and Vk2\".\n\n4) Add the 3 GED together.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to modify the levenshtein edit distance function to be more flexible\r\n                \r\nI am looking to find a version of the levenshtein edit distance function that has 3 main differences.\n\n\nIt works with tokenized inputs, so instead of ```\n'hello world'```\n its ```\n['hello', 'world']```\n. So each char would instead be treated as a word comparing to the other word as a whole. So distance of ```\n['hello', 'world']```\n and ```\n['world', 'hello']```\n is 2 for 2 updates. By default the common implementations already seems to work like this anyways if I pass them as tokenized inputs.\nI want to define a custom number cost for insert, update and delete. The npm natural.js seems to support this.\nI want to define a custom function to checking equality. All the implementation right now use ```\n===```\n or don't have a way to change this.\n\n\nThe below is the best I could do so far by slightly modifying the code from the link below. It seems to work with #1 and #3 above, but I don't know how to modify this code below to do #2.\n\n```\n// https://rosettacode.org/wiki/Levenshtein_distance\nfunction levenshtein(a, b, customCheck) {\n    var t = [], u, i, j, m = a.length, n = b.length;\n    if (!m) { return n; }\n    if (!n) { return m; }\n    for (j = 0; j <= n; j++) { t[j] = j; }\n    for (i = 1; i <= m; i++) {\n        for (u = [i], j = 1; j <= n; j++) {\n            u[j] = customCheck(a[i - 1], b[j - 1]) ? t[j - 1] : Math.min(t[j - 1], t[j], u[j - 1]) + 1;\n        } t = u;\n    } return u[n];\n}\n\nfunction customCheck(a, b) {\n    return a === b;\n}\n```\n\n\nDoes anyone know?\n\n\n\nEDIT: Tried this but it gave me cost of 1. I made cost of removal 100, cost of update 1 and cost of insert 10. I was expecting cost of 100 for removal of ```\nb```\n.\n\n```\nfunction levenshtein(a, b, customCheck) {\n    var t = [], u, i, j, m = a.length, n = b.length;\n    if (!m) { return n; }\n    if (!n) { return m; }\n    for (j = 0; j <= n; j++) { t[j] = j; }\n    for (i = 1; i <= m; i++) {\n        for (u = [i], j = 1; j <= n; j++) {\n            u[j] = customCheck(a[i - 1], b[j - 1]) ? t[j - 1] : Math.min(t[j - 1] + 100, t[j] + 1, u[j - 1] + 10); // removal, update, insert\n        } t = u;\n    }\n}\n\nfunction customCheck(a, b) {\n    return a === b;\n}\n\nconsole.log(levenshtein('ab', 'a', customCheck));\n```\n\n    ", "Answer": "\r\nIt's in this part: \n\n```\n Math.min(t[j - 1], t[j], u[j - 1]) + 1\n```\n\n\n```\nt[j - 1]```\n is the cost before removal on ```\na```\n, ```\nt[j]```\n is the cost of replacing, and ```\nu[j - 1]```\n is the cost of removal on ```\nb```\n (or insertion on ```\na```\n). 1 is the cost of the current step, so all operations. We can safely move 1 into the ```\nMath.min```\n:\n\n```\n Math.min(t[j - 1] + 1, t[j] + 1, u[j - 1] + 1)\n```\n\n\nand then you can change the cost for each type easily.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance in java [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     Questions asking for code must demonstrate a minimal understanding of the problem being solved. Include attempted solutions, why they didn't work, and the expected results. See also: Stack Overflow question checklist\r\n                \r\n                    \r\n                        Closed 9 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI have a list of names (surnames) and a simple search mechanism. I would like to have words with minor changes (typos) shown in search results.\n\nExample search text: ```\nbraniecka```\n\n\nExample result: ```\nBranicka```\n, ```\nKraniecka```\n, ```\nBraniecki```\n\n\nAny help appreciated.\n    ", "Answer": "\r\nYou can implement the Levenshtein distance. It is a widely used algorithm.\n\nYou could also consider upgrading your solution to Lucene, especially if you are doing any production work. Lucene handles your requirement in an extremely performant way (no brute-force exhaustive search).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "perl: Text::Fuzzy, different string giving same edit distance?\r\n                \r\nChecking distance between ```\n$barcode```\n and two strings, first string has same 12 characters at the front and another is completely different but both gives same distance?\n\n```\n#!/usr/bin/perl\nuse warnings;\nuse strict;\nuse Text::Fuzzy;\nmy $barcode =  \"TCCCTTGTCTCC\";\n\nforeach my $line1 (<DATA>) {\n    print \"New string\\n\";\n    print \"Barcode length:\", length $barcode, \"\\nSequence length:\",\n    length $line1, \"\\n\";\n    my $tf = Text::Fuzzy->new($barcode);\n    my $ed = $tf->distance($line1);\n    print \"Edit distance: \", $ed ,\"\\n\\n\";\n}\n\n__DATA__\nTCCCTTGTCTCCCCTGATATCCTGTAAAATCCTTTTCTTCTGATGGGTGCCATTTGCCACTAGAGGAAGCTGAACAGACCTGACTACCTGGA\nGACGAGACTGATCACCTGATATCCTGTAAAATCCTTTTCTTCTGATGGGTGCCATTTGCCACTAGAGGAAGCTGCAGACCTGACTACCTGGA\n```\n\n\nOutputs:\n\n```\nNew string\nBarcode length:12\nSequence length:93\nEdit distance: 81\n\nNew string\nBarcode length:12\nSequence length:93\nEdit distance: 81\n```\n\n    ", "Answer": "\r\nThat seems right since all the characters of subsequence are present in the longer sequence both would have the same Levenshtein  edit distance. This is so because all it would need is deletions to transform the  longer to shorter sequence \n\nExample :\n\n```\nartic => arc```\n edit distance 2, i.e deletions 2 \n```\narche => arc```\n would have the same edit distance 2 i.e deletions 2\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "SQL Edit Distance: How have you handled Fuzzy String Matching using SQL in the past?\r\n                \r\nI have always wanted to ask for your views on this topic, so here we go:\nMy team just provided me with a list of customer accounts we need to match with other databases and the main challenge we face is the fact our list is non-standarized so we call similarly but differently the same accounts than in our databases. For example:\n```\nMy_List.Customers_Name         Customers_Database.Customers_Name\n-                              -\nCharles Schwab                 Charles Schwab Corporation\n```\n\nSo for example, I use Jaro Wrinkler Similarity function and Edit Distance in order to gather a list of similar strings and then manually match the accounts if needed. My question is:\nWhich rules/filters do you apply to the results of the fuzzy data matching in order to reduce the amount of manual match?\nI am refering to rules like:\nIf the string has more than 20 characters and Edit Distance <= 1 then it will probably be the same so consider it a match. If the string has less than 4 characters and Edit Distance >0 then it will probably not be the same account so consider it a mismatch.\nThese rules I apply are completely made up from my side, I am wondering if there are some standard convention for applying text string fuzzy matching in order to only retrieve useful results and reduce manual match workload.\nIf there are not, could you tell your experience and how you handled this before?\nThank you so much\n    ", "Answer": "\r\nI've done this a few times. It's hugely dependent on the data sets, and the rules change every time.\nMy process is:\n\nselect a random set of sample records to check my rule set - large enough to be representative, small enough to be able to scan visually.\ncreate a \"match\" table with \"original\", \"match\" and \"confidence score\" columns.\nwrite the rules, as \"insert\" or \"update\" statements to create records in the \"match\" table\nrun the rules on my sample data set\nevaluate the matches on the samples. Tweak, add, configure the rules.\nrinse & repeat\n\nThe \"rules\" depend hugely on the data set. Commonly I use the following:\n\nstrip out punctuation\napply common substitutions (e.g. \"Corp\" becomes \"Corporation\")\nsplit into separate words; apply fraction of each exact match out of 10 (so \"Charles Schwab\" matching \"Charles Schwab Corporeation\" would be 2/3 = 7 points, \"HSBC\" matching \"HSBC\" is 1/1 = 10 points\nsplit into separate words; apply fraction of each close match out of 5 (so \"Chls Schwab\" matching \"Charles Schwab Corporation\" would be 2/3 = 3 points, \"HSBC\" matching \"HSCB\" is 1/1 = 5 points)\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Elasticsearch inconsistency in the fuzziness parameter with edit distance when used in fuzzy matching\r\n                \r\nI am trying to understand the effect of fuzziness in fuzzy search using span_near multiple clauses.\nHere I am doing a document count. Here is one query\n```\nGET wikipedia-20200820/_search\n{  \n   \"query\":{  \n      \"bool\":{  \n         \"must\":[  \n            {  \n               \"span_near\":{  \n                  \"clauses\":[\n                     {  \n                        \"span_multi\":{  \n                           \"match\":{  \n                              \"fuzzy\":{  \n                                 \"text\":{  \n                                    \"value\":\"labor\",\n                                    \"fuzziness\":\"2\"\n                                 }\n                              }\n                           }\n                        }\n                     },\n                     {  \n                        \"span_multi\":{  \n                           \"match\":{  \n                              \"fuzzy\":{  \n                                 \"text\":{  \n                                    \"value\":\"unionism\",\n                                    \"fuzziness\":\"2\"\n                                 }\n                              }\n                           }\n                        }\n                     },\n                     {  \n                        \"span_multi\":{  \n                           \"match\":{  \n                              \"fuzzy\":{  \n                                 \"text\":{  \n                                    \"value\":\"among\",\n                                    \"fuzziness\":\"2\"\n                                 }\n                              }\n                           }\n                        }\n                     },\n                     {  \n                        \"span_multi\":{  \n                           \"match\":{  \n                              \"fuzzy\":{  \n                                 \"text\":{  \n                                    \"value\":\"negros\",\n                                    \"fuzziness\":\"2\"\n                                 }\n                              }\n                           }\n                        }\n                     }\n                  ],\n                  \"slop\":1,\n                  \"in_order\":true\n               }\n            }\n         ],\n         \"filter\": [ \n        { \"term\":  { \"ns\": 0 }},\n        { \"range\": { \"timestamp\": { \"gte\": \"2020-01-01\" }}}\n      ]\n      }\n   }\n}\n```\n\nthat is giving me the expected result:\n```\n{\n  \"took\" : 284,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"skipped\" : 0,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : {\n      \"value\" : 1,\n      \"relation\" : \"eq\"\n    },\n    \"max_score\" : 2238.6646,\n    \"hits\" : [\n      {\n        \"_index\" : \"wikipedia-20200820\",\n        \"_type\" : \"_doc\",\n        \"_id\" : \"GhxaT3UBz_UQYd3DEAJT\",\n        \"_score\" : 2238.6646,\n        \"_source\" : {\n          \"page_title\" : \"National Brotherhood of Workers of America\",\n          \"ns\" : 0,\n          \"page_id\" : 28365131,\n          \"redirect\" : null,\n          \"revision_id\" : 965097134,\n          \"revision_parent_id\" : 891907364,\n          \"timestamp\" : \"2020-06-29T11:26:55Z\",\n          \"contributor\" : {\n            \"username\" : \"JJMC89 bot III\",\n            \"id\" : 35936988\n          },\n          \"comment\" : \"Moving [[:Category:African-American history between emancipation and the Civil Rights Movement]] to [[:Category:African-American history between emancipation and the civil rights movement]] per [[Wikipedia:Categories for discussion/Speedy]]\",\n          \"sha1\" : \"q1rz1h5n6qk9cpdt5qf0zqrydd9vx7e\",\n          \"minor\" : \"TRUE\",\n          \"model\" : \"wikitext\",\n          \"format\" : \"text/x-wiki\",\n          \"text\" : \"\"\"The '''National Brotherhood of Workers of America''' (NBWA) was the largest body of organised [[African American]] workers in the [[United States of America]] in 1919.&lt;ref&gt;''Revolutionary radicalism: its history, purpose and tactics with an exposition and discussion of the steps being taken and required to curb it, being the report of the Joint Legislative Committee Investigating Seditious Activities'', filed 24 April 1920, in the Senate of the State of New York&lt;/ref&gt;\n\n==First congress==\nThe organisation was formed by T.J. Pree and R.T. Sims. [[Philip Randolph]] was also on the board.&lt;ref name=yho&gt;[http://hierographics.org/yourhistoryonline/yourhistoryonlineVIII.htm Your History online], accessed 17 August 2010&lt;/ref&gt; The NBWA held its congress in [[Washington DC]] from  8–14 September 1919. There were 115 delegates primarily from the South. Three delegates were from the [[Industrial Workers of the World]] and fifteen from the Society for the Advancement of Trade Unionism among Negroes. No delegates were accepted from the [[American Federation of Labor]].&lt;ref&gt;''Revolutionary radicalism: its history, purpose and tactics with an exposition and discussion of the steps being taken and required to curb it, being the report of the Joint Legislative Committee Investigating Seditious Activities'', filed 24 April 1920, in the Senate of the State of New York&lt;/ref&gt;\n\n==Resolutions==\nThey passed resolutions to the effect that:\n\n&quot;. . . The combination of black and white workers will be a powerful lesson to the capitalists of the solidarity of labor. It will show that labor, black and white, is conscious of its interests and power. This will serve to convert a class of worker, which has been used by the capitalist class \nto defeat organised labor, into an ardent, class-conscious, intelligent, militant group.&quot;\n\n&quot;and be it further resolved, that we recommend to all the working people of our race, that they immediately make themselves acquainted more in detail with the \naims, objects and methods of said organization, the National Association for the Protection of \nLabor Unionism Among Negroes, in order that we may, as speedily as possible, align ourselves with and join the industrial unions that have already organized, and help to organize new industrial unions in such industries where they do not yet exist. \n\n\n&quot;and be it further resolved, that we shall henceforth devote all our energies to building up the new order of society along lines above indicated, to the exclusion of efforts hitherto expended in other directions.&quot; &lt;ref&gt;''Revolutionary radicalism: its history, purpose and tactics with an exposition and discussion of the steps being taken and required to curb it, being the report of the Joint Legislative Committee Investigating Seditious Activities'', filed 24 April 1920, in the Senate of the State of New York&lt;/ref&gt;\n\nThe NBWA functioned as an independent Union, drawing most of its support from shipyard and dock workers in the [[Tidewater region of Virginia]] area.&lt;ref&gt;''Crisis'', November 1951, p626&lt;/ref&gt; The union dissolved in 1921, under pressure from the American Federation of Labor.&lt;ref name=yho/&gt;\n\n==References==\n{{reflist}}\n[[Category:African-American leftism]]\n[[Category:African-American history between emancipation and the civil rights movement]]\n[[Category:Trade unions established in 1919]]\n[[Category:Trade unions disestablished in 1921]]\n[[Category:African-American trade unions]]\"\"\"\n        }\n      }\n    ]\n  }\n}\n```\n\nAlthough it is documented that the ```\nfuzziness```\n parameter represents Edit Distance / Levenshtein Distance, it seemed inconsistent to me. Then to understand the effect of fuzziness I tried to change the some single term in the query and sometimes changed the ```\nfuzziness```\n parameter, keeping everything else same.\nI am summarizing some changes I tried in the query in the following table and whether or not the query resulted in the same result or not:\n```\n+----------------+---------------+-----------+---------------+-----------------------------+\n| iniatial_query | updated_query | fuzziness | edit_distance | elastic_returns_same_result |\n+----------------+---------------+-----------+---------------+-----------------------------+\n| \"labor\"        | \"labot\"       |         2 | 1             | Yes                         |\n| \"labor\"        | \"labori\"      |         2 | 1             | No                          |\n| \"labor\"        | \"lab\"         |         2 | 2             | No                          |\n| \"labor\"        | \"lab\"         |         3 | 2             | No                          |\n| \"labor\"        | \"lab\"         |         4 | 2             | No                          |\n| \"labor\"        | \" labor\"      |         2 | 1 (1 space)   | Yes                         |\n| \"labor\"        | \" labor\"      |         1 | 1 (1 space)   | Yes                         |\n| \"labor\"        | \"  labor\"     |         2 | 2 (2 space)   | No                          |\n| \"unionism\"     | \"unionismi\"   |         2 | 1             | Yes                         |\n| \"unionism\"     | \"unionismi\"   |         1 | 1             | Yes                         |\n| \"among\"        | \"amonng\"      |         2 | 1             | Yes                         |\n| \"among\"        | \"amonng\"      |         1 | 1             | Yes                         |\n| \"among\"        | \"amonnng\"     |         2 | 2             | No                          |\n| \"among\"        | \"amonnng\"     |         3 | 2             | No                          |\n| \"among\"        | \"amonnng\"     |        10 | 2             | No                          |\n+----------------+---------------+-----------+---------------+-----------------------------+\n```\n\nCan someone explain what is the effect of ```\nfuzziness```\n parameter on the query?\n    ", "Answer": "\r\nFuzziness Parameters :\n\nAUTO\nGenerates an edit distance based on the length of the term. Low and\nhigh distance arguments may be optionally provided AUTO:[low],[high].\nIf not specified, the default values are 3 and 6, equivalent to\nAUTO:3,6 that make for lengths:\n0..2 Must match exactly\n3..5 One edit allowed\n5 Two edits allowed AUTO should generally be the preferred value for\nfuzziness.\n\nIn the question, you have mentioned that when updated_value is ```\nlab```\n, then elasticsearch did not return the same result as before. But as you can see below, the results are coming accurately. This is because the edit distance is 2.\n```\nlab --> labo --> labor\n```\n\nSimilarly, the results are coming accurately for ```\nlabori```\n, ```\n  labor```\n and ```\namonnng```\n\nIndex Data:\n```\n{\n  \"text\":\"The organisation was formed by T.J. Pree and R.T. Sims. [[Philip Randolph]] was also on the board.&lt;ref name=yho&gt;[http://hierographics.org/yourhistoryonline/yourhistoryonlineVIII.htm Your History online], accessed 17 August 2010&lt;/ref&gt; The NBWA held its congress in [[Washington DC]] from  8–14 September 1919. There were 115 delegates primarily from the South. Three delegates were from the [[Industrial Workers of the World]] and fifteen from the Society for the Advancement of Trade Unionism among Negroes. No delegates were accepted from the [[American Federation of Labor]].&lt;ref&gt;''Revolutionary radicalism: its history, purpose and tactics with an exposition and discussion of the steps being taken and required to curb it, being the report of the Joint Legislative Committee Investigating Seditious Activities'', filed 24 April 1920, in the Senate of the State of New York&lt;/ref&gt;\"\n}\n```\n\nSearch Query:\n```\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"span_near\": {\n            \"clauses\": [\n              {\n                \"span_multi\": {\n                  \"match\": {\n                    \"fuzzy\": {\n                      \"text\": {\n                        \"value\": \"lab\",         <-- note this\n                        \"fuzziness\": \"2\"\n                      }\n                    }\n                  }\n                }\n              }\n            ],\n            \"slop\": 1,\n            \"in_order\": true\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\nSearch Result:\n```\n \"hits\": [\n      {\n        \"_index\": \"64492853\",\n        \"_type\": \"_doc\",\n        \"_id\": \"1\",\n        \"_score\": 2.2105305,\n        \"_source\": {\n          \"text\": \"The organisation was formed by T.J. Pree and R.T. Sims. [[Philip Randolph]] was also on the board.&lt;ref name=yho&gt;[http://hierographics.org/yourhistoryonline/yourhistoryonlineVIII.htm Your History online], accessed 17 August 2010&lt;/ref&gt; The NBWA held its congress in [[Washington DC]] from  8–14 September 1919. There were 115 delegates primarily from the South. Three delegates were from the [[Industrial Workers of the World]] and fifteen from the Society for the Advancement of Trade Unionism among Negroes. No delegates were accepted from the [[American Federation of Labor]].&lt;ref&gt;''Revolutionary radicalism: its history, purpose and tactics with an exposition and discussion of the steps being taken and required to curb it, being the report of the Joint Legislative Committee Investigating Seditious Activities'', filed 24 April 1920, in the Senate of the State of New York&lt;/ref&gt;\"\n        }\n      }\n    ]\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Python multiprocessing edit-distance calculation\r\n                \r\nI have only been programming for about a year so know the basics very well but I'm struggling to get my head around the python multiprocessing documentation. It would be great if someone could point me in the right direction for the problem at hand.\n\nI am using the python-Levenshtein c-module to calculate the pair-wise distance between a large number of DNA sequences (~5000-2000) and would like to speed things up using multiprocessing. Pseudo-code for a basic version of my problem is below:\n\n```\ndef edit_distance(seqA, seqB):\n    ...\n    return distance\n\nsequence_list = [seq1, seq2, ... seq10000]\nresults_dict = {}    \n\ncentroid = sequence_list[0]\nresults_dict[centroid] = {}\nfor target in sequence_list[1:]:\n    results_dict[centroid][target] = edit_distance(centroid, target)\n```\n\n\nWhen fully implemented this will be carried out using each seq as a centroid. It would be great if someone could point out which method would be best to multiprocess the calculation of distances for all targets in sequence_list[1:]. Thanks.\n    ", "Answer": "\r\nIt is straightforward to get multiprocessing version of your code:\n\n```\nfrom functools import partial\nfrom multiprocessing import Pool\n\ndef dist_mp(centroid, target):\n    return target, edit_distance(centroid, target)\n\ndef main():\n    # ...\n    pool = Pool() # use all CPUs\n    for target, d in pool.imap_unordered(partial(dist_mp, centroid),\n                                         sequence_list[1:]):\n        results_dict[centroid][target] = d\n    pool.close()\n    pool.join()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n\nIf ```\nedit_distance()```\n function releases GIL then you could use threads instead of processes:\n\n```\nfrom multiprocessing.dummy import Pool # use threads\n```\n\n\nNote: you might get better time performance if you use an algorithm with better time complexity (avoid calling ```\nedit_distance()```\n function ```\nO(n**2)```\n times in \"when fully implemented\" case) instead of just improving it by a constant factor by using multiprocessing.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance in Java: How arrange the code?\r\n                \r\nI am working on a Java project about Edit Distance, i.e. the minimum number of operations (out of three operations that are defined, see here for more info!). I am completely new to Java, and it seems like a great object oriented language, but maybe less numerical oriented like Matlab for instance. The problem is I do not know what all the corresponding functions in Matlab or Python are in Java that would realize my solution for this project, so all I need is a little constructive help on how to this.\n\nThe code is below (don't worry, I don't expect anyone to understand the code/algorithm, but it works!)\n\nCODE\n\n```\nimport java.util.LinkedList;\nimport java.util.List;\n\npublic class ClosestWords {\n  LinkedList<String> closestWords = null;\n  int closestDistance = -1;\n\n  int[][] partDist(String w1, String w2, int w1len, int w2len) {\n      int[][] M = new int[w1len+1][w2len+1];\n      for(int i=0;i<=w1len;i++) {\n          for(int j=0;j<=w2len;j++) {\n              if( i == 0) {\n                  M[i][j] = j;\n                  }\n              else if(j==0) {\n                  M[i][j] = i;\n                  }\n              else {\n                  char a = w1.charAt(i-1);\n                  char b = w2.charAt(j-1);\n                  int I = (a == b ? 0:1);\n                  M[i][j] = Math.min(Math.min(M[i-1][j]+1,M[i][j-1]+1),M[i-1][j-1]+I);\n              }\n          }\n      }\n  return M;\n  }\n\n  int[][] Distance(String w1, String w2) {\n    return partDist(w1, w2, w1.length(), w2.length());\n  }\n\n  public ClosestWords(String w, List<String> wordList) {\n      for (String s : wordList) {\n          int[][] M = Distance(w, s);\n          int dist = M[w.length()-1][s.length()-1];\n          // int dist = Distance(w, s);\n          // System.out.println(\"d(\" + w + \",\" + s + \")=\" + dist);\n          if (dist < closestDistance || closestDistance == -1) {\n              closestDistance = dist;\n              closestWords = new LinkedList<String>();\n              closestWords.add(s);\n              }\n          else if (dist == closestDistance)\n              closestWords.add(s);\n          }\n      }\n\n  int getMinDistance() {\n    return closestDistance;\n  }\n\n  List<String> getClosestWords() {\n    return closestWords;\n  }\n}\n```\n\n\nNow, what I would like to do (but I don'tknow how to do), is to update the matrix ```\nM```\n inside the ```\nfor```\n loop in ```\nClosestWords```\n. In Matlab this would be easy: I'd just set the matrix to some initial form, then for every loop we would obtain a new matrix from the function call ```\nDistance(w, s)```\n. This new matrix, in turn, I would like to modify, that is, remove a number of the last rows from it. How do I do this? For example, I have a ```\nM```\n matrix that is 4 by 4, then I remove the last row so I get ```\nM_new```\n that is 3 by 4. Is it possible?\n\nAlso, if I have to strings of possible different lengths, how do I check (in the easiest way) how many of their first letters that are the same? That is, the maximum length of the substrings of the strings starting at the left and that are equal to one another? For example, ```\ncompute```\n and ```\ncommute```\n would have three first letters (starting from the left) in common, thus three of the first letters are the same.\n\nBest regards,\n    ", "Answer": "\r\nJava is not very well suited for this type of work (APL comes to mind here). If this is not an exercise, I would use existing libairies to do this. If this is an exercise, I would check how an open source library does this. \n\nIn the end, you could:\n\n1) Copy the original content into a newly allocated matrix of smaller size.\n\n2) Shift values in your current matrix and having external data to keep track of matrix's logical size.\n\n3) ...\n\nFor your second question, I would add the words to a tree structure and find the longest sub-branch starting from the root having at least two sub-branches.\n\nOr simply sort alphabetically and compare every adjacent string.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein / edit distance for arbitrary sequences [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has an answer here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Levenshtein type algorithm with numeric vectors\r\n                            \r\n                                (1 answer)\r\n                            \r\n                    \r\n                Closed 6 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI want to compute the Levenshtein distance between two arbitrary sequences.\n\n```\na <- 1:100\nb <- c(1, 1:100)\n\nedit_distance(a, b) == 1\n```\n\n\nI am aware of the ```\nadist```\n function and the ```\nstringdist```\n package, but they only work on character vectors. If the number of symbols in the sequences were small, I could just encode them as characters and use the above functions. \n\nBut there will typically be on the order of 1000 different symbols. Another option would be to encode them as Unicode characters (```\nadist```\n works on them: ```\nadist(\"\\U00001\", \"\\U00001\\U00002\")```\n), but I don't know how to do this.\n    ", "Answer": "\r\nYou can use ```\nintToUtf8```\n to map your integers to Unicode characters:\n\n```\na2 <- intToUtf8(a)\nb2 <- intToUtf8(b)\n\nadist(a2, b2)\n#      [,1]\n# [1,]    1\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance in C function called in Python does not return the correct result\r\n                \r\nI'm trying to test the call to a C (edit distance) function in Python, to compare the execution time with a similar function written in Python, it's pure curiosity and a way to understand the C/Python articulation.\nHere the implementation of the Levenshtein distance in C (source : wikibooks) in ```\n_distance.c```\n file :\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\nint min(int a, int b, int c)\n{\n    if(a <= b && a <= c)\n    {\n        return a;\n    }\n    else if(b <= a && b <= c)\n    {\n        return b;\n    }\n    else if(c <= a && c <= b)\n    {\n        return c;\n    }\n}\n\nint levenshtein(char *s1, char *s2) {\n    unsigned int x, y, s1len, s2len;\n    s1len = strlen(s1);\n    s2len = strlen(s2);\n    unsigned int matrix[s2len+1][s1len+1];\n    matrix[0][0] = 0;\n    for (x = 1; x <= s2len; x++)\n        matrix[x][0] = matrix[x-1][0] + 1;\n    for (y = 1; y <= s1len; y++)\n        matrix[0][y] = matrix[0][y-1] + 1;\n    for (x = 1; x <= s2len; x++)\n        for (y = 1; y <= s1len; y++)\n            matrix[x][y] = min(matrix[x-1][y] + 1, matrix[x][y-1] + 1, matrix[x-1][y-1] + (s1[y-1] == s2[x-1] ? 0 : 1));\n\n    return(matrix[s2len][s1len]);\n}\n\n```\n\nThen I created a ```\n_distance.so```\n file, and I call my function as follows :\n```\n\nfrom ctypes import *\n\nso_file = './_distance.so'\ndll = cdll.LoadLibrary(so_file)\n\n# source text\nreference = \"\"\"\nNe vous défiez jamais de votre voisin de gauche qui a une chemise de grosse toile, une cravate blanche, un habit propre, \nmais de drap commun ; suivez plutôt très-attentivement les mouvemens de ce voisin de droite, dont la cravate est bien \nmise et fine, qui a de grosses breloques, des favoris, un air d’honnête homme, le parler hardi ; \nc’est celui-là qui vous volera votre mouchoir ou votre montre. \n\"\"\"\n\n# target text with errors\nhypothesis = \"\"\"\nNe NOOOS défiez jamais de votre voisin de Gauche qui a une chemises de grosse toile, une cravate blanche, un habit propre, \nmais de drap commun ; suivre plu très-attentivement les Mouvemens de ce voisin de droite, dont la cravate est bien \nmise et fine, qui a de grosses breloques. \n\"\"\"\n\nprint(dll.levenshtein(reference, hypothesis))\n\n```\n\nwhich returns me ```\n0```\n as a result, while i will have to recover ```\n132```\n (The result of my Python function).\nI can't figure out if this is from the C code, the type of input variables to ```\ndll.levenshtein ()```\n, or an error in using ```\nctypes```\n package?\nthank you very much for your help\n[EDIT]\nI tested my ```\nlevenshtein()```\n function in C code directly:\n```\nint main()\n{\n    char *ref = \"Ne vous défiez jamais de votre voisin de gauche qui a une chemise de grosse toile, une cravate blanche, un habit propre,\",\n\n    *target = \"Ne NOOOS défiez jamais de votre voisin de Gauchate blanche, un habit propre,\";\n    int result;\n\n    result = levenshtein(ref, target);\n    printf(\"%i\", result);\n\n    return result;\n}\n\n```\n\nand I have an output with a result.\nSo I think this came from my call in Python code.\n    ", "Answer": "\r\nPython string is not ```\nchar*```\n in C, which points bytes, especially if you are using Python 3.\nYou should convert a Python string to bytes before you pass it to C.\n```\n# source text\nreference = \"\"\"\n...\n\"\"\".encode('utf-8')\n\n# target text with errors\nhypothesis = \"\"\"\n...\n\"\"\".encode('utf-8')\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "what modifications should be done in edit distance algo if there are diffrent weights for addition/deletion or replacement\r\n                \r\nP.S.  if there are diffrent weightage for addition , replacement and deletion . Than is there any algorithm which could help me .\n\nOr, what sort of modifications are required in Wagner–Fischer algorithm so as to minimize the edit distance if weights for addition/deletion and replacement are diffrent ?\n    ", "Answer": "\r\nMost optimal I know by far is Levenshtein, you can also have a look into this publication. Hope it helps :)\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Finding which error(s) are detected by Damerau-Levenshtein edit distance algorithm\r\n                \r\nI'm creating a spelling correction tool and wanted to implement a noisy channel with Bayes theorem. In order to do so, I need to calculate the probability P(X|W), where X is the given (misspelled) word, and W is the possible correction. The probability is given by getting a value from a confusion matrix, that depends on knowing which type of error happened, meaning that if for example X = \"egh\" and W = \"egg\" then the edit distance would be 1, and the error would be a substitution error that happened on character number 2.\nI'm trying to find a way to get the error \"type\" as well as the character it happened for, but can't seem to make it work.\nI've tried creating a TreeMap and inserting i/j values whenever an error is detected, but it didn't work.\nI may assume that there's only one error, meaning that the edit distance is exactly 1.\nHere's my code:\n```\npublic static int DLD(String s1, String s2) {\n    if (s1 == null || s2 == null) {  // Invalid input\n        return -1;\n    }\n\n    if (s1.equals(s2)) {  // No distance to compute\n        return 0;\n    }\n\n    // The max possible distance\n    int inf = s1.length() + s2.length();\n\n    // Create and initialize the character array indices\n    HashMap<Character, Integer> da = new HashMap<>();\n    for (int i = 0; i < s1.length(); ++i) {\n        da.put(s1.charAt(i), 0);\n    }\n    for (int j = 0; j < s2.length(); ++j) {\n        da.put(s2.charAt(j), 0);\n    }\n\n    // Create the distance matrix H[0 .. s1.length+1][0 .. s2.length+1]\n    int[][] distances = new int[s1.length() + 2][s2.length() + 2];\n\n    // initialize the left and top edges of H\n    for (int i = 0; i <= s1.length(); ++i) {\n        distances[i + 1][0] = inf;\n        distances[i + 1][1] = i;\n    }\n\n    for (int j = 0; j <= s2.length(); ++j) {\n        distances[0][j + 1] = inf;\n        distances[1][j + 1] = j;\n\n    }\n\n    // fill in the distance matrix H\n    // look at each character in s1\n    for (int i = 1; i <= s1.length(); ++i) {\n        int db = 0;\n\n        // look at each character in s2\n        for (int j = 1; j <= s2.length(); ++j) {\n            int i1 = da.get(s2.charAt(j - 1));\n            int j1 = db;\n\n            int cost = 1;\n            if (s1.charAt(i - 1) == s2.charAt(j - 1)) {\n                cost = 0;\n                db = j;\n            }\n\n            distances[i + 1][j + 1] = min(\n                    distances[i][j] + cost, // substitution\n                    distances[i + 1][j] + 1, // insertion\n                    distances[i][j + 1] + 1, // deletion\n                    distances[i1][j1] + (i - i1 - 1) + 1 + (j - j1 - 1));\n\n        }\n\n        da.put(s1.charAt(i - 1), i);\n    }\n\n    return distances[s1.length() + 1][s2.length() + 1];\n}\n```\n\nAny hint/direction towards solving this would be much appreciated.\nThanks!\nEdit 1:\nI figured something out and it seems to be working, although I'm not 100% sure. I replaced the code segment where I use the min() method with this:\n```\nint sub = distances[i][j] + cost;\nint ins = distances[i + 1][j] + 1;\nint del = distances[i][j + 1] + 1;\nint trans = distances[i1][j1] + (i - i1 - 1) + 1 + (j - j1 - 1);\n\ndistances[i + 1][j + 1] = min(sub, ins, del, trans);\n\nif ((distances[i][j] == 0 || distances[i - 1][j] == 0 || \n     distances[i][j - 1] == 0 || distances[i + 1][j + 1] == trans) &&\n                    distances[i + 1][j + 1] == 1) {\n                \n    TreeMap<String, Integer> error = mappingTermAndError.getOrDefault(s2, null);\n    if (error != null) {\n        error.clear();\n    } else {\n        error = new TreeMap<>();\n    }\n\n    if (distances[i + 1][j + 1] == trans) {\n        error.put(\"trans\", i - 2);\n\n    } else if (distances[i + 1][j + 1] == del) {\n        error.put(\"del\", i - 1);\n\n    } else if (distances[i + 1][j + 1] == ins) {\n        error.put(\"ins\", i - 1);\n\n    } else {  // distances[i + 1][j + 1] == sub\n        error.put(\"sub\", i - 1);\n    }\n    mappingTermAndError.put(s2, error);\n}\n```\n\nWhat it basically does is get the value for each error type, then calculate the minimum.\nif The new minimum is 1 (so this is the first error) and also one of the previous cells in the distance matrix is 0 (meaning there's a path with no errors leading to that point) or if the error is transposition (which we can only know about after we've already had an error) than I replace the previously registered error with the new one, and get the 'i' corresponding with the character the error was done for.\nI'm aware that this solution is pretty ugly and probably not very efficient, so if someone has any thoughts on how to improve that it would be great.\n    ", "Answer": "\r\nThe error type and characters involved have to be stored somewhere. You can have them in separate data structures, or you can have them in encapsulated in objects.\nHere's what it could look like using objects. For simplicity I'm implementing only Levenshtein distance, but I'm sure you can easily apply the technique to Damerau–Levenshtein.\nFirst you need to define a class that encapsulates the information about an edit: cost, parent, and any extra information like type (replace, insert, delete) or the characters involved. To keep things simple I'm keeping a single string called \"type\" for this extra info, but you would want to add separate fields for the type of error, the character indices, etc. You may even want to use inheritance to create different subtypes of edits with different behavior.\n```\nclass Edit implements Comparable<Edit> {\n    int cost;\n    Edit parent;\n    String type;\n\n    public Edit() {\n        // create a \"start\" node with no parent and zero cost\n    }\n\n    public Edit(String type, Edit parent, int cost) {\n        this.type = type;\n        this.cost = parent.cost + cost;\n        this.parent = parent;\n    }\n\n    @Override\n    public int compareTo(Edit o) {\n        return Integer.compare(this.cost, o.cost);\n    }\n\n    @Override\n    public String toString() {\n        return type;\n    }\n}\n```\n\nThen you use this class instead of just ```\nint```\n for the distance table. At 0,0 there is a special start node with no parent. At all other points you choose a node with one parent or another according to the minimum cost it takes to arrive at that node. To be more flexible, let's split out the building of the matrix out of the editDistance method:\n```\nEdit[][] buildMatrix(String s1, String s2) {\n    Edit[][] distance = new Edit[s1.length() + 1][s2.length() + 1];\n\n    distance[0][0] = new Edit();\n    for (int i = 1; i <= s1.length(); i++) {\n        distance[i][0] = new Edit(\"-\" + s1.charAt(i - 1), distance[i - 1][0], 1);\n    }\n    for (int j = 1; j <= s2.length(); j++) {\n        distance[0][j] = new Edit(\"+\" + s2.charAt(j - 1), distance[0][j - 1], 1);\n    }\n\n    for (int i = 1; i <= s1.length(); i++) {\n        for (int j = 1; j <= s2.length(); j++) {\n            int replaceCost = s1.charAt(i - 1) == s2.charAt(j - 1) ? 0 : 1;\n            distance[i][j] = Collections.min(List.of(\n                // replace or same\n                new Edit(s1.charAt(i - 1) + \"/\" + s2.charAt(j - 1), distance[i - 1][j - 1], replaceCost),\n                // delete\n                new Edit(\"-\" + s1.charAt(i - 1), distance[i - 1][j], 1),\n                // insert\n                new Edit(\"+\" + s2.charAt(j - 1), distance[i][j - 1], 1)));\n        }\n    }\n\n    return distance;\n}\n```\n\nThen the \"edit distance\" function only needs to take the cost of the last node:\n```\nint editDistance(String s1, String s2) {\n    Edit[][] distance = buildMatrix(s1, s2);\n    return distance[s1.length()][s2.length()].cost;\n}\n```\n\nBut thanks to the \"parent\" pointers, you can also easily construct the list of edits needed to change one string to the other, also known as a \"diff\":\n```\nList<Edit> diff(String s1, String s2) {\n    Edit[][] distance = buildMatrix(s1, s2);\n    List<Edit> diff = new ArrayList<>();\n    Edit edit = distance[s1.length()][s2.length()];\n    while (edit != distance[0][0]) {\n        diff.add(edit);\n        edit = edit.parent;\n    }\n    Collections.reverse(diff);\n    return diff;\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein edit-distance between rows and columns\r\n                \r\nI have created the following empty DataFrame:\n```\ncolumns = ['Band','Tree','Foot']\nrows = ['Hand', 'Foot', 'Shoulder']\n\ndf = pd.DataFrame(index=rows, columns=columns)\n```\n\nI want to calculate the distance between the columns and the rows and am currently using the following code:\n```\nimport pandas as pd\nimport nltk\n\ndef distance(x):\n    i = x.index\n    j = x.name\n    return nltk.edit_distance(i,j)\n\ndf = df.apply(distance)\n```\n\nBut this returns:\n\n\n\n\n-\n-\n\n\n\n\nBand\n4\n\n\nTree\n4\n\n\nFoot\n4\n\n\n\n\nI would like it to return the distance between the corresponding column and row for each cell.\n\n\n\n\n\nBand\nTree\nFoot\n\n\n\n\nHand\n1\n4\n4\n\n\nFoot\n4\n4\n0\n\n\nShoulder\n7\n7\n7\n\n\n\n\nWhat am I missing?\n    ", "Answer": "\r\n```\nedit_distance```\n expects 2 strings, so you have to iterate over the indexes. One option is to apply a lambda that does that on ```\ndf```\n:\n```\ndf.apply(lambda col: [nltk.edit_distance(col.name, i) for i in col.index])\n```\n\nBut, instead of filling in a DataFrame, I think it's simpler to first create a dictionary with the values; then build a DataFrame as follows:\n```\ndf = pd.DataFrame({j: {i: nltk.edit_distance(i,j) for i in rows} for j in columns})\n```\n\nOutput:\n```\n          Band  Tree  Foot\nHand         1     4     4\nFoot         4     4     0\nShoulder     7     7     7\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How Levenshtein's edit distance algorithm works?\r\n                \r\nI was going through Levenshtein distance algorithm in which I grasped some of its initial steps but in subsequent steps when it starts counting \"cost\" ,I had hard time to understand it. I don't get the purpose of counting cost and how it is helping the algorithm to achieve its goal. Please help me understand this algorithm.\n    ", "Answer": "\r\nThere are many different ways to edit the first string by means of insertions/deletions/substitutions to get the second string (actually infinitely many). Each has a specific count of elementary edit operation.\n\nThe Levenshtein distance is defined as the minimum number of operations required, i.e. the length of the shortest sequence. This number is well defined, even though it can be achieved by several different edit sequences.\n\nThis is very similar to the case of the Euclidean distance: you can go from one point to another following various trajectories, but only the straight line achieves the minimum path length.\n\nUPDATE:\n\nNeed to add that assigning different costs to the different operations provides more flexibility and allows to give favor to one or another. Even more, you can assign individual weights to every characters, so that, for instance, trading a 'O' for a '0' is considered more \"serious\" than inserting a space.\n\nAnd the cost minimization principle remains.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to analyze the time complexity and space complexity of the edit distance using the Depth First Search (DFS) +memo approach?\r\n                \r\nit is well known that the edit-distance problem could be solved using the iterative DP approach. However, if we solve the problem using the recursive Depth First Search (DFS) + memo approach, how to calculate the time complexity? Why?\n\nThe codes are provided as below\n\n```\nclass Solution:\n    def minDistance(self, word1, word2):\n        return self.dfs(word1, 0, word2, 0, {})\n\n    def dfs(self, word1, p1, word2, p2, memo): ### p1, p2 refer to the current position in word1, word2 respectively\n        if p1 == len(word1):\n            return len(word2[p2:]) ### termination of the recursion\n        if p2 == len(word2):\n            return len(word1[p1:]) ### termination of the recursion\n        if (p1, p2) in memo:    ### the key of the memo is the two positions, value is the mini edit distance for word1[p1:] and word2[p2:]\n            return memo[(p1, p2)]\n        if word1[p1] == word2[p2]:  ### Case1: word1[p1] equals to word2[p2]\n            temp = self.dfs(word1, p1+1, word2, p2+1, memo)\n        else:  ### Case2: word1[p1] not equals to word2[p2].  \n            temp = min(self.dfs(word1, p1, word2, p2+1, memo), self.dfs(word1, p1+1, word2, p2, memo), self.dfs(word1, p1+1, word2, p2+1, memo)) + 1\n        memo[(p1, p2)] = temp\n        return temp\n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "One Edit Distance Away\r\n                \r\nI  am currently doing some coding in the break to be fresh ready for semester 2 at university.\nI have encountered a CTCI problem which I am struggling to understand, I have also looked at the hints, but still am a bit clueless on how to approach it\nThe question\nOne Away: There are three types of edits that can be performed on strings: Insert a character, remove a character or replace a character. Given two strings write a function to check if they are one or zero edits away\nSample INPUT AND OUTPUT\n\nInput -> pale, ple  Output-> true\nInput -> pales, pale Output -> true\nInput -> pale, bale Output -> true\nInput -> pale, bake Output -> false\n\nPLEASE DO NOT GIVE ME THE SOLUTION\nI have read the hints, and still do not understand how I should approach this problem, I understand that in order for an insertion to be valid, the length of String word1 and word2 must have a difference of 1.\nCan someone please give me some hints on where I should start, when completing this problem. Thank you.\n    ", "Answer": "\r\nStart by breaking the problem into smaller pieces.\nIf the strings are the same, there has been no change, so first check equality.\nIf the strings are different, there are 3 different outcomes:\n\na character has been replaced\na character has been removed\na character has been added\n\nTreat each case separately. Add a new method that tries to detect each case, and call these methods from the main method of your solution. This will make the code structure easier to understand and to test.\nIn each case, you will use a loop to compare the characters in the two strings.\nTo find if one character has been replaced, count how many positions have different characters. If exactly 1, it's a replacement. If more than 1, it's a different edit.\nMake sure that you can detect 1 character replacements before you continue with the remove and add cases.\nTo find if a character has been removed, count the number of positions with different characters like above, but with a slight modification: when you find a difference, increment the position of one of the counters so that you skip over a character in one of the string. This sounds confusing now, but it will be clearer  once you have written working code to detect the replacement case above. If you get stuck, you can always post a new a question here and get help with your code.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is there a way to identify or return characters contributing to difference between 2 words in edit distance in python?\r\n                \r\nI am looking find patterns in spelling mistakes. I am able to use edit distance to see how similar 2 words(correct word and candidate word) are. However, I am also interested to know which characters are contributing in creating the difference between the 2 words.\n    ", "Answer": "\r\nIt depends on the library and method that you are using. For example, in this library you can use ```\neditops```\n function to find all edit operations in addition to the Levenstein distance of two strings:\n\n```\neditops('spam', 'park')\n# [('delete', 0, 0), ('insert', 3, 2), ('replace', 3, 3)]\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to calculate graph edit distance between two multi-graphs using networkx?\r\n                \r\nSay I have two graphs G and H:\n\n```\n    G = nx.MultiDiGraph()\n    G.add_node(0, id = 'a', f = 'none')\n    G.add_node(1, id = 'b', f = 'k')\n    G.add_edge(0, 1, r = 'bicycle')\n    G.add_edge(1, 0, r = 'bicycle_r')\n    G.add_edge(1, 0, r = 'bicycle_rr')\n\n    H = nx.MultiDiGraph()\n    H.add_node(0, id = 'a', f = 'none')\n    H.add_node(1, id = 'b', f = 'k')\n    H.add_edge(0, 1, r = 'bicycle')\n    H.add_edge(1, 0, r = 'bicycle_r')\n```\n\n\n```\nnetworkx```\n provides us a very simple api to calculate the edit distance between two graphs of the same type, i.e., ```\nnx.graph_edit_distance(G, H, node_match=node_match, edge_match=edge_match)```\n. Here for ```\nnode_match```\n and ```\nedge_match```\n, I simply define them as exact match for all labels (i.e., ```\nid```\n and ```\nf```\n for nodes, and ```\nr```\n for edges). This is trivial and not important here since there is no substitution operation between 2 graphs in my example.\n\nThe edit distance between G and H is expected to be 1, because G has one more edge between node 1 and 0. However, the answer returned by ```\nnx.graph_edit_distance```\n is 0. It seems like it just ignores the multi-edges between two graph nodes. I have tried it with simple directed graph (i.e. ```\nnx.DiGraph()```\n), it works well. But I really don't know how to tell the distance calculator to take the deletion of multi-edge into consideration.\n\nI think something is wrong with ```\nnx.graph_edit_distance```\n when applying it to multi-graphs. I have tried ```\nnx.is_isomorphic```\n on multi-graphs. This function returns whether two graphs are isomorphic (i.e., whether the distance is 0), and takes exactly the same input as ```\nnx.graph_edit_distance```\n. They both have ```\nnode_match```\n and ```\nedge_match```\n. As suggested by the documentation, the input for ```\nedge_match```\n should be ```\nG1[u1][v1], G2[u2][v2]```\n, however, when I check the input to ```\nedge_match```\n for ```\nnx.graph_edit_distance```\n and ```\nnx.is_isomorphic```\n, I found that the input edges for ```\nedge_match```\n in ```\nnx.graph_edit_distance```\n doesn't have the shape of edges in multi-graph, while it does have that shape for ```\nedge_match```\n in ```\nnx.is_isomorphic```\n. I think this could be a serious bug of networkx.\n\nEdit:\nI checked the source code of networkx. Now I am pretty sure it's a bug. The usage of ```\nedge_match```\n is not consistent with what is claimed in the documentation.\n\n```\n    elif edge_match:\n        C[0:m, 0:n] = np.array([1 - int(edge_match(G1.edges[g], G2.edges[h]))\n                                for g in pending_g for h in pending_h]\n                               ).reshape(m, n)\n```\n\n\nHere is my implementation for ```\nnode_match```\n and ```\nedge_match```\n:\n\n```\ndef node_match(n1, n2):\n    if n1['id'] == n2['id'] and n1['f'] == n2['f']:\n        return True\n    else:\n        return False\n\ndef edge_match(e1, e2):\n    if e1['r'] == e2['r']:\n        return True\n    else:\n        return False\n\ndef multi_edge_match(e1, e2):\n    if e1.keys() != e2.keys():\n        return False\n    for k in e1.keys():\n        if e1[k]['r'] != e2[k]['r']:\n            return False\n    return True\n\n```\n\n\nNote that, based on the documentation, for multi-graphs, ```\nedge_match```\n should not work, because in multi-graph G[u1][u2] is first indexed by an edge id, so I defined ```\nmulti_edge_match```\n. However,```\nnx.graph_edit_distance(G, H, node_match, multi_edge_match)```\n will lead to running error. We can only call ```\nnx.graph_edit_distance(G, H, node_match, edge_match)```\n and then get unexpected results. While for ```\nnx.is_isomorphic```\n, we can only call ```\nnx.is_isomorphic(G, H, node_match, multi_edge_match)```\n instead of ```\nnx.is_isomorphic(G, H, node_match, edge_match)```\n, and this is what I have expected. So I think it's likely that the implementation for ```\nnx.graph_edit_distance```\n is not correct for multi-graphs. At least, it is not consistent with the documentation, if not incorrect.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance similarity sas?\r\n                \r\nI have a list of domains in a table V_tablas.arreglo(columns--> domainsBad):\n   ```\n@hotmai.es\n    @ghotmail.es\n    @hotmaol.com\n    @hotmai.com\n    @otmail.com.....etc(more than 10k)```\n\nAnd need to correct this domains to \"@hotmail.com\"\nMy questions is about EDIT_DISTANCE_SIMILARITY(fuzzy logic) of oracle for get 'Returns an integer Between 0 and 100, Where 0 Indicates no similarity at all and 100 Indicates a perfect match' Is it posible?\n    ", "Answer": "\r\nSAS has at least a couple functions for calculating edit distance between two strings:\n\nCompged, for general edit distance:\nhttp://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a002206133.htm\n\nComplev, for Levenshtein distance:\nhttp://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a002206137.htm\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Algorithm to find all substrings of a string within a given edit distance of another string\r\n                \r\nI know the title is a bit messy, so let me explain in detail:\n\nI have two strings, T and P. T represents the text to be searched, and P represents the pattern to be searched for. I want to find ALL substrings of T which are within a given edit distance of P.\n\nExample:\n\n```\nT = \"cdddx\"\nP = \"mdddt\"\n```\n\n\nSay I wanted all substrings within edit distance 2, the answers would be:\n\n```\ncdddx //rewrite c and x\ndddx // insert m, rewrite x\ncddd //insert t, rewrite c\nddd // insert m and t\n```\n\n\nDon't know if that's all of them, but you get the point.\n\nI know the Wagner–Fischer algorithm can be employed for solution of this problem - I check the numbers of the last row of the Wagner–Fischer matrix and see if they fulfill this condition and find the substrings that way, then run the algorithm again for T', where T' is T where the first letter has been removed, and so on. The problem is the time complexity of this shoots up to a staggering O(T^3*P). I'm looking for a solution close to the original time complexity of the Wagner-Fisher algorithm, i.e. O(T*P). \n\nIs there a way to get this done in such time or something better than what I have right now? Note that I am not necessarily looking for a Wagner-Fischer solution, but anything is ok. Thanks!\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is it possible to calucate the edit distance between a regexp and a string?\r\n                \r\nIf so, please explain how.\n\nRe: what is distance -- \"The distance between two strings is defined as the minimal number of edits required to convert one into the other.\"\n\nFor example, xyz to XYZ would take 3 edits, so the string xYZ is closer to XYZ and xyz.\n\nIf the pattern is [0-9]{3} or for instance 123, then a23 would be closer to the pattern than ab3.\n\nHow can you find the shortest distance between a regexp and a non-matching string? \n\nThe above is the Damerau–Levenshtein distance algorithm.\n    ", "Answer": "\r\nYou can use Finite State Machines to do this efficiently (that is, linear in time).  If you use a transducer, you can even write the specification of the transformation fairly compactly and do far more nuanced transformations than simply inserts or deletes - see wikipedia for Finite State Transducer as a starting point, and software such as the FSA toolkit or FSA6 (which has a not entirely stable web-demo) too.  There are lots of libraries for FSA manipulation; I don't want to suggest the previous two are your only or best options, just two I've heard of.\n\nIf, however, you merely want the efficient, approximate searching, a less flexibly but already-implemented-for-you option exists: TRE, which has an approximate matching function that returns the cost of the match - i.e., the distance to the match, from your perspective.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "What algorithm can compute the edit distance between two strings with arbitrary insert, delete, replace and swap costs?\r\n                \r\nI want to compute the edit distance between two strings, using 4 operations: character insertion, deletion, replacement and swapping. Each operation is associated with a cost. For instance, if the first three costs are equal to 10 and the last is equal to 1, we have:\n\ndistance(abc,bca) = 2 (swap a and b then swap a and c)\n\ndistance(abc,bae) = 11 (swap b and a then replace c by e)\n\ndistance(abcd,bdca) = 4\n\nI read the Wikipedia article about Damerau-Levenshtein distance, but the algorithm they give works only if 2 × swapping cost ≥ insert cost + delete cost. \n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Scala: Find edit-distance for all elements of a list not fitting in memory\r\n                \r\nIn my previous question I was asking for advice on an algorithm to compare all elements in huge list:\nScala: Compare all elements in a huge list\n\nA more general problem I am facing and would be grateful to get some advice is to do approximate comparison of list elements for a list not fitting into memory all at once. I am building this list from SQL request that returns a cursor to iterate a single string field of about 70 000 000 records. I need to find edit-distance (http://en.wikipedia.org/wiki/Edit_distance) between every two string elements in this list.\n\nMy idea here is to use sliding window of N records to compare all 70 000 000 records:\n\n\nRead N elements into a list that nicely fit into memory (N ~ 10 000)\nCalculate edit-distance for all elements in this list using algorithm described here:\nScala: Compare all elements in a huge list\nRead next N elements (from N to 2N-1) into a new list. Compare all these as in 2.\nRewind SQL query cursor to the first record\nCompare every string starting from index 0 to N  with all strings in this new list using the same algorithm as in 2.\nSlide window to read strings form 2N to 3N-1 records into a new list\nCompare every string starting from index 0 to 2N with all strings in this new list using the same algorithm as in 2.\netc.\n\n\nAll comparison results I need to write into DB as (String, String, Distance) records where first two elements are strings to match and third is a result.\n\nQuestions:\n\n\nHow to force Scala to garbage collect unneeded lists from the previous steps of this algorithm?\nThis algorithm is awful in terms of number of calculations required to do the job. Any other algorithms, ideas on how to reduce complexity?\n\n\nThanks!\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Minimum edit distance of two anagrams given two swap operations\r\n                \r\nGiven two anagrams S and P, what is the minimum edit distance from S to P when there are only two operations:\n\n\nswap two adjacent elements\nswap the first and the last element\n\n\nIf this question is simplified to only having the first operation (i.e. swap two adjacent elements) then this question is \"similar to\" the classical algorithm question of \"the minimum number of swaps for sorting an array of numbers\" (solution link is given below)\n\nSorting a sequence by swapping adjacent elements using minimum swaps\n\nI mean \"similar to\" because when the two anagrams have all distinct characters:\n\n```\nS:  A B C D\nP : B C A D \n```\n\n\nThen we can define the ordering in P like this\n\n```\nP: B C A D\n   1 2 3 4\n```\n\n\nThen based on this ordering the string S becomes\n\n```\nS: A B C D\n   3 1 2 4\n```\n\n\nThen we can use the solution given in the link to solve this question.\n\nHowever, I have two questions:\n\n\nIn the simplified question that we can only swap two adjacent elements, how can we get the minimum number of swaps if the anagrams contain duplicate elements. For example,\n\nS: C D B C D A A\n\nP: A A C D B C D\nHow to solve the complete question with two swap operations?\n\n    ", "Answer": "\r\nOne approach is to use http://en.wikipedia.org/wiki/A*_search_algorithm for the search.  Your cost function is half of the sum of the shortest distances from each element to the nearest element that could possibly go there.  The reason for half is that the absolutely ideal set of swaps will at all points move both elements closer to where they want to go.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is it possible for a 2 letter search term to match on a 3 letter indexed term if the edit distance = 2 between them\r\n                \r\nI'm trying to write unit tests for Fuzzy matching in my project. However, I have one test that is failing even though it shouldn't according to Lucene documentation. The search term is 2 characters in length, the indexed term is 3 characters in length and the edit distance is 2. According to lucene documentation, this search term should not match on this indexed term as edit distance should be > length of smallest matched term.\n\nThe search term is \"je\", the indexed term is \"jon\". According to Damareau levenshtein distance which is used by Lucene, the edit distance here is 2. According to Lucene official documentation on FuzzyQuery (https://lucene.apache.org/core/8_1_0/core/org/apache/lucene/search/FuzzyQuery.html) , the edit distance should be greater than the length of the shortest term (search term or indexed term). Therefore by this logic, my search query of \"je\" should not match on \"jon\". \n\nAs a background, I am using a boolean query with prefixQuery and fuzzyQuery. Partial matching is enabled with prefixQuery, and fuzzy matching with the fuzzyQuery. I am not storing any NGrams or stemming words. I am using a simple standardAnalyzer for indexing and searching. \n\nLastly, I printed out the explain score for this search result and the explanation seems to have validated my concern that this may be a bug in Lucene. The output for the Explain method is:\n\n\"0.0 = sum of: 0.0 = ConstantScore(name:jon)^0.0\"\n\nThis shows that the calculated relevance score is 0 but yet the result is being matched and returned.\n\nAny experiences with incorrect documentation by Lucene or bugs in the software?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Finding which elements in a vector have an edit distance of one and the same length?\r\n                \r\nI have a dataframe, an example in .csv format is shown below, that has a list of words (Word), the number of sounds in those words (NumSounds), and the transcription of the sounds in each word (Pronunciation). I have been trying to create a file that shows me what the minimal pairs are for each word in the list. This means, for every word, I need to know which other words in the list have an edit distance of 1 from that word while also having the same number of sounds. I have been doing this is R. The loop that my lab-mate wrote is shown below:\n\n```\nlibrary(stringdist)\nwords = vector(mode=\"character\", length=nrow(df))\npairs = vector(mode=\"character\", length=nrow(df))\n\npb = txtProgressBar(min=0, max=nrow(df), style=3)\nfor(i in 1:nrow(df)) {\n  word = df$Pronunciation[i]\n  nphones = df$NumSounds[i]\n  potential_minimal_pairs = as.list(df$Pronunciation[df$Word != word & df$NumSounds == nphones])\n  distances = stringdist(word, potential_minimal_pairs, method=\"lv\")\n  minimal_pairs = potential_minimal_pairs[distances == 1]\n  word = unique(df$Word[datf$Pronunciation == word])[1]\n  words = append(words, word)\n  words[i] = word\n  minimal_pairs = sapply(pairs, function(x) unique(df$Word[datf$Pronunciation == x])[1])\n  pairs[i] = paste(minimal_pairs, \", \")\n  setTxtProgressBar(pb, i)\n}\n\nmyminimalpairs = data.frame(word=words, pairs=pairs)\nhead(myminimalpairs, 10)\n```\n\n\n\r\n\r\n```\nWord,NumSounds,Pronunciation\r\nabbey,3,&bi\r\nabide,4,^b#d\r\nabort,5,^b>rt\r\nabroad,5,^br>d\r\nabrupt,6,^br^pt\r\nabsence,6,&bs^ns\r\nabsent,6,&bs^nt\r\nabsorb,6,^bz>rb\r\nabsorbed,7,^bz>rbd\r\nabstract,8,&bstr&kt\r\nabused,6,^byuzd\r\nabyss,4,^bIs\r\naccents,7,&ksEnts\r\naccepts,7,&ksEpts\r\naccessed,6,&ksEst\r\naccord,5,^k>rd\r\naccuse,5,^kyuz\r\nachieve,4,^Civ\r\nachieved,5,^Civd\r\naching,4,ekIN```\n\r\n\r\n\r\n\n    ", "Answer": "\r\nOne option is to use ```\nstringdistmatrix()```\n to create a matrix of distances that are equal to one and ```\nouter()```\n to create a second matrix of equal numsounds.   The array indices can then be used to return the word pairs that meet the criteria (using only the lower triangle to avoid duplicates).  \n\n```\nlibrary(stringdist)\n\nm1 <- as.matrix(stringdistmatrix(df$Word, method = \"lv\", useNames = TRUE)) == 1\nm2 <- outer(df$NumSounds, df$NumSounds, `==`)\nidx <- which(m1 & m2 & lower.tri(m1), arr.ind = TRUE)\ndata.frame(word1 = df$Word[idx[,1]], word2 = df$Word[idx[,2]], stringsAsFactors = FALSE)\n\n    word1   word2\n1 accepts accents\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance of a large string efficiently\r\n                \r\nIs there any other algorithm apart from the usual dynamic programming approach which takes O(n*m) time which takes less time than this? String input sizes can be atmost (100000) each. The usual algorithm do not work...Are more efficient algorithm available?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Analyze text (lemmatization, edit distance)\r\n                \r\nI need to analyze the text to exist in it banned words. Suppose the black list is the word: \"Forbid\". The word has many forms. In the text the word can be, for example: \"forbidding\", \"forbidden\", \"forbad\". To bring the word to the initial form, I use a process lemmatization. Your suggestions?\n\nWhat about typos?\nFor example: \"F0rb1d\".  I  think use damerau–Levenshtein or another. You suggestions?\n\nAnd what if the text is written as follows:\n\"ForbiddenInformation.Privatecorrespondenceofthecompany.\"  OR\n\"F0rb1dden1nformation.Privatecorresp0ndenceofthec0mpany.\" (yes, without whitespace)\n\nHow to solve this problem?\nPreferably fast algorithm, because text are processed in real time.\nAnd maybe what some tips to improve performance (how to store, etc)?\n    ", "Answer": "\r\nthere're two possible solutions as far as I know algorithms.\n\nYou could try to use dynamic programming , LCS (longest common subsequence). It will search original text for the desired word as pattern, I believe it's O(mn):\n\nhttp://en.wikipedia.org/wiki/Longest_common_subsequence_problem\nhttp://www.ics.uci.edu/~eppstein/161/960229.html\n\nAlthough the easier would be to use text search algorithm. The best I know is KMP and it's O(n). For character comparison you could group them into sets like {i I l(L) 1}, {o O 0} and so on. Yet you could modify this for not matching all letters (forbid -> forbad).\n\nhttp://en.wikipedia.org/wiki/Knuth-Morris-Pratt_algorithm\n\nSo now you could compare benefits of these two and yours suggestion.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance graph in NetworkX\r\n                \r\nI have 2 graphs created with networkx G_1 has 23 edges and 15 nodes, G_2 has 22 edges and 13 nodes. When I run the function nx.graph_edit_distance(G_1, G_2) it takes 20min to run. However when I run it on my graphs G4_ and G_5 that have 5 nodes and 6 edges eaches it only takes 3 seconds to run. Is there any way to speed up the process?\nThanks\nI didn't try much as it's a simple function and I'm new to programming and networkX. Thank you for any help\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Applying some edit distance function in pyspark\r\n                \r\nI have a question for you.. Which is the best way to make the comparisons between a recognized word and a set of possible words saved in my dataset, I tried to find the word that is similar through a score based on a weighted average between: Levenshtein, Edit and Jako distances. But, I have a lot of data to make the comparisons around 75 milions of comparison data.. What you can suggest to me??\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Difference between Jaro-Winkler and Levenshtein distance? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI want to do fuzzy matching of millions of records from multiple files. I identified two algorithms for that: Jaro-Winkler and Levenshtein edit distance.\nI was not able to understand what the difference is between the two. It seems Levenshtein gives the number of edits between two strings, and Jaro-Winkler provides a normalized score between 0.0 to 1.0.\nMy questions:\n\nWhat are the fundamental differences between the two algorithms?\n\nWhat is the performance difference between the two algorithms?\n\n\n    ", "Answer": "\r\nLevenshtein counts the number of edits (insertions, deletions, or substitutions) needed to convert one string to the other. Damerau-Levenshtein is a modified version that also considers transpositions as single edits. Although the output is the integer number of edits, this can be normalized to give a similarity value by the formula \n\n```\n1 - (edit distance / length of the larger of the two strings)\n```\n\n\nThe Jaro algorithm is a measure of characters in common, being no more than half the length of the longer string in distance, with consideration for transpositions. Winkler modified this algorithm to support the idea that differences near the start of the string are more significant than differences near the end of the string. Jaro and Jaro-Winkler are suited for comparing smaller strings like words and names.\n\nDeciding which to use is not just a matter of performance. It's important to pick a method that is suited to the nature of the strings you are comparing. In general though, both of the algorithms you mentioned can be expensive, because each string must be compared to every other string, and with millions of strings in your data set, that is a tremendous number of comparisons. That is much more expensive than something like computing a phonetic encoding for each string, and then simply grouping strings sharing identical encodings.\n\nThere is a wealth of detailed information on these algorithms and other fuzzy string matching algorithms on the internet. This one will give you a start:\n\nA Comparison of Personal Name\nMatching: Techniques and Practical\nIssues\n\nAccording to that paper, the speed of the four Jaro and Levenshtein algorithms I've mentioned are from fastest to slowest:\n\n\nJaro \nJaro-Winkler \nLevenshtein \nDamerau-Levenshtein\n\n\nwith the slowest taking 2 to 3 times as long as the fastest. Of course these times are dependent on the lengths of the strings and the implementations, and there are ways to optimize these algorithms that may not have been used.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "looking for python library which can perform levenshtein/other edit distance at word-level\r\n                \r\nI've seen a bunch of similar questions on SO/elsewhere but none of the answers quite satisfy my needs, so I don't think this is a dup.\n\nAlso, I totally know how to implement this myself, but I'm trying not to have to re-invent the wheel.\n\nDoes anyone know any python packages which can perform levenshtein/other edit-distance comparing 2 lists of words (I've found a few), but also allow one to specify your own costs for insertion, deletion, substitution, and transpositions?\n\nbasically, I want the distances computed to be the number of edits on words in the sentences, not on the number of characters the sentences differ by.\n\nI'm trying to replace a custom python extension module which is actually written in C, using python2's C api.  I could re-write in either pure-python or cython, but I'd rather simply add a dependency to the project.  The only problem is that this code allows one to specify your own costs for the various options, and I haven't found a package which allows this so far.\n    ", "Answer": "\r\nNLTK has the function named edit_distance. It calculates the Levenshtein distance between two strings. But it works good with lists of strings too:\n\n```\nimport nltk\n\ns1 = 'WAKA WAKA QB WTF BBBQ WAKA LOREM IPSUM WAKA'.split()\ns2 = 'WAKA OMFG QB WTF WAKA WAKA LOREM IPSUM WAKA'.split()\nprint(s1)\nprint(s2)\nprint(nltk.edit_distance(s1, s2))\n```\n\n\n```\n['WAKA', 'WAKA', 'QB', 'WTF', 'BBBQ', 'WAKA', 'LOREM', 'IPSUM', 'WAKA']\n['WAKA', 'OMFG', 'QB', 'WTF', 'WAKA', 'WAKA', 'LOREM', 'IPSUM', 'WAKA']\n\n2\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Google Style Search Suggestions with Levenshtein Edit Distance\r\n                \r\nOk guys working on search suggestions using jQuery-UI AutoComplete with results from sql-sever 2008 db. Using AdventureWorks DB Products table for testing. I want to search across 2 fields in this example. ProductNumber and Name.\n\nI asked 2 questions earlier relating to this...here and here\n\nand ive come up with this so far...\n\n```\nCREATE procedure [dbo].[procProductAutoComplete]\n(\n    @searchString nvarchar(100)\n)\nas\nbegin\n\n    declare @param nvarchar(100);\n    set @param = LOWER(@searchString);\n\nWITH Results(result)\nAS\n(\n    select TOP 10 Name as 'result'\n    from Production.Product \n    where LOWER(Name) like '%' + @param + '%' or (0 <= dbo.lvn(@param, LOWER   (Name), 6))\n    union\n    select TOP 10 ProductNumber as 'result'\n    from Production.Product\n    where LOWER(ProductNumber) like '%' + @param + '%' or (0 <= dbo.lvn(@param,  LOWER(ProductNumber), 6))\n)\n\nSELECT TOP 20 * from Results\n\nend;\n```\n\n\nMy problem now is ordering of the results...I am getting the correct results but they are just ordered by the Name or product number and are not relevant to the input string...\n\nfor example I can search for product Number starting with \"BZ-\" and the top returned results are ProductNums starting with \"A\" although I do get more relevant results elsewhere in the list..\n\nany ideas for sorting the results in terms of relevance to the search string??\n\nEDIT:\n\nin regards to the tql implementation of the levenschtein distance found here(linked to in previous question)...\n\nI am wondering what would be the best way to determine the MAX value to send to the function (6 in my example above)\n\nWould it be best to choose an arbitrary value based on what \"seems\" to work well for my given data set? or would it be best to adjust it dynamically based on the length of the input string...\n\nMy initial thoughs were that the value to should be inverely proportional to the length of the searchString...so as the search string grows and becomes more specific..the tolerance decreases...thoughts??\n    ", "Answer": "\r\nThe Full Text Search feature seems to be the way go when using SQL Server \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Computing edit distance of DNA sequence python\r\n                \r\nSo I am given the task of aligning the lowest cost between 2 DNA sequences.  One of the failing inputs is:\n\n```\nCGCAATTCTGAAGCGCTGGGGAAGACGGGT & TATCCCATCGAACGCCTATTCTAGGAT \n```\n\n\nThe proper alignment costs 24, but I am getting a cost of 23.\n\nI have to read the cost of switching bases say an A -> T, G -> C etc, etc.... The cost file I have been given is:\n\n```\n*,-,A,T,G,C\n-,0,1,2,1,3\nA,1,0,1,5,1\nT,2,1,0,9,1\nG,1,5,9,0,1\nC,3,1,1,1,0\n```\n\n\nI have made a python dictionary that links all bases perfectly that looks like:\n\n```\n{'AT': '1', '-C': '3', 'TG': '9', '-G': '1', 'AC': '1', 'C-': '3', 'CA': '1', 'AA': '0', '--': '0', 'TA': '1', 'T-': '2', 'CG': '1', '-T': '2', 'CC': '0', 'GG': '0', 'A-': '1', 'CT': '1', 'AG': '5', 'GC': '1', 'GT': '9', 'GA': '5', 'G-': '1', '-A': '1', 'TC': '1', 'TT': '0'}\n```\n\n\nCurrently my implementation works for some cases, and in other cases I am off by +/- 1. \n\nHere is a snippet of my code:\n\n```\ndef align(one_Line, costBook):\n    Split_Line = one_Line.split(\",\")\n    array = [[0] * len(Split_Line[1]) for i in Split_Line[0]]  # Zero fill array,\n\n    xLine = Split_Line[0]\n    yLine = Split_Line[1]\n\n    for i in range(1, len(xLine)):\n        array[i][0] = array[i - 1][0] + int(costBook['-' + xLine[i - 1]])\n    for i in range(1, len(yLine)):\n        array[0][i] = array[0][i - 1] + int(costBook[yLine[i - 1] + '-'])\n\n    for i in range(1, len(xLine)):\n        for j in range(1, len(yLine)):\n            array[i][j] = min(array[i - 1][j] + diff('-', xLine[i], costBook), \n                              array[i][j - 1] + diff(yLine[j], '-', costBook), \n                              array[i - 1][j - 1] + diff(yLine[j], xLine[i], costBook))\n```\n\n\nWhere the diff function is:\n\n```\ndef diff(x, y, cost):\n    if x == y:\n        return 0\n    keyStr = x + y\n    return int(costBook[keyStr])\n```\n\n\nWhere am I going wrong? Is it in the actual array filling itself, or am I doing the base cases wrong?\n\nEDIT:\nHere is a semi-working version, atleast the edit cost is right:\n\n```\nAGTTGTGAAAGAACAAGCGCACAATATTGCCGCGCCGAAAGCT,TTCTTTCATTATTCAA‌​ATGTATAGTTTAGAGCGTTA‌​A\n```\n\n    ", "Answer": "\r\nI believe the reason that your algorithm is failing is that you are failing to account for the \"gap\" row in your array (scoreing matrix). \n\nConsider two sequences ```\nA```\n and ```\nB```\n, each having length  ```\nn```\n. If we look at the wikipedia articles for the Needleman-Wunsch and Smith-Waterman algorithms, we see that in their respective \"scoring\" matrices there is an extra row at the beginning. This is to represent either the first character of ```\nA```\n or ```\nB```\n being paired with a gap. I recommend you quickly review those pages to see what I mean\n\nWhen you define your array as:\n\n```\narray = [[0] * len(Split_Line[1]) for i in Split_Line[0]]\n```\n\n\nYou are not including this additional row.\n\nYou would need modify the ```\nalign```\n function to add this extra row, and alter logic for calculating the score. i.e:\n\n```\ndef align(one_Line, costBook):\n    Split_Line = one_Line.split(\",\")\n\n    # Defining an array with an extra row and col to represent a leading gap\n    array = [[0] * (len(Split_Line[1])+1) for i in range(len(Split_Line[0])+1)]  # Zero fill array,\n\n    xLine = Split_Line[0]\n    yLine = Split_Line[1]\n\n    # Changed so we include our extra line in the loop\n    for i in range(1, len(xLine)+1):\n        array[i][0] = array[i - 1][0] + int(costBook['-' + xLine[i - 1]])\n    for i in range(1, len(yLine)+1):\n        array[0][i] = array[0][i - 1] + int(costBook[yLine[i - 1] + '-'])\n\n    # Changed so we include our extra row/col in the loop\n    for i in range(1, len(xLine)+1):\n        for j in range(1, len(yLine)+1):\n            # The references to the original string now need -1 (i.e. i-1)\n            array[i][j] = min(array[i - 1][j] + diff('-', xLine[i-1], costBook), \n                              array[i][j - 1] + diff(yLine[j-1], '-', costBook), \n                              array[i - 1][j - 1] + diff(yLine[j-1], xLine[i-1], costBook))\n    return array\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Algorithm to transform one word to another through valid words\r\n                \r\nI came across this variation of edit-distance problem:\n\nDesign an algorithm which transforms a source word to a target word. for example: from head to tail, in each step, you just can replace one character, and the word must be valid. You'll be given a dictionary.\n\nIt clearly is a variation of the edit distance problem, but in edit distance I do not care about if the word is valid or not. So how do I add this requirement to edit distance.\n    ", "Answer": "\r\nThis can be modelled as a graph problem. You can think of the words as nodes of the graph and two nodes are connected if and only if they are of same length and differ in one char.\n\nYou can preprocess the dictionary and create this graph, should look like:\n\n```\n   stack  jack\n    |      |\n    |      |\n   smack  back -- pack -- pick\n```\n\n\nYou can then have a mapping from the word to the node representing the word, for this you can use a hash table, height balanced BST ...\n\nOnce you have the above mapping in place, all you have to do is see if there exists a path between the two graph nodes, which can easily be done using BFS or DFS.\n\nSo you can summarize the algorithm as:\n\n```\npreprocess the dictionary and create the graph.\nGiven the two inputs words w1 and w2\nif length(w1) != length(w2)\n Not possible to convert\nelse\n n1 = get_node(w1)\n n2 = get_node(w2)\n\n if(path_exists(n1,n2))\n   Possible and nodes in the path represent intermediary words\n else\n   Not possible\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Minimum edit distance of zig zag string\r\n                \r\nI have string like this xxoxxooo and I wanna edit it to this form xoxoxoxo, my question is how to find minimum number of swaps and I can only swap 2 neighbours as swap. I thought about going through the string and finding the closest redundant x and move it to current place but thats too slow I think, beacuse string can have 1e6 * 2 chars. Any ideas?\n    ", "Answer": "\r\nLets denote ```\ns_i```\n the swap between position ```\ni```\n and ```\ni+1```\n\n\nSuppose that you have a minimal swap sequences ```\nS = s_{i1} s_{i2} ...```\n going from ```\nA```\n to ```\nB```\n. Because it is minimal you only swap ```\nx```\n with ```\no```\n and never an ```\nx```\n with an ```\nx```\n or an ```\no```\n with an ```\no```\n. Therefore the action of ```\nS```\n is to send the first ```\no```\n of ```\nA```\n to the first ```\no```\n of ```\nB```\n, the second ```\no```\n of ```\nA```\n to the second ```\no```\n of ```\nB```\n and so on. Therefore, the number of swap can't be smaller than\n\n```\nSum_i abs(pos of i-st o in A - pos of i-st o in B)\n```\n\n\nNow it's easy to find a sequence with exactly this number of swaps this is therefore the correct value.\n\nHere is an algorithm to compute it\n\n```\nInput: s1 and s2 of common length n\nI'm assuming that they contains the same number of 'x' and 'o'\n\nres = 0;\ni1 = 0; i2 = 0;\nwhile true do\n    // find the next o\n    while i1 < n and s1[i1] == 'x' do\n        i1++\n    if i1 == n return res\n    // no check that i2 < n because of assumption\n    while s2[i2] == 'x' do \n        i2++\n    res += abs(i1-i2)\n    i1++; i2++\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Comparing similar strings, using levenstein edit distance, ran into problems\r\n                \r\nI am using the levenstein edit distance to find how similar two strings are. The two strings are as such. The first one is the longer of the two if at all; also it is the non-truncated non-modified string I wish to compare the other too. The second string could be truncated at the end, and missing characters. There can be multiple unique string one and string twos. \n\nI read in the list of second strings and each is contained on a line with this format\n\"[string two] - $0.00\" So it is string two plus a space, a dash, a space, and then a price.\n\nSo I have a list of second strings (in the format) and I have two options. Remove the price and the \" - \" or keep it there. \n\n\nIf I remove it. I read in each string two and tokenize it with the delimiter \"$\". I do not know how long any string two is so I must do a stringtwo.removeAll(\"-\") to get rid of the dash and then a .trim() for the white space. Well if there is a dash in string two it will also be removed un-voluntairly. So with this I get either exact strings (levenstein = 0), truncated but still exact strings (strings are the same up to length string one - levenstein), truncated and missing a integer amount of dashes (strings the same in a few places between dashes, and if truncated also missing at the end), or not truncated but missing an integer number of dashes. \nIf I leave it. Still read in each string two and tokenize with delimiter \"$\". So now I have this format for string two \"[string two] - \". So all levenstein distance will be off by 3. The problem here is if I have a string one Ex. \"dog food is yummy\" and the string two I try to compare is \"dog food is yum - \" the levD = 3 but this is the same levD as if I have the string two \"dog food is yummy - \". \n\n\nAs you can see both options yield problems. It seems I cannot overcome these problems in my program to try and match the input list of string twos to my list of string ones. \n\nCan anyone see a better way of doing this, are there any other string comparators that I could use to make this less problematic?\n    ", "Answer": "\r\nTry this: should truncate the String at the last \"-\" found in each string while keeping the rest of the string intact.\n\n```\nStringTwo.substring(0, StringTwo.lastIndexOf(\"-\")).trim();\n```\n\n\nThese String manipulations can be expensive so if you are working with a lot of string you might look into other optimizations.\n\nAlso this solutions is brittle because it hardcodes the value to determine where to trim into the code. This can be defined elsewhere and passed in so it can vary.\n\nOnce you have that working relatively well and safe, next try and look into StringUtils from Apache which has more extensive String manipulations. \n\n```\norg.apache.commons.lang.StringUtils from Apache Commons Lang\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Using Python to extract the specific edit when Damerau-Levenshtein distance equals 1\r\n                \r\nI have a large Pandas dataframe containing data entered at a keyboard. One of the columns in the dataframe represents UK postcode data. Inevitably, with large datasets, there are a number of typing errors. I'm using the pyxDamerauLevenshtein library to calculate the edit distance between unrecognised postcodes and an array containing all possible postcodes and then presenting postcodes that are only a single edit away from entered data (DL distance = 1) to the user as possible alternatives. This works pretty well and I'm reasonably happy with the speed. However, a single edit in postcode terms means there may be 50-60 alternatives. I'd like to be able to order the alternatives based on the type of edit identified. So, for an example, G substituting for F (adjacent on a QWERTY keyboard) will probably be more likely than L substituting for F. Also, an insertion of the same letter twice would be more likely than the insertion of an adjacent letter which, in turn, would be more likely than the insertion of a completely different letter from the opposite end of the keyboard. The order that alternative postcodes are presented should reflect these probabilities.\n\nAn answer by marmeladze at Edit distance such as Levenshtein taking into account proximity on keyboard suggested using the Euclidean distance between keyboard keys; this seems like a reasonable idea. However, my question is, how can I efficiently extract the specific edit involved between 2 strings when the Damerau-Levenshtein distance equals one?\n\nAs an example, if I have a postcode ZE2 9YM (which does not exist), the code should identify all other postcodes that are just one edit away but should also indicate the nature of the edit, maybe something like:\n\n```\nEntered code    Possible alternative    DL dist       Edit type    Edit\n     ZE2 9YM                 ZE2 9YA          1    Substitution     A-M\n     ZE2 9YM                 ZE2 9YN          1    Substitution     N-M\n         ...\n```\n\n\nAnd, in the above case, it would be more likely that the M was substituted for the N (adjacent keys) rather than the M being substituted for the A.\n\nIs anyone aware of a Python library that will calculate the Damerau-Levenshtein distance AND will output the matrix (together with summary of edits)?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Branch and Bound approach to the edit distance algorithm\r\n                \r\nI am trying to implement Branch and Bound  approach to the ```\nedit distance```\n algorithm. I can't find any hints over internet to start with. Can anyone help me to get into the track of the algorithm.\n    ", "Answer": "\r\nMy apologies for posting this as an answer, I wanted to add it as just a comment, but I don't have sufficient reputation to add comments.\n\nTry to approach your literature search of this subject rather in the context of the graph similarity problem than for the edit distance algorithm; the prior is a more general and well-studied problem which the problem of finding the minimum edit distance fall under. The strings in any instance of your edit distance problem can be described as simple directional graphs, and the operations of insertion/deletion and substitution in the edit distance algorithm has similar operations in the graph similarity problem (e.g., insertion of vertices and edges/deletion of vertices and edges/changing labels on edges and so on).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Getting bottlenecked using graph edit distance for networkx graphs. Any good alternatives?\r\n                \r\nI am currently getting bottlenecked using graph edit distance for networkx graphs. My code currently looks like this (I am appending 50 pairwise graph analysis results to a list of dictionaries).\n```\nlst_of_dct = []\nfor i in range(len(pair_list)):\n    dict = {}\n    # extract edges values for graphs 1 & 2\n    edge_1 = [list(e) for e in pair_list[i][0].edges]\n    edge_2 = [list(e) for e in pair_list[i][1].edges]\n    dict[\"my_graph1\"] = edge_1\n    dict[\"my_graph2\"] = edge_2\n    node1 = pair_list[i][0].nodes\n    node2 = pair_list[i][1].nodes\n    dict[\"labels_1\"] = node1\n    dict[\"labels_2\"] = node2\n    for v in nx.optimize_graph_edit_distance(pair_list[i][0], pair_list[i][1]):\n        ged = v\n    #ged = nx.graph_edit_distance(pair_list[i][0], pair_list[i][1])\n    dict[\"ged\"] = ged\n    lst_of_dct.append(dict)\n    print('1')\n```\n\nAfter about 3 iterations of my for loop, it just gets stuck calculating the fourth GED. I calculated the GED individually on a scratch sheet to see how long it takes without the for loop, and the graph pair for the fourth iteration only takes ~3 seconds. I was just wondering why am I getting bottlenecked at the fourth iteration if it only takes ~3 seconds to calculate without a for loop?\nI guess this really isn't a what have I tried problem. Just a matter of why is this happening? Things I could try but would be kind of annoying would be to iterate 5 for loops at a time... which would result in 20 different for loops appending the same dictionary again and again. I think this is not a great idea, but this could be a last result if it comes down to it. I could just hard-code it in, but I would rather not have to do this because I am expected to do this same procedure for a much larger dataset in the future.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Real world typo statistics? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 11 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nWhere can I find some real world typo statistics?  \n\nI'm trying to match people's input text to internal objects, and people tend to make spelling mistakes.\nThere are 2 kinds of mistakes:  \n\n\n```\ntypos```\n - \"Helllo\" instead of \"Hello\" / \"Satudray\" instead of \"Saturday\" etc.  \n```\nSpelling```\n - \"Shikago\" instead of \"Chicago\" \n\n\nI use  Damerau-Levenshtein distance for the typos and Double Metaphone for spelling (Python implementations here and here).\n\nI want to focus on the Damerau-Levenshtein (or simply ```\nedit-distance```\n). The textbook implementations always use '1' for the weight of deletions, insertions substitutions and transpositions. While this is simple and allows for nice algorithms it doesn't match \"reality\" / \"real-world probabilities\".  \n\nExamples:   \n\n\nI'm sure the likelihood of \"Helllo\" (\"Hello\") is greater than \"Helzlo\", yet they are both 1 edit distance away.\n\"Gello\" is closer than \"Qello\" to \"Hello\" on a QWERTY keyboard.\nUnicode transliterations: What is the \"real\" distance between \"München\" and \"Munchen\"?\n\n\nWhat should the \"real world\" weights be for deletions, insertions, substitutions, and transpositions?  \n\nEven Norvig's very cool spell corrector uses non-weighted edit distance.\n\nBTW- I'm sure the weights need to be functions and not simple floats (per the above \nexamples)...\n\nI can adjust the algorithm, but where can I \"learn\" these weights? I don't have access to Google-scale data...  \n\nShould I just guess them?\n\nEDIT - trying to answer user questions:\n\n\nMy current non-weighted algorithm fails often when faced with typos for the above reasons. \"Return on Tursday\": every \"real person\" can easily tell Thursday is more likely than Tuesday, yet they are both 1-edit-distance away! (Yes, I do log and measure my performance).\nI'm developing an NLP Travel Search engine, so my dictionary contains ~25K destinations (expected to grow to 100K), Time Expressions ~200 (expected 1K), People expressions ~100 (expected 300), Money Expressions ~100 (expected 500), \"glue logic words\" (\"from\", \"beautiful\", \"apartment\") ~2K (expected 10K) and so on...\nUsage of the edit distance is different for each of the above word-groups. I try to \"auto-correct when obvious\", e.g. 1 edit distance away from only 1 other word in the dictionary. I have many other hand-tuned rules, e.g. Double Metaphone fix which is not more than 2 edit distance away from a dictionary word with a length > 4... The list of rules continues to grow as I learn from real world input.\n\"How many pairs of dictionary entries are within your threshold?\": well, that depends on the \"fancy weighting system\" and on real world (future) input, doesn't it? Anyway, I have extensive unit tests so that every change I make to the system only makes it better (based on past inputs, of course). Most sub-6 letter words are within 1 edit distance from a word that is 1 edit distance away from another dictionary entry.\nToday when there are 2 dictionary entries at the same distance from the input I try to apply various statistics to better guess which the user meant (e.g. Paris, France is more likely to show up in my search than Pārīz, Iran).\nThe cost of choosing a wrong word is returning semi-random (often ridiculous) results  to the end-user and potentially losing a customer. The cost of not understanding is slightly less expensive: the user will be asked to rephrase.\nIs the cost of complexity worth it? Yes, I'm sure it is. You would not believe the amount of typos people throw at the system and expect it to understand, and I could sure use the boost in Precision and Recall.\n\n    ", "Answer": "\r\nPossible source for real world typo statistics would be in the Wikipedia's complete edit history. \n\nhttp://download.wikimedia.org/\n\nAlso, you might be interested in the AWB's RegExTypoFix\n\nhttp://en.wikipedia.org/wiki/Wikipedia:AWB/T\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Given a list of strings find each of its string's closest match (edit distance) in another big list of strings\r\n                \r\nI have a list of strings ```\nsmall_list = ['string1', 'this is string 2', ...]```\n and a larger list of strings ```\nbig_list = ['is string 2', 'some other string 3', 'string 1', ...]```\n. I want to find the string that is closest by edit distance for all of the strings in small_list in big_list.\n\nI found this which does the same with numbers.\n\nSolution 1 that I tried: \n\n```\nfrom difflib import get_close_matches\nimport datetime\n\na = datetime.datetime.now()\nprint(get_close_matches(str(small_list.iloc[0]), big_list.values.astype(str), n=3, cutoff=0.7))\nb = datetime.datetime.now()\nc = b - a\nprint(c.seconds)\n```\n\n\nBut for my dataset and for that one record, it took me ```\n834 seconds```\n. ```\nlen(big_list) = 27989793```\n and ```\nlen(small_list) = 9329931```\n so performance is of at most importance.\n\nSolution 2 that I tried:\n\n```\ns = str(small_list.iloc[0])\na = datetime.datetime.now()\nfor i in big_list:\n    m = editdistance.eval(i[0], s)\n    if m < min:\n        min = m\n        i_s = i\nb = datetime.datetime.now()\nc = b - a\nprint(c.seconds)\n```\n\n\nFor this I used the editdistance package that is implemented efficiently in C++ and I got a time of 48 seconds.\n\nTo improve on the above solutions I require that I not exhaustively go over all the values in big_list. I'm looking for ways to do the same.\n\nOne of the approaches that I came up with was creating a trie (or some kind of a suffix-tree) with the concatenated big_list of strings and query that trie to find matches. Due to lack of experience in this, was hoping for some package suggestions or some code that I could start off of. Another approach would be to modify the KNN algorithm that uses edit distance as a metric. Any sklearn or other packages that do this?\n\nExpected output: ```\n[3, 1, ...]```\n which are positions of the closest match in big_list.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Javascript fuzzy search that makes sense [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 9 months ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI'm looking for a fuzzy search JavaScript library to filter an array. I've tried using fuzzyset.js and fuse.js, but the results are terrible (there are demos you can try on the linked pages).\nAfter doing some reading on Levenshtein distance, it strikes me as a poor approximation of what users are looking for when they type. For those who don't know, the system calculates how many insertions, deletions, and substitutions are needed to make two strings match.\nOne obvious flaw, which is fixed in the Levenshtein-Demerau model, is that both blub and boob are considered equally similar to bulb (each requiring two substitutions). It is clear, however, that bulb is more similar to blub than boob is, and the model I just mentioned recognizes that by allowing for transpositions.\nI want to use this in the context of text completion, so if I have an array ```\n['international', 'splint', 'tinder']```\n, and my query is int, I think international ought to rank more highly than splint, even though the former has a score (higher=worse) of 10 versus the latter's 3.\nSo what I'm looking for (and will create if it doesn't exist), is a library that does the following:\n\nWeights the different text manipulations\nWeights each manipulation differently depending on where they appear in a word (early manipulations being more costly than late manipulations)\nReturns a list of results sorted by relevance\n\nHas anyone come across anything like this? I realize that StackOverflow isn't the place to be asking for software recommendations, but implicit (not anymore!) in the above is: am I thinking about this the right way?\n\nEdit\nI found a good paper (pdf) on the subject. Some notes and excerpts:\n\nAffine edit-distance functions assign a relatively lower cost to a sequence of insertions or deletions\n\n\nthe Monger-Elkan distance function (Monge & Elkan 1996), which is an affine variant of the Smith-Waterman distance function (Durban et al. 1998) with particular cost parameters\n\nFor the Smith-Waterman distance (wikipedia), \"Instead of looking at the total sequence, the Smith–Waterman algorithm compares segments of all possible lengths and optimizes the similarity measure.\" It's the n-gram approach.\n\nA broadly similar metric, which is not based on an edit-distance model, is the\nJaro metric (Jaro 1995; 1989; Winkler\n1999). In the record-linkage literature, good results have been obtained using variants of this method, which is based on the number and order of the common characters between two strings.\n\n\nA variant of this due to Winkler (1999) also uses the length P of the longest common prefix\n\n\n(seem to be intended primarily for short strings)\n\nFor text completion purposes, the Monger-Elkan and Jaro-Winkler approaches seem to make the most sense. Winkler's addition to the Jaro metric effectively weights the beginnings of words more heavily. And the affine aspect of Monger-Elkan means that the necessity to complete a word (which is simply a sequence of additions) won't disfavor it too heavily.\nConclusion:\n\nthe TFIDF\nranking performed best among several token-based distance\nmetrics, and a tuned affine-gap edit-distance metric proposed by Monge and Elkan performed best among several\nstring edit-distance metrics. A surprisingly good distance\nmetric is a fast heuristic scheme, proposed by Jaro and later extended by Winkler.\nThis works almost as well as the Monge-Elkan scheme, but\nis an order of magnitude faster.\nOne simple way of combining the TFIDF method and the\nJaro-Winkler is to replace the exact token matches used in\nTFIDF with approximate token matches based on the Jaro-\nWinkler scheme. This combination performs slightly better than either Jaro-Winkler or TFIDF on average, and occasionally performs much better. It is also close in performance to a learned combination of several of the best metrics\nconsidered in this paper.\n\n    ", "Answer": "\r\nI tried using existing fuzzy libraries like fuse.js and also found them to be terrible, so I wrote one which behaves basically like sublime's search. https://github.com/farzher/fuzzysort\n\nThe only typo it allows is a transpose. It's pretty solid (1k stars, 0 issues), very fast, and handles your case easily:\n\n```\nfuzzysort.go('int', ['international', 'splint', 'tinder'])\n// [{highlighted: '*int*ernational', score: 10}, {highlighted: 'spl*int*', socre: 3003}]\n```\n\n\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Custom Edit distance weights for operations in Lucene FuzzySearch\r\n                \r\nI came across this python library https://pypi.org/project/weighted-levenshtein/ which allows to specify different costs/weights for different operations(insertion, substitution, deletion and transposition) which is very helpful in detecting and correcting keystroke errors.\nI have been searching through lucene library FuzzySearch which uses Damerau-Levenstein distance to check if something like this is supported to specify different costs/weights for different operations but not able to find any.\nPlease let me know if there exists a way to specify our custom costs/weights within Lucene Fuzzy-Search.\nThanks in advance!\n    ", "Answer": "\r\nTo accomplish this you would have to extend and/or edit lucene code.  To support fuzzy matching, lucene compiles an ```\nAutomaton```\n using the ```\nLevenshteinAutomata```\n class, which implements this algorithm, and not only doesn't support edit weights, but only supports matching for up to 0 to 2 edits.\nHow one might edit this algorithm to produce an automaton that supports weighted edits is beyond my knowledge, but could be worth a try as it would make your customization simple (would only have to override the ```\ngetAutomaton```\n method) and would (theoretically) keep performance consistent.\nThe alternative would be to forgo the idea of an automaton to support fuzzy matching and simply implement a weighted levenshtein algorithm, like the one you have linked to, directly in the actual fuzzy match check.  By doing this, however, you could pay a rather high performance cost depending on the nature of the fuzzy queries you handle and the content of your index.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Implementing Levenshtein distance in python\r\n                \r\nI have implemented the algorithm, but now I want to find the edit distance for the string which has the shortest edit distance to the others strings. \n\nHere is the algorithm: \n\n```\ndef lev(s1, s2):\n    return min(lev(a[1:], b[1:])+(a[0] != b[0]), lev(a[1:], b)+1, lev(a, b[1:])+1)\n```\n\n    ", "Answer": "\r\nYour \"implementation\" has several flaws:\n\n(1) It should start with ```\ndef lev(a, b):```\n, not ```\ndef lev(s1, s2):```\n. Please get into the good habits of (a) running your code before asking questions about it (b) quoting the code that you've actually run (by copy/paste, not by (error-prone) re-typing).\n\n(2) It has no termination conditions; for any arguments it will eventually end up trying to evaluate ```\nlev(\"\", \"\")```\n which would loop forever were it not for Python implementation limits: ```\nRuntimeError: maximum recursion depth exceeded```\n.\n\nYou need to insert two lines:\n\n```\nif not a: return len(b)\nif not b: return len(a)\n```\n\n\nto make it work.\n\n(3) The Levenshtein distance is defined recursively. There is no such thing as \"the\" (one and only) algorithm. Recursive code is rarely seen outside a classroom and then only in a \"strawman\" capacity.\n\n(4) Naive implementations take time and memory proportional to ```\nlen(a) * len(b)```\n ... aren't those strings normally a little bit longer than 4 to 8?\n\n(5) Your extremely naive implementation is worse, because it copies slices of its inputs.\n\nYou can find working not-very-naive implementations on the web ... google(\"levenshtein python\") ... look for ones which use ```\nO(max(len(a), len(b)))```\n additional memory.\n\nWhat you asked for (\"the edit distance for the string who has the shortest edit distance to the others strings.\") Doesn't make sense ... \"THE string\"??? \"It takes two to tango\" :-)\n\nWhat you probably want (finding all pairs of strings in a collection which have the minimal distance), or maybe just that minimal distance, is a simple programming exercise. What have you tried? \n\nBy the way, finding those pairs by a simplistic algorithm will take O(N ** 2) executions of ```\nlev()```\n where N is the number of strings in the collection ... if this is a real-world application, you should look to use proven code rather than try to write it yourself. If this is homework, you should say so.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Shortest string with lowest edit distance from set of strings\r\n                \r\nSuppose I have a list of strings, which are similar. I wish to figure out the common part or characteristic of all these strings. Is there a known way to figure out a string which is most similar to all strings in a given set, and does not belong to the set?\n\nFor example, if I have the following set:\n\n```\nHello\nHell\nHelp\nHepl\n```\n\n\n'Hel' gives a levenshtein distance of 2,1,1,1. Currently I am thinking of taking different substrings as base, and computing the distance (my sets are fairly small, so brute forcing will not be an issue), but this solution does not find strings which are not essentially substrings of any given string in the set, but might be the most optimal solution (cases like where the solution is conjugation of two substrings).\n\nAny leads regarding this would be appreciated.\n    ", "Answer": "\r\nYou said brute force is acceptable :-). The classic approach then is breadth first search. For each string in your list you generate all strings with an edit distance of 1. from those you do all distance 2 strings and so on. for every given string you get a tree of mutated strings. After every round (distance) you check if there is a string common to every tree.\n\npseudocode for a levenshtein distance:\n\n```\nalphabet = \"abcd...\"\nstarters = \"Hello\", \"Hell\", \"Help\", \"Hepl\"\nrelatives = set()\ndistance = 0\nfor word in starters\n    trees[word][distance] = word\n\nwhile len(relatives) == 0\n    distance++\n    for tree in trees\n        for word in tree[distance-1]\n            for pos in range(len(word))\n                new_word = word.erase(pos)\n                if new_word not in tree\n                    tree[distance].insert(new_word)\n                    dict[new_word] += 1\n                    if dict[new_word] == len(starters)\n                        relatives.insert(new_word)\n            for pos in range(len(word))\n                for letter in alphabet\n                    new_word = word.replace(pos, letter)\n                    if new_word not in tree:\n                        tree[distance].insert(new_word)\n                        dict[new_word] += 1\n                        if dict[new_word] == len(starters)\n                            relatives.insert(new_word)\n            for pos in range(len(word) + 1):\n                for letter in alphabet\n                    new_word = word.insert(pos, letter)\n                    if new_word not in tree\n                        tree[distance].insert(new_word)\n                        dict[new_word] += 1\n                        if dict[new_word] == len(starters)\n                            relatives.insert(new_word)\nprint relatives\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to measure the graph edit distance by considering the node names\r\n                \r\nI have defined two Graphs in python using the library networkx and now I want to measure the distance between those two graphs considering the nodes' \"names\"\n\nI created two Graphs, looking totally the same (tree graphs)\n\n```\nG=nx.Graph()\nG.add_edges_from([(\"A\",\"B\"),(\"A\",\"C\")])\n\nH=nx.Graph()\nH.add_edges_from([(\"X\",\"Y\"),(\"X\",\"Z\")])\n\nres=nx.graph_edit_distance(G,H)\nres2=nx.optimize_edit_paths(G,H)\nres3=nx.optimal_edit_paths(G,H)\n```\n\n\nI want all nodes to be replaced, because the nodes don't have the same name, but I get the result for the cost (changes/ distance) 0. This means that the function is not considering the nodes' names. \n\nIn the documentation it is suggested to use the function \"node_math\" but I don't know how to use it. It does not seem to be a networkx function. \n    ", "Answer": "\r\nThe reason it is giving the answer as ```\n0```\n is because it is considering the nodes to be equal. If you see the documentation, it mentions that that\n\n\n  node_match (callable) – A function that returns True if node n1 in G1\n  and n2 in G2 should be considered equal during matching.\n  \n  The function will be called like\n\n```\nnode_match(G1.nodes[n1], G2.nodes[n2]).\n```\n\n  \n  That is, the function will receive the node attribute dictionaries for\n  n1 and n2 as inputs.\n\n\nNow, since you have not declared any attributes, then in that case\n\n```\nG.nodes['A'] will return {}\n```\n\n\nand\n\n```\nH.nodes['Z'] will return {}\n```\n\n\nNow since both are empty dictionaries, they will be considered equal.\n\nHere is modified version of your code, where I have added a labels attribute for your node. \n\n```\nimport networkx as nx\n\nG=nx.Graph()\nG.add_nodes_from([(\"A\", {'label':'a'}), (\"B\", {'label':'b'}),\n                  (\"C\", {'label':'c'})])\n\nG.add_edges_from([(\"A\",\"B\"),(\"A\",\"C\")])\n\nH=nx.Graph()\nH.add_nodes_from([(\"X\", {'label':'x'}), (\"Y\", {'label':'y'}),\n                  (\"Z\", {'label':'z'})])\nH.add_edges_from([(\"X\",\"Y\"),(\"X\",\"Z\")])\n\n# This is the function which checks for equality of labels\ndef return_eq(node1, node2):\n    return node1['label']==node2['label']\n\nprint(nx.graph_edit_distance(G, H, node_match=return_eq))\n# Output: 3\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Lucene 3.0.3 - How is fuzzy search similarity correlated to later versions edit distance? (e.g. 4.x)\r\n                \r\nPrior to versions 4.x, you set the similarity for a fuzzy search with a float between 0.1 to 1.0.\nLater versions use a value between 0 and 2 as edit distances.\n\nHow are these values correlated? I cannot find anywhere in the documentation what an actual float range from 0.1 to 1.0 means.\n\nI'm using lucene.net 3.0.3\n    ", "Answer": "\r\nVersion 4.0 onward just use a Damerau-Levenshtein edit distance.\n\nVersion 3.0.3 instead compares the edit distance to the length of the term.  If ```\nlength(term) * minSimilarity >= edit distance```\n (where minSimilarity is the float argument you are referring to), the term is considered a match.\n\nSo, if you set it to 0.5, a term of length 4 could have an edit distance of up to 2, while a term of length 6 could have a distance of 3 and still be a match.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Can every solution of the edit distance problem be shown on the matrix obtained from the dynamic programming algorithm?\r\n                \r\nI watched a video on Youtube explaining how to calculate which operations are used and how many times by going back to 1 from the last cell of the matrix obtained from the dynamic programming algorithm. Although I understood the example in the video, when I tried to do it with other examples, I couldn't. Is it possible to show every solution on the matrix?\nI also added the code that calculates the matrix at the end.\nLet's assume our words are \"apple\" and \"paper\".\nMatrix obtained from dynamic programming algorithm (I used Levensthein Distance operations.):\n\nMinimum edit distance of \"apple\" and \"paper\" is 4.\nOne of the solutions is 4 replace operations.\n1. apple -> ppple (replace \"a\" with \"p\")\n2. ppple -> paple (replace \"p\" with \"a\")\n3. paple -> papee (replace \"l\" with \"e\")\n4. papee -> paper (replace \"e\" with \"r\")\nSolution on matrix:\n\nIs it possible to show other solutions on this matrix?\nFor example:\n1. apple -> papple (insert \"p\")\n2. papple -> paple (remove \"p\")\n3. paple -> pape (remove \"l\")\n4. pape -> paper (insert \"r\")\ncode:\n```\ndef min_edit_count(word1, word2):\n\n    word1 = ' ' + word1     #\n    word2 = ' ' + word2     # add a space before original words\n\n    len_w1 = len(word1)     #\n    len_w2 = len(word2)     # calculate the lengths of new words\n\n    edit_matrix = np.zeros((len_w2, len_w1), dtype = int)\n    # create a matrix with all zeros\n\n    edit_matrix[0, :] = range(len_w1)  \n    # assign numbers from 0 to len_w1 in the first row of the edit_matrix \n    edit_matrix[:, 0] = range(len_w2)\n    # assign numbers from 0 to len_w2 in the first column of the edit_matrix \n\n    for i in range(1, len_w2):\n        for j in range(1, len_w1):\n            # edit_matrix[i-1][j] --> remove\n            # edit_matrix[i][j-1] --> insert\n            # edit_matrix[i-1][j-1] --> replace\n\n            temp1 = edit_matrix[i-1][j] + 1\n            temp2 = edit_matrix[i][j-1] + 1\n            # add 1 to edit_matrix[i-1][j] and edit_matrix[i][j-1]\n\n            temp3 = edit_matrix[i-1][j-1] + 1 if word1[j] != word2[i] else edit_matrix[i-1][j-1]\n            # if last characters are same don't add 1 to edit_matrix[i-1][j-1].\n            # no need to replace\n\n            edit_count = min(temp1, temp2, temp3)\n            # find min between three numbers\n            edit_matrix[i][j] = edit_count\n\n    min_edit = int(edit_matrix[len_w2 - 1, len_w1 - 1])\n    # minimum edit count is the last number calculated\n    \n    return min_edit, edit_matrix\n```\n\n    ", "Answer": "\r\nYes, you can backtrack from the end going over the cells that could contribute to the solution. \nThe complexity would be ```\nO((n+m) * num_solutions)```\n.\n```\ndef getSolutions(edit_matrix, word1, word2):\n  pos = [] \n  def backtrack(i,j):\n    pos.append((i,j))\n    if i==0 and j==0:\n      # This is a solution\n      print(pos)\n    if i>0 and edit_matrix[i-1,j] + 1 == edit_matrix[i,j]:\n      backtrack(i-1,j)\n    if j>0 and edit_matrix[i,j-1] + 1 == edit_matrix[i,j]:\n      backtrack(i, j-1)\n    if i>0 and j>0 and (edit_matrix[i-1][j-1] + 1 if word1[j] != word2[i] else edit_matrix[i-1][j-1]) == edit_matrix[i,j]:\n      backtrack(i,j)\n    pos.pop()\n  backtrack(len(word1) - 1,len(word2) - 1)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "group_by edit distance between rows over multiple columns\r\n                \r\nI have the following data frame.\nInput:\n```\nclass   id  q1  q2  q3  q4\nAli     12  1   2   3   3\nTom     16  1   2   4   2\nTom     18  1   2   3   4\nAli     24  2   2   4   3\nAli     35  2   2   4   3\nTom     36  1   2   4   2\n```\n\n\nclass indicates the teacher's name,\nid indicates the student user ID, and,\nq1, q2, q3 and q4 indicate marks on different test questions\n\nRequirement:\nI am interested in finding potential cases of cheating. I hypothesise that if the students are in the same class and have similar scores on different questions, they are likely to have cheated.\nFor this, I want to calculate absolute distance or difference, grouped by class name, across multiple columns, i.e., all the test questions q1, q2, q3 and q4. And I want to store this information in a couple of new columns as below:\n\ndifference:\nFor a given class name, it contains the pairwise distance or difference with all other students' id. For a given class name, it stores the information as (id1, id2 = difference)\ncheating:\nThis column lists any id's based on the previously created new column where the difference was zero (or some threshold value). This will be a flag to alert the teacher that their student might have cheated.\n\n```\nclass   id  q1  q2  q3  q4  difference                  cheating\nAli     12  1   2   3   3   (12,24 = 2), (12,35 = 2)    NA\nTom     16  1   2   4   2   (16,18 = 3), (16,36 = 0)    36\nTom     18  1   2   3   4   (16,18 = 3), (18,36 = 3)    NA\nAli     24  2   2   4   3   (12,24 = 2), (24,35 = 0)    35\nAli     35  2   2   4   3   (12,35 = 2), (24,35 = 0)    24\nTom     36  1   2   4   2   (16,36 = 0), (18,36 = 3)    16\n```\n\nIs it possible to achieve this using dplyr?\nRelated posts:\nI have tried to look for related solutions but none of them address the exact problem that I am facing e.g.,\n\nThis post calculates the difference between all pairs of rows. It does not incorporate the group_by situation plus the solution is extremely slow: R - Calculate the differences in the column values between rows/ observations (all combinations)\n\nThis one compares only two columns using stringdist(). I want my solution over multiple columns and with a group_by() condition: Creating new field that shows stringdist between two columns in R?\n\nThe following post compares the initial values in a column with their preceding values: R Calculating difference between values in a column\n\nThis one compares values in one column to all other columns. I would want this but done row wise and through group_by(): R Calculate the difference between values from one to all the other columns\n\n\ndput()\nFor your convenience, I am sharing data dput():\n```\nstructure(list(class = \nc(\"Ali\", \"Tom\", \"Tom\", \"Ali\", \"Ali\", \"Tom\"), \nid = c(12L, 16L, 18L, 24L, 35L, 36L), \nq1 = c(1L, 1L, 1L, 2L, 2L, 1L), \nq2 = c(2L, 2L, 2L, 2L, 2L, 2L), \nq3 = c(3L, 4L, 3L, 4L, 4L, 4L), \nq4 = c(3L, 2L, 4L, 3L, 3L, 2L)), row.names = c(NA, -6L), class = \"data.frame\")\n```\n\nAny help would be greatly appreciated!\n    ", "Answer": "\r\nYou could try to clustering the data, using ```\nhclust()```\n for example. Once the relative distances are calculated and mapped, the cut the tree at the threshold of expected cheating.\nThis example I am using the standard ```\ndist()```\n function to calculate differences, the stringdist function may be better or maybe another option is out there to try.\n```\ndf<- structure(list(class = \nc(\"Ali\", \"Tom\", \"Tom\", \"Ali\", \"Ali\", \"Tom\"), \nid = c(12L, 16L, 18L, 24L, 35L, 36L), \nq1 = c(1L, 1L, 1L, 2L, 2L, 1L), \nq2 = c(2L, 2L, 2L, 2L, 2L, 2L), \nq3 = c(3L, 4L, 3L, 4L, 4L, 4L), \nq4 = c(3L, 2L, 4L, 3L, 3L, 2L)), row.names = c(NA, -6L), class = \"data.frame\")\n\n#apply the standard distance function\nscores <- hclust(dist(df[ , 3:6]))\n\nplot(scores)\n\n#divide into groups based on level of matching too closely\ngroups <- cutree(scores, h=0.1)\n\n#summary table\nsummarytable <- data.frame(class= df$class, id =df$id, groupings =groups)\n\n#select groups with more than 2 people in them\nsuspectgroups <- table(groups)[table(groups) >=2]\n\npotential_cheaters <- summarytable %>% filter(groupings %in% names(suspectgroups)) %>% arrange(groupings) \npotential_cheaters\n```\n\nThis works for this test case, but for larger datasets the height in the ```\ncutree()```\n function may need to be adjusted. Also consider splitting the initial dataset by class to eliminate the chance of matching people between classes (depending on the situation of course).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Similarity scores based on string comparison in R (edit distance)\r\n                \r\nI am trying to assign similarity score based on comparison between 2 strings. Is there a function for the same in R. I am aware of such a function in SAS by the name of SPEDIS. Please let me know if there is such a function in R.\n    ", "Answer": "\r\nThe function adist computes the Levenshtein edit distance between two strings. This can be transformed into a similarity metric as 1 - (Levenshtein edit distance / longer string length).\n\nThe ```\nlevenshteinSim```\n function in the RecordLinkage package also does this directly, and might be faster than ```\nadist```\n. \n\n```\nlibrary(RecordLinkage)\n> levenshteinSim(\"apple\", \"apple\")\n[1] 1\n> levenshteinSim(\"apple\", \"aaple\")\n[1] 0.8\n> levenshteinSim(\"apple\", \"appled\")\n[1] 0.8333333\n> levenshteinSim(\"appl\", \"apple\")\n[1] 0.8\n```\n\n\nETA: Interestingly, while ```\nlevenshteinDist```\n in the RecordLinkage package appears to be slightly faster than ```\nadist```\n, ```\nlevenshteinSim```\n is considerably slower than either. Using the rbenchmark package:\n\n```\n> benchmark(levenshteinDist(\"applesauce\", \"aaplesauce\"), replications=100000)\n                                         test replications elapsed relative\n1 levenshteinDist(\"applesauce\", \"aaplesauce\")       100000   4.012        1\n  user.self sys.self user.child sys.child\n1     3.583    0.452          0         0\n> benchmark(adist(\"applesauce\", \"aaplesauce\"), replications=100000)\n                               test replications elapsed relative user.self\n1 adist(\"applesauce\", \"aaplesauce\")       100000   4.277        1     3.707\n  sys.self user.child sys.child\n1    0.461          0         0\n> benchmark(levenshteinSim(\"applesauce\", \"aaplesauce\"), replications=100000)\n                                        test replications elapsed relative\n1 levenshteinSim(\"applesauce\", \"aaplesauce\")       100000   7.206        1\n  user.self sys.self user.child sys.child\n1      6.49    0.743          0         0\n```\n\n\nThis overhead is due simply to the code for ```\nlevenshteinSim```\n, which is just a wrapper around ```\nlevenshteinDist```\n:\n\n```\n> levenshteinSim\nfunction (str1, str2) \n{\n    return(1 - (levenshteinDist(str1, str2)/pmax(nchar(str1), \n        nchar(str2))))\n}\n```\n\n\nFYI: if you are always comparing two strings rather than vectors, you can create a new version that uses ```\nmax```\n instead of ```\npmax```\n and shave ~25% off the running time:\n\n```\nmylevsim = function (str1, str2) \n{\n    return(1 - (levenshteinDist(str1, str2)/max(nchar(str1), \n        nchar(str2))))\n}\n> benchmark(mylevsim(\"applesauce\", \"aaplesauce\"), replications=100000)\n                                  test replications elapsed relative user.self\n1 mylevsim(\"applesauce\", \"aaplesauce\")       100000   5.608        1     4.987\n  sys.self user.child sys.child\n1    0.627          0         0\n```\n\n\nLong story short- there is little difference between ```\nadist```\n and ```\nlevenshteinDist```\n in terms of performance, though the former is preferable if you don't want to add package dependencies. How you turn it into a similarity measure does have a bit of an effect on performance.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is there an edit distance metric which doesn't rely on order at all?\r\n                \r\nFor example, let's say I have these two lists:\n\n```\nvar a = [1,2,3];\nvar b = [3,2,1];\n```\n\n\nThe Levenshtein distance between them would be 2. I'm looking for a metric where the distance would be 0, i.e. lists with the same elements are regarded as the same list, regardless of order. I've searched online, but I'm not really sure what terminology is used for this sort of thing.\n    ", "Answer": "\r\nSounds like you want a set distance like the Jaccard distance.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "diff text documents but ignore single character differences? Set a minimum edit distance filter?\r\n                \r\nI have two versions of a large book in txt format and I'd like to compare them to find significant changes between the versions, ignoring small single character differences.\n\nThere are lots of diffing tools that can ignore whitespace differences, but I also want to ignore small typos and single or couple character differences.  For example, one version of the book has a repeated misspelling of ```\nleige```\n hundreds of times and this is corrected in the next version to ```\nliege```\n.  Some proper nouns have also changed their spelling.  (I could make custom workarounds for each misspelling, but would like something more general purpose)\n\nSince I only care about more significant multi-word differences want I really want is to set a filter that ignores changes for a line unless the Levenshtein edit distance is above some threshold.\n\nLooking around all the diff/comparisons tools I find seem to have code in mind so they lack any feature around ignoring small text changes.  Google's diff_match_patch library is great for diffing plaintext and ignoring whitespace changes (demo here) but doesn't seem to have an out of the box way to ignore single character non-whitespace differences.\n\ntl;dr; Are there any diff tools that can compare text documents but filter out minor single character non-whitespace differences?\n    ", "Answer": "\r\nIn Beyond compare you can define \"replacements\".\n\nAn example:\nDifferences are marked red:\n\n\nThen you can go to Session->Session Settings and set a replacement:\n\n\nOr even easier: Mark the text and define the replacement immediate:\n\n\n\nNow the difference is unimportant and marked blue:\n\n\nWith one click you can ignore the unimportant differences (red arrow in the screenshot). \n\nTechnical remark: I use BC4 with the pro edition. \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "performance issue, edit distance for large strings LCP vs Levenshtein vs SIFT\r\n                \r\nSo I'm trying to calculate the distance between two large strings (about 20-100).\nThe obstacle is the performance, I need to run 20k distance comparisons. (It takes hours)\n\nAfter investigating, I came a cross few algorithms, And I'm having trouble to decide which to choose. (based on performance VS accuracy)\n\nhttps://github.com/tdebatty/java-string-similarity - performance list for each of the algorithms.\n\n** EDITED **\n\n\nIs SIFT4 algorithm well-proven / reliable?\nIs SIFT4 the right algorithm for the task?\nHow come it's so much faster than LCP-based / Levenshtein algorithm?\nIs SIFT also used in image processing? or is it a different thing? answered by AMH\n\n\nThanks.\n    ", "Answer": "\r\nAs far as i know Scale-invariant feature transform (SIFT) is an algorithm in computer vision detect and describe local features in images.\n\nalso if you want to find similar images you must compare local features of images to each other by calculating their distance which may do what you intend to do. but local features are vector of numbers as i remember. it uses Brute-Force matcher:Feature Matching - OpenCV Library - SIFT\n\nplease read about SIFT here: http://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html\n\nSIFT4 which is mentioned on your provided link is completely different thing.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "modify edit distance algorithm between two text strings in python\r\n                \r\nI am trying to add two modifications to the standard algorithm.\n\nMy strings are text and case sensitive.\n\nSay I have a word \"cage\". The hamming distance between \"cage\" and \"Cage\" would be 0 (first letter). Any other letter would be 0.5. (say, \"cage\" and \"cAge\". \n\nTwo, \"cage\" and \"caKe\" would be 1.5 (different letter=1 plus different caps =0.5), Three, \"cake\" and \"caqe\" would be 0 (consider k and q to be same letter). \n\nSame rules apply for longs sentences too. (say \"Happy Birthday\" and \"happy BudthDay\" distance = 1+1+0.5=2.5)\n\nI would like to pass in any set of words/sentences and modified algorithm instead of standard algorithm needs to be applicable.\n\nI have written a sample code in python for case 1 but unable to understand how to move forward with capitalization.\n\n```\n def editDistance(str1, str2):  if str1[1]==str2[1]:\n            return editDistance(str1,str2)\n print editDistance(str1, str2, len(str1), len(str2))\n```\n\n\nPS: Any explanation in R would be great too.\n    ", "Answer": "\r\ncheck this code out - I have put comments too against it for explanation.\n\n```\ndef editDistance(str1, str2):\n    if (str1 == str2): # if both strings equal, print 0\n        print 0\n    else:\n        counter = 0\n        for c in range(1, len(str1)-1): # iterate through each character in string\n            if (str1[c] == str2[c]): # if characters are equal, don't increment counter\n                counter += 0\n            elif (((str1[c].lower()) == str2[c]) or ((str2[c].lower()) == str1[c])):\n                counter += 0.5 # if the lowercase of characters are equal, increment 0.5\n            elif ((str1[c].islower()) and (str2[c].islower())):\n                counter += 1 # else if both unequal, both lowercase, increment 1\n            elif ((str1[c].isupper()) and (str2[c].isupper())):\n                counter += 1 # else if both unequal, both uppercase, increment 1\n            else:\n                counter += 1.5 # reaches here if both unequal and different case, so 1.5\n        print counter\n\neditDistance(str1, str2); # call the function with the strings\n```\n\n\nI am not sure why you were calling the function with the length of strings twice. I have tried this and it works as you expected. Hope this helps!\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Why is this F# code so slow?\r\n                \r\nA Levenshtein implementation in C# and F#. The C# version is 10 times faster for two strings of about 1500 chars. C#: 69 ms, F# 867 ms. Why? As far as I can tell, they do the exact same thing? Doesn't matter if it is a Release or a Debug build. \n\nEDIT: If anyone comes here looking specifically for the Edit Distance implementation, it is broken. Working code is here.\n\nC#:\n\n```\nprivate static int min3(int a, int b, int c)\n{\n   return Math.Min(Math.Min(a, b), c);\n}\n\npublic static int EditDistance(string m, string n)\n{\n   var d1 = new int[n.Length];\n   for (int x = 0; x < d1.Length; x++) d1[x] = x;\n   var d0 = new int[n.Length];\n   for(int i = 1; i < m.Length; i++)\n   {\n      d0[0] = i;\n      var ui = m[i];\n      for (int j = 1; j < n.Length; j++ )\n      {\n         d0[j] = 1 + min3(d1[j], d0[j - 1], d1[j - 1] + (ui == n[j] ? -1 : 0));\n      }\n      Array.Copy(d0, d1, d1.Length);\n   }\n   return d0[n.Length - 1];\n}\n```\n\n\nF#:\n\n```\nlet min3(a, b, c) = min a (min b c)\n\nlet levenshtein (m:string) (n:string) =\n   let d1 = Array.init n.Length id\n   let d0 = Array.create n.Length 0\n   for i=1 to m.Length-1 do\n      d0.[0] <- i\n      let ui = m.[i]\n      for j=1 to n.Length-1 do\n         d0.[j] <- 1 + min3(d1.[j], d0.[j-1], d1.[j-1] + if ui = n.[j] then -1 else 0)\n      Array.blit d0 0 d1 0 n.Length\n   d0.[n.Length-1]\n```\n\n    ", "Answer": "\r\nThe problem is that the ```\nmin3```\n function is compiled as a generic function that uses generic comparison (I thought this uses just ```\nIComparable```\n, but it is actually more complicated - it would use structural comparison for F# types and it's fairly complex logic).\n\n\n\n```\n> let min3(a, b, c) = min a (min b c);;\nval min3 : 'a * 'a * 'a -> 'a when 'a : comparison\n```\n\n\nIn the C# version, the function is not generic (it just takes ```\nint```\n). You can improve the F# version by adding type annotations (to get the same thing as in C#):\n\n```\nlet min3(a:int, b, c) = min a (min b c)\n```\n\n\n...or by making ```\nmin3```\n as ```\ninline```\n (in which case, it will be specialized to ```\nint```\n when used):\n\n```\nlet inline min3(a, b, c) = min a (min b c);;\n```\n\n\nFor a random string ```\nstr```\n of length 300, I get the following numbers:\n\n```\n> levenshtein str (\"foo\" + str);;\nReal: 00:00:03.938, CPU: 00:00:03.900, GC gen0: 275, gen1: 1, gen2: 0\nval it : int = 3\n\n> levenshtein_inlined str (\"foo\" + str);;\nReal: 00:00:00.068, CPU: 00:00:00.078, GC gen0: 0, gen1: 0, gen2: 0\nval it : int = 3\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Available code to compute affine gap distance\r\n                \r\nGiven the ubiquitous availability of code (in C, R, python, Java) which computes the Levenshtein edit distance, I am somewhat surprised at the lack of implementations of other edit distances such as the affine gap distance.\nAre there easily usable libraries that compute this measure ?\n\nThanks!\nML\n    ", "Answer": "\r\nThere is an open java similarity library here, may help: https://code.google.com/p/java-similarities/ and http://sourceforge.net/projects/simmetrics/\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein algorithm - fail-fast if edit distance is bigger than a given threshold\r\n                \r\nFor the Levenshtein algorithm I have found this implementation for Delphi. \n\nI need a version which stops as soon as a maximum distance is hit, and return the distance found so far.\n\nMy first idea is to check the current result after every iteration:\n\n```\nfor i := 1 to n do\n    for j := 1 to m do\n    begin\n      d[i, j] := Min(Min(d[i-1, j]+1, d[i,j-1]+1), d[i-1,j-1]+Integer(s[i] <> t[j]));\n\n      // check   \n      Result := d[n, m];\n      if Result > max then\n      begin\n        Exit;\n      end; \n\n    end;\n```\n\n    ", "Answer": "\r\nI gather what you want is to find the levenstein distance, if it is below ```\nMAX```\n, right?\n\nIf so, reaching a value larger than ```\nMAX```\n is not enough, since it only means that some path is longer than that, but not that there exists no shorter path. To make sure no path shorter than ```\nMAX```\n can be found, one has to monitor the minimum possible length of a path until the current point, i.e. the minimum over a column in the distance table.\n\nI'm not good at Delphi, but I think the code should look something like this:\n\n```\nfor i := 1 to n do\nbegin;\n    min := MAX + 1\n    for j := 1 to m do\n    begin;\n      d[i, j] := Min(Min(d[i-1, j]+1, d[i,j-1]+1), d[i-1,j-1]+Integer(s[i] <> t[j]));\n      min := Min(min, d[i,j])\n    end;\n    if min >= MAX then\n        Exit;\nend;\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is Levenshtein's distance the right way to tackle this Edit Steps problem?\r\n                \r\nI'm familiar with Levenshtein's distance, so I decided I would use it to solve UVA's Edit Steps Ladder problem.\n\nMy solution is:\n\n```\nimport java.io.*;\nimport java.util.*;\n\nclass LevenshteinParaElJuez implements Runnable{\n    static String ReadLn(int maxLength){  // utility function to read from stdin,\n                                          // Provided by Programming-challenges, edit for style only\n        byte line[] = new byte [maxLength];\n        int length = 0;\n        int input = -1;\n        try{\n            while (length < maxLength){//Read untill maxlength\n                input = System.in.read();\n                if ((input < 0) || (input == '\\n')) break; //or untill end of line ninput\n                line [length++] += input;\n            }\n\n            if ((input < 0) && (length == 0)) return null;  // eof\n            return new String(line, 0, length);\n         }catch (IOException e){\n            return null;\n        }\n    }\n\n    public static void main(String args[]) // entry point from OS\n    {\n        LevenshteinParaElJuez myWork = new LevenshteinParaElJuez();  // Construct the bootloader\n        myWork.run();            // execute\n    }\n\n    public void run() {\n        new myStuff().run();\n    }\n}\nclass myStuff implements Runnable{\n    public void run(){\n\n        ArrayList<String> theWords = new ArrayList<String>();\n        try\n        {\n\n        /// PLACE YOUR JAVA CODE HERE\n\n        String leido=LevenshteinParaElJuez.ReadLn(100);\n\n        //System.out.println(\"lo leido fue \"+leido);\n\n        while (leido.length() != 0){\n        theWords.add(leido);\n        leido=LevenshteinParaElJuez.ReadLn(100);\n        }\n\n\n        }catch(Exception e){\n            System.out.println(\"El programa genero una excepcion\");\n        }\n\n\n        int maxEdit=0;\n        int actualEdit=0;\n\n     int wordsIndex1 =0, wordsIndex2=0;\n\n\n     while (wordsIndex1<= theWords.size())\n     {\n      while (wordsIndex2<= theWords.size()-1){\n         actualEdit=Levenshtein.computeLevenshteinDistance(theWords.get(wordsIndex1),theWords.get(wordsIndex2));\n         if (actualEdit>maxEdit){maxEdit=actualEdit;}\n         wordsIndex2++;\n      }\n     wordsIndex1++;\n\n     }\n\n     System.out.println(maxEdit+1);\n\n\n\n    }\n\n\n}\nclass Levenshtein {\n    private static int minimum(int a, int b, int c) {\n        if(a<=b && a<=c)\n            return a;\n        if(b<=a && b<=c)\n            return b;\n        return c;\n    }\n\n    public static int computeLevenshteinDistance(String str1, String str2) {\n        return computeLevenshteinDistance(str1.toCharArray(),\n                                          str2.toCharArray());\n    }\n\n    private static int computeLevenshteinDistance(char [] str1, char [] str2) {\n        int [][]distance = new int[str1.length+1][str2.length+1];\n\n        for(int i=0;i<=str1.length;i++)\n                distance[i][0]=i;\n\n        for(int j=0;j<=str2.length;j++)\n            distance[0][j]=j;\n\n        for(int i=1;i<=str1.length;i++)\n            for(int j=1;j<=str2.length;j++)\n                distance[i][j]= minimum(distance[i-1][j]+1,\n                                        distance[i][j-1]+1,\n                                        distance[i-1][j-1]+\n                                        ((str1[i-1]==str2[j-1])?0:1));\n\n        return distance[str1.length][str2.length];\n    }\n\n\n}\n```\n\n\nWith this input:\n\n```\ncat\ndig\ndog\nfig\nfin\nfine\nfog\nlog\nwine\n```\n\n\nit produces the correct output for this sample:\n\n```\n5\n```\n\n\nThe judge is rejecting my answer. This is my first attempt at solving an online judge's problem, and I think I maybe forcing a correct answer here:\n\n```\n System.out.println(maxEdit+1);\n```\n\n\nsince maxEdit has a value of 4 when computed simply with Levenshtein. Is that what's going on?\n    ", "Answer": "\r\nLevinshtein is relevant, but will not give you a value used in your output. In this problem, use it to determine if two words have an edit distance of exactly 1, indicating the two words compared are adjacent in the edit step ladder.\n\nIterate over the words in the dict. and if the next word has an edit distance of 1 from the current word, you may make that the current word, otherwise it must be skipped.\n\nThe trick to this problem is finding all possible sequences - just because the next word has an edit distance of 1 doesn't mean using it in the ladder will give you the longest possible ladder.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "In R distance between two sentences: Word-level comparison by minimum edit distance\r\n                \r\nWhile trying to learn R, I want to implement the algorithm below in R. Consider the two lists below:\n\n```\nList 1: \"crashed\", \"red\", \"car\"\nList 2: \"crashed\", \"blue\", \"bus\"\n```\n\n\nI want to find out how many actions it would take to transform 'list1' into 'list2'.\nAs you can see I need only two actions: \n```\n1. Replace \"red\" with \"blue\".```\n\n```\n2. Replace \"car\" with \"bus\".```\n\n\nBut, how we can find the number of actions like this automatically.\nWe can have several actions to transform the sentences: ADD, REMOVE, or REPLACE the words in the list.\nNow, I will try my best to explain how the algorithm should work:\n\nAt the first step: I will create a table like this: \n\nrows: i= 0,1,2,3,\n   columns: j = 0,1,2,3\n\n```\n(example: value[0,0] = 0 , value[0, 1] = 1 ...)```\n\n\n```\n                 crashed    red     car\n         0          1        2       3\n\ncrashed  1\nblue     2\nbus      3\n```\n\n\nNow, I will try to fill the table. Please, note that each cell in the table shows the number of actions we need to do to reformat the sentence (ADD, remove, or replace). \nConsider the interaction between \"crashed\" and \"crashed\" (```\nvalue[1,1]```\n), obviously we don't need to change it so the value will be '0'. Since they are the same words. Basically, we got the diagonal value = ```\nvalue[0,0]```\n\n\n```\n                 crashed    red     car\n         0          1        2       3\n\ncrashed  1          0\nblue     2\nbus      3\n```\n\n\nNow, consider \"crashed\" and the second part of the sentence which is \"red\". Since they are not the same word we can use calculate the number of changes like this :\n\n```\nmin{value[0,1] , value[0,2] and value[1,1]} + 1 \nmin{ 1, 2, 0} + 1 = 1 \n```\n\n\nTherefore, we need to just remove \"red\".\nSo, the table will look like this:\n\n```\n                 crashed    red     car\n         0          1        2       3\n\ncrashed  1          0        1\nblue     2  \nbus      3\n```\n\n\nAnd we will continue like this :\n\"crashed\" and \"car\" will be :\n\n```\nmin{value[0,3], value[0,2] and value[1,2]} + 1 \nmin{3, 2, 1} +1 = 2\n```\n\n\nand the  table will be:\n\n```\n                 crashed    red     car\n         0          1        2       3\n\ncrashed  1          0        1       2\nblue     2  \nbus      3\n```\n\n\nAnd we will continue to do so. the final result will be :\n\n```\n             crashed    red     car\n         0      1        2       3\n\ncrashed  1      0        1       2\nblue     2      1        1       2\nbus      3      2        2       2 \n```\n\n\nAs you can see the last number in the table shows the distance between two sentences: value[3,3] = 2\n\nBasically, the algorithm should look like this:\n\n```\n if (characters_in_header_of_matrix[i]==characters_in_column_of_matrix [j] & \n                                            value[i,j] == value[i+1][j-1] )\n\nthen {get the 'DIAGONAL VALUE' #diagonal value= value[i, j-1]}\n\nelse{\nvalue[i,j] = min(value[i-1, j], value[i-1, j-1],  value[i, j-1]) + 1\n }\n  endif\n```\n\n\nfor finding the difference between the elements of two lists that you can see in the header and the column of the matrix, I have used the ```\nstrcmp()```\n function which will give us a boolean value(TRUE or FALSE) while comparing the words. But, I fail at implementing this. \nI'd appreciate your help on this one, thanks.\n    ", "Answer": "\r\nThe question\n\nAfter some clarification in a previous post, and after the update of the post, my understanding is that Zero is asking: 'how one can iteratively count the number of word differences in two strings'. \n\nI am unaware of any implementation in R, although i would be surprised if i doesn't already exists. I took a bit of time out to create a simple implementation, altering the algorithm slightly for simplicity (For anyone not interested scroll down for 2 implementations, 1 in pure R, one using the smallest amount of Rcpp). The general idea of the implementation:\n\n\nInitialize with ```\nstring_1```\n and ```\nstring_2```\n of length ```\nn_1```\n and ```\nn_2```\n \nCalculate the cumulative difference between the first ```\nmin(n_1, n_2)```\n elements, \nUse this cumulative difference as the diagonal in the matrix\nSet the first off-diagonal element to the very first element + 1 \nCalculate the remaining off diagonal elements as: ```\ndiag(i) - diag(i-1) + full_matrix(i-1,j)```\n\nIn the previous step i iterates over diagonals, j iterates over rows/columns (either one works), and we start in the third diagonal, as the first 2x2 matrix is filled in step 1 to 4\nCalculate the remaining ```\nabs(n_1 - n_2)```\n elements as ```\nfull_matrix[,min(n_1 - n_2)] + 1:abs(n_1 - n_2)```\n, applying the latter over each value in the prior, and bind them appropriately to the full_matrix. \n\n\nThe output is a matrix with dimensions row and column names of the corresponding strings, which has been formatted for some easier reading. \n\nImplementation in R\n\n```\nDist_between_strings <- function(x, y, \n                                 split = \" \", \n                                 split_x = split, split_y = split, \n                                 case_sensitive = TRUE){\n  #Safety checks\n  if(!is.character(x) || !is.character(y) || \n     nchar(x) == 0 || nchar(y) == 0)\n    stop(\"x, y needs to be none empty character strings.\")\n  if(length(x) != 1 || length(y) != 1)\n    stop(\"Currency the function is not vectorized, please provide the strings individually or use lapply.\")\n  if(!is.logical(case_sensitive))\n    stop(\"case_sensitivity needs to be logical\")\n  #Extract variable names of our variables\n  # used for the dimension names later on\n  x_name <- deparse(substitute(x))\n  y_name <- deparse(substitute(y))\n  #Expression which when evaluated will name our output\n  dimname_expression <- \n    parse(text = paste0(\"dimnames(output) <- list(\",make.names(x_name, unique = TRUE),\" = x_names,\",\n                        make.names(y_name, unique = TRUE),\" = y_names)\"))\n  #split the strings into words\n  x_names <- str_split(x, split_x, simplify = TRUE)\n  y_names <- str_split(y, split_y, simplify = TRUE)\n  #are we case_sensitive?\n  if(isTRUE(case_sensitive)){\n    x_split <- str_split(tolower(x), split_x, simplify = TRUE)\n    y_split <- str_split(tolower(y), split_y, simplify = TRUE)\n  }else{\n    x_split <- x_names\n    y_split <- y_names\n  }\n  #Create an index in case the two are of different length\n  idx <- seq(1, (n_min <- min((nx <- length(x_split)),\n                              (ny <- length(y_split)))))\n  n_max <- max(nx, ny)\n  #If we have one string that has length 1, the output is simplified\n  if(n_min == 1){ \n    distances <- seq(1, n_max) - (x_split[idx] == y_split[idx])\n    output <- matrix(distances, nrow = nx)\n    eval(dimname_expression)\n    return(output)\n  }\n  #If not we will have to do a bit of work\n  output <- diag(cumsum(ifelse(x_split[idx] == y_split[idx], 0, 1)))\n  #The loop will fill in the off_diagonal\n  output[2, 1] <- output[1, 2] <- output[1, 1] + 1 \n  if(n_max > 2)\n    for(i in 3:n_min){\n      for(j in 1:(i - 1)){\n        output[i,j] <- output[j,i] <- output[i,i] - output[i - 1, i - 1] + #are the words different?\n          output[i - 1, j] #How many words were different before?\n      }\n    }\n  #comparison if the list is not of the same size\n  if(nx != ny){\n    #Add the remaining words to the side that does not contain this\n    additional_words <- seq(1, n_max - n_min)\n    additional_words <- sapply(additional_words, function(x) x + output[,n_min])\n    #merge the additional words\n    if(nx > ny)\n      output <- rbind(output, t(additional_words))\n    else\n      output <- cbind(output, additional_words)\n  }\n  #set the dimension names, \n  # I would like the original variable names to be displayed, as such i create an expression and evaluate it\n  eval(dimname_expression)\n  output\n}\n```\n\n\nNote that the implementation is not vectorized, and as such can only take single string inputs!\n\nTesting the implementation\n\nTo test the implementation, one could use the strings given. As they were said to be contained in lists, we will have to convert them to strings. Note that the function lets one split each string differently, however it assumes space separated strings. So first I'll show how one could achieve a conversion to the correct format:\n\n```\nlist_1 <- list(\"crashed\",\"red\",\"car\")\nlist_2 <- list(\"crashed\",\"blue\",\"bus\")\nstring_1 <- paste(list_1,collapse = \" \")\nstring_2 <- paste(list_2,collapse = \" \")\nDist_between_strings(string_1, string_2)\n```\n\n\noutput\n\n```\n#Strings in the given example\n         string_2\nstring_1  crashed blue bus\n  crashed       0    1   2\n  red           1    1   2\n  car           2    2   2\n```\n\n\nThis is not exactly the output, but it yields the same information, as the words are ordered as they were given in the string. \nMore examples\nNow i stated it worked for other strings as well and this is indeed the fact, so lets try some random user-made strings:\n\n```\n#More complicated strings\nstring_3 <- \"I am not a blue whale\"\nstring_4 <- \"I am a cat\"\nstring_5 <- \"I am a beautiful flower power girl with monster wings\"\nstring_6 <- \"Hello\"\nDist_between_strings(string_3, string_4, case_sensitive = TRUE)\nDist_between_strings(string_3, string_5, case_sensitive = TRUE)\nDist_between_strings(string_4, string_5, case_sensitive = TRUE)\nDist_between_strings(string_6, string_5)\n```\n\n\nRunning these show that these do yield the correct answers. Note that if either string is of size 1, the comparison is a lot faster.\n\nBenchmarking the implementation\n\nNow as the implementation is accepted, as correct, we would like to know how well it performs (For the uninterested reader, one can scroll past this section, to where a faster implementation is given). For this purpose, i will use much larger strings. For a complete benchmark i should test various string sizes, but for the purposes i will only use 2 rather large strings of size 1000 and 2500. For this purpose i use the ```\nmicrobenchmark```\n package in R, which contains a ```\nmicrobenchmark```\n function, which claims to be accurate down to nanoseconds. The function itself executes the code 100 (or a user defined) number of times, returning the mean and quartiles of the run times. Due to other parts of R such as the Garbage Cleaner, the median is mostly considered a good estimate of the actual average run-time of the function.\nThe execution and results are shown below:\n\n```\n#Benchmarks for larger strings\nset.seed(1)\nstring_7 <- paste(sample(LETTERS,1000,replace = TRUE), collapse = \" \")\nstring_8 <- paste(sample(LETTERS,2500,replace = TRUE), collapse = \" \")\nmicrobenchmark::microbenchmark(String_Comparison = Dist_between_strings(string_7, string_8, case_sensitive = FALSE))\n# Unit: milliseconds\n# expr                   min      lq      mean   median       uq      max neval\n# String_Comparison 716.5703 729.4458 816.1161 763.5452 888.1231 1106.959   100\n```\n\n\nProfiling\n\nNow i find the run-times very slow. One use case for the implementation could be an initial check of student hand-ins to check for plagiarism, in which case a low difference count very likely shows plagiarism. These can be very long and there may be hundreds of handins, an as such i would like the run to be very fast. \nTo figure out how to improve my implementation i used the ```\nprofvis```\n package with the corrosponding ```\nprofvis```\n function. To profile the function i exported it in another R script, that i sourced, running the code 1 once prior to profiling to compile the code and avoid profiling noise (important). The code to run the profiling can be seen below, and the most important part of the output is visualized in an image below it.\n\n```\nlibrary(profvis)\nprofvis(Dist_between_strings(string_7, string_8, case_sensitive = FALSE))\n```\n\n\n\n\nNow, despite the colour, here i can see a clear problem. The loop filling the off-diagonal by far is responsible for most of the runtime. R (like python  and other not compiled languages) loops are notoriously slow. \n\nUsing Rcpp to improve performance\n\nTo improve the implementation, we could implement the loop in c++ using the ```\nRcpp```\n package. This is rather simple. The code is not unlike the one we would use in R, if we avoid iterators. A c++ script can be made in file -> new file -> c++ File. The following c++ code would be pasted into the corrosponding file and sourced using the source button.\n\n```\n//Rcpp Code\n#include <Rcpp.h>\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nNumericMatrix Cpp_String_difference_outer_diag(NumericMatrix output){\n  long nrow = output.nrow();\n  for(long i = 2; i < nrow; i++){ // note the \n    for(long j = 0; j < i; j++){\n      output(i, j) = output(i, i) - output(i - 1, i - 1) + //are the words different?\n                                  output(i - 1, j);\n      output(j, i) = output(i, j);\n    }\n  }\n  return output;\n}\n```\n\n\nThe corresponding R function needs to be altered to use this function instead of looping. The code is similar to the first function, only switching the loop for a call to the c++ function.\n\n```\nDist_between_strings_cpp <- function(x, y, \n                                 split = \" \", \n                                 split_x = split, split_y = split, \n                                 case_sensitive = TRUE){\n  #Safety checks\n  if(!is.character(x) || !is.character(y) || \n     nchar(x) == 0 || nchar(y) == 0)\n    stop(\"x, y needs to be none empty character strings.\")\n  if(length(x) != 1 || length(y) != 1)\n    stop(\"Currency the function is not vectorized, please provide the strings individually or use lapply.\")\n  if(!is.logical(case_sensitive))\n    stop(\"case_sensitivity needs to be logical\")\n  #Extract variable names of our variables\n  # used for the dimension names later on\n  x_name <- deparse(substitute(x))\n  y_name <- deparse(substitute(y))\n  #Expression which when evaluated will name our output\n  dimname_expression <- \n    parse(text = paste0(\"dimnames(output) <- list(\", make.names(x_name, unique = TRUE),\" = x_names,\",\n                        make.names(y_name, unique = TRUE),\" = y_names)\"))\n  #split the strings into words\n  x_names <- str_split(x, split_x, simplify = TRUE)\n  y_names <- str_split(y, split_y, simplify = TRUE)\n  #are we case_sensitive?\n  if(isTRUE(case_sensitive)){\n    x_split <- str_split(tolower(x), split_x, simplify = TRUE)\n    y_split <- str_split(tolower(y), split_y, simplify = TRUE)\n  }else{\n    x_split <- x_names\n    y_split <- y_names\n  }\n  #Create an index in case the two are of different length\n  idx <- seq(1, (n_min <- min((nx <- length(x_split)),\n                              (ny <- length(y_split)))))\n  n_max <- max(nx, ny)\n  #If we have one string that has length 1, the output is simplified\n  if(n_min == 1){ \n    distances <- seq(1, n_max) - (x_split[idx] == y_split[idx])\n    output <- matrix(distances, nrow = nx)\n    eval(dimname_expression)\n    return(output)\n  }\n  #If not we will have to do a bit of work\n  output <- diag(cumsum(ifelse(x_split[idx] == y_split[idx], 0, 1)))\n  #The loop will fill in the off_diagonal\n  output[2, 1] <- output[1, 2] <- output[1, 1] + 1 \n  if(n_max > 2) \n    output <- Cpp_String_difference_outer_diag(output) #Execute the c++ code\n  #comparison if the list is not of the same size\n  if(nx != ny){\n    #Add the remaining words to the side that does not contain this\n    additional_words <- seq(1, n_max - n_min)\n    additional_words <- sapply(additional_words, function(x) x + output[,n_min])\n    #merge the additional words\n    if(nx > ny)\n      output <- rbind(output, t(additional_words))\n    else\n      output <- cbind(output, additional_words)\n  }\n  #set the dimension names, \n  # I would like the original variable names to be displayed, as such i create an expression and evaluate it\n  eval(dimname_expression)\n  output\n}\n```\n\n\nTesting the c++ implementation\n\nTo be sure the implementation is correct we check if the same output is obtained with the c++ implementation.\n\n```\n#Test the cpp implementation\nidentical(Dist_between_strings(string_3, string_4, case_sensitive = TRUE),\n          Dist_between_strings_cpp(string_3, string_4, case_sensitive = TRUE))\n#TRUE\n```\n\n\nFinal benchmarks\n\nNow is this actually faster? To see this we could run another benchmark using the ```\nmicrobenchmark```\n package. The code and results are shown below:\n\n```\n#Final microbenchmarking\nmicrobenchmark::microbenchmark(R = Dist_between_strings(string_7, string_8, case_sensitive = FALSE),\n                               Rcpp = Dist_between_strings_cpp(string_7, string_8, case_sensitive = FALSE))\n# Unit: milliseconds\n# expr       min       lq      mean    median        uq       max neval\n# R    721.71899 753.6992 850.21045 787.26555 907.06919 1756.7574   100\n# Rcpp  23.90164  32.9145  54.37215  37.28216  47.88256  243.6572   100\n```\n\n\nFrom the microbenchmark median improvement factor of roughly ```\n21 ( = 787 / 37)```\n, which is a massive improvement from just implementing a single loop!\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Python: Efficient way to find Levenshtein edit distance in a matrix\r\n                \r\nI would like to identify the similarity between two lists after that I want to do clustering of descriptions.\n\n```\n          L2D1    L2D2     L2D2 .........L2Dn\n  L1D1     0       0.3     0.8............0.5  \n  L1D2     0.2     0.7     0.3............0.2\n  L1D3     0       0.3     0.8............0.5   \n  .        .        .       .              .\n  .        .        .       .              .   \n  .        .        .       .              .\n  L1Dn    0.6      0.1     0.9............0.4           \n\nfrom Levenshtein import distance\nList1 = list(new['Description'])\nList2 = list(clean['Description'])\n\nMatrix = np.zeros((len(List1),len(List2)),dtype=np.int)\n\nfor i in range(0,len(List1)):\n  for j in range(0,len(List2)):\n      Matrix[i,j] = distance(List1[i],List2[j])\n```\n\n\nSince the above method is time consuming as size and length of data.\n\nI tried to compare first five words of description if it matches only then calculate the distance between two string, else move to next description of the list in method2.\n\n```\n#Method2\nfor i in range(0,len(List1)):\n    K1[i]=str(List1[:1]).split()[0:5]\n    for j in range(0,len(List2)):\n        K1[i]=str(List2[:1]).split()[0:5]\n        if (distance(K1[i],K2[j]))==0:\n            Matrix[i,j]=distance(List1[i],List2[j])\n        else:\n            Matrix[i,j]=1000\n```\n\n\nBut as I am new to this missing some logic and getting:\n\n\n  TypeError: 'int' object does not support item assignment\n\n\nI also want to implement same for next 10 and 100 words. Thanks in advance.\n    ", "Answer": "\r\nI think, you should check numpy documentation and ndarray class.\n\nHere is little bit pythonic way:\n\n```\nfor i, new_value in enumerate(List1):\n   for j, clean_value in enumerate(List2):\n      Matrix[i,j] = distance(new_value, clean_value)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Hashing technique for strings with edit distance 2\r\n                \r\nIs there any hashing techniques which will hash same value for two strings with very few mismatches ? For example, I have two strings ```\nabcdabcdabcd```\n and ```\naacdabcdabcd```\n. I would like to use a hash function which can give same value for the above two strings. I was trying to use ```\nmetroHash```\n and ```\nmurmerHash```\n, but I could not find the way to resolve the above issue. \n    ", "Answer": "\r\nI challenge your question like this: \n\nImagine that I have double values like 7.3 and 8.5 and I want them to have the same hash. You would immedietly ask if 9.1 and then 10.0 and then 10.4 and then 10.8 and so forth should get the same hash too, wouldn't you? Because 9.1 is as near to 8.5 as 8.5 is to 7.3.\n\nSo for numbers you would partition into [0 ; 1[ , [1 ; 2[ and so forth and assign the same hash to each partition. What would be the analog way to partition strings?\n\nSo you have to find distinct areas maybe defined by representants and then generate the hash based on the nearest representant. You could chose the representants \"\", \"a\", \"aa\", \"aaa\" and so forth and return some hash of the representant that is NEXT to the given value where NEXT may be defined by the Levenshtein distance or by the mere string length.\n\nAll in all I guess you are on the wrong track and should explain your original problem in the first place. A hash is not meant to compute any meaningful cluster adress or partition number of a value.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Trying to implement a simple edit distance module\r\n                \r\nThis is the code for the function :\n\n```\ndef populateConfusionMatrix(word,errword):\n    dp = [[0]*(len(errword)+1) for i in range(len(word)+1)]\n    m = len(word)+1;\n    n = len(errword)+1;\n    for i in range(m):\n        for j in range(n):\n            dp[i][0] = i;\n            dp[0][j] = j;\n    for i in range(m):\n        for j in range(n): \n            print(i,j)\n            if i==0 or j==0 :\n                continue\n\n            dis = [0]*4\n            dis[0] = dp[i-1][j]+1\n            dis[1] = dp[i][j-1]+1\n            print(\"dis[1] is \",dp[i][j-1]+1)\n            if word[i-1] == errword[j-1]:\n                dis[2] = dp[i-1][j-1]\n            else :\n                dis[2] = dp[i-1][j-1]+1\n\n            if i>1 and j>1 and word[i] == errword[j-1] and word[i-1] == errword[j]:\n                dis[3] = dp[i-2][j-2] + 1 \n            if dis[3]!=0 :\n                dp[i][j] = min(dp[0:4])\n            else :\n                dp[i][j] = min(dp[0:3])\n\n    i = m\n    j = n\n    while(i>=0 and j>=0) :\n        if word[i-1] == errword[j-1] :\n            i=i-1\n            j=j-1\n            continue\n        if dp[i][j] == dp[i][j-1]+1 :\n            populate_ins(word[i],errword[j])\n            j=j-1\n        if dp[i][j] == dp[i-1][j]+1 :\n            populate_del(errword[j],word[i])\n            i=i-1\n        if dp[i][j] == dp[i-1][j-1] + 1 :\n            populate_sub(word[i],errword[j])\n            i=i-1\n            j=j-1\n        if i>1 and j>1 and word[i] == errword[j-1] and word[i-1] == errword[j] and dp[i][j] == dp[i-2][j-2]+1 :\n            populate_exc(word[i-1],word[i])\n            i=i-1\n            j=j-1\n```\n\n\nBut this code is showing this error on calling the function:\n\n```\npopulateConfusionMatrix(\"actress\",\"acress\")\n```\n\n\nError - \n\n```\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-25-d5ba10f95b61> in <module>()\n----> 1 populateConfusionMatrix(\"actress\",\"acress\")\n\n<ipython-input-24-e996de70e204> in populateConfusionMatrix(word, errword)\n     15             dis = [0]*4\n     16             dis[0] = dp[i-1][j]+1\n---> 17             dis[1] = dp[i][j-1]+1\n     18             print(\"dis[1] is \",dp[i][j-1]+1)\n     19             if word[i-1] == errword[j-1]:\n\nTypeError: can only concatenate list (not \"int\") to list\n```\n\n\nTried to print till what value of (i,j) the loop is working fine, I got this - \n\n```\n(0, 0)\n(0, 1)\n(0, 2)\n(0, 3)\n(0, 4)\n(0, 5)\n(0, 6)\n(1, 0)\n(1, 1)\n('dis[1] is ', 2)\n(1, 2)\n```\n\n    ", "Answer": "\r\nYour code is really difficult to understand, but the problem is definitely in these lines:\n\n```\nif dis[3]!=0 :\n    dp[i][j] = min(dp[0:4])\nelse :\n    dp[i][j] = min(dp[0:3])\n```\n\n\nSince your list ```\ndp```\n has the value:\n\n```\n[[0, 1, 2, 3, 4, 5, 6], [1, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0], [3, 0, 0, 0, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0], [5, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0], [7, 0, 0, 0, 0, 0, 0]]\n```\n\n\nWhen you use ```\ndp[i][j] = min(dp[0:3])```\n, you're calling ```\nmin```\n on a slice of ```\ndp```\n, or:\n\n```\nmin([0, 1, 2, 3, 4, 5, 6], [1, 0, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0])\n```\n\n\nThat's why you're getting errors later on while trying to add a number to a list:\n\n```\ndis[1] = dp[i][j-1] + 1  # evaluates to something like [0,0,0,0] + 1\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "edit distance solution with O(n) space issue\r\n                \r\nFound a few different solutions and debugging, and especially interested in below solution which requires only O(n) space, other than store a matrix (M*N). But confused about what is the logical meaning of cur[i]. If anyone have any comments, it will be highly appreciated.\n\nI posted solution and code.\n\n```\nGiven two words word1 and word2, find the minimum number of steps required to convert word1 to word2. (each operation is counted as 1 step.)\n\nYou have the following 3 operations permitted on a word:\n\na) Insert a character\nb) Delete a character\nc) Replace a character\n\nclass Solution { \npublic:\n    int minDistance(string word1, string word2) {\n        int m = word1.length(), n = word2.length();\n        vector<int> cur(m + 1, 0);\n        for (int i = 1; i <= m; i++)\n            cur[i] = i;\n        for (int j = 1; j <= n; j++) {\n            int pre = cur[0];\n            cur[0] = j;\n            for (int i = 1; i <= m; i++) {\n                int temp = cur[i];\n                if (word1[i - 1] == word2[j - 1])\n                    cur[i] = pre;\n                else cur[i] = min(pre + 1, min(cur[i] + 1, cur[i - 1] + 1));\n                pre = temp;\n            }\n        }\n        return cur[m]; \n    }\n}; \n```\n\n    ", "Answer": "\r\nYou can think of ```\ncur```\n as being as a mix of the previous line and the current line in the edit distance matrix. For example, think of a 3x3 matrix in the original algorithm. I'll number each position like below:\n\n```\n1 2 3\n4 5 6\n7 8 9\n```\n\n\nIn the loop, if you are computing the position ```\n6```\n, you only need the values from ```\n2```\n, ```\n3```\n and ```\n5```\n. In that case, ```\ncur```\n will be exactly the values from:\n\n```\n4 5 3\n```\n\n\nSee the ```\n3```\n in the end? That's because we didn't updated it yet, so it still has a value from the first line. From the previous iteration, we have ```\npre = 2```\n, because it was saved before we computed the value at 5.\n\nThen, the new value for the last cell is the minimum of ```\npre = 2```\n, ```\ncur[i-1] = 5```\n and ```\ncur[i] = 3```\n, exactly the values mentioned before.\n\nEDIT: completing the analogy, if in the O(n^2) version you compute ```\nmin(M[i-1][j-1], M[i][j-1], M[i-1][j])```\n, in this O(n) version you'll compute ```\nmin(pre, cur[i-1], cur[i])```\n, respectively.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Given the pairwise edit distance of a and b and b and c, can we find the pairwise edit distance of a and c?\r\n                \r\nIf we have three string a, b, c and we know ( or already calculated ) edit_distance(a,b) and edit_distance(b,c), can we efficiently calculate edit_distance(a,c) without actually comparing a and c.\n\n*edit_distance(a,b) = number of character insertion, deletion and replacement required to convert a into b.*\n    ", "Answer": "\r\nIn general, no.  For example, take\n\n\na = CAP\nb = CAT\nc = CAR\n\n\nHere, edit_distance(a, b) = 1 and edit_distance(b, c) = 1.  Moreover, edit_distance(a, c) = 1.\n\nHowever, we could also have\n\n\na = CAP\nb = CAT\nc = RAT\n\n\nHere, edit_distance(a, b) = 1 and edit_distance(b, c) = 1, but edit_distance(a, c) = 2.  Therefore, there is no way to purely use the edit distances of a and b and of b and c to compute the edit distance of a and c.\n\nHowever, we do know that edit_distance(a, c) ≤ edit_distance(a, b) + edit_distance(b, c), since you can always apply the transformations in sequence to turn a into c.  More generally, edit distance forms a discrete distance metric, which forms the basis of the BK-tree data structure.\n\nHope this helps!\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Add counter and distance to dictionary\r\n                \r\nHello I have a specific string and I am trying to calculate its distance using edit distance and I want to see the number of counts of the string that occurs and then sort it.\n\n```\nstr= \"Hello\"\n```\n\n\nand a txt file named- xfile I am comparing with is:\n\n```\n\"hola\"\n\"how are you\"\n\"what is up\"\n\"everything good?\"\n\"hola\"\n\"everything good?\"\n\"what is up?\"\n\"okay\"\n\"not cool\"\n\"not cool\"\n```\n\n\nI want to make a dictionary that compares all the lines with the xfile and give it's edit distance and count. \nFor now, I am able to get it's key and distance, but not it's count. \nCan someone please suggest me it?\n\nMy code is:\n\n```\ndata= \"Hello\"\n\nUtterences = {}\n\nfor lines in readFile:\n    dist= editdistance.eval(data,lines)\n    Utterances[lines]= dist\n```\n\n    ", "Answer": "\r\nFor every utterance you can have a dictionary containing the distance and count:\n\n```\nimport editdistance\n\ndata = 'Hello'\n\nutterances = {}\n\nxlist = [\n    'hola',\n    'how are you',\n    'what is up',\n    'everything good?',\n    'hola',\n    'everything good?',\n    'what is up?',\n    'okay',\n    'not cool',\n    'not cool',\n]\n\nfor line in xlist:\n    if line not in utterances:\n        utterances[line] = {\n            'distance': editdistance.eval(data, line),\n            'count': 1\n        }\n    else:\n        utterances[line]['count'] += 1\n```\n\n\nThen if you need the utterances sorted by distance or count you can use an OrderedDict:\n\n```\nfrom collections import OrderedDict\n\nsorted_by_distance = OrderedDict(sorted(utterances.items(), key=lambda t: t[1]['distance']))\nsorted_by_count = OrderedDict(sorted(utterances.items(), key=lambda t: t[1]['count']))\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Editing distance between tick marks\r\n                \r\nI am currently using subplots in ```\nMATLAB```\n and this is my x tick mark data:```\n[.4 .5 .6 .9 1.2 1.5 2 2.5 3 4 5 6 7 8 9 10 12 15 20 30 40]```\n\n\nI am trying to determine if there is a way to space the tick marks evenly, or expand the spacing for the lower values so you can actually read the numbers. In short I would like the physical spacing of the tick marks to be based on some predetermined constant and not the actual numerical values.\n    ", "Answer": "\r\nYou could use logarythmic scale for x-axis - use ```\nsemilogx```\n instead of ```\nplot```\n.(IMO this would be better in your case)\n\n```\nx = [.4 .5 .6 .9 1.2 1.5 2 2.5 3 4 5 6 7 8 9 10 12 15 20 30 40];\ny = x/2; % some example data\nfigure\nsemilogx(x,y, '.')\nset(gca,'xtick', x)\n```\n\n\nAnother option is to change labels on x-ticks setting ```\nxticklabel```\n property. Note that you can set custom values in ```\nticks```\n vector. \n\n```\nx = [.4 .5 .6 .9 1.2 1.5 2 2.5 3 4 5 6 7 8 9 10 12 15 20 30 40];\ny = x/2; % some example data\nticks = [];\nfor t = 1:size(x,2)\n  ticks = [ticks t];\nend\nfigure\nplot(ticks, y, '.') % in this example same as 'plot(y)'\nset(gca, 'xtick', ticks,'xticklabel', {.4 .5 .6 .9 1.2 1.5 2 2.5 3 4 5 6 7 8 9 10 12 15 20 30 40})\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Calculating levenshtein distance between two strings\r\n                \r\nIm executing the following Postgres query. \n\n```\nSELECT *  FROM description WHERE levenshtein(desci, 'Description text?') <= 6  LIMIT 10;\n```\n\n\nIm using the following code execute the above query.\n\n```\npublic static boolean authQuestion(String question) throws SQLException{\n    boolean isDescAvailable = false;\n    Connection connection = null;\n    try {\n        connection = DbRes.getConnection();\n        String query = \"SELECT *  FROM description WHERE levenshtein(desci, ? ) <= 6\";\n        PreparedStatement checkStmt = dbCon.prepareStatement(query);\n        checkStmt.setString(1, question);\n        ResultSet rs = checkStmt.executeQuery();\n        while (rs.next()) {     \n            isDescAvailable = true;\n        }\n    } catch (URISyntaxException e1) {\n        e1.printStackTrace();\n    } catch (SQLException sqle) {\n        sqle.printStackTrace();\n    } catch (Exception e) {\n        if (connection != null)\n            connection.close();\n    } finally {\n        if (connection != null)\n            connection.close();\n    }\n    return isDescAvailable;\n}\n```\n\n\nI want to find the edit distance between both input text and the values that's existing in the database. i want to fetch all datas that has edit distance of 60 percent.  The above query doesnt work as expected. How do I get the rows that contains 60 percent similarity?\n    ", "Answer": "\r\nUse this:\n\n```\nSELECT *\nFROM description\nWHERE 100 * (length(desci) - levenshtein(desci, ?))\n         / length(desci) > 60\n```\n\n\nThe Levenshtein distance is the count of how many letters must change (move, delete or insert) for one string to become the other. Put simply, it's the number of letters that are different.\n\nThe number of letters that are the same is then ```\nlength - levenshtein```\n.\n\nTo express this as a fraction, divide by the length, ie ```\n(length - levenshtein) / length```\n.\n\nTo express a fraction as a percentage, multiply by ```\n100```\n.\n\nI perform the multiplication by ```\n100```\n first to avoid integer division truncation problems.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Unable to follow the intuition behind minimum edit distance\r\n                \r\nSo i just began reading on MED, but am totally unable to follow it.\nSuppose I have to convert \"WATER\" to \"ATERW\"\nNow I can substitute : \n\n```\nW->A, A->T, T->E, E->R, R->W\n```\n\n\nThus total cost = 2+2+2+2+2 =10 (all substitutions)\n\nHowever that isnt correct I know, it should be like this \n\n```\nWATER-\n-ATERW\n```\n\n\nThus total cost here = 1+1 =2 (deletion and insertion)\nBut then my question is that how does the program know that it shouldnt match ```\n'W'->'A'```\n, and rather delete ```\n'W'```\n and match ```\n'ATER'```\n in both the string ?? How is that intuition/logic inculcated into the program ?\n    ", "Answer": "\r\nFirst you should check the wikipedia page about Levenshtein distance: https://en.wikipedia.org/wiki/Levenshtein_distance\n\nIt is just like edit distance, except for the edit cost.\n\nAs you can see, to solve this problem you have to build a matrix (it is a dynamic programing approach). Rows represent the source word, columns represent the target word.\n\nFirst you initialize the matrix with base cases:\n\n\nTo transform the empty string into an empty string you need 0 operations;\nTo transform the empty string into A you need 1 operation (insertion);\nTo transform the empty string into AT you need 2 operations (insertion);\nTo transform the empty string into ATE you need 3 operations (insertion);\nand so on...\nTo transform W into an empty string you need 1 operation (deletion);\nTo transform WA into an empty string you need 2 operations (deletion);\nTo transform WAT into an empty string you need 3 operations (deletion);\nand so on...\n\n\nNow you got your first row and your first column.\n\n```\n  _ A T E R W\n_ 0 1 2 3 4 5\nW 1 ? ? ? ? ?\nA 2 ? ? ? ? ?\nT 3 ? ? ? ? ?\nE 4 ? ? ? ? ?\nR 5 ? ? ? ? ?\n```\n\n\nThe idea is to fill the matrix cell by cell. The first one to fill is gonna be the cell of the second column of the second row (2,2). It corresponds to the letter W of the source word (ie WATER), and to the letter A of the target word (ATERW).\n\nLet's take a look at the value around and let's add an edit cost to each of these values. We'll then pick the minimum.\nFor an insertion (left cell) or a deletion (upper cell) the edit cost is always 1. For a substitution (upper-left cell) the edit cost is 1 if the letters are different, 0 otherwise. \n\nWe have:\n\n\nINSERTION (from cell 2,1): 1 (cell's value) + 1 (edit cost);\nSUBSTITUTION (from cell 1,1): 0 (cell's value) + 1 (edit cost for different letters);\nDELETION (from cell 1,2): 1 (cell's value) + 1 (edit cost).\n\n\nNow pick the minimum value: 1 (SUBSTITUTION). Cell 2,2 as now value 1.\n\n```\n  _ A T E R W\n_ 0 1 2 3 4 5\nW 1 1 ? ? ? ?\nA 2 ? ? ? ? ?\nT 3 ? ? ? ? ?\nE 4 ? ? ? ? ?\nR 5 ? ? ? ? ?\n```\n\n\nNow let's do the same for cell 2,3. It corresponds to the later W of the source word (ie WATER), and to the letter T of the target word (ATERW).\n\nWe have:\n\n\nINSERTION (from cell 2,2): 1 (cell's value) + 1 (edit cost);\nSUBSTITUTION (from cell 1,2): 1 (cell's value) + 1 (edit cost for different letters);\nDELETION (from cell 1,3): 2 (cell's value) + 1 (edit cost).\n\n\nNow pick the minimum value: 2 (INSERTION or SUBSTITUTION). Cell 2,3 as now value 2.\n\nIt means that the cost to convert W into AT is 2.\n\n```\n  _ A T E R W\n_ 0 1 2 3 4 5\nW 1 1 2 ? ? ?\nA 2 ? ? ? ? ?\nT 3 ? ? ? ? ?\nE 4 ? ? ? ? ?\nR 5 ? ? ? ? ?\n```\n\n\nAs you can see we use previous computation (value in cell 2,2) to fill the current cell (2,3). That's the idea of dynamic programming. \n\nRepeat until the matrix is filled. It should looks like this:\n\n```\n  _ A T E R W\n_ 0 1 2 3 4 5\nW 1 1 2 3 4 4\nA 2 1 2 3 4 5\nT 3 2 1 2 3 4\nE 4 3 2 1 2 3\nR 5 4 3 2 1 2\n```\n\n\nTake a look at the very last cell (6,6): value is 2. It corresponds to the cost to transform 'WATER' into 'ATERW'.\n\nSo as to recover the sequence of edit operations performed you can use a backpointer table. Each row gives, for a given cell, the cell from which you picked the minimum value.\n\n```\n2,2 1,1\n2,3 2,2\n2,4 2,3\n2,5 2,4\n2,6 1,5\n3,2 2,1\n...\n6,5 5,4\n6,6 6,5\n```\n\n\nNow you can parse the table backward and build the path, i.e., (6,6) -> (6,5) -> (5,4)... \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Kendall tau distance\r\n                \r\nthe wikipedia article explains the Kendall tau edit distance but doesn't say too much about possible applications. When or for what do you use the tau distance? \nI'm searching for real world examples, not invented examples of mathematics.\n    ", "Answer": "\r\nKendall Tau can be used as a metric to compare similarities between search results. For example, one can compare how close the top-10 results produced by Google and Bing (for the same query) are.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How can I optimize retrieving lowest edit distance from a large table in SQL?\r\n                \r\nI'm having troubles optimizing this Levenshtein Distance calculation I'm doing. I need to do the following:\n\nGet the record with the minimum distance for the source string as well as a trimmed version of the source string\nPick the record with the minimum distance\nIf the min distances are equal (original vs trimmed), choose the trimmed one with the lowest distance\nIf there are still multiple records that fall under the above two categories, pick the one with the highest frequency\n\n\nHere's my working version:\n\n```\nDECLARE @Results TABLE\n(\n    ID int,\n    [Name] nvarchar(200), \n    Distance int, \n    Frequency int, \n    Trimmed bit\n)\n\n\nINSERT INTO @Results\n    SELECT ID, \n           [Name], \n           (dbo.Levenshtein(@Source, [Name])) As Distance,\n           Frequency, \n           'False' As Trimmed\n    FROM\n           MyTable\n\nINSERT INTO @Results\n    SELECT ID, \n           [Name], \n           (dbo.Levenshtein(@SourceTrimmed, [Name])) As Distance,\n           Frequency, \n           'True' As Trimmed\n    FROM\n           MyTable\n\nSET @ResultID = (SELECT TOP 1 ID FROM @Results ORDER BY Distance, Trimmed, Frequency)\nSET @Result = (SELECT TOP 1 [Name] FROM @Results ORDER BY Distance, Trimmed, Frequency)\nSET @ResultDist = (SELECT TOP 1 Distance FROM @Results ORDER BY Distance, Trimmed, Frequency)\nSET @ResultTrimmed = (SELECT TOP 1 Trimmed FROM @Results ORDER BY Distance, Trimmed, Frequency)\n```\n\n\nI believe what I need to do here is to..\n\n\nNot dumb the results to a temporary table\nDo only 1 select from `MyTable`\nSetting the results right in the select from the initial select statement. (Since select will set variables and you can set multiple variables in one select statement)\n\nI know there has to be a good implementation to this but I can't figure it out... this is as far as I got:\n\n```\nSELECT top 1 @ResultID = ID, \n             @Result = [Name], \n            (dbo.Levenshtein(@Source, [Name])) As distOrig,\n             (dbo.Levenshtein(@SourceTrimmed, [Name])) As distTrimmed,\n             Frequency\nFROM\n    MyTable\nWHERE /* ... yeah I'm lost */\nORDER BY distOrig, distTrimmed, Frequency \n```\n\n\nAny ideas?\n    ", "Answer": "\r\nI think your attempt differs from the code that you say works in that the working code orders by distance first, whether or not that is original or trimmed distance. Your attempt orders by original distance first, then trimmed.\n\nI'm not sure I understand what you're trying to do entirely, but does the following do what you need?\n\n```\nSELECT TOP 1\n    @ResultId = ID,\n    @Result = [Name],\n    @ResultDist = distOrig,\n    @ResultTrimmed = distTrimmed\nFROM (\n    SELECT\n        ID, [Name], \n        dbo.Levenshtein(@Source, [Name]) As distOrig,\n        dbo.Levenshtein(@SourceTrimmed, [Name])) As distTrimmed,\n        Frequency\n    FROM MyTable\n) AS T\nORDER BY\n    CASE WHEN distOrig > distTrimmed THEN distOrig ELSE distTrimmed END, -- Distance\n    CASE WHEN distOrig > distTrimmed THEN 1 ELSE 0 END,                  -- Trimmed\n    Frequency                                                            -- Frequency\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Pseudocode for script to check transcription accuracy / edit distances\r\n                \r\nI need to write a script, probably in Ruby, that will take one block of text and compare a number of transcriptions of recordings of that text to the original to check for accuracy.  If that's just completely confusing, I'll try explaining another way...\n\nI have recordings of several different people reading a script that is a few sentences long.  These recordings have all been transcribed back to text a number of times by other people.  I need to take all of the transcriptions (hundreds) and compare them against the original script for accuracy.\n\nI'm having trouble even conceptualising the pseudocode, and wondering if someone can point me in the right direction.  Is there an established algorithm I should be considering?  The Levenshtein distance has been suggested to me, but this seems like it wouldn't cope well with longer strings, considering differences in punctuation choices, whitespace, etc.--missing the first word would wreck the entire algorithm, even if every other word were perfect.  I'm open to anything--thank you!\n\nEdit:\n\nThanks for the tips, psyho.  One of my biggest concerns, however, is a situation like this:\n\nOriginal Text:\n\n```\nI would've taken that course if I'd known it was available!```\n\n\nTranscription\n\n```\nI would have taken that course if I'd known it was available!```\n\n\nEven with a word-wise comparison of tokens, this transcription will be marked as quite errant, even though it's almost perfect, and this is hardly an edge-case!  \"would've\" and \"would have\" are commonly pronounced extremely similarly, especially in this part of the world.  Is there a way to make the approach you suggest robust enough to deal with this?  I've thought about running a word-wise comparison both forward and backward and building a sort of composite score, but this would fall apart with a transcription like this:\n\n```\nI would have taken that course if I had known it was available!```\n\n\nAny ideas?\n    ", "Answer": "\r\nSimple version:\n\n\nTokenize your input into words (convert a string containing words, punctuation, etc. into an array of lowercase words, without punctuation).\nUse the Levenshtein distance (wordwise) to compare the original array with the transcription arrays.\n\n\nPossible improvements:\n\n\nYou could introduce tokens for punctuation (or replace them all with a simple token like '.').\nLevenshtein distance algorithm can be modified so that misspelling a character that with a character that is close on the keyboard generates a smaller distance. You could potentialy apply this, so that when comparing individual words, you would use Levenshtein distance (normalized, so that it's value ranges from 0 to 1, for example by dividing it by the length of the longer of the two words), and then use that value in the \"outer\" distance calculation.\n\n\nIt's hard to say what algorithm will work best with your data. My tip is: make sure you have some automated way of visualizing or testing your solution. This way you can quickly iterate and experiment with your solution and see how your changes affect the end result.\n\nEDIT:\nIn response to your concerns:\n\nThe easiest way would be to start with normalizing the shorter forms (using gsub):\n\n```\nstr.gsub(\"n't\", ' not').gsub(\"'d\", \" had\").gsub(\"'re\", \" are\")\n```\n\n\nNote, that you can even expand \"'s\" to \" is\", even if it's not grammatically correct, because if John's means \"John is\", then you will get it right, and if it means \"owned by John\", then most likely both texts will contain the same form, so you will not further the distance by expanding both \"incorrectly\". The other case is when it should mean \"John has\", but then after \"'s\" there probably will be \"got\", so you can handle that easily as well.\n\nYou will probably also want to deal with numeric values (1st = first, etc.). Generally you can probably improve the result by doing some preprocessing. Don't worry if it's not always 100% correct, it should just be correct enough:)\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Minimal Levenshtein distance fast estimation\r\n                \r\nWe have spellchecker implementation based on Levenshtein distance. As we couldn't calculate distance for all possible substitutions (Levenshtein distance between two string calculated in ```\nO(n^2)```\n) we use K-gram index for retrieving candidates for substitution.\n\nSo K-gram index is just one of the ways for fast eliminating irrelevant substitution. I'm interested in other ways as well. At the moment we used several more tricks. Considering that we are only interested in substitutions of edit distance no more the d from the original string we could use following rules:\n\n\nthe edit distance between two string couldn't be less that length difference between them. So substitutions with length difference greater than d could be eliminated;\none character change/remove in string change at least ```\nk```\n k-grams. So the strings with count difference of k-grams ```\nk * d```\n could not have edit distance less than d: .\n\n\nAre those assumptions correct? And what other ways of substitutions eliminating exists applicable to spellcheckers?\n    ", "Answer": "\r\nyou can use the simple rule to restrict the search to dictionary terms beginning with the same letter as the query string. The hope is that users do not misspell the first letter.\n\nAlso, you can use a permuterm index. consider all rotations of the query and traverse the B tree to find any dictionary terms that match any rotation. You can also  refine this rotation scheme by omitting a suffix of l characters before performing the traversal\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "how to convert python/cython unicode string to array of long integers, to do levenshtein edit distance [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has an answer here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 11 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\n\n  Possible Duplicate:\n  How to correct bugs in this Damerau-Levenshtein implementation?  \n\n\n\n\nI have the following Cython code (adapted from the bpbio project) that does Damerau-Levenenshtein edit-distance calculation:\n\n```\n#---------------------------------------------------------------------------\ncdef extern from \"stdlib.h\":\n  ctypedef unsigned int size_t\n  size_t strlen(char *s)\n  void *malloc(size_t size)\n  void *calloc(size_t n, size_t size)\n  void free(void *ptr)\n  int strcmp(char *a, char *b)\n  char * strcpy(char *a, char *b)\n\n#---------------------------------------------------------------------------\ncdef extern from \"Python.h\":\n  object PyTuple_GET_ITEM(object, int)\n  void Py_INCREF(object)\n\n#---------------------------------------------------------------------------\ncdef inline size_t imin(int a, int b, int c):\n  if a < b:\n    if c < a:\n      return c\n    return a\n  if c < b:\n    return c\n  return b\n\n#---------------------------------------------------------------------------\ncpdef int editdistance( char *a, char *b ):\n  \"\"\"Given two byte strings ``a`` and ``b``, return their absolute Damerau-\n  Levenshtein distance. Each deletion, insertion, substitution, and\n  transposition is counted as one difference, so the edit distance between\n  ``abc`` and ``ab``, ``abcx``, ``abx``, ``acb``, respectively, is ``1``.\"\"\"\n\n  #.........................................................................\n  if strcmp( a, b ) == 0: return 0\n  #.........................................................................\n  cdef int    alen    = strlen( a )\n  cdef int    blen    = strlen( b )\n  cdef int    R\n  cdef char   *ctmp\n  cdef size_t i\n  cdef size_t j\n  cdef size_t achr\n  cdef size_t bchr\n  #.........................................................................\n  if alen > blen:\n    ctmp = a;\n    a = b;\n    b = ctmp;\n    alen, blen = blen, alen\n  #.........................................................................\n  cdef char   *m1     = <char *>calloc(   blen + 2,    sizeof( char ) )\n  cdef char   *m2     = <char *>calloc(   blen + 2,    sizeof( char ) )\n  cdef char   *m3     = <char *>malloc( ( blen + 2 ) * sizeof( char ) )\n  #.........................................................................\n  for i from 0 <= i <= blen:\n    m2[ i ] = i\n  #.........................................................................\n  for i from 1 <= i <= alen:\n    m1[ 0 ] =    i + 1\n    achr    = a[ i - 1 ]\n    for j from 1 <= j <= blen:\n      bchr = b[ j- 1 ]\n      if achr == bchr:\n        m1[ j ] = m2[ j - 1 ]\n      else:\n        m1[ j ] = 1 + imin( m1[ j - 1 ], m2[ j - 1 ], m2[ j ] )\n      if i != 1 and j != 1 and achr == b[ j - 2 ] and bchr == a[ i - 2 ]:\n        m1[ j ] = m3[ j - 1 ]\n    #.......................................................................\n    m1, m2 = m2, m1\n    strcpy( m3, m2 )\n  #.........................................................................\n  R = <int>m2[ blen ]\n  #.........................................................................\n  # cleanup:\n  free( m3 )\n  free( m1 )\n  free( m2 )\n  #.........................................................................\n  return R\n```\n\n\nThe code runs fine and fast (300,000...400,000 comparisons per second on my PC).\n\nthe challenge is to make this code work with unicode strings as well. i am running Python 3.1 and retrieve texts from a database that are then matched to a query text. \n\nencoding these strings to ```\nbytes```\n before passing them to the Cython function for comparison is not be a good idea, since performance would suffer considerably (tested) and results would likely be wrong for any text containing characters outside of 7bit US ASCII. \n\nthe (very terse) Cython manual does mention unicode strings, but is hardly helpful for the problem at hand. \n\nas i see it, a unicode string can be conceived of as an array of integer number, each representing a single codepoint, and the code above is basically operating on arrays of ```\nchar```\ns already, so my guess is that i should (1) extend it to handle C arrays of integers; (2) add code to convert a python unicode string to a C array; (3) profit!. \n\n( Note: there are two potential issues with this approach: one is handling unicode surrogate characters, but i guess i know what to do with those. the other problem is that unicode codepoints do not really map 1:1 to the concept of 'characters'. i am well aware of that but i consider it outside of the scope of this question. please assume that one unicode codepoint is one unit of comparison.)\n\nso i am asking for suggestions how to \n\n\nwrite a fast Cython function that accepts a python unicode string and returns a C array of Cython ```\nunsigned int```\ns (4 bytes);\nmodify the code shown to handle those arrays and do the correct memory allocations / deallocations (this is pretty foreign stuff to me).\n\n\nEdit: John Machin has pointed out that the curious typecasts ```\nchar *m1```\n etc are probably done for speed and/or memory optimization; these variables are still treated as arrays of numbers. i realize that the code does nothing to prevent a possible overflow with long strings; erroneous results may occur when one array element exceeds 127 or 255 (depending on the C compiler used). sort of surprising for code coming from a bioinformatics project.\n\nthat said, i am only interested in precise results for largely identical strings of less than say a hundred characters or so. results below 60% sameness could for my purposes be safely reported as 'completely different' (by returning the length of the longer text), so i guess it will be best to leave the ```\nchar *m1```\n casts in place, but add some code to check against overflow and early abortion in case of rampant dissimilarity. \n    ", "Answer": "\r\nUse ```\nord()```\n to convert characters to their integer code point.  It works characters from either ```\nunicode```\n or ```\nstr```\n string types:\n\n```\ncodepoints = [ord(c) for c in text]\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Increasing Label Error Rate (Edit Distance) and Fluctuating Loss?\r\n                \r\nI am training a handwriting recognition model of this architecture: \n\n```\n{\n\"network\": [\n{\n\"layer_type\": \"l2_normalize\"\n},\n{\n\"layer_type\": \"conv2d\",\n\"num_filters\": 16,\n\"kernel_size\": 5,\n\"stride\": 1,\n\"padding\": \"same\"\n},\n{\n\"layer_type\": \"max_pool2d\",\n\"pool_size\": 2,\n\"stride\": 2,\n\"padding\": \"same\"\n},\n{\n\"layer_type\": \"l2_normalize\"\n},\n{\n\"layer_type\": \"dropout\",\n\"keep_prob\": 0.5\n},\n{\n\"layer_type\": \"conv2d\",\n\"num_filters\": 32,\n\"kernel_size\": 5,\n\"stride\": 1,\n\"padding\": \"same\"\n},\n{\n\"layer_type\": \"max_pool2d\",\n\"pool_size\": 2,\n\"stride\": 2,\n\"padding\": \"same\"\n},\n{\n\"layer_type\": \"l2_normalize\"\n},\n{\n\"layer_type\": \"dropout\",\n\"keep_prob\": 0.5\n},\n{\n\"layer_type\": \"conv2d\",\n\"num_filters\": 64,\n\"kernel_size\": 5,\n\"stride\": 1,\n\"padding\": \"same\"\n},\n{\n\"layer_type\": \"max_pool2d\",\n\"pool_size\": 2,\n\"stride\": 2,\n\"padding\": \"same\"\n},\n{\n\"layer_type\": \"l2_normalize\"\n},\n{\n\"layer_type\": \"dropout\",\n\"keep_prob\": 0.5\n},\n{\n\"layer_type\": \"conv2d\",\n\"num_filters\": 128,\n\"kernel_size\": 5,\n\"stride\": 1,\n\"padding\": \"same\"\n},\n{\n\"layer_type\": \"max_pool2d\",\n\"pool_size\": 2,\n\"stride\": 2,\n\"padding\": \"same\"\n},\n{\n\"layer_type\": \"l2_normalize\"\n},\n{\n\"layer_type\": \"dropout\",\n\"keep_prob\": 0.5\n},\n{\n\"layer_type\": \"conv2d\",\n\"num_filters\": 256,\n\"kernel_size\": 5,\n\"stride\": 1,\n\"padding\": \"same\"\n},\n{\n\"layer_type\": \"max_pool2d\",\n\"pool_size\": 2,\n\"stride\": 2,\n\"padding\": \"same\"\n},\n{\n\"layer_type\": \"l2_normalize\"\n},\n{\n\"layer_type\": \"dropout\",\n\"keep_prob\": 0.5\n},\n{\n\"layer_type\": \"collapse_to_rnn_dims\"\n},\n{\n\"layer_type\": \"birnn\",\n\"num_hidden\": 128,\n\"cell_type\": \"LSTM\",\n\"activation\": \"tanh\"\n}\n],\n\"output_layer\": \"ctc_decoder\"\n}\n```\n\n\nThe training ctc loss suddenly drops on the first training epoch but it plateaus fluctuates for the rest of the epochs. The label error rate not only fluctuates but it doesn't really seem to go lower.\n\n\n\nI should mention that the sequence length of each sample is really close to the length of the longest ground truth (i.e. from 1024, it becomes 32 by the time it enters the ctc_loss which is close to the longest ground truth length of 21).\n\nAs for the preprocessing of images, I made sure that they the aspect ratio is maintained when I resize it, and right padded the image to make it a square so that all the images will have the width and the handwritten words will be on the left. I also inverted the color of the images such that the handwritten characters have the highest pixel value (255) and the background while the rest have the lowest pixel value (0).\n\n\n\nThe predictions are something like this. A random set of strings on the first part then a bunch of zeroes at the end (which is probably expected because of the padding).\n\n```\nINFO:tensorflow:outputs = [[59 45 59 45 59 55 59 55 59 45 59 55 59 55 59 55 45 59  8 59 55 45 55  8\n  45  8 45 59 45  8 59  8 45 59 45  8 45 19 55 45 55 45 55 59 45 59 45  8\n  45  8 45 55  8 45  8 45 59 45 55 59 55 59  8 55 59  8 45  8 45  8 59  8\n  59 45 59 45 59 45 59 45 59 45 59 45 19 45 55 45 22 45 55 45 55  8 45  8\n  59 45 59 45 59 45 59 55  8 45 59 45 59 45 59 45 19 45 59 45 19 59 55 24\n   4 52 54 55]]\n```\n\n\nHere's how I collapse cnn outputs to rnn dims:\n\n```\ndef collapse_to_rnn_dims(inputs):\n    batch_size, height, width, num_channels = inputs.get_shape().as_list()\n    if batch_size is None:\n        batch_size = -1\n    time_major_inputs = tf.transpose(inputs, (2, 0, 1, 3))\n    reshaped_time_major_inputs = tf.reshape(time_major_inputs,\n                                            [width, batch_size, height * num_channels]\n                                            )\n    batch_major_inputs = tf.transpose(reshaped_time_major_inputs, (1, 0, 2))\n    return batch_major_inputs\n```\n\n\nAnd here's how I collapse rnn to ctc dims:\n\n```\ndef convert_to_ctc_dims(inputs, num_classes, num_steps, num_outputs):\n    outputs = tf.reshape(inputs, [-1, num_outputs])\n    logits = slim.fully_connected(outputs, num_classes,\n                                  weights_initializer=slim.xavier_initializer())\n    logits = slim.fully_connected(logits, num_classes,\n                                  weights_initializer=slim.xavier_initializer())\n    logits = tf.reshape(logits, [num_steps, -1, num_classes])\n    return logits\n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "lavinshtein distance with dictionary [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 5 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nHow to advance edit distance with operation take an anagram of the existing word. every interim step must be a word from a list of words .\n    ", "Answer": "\r\nThe standard technique for anagrams is to store words in canonical sorted order, e.g. \"Banana\" becomes \"aaabnn\". Do that for all valid words, then consider Levenshtein distances between those. You will want to map from canonical to a valid set, e.g. ```\nvalid['dgo'] = {'dog', 'god'}```\n\n\nTake a look at ```\ntail /usr/share/dict/words```\n if you need a set of valid words.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Reverse Levenshtein distance\r\n                \r\nIn levenshtein distance you ask the question, given these two strings, what is their levenshtein distance. How would you go about taking a string and a levenshtein distance and generating all the strings within that levenshtein distance. (It would also take in a character set). So if i pass in a string x and a distance d. then it would give me all the strings within that edit distance, including d-1 and d-2....d-n; (n < d).\n\nExpected functionality:\n\n```\n>>> getWithinDistance('apple',2,{'a','b',' '})\n['applea','appleb','appel','app le'...]\n```\n\n\nPlease note that the program is able to produce ```\napp le```\n as space is included in the character set.\n    ", "Answer": "\r\nThere's a data structure that does this called the Levenshtein automaton. You construct it from a set of strings (which may have only one member) and a fixed distance k, and then you can query it for all strings with distance at most k of any of the strings it stores. A Python implementation is discussed here.\n\nAlternatively, you can do a depth-limited search with backtracking for such strings.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to write nested loops in idiomatic Scala?\r\n                \r\nThis is my pseudo code, obviously this is not idiomatic scala as I use a lot of mutable variables and loops. \n\nMy goal is to transform this into idiomatic scala in a functional way. \n\nThe objective of the algorithm is that given a list of strings, do a N^2 comparison on itself to find matches using edit distance. Because edit distance checks are example, I want to avoid doing edit distance checks on string pairs that have been checked.\n\n```\ndef pairwisecomp(block: Iterable[String]): List[(String, String)] = {\n    var matches = new ListBuffer[(String, String)]\n\n    // pseudo code; not intended to be valid scala\n    for (var i = 0; i < block.size; i++) {\n       val str1 = block[i]\n       // iterate from i because no need to go backwards to compare\n       for (var j = i+1; j < block.size; j++) {\n           val str2 = block[j]\n           // check edit distance\n           // if less than 2, append into the matches\n           if (editDistance(str1, str2) < 2) {\n              matches.append((str1, str2))\n           }\n       }\n    }\n\n\n    return matches.toList\n  }\n```\n\n    ", "Answer": "\r\nI assume it can be shortened though, I think the code blow is doing exactly the same as you wrote.\n\n```\nblock.flatMap{ str1 =>\n  block.zipWithIndex.filter{ case (elem,index) => index > i}.map{ case (str2,index) =>\n    ((str1,str2),editDistance(str1,str2) < 2)\n  }.filter{_.2}.map{_1}\n}.toList\n```\n\n\nEDIT: \nIt maybe simpler in for-comprehensions.\n\n```\nval b = block.zipWithIndex\n(for {\n  (str1,outerIndex) <- b\n  (str2,innerIndex) <- b if innerIndex > outerIndex && editDistance(str1,str2) < 2\n} yield (str1,str2)).toList\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein Distance: Inferring the edit operations from the matrix\r\n                \r\nI wrote Levenshtein algorithm in in C++\n\nIf I input:\nstring s: democrat\nstring t: republican\n\nI get the matrix D filled-up and the number of operations (the Levenshtein distance) can be read in D[10][8] = 8\nBeyond the filled matrix I want to construct the optimal solution. How must look this solution? I don't have an idea.\nPlease only write me HOW MUST LOOK for this example.\n    ", "Answer": "\r\nThe question is\nGiven the matrix produced by the Levenshtein algorithm, how can one find \"the optimal solution\"?\ni.e. how can we find the precise sequence of string operations: inserts, deletes and substitution [of a single letter], necessary to convert the 's string' into the 't string'?\n\nFirst, it should be noted that in many cases there are SEVERAL optimal solutions. While the Levenshtein algorithm supplies the minimum number of operations (8 in democrat/republican example) there are many sequences (of 8 operations) which can produce this conversion.\n\nBy \"decoding\" the Levenshtein matrix, one can enumerate ALL such optimal sequences.\nThe general idea is that the optimal solutions all follow a \"path\", from top left corner to bottom right corner (or in the other direction), whereby the matrix cell values on this path either remain the same or increase by one (or decrease by one in the reverse direction),  starting at 0 and ending at the optimal number of operations for the strings in question (0 thru 8 democrat/republican case).   The number increases when an operation is necessary, it stays the same when the letter at corresponding positions in the strings are the same.\n\nIt is easy to produce an algorithm which produces such a path (slightly more complicated to produce all possible paths), and from such path deduce the sequence of operations.\n\nThis path finding algorithm should start at the lower right corner and work its way backward.  The reason for this approach is that we know for a fact that to be an optimal solution it must end in this corner, and to end in this corner, it must have come from one of the 3 cells either immediately to its left, immediately above it or immediately diagonally.  By selecting a cell among these three cells, one which satisfies our \"same value or decreasing by one\" requirement, we effectively pick a cell on one of the optimal paths.  By repeating the operation till we get on upper left corner (or indeed until we reach a cell with a 0 value), we effectively backtrack our way on an optimal path.\n\nIllustration with the democrat - republican example\n\nIt should also be noted that one can build the matrix in one of two ways: with 'democrat' horizontally or vertically.  This doesn't change the computation of the Levenshtein distance nor does it change the list of operations needed; it only changes the way we interpret the matrix, for example moving horizontally on the \"path\" either means inserting a character [from the t string] or deleting a character [off the s string] depending whether 'string s' is \"horizontal\" or \"vertical\" in the matrix.\n\nI'll use the following matrix. The conventions are therefore (only going in the left-to-right and/or top-to-bottom directions)\n\n\nan horizontal move is an INSERTION of a letter from the 't string'\nan vertical move is a DELETION of a letter from the 's string'\na diagonal move is either:\n\n\na no-operation   (both letters at respective positions are the same); the number doesn't change\na SUBSTITUTION   (letters at respective positions are    distinct);  the number increase by one.\n\n\n\nLevenshtein matrix for s = \"democrat\", t=\"republican\"\n\n```\n      r  e  p  u  b  l  i  c  a  n\n   0  1  2  3  4  5  6  7  8  9  10\nd  1  1  2  3  4  5  6  7  8  9  10\ne  2  2  1  2  3  4  5  6  7  8  9\nm  3  3  2  2  3  4  5  6  7  8  9\no  4  4  3  3  3  4  5  6  7  8  9\nc  5  5  4  4  4  4  5  6  6  7  8\nr  6  5  5  5  5  5  5  6  7  7  8\na  7  6  6  6  6  6  6  6  7  7  8\nt  8  7  7  7  7  7  7  7  7  8  8\n```\n\n\nThe arbitrary approach I use to select one path among several possible optimal paths is loosely described below:\n\n```\nStarting at the bottom-rightmost cell, and working our way backward toward \nthe top left.\n\nFor each \"backward\" step, consider the 3 cells directly adjacent to the current\ncell   (in the left, top or left+top directions)\n\n   if the value in the diagonal cell (going up+left) is smaller or equal to the\n      values found in the other two cells\n   AND \n      if this is same or 1 minus the value of the current cell \n   then  \"take the diagonal cell\"\n         if the value of the diagonal cell is one less than the current cell:\n            Add a SUBSTITUTION operation (from the letters corresponding to\n            the _current_ cell)\n         otherwise: do not add an operation this was a no-operation.\n\n   elseif the value in the cell to the left is smaller of equal to the value of\n       the of the cell above current cell\n   AND \n       if this value is same or 1 minus the value of the current cell \n   then \"take the cell to left\", and\n        add an INSERTION of the letter corresponding to the cell\n   else\n       take the cell above, add\n       Add a DELETION operation of the letter in 's string'\n```\n\n\nFollowing this informal pseudo-code, we get the following:\n\nStart on the \"n\", \"t\" cell at bottom right.\nPick the [diagonal] \"a\", \"a\" cell as next destination  since it is less than the other two (and satisfies the same or -1 condition).\nNote that the new cell is one less than current cell\ntherefore the step 8 is substitute \"t\" with \"n\":     democra N\n\nContinue with \"a\", \"a\" cell,\n    Pick the [diagonal] \"c\", \"r\" cell as next destination...\n    Note that the new cell is same value as current cell  ==> no operation needed.\n\nContinue with \"c\", \"r\" cell,\n   Pick the [diagonal] \"i\", \"c\" cell as next destination...\n   Note that the new cell is one less than current cell\n     therefore the step 7 is substitute \"r\" with \"c\":     democ C an\n\nContinue with \"i\", \"c\" cell,\n   Pick the [diagonal] \"l\", \"o\" cell as next destination...\n   Note that the new cell is one less than current cell\n     therefore the step 6 is substitute \"c\" with \"i\":     demo I can\n\nContinue with \"l\", \"o\" cell,\n   Pick the [diagonal] \"b\", \"m\" cell as next destination...\n   Note that the new cell is one less than current cell\n     therefore the step 5 is substitute \"o\" with \"l\":     dem L ican\n\nContinue with \"b\", \"m\" cell,\n   Pick the [diagonal]\"u\", \"e\" cell as next destination...\n   Note that the new cell is one less than current cell\n     therefore the step 4 is substitute \"m\" with \"b\":     de B lican\n\nContinue with \"u\", \"e\" cell,\n   Note the \"diagonal\" cell doesn't qualify, because the  \"left\" cell is less than it.\n   Pick the [left] \"p\", \"e\" cell as next destination...\n        therefore the step 3 is instert \"u\" after  \"e\":      de U blican\n\nContinue with \"p\", \"e\" cell,\n   again the \"diagonal\" cell doesn't qualify\n   Pick the [left] \"e\", \"e\" cell as next destination...\n        therefore the step 2 is instert \"p\" after  \"e\":     de P ublican\n\nContinue with \"e\", \"e\" cell,\n   Pick the [diagonal] \"r\", \"d\" cell as next destination...\n   Note that the new cell is same value as current cell  ==> no operation needed.\n\nContinue with \"r\", \"d\" cell,\n   Pick the [diagonal] \"start\" cell as next destination...\n   Note that the new cell is one less than current cell\n     therefore the step 1 is substitute \"d\" with \"r\":     R epublican\n\nYou've arrived at a cell which value is 0 : your work is done!\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "What would be the best implementation of the __hash__ function if the __eq__ funciton determines equality using edit distance?\r\n                \r\nI have a weird requirement where I need to find common \"Customers\" from two different and very large lists. Each entry in both lists is a Customer object that contains the first and last names of the customer and his address (broken down by address lines, like address_line1, address_line2 etc). The issue is that there is a possibility that the data in either list could be incomplete, for example, for one of the records in the first list, the customer's first name could be missing whereas in the second list, for the same customer, his address (line 2 and line 3) could be missing. What I need to do is find customers present in both lists. A point to note is that the lists can be large. Another point to remember is that the names and addresses could semantically be the same but may not return a result when you do an exact String match. For example, in the first list, the address of a customer in the first list could be of the form ```\nB-502 ABC Street```\n whereas the address of the same customer in the second list could be in the form ```\nB 502 ABC Street```\n. The reason I am using edit distance is to account for user input errors in the list and to handle certain other minor differences in the data present in both lists\n\nWhat I did was to implement the eq function in the Customer class as follows\n\n```\nimport re\nimport editdistance # Using this: https://pypi.python.org/pypi/editdistance\n\nclass Customer:\n    def __init__(self, fname, lname, address1, address2, address3, city):\n        # Removing special characters from all arguments and converting them to lower case\n        self.fname = re.sub(\"[^a-zA-Z0-9]\", \"\", fname.lower())\n        self.lname = re.sub(\"[^a-zA-Z0-9]\", \"\", lname.lower())\n        self.address1 = re.sub(\"[^a-zA-Z0-9]\", \"\", address1.lower())\n        self.address2 = re.sub(\"[^a-zA-Z0-9]\", \"\", address2.lower())\n        self.address3 = re.sub(\"[^a-zA-Z0-9]\", \"\", address3.lower())\n        self.city = re.sub(\"[^a-zA-Z0-9]\", \"\", city.lower())\n\n    def __eq__(self, other):\n        if self.lname == \"\" or self.lname != other.lname:\n            return False\n\n        t = 0\n\n        if self.fname != \"\" and other.fname != \"\" and self.fname[0] == other.fname[0]:\n            t += 1\n\n        if editdistance.eval(self.fname, other.fname) <= 2:\n            t += 3\n\n        if editdistance.eval(self.address1, other.address1) <= 3:\n            t += 1\n\n        if editdistance.eval(self.address2, other.address2) <= 3:\n            t += 1\n\n        if editdistance.eval(self.address3, other.address3) <= 3:\n            t += 1\n\n        if editdistance.eval(self.city, other.city) <= 2:\n            t += 1\n\n        if t >= 4:\n            return True\n\n        return False\n\n    def __hash__():\n        # TODO:  Have a robust implementation of a hash function here. If two objects are \"equal\", their hashes should be the same\n```\n\n\nIn order to get the customers present in both lists, I would be doing the following: \n\n```\nset(first_list).intersection(set(second_list))```\n\n\nBut in order for this to work, the Customer object needs to be hashable.\n\nCould someone help me out with a good hashing mechanism?\n    ", "Answer": "\r\nYour only option is normalize data. If you need to compare equality and you may hace different formats the solution is normalization. Transform everything so it will be in the same format in both lists.\n\nI've worked for months in a normalization algorithm for addresses in Spain. The combination of different user inputs for the same address is endless (I was working on a 7 million rows database). Using that distance function may not be accurate enough unless you know exactly the different possible formats for a same address and the distance returned form the function for those differences.\n\nThe first key question would be, what's the percentage of error you can afford? Because with user input and big data you will always have some.\n\nNext step would be to measure the percentage of error you will get with that distance algorithm (or any other algorithm). Choose the sample data carefully, so that percent won't vary with full data.\n\nIf that percentage suits you use that algorithm, if not, find other algorithms and measure them.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Implementing edit distance method using recursion results in object heap error\r\n                \r\n```\n    private static int editDistance(ArrayList<String> s1, ArrayList<String> s2) {\n        if (s1.size()==0) {\n            return s2.size();\n        } \n        else if (s2.size()==0) {\n            return s1.size();\n        } \n        else {\n            String temp1 = s1.remove(s1.size()-1);\n            String temp2 = s2.remove(s2.size()-1);\n            if (temp1.equals(temp2)) {\n                return editDistance((ArrayList<String>)s1.clone(),(ArrayList<String>)s2.clone());\n            } else {\n                s1.add(temp1);\n                int first = editDistance((ArrayList<String>)s1.clone(),(ArrayList<String>)s2.clone())+1;\n                s2.add(temp2);\n                s1.remove(s1.size()-1);\n                int second = editDistance((ArrayList<String>)s1.clone(),(ArrayList<String>)s2.clone())+1;\n                s2.remove(s2.size()-1);\n                int third = editDistance((ArrayList<String>)s1.clone(),(ArrayList<String>)s2.clone())+1;\n                if (first <= second && first <= third ) {\n                    return first;\n                } else if (second <= first && second <= third) {\n                    return second;\n                } else {\n                    return third;\n                }\n            }\n        }\n    }\n```\n\n\nFor example, the input can be ```\n[\"div\",\"table\",\"tr\",\"td\",\"a\"]```\n and ```\n[\"table\",\"tr\",\"td\",\"a\",\"strong\"]```\n and the corresponding output should be ```\n2```\n.\n\nMy problem is when either input list has a size too big, e.g., 40 strings in the list, the program will generate a ```\ncan't reserve enough space for object heap```\n error. The JVM parameters are ```\n-Xms512m -Xmx512m```\n. Could my code need so much heap space? Or it is due to logical bugs in my code?\n\nEdit: With or without cloning the list, this recursive approach does not seem to work either way. Could someone please help estimate the total heap memory it requires to work for me? I assume it would be shocking. Anyway, I guess I have to turn to the dynamic programming approach instead.\n    ", "Answer": "\r\nYou ```\nclone()```\n each ```\nArrayList```\n instance before each recursive call of your method. That essentially means that you get yet another copy of the whole list and its contents for each call - it can easily add-up to a very large amount of memory for large recursion depths.\n\nYou should consider using ```\nList#sublist()```\n instead of ```\nclone()```\n, or even adding parameters to your method to pass down indexes towards a single set of initial ```\nList```\n objects.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to edit the finger swipe's minimum distance gesture to UIScrollView start scrolling?\r\n                \r\nHow to edit the minimum distance range that i have to swipe the finger on  device to UIScrollView start scrolling? In my case, I am using a horizontal scrolling to change between Views that act like pages. I want to decrease the minimum value because a kind of ugly delay is happening when scrolling (the scroll delays to start). I can provide more information if necessary. Thanks in advance.\n\nThe swipe gesture that i mean\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit distance for a four-digit sequential ranking in R? (stringdist)\r\n                \r\nRight now, I am trying to create scale scores for participants who ranked four job candidates (A, B, C, and D) to a role from best fit to worst fit. The correct order is A, D, C, B. As far as my dataframe goes, the correct sequence for columns A, B, C, and D should therefore be 1, 4, 3, 2. Below is a sample of my dataframe with \"Edit_score\" representing what I think is degree of correctness, i.e. the degree to which the values in Concatted resemble \"1432\". I used ```\nstringdist```\n in the following code to produce this column:\n```\ndata$edit_score <- stringdist(\"1432\", data$Concatted, method = \"jw\")```\n\nI am not sure if the Jaro-Winkler method is the most appropriate for this type of variable. Should I be using a different ```\nstringdist```\n method? Is ```\nstringdist```\n the function I should be using to calculate this? I am trying to take into account both placement and sequence and really just need to assign scores to Concatted valued based on how closely they resemble the sequence \"1432\".\n\n\n\n\nA\nB\nC\nD\nConcatted\nEdit_score\n\n\n\n\n4\n2\n3\n1\n4231\n0.33333333\n\n\n1\n2\n4\n3\n1243\n0.16666667\n\n\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Most effective way to calculate Edit distance between two lists?\r\n                \r\nI have two files one is CSV and other one is a text file. Both of them contains Unicode words. My task is to compare words from these two files to correct spelling mistakes.(CSV file contains miss spelled words and text file contains correct words) CSV file contains around 1000 words and text file contains 5000 words.\nI have Implemented following code for this task and since I'm new to python it is very inefficient. What are the suggestion to make it more efficient.\n```\nimport pandas as pd\nimport nltk\n \ndf = pd.read_csv('C:/mis_spel.csv',encoding='utf8')\nlist_check_words = df['words'].tolist()\n\ndf2 = pd.read_csv('C:/words.txt',encoding='utf8',delimiter='\\t')\nlist_words = df2['word'].tolist()\n \nfor word in list_check_words:\n    for dix in list_words:\n        ed = nltk.edit_distance(word, dix)\n        if (ed<2):\n            print(word, dix, ed)\n```\n\n    ", "Answer": "\r\nThis might be an overkill for your use-case, but still I'm putting it here anyways. AFAIK, these days the industry standard for spelling auto-correction involves looking at the problem through the lens of word embeddings. In older times, an n-gram based probabilistic approach was being used, but not any more.\nWhat you'd want to do, is probably something like the following:\n\nTrain a model to produce character-level word embeddings\nProject the entire dictionary to your vector space and build an index for efficient search\nFor each misspelled word, pair it with its nearest neighbor.\n\nI'm adding reference to two different articles below, which explain this in much greater detail. One suggestion though, please try exploring ANNOY indexing from gensim, it's crazy fast for approx nearest neighbors search. Speaking from personal experience.\n\narticle 1: Embedding for spelling correction\narticle 2: Spelling Correction Using Deep Learning: How Bi-Directional LSTM with Attention Flow works in Spelling Correction\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to normalize Levenshtein distance between 0 to 1\r\n                \r\nI have to normalize the Levenshtein distance between 0 to 1. I see different variations floating in SO.\nI am thinking to adopt the following approach:\n\nif two strings, s1 and s2\nlen = max(s1.length(), s2.length());\nnormalized_distance = float(len - levenshteinDistance(s1, s2)) / float(len);\n\nThen the highest score 1.0 means an exact match and 0.0 means no match.\nBut I see variations here:\ntwo whole texts similarity using levenshtein distance where  1- distance(a,b)/max(a.length, b.length)\nDifference in normalization of Levenshtein (edit) distance?\nExplanation of normalized edit distance formula\nI am wondering is there a canonical code implementation in Java? I know ```\norg.apache.commons.text```\n only implements LevenshteinDistance and not normalized LevenshteinDistance.\nhttps://commons.apache.org/proper/commons-text/apidocs/org/apache/commons/text/similarity/LevenshteinDistance.html\n    ", "Answer": "\r\nYour first answer begins with \"The effects of both variants should be nearly the same\". The reason normalized LevenshteinDistance doesn't exist is because you (or somebody else) hasn't seen fit to implement it. Besides, it seems a rather trivial once you have the Levenshtein distance:\n```\nprivate double normalizedLevenshteinDistance(double levenshtein, String s1, String s2) {\n    if (s1.length() >= s2.length()) {\n        return levenshtein / s1.length();\n    }\n    else {\n        return levenshtein / s2.length();\n    }\n}\n```\n\nAfter 3 days, once this has been thoroughly ripped to shreds, I'll add it as a Github issue on commons-text.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Modifying Levenshtein Distance for positional Bias\r\n                \r\nI am using the Levenshtein distance algorithm to compare a company name provided as a user input against a database of known company names to find closest match. By itself, the algorithm works okay, but I want to build in a Bias so that the edit distance is considered lower if the initial parts of the strings match.\n\nFor Example, if the search criteria is \"ABCD\", then both \"ABCD Co.\" and \"XYX ABCD\" have identical Edit Distance. However I want to add weight to the fact that the initial parts of the first string matches the search criteria more closely than the second string.\n\nOne way of doing this might be to modify the insert/delete/replace costs to be higher at the beginning of the strings and lower towards the end. Does anyone have an example of a successful implementation of this? Is using Levenshtein distance still the best way to do what I am trying to achieve? Is my assumption of the approach accurate?\n\nUPDATE: For my immediate purposes I have decided to forgo the above and instead use the Jaro Winkler edit distance which seems to solve the problem. However I will leave this open for further inputs.\n    ", "Answer": "\r\nWhat you're looking for looks like a Smith-Waterman local alignment: http://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How do I use a Trie for spell checking\r\n                \r\nI have a trie that I've built from a dictionary of words. I want to use this for spell checking( and suggest closest matches in the dictionary , maybe for a given number of edits x). I'm thinking I'd use levenshtein distance between the target word and words in my dictionary, but is there a smart way to traverse the trie without actually running the edit distance logic over each word separately? How should I do the traversal and the edit distance matching? \n\nFor e.g, if I have words MAN, MANE, I should be able to reuse the edit distance computation on MAN in MANE. Otherwise the Trie wouldnt serve any purpose\n    ", "Answer": "\r\nI think you should instead give a try to bk-trees; it's a data structure that fits well spell-checking as it will allow you to compute efficiently the edit distance with the words of your dictionary.  \n\nThis link gives a good insight into BK-trees applied to spell-checking\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How do I use a Trie for spell checking\r\n                \r\nI have a trie that I've built from a dictionary of words. I want to use this for spell checking( and suggest closest matches in the dictionary , maybe for a given number of edits x). I'm thinking I'd use levenshtein distance between the target word and words in my dictionary, but is there a smart way to traverse the trie without actually running the edit distance logic over each word separately? How should I do the traversal and the edit distance matching? \n\nFor e.g, if I have words MAN, MANE, I should be able to reuse the edit distance computation on MAN in MANE. Otherwise the Trie wouldnt serve any purpose\n    ", "Answer": "\r\nI think you should instead give a try to bk-trees; it's a data structure that fits well spell-checking as it will allow you to compute efficiently the edit distance with the words of your dictionary.  \n\nThis link gives a good insight into BK-trees applied to spell-checking\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to optimize levenstien edit distance on pandas dataframe using python?\r\n                \r\nI am running levenstein comparison on 50k records. I need to compare each record between each other. Is there a way how to optimize the following code to run it faster? The data is stored in pandas dataframe.\n\n```\nimport pandas as pd\nimport numpy as np\nimport Levenshtein    \n\ndf_s_sorted = df.sort_values(['nonascii_2', 'birth_date'])\n    df_similarity = pd.DataFrame()\n    q=0\n    for index, p in df_s_sorted.iterrows():\n        q = q + 1\n        print(q)\n        for index, p1 in df_s_sorted.iterrows():\n             if ((p[\"birth_date\"] == p1[\"birth_date\"]) & (p[\"name\"] != p1[\"name\"] )):\n                    if (Levenshtein.distance(p[\"name\"],p1[\"name\"]) == 1):\n                        df_similarity = df_similarity.append(p)\n                        print(p)\n        df_s_sorted.drop(index, inplace=True)\n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Redshift: Any ways to compute fuzzy string similarity / string edit distance?\r\n                \r\nIn PSQL (which I believe Redshift is based), there are string similarity functions like ```\nlevenshtein```\n / ```\nlevenshtein_less_equal```\n [ http://www.postgresql.org/docs/9.1/static/fuzzystrmatch.html ]. These features don't seem to have made it into Redshift [ http://docs.aws.amazon.com/redshift/latest/dg/String_functions_header.html ]. Am I wrong, or has anyone made a creative query to work around this limitation?\n    ", "Answer": "\r\nRedshift is based on Postgres 8.0, which is extremely old (and no longer supported), so they've diverged substantially over the years. The development version of Postgres is currently 9.4, and 8.x and 9.x versions of Postgres have some substantial differences and additions in 9.0 and up.\n\nThe ```\nlevenshtein```\n function is part of the ```\nfuzzystrmatch```\n module that you linked above, and that module was introduced in Postgres 8.3, which is likely why it didn't make the cut for Redshift (and it apparently has not since been added). \n\nNormally I would say your best bet would be to define a custom PL/pgSQL function to implement the Levenshtein Distance algorithm, but according to the Redshift doc, User-defined functions and stored procedures are one of the many major features of Postgres that Redshift does not support, so I think you're unfortunately out of luck on that front.\n\nDepending on your requirements, you may be able to use ```\nLIKE```\n to achieve similar results. See this SO answer for more info on that (note that some of the alternative suggestions in that answer, such as full text are also not supported in Redshift).\n\nUpdate, 2016-04-25\n\nIt appears as though since I originally answered this question back in Oct. 2014, an ability to create Python-based User Defined Functions (UDFs) has been added. This is not as ideal as being able to create a custom Postgres function inline (the doc lists an assortment of caveats for the UDFs), but should allow the levenshtein distance algorithm to be implemented in Python and processed within the context of a Redshift query.\n\nUDFs in Apache Hive, a data warehousing project used in the Hadoop ecosystem, allows for user-defined functions in a similar manner (Java or Python based).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Computing edit distance using two simple columns from iris dataset\r\n                \r\nIn the following code below, I want to compute similarity between two columns of text strings.To achieve this, I take first 10 rows of \"Petal.Length\" column from iris and assign it to a1 , and first 4 rows from \"Sepal.Length\" column from iris and assign it to a2. My objective is that each \"a2\" value should be compared to every a1 value using the formula in the last line such that I get a final vector percent_calc with 40 values. \n\n```\nlibrary(stringdist)\nlibrary(RecordLinkage)\n\na1 = iris$Petal.Length[1:10] * 1000\na2 = iris$Sepal.Length[1:4]  * 1000\na1 = as.character(a1)\na2 = as.character(a2)\n\npercent_calc = RecordLinkage::levenshteinSim(a2,a1)\n```\n\n    ", "Answer": "\r\nGet all combinations, then get distance:\n\n```\na12 <- expand.grid(a1, a2, stringsAsFactors = FALSE)\n\npercent_calc <- levenshteinSim(a12$Var1, a12$Var2)\n\npercent_calc\n# [1] 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50\n# [19] 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.75 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50\n# [37] 0.50 0.50 0.50 0.50\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Given two words which are anagrams. Swap one word (only adjacent swapping of letters allowed) to reach to the other word [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Sorting a sequence by swapping adjacent elements using minimum swaps\r\n                            \r\n                                (2 answers)\r\n                            \r\n                    \r\n                Closed 8 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nThis is the interview question\n\nGiven two words that are anagram of each other. Swap one word (only adjacent swapping\nof letters allowed) to reach to the other word ?\n\nfor example \n\n```\ngiven = abcd\ntarget = dbac\n```\n\n\nto reach dbac\n\n```\n[Given] abcd\n[1]bacd\n[2]badc\n[3]bdac\n[4]dbac\n```\n\n\nI thought of solving it using edit distance, but edit distance does n't take into account\nonly adjacent swapping of letters\n\nWhat should be the approach to solve this?\n\nMy code using edit distance\n\n```\n#define STRING_X \"abcd\"\n#define STRING_Y \"dbac\"\n\n\n\n// Returns Minimum among a, b, c\nint Minimum(int a, int b, int c)\n{\n    return min(min(a, b), c);\n}\n\n// Recursive implementation\nint EditDistanceRecursion( char *X, char *Y, int m, int n )\n{\n    // Base cases\n    if( m == 0 && n == 0 )\n        return 0;\n\n    if( m == 0 )\n        return n;\n\n    if( n == 0 )\n        return m;\n\n    // Recurse\n    int left = EditDistanceRecursion(X, Y, m-1, n) + 1;\n    int right = EditDistanceRecursion(X, Y, m, n-1) + 1;\n    int corner = EditDistanceRecursion(X, Y, m-1, n-1) + (X[m-1] != Y[n-1]);\n\n    return Minimum(left, right, corner);\n}\n\nint main()\n{\n    char X[] = STRING_X; // vertical\n    char Y[] = STRING_Y; // horizontal\n\n    printf(\"Minimum edits required to convert %s into %s is %d by recursion\\n\",\n           X, Y, EditDistanceRecursion(X, Y, strlen(X), strlen(Y)));\n\n    return 0;\n\n}\n```\n\n    ", "Answer": "\r\nYou may solve this easily using a breadth first search on a graph: \n\n\nstrings are nodes,\nadjacent transposals are edges\nsequence of transposals are paths \n\n\nWith that in mind, you could use the boost graph library.  Or, for this simple problem, you could use standard library with vectors (for the sequence of transposals), lists (for the breath first search), and algorithms:  \n\n```\n#include <string>\n#include <list>\n#include <vector>\n#include <algorithm>\nusing namespace std; \n\ntypedef vector<string> sequence;  // sequence of successive transpositions \n```\n\n\nWith these standard data structures, the search function would look like:  \n\n```\nvector<string> reach (string source, string target) \n{\n  list<sequence> l;              // exploration list\n\n  sequence start(1, source);     // start with the source\n  l.push_back(start); \n\n  while (!l.empty()) {           // loop on list of candidate sequences \n    sequence cur = l.front();    // take first one \n    l.pop_front(); \n    if (cur[cur.size()-1]==target)  // if reaches target \n      return cur;                      // we're done !\n                          // otherwhise extend the sequence with new transpos\n    for (int i=0; i<source.size()-1; i++) { \n      string s=cur[cur.size()-1];     // last tranposition of sequence to extend\n      swap (s[i], s[i+1]);            // create a new transposition\n      if (find(cur.begin(), cur.end(), s)!=cur.end())\n         continue;      // if new transpo already in sequence, forget it\n      sequence newseq = cur;          // create extended sequence \n      newseq.push_back(s);\n      if (s==target)                  // did we reach target ? \n         return newseq;\n      else l.push_back(newseq);       // put it in exploration list\n    }\n  }\n                      // If we're here, we tried all possible transpos,\n  sequence badnews;   // so, if no path left, ther's no solution \n  return badnews; \n }\n```\n\n\nYou can then try the algorithm with the following:  \n\n```\n  sequence r = reach (\"abcd\", \"dbac\");\n\n  if (r.empty()) \n    cout << \"Not found\\n\"; \n  else {\n    for (auto x:r)\n      cout<<x<<endl;\n    cout <<r.size()-1<<\" transpositions\\n\";\n  }\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein distance with substitution, deletion and insertion count\r\n                \r\nThere's a great blog post here https://davedelong.com/blog/2015/12/01/edit-distance-and-edit-steps/ on Levenshtein distance. I'm trying to implement this to also include counts of subs, dels and ins when returning the Levenshtein distance. Just running a smell check on my algorithm.\n\n```\ndef get_levenshtein_w_counts(s1: str, s2: str):\n    row_dim = len(s1) + 1  # +1 for empty string\n    height_dim = len(s2) + 1\n\n    # tuple = [ins, del, subs]\n    # Moving across row is insertion\n    # Moving down column is deletion\n    # Moving diagonal is sub\n    matrix = [[[n, 0, 0] for n in range(row_dim)] for m in range(height_dim)]\n\n    for i in range(1, height_dim):\n        matrix[i][0][1] = i\n\n    for y in range(1, height_dim):\n        for x in range(1, row_dim):\n            left_scores = matrix[y][x - 1].copy()\n            above_scores = matrix[y - 1][x].copy()\n            diagonal_scores = matrix[y - 1][x - 1].copy()\n\n            scores = [sum_list(left_scores), sum_list(diagonal_scores), sum_list(above_scores)]\n            min_idx = scores.index(min(scores))\n\n            if min_idx == 0:\n                matrix[y][x] = left_scores\n                matrix[y][x][0] += 1\n            elif min_idx == 1:\n                matrix[y][x] = diagonal_scores\n                matrix[y][x][2] += (s1[x-1] != s2[y-1])\n            else:\n                matrix[y][x] = above_scores\n                matrix[y][x][1] += 1\n\n    return matrix[-1][-1]\n```\n\n\nSo according to the blog post if you make a matrix where the row is the first word + and empty str and the column is the 2nd word plus an empty string. You store the edit distance at each index. Then you get the smallest from the left, above and diagonal. If the min is diagonal then you know you're just adding 1 sub, if the min is from the left then you're just adding 1 insertion. If the min is from above then you're just deleting 1 character.\n\nI think I did something wrong cause get_levenshtein_w_counts(\"Frank\", \"Fran\") returned [3, 2, 2]\n    ", "Answer": "\r\nThe problem was that Python does address passing for objects so I should be cloning the lists to the variables rather than doing a direct reference.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Extracting syntactic information from tweets using DBSCAN\r\n                \r\nI am attempting to use minimum edit distance with DBSCAN to extract high-occurrence syntactic structures of tweets.  \n\nI POS tag 10k tweets, then precomputed their min-ed distance and store it in a distance matrix.  I then give the distance matrix to DBSCAN.  No matter how large or small my eps and min_sample size are, I don’t get more than 4 clusters (eps=1, min_sample=13).  With data of this size the syntactic structure should produce more and/or larger clusters.  \n\nHere’s my working theory of what’s happening:\n\n\nA has an edit-distance of 1 to B\nB has an edit-distance of 1 to C\nC has an edit-distance of 2 to A\nEven though A and C have an edit-distance of 2, they are put in the same cluster given their relationship to B. \n\n\nLet me know if there is something you believe I am missing or if there is another algorithm that might help me accomplish the same clustering task.\n\n```\ndb = DBSCAN(eps=3, min_samples=10, metric='precomputed')\ndb.fit(df)\nprint (set(db.labels_))\n{0, -1}\n```\n\n\nI chose DBSCAN since I do not know the number of clusters that will occur. \n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein distance in R on sentence level\r\n                \r\nHow can i do Levenshtein distance measurement on word (not character) Level in R?\n\nSee the following:\n\nExpected result 1) \n\n```\n# levenshtein operations needed: Delete*2 --> 2 operations\narray1 <- c(\"word\", \"car\")\narray2 <- c(\"word\", \"pool\", \"beer\", \"car\")\n```\n\n\nI am seeking a function ```\nlevenshtein()```\n, so that the distance of 2 is returned for the example above:\n\n```\nlevenshtein(array1, array2)\n--> 2\n```\n\n\nExpected result 2)\n\n```\n# levenshtein operations needed: Delete and insert --> 2 operations\narray1 <- c(\"word\", \"car\", \"pool\")\narray2 <- c(\"word\", \"pool\", \"car\")\n```\n\n\nI am seeking a function ```\nlevenshtein()```\n, so that the distance of 2 is returned for the example above:\n\n```\nlevenshtein(array1, array2)\n--> 2\n```\n\n\nI found the following:\nWord-level edit distance of a sentence\nBut i didnt find a working needleman-wunsch implentation that yields the expected results, described above.\n    ", "Answer": "\r\nNot entirely sure what you're asking... but is this what you are after?\n\n```\nlapply(array1, function(i){\n    m <- drop(attr(adist(i, array2, counts = TRUE), \"counts\")) \n    row.names(m) <- array2\n    setNames(list(m %>% as.data.frame()), i)\n}) %>% unlist(recursive = FALSE)\n$word\n     ins del sub\nword   0   0   0\npool   0   0   3\nbeer   1   1   2\ncar    0   1   2\n\n$car\n     ins del sub\nword   1   0   2\npool   1   0   3\nbeer   1   0   2\ncar    0   0   0\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Solr spellcheck's top suggestion is unexpected\r\n                \r\nI'm using solr 4.6.1 spellcheck component for spelling suggestions. I configured it to use DirectSolrSpellChecker with default distance function and comparator, which, as I understand, means the suggestions are ranked by edit distance (primary key), followed by document frequency (secodary key).\n\nhowever, for the term papaer, the top suggestion is papier, which has far less document frequency than that of paper. both the alternatives are 1 edit distance away from papaer.\n\nIs this a bug or a quirk of the edit distance algorithm I don't understand?\n\nmy spellcheck config:\n\n```\n<!-- a spellchecker built from a field of the main index -->\n<lst name=\"spellchecker\">\n  <str name=\"name\">default</str>\n  <str name=\"field\">spellfield</str>\n  <str name=\"classname\">solr.DirectSolrSpellChecker</str>\n  <!-- the spellcheck distance measure used, the default is the internal levenshtein -->\n  <str name=\"distanceMeasure\">internal</str>\n  <!-- minimum accuracy needed to be considered a valid spellcheck suggestion -->\n  <float name=\"accuracy\">0.5</float>\n  <!-- Sort Results by frequency -->\n  <str name=\"comparatorClass\">score</str>\n  <!-- the maximum #edits we consider when enumerating terms: can be 1 or 2 -->\n  <int name=\"maxEdits\">2</int>\n  <!-- the minimum shared prefix when enumerating terms -->\n  <int name=\"minPrefix\">0</int>\n  <!-- maximum number of inspections per result. -->\n  <int name=\"maxInspections\">5</int>\n  <!-- minimum length of a query term to be considered for correction -->\n  <int name=\"minQueryLength\">3</int>\n  <!-- maximum threshold of documents a query term can appear to be considered for correction -->\n  <float name=\"maxQueryFrequency\">0.01</float>\n  <!-- uncomment this to require suggestions to occur in 1% of the documents-->\n  <float name=\"thresholdTokenFrequency\">2</float>\n</lst>\n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Fastest way find the edit Distance of a word, given a list of Million words\r\n                \r\nI have a file with more than a million words, one word in each line. I am trying to write code where if I were given a word I need to find out if that word is present in the file. The thing here is, each word has to be checked for ```\n26^(word.length()-1)```\n times. Hence, going through every word in the file is not a good solution. I tried finding algorithms online but not have found any appreciable answer yet. \n\nEDIT\nI have thought about both a ```\nHashMap```\n and ```\nTrie```\n. The actual problem here is say I have the word ```\nabc```\n. Now, my task is to add, remove, or substitute exactly one letter in word ```\nabc```\n to create word X and then check if X is in the file. Hence am confused as to which solution might be a better approach.\n    ", "Answer": "\r\nYou can build a trie from the words in your file. This will use much less memory than a Hashset and allow you to check the existance of a word in O(number of characters in word). If memory is no concern, of course a Hashset will do (since that is built in its also much less effort).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Computing the shortest path between two words?\r\n                \r\nI need to calculate the distance of two words that are found in a list. And by distance I mean the number of words that are found in between the source and target word.\nex. dog -> cog -> cot -> cat\nTherefore the path distance would be three, as the edit distance between each word in the path is one. But I don't know how to deal with words with edit distances greater then one. \n    ", "Answer": "\r\nThis might help you:\n```\n    List<String> animals = new ArrayList<String>();\n    Boolean done=false;\n    Boolean found=false;\n    int dist=0;\n    string begin=\"dog\";\n    string end=\"cat\";\n\n    // add 4 different values to list\n    animals.add(\"dog\");\n    animals.add(\"cog\");\n    animals.add(\"cot\");\n    animals.add(\"cat\");\n    int i = 0;\n    while (i < animals.size()&&!done) {\n        if(animals.get(i).equals(begin)) found=true;\n        if(found){\n            dist++;\n            if(animals.get(i).equals(end))done=true;\n        }\n        i++;\n    }\n    System.out.println(dist);\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit/Levenstein distance with time stamp - different paths with similar (minimal) cost\r\n                \r\nI am using the Edit/Levenstein distance to measure similarity between words. Unlike the simplest implementation, my letters have time stamps, let's say in sample numbers N=0,1,2,...\n\nThe problem I am facing is that I can get different paths along the cost matrix which end in the same (minimal) cost, and these different paths are associated with different target string. For example, if I measure the distance between the source string ```\naa```\n and target string ```\nbab```\n, and I assume the source string starts on time stamp N=0, then I have 2 paths with the same cost of 2 (one addition and one substitution):\n\n\nAdd ```\nb```\n at N=-1, leave the 1st ```\na```\n as it is, and substitute the 2nd ```\na```\n with a ```\nb```\n.\nSubstitute the 1st ```\na```\n with a ```\nb```\n, leave the 2nd ```\na```\n as it is, and add ```\nb```\n at N=2.\n\n\nAligned on the time line, these 2 results are different:\n\n```\nTime:    ... -1 0 1 2 3 ...\nSource:         a a\nTarget1:      b a b\nTarget2:        b a b\n```\n\n\nI need to know when that happens, so I can choose between the two possible targets based on some criteria. Is there any other way other then tracing the path along the way and keeping track of all possible paths which lead to the minimal cost? \n\nI've considered using Dynamic Time Warp instead, since the time-line is part of the model in the first place, but it seems that since the cost matrix is initialized to infinity (except for the [0,0] entry), the first step will always be matching the 1st frame of the target to the 1st frame of the source, resulting in the target starting at the same time stamp as the source. Anyway, using DTW I still have to trace all paths leading to the same minimal cost.\n\nAny help or insights are welcomed.\n    ", "Answer": "\r\nThinking some more about your problem, it  seems a bit that there is a misunderstanding of DTW or Levensthein. Both algorithms try to squish and expand the sequences to match them with each other. So in the DTW case your example would have the following solutions:\n\n```\nSolution1:\n  a a\n /| |\nb a b\n\nSolution2:\na a\n| |\\\nb a b\n\nSolution3:\na a\n|\\|\\\nb a b\n```\n\n\nIf you have a look at these solutions, you will notice that all of these have a cost of 2, i.e. in all cases 2 ```\nb```\ns get assigned to as. What these examples mean is, that in the first sequence one timestamp gets squished together compared to the second sequence. For example in the first solution the first two timestamps of ```\nb a```\n get squished to form a single timestep corresponding to the first ```\na```\nof the second sequence (the second sequence is just reversed, the third solution is more complex). DTW is meant to deal with sequences that are played at different speed at certain parts, hence the \"time-warping\" analogy.\n\nIf your timesteps are really fixed and you only need to align them, without any actual warping considered, you might just try all alignments and calculate the costs.\n\nSomething like this (assuming str2 to be the shorter one):\n\n```\nfor i = 0 to length( str1 ) - length( str2 ) do\n  shift str2 by i to the left\n  calculate number of different position between shifted str2 and str1\ndone\nreturn i with lowest difference\n```\n\n\nAssuming you need both shifting as well as warping (something might have been added to the beginning and the timesteps might not match), then considere subsequence DTW. For this you just need to relax the boundary conditions.\n\nAssuming you index your string at one instead of zero you can write DTW like this:\n\n```\ndiff( x, y ) = 1 if str1 at x != str2 at x \n               0 otherwise\n\ncost( 0, 0 ) = 0;\ncost( 0, * ) = infinity;\ncost( *, 0 ) = infinity;\ncost( x, y ) = min( cost( x-1, y-1 ), cost( x-1, y ), cost( y, y-1) ) + diff( x, y )\n```\n\n\nDTW-Cost then is ```\ncost( length( str1 ), length( str2 ) )```\n and your path can be traced back from there. For subsequence DTW you simply change this:\n\n```\ndiff( x, y ) = 1 if str1 at x != str2 at x \n               0 otherwise\n\ncost( 0, 0 ) = 0;\ncost( 0, * ) = 0;\ncost( *, 0 ) = infinity; // yes this is correct and needed\ncost( x, y ) = min( cost( x-1, y-1 ), cost( x-1, y ), cost( y, y-1) ) + diff( x, y )\n```\n\n\nThen you pick your DTW-cost as ```\nmin( cost( x, length( str2 ) )```\n and trace back from ```\nargmin( cost( x, length( str2 ) )```\n. This assumes you know one string to be the substring of the other. If you do not know this and both might only have a common warped middle you will have to do partial matching, which as far as I know is still a open research topic, since one needs to pick a notion of \"optimality\" which cannot clearly be defined.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "java.lang.StringIndexOutOfBoundsException:edit distance between two strings is one\r\n                \r\nI am trying to Check if edit distance between two strings is one\n\nAn edit between two strings is one of the following changes.\n\nAdd a character\nDelete a character\nChange a character\n\nAnd I am facing \"StringIndexOutOfBoundsException\".\n\nDo I need to check anything to prevent this exception in my code?\n\n```\npublic class OneDiff { \npublic OneDiff(String s,String s2){\nint count=0;    \nfor (int i=0;i<(s.length()+s2.length());i++){\n    if(s.charAt(i)!= s2.charAt(i)){\n        count++;\n            }\n}\nif(count==1){\n    System.out.println(\"one difference\");\n}\n}\npublic static void main(String args[]){\nString s= \"xxx\";\nString s1=\"xxxy\";\nOneDiff od=new OneDiff(s,s1);\n}\n}\n```\n\n    ", "Answer": "\r\nAs @Ramanlfc and @YoungHobbit explained, because your length is 0, so it will throw ```\nStringIndexOutOfBoundsException```\n\n\nbut for your calculation of string's distance, Your algorithm is wrong, you need to come true Levenshtein distance to calculate the string's edit distance.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "C Levenshtein program works only with txt and pdf file\r\n                \r\nI'm writing a project that has the task of calculating the Edit distance between two input files.\nThe goal is to compare any type of file.\nThe problem comes when I go to run and this is the result.\nOUTPUT\n\nfiledistance distance pdf1.pdf pdf2.pdf\nEDIT DISTANCE: 216\nfiledistance distance sample.jpg sample2.jpg\nEDIT DISTANCE: 0\nfiledistance distance file1.txt file2.txt\nEDIT DISTANCE: 11\n\nWith a .txt file or a .pdf file it works fine. It doesn't work with a file other than these.\nObviously the compared files are different, so it is impossible that b.jpg has a distance from c.jpg equal to 0\nThis is my solution, but I don't see any problem.\nI'm going to read the files as binary files, in this way I shouldn't have any problem for any extension.\n```\nint levenshtein_distance(char *file1, char *file2){\n  char *str1 = getString(file1);\n  char *str2 = getString(file2);\n  int n = (int)strlen(str1) + 1;\n  int m = (int)strlen(str2) + 1;\n  if (n == 0)\n      return m;\n  if (m == 0)\n      return n;\n  int **matrix = matGenerate(str1, n, str2, m);\n  int distance = matrix[n - 1][m - 1];\n  deallocateMat(n, matrix);\n  free(str1);\n  free(str2);\n  return distance;\n}\n\nchar *getString(char *file){\n  FILE *inputFile = fopen(file, \"rb\");\n  if (inputFile == NULL){\n    perror(\"Could not open file\");\n    exit(1);\n  }\n  fseek(inputFile, 0L, SEEK_END);\n  long sizeFile = ftell(inputFile);\n  rewind(inputFile);\n  char *string = calloc(1, sizeFile);\n  if (string != NULL){\n      fread(string, sizeFile, 1, inputFile);\n  }\n  fclose(inputFile);\n  return string;\n}\n```\n\nMethods like matGenerate and deallocateMat are used to fill and empty the array. If requested, I can attach them.\nI can't understand why it doesn't work with files other than pdf and txt.\nCan you help me?\n    ", "Answer": "\r\n```\nstrlen```\n stops at ```\n\\0```\n. You need to return the size somehow. Something like this:\n```\nstruct filebuffer {\n    char *data;\n    size_t size;\n}\n\nstruct filebuffer getString (char *file){\n    struct filebuffer ret;\n\n    FILE *inputFile = fopen(file, \"rb\");\n    if (inputFile == NULL){\n        perror(\"Could not open file\");\n        exit(1);\n    }\n \n    fseek(inputFile, 0L, SEEK_END);\n    ret.size = ftell(inputFile);\n    rewind(inputFile);\n    ret.data = calloc(1, sizeFile);\n    if (ret.data != NULL){\n        fread(string, sizeFile, 1, inputFile);\n    } else {\n        // Handle error\n    }\n\n    fclose(inputFile);\n    return ret;\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Calculating Edit distance between pandas dataframe column values and a given string\r\n                \r\nI have one column in a pandas dataframe (it's actually fairly large, about 1.5 million rows of text data) that I want to compare against one string. For a simple sanity check/test, I wanted to try this only on the first 100 rows to get an idea that it won't take super long to execute. So a minimum sample of the dataframe looks like this:\n\n```\nText\nHello, this is Peter, what would you need me to help you with today? I need you\nGood Morning, John here, are you calling regarding your cell phone bill? I am not\n......\n```\n\n\nand I have a fixed string\n\n```\n\"Can I help you today?\"\n```\n\n\nWhat my goal is to get a similarity score (I am still deciding which metric I am using, Levenstein vs Jaccard or Cosine) but that's not my main question, to get a similarity score between each pandas dataframe value and the fixed string value, and then probably just sort them by order.\n\nHere is the code I have written:\n\n```\nimport nltk\nnltk.download()\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nLevenstein = []\nCounter = 0\n\nfor x in All_sentences.rows:\n    while Counter < 100:\n        distance = nltk.edit_distance(All_sentences['Text'], \"what I wanted \n        to calling because I lost my  ATM card debit card\")\n        Levenstein.append(distance)\n        Counter +=1\n```\n\n\nwhen I run this code, firstly, it pops up a dialog box with the NLTK downloader \n\n```\n[WinError 10060] A connection attempt failed because the connected party did \nnot properly respond after a period of time, or established connection \nfailed because connected host has failed to respond.\n```\n\n\nSecondly, I am seeing a message that says (below my code that is running but not finishing the execution):\n\n```\n​showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n```\n\n\nAnd I am waiting for a long time and nothing shows up in the output (it is still running, I only see the  * that it is still processing). \n\nWhat are those messages and most importantly, why would it take so long to process if I am only doing a sample comparison of 100 values instead of the whole dataset?\n    ", "Answer": "\r\nLook, I think the problem is within downloading NLTK packages. First, make sure your internet connection is working and stable. Then, open the terminal and write the following commands:\n\n```\n$ python\n>>> import nltk\n>>> nltk.download('popular')\n```\n\n\nThis will open the python shell and download the popular packages in NTLK. This would look like this:\n\nNow, run the code after deleting:\n\n```\nnltk.download()\nnltk.download('stopwords')\nnltk.download('wordnet')\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is there a way to perform edit distance (Levenshtein) character by character between two string columns?\r\n                \r\nI have two datasets: dataset1 & dataset2, which have a common column called ```\nSAX```\n which is a string object.\n```\ndataset1=\n         SAX\n0    gangsyu\n1    zicobgm\n2    eerptow\n3    cqbsynt\n4    zvmqben\n..       ...\n475  rfikekw\n476  bnbzvqx\n477  rsuhgax\n478  ckhloio\n479  lbzujtw\n\n480 rows × 2 columns\n```\n\nand\n```\ndataset2=\n         SAX\n0    gdmgsyu\n1    zifgbgm\n2    esdftow\n3    cqtjgnt\n4    znweben\n..       ...\n475  rfikekw\n476  bnbzvqx\n477  rsuhgax\n478  ckhloio\n479  lbzujtw\n\n480 rows × 2 columns\n```\n\nI need the output to be a column which is the sum of the number of the edits/changes it takes for SAX(dataset1) to become SAX(dataset2).The variation is basically what I am considering as an \"edit/change\" . (Example shown below)\n```\nTaking the first row of SAX from dataset1 and dataset 2 and comparing.\n\"gangsyu\" and \"gdmgsyu\"\n\nFirst character \"g\" is a match, so move on.\nSecond character is not a match, it takes 3 edits to change \"a\" to \"d\". \nThird character is not a match, it takes 1 edit to change \"n\" to \"m\"\nRest of the characters match.\nI want the column to be a sum of the edits/changes which is 3+1 = 4.(shown below)\ndataset3= \n     sum_edits\n0    4 (for the example shown right above)\n1    0\n2    1\n3    2\n4    0\n..       ...\n475  3\n476  0\n477  8\n478  1\n479  4\n\n480 rows × 2 columns\n```\n\nIs there a function/method to accomplish this? Would appreciate it a lot.\nThanks.\n    ", "Answer": "\r\nThere is a library that makes calculating Levenshtein distance easy (python-Levenshtein).\nIf you're willing to use the library, you can simply iterate through your datasets and calculate ```\ndistance(item_dataset_1, item_dataset_2)```\n.\nBut in your example it is unclear to me why you count the change from a to d as 3 edits. It should be counted as 1 edit to calculate the Levenshtein distance.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "DP top left vs bottom right table filling. Which to use when?\r\n                \r\nThere are many problems which require table filling from top left(ex: Edit Distance) and starting from bottom right(ex: Palindromic substrings).\nIs there any intuitive explanation of which to use when?\nReference:\nhttp://www.geeksforgeeks.org/dynamic-programming-set-5-edit-distance/\nhttps://leetcode.com/problems/palindromic-substrings/discuss/\n    ", "Answer": "\r\nThe direction of the table itself doesn't make any difference, it's isomorphic under a trivial indexing transformation. That is, you can trivially flip the table around in any dimension by subtracting that index from the size of that dimension. That doesn't make any difference and you could say that the algorithm doesn't even \"know\" that you're doing that, you could even make a table-like object that abstracts that transformation away so the code looks exactly the same but deep down there is a table being filled in the opposite direction.\n\nThe order in which the sub-problems are solved matters, it must obey the dependency structure. Finding an appropriate order to solve the sub-problems in is actually an important step in the \"turn recursion into DP\"-checklist, it is often overlooked because usually there is some trivial order that works that you don't even have to think about. But for example, here is the structure of the Fibonacci recursion, courtesy of wikipedia:\n\n\n\nWhat you can see here (and what you also immediately obvious from the recursive definition) is that invocations with a some argument only depend on invocations with smaller arguments. Therefore, filling the table in order from lowest argument to higher is a valid order, which guarantees that when a cell in the table is needed, it has already been computed. (usually this is optimized further to keep only the previous two items rather than all preceding, but that is not the point here)\n\nIt is not always that simple, especially in higher dimensions, but in your example of Edit Distance: (src: geeksforgeeks.org)\n\n\n\nYou can see that every eD(x,y) depends only on eD(x-dx,y-dy) where dx and dy are 0 or 1 and not both 0, which is satisfied by many orders, such as (not limited to):\n\n\nlexicographical on (y,x) (presented in their answer)\nlexicographical on (x,y) (just invert the inner and outer loop)\nanti-diagonals\nstarting at 0,0, extending the rectangle of filled cells by 1 step down, then right, down again, etc\nextending it by k steps\nshrinking the rectangle of empty cells by a step down, then right, etc\nquarter-circles of expanding radius with the center at 0,0\n\n\nAll you have to preserve is that when eD(a,b) is calculated, everything it needs has already been calculated. This leaves a lot of freedom, you could even take all of the empty cells that have filled cells to their left and top and pick one to fill at random. However, an order that definitely does not work for this problem is filling the table starting at (m,n) - just think about which cells it would need: cells that you haven't filled yet (also if you could do it, then you could compute the final answer in one step).\n\nIn terms of the dependency graph, any topological order will work.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Algorithm for Levenshtein distance of multi-dimensional strings [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     Questions asking us to recommend or find a tool, library or favorite off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Instead, describe the problem and what has been done so far to solve it.\r\n                \r\n                    \r\n                        Closed 9 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI am looking for an extension for Levenshtein distance (Edit distance) for multi dimensional strings. I am not sure if there is a formal definition for multi dimensional or not, but here is what I am talking about:\n\n1-D string: is the regular string\n\n2-D string: is something like a list of 1-D strings like\n\n```\ndfdsfdsfdsf\ndsffgdfdgfdsdaf\ndsfdsf\nfdgfdgfdg\n```\n\n\nN-D string: is list of (N-1)-D strings\n\nHow can I calculate the Levenshtein distance between such multidimensional strings?\n    ", "Answer": "\r\nEdit distance is based on the minimum cost sequence of operations that turns one string into another. If these operations represent rare mistakes then that distance is a rough measure of the probability of one string being corrupted into another.\n\nTo find a 2-d variant, you have to decide what sort of operations are permissible, which will depend on why you want to work this out. If each string in one list maps to a corresponding string in the other list, then you might just want the sum of the edit distances in the resulting pairs. If there is no correspondence at all, you might work out the edit distance of all n * m pairs of string and then find the least cost matching which associates one string in the first list with one string in the second list, and scores a matching with the sum of the edit distance of matched pairs of strings. If the corruption process inserts and deletes entire strings, as well as inserting and deleting characters within a string, then you might end up computing the edit distance between all pairs of strings and using these costs to do something like an edit distance calculation between two lists of strings instead of two lists of characters - it all depends on why you want this and what operations might transform one list of strings into another.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Getting Levenshtein distance from `diifflib.ndiff`\r\n                \r\nThe levenshtein distance algorithm in Python is too slow as I am comparing many strings.\nSo I want to use ```\ndifflib.ndiff```\n to do it.\nI tried parsing the output by interpreting \"+\", \"-\", \" \" from the result of ```\nndiff```\n, but failed.\nHere is what I tried:\n```\nimport difflib\nedit_dist = sum(op[0] == \"-\" for op in difflib.ndiff(\"split\", \"sitting\"))\n```\n\nBut the returned result is incorrect.\nI want to use solutions from ```\nstdlib```\n. Any advice / solutions?\nP.S. I need to get the nunber edit distance, not ratio so ```\nSequenceMatcher.ratio```\n doesn't work\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Clusters of words with Hamming distance of 1\r\n                \r\n\nYou are given a set of words, e.g.\n```\n{ ded, aaa, dec, aab, cab, def }\n```\n\nYou need to add each word into a group. A word can be added to a group if:\n\nIt is the only word in the group\nThere is at least one other word already in the group that is at the most 1 edit distance away from the given word.\n\nYour function must return the minimum possible number of such groups that can be formed by using all the given words.\nExample, for the given input string, the groups would look like this:\n```\n{ aaa, aab, cab }, { ded, def, dec }\n```\n\nExplanation: ```\ndistance(aaa, aab)```\n is 1 so they belong in the same group. Also, ```\ndistance(aab, cab)```\n is also 1, so they also belong in the same group. But no words in the second group are at an edit distance of 1 from any other word in the first group, but are at an edit distance of 1 from at least one other word in their own group.\nIf we were given two more words in addition to the ones in the example, let's say ```\ncad, ced```\n, then the answer would change to 1, because now ```\ndistance(cab, cad)```\n is 1, hence cad is in group 1 and ```\ndistance(cad, ced)```\n is 1, so ced is in group 1. Also, distance(ded, ced) is 1, so the second group will be \"connected\" with the first group, hence we will be left with only 1 group.\nWe're only interested in the number of groups, not the groups themselves.\nConstraints: all words will have the same length, but that length is not fixed and could be large.\n\nI could only come up with O(mn^2) where m is the length of any word and n is number of words. Did this using graph approach (Each word as a node and word with edit distance 1 as a neighbouring node).\nExpected solution is O(mn).\n    ", "Answer": "\r\nFound a solution which is an extension of the accepted solution here:\nEfficiently build a graph of words with given Hamming distance\n\nBasically, the idea is to store the strings in a Set where lookup and delete are O(1) on average. Putting them in a set means we'd be overwriting strings with edit distance of 0 i.e. equal strings. But we don't care for them anyway, as they will always be in the same group.\n\n\nCreate an empty list of \"start nodes\" N.\nAdd next item from the set S in the list\nRemove this string from the set S and call 4 for this string.\nGenerate all strings with Hamming distance 1 from string passed in parameter. For each such generated string, if it exists in the set, remove it from the set and call 4 for this string.\nWhile set is not empty, repeat 2\nReturn size of \"start nodes\" list \n\n\nExplanation of why this would work:\n\nWe traverse each node only once and remove it from the set. After we remove the string from the set, we also recursively remove any item in the set that was \"adjacent\" to it. But only the first node in the recursion is added to the start nodes list.\n\nIn our example, ded would get added to the node list and dec, def would get removed. Then aaa would get added to the node list and aab would be removed. While removing aab, recursively, cab would also be removed. The returned answer would be 2.\n\nTime complexity:\n\nO(mnC) where C is the size of the charset, m is the length of the string and n is the number of strings.\n\nC substitutions made for each character in string m times. This is done once for each item in the string set.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Generate test cases for levenshtein distance implementation with quickCheck\r\n                \r\nAs part of me learning about ```\nquickCheck```\n I want to build a test generator for a levenshtein edit distance implementation. The obvious approach I think is to start with two equal strings and a random non-reducable series of insert/delete/traspose actions, apply that to one of the strings and assert that the levenshtein distance is the length of the random series.\n\nI am quite stuck with this can someone help?\n    ", "Answer": "\r\nGetting \"non-reducible\" right sounds pretty hard. I would try to find a larger number of less complicated invariants. Here are some ideas:\n\n\nThe edit distance between any string and itself is 0.\nNo two strings have a negative edit distance. \nFor an arbitrary string ```\nx```\n, if you apply exactly one change to it, producing ```\ny```\n, the edit distance between ```\nx```\n and ```\ny```\n should be 1.\nGiven two strings ```\nx```\n and ```\ny```\n, compute the distance ```\nd```\n between them. Then, change ```\ny```\n, yielding ```\ny'```\n, and compute its distance from ```\nx```\n: it should differ from ```\nd```\n by at most 1.\nAfter applying ```\nn```\n edits to a string ```\nx```\n, the distance between the edited string and ```\nx```\n should be at most ```\nn```\n. Note that case (1) is a special case of this, where n=0, so you could omit that one for concision if you like. Or, keep it around, since case (1) may generate simpler counterexamples.\nThe function should be symmetric: the edit distance from ```\nx```\n to ```\ny```\n should be the same as from ```\ny```\n to ```\nx```\n.\n\n\nIf you have another, known-good implementation of the algorithm to test against, you could compare to that, and assert that you always get the same answer as it does.\n\nThe above were all just things that appealed to me without any research. You could do more: for example, encode the lower and upper bounds as defined by wikipedia.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "average value when comparing each element with each other element in a list\r\n                \r\nI have number of strings (n strings) and I am computing edit distance between strings in a way that I take first one and compare it to the (n-1) remaining strings, second one and compare it to (n-2) remaining, ..., comparing until I ran out of the strings.\n\nWhy would an average edit distance be computed as sum of all the edit distances between all the strings divided by the number of comparisons squared. This squaring is confusing me.\n\nThanks,\nJannine\n    ", "Answer": "\r\nI assume you have somewhere an answer that seems to come with a squared factor -which I'll take as n^2, where n is the number of strings (not the number of distinct comparisons, which is n*(n-1)/2, as +flaschenpost points to ). It would be easier to give you a more precise answer if you'd exactly quote what that answer is. \n\nFrom what I understand of your question, it isn't, at least it's not the usual sample average. It is, however, a valid estimator of central tendency with the caveat that it is a biased estimator.\nSee https://en.wikipedia.org/wiki/Bias_of_an_estimator.\n\nLet's define the sample average, which I will denote as X', by\nX' = \\sum^m_i X_i/N \n\nIF N=m, we get the standard average. In your case, this is the number of distinct pairs which is m=n*(n-1)/2. Let's call this average Xo.\n\nThen if N=n*n, it is \nX' = (n-1)/(2*n) Xo\n\nXo is an unbiased estimator of the population mean \\mu. Therefore, X' is biased by a factor f=(n-1)/(2*n). For n very large this bias tends to 1/2.\n\nThat said, it could be that the answer you see has a sum that runs not just over distinct pairs. The normalization would then change, of course. For instance, we could extend that sum to all pairs without changing the average value: The correct normalization would then be N = n*(n-1); the value of the average would still be Xo though as the number of summands has double as well.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance algorithm for finding minimum operations to convert one string to other\r\n                \r\nLets take example, str1=\"ABC\",str2=\"AFB\"\nTo find minimum steps we consider three possibilites-\n(since last letter of strings don't match)\n\n```\n1) 1+f(\"ABC\",\"AF\")\n2) 1+f(\"AB\",\"AFB\")\n3) 1+f(\"AB\",\"AF\")\n```\n\n\nand take minimum of the three..\nMy doubt is why don't we consider,  1+f(\"ABC\",\"AB\")\nPlease explain for a general case and not this one.Thanks.\n    ", "Answer": "\r\nThere are indeed many more possibilities. But the algorithm doesn't need to take them into account immediately.\n\nThe way to calculate the distance is to reduce it to smaller tasks by asking some questions and enumerating answers for them.\n\nIn your case the question probably was: what will the optimal edit sequence do to make the last characters equal?\n\nThere are 3 possible answers:\n\n\nit will remove ```\nC```\n from ```\nABC```\n, then we get subproblem ```\n(AB,AFB)```\n\nit will add ```\nB```\n to ```\nABC```\n, then we get subproblem ```\n(ABCB,AFB)```\n which is equivalent to ```\n(ABC,AF)```\n\nit will replace ```\nC```\n with ```\nB```\n, then we get subproblem ```\n(ABB,AFB)```\n which is equivalent to ```\n(AB,AF)```\n\n\n\nFrom these options we, of course, select the shortest answer.\n\nIt's certainly possible to ask different questions, but the main idea is that we want to ask for some smaller information to reduce problem to a sequence (or a tree) of subproblems.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Semi-local Levenshtein distance\r\n                \r\n[Reposted from https://cs.stackexchange.com/questions/12986/sliding-window-edit-distance ]\n\nIf you have a long string of length n and a shorter string of length m, what is a suitable recurrence to let you compute all n-m+1 Levenshtein distances between the shorter string and all substrings of the longer string of length m?\n\nCan it in fact be done in O(nm) time?\n    ", "Answer": "\r\nComputing the Levenshtein distances for a sliding window boils down to computing the distances between several pairs of vertices in an acyclic directed planar graph that looks like this one (capital letters denote the pairs).\n\n```\n   h a y s t a c k\n\nn  A-B-C-D-E-F-*-*\n   |\\|\\|\\|\\|\\|\\|\\|\ne  *-*-*-*-*-*-*-*\n   |\\|\\|\\|\\|\\|\\|\\|\ne  *-*-A-B-C-D-E-F\n```\n\n\nThe horizontal and vertical arcs have cost 1; the diagonal arcs have cost 0 if the corresponding letters match or 1 otherwise.\n\nSince all of the paired vertices lie on the infinite face, Klein's or Cabello-Chambers's multiple-source shortest paths algorithm can be used to compute the needed distances in time O(m n log (m n)).\n\nTo shave the final log (and practically speaking, it's much worse than for, e.g., Dijkstra's algorithm), you might look in Alexander Tiskin's manuscript Semi-local string comparison: Algorithmic techniques and applications, which treats problems similar to this one if not this one itself. (Probably that should be my primary answer, but I haven't read it and know the multiple-source shortest path techniques a lot better.)\n\nIt's also possible that, with some additional logic to handle the unidirectional edges, my multiple-source shortest path algorithm with Klein could be made to achieve O(m n).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Displaying output in shiny main panel of the shiny UI (code is running without any error still result is not displayed on the UI)\r\n                \r\nI am trying to create a UI on which I can upload a file and also there is a text input where I can write the product name which I want to search in the uploaded file. I am doing that using the Levenshtein Distance function (adist() function). Now, once i get the results for which the edit distance is 0, I want to display those rows in the Table on the Main Panel. Whatever input is given in the Text input on the UI is searched against the items column in the file uploaded. A sample image of the CSV file which is uploaded is this-\n\nSample image of the CSV file which is input by the user\n\nOnce I run the code and find the edit distance for all the words, I store them in a vector and then use this to print the rows from the file which have edit distance equal to 0. The problem is that when I click on submit, the result is not displayed on the UI but it is displayed on the R-studio console. How do I fix this?\n\nPlease help me with the code.\n\n```\nlibrary(shiny)\n\nui = shinyUI(fluidPage(\n  titlePanel(\"LEVENSHTEIN DISTANCE function trial\"),\n\n  sidebarLayout(\n\n    sidebarPanel(\n      numericInput(\"rows\",\"Enter the number of rows\",value=NULL),\n      textInput(\"product\", \"input product name\"),\n      br(),\n      br(),\n      fileInput(\"file\", \"Input the file\"),\n      submitButton(\"Find!\")),\n\n    mainPanel(\n      tableOutput(\"result\")\n    )\n  )\n))\n\nserver = shinyServer(function(input,output) {\n  output$result <- renderPrint({ if (is.null(input$file)) return( ); \n    trial = read.csv(input$file$datapath)\n\n    ls = vector('list', length = input$rows)\n\n    for(i in 1:input$rows) {\n      edit = adist(\"earbuds\", trial$items[i])\n      new_edit = as.numeric(edit)\n      ls[i] = edit\n      if(ls[i]==0) print(trial[i, ])\n    }\n\n  })\n})\n\nshinyApp(ui=ui,server=server)\n```\n\n\nThank You!\n    ", "Answer": "\r\nIt is very hard to provide working code without sample input date. But, here is my attempt at giving you what I think should work.\n\n```\nserver = shinyServer(function(input,output) {\n  output$result <- renderTable({\n    if (!is.null(input$file)) { \n      trial = read.csv(input$file)\n      trial <- trial[adist('earbuds', trial$items) == 0), ]\n    }\n  })\n})\n```\n\n\nIf you provide input data and expected output table, I can edit the answer to be more precise.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "List off-diagonal values from levenshtein distance matrix\r\n                \r\nUsing the following data, how can I create a DataFrame with the column 'id' as an index and a second column containing a list of off-diagonal values from a Levenshtein distance matrix for the list of strings corresponding to each id?\n\n```\nd = {'id':[1,1,1,2,2],'string':['roundys','roundys','ppg','brewers','cubs']}\ndf = pd.DataFrame(data=d)\n```\n\n\nThe goal is to generate a DataFrame that looks something like\n\n```\ndf_diag = pd.DataFrame({'id':[1,2],'diag_val':['0.0,7.0,7.0','6.0']})\n```\n\n\nI have built some rough pieces that work with a single list but have not been able to iterate by 'id' across multiple lists. Am using pandas as 'pd', numpy as 'np' and distance from Levenshtein as 'dist'\n\nStep 1 Create a Test List\n\n```\naTest = ['roundys','roundys','ppg']\n```\n\n\nStep 2 Create Function that Returns Edit Distance Matrix from aTest\n\n```\ndef editDistance(list_o_strings):\n    matrix = np.zeros(shape = (len(list_o_strings),len(list_o_strings)))\n\n    for i in range(len(list_o_strings)):\n        for j in range(i, len(list_o_strings)):\n            matrix[i][j] = dist(list_o_strings[i],list_o_strings[j])\n    for i in range(0, len(list_o_strings)):\n        for j in range(0,len(list_o_strings)):\n            if i == j:\n                matrix[i][j] = 0\n            elif i > j:\n                matrix[i][j] = matrix[j][i]\n    return matrix\n```\n\n\nStep 3 Create Function that Returns Off-diagonal Edit Distance Terms\n\n```\ndef selectElements(matrix):\n    ws = []\n    for i in range(0, matrix.shape[0]):\n        for j in range(0, matrix.shape[1]):\n            if i <> j and i>j:\n                ws.append(matrix[i,j])\n    return ws \n```\n\n\nStep 4 Test the Example List\n\n```\ntestDistance = editDistance(aTest)\ntestOffDiag = selectElements(testDistance)\n```\n\n\nMy next step is to iterate the functions over the unique id values within the dataset.  I created a new data frame of id paired to a list of strings with\n\n```\ndf1 = df.groupby('id').agg(lambda x: ','.join(x))\n```\n\n\nMy attempts to have the functions loop through the id terms have failed miserably, any suggestions?\n    ", "Answer": "\r\nYou can get Levenshtein distance with a ```\npip```\n install\n\n```\npip install python-Levenshtein\n```\n\n\nThen you can do something like this\n\n```\nfrom Levenshtein import distance\nfrom itertools import combinations\n\ndef lm(a):\n  return [distance(*b) for b in combinations(a, 2)]\n\ndf.groupby('id').string.apply(lm).reset_index(name='diag_val')\n\n   id   diag_val\n0   1  [0, 7, 7]\n1   2        [6]\n```\n\n\nOr\n\n```\ndef lm(a):\n  return ','.join([str(distance(*b)) for b in combinations(a, 2)])\n\ndf.groupby('id').string.apply(lm).reset_index(name='diag_val')\n\n   id diag_val\n0   1    0,7,7\n1   2        6\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Declaring 2-dimensional array in \"Edit distance\" on spoj locally gives runtime error?\r\n                \r\nWhile solving http://www.spoj.com/problems/EDIST/ , when I declare the 2-d array globally: (http://ideone.com/jG3jPW)\n\n```\n#include <iostream>\nusing namespace std;\n\nlong long int s[2001][2001];\n\nint main() {\nint t;\nstring a,b;\nlong long int i,j;\nfor(i=0;i<2001;i++)\n{\n    s[i][0]=i;\n    s[0][i]=i;\n}\ncin>>t;\nwhile(t>0)\n{\n    cin>>a>>b;\n    t--;\n    for(i=1;i<=a.length();i++)\n    {\n        for(j=1;j<=b.length();j++)\n        {\n            if(a[i-1] == b[j-1])\n                s[i][j]=s[i-1][j-1];\n            else\n                s[i][j] = min(min(s[i-1][j],s[i-1][j-1]),s[i][j-1]) + 1;\n        }\n    }\n    cout<<s[i-1][j-1]<<\"\\n\";\n}\nreturn 0;\n}\n```\n\n\nno error occurs. But when i declare the same array locally(http://ideone.com/Tyj6UU), \n\n```\n#include <iostream>\nusing namespace std;\n\n\nint main() {\nint t;\nstring a,b;\nlong long int i,j;\n\nlong long int s[2001][2001]; //declared locally\n\nfor(i=0;i<2001;i++)\n{\n    s[i][0]=i;\n    s[0][i]=i;\n}\ncin>>t;\nwhile(t>0)\n{\n    cin>>a>>b;\n    t--;\n    for(i=1;i<=a.length();i++)\n    {\n        for(j=1;j<=b.length();j++)\n        {\n            if(a[i-1] == b[j-1])\n                s[i][j]=s[i-1][j-1];\n            else\n                s[i][j] = min(min(s[i-1][j],s[i-1][j-1]),s[i][j-1]) + 1;\n        }\n    }\n    cout<<s[i-1][j-1]<<\"\\n\";\n}\nreturn 0;\n}\n```\n\n\nruntime error occurs. Why? \n    ", "Answer": "\r\nIt seems that you are having some kind of problem with memory allocation.\n\nIn your second approach (i.e., the one with local declaration), change:\n\n```\nlong long int s[2001][2001];\n```\n\n\nTo:\n\n```\nlong long int ** s = new long long int * [2001];\nfor (i=0;i<2001;i++)\n    s[i] = new long long int[2001];\n```\n\n\nThis will solve your problem.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is there a way to perform edit distance between two string columns in a dataframe?\r\n                \r\nI have two datasets: dataset1 & dataset2 (image link provided), which have a common column called ```\nSAX```\n which is a string object.\n```\ndataset1=\n         SAX\n0    glngsyu\n1    zicobgm\n2    eerptow\n3    cqbsynt\n4    zvmqben\n..       ...\n475  rfikekw\n476  bnbzvqx\n477  rsuhgax\n478  ckhloio\n479  lbzujtw\n\n480 rows × 2 columns\n```\n\nand\n```\ndataset2=\n         SAX\n0    glngsyu\n1    zicobgm\n2    eerptow\n3    cqbsynt\n4    zvmqben\n..       ...\n475  rfikekw\n476  bnbzvqx\n477  rsuhgax\n478  ckhloio\n479  lbzujtw\n\n480 rows × 2 columns\n```\n\nI need the output to be a column of minimum number of edits (operations) required to convert \"SAX\" column of dataset1 to \"SAX\" of dataset2. Is there a way to accomplish that?\nThanks.\n    ", "Answer": "\r\nUsing Levenshtein distance from the textdistance module:\n```\nfrom textdistance import levenshtein\n\n# Merge the two columns in one dataframe\ndf = dataset1[['SAX']].merge(dataset2[['SAX']], left_index=True, right_index=True, suffixes=('_1', '_2'))\n\n# Compute the Levenshtein distance\ndf['distance'] = df.apply(lambda x: levenshtein.distance(x['SAX_1'],  x['SAX_2']), axis=1)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Dictionary-Based Named Entity Recognition with zero edit distance: LingPipe, Lucene or what?\r\n                \r\nI'm trying to perform a dictionary-based NER on some documents. My dictionary, regardless of the datatype, consists of key-value pairs of strings. I want to search for all the keys in the document, and return the corresponding value for that key whenever a match occurs.\n\nThe problem is, my dictionary is fairly large: ~7 million key-values - average length of keys: 8 and average length of values: 20 characters.\n\nI've tried LingPipe with MapDictionary but on my desired environment setup, it runs out of memory after 200,000 rows are inserted. I don't know clearly why LingPipe uses a map and not a hashmap in their algorithm.\n\nSo the thing is, I don't have any previous experience with Lucene and I want to know if it makes such thing with such number possible in an easier way.\n\nps. I've already tried chunking the data into several dictionaries and writing them on disk but it's relatively slow.\n\nThanks for any help.\n\nCheers \nParsa\n    ", "Answer": "\r\nI suppose if you wanted to reuse LingPipe's ExactDictionaryChunker to do the NER, you could override their MapDictionary to store & retrieve from your choice of key/value database instead of their ObjectToSet (which does extend HashMap, by the way). \n\nLucene/solr can be used as a key/value store, but if you don't need the extra searching capabilities, just a pure look-up, other options might be better for what you're doing.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "calculate actual distance travelled by mobile [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 7 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI want to calculate actual distance traveled by mobile (iOS and Android). I know through google map API, we can find optimum route distance between 2 coordinates. But I want to calculate distance, actual path mobile(in vehicle) has covered.\n\nOne Algo I came to know is saving coordinates after x seconds, lets say after 5 or 10 seconds, and then calculate distance between consecutive coordinates, and there sum will give total distance.\n\nI want to discuss better approach of its solution , Is there any better solution?\n\nEdit : How Apps like Nike running app and Uber works?\n    ", "Answer": "\r\n------------------UPDATE----------------------\n\nThere is two major point in your question. \n\n1) get the phone coordinates (which has been treated in the first part of this response)\n\n2) Calculate the real distance between this two coordinates\n\nIMHO, calculus could be done by a web service: calculation based only\non the distance between two coordinates can lead to really wrong result.\n\nHere is an exemple of such a web service\nhttps://graphhopper.com/#directions-api\n\nThe demo app: https://graphhopper.com/api/1/examples/\n\nIt's based on traffic flow (as many of this tools)\nSo you have to be careful with the order of the coordinates\nbecause it can bring wrong result.\n\nFor exemple with two point in the right order:\n\n\nThis gives a good result\n\nBut if you give wrong order (with the same coordinates)\n\n\nFor the same coordinates, it can lead to an extremely different result.\n\nSo for coordinates ABCD (in chrnological order)\nyou need to do:\n\nA->B B->C C->D \n\nGraphhopper seems able to do offline distance calculus\n\nHere are the lib on iOS and Android\n\nhttps://github.com/graphhopper/graphhopper-ios/\n\nhttps://github.com/graphhopper/graphhopper/tree/master/android\n\n---------------------------------------------------\n\nYou have to define how your app work. Foreground or background?\n\nAs said in other responses, you'll have to get the user position every X seconds. Then calculate the distance.\n\nFor iOS:\n\n\nYou can use information on this website: http://mobileoop.com/\n\n\nIt talks about tracking user location on iOS when the app is in background.\n\n\nHere is the github: https://github.com/voyage11/Location\n\n\nThen you have to convert the point thanks to \n\n```\nCLLocationDistance distance = [aCLLocationA distanceFromLocation:aCLLocationB];\n```\n\n\nYou can also check this (from apple doc) https://developer.apple.com/library/ios/documentation/UserExperience/Conceptual/LocationAwarenessPG/CoreLocation/CoreLocation.html:\n\n\n  Make sure the location manager’s pausesLocationUpdatesAutomatically property is set to YES. When this property is set to YES, Core Location pauses location updates (and powers down the location hardware) whenever it makes sense to do so, such as when the user is unlikely to be moving anyway. (Core Location also pauses updates when it can’t obtain a location fix.)\n  \n  Assign an appropriate value to the location manager’s activityType property. The value in this property helps the location manager determine when it is safe to pause location updates. For an app that provides turn-by-turn automobile navigation, setting the property to CLActivityTypeAutomotiveNavigation causes the location manager to pause events only when the user does not move a significant distance over a period of time.\n\n\nCLActivityTypeAutomotiveNavigation insure you to get a position which is on a road.\n\nFor Android:\n\n\nYou can use this project:\nhttps://github.com/quentin7b/android-location-tracker\n\n\nThat can easily helps you to get the user's position thru time\nThanks to the TrackerSettings object \n\n```\nTrackerSettings settings =\n        new TrackerSettings()\n            .setUseGPS(true)\n            .setUseNetwork(true)\n            .setUsePassive(true)\n            .setTimeBetweenUpdates(30 * 60 * 1000)\n            .setMetersBetweenUpdates(100);\n```\n\n\n\nTo find the distance between two point on Android, you can check this:\nGet the distance between two geo points\n\n\nBoth OS\n\nBased on a position picked up every X second you have to reduce time between picking location data to improve accuracy.\n\nAs you want to calculate distance on a road context, setup the Location manager in navigation mode, this mode gives you  coordinates that are on road.\n\nFinally\n\nIf you want to improve the accuracy of your distance calculus,\nyou can use a google API:\nhttps://developers.google.com/maps/documentation/distance-matrix/intro\n\nBy setting the right mode parameter:\n\n\n  Optional parameters \n  \n  mode (defaults to driving) — Specifies the mode of transport to use when calculating distance. Valid values and other request details are specified in the Travel Modes section of this document.\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "TypeError: unhashable 'list'\r\n                \r\nI am building a program for comparing each promocode(might contain ocr error) in a list to all the promocode in another list(list of correct promocodes) \n\nthe expected output is edit distance and the promo code with least edit distance to the one which is getting compared.\n\nmy code\n\n```\nimport csv\nfrom nltk.metrics import distance\n\nwith open(\"all_correct_promo.csv\",\"rb\") as file1:\n    reader1 = csv.reader(file1)\n    correctPromoList = list(reader1)\n    #print correctPromoList\n\nwith open(\"all_extracted_promo.csv\",\"rb\") as file2:\n    reader2 = csv.reader(file2)\n    extractedPromoList = list(reader2)\n    #print extractedPromoList\n\ndef find_min_edit(str_,list_):\n    nearest_correct_promos = []\n    distances = {}\n    min_dist = 100 # arbitrary large assignment\n    for correct_promo in list_:\n        dist = distance.edit_distance(extracted,correct_promo,True) # compute Levenshtein distance\n        distances[correct_promo] = dist # store each score for real promo codes\n        if dist<min_dist:\n            min_dist = dist # store min distance\n    # extract all real promo codes with minimum Levenshtein distance\n    nearest_correct_promos.append(','.join([i[0] for i in distances.items() if i[1]==min_dist])) \n    return ','.join(nearest_correct_promos) # return a comma separated string of nearest real promo codes\n\nincorrectPromo = {}\ncount = 0\nfor extracted in extractedPromoList:\n    print 'Computing %dth promo code...' % count\n    incorrectPromo[extracted] =  find_min_edit(extracted,correctPromoList) # get comma separated str of real promo codes nearest to extracted\n    count+=1\nprint incorrectPromo\n```\n\n\nExpected output\n\n```\nComputing 0th promo code...\nComputing 1th promo code...\nComputing 2th promo code...\n{'abc': 'abc', 'abd': 'abx,aba,abz,abc', 'acd': 'abx,aba,abz,abc'}\n```\n\n\nBUT, my code is showing the following errors\n\n```\nComputing 0th promo code...\n\nTraceback (most recent call last):\n\n  File \"correctpromo_test4.py\", line 31, in <module>\n\n    incorrectPromo[extracted] =  find_min_edit(extracted,correctPromoList) # get \ncomma separated str of real promo codes nearest to extracted\n\n File \"correctpromo_test4.py\", line 20, in find_min_edit\n\n    distances[correct_promo] = dist # store each score for real promo codes\n\nTypeError: unhashable type: 'list'\n```\n\n    ", "Answer": "\r\nYou are reading the CSV as a list of lists - \nthe function find_min_edit() is expecting a list of strings as its second argument; what you are passing is a list of lists of strings.\n\nChanging the way you read the csv files with sort this stuff out - \n\nInstead of \n\n```\nwith open(\"all_correct_promo.csv\",\"rb\") as file1:\n    reader1 = csv.reader(file1)\n    correctPromoList = list(reader1)\n```\n\n\nJust use this\n\n```\nwith open(\"all_correct_promo.csv\",\"rb\") as file1:\n    reader1 = csv.reader(file1)\n    correctPromoList = [''.join(i) for i in reader1]\n    print correctPromoList\n```\n\n\nDo this for both the CSVs, that will sort it out...\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How do you implement Levenshtein distance in Delphi?\r\n                \r\nI'm posting this in the spirit of answering your own questions.\n\nThe question I had was: How can I implement the Levenshtein algorithm for calculating edit-distance between two strings, as described here, in Delphi?\n\nJust a note on performance:\nThis thing is very fast.  On my desktop (2.33 Ghz dual-core, 2GB ram, WinXP), I can run through an array of 100K strings in less than one second.\n    ", "Answer": "\r\n```\nfunction EditDistance(s, t: string): integer;\nvar\n  d : array of array of integer;\n  i,j,cost : integer;\nbegin\n  {\n  Compute the edit-distance between two strings.\n  Algorithm and description may be found at either of these two links:\n  http://en.wikipedia.org/wiki/Levenshtein_distance\n  http://www.google.com/search?q=Levenshtein+distance\n  }\n\n  //initialize our cost array\n  SetLength(d,Length(s)+1);\n  for i := Low(d) to High(d) do begin\n    SetLength(d[i],Length(t)+1);\n  end;\n\n  for i := Low(d) to High(d) do begin\n    d[i,0] := i;\n    for j := Low(d[i]) to High(d[i]) do begin\n      d[0,j] := j;\n    end;\n  end;\n\n  //store our costs in a 2-d grid  \n  for i := Low(d)+1 to High(d) do begin\n    for j := Low(d[i])+1 to High(d[i]) do begin\n      if s[i] = t[j] then begin\n        cost := 0;\n      end\n      else begin\n        cost := 1;\n      end;\n\n      //to use \"Min\", add \"Math\" to your uses clause!\n      d[i,j] := Min(Min(\n                 d[i-1,j]+1,      //deletion\n                 d[i,j-1]+1),     //insertion\n                 d[i-1,j-1]+cost  //substitution\n                 );\n    end;  //for j\n  end;  //for i\n\n  //now that we've stored the costs, return the final one\n  Result := d[Length(s),Length(t)];\n\n  //dynamic arrays are reference counted.\n  //no need to deallocate them\nend;\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit the distance between two inline elements\r\n                \r\nAt present I have 2 columns of content displaying inline on the same line.\n\nThe next challenge I am trying to overcome is getting these two elements to be closer together on that same line.\n\nI think I may have found the answer here, but not really sure what it means. If this is the correct way of achieving said outcome, an explanation of the contents would be very helpful: Two column width 50% css\n\nAlternatively, I have the current CSS set up on my stylesheet: \n\n\r\n\r\n```\n.fields-1 {\r\n\tfloat: left;\r\n\twidth: 46%;\r\n\ttext-align: center;\r\n\tmargin: auto auto auto 0;\r\n}\r\n\r\n.fields-2 {\r\n\tfloat: right;\r\n\ttext-align: center;\r\n\tdisplay: inline;\r\n\twidth: 46%;\r\n\tpadding-top: 5px;\r\n\tpadding-left: -15px;\r\n\tmargin: auto 0 auto auto;\r\n}\r\n\r\n.fields-2 p {\r\n\tfont-size: 25px;\r\n\tfont-weight: 500px;\r\n}\r\n\r\n#disclaimer {\r\n\tfont-size: 16px;\r\n\tline-height: 17px;\r\n\tfont-family: calibri;\r\n\tfont-style: strong;\r\n\tpadding-bottom: 15px;\r\n\twidth: 45%;\r\n}\r\n\r\n#your-name {\r\n\twidth: 45%;\r\n\tmargin-right: 2px;\r\n}\r\n\r\n#your-email {\r\n\twidth: 45%;\r\n\tmargin-right: 2px;\r\n}\r\n\r\n#NewsletterOptions {\r\n\twidth: 45%;\r\n\theight: 45px;\r\n}```\n\r\n```\n<div class=\"fields-1\">\r\n<p style=\"text-align: center\">[text* your-name id:your-name placeholder: \"Team Name/Filmmaker\"] <b>(required)</b></p>\r\n\r\n<p style=\"text-align: center\">[email* your-email id:your-email placeholder: \"Email Address\"] <b>(required)</b></p>\r\n<p id=\"disclaimer\">*Your e-mail helps us discuss your contribution with you; this email will not be used for any third party or internal promotions without consent.</p>\r\n</div>\r\n<div class=\"fields-2\">\r\n<p>Would you like 3 new short films to watch each month? </p> <br>\r\n[select* NewsletterOptions id:NewsletterOptions \"Yes sure, sounds good!\" \"Not at the moment, thank you.\" \"Already signed up.\"]\r\n\r\n</div>```\n\r\n\r\n\r\n\n\nJust for context it is 1 half of a contact form.\n\nAny feedback or information, you can provide on this matter would be much appreciated. \n\nAlso, if you cold suggest a way for me to line up the Disclaimer text at the bottom with the other elements in that first '.div' I would be most grateful.\n\nKind regards,\n\nDan\n    ", "Answer": "\r\nYou can put a wrapper around the two divs and use flexbox, which will give you a column-like distribution of the two divs next to each other:\n\n```\n.wrapper {\n      display: flex; \n      justify content: space-around;\n}\n```\n\n\nIn the snippet below I erased some superfluous stuff - not sure what you'd want to keep and what not.\n\n\r\n\r\n```\n.wrapper {\r\n  display: flex; \r\n  justify content: center;\r\n }\r\n.fields-1 {\r\n\twidth: 46%;\r\n\ttext-align: center;\r\n}\r\n\r\n.fields-2 {\r\n\ttext-align: center;\r\n\twidth: 46%;\r\n\tpadding-top: 5px;\r\n}\r\n\r\n.fields-2 p {\r\n\tfont-size: 25px;\r\n\tfont-weight: 500px;\r\n}\r\n\r\n#disclaimer {\r\n\tfont-size: 16px;\r\n\tline-height: 17px;\r\n\tfont-family: calibri;\r\n\tfont-style: strong;\r\n\tpadding-bottom: 15px;\r\n}\r\n\r\n#your-name {\r\n\tmargin-right: 2px;\r\n}\r\n\r\n#your-email {\r\n\tmargin-right: 2px;\r\n}\r\n\r\n#NewsletterOptions {\r\n\twidth: 45%;\r\n\theight: 45px;\r\n}```\n\r\n```\n<div class=\"wrapper\">\r\n<div class=\"fields-1\">\r\n<p style=\"text-align: center\">[text* your-name id:your-name placeholder: \"Team Name/Filmmaker\"] <b>(required)</b></p>\r\n\r\n<p style=\"text-align: center\">[email* your-email id:your-email placeholder: \"Email Address\"] <b>(required)</b></p>\r\n<p id=\"disclaimer\">*Your e-mail helps us discuss your contribution with you; this email will not be used for any third party or internal promotions without consent.</p>\r\n</div>\r\n<div class=\"fields-2\">\r\n<p>Would you like 3 new short films to watch each month? </p> <br>\r\n[select* NewsletterOptions id:NewsletterOptions \"Yes sure, sounds good!\" \"Not at the moment, thank you.\" \"Already signed up.\"]\r\n\r\n</div>\r\n  </div>```\n\r\n\r\n\r\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "two whole texts similarity using levenshtein distance [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is off-topic. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\r\n                \r\n                    \r\n                        Closed 10 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI have two text files which I'd like to compare. What I did is:\n\n\nI've split both of them into sentences.\nI've measured levenshtein distance between each of the sentences from one file with each of the sentences from second file.\n\n\nI'd like to calculate average similarity between those two text files, however I have trouble to deliver any meaningful value - obviously arithmetic mean (sum of all the distances [normalized] divided by number of comparisions) is a bad idea.\n\nHow to interpret such results?\n\nedit:\nDistance values are normalized.\n    ", "Answer": "\r\nThe levenshtein distances has a maximum value, i.e. the max. length of both input strings. It cannot get worse than that. So a normalized similarity index (0=bad, 1=match) for two strings a and b can be calculated as 1- distance(a,b)/max(a.length, b.length).\n\nTake one sentence from File A. You said you'd compare this to each sentence of File B. I guess you are looking for a sentence out of B which has the smallest distance (i.e. the highest similarity index).\n\nSimply calculate the average of all those 'minimum similarity indexes'. This should give you a rough estimation of the similarity of two texts.\n\nBut what makes you think that two texts which are similar might have their sentences shuffled? My personal opinion is that you should also introduce stop word lists, synonyms and all that.\n\nNevertheless: Please also check trigram matching which might be another good approach to what you are looking for.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is Levenshtein distance related to largest common subsequence?\r\n                \r\nI don't have proof but i have gut feeling that , suppose s1 is string which needs to be converted to s2 then we can keep the largest common subsequence in s1 as it is and edit distance  is number of elements we need to replace/remove/insert.\n\n```\nFor example : s1 = \"adjsjvnejnv\"\n              s2 = \"djpppne\"\n\n```\n\n\nHere LCS is \"djne\" , now we need to remove 3 element string \"jnv\" at right side of \"djne\" ,we can replace \n\"sjv\" with \"ppp\" in s1 and and we can delete \"a\" from s1. so total edit distance is 3+3+1 = 7 .\n\nIdea is to replace or delete elements inbetween the elements of LCS and add or remove elements\nfrom right and left part of LCS .\n\nI am not able to prove it . Can someone provide counterexample or proof ?\n\nNote that i am not talking about LCS distance (which involves deletion and insertion) , i am talking about LCS and saying can we fill / replace / remove in between the sequence and left and right side of sequence .\n    ", "Answer": "\r\nYes, it is.\nBoth the Levenshtein and the LCS distances are part of a group of distances called edit distances.  \n\n\nLCS distance allows for insertion and deletions in the strings.  \nLevenshtein distance allows for insertion, deletion and substitution in the strings.  \n\n\nBoth of them can be computed using the Wagner-Fischer algorithm (originally published by Damerau in 1964) which is a dynamic programming algorithm that computes the edit distance between two strings.\nThe only difference between the LCS distance and the Levenshtein distance will then be the 'cost function' to minimize used in the dynamic programming.  \n\nNevertheless, the LCS is easier to compute than the Levenshtein distances, and there exists several LCS algorithms taking adavantage of properties of the cost function to improve dramatically the performances of LCS algorithms.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Adding exceptions to Levenshtein-Distance-like algorithm\r\n                \r\nI'm trying to compute how similar a sequence of up to 6 variables are. Currently I'm using a Collections Counter to return the frequency of different variables  as my edit-distance.\n\nBy default, the distance in editing a variable (add/sub/change) is either 1 or 0. I'd like to change the distance depending on the variable and what value I set for that variable.\n\nSo I can say certain variables are similar to other variables, and provide a value for how similar they are. \nI also want to say certain variables are worth less or more distance than usual.\n\nHere is my previous post as context: Modify Levenshtein-Distance to ignore order\n\nExample:\n\n```\n# 'c' and 'k' are quite similar, so their distance from eachother is 0.5 instead of 1\n>>> groups = {['c','k'] : 0.5}\n\n# the letter 'e' is less significant, and 'x' is very significant\n>>> exceptions = {'e': 0.3, 'x': 1.5}\n\n>>> distance('woke', 'woc')\n0.8\n```\n\n\nExplanation:\n\n```\nwoke\nk -> c = 1\nwoce\n-e = 1\nwoc\nDistance = 2\n\n# With exceptions:\nwoke\nk -> c = 0.5\nwoce\n-e = 0.3\nwoc\nDistance = 0.8\n```\n\n\nHow could I achieve this? Would this be possible to implement with this Counter algorithm?\n\nCurrent code (thank you David Eisenstat)\n\n```\ndef distance(s1, s2):\n    cnt = collections.Counter()\n    for c in s1:\n        cnt[c] += 1\n    for c in s2:\n        cnt[c] -= 1\n    return sum(abs(diff) for diff in cnt.values()) // 2 + \\\n        (abs(sum(cnt.values())) + 1) // 2\n```\n\n    ", "Answer": "\r\nGenerally, this is the assignment problem. You have a set of characters from one string, a set of characters fro another string, and a set of costs for assigning a character from one string to a character in another string. You can add some dummy characters to both strings to treat the add/delete operations.\n\nThere are many known algorithms for the assignment problem. One of them is the Hungarian algorithm, the linked Wikipedia article contains links to some of the implementations. Other method is reducing the assignment problem to maxflow-mincost problem, which you can find simpler to adjust for add/delete operations.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Find minimum Levenshtein Distance between one word and an array of thousands\r\n                \r\nSo my users wrote their addresses in a registration form, but a lot of them have typos. I have another list retrieved from the city records with the correct spelling of those addresses. So let's say I have \"Brooklny\" typed by them and I have the list of correct names: Brooklyn, Manhattan, Bronx, Staten Island, Queens (this is an example, the actual addresses are in Spanish and refer to neighborhoodS in Mexico City).\n\nI want to find the edit distance between Brooklyn and each of the borough names and then find the word to whick Brooklyn has the minimum edit distance.\n\nSo edit distance between: Brooklny-Brooklyn is 2, Brooklny-Bronx is 4 and so on. The minimum of course is 2 with Brooklyn. \n\nImagine that I have Brooklny in cell A1 and Brooklyn, Manhattan, Bronx, Staten Island, Queens each in a cell from B1:B6\n\nIm doing this in VBA for a user defined function in Excel and so far I have this code but it doesnt work.\n\n```\nFunction Minl(ByVal string1 As String, ByVal correctos As Range) As Variant\n\nDim distancias(3) As Integer\nDim i, minimo As Integer\ni = 0\nFor Each c In correctos.Cells\n    distancias(i) = Levenshtein(string1, c.Value)\n    i = i + 1\nNext c\n\nMinl = Minrange(distancias)\n\nEnd Function\n\nFunction Levenshtein(ByVal string1 As String, ByVal string2 As String) As Long\n\nDim i As Long, j As Long\nDim string1_length As Long\nDim string2_length As Long\nDim distance() As Long\n\nstring1_length = Len(string1)\nstring2_length = Len(string2)\nReDim distance(string1_length, string2_length)\n\nFor i = 0 To string1_length\ndistance(i, 0) = i\nNext\n\nFor j = 0 To string2_length\n    distance(0, j) = j\nNext\n\nFor i = 1 To string1_length\n    For j = 1 To string2_length\n        If Asc(Mid$(string1, i, 1)) = Asc(Mid$(string2, j, 1)) Then\n            distance(i, j) = distance(i - 1, j - 1)\n        Else\n            distance(i, j) = Application.WorksheetFunction.Min _\n            (distance(i - 1, j) + 1, _\n            distance(i, j - 1) + 1, _\n            distance(i - 1, j - 1) + 1)\n        End If\n    Next\nNext\n\nLevenshtein = distance(string1_length, string2_length)\n\nEnd Function\n\nFunction Minrange(ParamArray values() As Variant) As Variant\nDim minValue, Value As Variant\nminValue = values(0)\nFor Each Value In values\n   If Value < minValue Then minValue = Value\nNext\nMinrange = minValue\nEnd Function\n```\n\n\nI think the algorithm is right but I think I might be having trouble with the syntax. The levenshtein function works but im not sure about the other two.\n    ", "Answer": "\r\nTo get the closest output you could use this:\n\n```\nFunction get_match(ByVal str As String, rng As Range) As String\n  Dim itm As Variant, outp(0 To 2) As Variant\n  outp(1) = 0: outp(2) = \"\"\n  For Each itm In rng.Text\n    outp(0) = Levenshtein(itm, str)\n    If outp(0) = 0 Then\n      get_match = itm\n      Exit Function\n    ElseIf outp(1) = 0 Or outp(0) < outp(1) Then\n      outp(1) = outp(0)\n      outp(2) = itm\n    End If\n  Next\n  get_match = outp(1)\nEnd Function\n```\n\n\nto get the distance later, you simply could run an ```\nLevenshtein(string,get_match(string,range))```\n\n\nStill... I'm not exactly sure what you are looking for :/\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Matching Oracle duplicate column values using Soundex, Jaro Winkler and Edit Distance (UTL_MATCH)\r\n                \r\nI am trying to find a reliable method for matching duplicate person records within the database. The data has some serious data quality issues which I am also trying to fix but until I have the go-ahead to do so I am stuck with the data I have got.\n\nThe table columns available to me are:\n\n```\nSURNAME       VARCHAR2(43)\nFORENAME      VARCHAR2(38)\nBIRTH_DATE    DATE\nADDRESS_LINE1 VARCHAR2(60)\nADDRESS_LINE2 VARCHAR2(60)\nADDRESS_LINE3 VARCHAR2(60)\nADDRESS_LINE4 VARCHAR2(60)\nADDRESS_LINE5 VARCHAR2(60)\nPOSTCODE      VARCHAR2(15)\n```\n\n\nThe ```\nSOUNDEX```\n function is relatively limited for this use but the ```\nUTL_MATCH```\n package seems to offer a better level of matching using the Jaro Winker algorithm.\n\nRather than re-inventing the wheel, has anyone implemented a reliable method for matching this type of data?\n\nData Quality issues to contend with:\n\n\nThe postcode, though mandatory, isn't always fully entered.\nThe address data is relatively poor quality with addresses entered in no fixed format (i.e. some may have line1 as \"Flat 1\" whereas some may have line1 as \"Flat1, 22 Acacia Ave\").\nThe forename column can contain an initial, a full forename or sometimes more than one forename.\n\n\nFor example I was considering:\n\nConcatenating all address fields and applying the Jaro Winkler algorithm to the full address combined with a similar test of the full name concatenated together.\n\nThe birth date can be compared directly for a match but due to the large volume of data just matching upon this isn't enough. \n\nOracle 10g R2 Enterprise Edition.\n\nAny helpful suggestions welcome. \n    ", "Answer": "\r\n\n  \"I am trying to find a reliable method for matching duplicate person\n  records within the database.\"\n\n\nAlas there is no such thing.  The most you can hope for is a system with a reasonable element of doubt. \n\n```\nSQL> select n1\n       , n2\n       , soundex(n1) as sdx_n1\n       , soundex(n2) as sdx_n2\n       , utl_match.edit_distance_similarity(n1, n2) as ed\n       , utl_match.jaro_winkler_similarity(n1, n2) as jw   \nfrom t94\norder by n1, n2\n/\n\n\n  2    3    4    5    6    7    8    9  \nN1                   N2                   SDX_ SDX_         ED         JW\n-------------------- -------------------- ---- ---- ---------- ----------\nMARK                 MARKIE               M620 M620         67         93\nMARK                 MARKS                M620 M620         80         96\nMARK                 MARKUS               M620 M622         67         93\nMARKY                MARKIE               M620 M620         67         89\nMARSK                MARKS                M620 M620         60         95\nMARX                 AMRX                 M620 A562         50         91\nMARX                 M4RX                 M620 M620         75         85\nMARX                 MARKS                M620 M620         60         84\nMARX                 MARSK                M620 M620         60         84\nMARX                 MAX                  M620 M200         75         93\nMARX                 MRX                  M620 M620         75         92\n\n11 rows selected.\n\nSQL> SQL> SQL> \n```\n\n\nThe big advantage of SOUNDEX is that it tokenizes the string.  This means it gives you something which can be indexed: this is incredibly valuable when it comes to large amounts of data.  On the other hand it is old and crude.  There are newer algorithms around, such as Metaphone and Double Metaphone.  You should be able to find PL/SQL implemenations of them via Google.\n\nThe advantage of scoring is that they allow for a degree of fuzziness; so you can find all rows ```\nwhere name_score >= 90%```\n.  The crushing disadvantage is that the scores are relative and so you cannot index them.  This sort of comparison kills you with large volumes.\n\nWhat this means is:\n\n\nYou need a mix of strategies.  No single algorithm will solve your problem.\nData cleansing is useful.  Compare the scores for MARX vs MRX and M4RX: stripping numbers out of names improves the hit rate.\nYou cannot score big volumes of names on the fly.  Use tokenizing and pre-scoring if you can.  Use caching if you don't have a lot of churn.  Use partitioning if you can afford it.  \nUse a Oracle Text (or similar) to build a thesaurus of nicknames and variants. \nOracle 11g introduced specific name search functionality to Oracle Text. Find out more.\nBuild a table of canonical names for scoring and link actual data records to that.\nUse other data values, especially indexable ones like date of birth, to pre-filter large volumes of names or to increase confidence in proposed matches.\nBe aware that other data values come with their own problems: is someone born on 31/01/11 eleven months old or eighty years old? \nRemember that names are tricky, especially when you have to consider names which have been romanized: there are over four hundred different ways of spelling Moammar Khadaffi (in the roman alphabet) - and not even Google can agree on which variant is the most canonical.\n\n\nIn my experience concatenating the tokens (first name, last name) is a mixed blessing.  It solves certain problems (such as whether the road name appears in address line 1 or address line 2) but causes other problems: consider scoring GRAHAM OLIVER vs OLIVER GRAHAM against scoring OLIVER vs OLIVER, GRAHAM vs GRAHAM, OLIVER vs GRAHAM and GRAHAM vs OLIVER.  \n\nWhatever you do you will still end up with false positives and missed hits.  No algorithm is proof against typos (although Jaro Winkler did pretty good with MARX vs AMRX).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Deletion distance between 2 strings\r\n                \r\nI'm having trouble figuring out the algorithm for this problem. I'll paste the problem description and how I kind of solved it, although it's not the correct solution.\nIt is similar to the edit distance algorithm and I used the same approach, but something is off and i cannot figure out exactly what\n\n\n  The deletion distance between two strings is the minimum sum of ASCII\n  values of characters that you need to delete in the two strings in\n  order to have the same string. The deletion distance between cat and\n  at is 99, because you can just delete the first character of cat and\n  the ASCII value of 'c' is 99. The deletion distance between cat and\n  bat is 98 + 99, because you need to delete the first character of both\n  words. Of course, the deletion distance between two strings can't be\n  greater than the sum of their total ASCII values, because you can\n  always just delete both of the strings entirely. Implement an\n  efficient function to find the deletion distance between two\n  strings.You can refer to the Wikipedia article on the algorithm for\n  edit distance if you want to. The algorithm there is not quite the\n  same as the algorithm required here, but it's similar.\n\n\nThis is my code. I used a dynamic programming approach.\nI would say the line after the last \"else\" needs to be changed, but feel free to correct any mistake\n\n```\ndef delete_distance(s1, s2):\n    m = [[0 for j in range(len(s2)+1)] for i in range(len(s1)+1)]\n    for i in range(len(s1)+1):\n        for j in range(len(s2)+1):\n            if i == 0:\n                m[i][j] = sum(bytearray(s2[:j]))\n            elif j == 0:\n                m[i][j] = sum(bytearray(s1[:i]))\n            elif s1[i-1] == s2[j-1]:\n                m[i][j] = m[i-1][j-1]\n            else:\n                m[i][j] = ord(s1[i-1]) + ord(s2[j-1]) + min(m[i-1][j-1], m[i-1][j], m[i][j-1])\n    return m[len(s1)][len(s2)]\n```\n\n\nI know it's wrong because the output of delete_distance('cat', 'cbat') is 197, and the correct result should be 98, because we only need to delete b which has an ASCII value of 98.\n    ", "Answer": "\r\nAs mentioned in the previous answer by Ken Y-N, the else part should be minimum of 3 operations cost.\nThe only change in this answer is - it is rephrased to suite your problem.\n\nThe 3 operations are:\n\n\nS1 delete\nS2 delete\nBoth S1 & S2 delete\n\n\nThe following should work - I guess:\n\n```\ndef delete_distance(s1, s2):\n    m = [[0 for j in range(len(s2)+1)] for i in range(len(s1)+1)]\n    for i in range(len(s1)+1):\n        for j in range(len(s2)+1):\n            if i == 0:\n                m[i][j] = sum(bytearray(s2[:j]))\n            elif j == 0:\n                m[i][j] = sum(bytearray(s1[:i]))\n            elif s1[i-1] == s2[j-1]:\n                m[i][j] = m[i-1][j-1]\n            else:\n                s1del = ord(s1[i-1])\n                s2del = ord(s2[j-1])\n                s1s2del = s1del + s2del\n                m[i][j] = min(m[i-1][j-1] + s1s2del, m[i-1][j] + s1del, m[i][j-1] + s2del)\n    return m[len(s1)][len(s2)]\n```\n\n\nHope it helps!\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Python implementation of a graph-similarity-grading algorithm\r\n                \r\nI am looking for a Python implementation of an algorithm which performs the following task:  \n\nGiven two directed graphs, that may contain cycles, and their roots,\nproduce a score to the two graphs' similarity.\n\n(The way that Python's ```\ndifflib```\n can perform for two sequences)\n\nHopefully, such an implementation exists. Otherwise, I'll try and implement an algorithm myself. In which case, what's the preferable algorithm to implement (with regard to simplicity).\n\nThe way the algorithm works is of no importance to me, though its' complexity is.\nAlso, an algorithm which works with a different data-structure is also acceptable, as long as a graph, such as I described, can be represented with this DS.\n\nI'll emphasize, an implemetation would be much nicer.\n\nEdit:\nIt seems an isomorphism algortihm is not relevant. It was suggested that graph edit distance is more to the point, which narrows down my search to a solution that either executes graph edit distance or reduces a graph to a tree and then performs tree edit distance.\nThe nodes themseleves consist of a few lines of assembly code each.\n    ", "Answer": "\r\nAnother method is to use what is called Eigenvector Similarity.  Basically, you calculate the Laplacian eigenvalues for the adjacency matrices of each of the graphs.  For each graph, find the smallest k such that the sum of the k largest eigenvalues constitutes at least 90% of the sum of all of the eigenvalues.  If the values of k are different between the two graphs, then use the smaller one.  The similarity metric is then the sum of the squared differences between the largest k eigenvalues between the graphs.  This will produce a similarity metric in the range [0, ∞), where values closer to zero are more similar.\n\nFor example, if using ```\nnetworkx```\n:\n\n```\ndef select_k(spectrum, minimum_energy = 0.9):\n    running_total = 0.0\n    total = sum(spectrum)\n    if total == 0.0:\n        return len(spectrum)\n    for i in range(len(spectrum)):\n        running_total += spectrum[i]\n        if running_total / total >= minimum_energy:\n            return i + 1\n    return len(spectrum)\n\nlaplacian1 = nx.spectrum.laplacian_spectrum(graph1)\nlaplacian2 = nx.spectrum.laplacian_spectrum(graph2)\n\nk1 = select_k(laplacian1)\nk2 = select_k(laplacian2)\nk = min(k1, k2)\n\nsimilarity = sum((laplacian1[:k] - laplacian2[:k])**2)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Efficient way to sort an array with a condition\r\n                \r\nI have a numpy array:\n\n```\na = np.array([\"dcba\", \"abc\", \"bca\", \"bcda\", \"tda\", \"a\"])\n```\n\n\nNow I have a vectorized Levenshtein edit distance function which measures distance of given string with given array, for example, for string ```\nab```\n:\n\n```\nl_distv(\"ab\", a)\n```\n\n\nreturns:\n\n```\narray([3, 1, 3, 4, 3, 1])\n```\n\n\nI'd like to sort an array in a way so that any element with edit distance smaller than 2 moves to first positions, while the rest are moved behind them without changing their order. So result would be:\n\n```\narray([\"abc\", \"a\", \"dcba\", \"bca\", \"bcda\", \"tda\"])\n```\n\n\nI've done this, but it's pretty ugly, I assume there is a more efficient way. \n    ", "Answer": "\r\nAdd the elements and the edit distances in a dictionary \n\n```\ndictionary = dict(zip(a,array))\n```\n\n\nthen sort the dictionary according to the edit distance\n\n```\nsorted_dictionary = sorted(dictionary.items(), key=operator.itemgetter(1))\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Why my levenshtein distance calculator fails with PDF file?\r\n                \r\nI'm trying to create a program that calculate edit distance between two files. I read with the funcution fread and I use the code to read binary  (\"rb\"). I put in input two PDF files and during the debug I found out that when I try to fill the matrix of the Levenshtein distance algorithm I get a \"SIGSEGV (Segmentation fault)\" at char n° 1354 of the first file and the program exit with:\n\nProcess finished with exit code -1073741819 (0xC0000005)\n\nI controlled and char n° 1354 is \\n .\nThe code that I use to read the files is:\n```\nlong getFileSize(FILE *file) {\nlong int size;\nfseek(file, 0, SEEK_END);\nsize = ftell(file);\nfseek(file, 0, SEEK_SET);\nreturn size;\n}\n\nchar *readFromBinary(char *path) {\nFILE *file;\nfile = fopen(path, \"rb\");\nif (file == NULL)\n    printf(\"Error!\\n\");\n\nlong fileSize = getFileSize(file);\nchar *buffer = malloc((fileSize + 1) * sizeof(char));\n\nfread(buffer, sizeof(char), fileSize, file);\nreturn buffer;\n}\n```\n\nThis is the code that I use to calculate the edit distance:\n```\nint calculateDistance(char *pathFile1, char *pathFile2, int choice, char *path) {\nFILE *f1 = fopen(pathFile1, \"rb\");\nFILE *f2 = fopen(pathFile2, \"rb\");\nchar *contentFile1 = readFromBinary(pathFile1);\nchar *contentFile2 = readFromBinary(pathFile2);\n\nint distance = 0;\nint dim1 = getFileSize(f1);\nint dim2 = getFileSize(f2);\n\nint **matrix = constructMatrix(dim1, dim2);\nfillMatrix(matrix, dim1, dim2, contentFile1, contentFile2);\n\ndistance = matrix[dim1][dim2];\nstruct Instruction instruction[distance + 1];\n\nint initActions = initInstructions(matrix, pathFile1, &dim1, pathFile2, &dim2, instruction);\nendInstructions(pathFile1, &dim1, pathFile2, &dim2, instruction, initActions);\n\nif (choice == 1)\n    printOnFile(instruction, distance, path);\n\nfor (int i = 0; i <= dim1; i++)\n    free(matrix[i]);\nfree(matrix);\n\nif (numberOfDivisions > 0)\n    numberOfDivisions--;\n\nreturn distance;\n}\n```\n\nAnd this is the code that i use to create and fill the matrix:\n```\nint **constructMatrix(int dim1, int dim2) {\n//matrice di puntatori\nint **matrice = (int **) malloc((dim1 + 1) * sizeof(int *));\n\n//matrice di puntatori\nfor (int i = 0; i <= dim1; i++)\n    matrice[i] = (int *) malloc((dim2 + 1) * sizeof(int));\n\nreturn matrice;\n}\n\n void fillMatrix(int **matrix, int dim1, int dim2, char *file1, char *file2) {\n  for (int i = 0; i <= dim1; i++)\n    matrix[i][0] = i;\n  for (int j = 1; j <= dim2; j++)\n    matrix[0][j] = j;\n  for (int i = 1; i <= dim1; i++) {\n    for (int j = 1; j <= dim2; j++) {\n        if (file1[i - 1] != file2[j - 1]) {\n            int k = minimum(matrix[i][j - 1], matrix[i - 1][j], matrix[i - 1][j - 1]);\n            matrix[i][j] = k + 1;\n        } else\n            matrix[i][j] = matrix[i - 1][j - 1];\n    }\n  }\n}\n```\n\nIn particular the debugger stops in this line of calculateDistance(```\nfillMatrix(matrix, dim1, dim2, contentFile1, contentFile2);```\n), and in this line of fillMatrix(```\nmatrix[i][0] = i;```\n) when i=1354.\nInformation about PDF:\nThe PDF file is 188671 byte\nIt has 1355 lines\nPS. My program works with txt files.\n    ", "Answer": "\r\nWhen any of the memory allocation functions, including malloc, calloc,  and realloc() make a request to the OS to obtain memory, unless the OS can find a single block of contiguous memory of the size requested, the function will return ```\nNULL```\n.  Since you are asking for a block of incredible size, it is likely to fail.\nIt is always recommended that the return of any of these functions is tested before attempting to use the value that was returned:\n```\nchar *buffer = malloc((fileSize + 1) * sizeof(char));\nif(!buffer)\n{\n    //handle error\n```\n\nAnd in this case, it would be good to re-evaluate your algorithm.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Best string-comparison algorithm for regex\r\n                \r\nGiven a regex, I want to compare it with a list of other regex, and output a similarity score.\n\nThere are several edit distance algorithms out there (e.g. levenshtein distance), but they fail to compare regex's, e.g.:\n\n```\nR1:       [a-z0-9]+\nR2:       [0-9]{1}[a-z0-9]+\nDistance: 9\n```\n\n\nIn the example above, both regex's are quite similar, however they have a quite high edit distance. I suppose an approach using character n-grams would be more suitable for such cases.\n\nWhat algorithm/approach would you consider for this problem?\n    ", "Answer": "\r\nIt seems you're unlikely to improve upon the regular expression parsing algorithm present in an engine itself, because you're ultimately going to be making inferences about combinations of rules. \n\nThere are a number of open source regular expression engines, many listed on wikipedia, possibly including the one you're using.\n\nWithout having looked at the internals myself (not an insignificant caveat,) my recommendation is to see if it's possible to modify a regex engine (or leverage some pre-existing debugging or testing code) to output pertinent rules-processing metadata, sub-scores, if you will, from which you can then calculate an aggregate. The engines ultimately do their work deterministically, so this is theoretically possible.\n\nIf it works, this will amongst other things, enable you classify constructs, which you define as similar, with similar weights, and to possibly ignore others entirely.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Find a word in a list of words that has minimum distance with a given word\r\n                \r\nlet say given a word list ```\n['windows','hello','python','world','software','desk']```\n and an input word ```\n'widow'```\n, how to (quickly) find the word from the word list that has the minimum edit distance with the input word ```\n'widow'```\n (The answer in this example is ```\n'windows'```\n)? Are there available libraries/functions to achieve it? Thanks!\n    ", "Answer": "\r\nBuilt-in ```\ndifflib```\n\n```\nimport difflib\ndifflib.get_close_matches(\"widow\", lst, n=1)\n\n#out: ['windows']\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Sort an array of strings by \"uniqueness\"\r\n                \r\nI found the ```\nLevenshtein```\n edit distance algorithm (via the ```\ndamerau-levenshtein```\n gem) and I think it suits my purpose well enough.\n\nThis code compares every element to every other element in the array, adding the result of each comparison to a set of hashes which will be sorted by the ```\n:distance```\n key.\n\nWhen this code is in use, the data in the array is logs from java services, so large edit distances show me which logs are most unique compared to the rest.\n\nInput data is in this form:\n```\n[\"Failed to process service event Error: 404 Not Found\", \"Failed to process service event Error: Resource not found in Storage service\", \"Throughput exceeded for table test-us-east-1-service-table.\"]```\n\n\n```\ndef get_edit_distances(arr)\n  if arr.empty?\n    return []\n  end\n  if arr.length == 1\n    return [arr[0]]\n  end\n  dl = DamerauLevenshtein\n  results = Set.new\n  i = 0 #array position\n  while i < arr.length\n    j = i + 1 #element to compare arr[i] against\n\n    while j < arr.length\n      results.add({message: arr[i], distance: dl.distance(arr[i], arr[j], 1, 256)})\n\n      #This is to make sure we have every element in the final results\n      if j+1 == arr.length \n        results.add({message: arr[j], distance: dl.distance(arr[0], arr[j], 1, 256)})\n        break\n      end\n\n      j += 1 #increment \n    end\n    i += 1\n  end\n  final_results = results.to_a\n  #sort in descending order by distance\n  final_results.sort! {|a,b| b[:distance] <=> a[:distance]}\n  #remove duplicates of messages now that everything is sorted\n  final_results.uniq! {|m| m[:message]}\n  #return array of messages\n  final_results.map {|r| r[:message]}\nend\n```\n\n\nThe output of this code is an array of the messages, ordered by uniqueness:\n```\n[\"Throughput exceeded for table test-us-east-1-service-table.\", \"Failed to process service event Error: Resource not found in Storage service\", \"Failed to process service event Error: 404 Not Found\"]```\n\n\nFor an array of ```\n928```\n elements (normally there will be ~10,000,000), I got an output of ```\n11801```\n elements (there were multiple edit distances for a single result, the ```\nset```\n prevented duplicate messages for the same distance).\n\nBenchmark results for the whole loop:\n\n```\n                    user      system     total       real\nEdit Dist Loop:  62.260000   0.110000  62.370000 ( 62.456783)\n```\n\n\nQuestion: Is there a better way to create a sorted array/set of unique elements, ordered by uniqueness?\n    ", "Answer": "\r\nHopefully I understood your original problem correctly, \"sorting an array of log messages by uniqueness\";find the logs that have the rarest occurrence.\n\nif that is the case, try this:\n\n```\ndef sort_by_uniqueness(arr)\n  h = {}\n  arr.each do |entry|\n    a[entry] = 0 unless a.key?(entry)\n    a[entry] += 1 \n  end\n  a.sort_by { |k, v| v }.map(&:first)\nend\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Django Haystack + Elasticsearch - how to return results with a small edit distance from query?\r\n                \r\nSo we are using Django-haystack with the Elasticsearch backend to index a bunch of data for searching. It is very fast and is working great for the most part, but I notice something that I want that seems to be absent. For example, consider the search query ```\n\"cellar door\"```\n. I would want a query that is slightly off, like a misspelling, e.g. ```\n\"cellar dor\"```\n or ```\n\"celar door\"```\n to match results for ```\n\"cellar door\"```\n. If I try queries like this with our current setup it returns 0 results. I tried using an EdgeNgramField in the search index on the field we wanted to index, but this seems to have absolutely no effect. \n\nThanks.\n    ", "Answer": "\r\nUse suggest to perform spell check.\n\n```\ncurl -XPOST 'localhost:9200/index/_search?search_type=count' -d '{\n\n{\n   \"suggest\": {\n      \"body\": {\n         \"text\": \"celar door\",\n         \"term\": {\n            \"field\": \"summary\",\n            \"analyzer\": \"simple\"\n         }\n      }\n   }\n}'\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Fast Levenshtein Distance calculation against large list\r\n                \r\nI have a reference list of 90,000 strings in a text file. The strings all have a length of 15 chars.\nNow I want to compare about 2 million strings with length 14-16 to the reference list. I want to calculate the Levenshtein-Distance (Edit-Distance) and get the best hit as well as its distance.\nOr different -- the hits have to be of distance 0 or 1 and every larger distance is considered \"no hit\", this might be a way for potential speed up?\nI want to do this in Python (ideally, but not necessarily if another solution like C is needed). I am aware of implementations in Python (like here: https://rawgit.com/ztane/python-Levenshtein/master/docs/Levenshtein.html#Levenshtein-distance) but I fear that this is too slow if I use this function on every comparison.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Matlab best match of a sequence within a matrix\r\n                \r\nI want to find the best match of a sequence of integers within a NxN matrix. The problem is that I don't know how to extract the position of this best match. The following code that I have should calculate the edit distance but I would like to know where in my grid that edit distance is shortest!\n\n```\nfunction res = searchWordDistance(word,grid)\n\n% wordsize = length(word); % extract the actual size\n% [x ,y] = find(word(1) == grid);\nD(1,1,1)=0;\nfor i=2:length(word)+1\n    D(i,1,1) = D(i-1,1,1)+1;\nend\nfor j=2:length(grid)\n    D(1,1,j) = D(1,1,j-1)+1;\n    D(1,j,1) = D(1,j-1,1)+1;\nend\n% inspect the grid for best match\nfor i=2:length(word)\n    for j=2:length(grid)\n        for z=2:length(grid)\n        if(word(i-1)==grid(j-1,z-1))\n            d = 0;\n        else\n            d=1;\n        end\n        c1=D(i-1,j-1,z-1)+d;\n        c2=D(i-1,j,z)+1;\n        c3=D(i,j-1,z-1)+1;\n        D(i,j,z) = min([c1 c2 c3]);\n\n        end\n    end\nend\n```\n\n\nI have used this code (in one less dimension) to compare two strings. \nEDIT Using a 5x5 matrix as example\n\n```\n15 17 19 20 22\n14 8  1  15 24\n11 4  17 3  2\n14 2  1  14 8\n19 23 5  1  22\n```\n\n\nnow If I have a sequence ```\n[4,1,1]```\n and ```\n[15,14,12,14]```\n they should be found using the algorithm. The first one is a perfect match(diagonal starts at (3,2)). The second one is on the first column and is the closest match for that sequence since only one number is wrong. \n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Not Converging CTC Loss and Fluctuating Label Error Rate (Edit Distance) on Single Example?\r\n                \r\nI am trying to overfit a handwriting recognition model of this architecture:\n\n```\nfeatures = slim.conv2d(features, 16, [3, 3])\nfeatures = slim.max_pool2d(features, 2)\nfeatures = mdrnn(features, 16)\nfeatures = slim.conv2d(features, 32, [3, 3])\nfeatures = slim.max_pool2d(features, 2)\nfeatures = mdrnn(features, 32)\nfeatures = slim.conv2d(features, 64, [3, 3])\nfeatures = slim.max_pool2d(features, 2)\nfeatures = mdrnn(features, 64)\nfeatures = slim.conv2d(features, 128, [3, 3])\nfeatures = mdrnn(features, 128)\nfeatures = slim.max_pool2d(features, 2)\nfeatures = slim.conv2d(features, 256, [3, 3])\nfeatures = slim.max_pool2d(features, 2)\nfeatures = mdrnn(features, 256)\nfeatures = _reshape_to_rnn_dims(features)\nfeatures = bidirectional_rnn(features, 128)\nfeatures = bidirectional_rnn(features, 128)\nfeatures = bidirectional_rnn(features, 128)\nfeatures = bidirectional_rnn(features, 128)\nfeatures = bidirectional_rnn(features, 128)\n```\n\n\nWith this mdrnn code from tensorflow (with a few modifications):\n\n```\ndef mdrnn(inputs, num_hidden):\n    with tf.variable_scope(scope, \"multidimensional_rnn\", [inputs]):\n        hidden_sequence_horizontal = _bidirectional_rnn_scan(inputs,\n                                                             num_hidden // 2)\n        with tf.variable_scope(\"vertical\"):\n            transposed = tf.transpose(hidden_sequence_horizontal, [0, 2, 1, 3])\n            output_transposed = _bidirectional_rnn_scan(transposed, num_hidden // 2)\n        output = tf.transpose(output_transposed, [0, 2, 1, 3])\n        return output\n\ndef _bidirectional_rnn_scan(inputs, num_hidden):\n    with tf.variable_scope(\"BidirectionalRNN\", [inputs]):\n        height = inputs.get_shape().as_list()[1]\n        inputs = images_to_sequence(inputs)\n        output_sequence = bidirectional_rnn(inputs, num_hidden)\n        output = sequence_to_images(output_sequence, height)\n        return output\n\ndef images_to_sequence(inputs):\n    _, _, width, num_channels = _get_shape_as_list(inputs)\n    s = tf.shape(inputs)\n    batch_size, height = s[0], s[1]\n    return tf.reshape(inputs, [batch_size * height, width, num_channels])\n\ndef sequence_to_images(tensor, height):\n    num_batches, width, depth = tensor.get_shape().as_list()\n    if num_batches is None:\n        num_batches = -1\n    else:\n        num_batches = num_batches // height\n    reshaped = tf.reshape(tensor,\n                          [num_batches, width, height, depth])\n    return tf.transpose(reshaped, [0, 2, 1, 3])\n\ndef bidirectional_rnn(inputs, num_hidden, concat_output=True,\n                      scope=None):\n    with tf.variable_scope(scope, \"bidirectional_rnn\", [inputs]):\n        cell_fw = rnn.LSTMCell(num_hidden)\n        cell_bw = rnn.LSTMCell(num_hidden)\n        outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n                                                     cell_bw,\n                                                     inputs,\n                                                     dtype=tf.float32)\n        if concat_output:\n            return tf.concat(outputs, 2)\n        return outputs\n```\n\n\nThe training ctc_loss decreases but it doesn't converge even after a thousand epochs. The label error rate just fluctuates.\n\n\n\nI preprocess the image such that it looks like this:\n\n\n\nI also noticed that the network generates the same predictions at some points:\n\n```\nINFO:tensorflow:outputs = [[51 42 70 42 34 42 34 42 34 29 42 29 42 29 42 29 42 29 42 29 42 29 42 29\n  42 29  4 72 42 58 20]] (1.156 sec)\nINFO:tensorflow:labels = [[38 78 52 29 70 51 78  8  1 78 15  8  1 22 78 52  4 24 78 28  3  9  8 15\n  11 14 13 13 78  2  4  1 16]] (1.156 sec)\nINFO:tensorflow:label_error_rate = 0.93939394 (1.156 sec)\nINFO:tensorflow:global_step/sec: 0.888003\nINFO:tensorflow:outputs = [[51 42 70 42 34 42 34 42 34 29 42 29 42 29 42 29 42 29 42 29 42 29 42 29\n  42 29  4 65 42 58 20]] (1.126 sec)\nINFO:tensorflow:labels = [[38 78 52 29 70 51 78  8  1 78 15  8  1 22 78 52  4 24 78 28  3  9  8 15\n  11 14 13 13 78  2  4  1 16]] (1.126 sec)\nINFO:tensorflow:label_error_rate = 0.969697 (1.126 sec)\nINFO:tensorflow:global_step/sec: 0.866796\nINFO:tensorflow:outputs = [[51 42 70 42 34 42 34 42 34 29 42 29 42 29 42 29 42 29 42 29 42 29 42 29\n  42 29  4 65 42 58 20]] (1.154 sec)\nINFO:tensorflow:labels = [[38 78 52 29 70 51 78  8  1 78 15  8  1 22 78 52  4 24 78 28  3  9  8 15\n  11 14 13 13 78  2  4  1 16]] (1.154 sec)\nINFO:tensorflow:label_error_rate = 0.969697 (1.154 sec)\nINFO:tensorflow:global_step/sec: 0.88832\nINFO:tensorflow:outputs = [[51 42 70 42 34 42 34 42 34 29 42 29 42 29 42 29 42 29 42 29 42 29 42 29\n  42 29  4 65 42 58 20]] (1.126 sec)\nINFO:tensorflow:labels = [[38 78 52 29 70 51 78  8  1 78 15  8  1 22 78 52  4 24 78 28  3  9  8 15\n  11 14 13 13 78  2  4  1 16]] (1.126 sec)\nINFO:tensorflow:label_error_rate = 0.969697 (1.126 sec)\n```\n\n\nAny reason why this is happening? Here's a small reproducible example I've made https://github.com/selcouthlyBlue/CNN-LSTM-CTC-HIGH-LOSS\n\nUpdate\n\nWhen I changed the conversion from this:\n\n```\noutputs = tf.reshape(inputs, [-1, num_outputs])\nlogits = slim.fully_connected(outputs, num_classes)\nlogits = tf.reshape(logits, [num_steps, -1, num_classes])\n```\n\n\nTo this:\n\n```\noutputs = tf.reshape(inputs, [-1, num_outputs])\nlogits = slim.fully_connected(outputs, num_classes)\nlogits = tf.reshape(logits, [-1, num_steps, num_classes])\nlogits = tf.transpose(logits, (1, 0, 2))\n```\n\n\nThe performance somehow improved:\n\n(Removed the mdrnn layers here)\n\n\n\n(2nd RUN)\n\n\n\n(Added back the mdrnn layers)\n\n\n\nBut the loss is still not going down to zero (or getting close to it), and the label error rate is still fluctuting.\n\nAfter changing the optimizer from ```\nAdam```\n to ```\nRMSProp```\n with decay rate of ```\n0.9```\n, the loss now converges!\n\n\n\n\n\nBut the label error rate still fluctuates. However, it should go down now with the loss converging.\n\nMore updates\n\nI tried it on the real dataset I have, and it did improve!\n\nBefore\n\n\n\nAfter \n\n\n\nBut the label error rate is increasing for some reason still unknown.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Manhattan Distance and Triangle Inequality\r\n                \r\nThe question came around in CSAcademy Programming Contest Round 51 though. The problem statement reads:-\"Given Manhattan distances a,b and c, produce 3 points in 2D space such that the manhattan distances amongst them satisfies the aforementioned values\".My Approach\nThe distances given should satisfy:-\n\n```\n(a+b+c)%2==0\n```\n\n\nThe reason being:\nSort the distances first such that \n\n```\na<=b<=c\n```\n\n\nThen we have the following\n\n```\n|x2-x1|+|y2-y1|=a\n|x3-x2|+|y3-y2|=b\n|x3-x1|+|y3-y2|=c\n```\n\n\nNow, if 3 points have x1,x2,x3,y1,y2,y3 coordinates such that:-\n\n```\nx1<=x2<=x3\ny1<=y2<=y3\n```\n\n\nThen we can safely open modulus to get:-\n\n```\n2*(x3-x1)+2*(y3-y1)=a+b+c\n```\n\n\nThereafter, I fixed (0,0) and (a,0), and derived the 3rd point as:-\n\n```\nx3=(a+b-c)/2\ny3=(b+c-a)/2\n```\n\n\nHowever, I couldn't solve in the contest because I didn't take care of the fact that after sorting\n\n```\na+b>=c should hold (Triangle inequality over Manhattan Distance)\n```\n\n\nThe code for the same here.Thus, my questions are as follows:-\n\n\nHow does Manhattan distance satisfies triangle inequality?\nDoes any general distance metric hold the triangle inequality? (Like Manhattan, Levehnstein (edit distance), Hamming distance).\nIs the Triangle inequality a necessary condition for all distance measures? (That is it should fundamentally hold).\n\n    ", "Answer": "\r\nI think the meaning is simply that the points you have produced do not satisfy the basic requirements.\n\nI don't believe there is any explicit requirement for the points to satisfy the triangle inequality, this is just an emergent property.\n\nSuppose a=0, b=2, c=4.\n\nYour method will produce points:\n\n```\nx1,y1 = 0,0\nx2,y2 = 0,0\nx3,y3 = -1,3\n```\n\n\nNow distance 1 to 2 is 0, distance 2 to 3 is 4, but distance 1 to 3 is also 4.\n\nThe reason for mentioning the triangle inequality is that in this case you can immediately prove that there can be no solutions because of this inequality.\n\nThe triangle inequality will hold for distance metrics where the distance is defined as a constrained shortest path.  This is because it is equivalent to saying \"going from a to b to c\" is always at least as long as \"going straight from a to c\".\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Convert a string to another string in the shortest path\r\n                \r\nI have two strings, say ```\nstr1```\n and ```\nstr2```\n. I need to convert the first one to the second one while making the least number of edits. This is what is called as Edit Distance. Suppose we need to convert ```\nSunday```\n to ```\nSaturday```\n. The first letter is the same, and the last three are the same as well, so it boils down to converting ```\nun```\n to ```\natur```\n. This can be done in 3 steps - Replace 'n' with 'r', insert 't', insert 'a'. That gives the edit distance as 3. Following is the program to find out the edit distance -\n\n```\n// A Dynamic Programming based C++ program to find minimum\n// number operations to convert str1 to str2\n#include<bits/stdc++.h>\nusing namespace std;\n\n// Utility function to find minimum of three numbers\nint min(int x, int y, int z) \n{\n    return min(min(x, y), z);\n}\n\nint editDistDP(string str1, string str2, int m, int n)\n{\n    // Create a table to store results of subproblems\n    int dp[m+1][n+1];\n\n    // Fill d[][] in bottom up manner\n    for (int i=0; i<=m; i++)\n    {\n        for (int j=0; j<=n; j++)\n        {\n            // If first string is empty, only option is to\n            // isnert all characters of second string\n            if (i==0)\n                dp[i][j] = j;  // Min. operations = j\n\n            // If second string is empty, only option is to\n            // remove all characters of second string\n            else if (j==0)\n                dp[i][j] = i; // Min. operations = i\n\n            // If last characters are same, ignore last char\n            // and recur for remaining string\n            else if (str1[i-1] == str2[j-1])\n                dp[i][j] = dp[i-1][j-1];\n\n            // If last character are different, consider all\n            // possibilities and find minimum\n            else\n                dp[i][j] = 1 + min(dp[i][j-1],  // Insert\n                                   dp[i-1][j],  // Remove\n                                   dp[i-1][j-1]); // Replace\n        }\n    }\n\n    return dp[m][n];\n}\n\n// Driver program\nint main()\n{\n    // your code goes here\n    string str1 = \"sunday\";\n    string str2 = \"saturday\";\n\n    cout << editDistDP(str1, str2, str1.length(), str2.length());\n\n    return 0;\n}\n```\n\n\nWhile this returns the correct result, I also need to output the exact steps of conversion, i.e. something like \n\nSunday -> Surday -> Sturday -> Saturday.\n\nHow do I do the second step?\n    ", "Answer": "\r\nOnce you have created your ```\ndp```\n table, you can work your way back rom ```\n(m, n)```\n to ```\n(0, 0)```\n in the same way as you created the table.\n\nHere's a solution that prints the modifications, but you could also return a vector of modifications.\n\n```\nint editDistDP(string str1, string str2)\n{\n    int m = str1.length();\n    int n = str2.length();\n    int dp[m + 1][n + 1];\n    int i, j;\n\n    for (i = 0; i <= m; i++) {\n        for (j = 0; j <= n; j++) {\n            if (i == 0) {\n                dp[i][j] = j;\n            } else if (j == 0) {\n                dp[i][j] = i;\n            } else if (str1[i-1] == str2[j-1]) {\n                dp[i][j] = dp[i-1][j-1];\n            } else {            \n                dp[i][j] = 1 + min3(dp[i][j - 1],\n                                    dp[i - 1][j],\n                                    dp[i - 1][j - 1]);\n            }\n        }\n    }\n\n    i = m; j = n;\n\n    while (i && j) {\n        if (i == 0) {\n            cout << \"insert \" << str2[j - 1] << endl;\n            j--;\n        } else if (j == 0) {\n            cout << \"remove \" << str1[i - 1] << endl;\n            i--;\n        } else if (str1[i - 1] == str2[j - 1]) {\n            i--; j--;\n        } else {        \n            int k = imin3(dp[i][j - 1],\n                          dp[i - 1][j],\n                          dp[i - 1][j - 1]);\n\n            if (k == 2) {\n                cout << \"replace \" << str1[i - 1] \n                     << \" with \" << str2[j - 1] << endl;\n                i--; j--;\n            } else if  (k == 1) {\n                cout << \"remove \" << str1[i - 1] << endl;\n                i--;\n\n            } else {\n                cout << \"insert \" << str2[j - 1] << endl;\n                j--;\n            }\n        }\n    }\n\n    return dp[m][n];\n}\n```\n\n\nHere, ```\nimin3```\n is a function that returns the index 0, 1 or 2 of the minimum element in the list.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "What's the difference between Levenshtein distance and the Wagner-Fischer algorithm\r\n                \r\nThe Levenshtein distance is a string metric for measuring the difference between two sequences. \nThe Wagner–Fischer algorithm is a dynamic programming algorithm that computes the edit distance between two strings of characters.\n\nBoth using a matrix, and I don't see the difference? \nIs the difference the backtracking or is there no further difference by the fact that one is the \"literature\" and the other one is the programming? \n\nAlso I am just writing on a thesis, and I am not sure how to divide it- should I first get into explaining the Levenshtein distance first and afterwards the Wagner-Fisher algorithm or doing both in one? I got kinda confused here.\n    ", "Answer": "\r\nYou actually answer the question yourself in the first paragraph.\nIn the second paragraph you mix them up a bit. \n\nLevenshtein distance is an edit distance metric named after Vladimir Levenshtein who considered this distance in 1965 and have nothing to do with the dynamic programming \"matrix\". And the Wagner–Fischer algorithm is a dynamic programming algorithm that computes the edit distance between two strings of characters.\n\nHowever, the Levenshtein distance is normally computed using dynamic programming if what you need is a general purpose computation, that is, calculate the edit distance between two random input strings. But Levenshtein distance can also be used in a spell checker, when you compare one string with a dictionary. In cases like this its normally to slow to use a general purpose computation,and something like a  Levenshtein Automaton can provide linear time to get all spelling suggestions. Btw, this is also used in the fuzzy search in Lucene since version 4.\n\nAbout your thesis, well I think it depends. If its about the actual Levenshtein metric then I think thats where you should start, and if its about dynamic programming you should start with Wagner-Fischer. Anyway, thats my two cents about it. And good luck with you thesis.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Fast string retrieval based on metric distance in Java\r\n                \r\nGiven an arbitrary string s, I would like a method to quickly retrieve all strings S ⊆ M from a large set of strings M (where |M| > 1 million), where all strings of S have minimal edit distance < t (some minimum threshold) from s. \n\nAt worst, S may be empty if no strings in M match this criteria, and at best, S = {s} (an exact match). For any case in between, I completely expect that S may be quite large.\n\nIn general, I expect to have the maximum edit distance threshold fixed (e.g., 2), and need to perform this operation very many times over arbitrary strings s, thus the need for an efficient method, as naively iterating and testing all strings would be too expensive. \n\nWhile I have used edit distance as an example metric, I would like to use other metrics as well, such as the Jaccard index.\n\nCan anyone make a suggestion about an existing Java implementation which can achieve this, or point me to the right algorithms and data structures for solving this problem?\n\nUPDATE #1\n\nI have since learned that Metric trees are precisely the kind of structure I am after, which exploits the distance metric to organise subsets of strings in M based on their distance from each other with the metric. Both Vantage-Point, BK and other similar metric tree data structures and algorithms seem ideal for this kind of problem. Now, to find easy-to-use implementations in Java...\n\nUPDATE #2\n\nUsing a combination of this bk-tree and this Levenshtein distance implementation, I'm successfully able to retrieve subsets against arbitrary strings from a set (M) of one million strings with retrieval times of around 10ms.\n    ", "Answer": "\r\nBK trees are designed for such a case. It works with metric distance, such as Levenshtein or Jaccard index.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Why can't I normalize this random unicode string?\r\n                \r\nI need to evaluate levenshtein edit distance on unicode strings, which means that two strings containing identical content will need to be normalized to avoid biasing the edit distance.\n\nHere is how I generate random unicode strings for my tests:\n\n```\ndef random_unicode(length=10):\n    ru = lambda: unichr(random.randint(0, 0x10ffff))\n    return ''.join([ru() for _ in xrange(length)])\n```\n\n\nAnd here is the simple test case that is failing:\n\n```\nimport unicodedata\nuni = random_unicode()\nunicodedata.normalize(uni, 'NFD')\n```\n\n\nAnd here is the error:\n\n```\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 0-9: ordinal not in range(128)\n```\n\n\nI checked to make sure that ```\nuni```\n was, indeed, a unicode object:\n\n```\nu'\\U00020d93\\U000fb2e6\\U0005709a\\U000bc31e\\U00080262\\U00034f00\\U00059941\\U0002dd09\\U00074f6d\\U0009ef7a'\n```\n\n\nCan someone enlighten me?\n    ", "Answer": "\r\nYou've switched the parameters of ```\nnormalize```\n. From the relevant documentation:\n\n\n  ```\nunicodedata.normalize(form, unistr)```\n\n  \n  Return the normal form form for the Unicode string * unistr*. Valid values for form are ‘NFC’, ‘NFKC’, ‘NFD’, and ‘NFKD’.\n\n\nThe first argument is the form, and the second is the string to be normalized. This works just fine:\n\n```\nunicodedata.normalize('NFD', uni)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Vectorizing distance calculation between two list\r\n                \r\nI have two list, for example， \n\n```\nlist A = ['ABCD', 'AC', 'BCD']\nlist B = ['BCDD', 'AB']\n```\n\n\nfor each element in list A, I want to calculate the edit distance (levenshtein distance) to list B and calculate the number of results that smaller than 3. That is, the final result should give a list.\nIn the above example, I expect the result to be: ```\n[1, 1, 1]```\n\n\nMy codes are:\n\n```\nfor element in listA:\n    sub_len = sum(np.array(vdistance(element, listB)) <= 2)\n```\n\n\nThis works for small sample, however, when the size of list A and B are two large, the speed becomes really small. I wonder if there is any vectorized method to achieve my goal, thanks!\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "efficiently compute the edit distance between 1 string and a large set of other strings?\r\n                \r\nThe use case is auto-complete options where I want to rank a large set of other strings  by how like a fixed string they are.\n\nIs there any bastardization of something like a DFA RegEx that can do a better job than the start over on each option solution?\n\nThe guy who asked this question seems to know of a solution but doesn't list any sources.\n\n(p.s. \"Read this link\" type answer welcome.)\n    ", "Answer": "\r\nI did something like this recently. Unfortunately it's closed source.\n\nThe solution is to write a levenshtein automaton. Spoiler: it's a NFA. \n\n\n\nAlthough many people will try to convince you that simulating NFAs is exponential, it isn't. Creating a DFA from NFA is exponential. Simulating is just polynomial. Many regex engines are writen with sub-optimal algorithms based on this.\n\nNFA simulation is O(n*m) for a n-sized string and m states. Or O(n) amortized if you convert it to a DFA lazily (and cache it).\n\nI'm afraid you'll either have to deal with complex automata libraries or will have to write a lot of code (what I did).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Editing elements of a list of tuples based on hamming distance\r\n                \r\nI have a list of tuples:\n\n```\nf_list = [('AGCTCCCCGTTTTC', 34), ('TTCATTCCTCTCTC', 1), ('AGCTCCCCGGTTTC', 1)]\n```\n\n\nIf the hamming distance between any two strings is less than 3, I would like to merge the elements by adding the second entries of each element. If the above condition is not satisfied, I want to keep the element as it is. The output I want is:\n\n```\nf_list = [('AGCTCCCCGTTTTC', 35),('TTCATTCCTCTCTC', 1)]\n```\n\n\nI have the  function for Hamming distance:\n\n```\ndef hamming(s1, s2):\n   if len(s1) != len(s2):\n     raise ValueError(\"Undefined for sequences of unequal length\")  \nreturn sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))\n```\n\n\nI used the following to iterate through the list to identify the relevant elements, but I am not sure how to modify the original list:\n\n```\nfor e in f_list:\n    [item for item in f_list if hamming(e[0],item[0]) < 3]\n\nOutput: \n[('AGCTCCCCGTTTTC', 34), ('AGCTCCCCGGTTTC', 1)]\n[('TTCATTCCTCTCTC', 1)]\n[('AGCTCCCCGTTTTC', 34), ('AGCTCCCCGGTTTC', 1)]\n```\n\n    ", "Answer": "\r\nMy assumptions are that once an element has been merged, it doesn't need to be checked. I've added three extra elements, one that should match and another that will not match any, and one that will match the first element (so there is an element that has more than one match) to make the test a little bit more robust.\n\n```\nimport itertools # we'll use islice in case dataset is large and limited memory\n\ndef hamming(s1, s2):\n    if len(s1) != len(s2):\n        raise ValueError(\"Undefined for sequences of unequal length\")\n    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))    \n\n\nf_list = [('AGCTCCCCGTTTTC', 34), ('TTCATTCCTCTCTC', 1), ('GGGGCCCCCAAAAA', 99), \n          ('TGCATTCCTATCTC', 8), ('AGCTCCCCGGTTTC', 1), ('AGCTCCCCGTCTTC', 100)]\n\n\ndef merge_elements(f_list):\n    result = []\n    flag = [True for _ in f_list]\n\n    for i, seq1 in enumerate(f_list):\n        none_close = True\n        total = seq1[1]\n        for j, seq2 in enumerate(itertools.islice(f_list, i+1, len(f_list))):\n            if flag[i+j+1] and hamming(seq1[0],seq2[0]) < 3:\n                total += seq2[1]\n                none_close = False\n                flag[j+i+1] = False\n        if flag[i] and none_close:\n            result.append(seq1)\n        elif flag[i]:\n            result.append((seq1[0],total))\n    return result\n\n\nf_list = merge_elements(f_list)\nprint(f_list)\n```\n\n\nAnd the result is:\n\n```\n[('AGCTCCCCGTTTTC', 135), ('TTCATTCCTCTCTC', 9), ('GGGGCCCCCAAAAA', 99)]\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Given two strings, find if they are one edit away from each other\r\n                \r\nI came across this question recently:\n\n```\nGiven two strings, return true if they are one edit away from each other,else return false.\nAn edit is insert/replace/delete a character. \nEx. {\"abc\",\"ab\"}->true, {\"abc\",\"adc\"}->true, {\"abc\",\"cab\"}->false\n```\n\n\nOne way to solve this problem would be to find the edit distance between the two strings using dynamic programming, and check if it is 1. That would take O(N2) time. Is there a way to do this in linear time, as we've to only check if they are 1 edit away?\n\nThe code I wrote below works for most cases, but fails for cases like {\"m\",\"\"}\n\n```\npublic boolean isOneEditDistance(String s1,String s2){\n    if(s1==null || s2==null)\n        return true;\n    if(s1.equals(s2))\n        return false;\n    if (Math.abs(s1.length() - s2.length()) > 1)\n        return false;\n    int i = -1;\n    int j = -1;\n    while (true)\n    {\n        i++;\n        j++;\n        if (i == s1.length())\n            return true;\n        if (s1.charAt(i) == s2.charAt(j))\n            continue;\n        if (i != j)\n            return false;\n        if (s1.length() < s2.length())\n            i--;\n        else\n            j--;\n    }\n}\n```\n\n    ", "Answer": "\r\nHere is the solution for finding the one edit in O(n). Below are the scenario, I have covered in the implementation.\n\n\nThe length difference between two input strings should not be more than 1.\nWhen the length of the strings is same, the number of different chars should not be more than 1.\nIf the length difference is 1, then either one char can be inserted in the short string or deleted from the longer string. Considering that, the number of different char should not be more than 1. \n\n\n```\nprivate static boolean isOneEdit(String first, String second) {\n    // if the input string are same\n    if (first.equals(second))\n        return false;\n\n    int len1 = first.length();\n    int len2 = second.length();\n    // If the length difference of the stings is more than 1, return false.\n    if ((len1 - len2) > 1 || (len2 - len1) > 1  ) {\n        return false;\n    }\n    int i = 0, j = 0;\n    int diff = 0;\n    while (i<len1 && j<len2) {\n        char f = first.charAt(i);\n        char s = second.charAt(j);\n        if (f != s) {\n            diff++;\n            if (len1 > len2)\n                i++;\n            if (len2 > len1)\n                j++;\n            if (len1 == len2)\n                i++; j++;\n        }\n        else{\n            i++; j++;\n        }\n        if (diff > 1) {\n            return false;\n        }\n    }\n    // If the length of the string is not same. ex. \"abc\" and \"abde\" are not one edit distance.\n    if (diff == 1 && len1 != len2 && (i != len1 || j != len2)) {\n        return false;\n    }\n    return true;\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein distance in C with required memory of O(m)\r\n                \r\nI'm writing a code that calculates the edit distance of two given strings t and s with m = strlen(t) and n = strlen(s) and the code should only use memory in O(m). Furthermore, it should not need longer than 4 seconds for the calculation of two strings with about 50 characters. My written code satisfies the latter, but I'm not sure about the first, so I'd be happy if you could check if it uses not more than O(m) memory. If it doesn't, it would also be useful to hear some tips how to do so. Thanks.\n\n```\n#include <stdio.h>\n\n/************************************/\n/* TODO: Insert possible help       */\n/*       functions in here          */\n/************************************/\n\nint min (int a[], int l) {\n    int i, min = a[0];\n    for (i = 1; i < l; i++) {\n        if (a[i] < min) {\n            min = a[i];\n        }\n    }\n    return min;\n}\n\nint main() {\n    /* n, m, s, t are configurable in the source, i.e. the values here are \n            only an example and shall be changed to any value.*/\n    const int n = 58; /* length of s, > 0 */\n    const int m = 54; /* length of t, > 0 */\n    char *s = \"Calculateanddisplaythenumberofcharacterswithinarewrwegurie\";\n    char *t = \"Simplycopyeverythinginsideandpasteitwhereogwrgheigrber\";\n\n/* Save the edit distance in res */\n    int res;\n\nint matrix[n+1][m+1];\n\nint i, j;\n\nfor (i = 0; i <= m; i++) {\n        matrix[n][i] = m-i;\n    }\n    for (i = 0; i <= n; i++) {\n        matrix[i][m] = n-i;\n    }\n\nfor (i = n-1; i >= 0; i--) {\n        for (j = m-1; j >= 0; j--) {\n        int cost = (s[i] != t[j]);\n        int b[3];\n        b[0] = 1+matrix[i][j+1];\n        b[1] = 1+matrix[i+1][j];\n        b[2] = matrix[i+1][j+1]+cost;\n        matrix[i][j] = min(b, 3);\n        }\n    }\n\nres = matrix[0][0];\n\n/* Output */\n    printf(\"The edit distance is %d.\\n\\n\", res);\n    return 0;\n}\n```\n\n    ", "Answer": "\r\nYou only need to look 1 row back in the matrix, so the third row can go in the same place as the first row, the fourth row can go in the same place as the second row, etc. You only need two rows in the matrix. so:\n\n```\nint matrix[n+1][m+1];\n```\n\n\nbecomes\n\n```\nint matrix[2][m + 1];\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to calculate inter pupil and inter ocular distance for facial landmarks in python\r\n                \r\nI have been looking for a way to calculate the inter-pupil and interocular distance in python to evaluate the facial landmarks localization for 68 landmarks. The 68 landmarks do not have points on pupils. I was not able to understand the basic idea. I would really appreciate it if anyone could provide me some ideas or methods to calculate these metrics. Thank you in advance.\nEdit: distance calculation within an image not for the 3D model.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Calculate minimal string distance and find row which minimizes distance\r\n                \r\nI have a data frame with a column of class 'character'. I am trying to (a) create a new variable in some way summarizing how similar the value of a row in that column is to the most similar other value in the column and (b) identify the row of the most similar available value in that column for a given value in the column.\n\nMy existing approach is to calculate an edit distance measure using the stringdist package (https://cran.r-project.org/web/packages/stringdist/stringdist.pdf) except this seems to be incredibly computationally demanding and after hours of waiting still does not compute, but also it's not clear how to search for the smallest distance for each observation based on finding the distance of a given value from other values in the same vector. Furthermore, it doesn't appear to return the index of the most similar value.\n\nIs there any somewhat computationally tractable way to develop a minimal distance measure for each observation and the comparison row for which the distance is minimized?\n\n```\n# Create data\ndata.frame(x = c(\"a\",\"abbb\",\"aa\", \"abbbkdjsfjldkfjldfkjl\"))\n\n# Want something like\ndata.frame(smallest_distance = c(1,20,1,90), closest_match = c(3,3,1,2))  \n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Lucene FuzzyQuery Not working for random search terms\r\n                \r\nI have been trying to create a lucene fuzzy query match with max edit distance 1.\nI tried to add that in fuzzyquery.\nHere is my f# code:\n```\n            let subQuery = BooleanQuery()\n\n            subQuery.Add(\n                PrefixQuery(Term(fieldName, term)),\n                Occur.SHOULD\n            )\n\n            subQuery.Add(\n                FuzzyQuery(\n                    Term(fieldName, term),\n                    maxEdits       = 1,\n                    prefixLength   = 0,\n                    maxExpansions  = 1,\n                    transpositions = false\n                ),\n                Occur.SHOULD\n            )\n\n            query.Add(subQuery, Occur.SHOULD)\n            query\n```\n\nNow using this query with max edit 1, I expect to see words with 1 edit distance to match each other.\nbut When I type \"mustard\" it doesn't get any match with \"mustard\" products. however, surprizingly, \"mustrd\" and \"mustird\" is matching with \"mustard\"\nI have been using https://lucenenet.apache.org/docs/4.8.0-beta00015/\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Speed up a MED function?\r\n                \r\nI am trying to make a minimum edit distance function faster. Using numba ```\n@jit```\n makes it surprisingly slower and not faster! It says for loops should be faster! ```\n@njit```\n does not work. ```\nxrange```\n and ```\nrange```\n seem to be slightly better than ```\nndindex```\n.\n```\n #original: https://github.com/ferreirafabio/minimum-edit-distance-py\n #Calculate minimum edit distance, you can provide the cost of edit ops\n # @jit\n def med(tokens1, tokens2, subst=1, delete=1, insert=1):\n    n,m = len(tokens1), len(tokens2)\n    D = np.zeros((n, m), dtype=np.uint32)\n    for i in xrange(n):\n        for j in range(m):\n    # for i,j in np.ndindex(D.shape) :  \n            subst_cost = 0 if tokens1[i] == tokens2[j] else subst\n            D[i,j] = min(D[i-1, j] + insert, D[i, j-1] + delete, D[i-1, j-1] + subst_cost)\n    return D[n-1,m-1]\n```\n\nIt may be possible to use some 2D operation because if you look at it, it is a NxM kernel. Here is an example on a 2x2 kernel:\n```\n| subst  | insert |\n|--------|--------|\n| delete |   x    |\n\n| +1/0   |  +1    |\n|--------|--------|\n|    +1  |   min  |\n```\n\n    ", "Answer": "\r\nNumba ```\n@njit```\n does not work directly here since list of int/strings are apparently unsupported yet. As for ```\n@jit```\n, numba could not work efficiently on such data structure as no direct memory accesses are possible.\nHowever, numba can work very efficiently on numpy arrays containing native low-level types (eg. ```\nnp.int64```\n, but not python strings).\nHere is the trick: we can convert the input lists to numpy arrays, then use a fast vectorized numpy call to compare the converted input arrays and finally use numba to efficiently compute the ```\nD```\n values.\nHere is the resulting code:\n```\n@njit\ndef computeMedLine(D, subst_cost, i, delete, insert):\n    for j in range(len(subst_cost)):\n        D[i,j] = min(D[i-1, j] + insert, D[i, j-1] + delete, D[i-1, j-1] + subst_cost[j])\n\ndef fastMed(tokens1, tokens2, subst=1, delete=1, insert=1):\n    npTok1 = np.array(tokens1, dtype=object)\n    npTok2 = np.array(tokens2, dtype=object)\n\n    n,m = len(tokens1), len(tokens2)\n    D = np.zeros((n, m), dtype=np.uint32)\n    for i in range(n):\n        subst_cost = np.where(npTok1[i] == npTok2, 0, subst)\n        computeMedLine(D, subst_cost, i, delete, insert)\n    return D[n-1,m-1]\n```\n\nWith input lists containing 1000 items, the code above is about 300 times faster on my PC (without taking into account the JIT compilation time).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Distance in Python\r\n                \r\nI'm programming a spellcheck program in Python. I have a list of valid words (the dictionary) and I need to output a list of words from this dictionary that have an edit distance of 2 from a given invalid word. \n\nI know I need to start by generating a list with an edit distance of one from the invalid word(and then run that again on all the generated words). I have three methods, inserts(...), deletions(...) and changes(...) that should output a list of words with an edit distance of 1, where inserts outputs all valid words with one more letter than the given word, deletions outputs all valid words with one less letter, and changes outputs all valid words with one different letter.\n\nI've checked a bunch of places but I can't seem to find an algorithm that describes this process. All the ideas I've come up with involve looping through the dictionary list multiple times, which would be extremely time consuming. If anyone could offer some insight, I'd be extremely grateful.\n    ", "Answer": "\r\nThe thing you are looking at is called an edit distance and here is a nice explanation on wiki. There are a lot of ways how to define a distance between the two words and the one that you want is called Levenshtein distance and here is a DP (dynamic programming) implementation in python.\n```\ndef levenshteinDistance(s1, s2):\n    if len(s1) > len(s2):\n        s1, s2 = s2, s1\n\n    distances = range(len(s1) + 1)\n    for i2, c2 in enumerate(s2):\n        distances_ = [i2+1]\n        for i1, c1 in enumerate(s1):\n            if c1 == c2:\n                distances_.append(distances[i1])\n            else:\n                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n        distances = distances_\n    return distances[-1]\n```\n\nAnd a couple of more implementations are here.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is there an R function to do a pairwise Levenshtein distance calculation of two vectors of strings?\r\n                \r\nI have two vectors of strings:\n```\na <- c('Alpha', 'Beta', 'Gamma', 'Delta')\nb <- c('Epsilon', 'Zeta', 'Eta', 'Theta')\n```\n\nand I would like to compute the Levenshtein distance or edit distance for each pair of strings.\nIf I use\n```\nstringdist(a, b, method=\"lv\")\n```\n\nThe output is a vector with the Levenshtein distance of each string in vector a and the corresponding string in vector b (i.e., Alpha vs Epsilon, Beta vs Zeta, etc.).\nWhat I need instead is a pairwise comparison between each string in one vector and ALL the other strings in the other vector (i.e. Alpha vs Epsilon, Alpha vs. Zeta, Alpha vs Eta, Alpha vs. Theta, Beta vs Epsilon, etc.).\nThanks\n    ", "Answer": "\r\nThere is a straightforward way to do this using ```\nstringdistmatrix```\n and some reshaping:\n```\nlibrary(stringdist)\nlibrary(tidyverse)\n\na <- c('Alpha', 'Beta', 'Gamma', 'Delta')\nb <- c('Epsilon', 'Zeta', 'Eta', 'Theta')\n\nstringdistmatrix(a, b, method = \"lv\", useNames = \"string\") %>%\n  as_tibble(rownames = \"a\") %>%\n  pivot_longer(-1, names_to = \"b\", values_to = \"dist\")\n#> # A tibble: 16 x 3\n#>    a     b        dist\n#>    <chr> <chr>   <dbl>\n#>  1 Alpha Epsilon     7\n#>  2 Alpha Zeta        4\n#>  3 Alpha Eta         4\n#>  4 Alpha Theta       4\n#>  5 Beta  Epsilon     7\n#>  6 Beta  Zeta        1\n#>  7 Beta  Eta         2\n#>  8 Beta  Theta       2\n#>  9 Gamma Epsilon     7\n#> 10 Gamma Zeta        4\n#> 11 Gamma Eta         4\n#> 12 Gamma Theta       4\n#> 13 Delta Epsilon     6\n#> 14 Delta Zeta        2\n#> 15 Delta Eta         3\n#> 16 Delta Theta       3\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Reorder strings using similarity score algorithm\r\n                \r\nRe-orders a set of strings ```\n{ buzz, fuzz, jazz, fizz..}```\n so that sum of similarity scores between each pair of adjacent strings is the lowest. \n\n```\nbuzz-> fuzz (1)\nfuzz-> jazz (2)\njazz-> fizz (2)\n```\n\n\nsum of the scores is 5. If reordered based on lowest(4) final output is \n\n```\n{ buzz, fuzz, fizz, jazz..}\n\nbuzz-> fuzz (1)\nfuzz-> fizz (1)\nfizz-> jazz (2) \n```\n\n\nMy approach is to find Edit distance for every pair of strings and construct a weighted graph where edge represents the edit distance value. Use DFS to find the lowest path. Is this the efficient solution? can it be done any better?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein distance with weight/penalty for adjacency\r\n                \r\nI am using the string-edit distance (Levenshtein-distance) to compare scan paths from an eye tracking experiment. (Right now I am using the ```\nstringdist```\n package in R)\n\nBasically the letters of the strings refer to (gaze) position in a 6x4 matrix. The matrix is configured as follows:\n\n```\n     [,1] [,2] [,3] [,4]\n[1,]  'a'  'g'  'm'  's' \n[2,]  'b'  'h'  'n'  't'\n[3,]  'c'  'i'  'o'  'u'\n[4,]  'd'  'j'  'p'  'v'\n[5,]  'e'  'k'  'q'  'w'\n[6,]  'f'  'l'  'r'  'x'\n```\n\n\nIf I use the basic Levenshtein distance to compare strings, the comparison of ```\na```\n and ```\ng```\n in a string gives the same estimate as comparicon of ```\na```\n and ```\nx```\n. \n\nE.g.:\n\n```\n'abc' compared to 'agc' -> 1\n'abc' compared to 'axc' -> 1\n```\n\n\nThis means that the strings are equally (dis)similar\n\nI would like to be able to put weights on the string comparison in a way that incorporates adjacency in the matrix. E.g. the distance between ```\na```\n and ```\nx```\n should be weighted as larger then that between ```\na```\n and ```\ng```\n.\n\nOne way could be to calculate the \"walk\" (horizontal and vertial steps) from one letter to the other in the matrix and divide by the max \"walk\"-distance (i.e. from ```\na```\n to ```\nx```\n). E.g. the \"walk\"-distance from ```\na```\n to ```\ng```\n would be 1 and from ```\na```\n to ```\nx```\n it would be 8 resulting in a weight of 1/8 and 1 respectively.\n\nIs there a way to implement this (in either R or python)?\n    ", "Answer": "\r\nYou need a version of the Wagner-Fisher algorithm that uses non-unit cost in its inner loop. I.e. where the usual algorithm has ```\n+1```\n, use ```\n+del_cost(a[i])```\n, etc. and define ```\ndel_cost```\n, ```\nins_cost```\n and ```\nsub_cost```\n as functions taking one or two symbols (probably just table lookups).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Update dataframe column by comparing with existing data in another column using Levenshtein algorithm\r\n                \r\nHow can I update m_name column with Levenshtein algorithm to replace nulls ?\n\n```\n+--------------------+--------------------+-------------------+\n|       original_name|              m_name|            created|\n+--------------------+--------------------+-------------------+\n|            New York|            New York|2017-08-01 09:33:40|\n|            new york|                null|2017-08-01 15:15:06|\n|       New York city|                null|2017-08-01 15:15:06|\n|          california|          California|2017-09-01 09:33:40|\n| California,000IU...|                null|2017-09-01 01:40:00|\n|         Californiya|          California|2017-09-01 11:38:21|\n```\n\n\nFor every \"original_name\" value should be taken first nearest \"m_name\" value founded by algorithm based on Levenshtein distance (edit distance).\n\n```\nsimilarity(s1,s2) = [max(len(s1), len(s2)) − editDistance(s1,s2)] / max(len(s1), len(s2))\n```\n\n\n\"ideal\" final result should be like that \n\n```\n+--------------------+--------------------+-------------------+\n|       original_name|              m_name|            created|\n+--------------------+--------------------+-------------------+\n|            New York|            New York|2017-08-01 09:33:40|\n|            new york|            New York|2017-08-01 15:15:06|\n|       New York city|            New York|2017-08-01 15:15:06|\n|          california|          California|2017-09-01 09:33:40|\n| California,000IU...|          California|2017-09-01 01:40:00|\n|         Californiya|          California|2017-09-01 11:38:21|\n```\n\n    ", "Answer": "\r\nCredit goes to rossettacode Levenshtein_distance\n\nYou can do the following (commented for clarity and explanation)\n\n```\n//collecting the m_name to unique set and filtering out nulls and finally broadcasting to be used in udf function\nimport org.apache.spark.sql.functions._\nval collectedList = df.select(collect_set(\"m_name\")).rdd.collect().flatMap(row => row.getAs[Seq[String]](0).filterNot(_ == \"null\")).toList\nval broadcastedList = sc.broadcast(collectedList)\n\n//levenshtein distance formula applying\nimport scala.math.{min => mathmin, max => mathmax}\ndef minimum(i1: Int, i2: Int, i3: Int) = mathmin(mathmin(i1, i2), i3)\n\ndef editDistance(s1: String, s2: String) = {\n  val dist = Array.tabulate(s2.length + 1, s1.length + 1) { (j, i) => if (j == 0) i else if (i == 0) j else 0 }\n\n  for (j <- 1 to s2.length; i <- 1 to s1.length)\n    dist(j)(i) = if (s2(j - 1) == s1(i - 1)) dist(j - 1)(i - 1)\n    else minimum(dist(j - 1)(i) + 1, dist(j)(i - 1) + 1, dist(j - 1)(i - 1) + 1)\n\n  dist(s2.length)(s1.length)\n}\n\n//udf function definition to find the levenshtein distance and finding the closest first match from the broadcasted list with original_name column\ndef levenshteinUdf = udf((str1: String)=> {\n  val distances = for(str2 <- broadcastedList.value) yield (str2, editDistance(str1.toLowerCase, str2.toLowerCase))\n  distances.minBy(_._2)._1\n})\n\n\n//calling the udf function when m_name is null\ndf.withColumn(\"m_name\", when(col(\"m_name\").isNull || col(\"m_name\") === \"null\", levenshteinUdf(col(\"original_name\"))).otherwise(col(\"m_name\"))).show(false)\n```\n\n\nwhich should give you \n\n```\n+-------------------+----------+-------------------+\n|original_name      |m_name    |created            |\n+-------------------+----------+-------------------+\n|New York           |New York  |2017-08-01 09:33:40|\n|new york           |New York  |2017-08-01 15:15:06|\n|New York city      |New York  |2017-08-01 15:15:06|\n|california         |California|2017-09-01 09:33:40|\n|California,000IU...|California|2017-09-01 01:40:00|\n|Californiya        |California|2017-09-01 11:38:21|\n+-------------------+----------+-------------------+\n```\n\n\nNote : I didn't use your ```\nsimilarity(s1,s2) = [max(len(s1), len(s2)) − editDistance(s1,s2)] / max(len(s1), len(s2))```\n logic as its giving wrong output\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Edit Fuzzy C means function to take Haversine formula as distance metric\r\n                \r\nI'm new to r, and please correct me if what I'm asking for is impossible or insane.\n\nI want to cluster a set of geographical coordinates data(latitude, longitude) into a predetermined number of clusters with approximately the same size.\n\nI'm concerned with k-means and FCM algorithms since it's calculating based on euclidean distance. I'm thinking if I can substitute that with Haversine formula maybe if would work. I looked at the source code for the cmeans function, but has no idea what's going on..\n\nMy idea is to add a Haversine option under the metric method as below, and add the code accordingly. \n\n```\n  `dist <- pmatch(dist, c(\"euclidean\", \"manhattan\",\"Haversine\"))`\n```\n\n\nI've also tried DBSCAN, but since I need a fix number of cluster with similar size, I found it hard to achieve my goal.\n\nPlease let me know if this is possible. Any other thoughts on how I can perform the cluster is also welcomed, thanks!\n\n```\n#Fuzzy C Means\nfcmeans=function (x, centers, iter.max = 100, verbose = FALSE, dist = \"euclidean\", \n          method = \"cmeans\", m = 2, rate.par = NULL, weights = 1, control = list()) \n{\n  x <- as.matrix(x)\n  xrows <- nrow(x)\n  xcols <- ncol(x)\n  if (missing(centers)) \n    stop(\"Argument 'centers' must be a number or a matrix.\")\n  dist <- pmatch(dist, c(\"euclidean\", \"manhattan\"))\n  if (is.na(dist)) \n    stop(\"invalid distance\")\n  if (dist == -1) \n    stop(\"ambiguous distance\")\n  method <- pmatch(method, c(\"cmeans\", \"ufcl\"))\n  if (is.na(method)) \n    stop(\"invalid clustering method\")\n  if (method == -1) \n    stop(\"ambiguous clustering method\")\n  if (length(centers) == 1) {\n    ncenters <- centers\n    centers <- x[sample(1:xrows, ncenters), , drop = FALSE]\n    if (any(duplicated(centers))) {\n      cn <- unique(x)\n      mm <- nrow(cn)\n      if (mm < ncenters) \n        stop(\"More cluster centers than distinct data points.\")\n      centers <- cn[sample(1:mm, ncenters), , drop = FALSE]\n    }\n  }\n  else {\n    centers <- as.matrix(centers)\n    if (any(duplicated(centers))) \n      stop(\"Initial centers are not distinct.\")\n    cn <- NULL\n    ncenters <- nrow(centers)\n    if (xrows < ncenters) \n      stop(\"More cluster centers than data points.\")\n  }\n  if (xcols != ncol(centers)) \n    stop(\"Must have same number of columns in 'x' and 'centers'.\")\n  if (iter.max < 1) \n    stop(\"Argument 'iter.max' must be positive.\")\n  if (method == 2) {\n    if (missing(rate.par)) {\n      rate.par <- 0.3\n    }\n  }\n  reltol <- control$reltol\n  if (is.null(reltol)) \n    reltol <- sqrt(.Machine$double.eps)\n  if (reltol <= 0) \n    stop(\"Control parameter 'reltol' must be positive.\")\n  if (any(weights < 0)) \n    stop(\"Argument 'weights' has negative elements.\")\n  if (!any(weights > 0)) \n    stop(\"Argument 'weights' has no positive elements.\")\n  weights <- rep(weights, length = xrows)\n  weights <- weights/sum(weights)\n  perm <- sample(xrows)\n  x <- x[perm, ]\n  weights <- weights[perm]\n  initcenters <- centers\n  pos <- as.factor(1:ncenters)\n  rownames(centers) <- pos\n  if (method == 1) {\n    retval <- .C(\"cmeans\", as.double(x), as.integer(xrows), \n                 as.integer(xcols), centers = as.double(centers), \n                 as.integer(ncenters), as.double(weights), as.double(m), \n                 as.integer(dist - 1), as.integer(iter.max), as.double(reltol), \n                 as.integer(verbose), u = double(xrows * ncenters), \n                 ermin = double(1), iter = integer(1), PACKAGE = \"e1071\")\n  }\n  else if (method == 2) {\n    retval <- .C(\"ufcl\", x = as.double(x), as.integer(xrows), \n                 as.integer(xcols), centers = as.double(centers), \n                 as.integer(ncenters), as.double(weights), as.double(m), \n                 as.integer(dist - 1), as.integer(iter.max), as.double(reltol), \n                 as.integer(verbose), as.double(rate.par), u = double(xrows * \n                                                                        ncenters), ermin = double(1), iter = integer(1), \n                 PACKAGE = \"e1071\")\n  }\n  centers <- matrix(retval$centers, ncol = xcols, dimnames = list(1:ncenters, \n                                                                  colnames(initcenters)))\n  u <- matrix(retval$u, ncol = ncenters, dimnames = list(rownames(x), \n                                                         1:ncenters))\n  u <- u[order(perm), ]\n  iter <- retval$iter - 1\n  withinerror <- retval$ermin\n  cluster <- apply(u, 1, which.max)\n  clustersize <- as.integer(table(cluster))\n  retval <- list(centers = centers, size = clustersize, cluster = cluster, \n                 membership = u, iter = iter, withinerror = withinerror, \n                 call = match.call())\n  class(retval) <- c(\"fclust\")\n  return(retval)\n}\n```\n\n    ", "Answer": "\r\nSo, for starters, this function isn't using distance in the way that you are thinking. It is saying how to measure distance between points as either measured in Euclidean distance (e.g., as the crow flies) or Manhattan distance (which sticks to the gridlines of the Cartesian plane). \n\nThe problem with both methods is that they are going to compute the distance input wrong --- not that summing the squares or the Manhattan distance measures are wrong. \n\nYou can add your change to ```\npmatch(dist, ...```\n but you are going to have to also change the code that it calls out to. If you use ```\ncmeans```\n (and not ```\nufcl```\n) as the value for ```\nmethod```\n, the function calls out to the C program ```\ncmeans```\n in the ```\ne1071```\n package (this is what the ```\n.C```\n stuff is about). You can get the source for the package off CRAN. Everywhere this program computes a distance from something else, you are going to want to modify it to use the Haversine formula. At the very least this includes changes to ```\nufcl_dissimilarities```\n. Once you make those changes, you are going to need to recompile the source code (```\nR CMD build```\n from the terminal). \n\nI'm not familiar enough with the math to completely follow the ```\ncmeans.c```\n program, but you may have better luck. \n\nUnless you are familiar with C/C++/want a challenge, you are probably best off using Euclidean distance with the knowledge that it is computing the distance between points wrong. Since the error increases as the points move apart, in theory, it shouldn't hurt you too much since you are trying to place clusters such that they minimize distance. \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Can the edit distance of two binary numbers of unequal length be approximated with a Mathematical technique?\r\n                \r\nI am trying to efficiently find the closest binary number from a list of binary numbers to a given binary number. The binary numbers I am working with contain no consecutive zeros and 3 consecutive ones at max. I have tried the following techniques\n\nCalculate their absolute difference and count the set bits.\nGet the remainder from the division of the two numbers (greater/smaller) and count the set bits.\n\nThese seem to be working all right for the most part especially the second one but I was wondering if there were a more sophisticated technique that could produce more accurate results.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein distance with non uniform cost for insertions and substitutions:\r\n                \r\nI have been trying to implement a levenshtein distance function in C++ that gives different weights to substitutions and insertions based on which characters are being replaced or inserted. \n\nThe cost is calculated based on the distance of the keys on an qwerty keyboard. For example, in the standard edit distance algorithm, the distance between google, hoogle, and zoogle is the same; 1. What I want is different distances for these. Something like google -> hoogle = 1, google -> zoogle = 4, hoogle -> zoogle = 5. \n\nI followed the Wikipedia algorithm using the matrix for memoization and implemented it in c++. Here is my function.\n\n```\nint levDist(string s, string t) {\n\n    int i,j,m,n,temp,subsitutionCost, deletionCost, insertionCost, keyDist;\n    deletionCost = 1;\n\n    m = s.length();\n    n = t.length();\n    int d[m+1][n+1];\n\n    for(i=0;i<=m;i++)\n        d[i][0] = i;\n    for(j=0;j<=n;j++)\n        d[0][j] = j;\n\n    for (j=1;j<=n;j++)\n    {\n        for(i=1;i<=m;i++)\n        {\n            // getKeyboardDist(char a, char b) gives distance b/w the two keys\n            keyDist = getKeyboardDist(s[i-1],t[j-1]); \n\n            subsitutionCost = (s[i-1] == t[j-1]) ? 0 : keyDist;\n\n            // this line is the one i think the problem lies in\n            insertionCost = (i > j) ? getKeyboardDist(s[i-1],t[j-2]) : getKeyboardDist(s[i-2],t[j-1]);\n\n\n            insertionCost = insertionCost ? insertionCost : 1;\n\n            d[i][j] = min((d[i-1][j]   + deletionCost),\n                      min((d[i][j-1]   + insertionCost),\n                          (d[i-1][j-1] + subsitutionCost)));`\n        }\n    }\n    return d[m][n];\n}\n```\n\n\nNow the subsitutions work properly I beleive, but the problem is the insertions. I dont know how to find which characters to get the distance between for insertions. Especially the cases when the insertion is in the beginning or end of the string.\n\nI would appreciate any help in this, let me know if there is any other information needed.\n\nThanks in advance.\n    ", "Answer": "\r\nWhat you're trying to do makes sense for substitutions.  You're hypothesizing that a person attempting to strike a key X is more likely to make a mistake by striking a key physically near X than one that's far away.\n\nIt doesn't make so much sense for insertions and deletions because the act of striking an extra key (insertion error) or skipping a key strike (deletion error) doesn't have anything obvious to do with key distances.\n\nIt's possible you're being misdirected by the two different meanings of \"distance\" in play here. Levenshtein distance is measured between strings in insert/substitute/delete operations. Keyboard distance is a physical separation. These are apples and oranges that happen to be described with the same word. They don't mix very well.\n\nYou are attempting to determine weights for the Levenshtein operations. Physical distances between keys make reasonable weights for substitution.\n\nWeights for insert and delete - which involve only one character each - don't have any obvious relationship to physical separation. \n\nWhat you'd really want is frequency data regarding which keys people actually insert and delete by mistake. You'd give most common relatively low weights and the least common high ones.\n\n@user6952491's idea that repeating the previous key might be a high frequency insert error has merit, but it's hard to extend this to a complete weighting scheme.\n\nIf you're in a guessing mood, you could hypothesize that it's easier to mistakenly insert a key near the middle of the keyboard than at the edges. Say ```\nf```\n and ```\nj```\n get lowest weights and characters like ```\n~```\n that are shifted and at the keyboard extremes get high weights because it's unlikely you'd make the physical motions to type them without thinking.\n\nI'll leave it to you to formulate a similar guess about deletions.\n\nFor general typing, my guess is that keyboarding errors will have at least as much to do with spelling misapprehensions as physical mistakes.  I.e., people will type \"recieve\" because they forgot the rule \"i before e except after c,\" not due to the keyboard position of i relative to e.\n\nOther kinds of typing, e.g. computer code, might well have entirely different patterns for making mistakes. Forgotten semicolons come to mind! Those would have very low weights!\n\nConsequently, I'm virtually certain that the suggestions provided by modern spell checkers are rooted in machine learning algorithms that draw conclusions from mistakes that many thousands of people have made in the past on similar tasks, not simple metrics based on keyboard distance.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Can the edit distance of two binary numbers of unequal length be approximated with a Mathematical technique?\r\n                \r\nI am trying to efficiently find the closest binary number from a list of binary numbers to a given binary number. The binary numbers I am working with contain no consecutive zeros and 3 consecutive ones at max. I have tried the following techniques\n\nCalculate their absolute difference and count the set bits.\nGet the remainder from the division of the two numbers (greater/smaller) and count the set bits.\n\nThese seem to be working all right for the most part especially the second one but I was wondering if there were a more sophisticated technique that could produce more accurate results.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Fuzzy search a positional inverted index\r\n                \r\nUsing a positional inverted index structure, for example\n```\nvar index = new Dictionary<string, Dictionary<int, List<int>>>()\n{\n    [\"bar\"] = new Dictionary<int, List<int>>()\n    {\n        [3] = new List<int>() { 33, 45, 182 },\n        [18] = new List<int>() { 611, 794 },\n        ...\n    },\n    [\"foo\"] = new Dictionary<int, List<int>>()\n    {\n    ...\n}\n```\n\nwhich has a ```\n{ term: { docno: [...positions] } }```\n structure, how can I perform fuzzy lookups for phrase queries?\nElasticSearch and Lucene both have Levenshtein edit distance support but seems to be on a character level, ```\ngppgle```\n matches ```\ngoogle```\n if the ```\nfuzziness```\n parameter is 2 (for edit distance of 2).\nHowever I want to match on a word level, ```\nten people```\n should match ```\nten in people```\n, ```\none two three```\n should match ```\none and three```\n (depending on the \"fuzziness\" of the search).\nI'm not sure how to implement this efficiently considering I have an index at my disposal.\nPhrase queries can be implemented by simply checking if for each word's position in a document, the next word of the query appears in the same document one word further along.\nProximity queries are implemented in the same way as phrase query but allowing for the next word of the query to appear with some distance of the previous word. All terms have to exist in the document in order to match.\nHow can I implement an \"Edit distance\" query?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is there an algorithm for fuzzy search like Levenshtein Distance specialised for a set of ordered character?\r\n                \r\nI found an algorithm (on https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance) and after reading a bit more about levenshtein, I understood there should be a better way of telling the edit distance of twwo strings if these strings are strictly composed of ascii-aphabetically ordered and unique chars.\n\nMeaning, for every a and b like a < b, a will be prior to b, and the reciprocal (or contraposed or I don't remember) for every a, b, and c like a < b < c, if one strings reads ac and the other ab, one knows for sure the first one does not contain the b.\n\nAnd that precisely means there is a better way of determining the edit distance between two strings of this kind.\n\nIf it is any useful, the class I'm using to organize my characters is a TreeSet of Character.\n    ", "Answer": "\r\nThis is the solution I came up with :\nI used it with values :\nString tested, String target, 1, 0.\n\n```\n/** returns the cost of the difference between a tested CharSequence and a target CharSequence. CS = CharSequence\n * @param tested input, the CS which will be compared to the target. all letters sorted by ASCII order and unique\n * @param target is the CS to which the tested will be compared. all letters sorted by ASCII order and unique\n * @param positiveDifferenceCost is the cost to add when a letter is in the tested CS but not in the target.\n * @param negativeDifferenceCost is the cost to add when a letter is in the target CS but not in the tested.\n * @return int the number of differences.\n */\n\n\npublic static int oneSidedOAUDistance(final CharSequence tested, final CharSequence target,\n                                      final int positiveDifferenceCost, final int negativeDifferenceCost) {\n    int diffCount = 0;\n    int index_tested = 0;\n    int index_target = 0;\n\n    if (positiveDifferenceCost == 0 && negativeDifferenceCost == 0)\n        return 0;\n\n\n    for (; index_tested < tested.length() && index_target < target.length(); ) {\n        if (tested.charAt(index_tested) == target.charAt(index_target)) {\n            ++index_tested;\n            ++index_target;\n            continue;\n        }\n        if (tested.charAt(index_tested) < target.charAt(index_target)) {\n            //some letters should not be there in tested string.\n            diffCount+= positiveDifferenceCost;\n            index_tested++;\n\n        } else {\n            //some letters miss in tested string.\n            diffCount+=negativeDifferenceCost;\n            index_target++;\n        }\n    }\n\n\n    return diffCount;\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to connect two nodes if they are disconnected\r\n                \r\nI use a python NetworkX graph. How to check if 2 nodes are disconnected and then get a new version of the graph where these 2 nodes are connected.\nThe difference between 2 graphs should have min edit distance (Levenshtein distance)\nBefore and after for ```\nnodes=[1,2]```\n:\n  |    \n    ", "Answer": "\r\n```\nimport networkx as nx\n\nG = nx.DiGraph([(1, 1), (2, 3), (4, 3)])\n\n\ndef check_edge(source, target):\n    if not G.has_edge(source, target):\n        print('adding edge between {} and {}'.format(source, target))\n        G.add_edge(source, target)\n    else:\n        print('edge exists between {} and {}'.format(source, target))\n\n\ncheck_edge(1, 2)\n# adding edge between 1 and 2\ncheck_edge(1, 2)\n# edge exists between 1 and 2\n```\n\nyou can also have a condition to check for an edge of either direction with:\n```\nif not G.has_edge(source, target) and not G.has_edge(target, source):\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to modify Levenshtein algorithm, to know if it inserted, deleted, or substituted a character?\r\n                \r\nSo I am trying to devise a spin off of the Levenshtein algorithm, where I keep track of what transformations I did in the string(inserted a, or substitute a for b).\n\nExample:\n\nBasically, say I am computing the edit distance of \"bbd\" and \"bcd\"\n\nThe edit distance will be 1 and the transformation will be \"substitude b for c\"\n\nQuestion:\nHow would I approach this problem since the implementations i've seen do not concern themselves with knowing what kind of operation it was but only the total cost?\n    ", "Answer": "\r\nYou can use this module - there is an ```\neditops```\n function there, which returns a list with the operations needed to transform one string to another.\nExample:\n```\nLevenshtein.editops(\"FBBDE\", \"BCDASD\")\n[('delete', 0, 0), ('replace', 2, 1), ('insert', 4, 3), ('insert', 4, 4), ('replace', 4, 5)]\n```\n\nFrom the docs:\n\nFind sequence of edit operations transforming one string to another.\neditops(source_string, destination_string)\neditops(edit_operations, source_length, destination_length)\nThe result is a list of triples (operation, spos, dpos), where\noperation is one of 'equal', 'replace', 'insert', or 'delete';  spos\nand dpos are position of characters in the first (source) and the\nsecond (destination) strings.  These are operations on single\ncharacters.  In fact the returned list doesn't contain the 'equal',\nbut all the related functions accept both lists with and without\n'equal's.\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Does Perl 6 have an infinite Int?\r\n                \r\nI had a task where I wanted to find the closest string to a target (so, edit distance) without generating them all at the same time. I figured I'd use the high water mark technique (low, I guess) while initializing the closest edit distance to ```\nInf```\n so that any edit distance is closer: \n\n```\nuse Text::Levenshtein;\n\nmy @strings = < Amelia Fred Barney Gilligan >;\n\nfor @strings {\n    put \"$_ is closest so far: { longest( 'Camelia', $_ ) }\";\n    }\n\nsub longest ( Str:D $target, Str:D $string ) {\n    state Int $closest-so-far = Inf;\n    state Str:D $closest-string = '';\n\n    if distance( $target, $string ) < $closest-so-far {\n        $closest-so-far = $string.chars;\n        $closest-string = $string;\n        return True;\n        }\n\n    return False;\n    }\n```\n\n\nHowever, ```\nInf```\n is a Num so I can't do that: \n\n\n  Type check failed in assignment to $closest-so-far; expected Int but got Num (Inf)\n\n\nI could make the constraint a ```\nNum```\n and coerce to that:\n\n```\n    state Num $closest-so-far = Inf;\n    ...\n        $closest-so-far = $string.chars.Num;\n```\n\n\nHowever, this seems quite unnatural. And, since ```\nNum```\n and ```\nInt```\n aren't related, I can't have a constraint like ```\nInt(Num)```\n. I only really care about this for the first value. It's easy to set that to something sufficiently high (such as the length of the longest string), but I wanted something more pure.\n\nIs there something I'm missing? I would have thought that any numbery thing could have a special value that was greater (or less than) all the other values. Polymorphism and all that.\n    ", "Answer": "\r\n{new intro that's hopefully better than the unhelpful/misleading original one}\n@CarlMäsak, in a comment he wrote below this answer after my first version of it:\n\nLast time I talked to Larry about this {in 2014}, his rationale seemed to be that ... Inf should work for all of Int, Num and Str\n\n(The first version of my answer began with a \"recollection\" that I've concluded was at least unhelpful and plausibly an entirely false memory.)\nIn my research in response to Carl's comment, I did find one related gem in #perl6-dev in 2016 when Larry wrote:\n\nthen our policy could be, if you want an Int that supports ±Inf and NaN, use Rat instead\nin other words, don't make Rat consistent with Int, make it consistent with Num\n\nLarry wrote this post ```\n6.c```\n. I don't recall seeing anything like it discussed for ```\n6.d```\n.\n{and now back to the rest of my first answer}\n\n```\nNum```\n in P6 implements the IEEE 754 floating point number type. Per the IEEE spec this type must support several concrete values that are reserved to stand in for abstract concepts, including the concept of positive infinity. P6 binds the corresponding concrete value to the term ```\nInf```\n.\nGiven that this concrete value denoting infinity already existed, it became a language wide general purpose concrete value denoting infinity for cases that don't involve floating point numbers such as conveying infinity in string and list functions.\n\nThe solution to your problem that I propose below is to use a ```\nwhere```\n clause via a ```\nsubset```\n.\nA ```\nwhere```\n clause allows one to specify run-time assignment/binding \"typechecks\". I quote \"typecheck\" because it's the most powerful form of check possible -- it's computationally universal and literally checks the actual run-time value (rather than a statically typed view of what that value can be). This means they're slower and run-time, not compile-time, but it also makes them way more powerful (not to mention way easier to express) than even dependent types which are a relatively cutting edge feature that those who are into advanced statically type-checked languages tend to claim as only available in their own world1 and which are intended to \"prevent bugs by allowing extremely expressive types\" (but good luck with figuring out how to express them... ;)).\nA subset declaration can include a ```\nwhere```\n clause. This allows you to name the check and use it as a named type constraint.\nSo, you can use these two features to get what you want:\n```\nsubset Int-or-Inf where Int:D | Inf;\n```\n\nNow just use that ```\nsubset```\n as a type:\n```\nmy Int-or-Inf $foo; # ($foo contains `Int-or-Inf` type object) \n$foo = 99999999999; # works\n$foo = Inf;         # works\n$foo = Int-or-Inf;  # works\n$foo = Int;         # typecheck failure\n$foo = 'a';         # typecheck failure\n```\n\n1. See Does Perl 6 support dependent types? and it seems the rough consensus is no.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Sorting duration and distance using javascript\r\n                \r\nHow do I sort distance and duration where lowest duration with the highest distance will be on top.\nExample:\n```\nlet obj = [\n    {\n        duration: 50.1,\n        distance: 300,\n    },\n    {\n        duration: 60.9,\n        distance: 300,\n    },\n    {\n        duration: 30.08,\n        distance: 300,\n    },\n    {\n        duration: 90.0,\n        distance: 310,\n    },\n];\n```\n\nEdit:\nHere will be the desired results:\nIf distance is greater than other distance it will be on top even if the duration is highest.\n```\nlet obj = [\n      {\n         duration: 90.00,\n         distance: 310\n         \n      },\n      {\n        duration: 30.08,\n        distance: 300\n      },\n      {\n        duration: 50.10,\n        distance: 300\n      },\n      {\n        duration: 60.90,\n        distance: 300\n      }\n    ]\n```\n\n    ", "Answer": "\r\nWe can sort array using callback function.\nBased on the idea that distance is of higher priority than duraion, we can make a callback function like bellow.\n\r\n\r\n```\nlet obj = [\n  {\n    duration: 50.1,\n    distance: 300,\n  },\n  {\n    duration: 60.9,\n    distance: 300,\n  },\n  {\n    duration: 30.08,\n    distance: 300,\n  },\n  {\n    duration: 90.0,\n    distance: 310,\n  },\n];\n\nobj.sort(function(a, b) {\n  if (a.distance == b.distance) {\n    return a.duration - b.duration;\n  } else return b.distance - a.distance;\n});\n\nconsole.log(obj);```\n\r\n\r\n\r\n\nJavascript reference can be found here\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to print all words stored in a Tree , wherin trie has been implemented using Hashmap in Java?\r\n                \r\nI want to print or retrieve all the words stored in Trie Data Structure. This is because I want to compute Edit distance between a misspelled word and a word in Dictionary. \nTherefore I was thinking of retrieving each word from Trie and compute Edit distance. \nBut I am not able to retrieve. I want some code snippet for this. \nThis is how I have implemented  the Trie using ```\nHashMap```\n in Java\n\nNow please tell me how to write code for printing all words stored in Trie. Any help is very much appreciated\n\nTrieNode.java\n\n```\npackage triehash;\nimport java.io.Serializable;\nimport java.util.HashMap;\n\npublic class TrieNode implements Serializable {\n\nHashMap<Character, HashMap> root;\n\npublic TrieNode() {\n   root = new HashMap<Character, HashMap>();   \n   }\n}\n```\n\n\nTrieDict.java\n\n```\npackage triehash;\n\nimport java.io.FileOutputStream;\nimport java.io.ObjectOutputStream;;\nimport java.io.Serializable;\nimport java.util.HashMap;\nimport java.io.Serializable;\n\npublic class TrieDict {   \n public  TrieNode createTree()\n {\n     TrieNode t = new TrieNode();\n     return t;\n }\n\n public void add(String s, TrieNode root_node) {\n    HashMap<Character, HashMap> curr_node = root_node.root;\n    s = s.toLowerCase();\n    for (int i = 0, n = s.length(); i < n; i++) {\n        Character c = s.charAt(i);\n        if (curr_node.containsKey(c))\n            curr_node = curr_node.get(c);\n        else {\n            curr_node.put(c, new HashMap<Character, HashMap>());\n            curr_node = curr_node.get(c);\n        }\n    }\n    curr_node.put('\\0', new HashMap<Character, HashMap>(0)); // term\n  }\n\n public void serializeDict(TrieNode root_node)\n {    \n   try{\n        FileOutputStream fout = new FileOutputStream(\"/home/priya/NetBeansProjects/TrieHash/dict.ser\");\n\n    ObjectOutputStream oos = new ObjectOutputStream(fout);   \n    oos.writeObject(root_node);\n    oos.close();\n    System.out.println(\"Done\");\n\n   }catch(Exception ex){\n       ex.printStackTrace();\n   }\n}\n\n public void addAll(String[] sa,TrieNode root_node) {\n    for (String s: sa)\n        add(s,root_node);\n }\n\n public static void main(String[] args)\n {\n    TrieDict td = new TrieDict();\n    TrieNode tree = td.createTree();\n\n    String[] words = {\"an\", \"ant\", \"all\", \"allot\", \"alloy\", \"aloe\", \"are\", \"ate\", \"be\"};\n    for (int i = 0; i < words.length; i++)\n      td.add( words[i],tree);       \n    td.serializeDict(tree); /* seriliaze dict*/\n }   \n}\n```\n\n    ", "Answer": "\r\nFirst, it's worth noting that the declared type of the ```\nroot```\n instance variable is a little odd. (Specifically, the value type of ```\nHashMap<Character,HashMap>```\n excludes some of the generics you'd rather it used.) The code below should work, but you'll get some warnings as a result of this. You might try refactoring your code to use the type ```\nHashMap<Character,TrieNode>```\n instead. Sorry if that's pedantic. :)\n\nTry this, added as methods to ```\nTrieNode```\n:\n\n```\npublic Set<String> computeWords() {\n    Set<String> result;\n\n    if(root.size() == 0)\n        result = new HashSet<String>();\n    else\n        result = computeWords(root, \"\");\n\n    return result;\n}\n\nprotected static Set<String> computeWords(HashMap tree, String prefix) {\n    Set<String> result=new HashSet<String>();\n\n    if(tree.size() == 0)\n        result.add(prefix);\n    else\n        for(Object o : tree.keySet()) {\n            Character c=(Character) o;\n            prefix = prefix+c;\n            result.addAll(computeWords((HashMap) tree.get(c), prefix));\n            prefix = prefix.substring(0, prefix.length()-1);\n        }\n\n    return result;\n}\n```\n\n\nFor a given ```\nTrieNode```\n object ```\nt```\n, ```\nt.computeWords()```\n would return the set of all words words encoded in ```\nt```\n.\n\nI believe this answers the question you were trying to ask. However, to answer the question as stated in the header, you'd print all the words for the same ```\nt```\n like this:\n\n```\nfor(String word : t.computeWords())\n    System.out.println(word);\n```\n\n\nAlso, this definitely isn't the most efficient implementation, especially since we create a bunch of ```\nHashSet```\n objects in ```\ncomputeWords(HashMap,String)```\n, but it should work!\n\nEDIT: This code also assumes that you terminate words with an empty ```\nHashMap```\n. If you terminate words with ```\nnull```\n instead, you'd need to update the ```\nif(tree.size() == 0)```\n check in the ```\nstatic```\n method with ```\nif(tree == null)```\n. Sorry, should have called that out.\n\nEDIT: Explained how to print all the words, just in case it wasn't clear. \n\nEDIT: Fixed empty trie case.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Generate deletion, insertion, substitution, transpotions for a string\r\n                \r\nI am implementing a spell checker algorithm. I have constructed a ```\nTrie```\n that stores my words for quick searching.\n\nWhen a given input string is passed what I want to do is generate potential deletions, insertions, substitutions and transpositions for that string with an edit distance of 1. Using this super set I can then try to find the word in my ```\nTrie```\n and offer the user \"did you mean?\" type results.\n\nI have looked online and most solutions mention calculating the Levenstein Distance. That only works if you already know the two strings and you want to find the edit distance between the two.\n\nSuggestions?\n    ", "Answer": "\r\nI would use an 2 pass algo:\n\nPass 1\n\nlook and calculate the distance for all  words starting with the same letter as the word to spell check. This will be fast. you can stop the depth search when the number of chars is greater then spell word length + 2 (then this obiously another word)\nDisplay results of pass1, eg by marking word red underline\n\nPass 2\nlook for all words and stop when length + 3 or 4\nUpdate the results found in pass 1\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Does it matter which string is taken along the row and column in Min Edit Distance Problem?\r\n                \r\nThe Problem:-\n\nSo, i have taken the second string along the row and the first string along the columns-I am getting the wrong answer because of this. But i dont understand how and why??\nalso, for some value(i.e \"table\" and \"bal\") i am getting segmentation fault.\n```\n\n#include <vector>\n#include <string>\n#include <iostream>\nusing namespace std;\n\nvoid print(vector<vector<int>> &table)\n{\n    for(int i = 0; i < table.size(); ++i)\n    {\n        for(int j = 0; j < table[0].size(); ++j)\n            cout << table[i][j] << \" \";\n        cout << endl;\n    }\n}\nint levenshteinDistance(string a, string b) \n{\n    int r = b.size() + 1;\n    int c = a.size() + 1;\n    vector<vector<int>> table(r, vector<int>(c, 0));\n    for(int i = 0; i < r; ++i)\n        table[i][0] = i;\n    for(int j = 0; j < c; ++j)\n        table[0][j] = j;\n    \n    for(int i = 1; i < r; ++i)\n    {\n        for(int j = 1; j < c; ++j)\n        {\n            \n            if(a[j - 1] != b[i - 1])\n            {\n                table[i][j] = 1 + min(table[i - 1][j - 1], \n                                                min(table[j - 1][i], table[i - 1][j]));\n            }\n            else\n                table[i][j] = table[i - 1][j - 1];\n            \n        }\n        \n    }\n\n    print(table);\n  return table[r-1][c-1];\n}\n```\n\n    ", "Answer": "\r\nNo it does not matter in which way you define the matrix, as long as you access it appropriately in the nested loops afterwards.\nIn your case you access them here:\n```\ntable[i][j] = 1 + min(table[i - 1][j - 1], min(table[j - 1][i], table[i - 1][j]));\n```\n\nin your case i is the index for the first dimension of the matrix, while j is the index for the second dimension. So the access ```\ntable[j - 1][i]```\n should be ```\ntable[i][j - 1]```\n. This caused a segmentation fault, since you accessed elements outside of the vectors.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Comparing file paths using Levenshtein Distance\r\n                \r\nI need to figure out how close specific file paths are, the Levenshtein distance algorithm works great, but I need to somehow give weightings to directories that are higher up the directory tree.\n\nFor example:\n\nMy source is \"x:/t/c/d\"\n\nAnd my two targets are:\n\n\n\"a:/t/c/d\"\n\"x:/t/y/z\"\n\n\nI need the second target to identify as being closer even though \"as a string\" it's edit distance is greater (since target two is in the same parent directory \"x\" as the source, whilst the first target is looking at directory \"a\". \n\nHow would I go about giving a weighting to characters that appear earlier on in the strings?\n    ", "Answer": "\r\nIt seems to me the Levenshtein Distance on the full path is not the right algorithm for what you are trying to achieve.\n\nI would suggest you to split the path into a list of folder (eventually with a file on the end), then I would compare the directory name (or drive) at the corresponding position and give to that a high score if it is a perfect match, lowering the score as you go further down the directory tree.\n\nIf it's not a match, than you could still apply the Levenshtein Distance on the path and multiply it for a weight which would decrease as well as you go further down.\n\nThan sum it all up. \n\nFor example:\n\n```\nvar source = \"x:/t/c/d\";\nvar targets = new[] { \"a:/t/c/d\", \"x:/t/y/z\" };\n\nvar separator = '/';\nvar sourceParts = source.Split(separator);\nvar weight = 10;\nvar match = 100;\n\nvar scores = targets.Select(target =>\n{\n    var score = sourceParts\n        .Zip(target.Split(separator), (s, t) => new Tuple<string, string>(s, t))\n        .Select(\n            (tuple, i) => tuple.Item1 == tuple.Item2\n                ? match * GetWeight(i)\n                : LevenshteinDistance(tuple.Item1, tuple.Item2) * GetWeight(i)\n        ).Sum();\n\n    return new\n    {\n        Target = target,\n        Score = score\n    };\n});\n```\n\n\nwhere GetWeight() is something like:\n\n```\nprivate static int MaxWeight = 10;\nprivate static int GetWeight(int i) => i < MaxWeight ? MaxWeight - i : 1;\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "What tree distance libraries in javascript? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI'm looking for a library that implements any of the seemingly well documented* strategies for calculating the distance between trees.\n\nFor example, the library should show that these two trees \n\n```\n  a              a\n / \\            / \\\nb   c          b   c\n                    \\\n                     d\n```\n\n\nare more 'similar' than these two\n\n```\n  a              x\n / \\            / \\\nb   c          y   z\n```\n\n\nI'm using node.js so a node implementation would be nice. I'm aware that 'similar' seems somewhat ambiguous, but no matter what strategy you would use to compute the difference between these trees, I'm assuming you would always end up with some final 'score' that, if representing an edit distance, would be larger in the second case than in the first.\n\nWhat node.js libraries do anything like this?\n\nEDIT: Some added info on my specific case: This is for detecting repeating tree structures in html documents, so libraries more specialized to that problem would be even better.\n\nEDIT2: Even a levenshtein distance library for node.js would help so I could at least do comparisons on fingerprint text of the tree.\n\n*I'm new to the field, but scribd.com discusses strategies used by Lu and Tai.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Debugging Levenshtein distance implementation - how is the min distance being calculated?\r\n                \r\nI have been trying to understand this code for while but I can't get how the distances are calculated.  I know how the algorithm should work (it provides the edit distance in characters between two strings and I can do it on paper), but I don't understand how this line (as an example)\n\n\n  d(i, j) = d(i - 1, j - 1)\n\n\ncomes back as an integer. Same for min1, min2. We already define \n\n\n  d(i,0)\n\n\nand \n\n\n  d(0,j)\n\n\n, but how does it get a value when \n\n\n  d(i,j)\n\n\nis not 0 in one of the two arguments? \n\nCode:\n\n```\nOption Explicit\nPublic Function Levenshtein(s1 As String, s2 As String)\n\nDim i As Integer\nDim j As Integer\nDim l1 As Integer\nDim l2 As Integer\nDim d() As Integer\nDim min1 As Integer\nDim min2 As Integer\n\nl1 = Len(s1)\nl2 = Len(s2)\nReDim d(l1, l2)\nFor i = 0 To l1\n    d(i, 0) = i\nNext\nFor j = 0 To l2\n    d(0, j) = j\nNext\nFor i = 1 To l1\n    For j = 1 To l2\n        If Mid(s1, i, 1) = Mid(s2, j, 1) Then\n            d(i, j) = d(i - 1, j - 1)   \n        Else\n            min1 = d(i - 1, j) + 1\n            min2 = d(i, j - 1) + 1\n            If min2 < min1 Then\n                min1 = min2\n            End If\n            min2 = d(i - 1, j - 1) + 1\n            If min2 < min1 Then\n                min1 = min2\n            End If\n            d(i, j) = min1\n        End If\n    Next\nNext\nLevenshtein = d(l1, l2)\nEnd Function\n\n?Levenshtein(\"saturday\",\"sunday\")\n\n3\n\n```\n\n    ", "Answer": "\r\nI don't have much clue about Levenshtein, however I can explain what's going on in that array, please see comments in the code:\n\n```\nl1 = Len(s1) 'this sets a variable with the length of the first string\nl2 = Len(s2) 'this sets a variable with the length of the second string\n\nReDim d(l1, l2) 'redimension the array to this size, though this can be seen as ReDim d(0 to l1, 0 to l2)\n\nFor i = 0 To l1 'for i = 0 to length of first string\n    d(i, 0) = i 'allocate the value of i to each row in the array, at col 0\nNext i\n\nFor j = 0 To l2 'for j = 0 to length of second string\n    d(0, j) = j 'allocate the value of j to each column in the array, at row 0\nNext j\n```\n\n\nEDIT: I've added some debugging (will print to ActiveSheet). Blue for where the letters match, red for the others... though the colour doesn't matter.\n\nAs far as I can tell, with each loop iteration, it builds a matrix of the differences, starting from comparing 1:1 first letters, till the last. Based on the loop position, and with each iteration, it will use previous position values to calculate the difference at current position.\n\nThis is probably easier to follow with shorter strings.\n\n\nLoop through each letter in second word, then loop through each letter in first word\nat first outer loop (for each row), first inner loop (for each column), we have ```\nS```\n meet ```\nS```\n = 0. (```\nd(i, j) = d(i - 1, j - 1)```\n evaluates to ```\nd(i, j) = d(1 - 1, 1 - 1)```\n, i.e.: ```\n0```\n)\nat first outer loop, second inner loop, we have ```\nS```\n meet ```\nU```\n = 1.\netc, etc.\n\n\n\n\nOther than that, step through the code and see how the variables change based on the if conditions... not sure i can explain this any better.\n\n```\nPublic Function Levenshtein(s1 As String, s2 As String)\n\nDim i As Integer, j As Integer\nDim l1 As Integer, l2 As Integer\nDim min1 As Integer, min2 As Integer\nDim d() As Integer\n\n'For debugging purposes only\nCells.Clear\nDim rngOutput As Range: Set rngOutput = ActiveSheet.Range(\"A1\").Resize(Len(s1) + 2, Len(s2) + 2)\n\nWith rngOutput\n    .ColumnWidth = 3\n    .HorizontalAlignment = xlCenter\n    .VerticalAlignment = xlCenter\nEnd With\n\nl1 = Len(s1): l2 = Len(s2): ReDim d(l1, l2)\n\nFor i = 0 To l1\n    d(i, 0) = i\n    With rngOutput\n        .Cells(i + 3, 1) = Mid(s1, i + 1, 1)\n        If Not i = 0 Then .Cells(i + 2, 2) = i\n    End With\nNext i\n\nFor j = 0 To l2\n    d(0, j) = j\n    With rngOutput\n        .Cells(1, j + 3) = Mid(s2, j + 1, 1)\n        If Not j = 0 Then .Cells(2, j + 2) = j\n    End With\nNext j\n\nFor i = 1 To l1\n    For j = 1 To l2\n        If Mid(s1, i, 1) = Mid(s2, j, 1) Then\n            d(i, j) = d(i - 1, j - 1)\n\n            With rngOutput.Cells(i + 2, j + 2)\n                .Value = d(i, j)\n                .Font.Color = vbBlue\n            End With\n        Else\n            min1 = d(i - 1, j) + 1\n            min2 = d(i, j - 1) + 1\n            If min2 < min1 Then\n                min1 = min2\n            End If\n            min2 = d(i - 1, j - 1) + 1\n            If min2 < min1 Then\n                min1 = min2\n            End If\n            d(i, j) = min1\n\n            With Cells(i + 2, j + 2)\n                .Value = d(i, j)\n                .Font.Color = vbRed\n            End With\n        End If\n    Next\nNext\n\nLevenshtein = d(l1, l2)\nEnd Function\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Computing net distance (Euclidean distance) in R\r\n                \r\nI have asked about and receive great help for computing Euclidean distance in R before. Now, I need to compute the Euclidean distance from the first point relative to all the other points within the track data.\nHere is how my data looks like:\n\n```\ndput(head(t1))\nstructure(list(A = c(0L, 0L, 0L, 0L, 0L, 0L), T = 0:5, X = c(668L, \n668L, 668L, 668L, 668L, 668L), Y = c(259L, 259L, 259L, 259L, \n259L, 259L), V = c(NA, 0, 0, 0, 0, 0)), .Names = c(\"A\", \"T\", \n\"X\", \"Y\", \"V\"), row.names = c(NA, 6L), class = \"data.frame\")\n```\n\n\nAnd SimonO101 was so great in giving me a code that will compute the Euclidean distance from the starting position to the final position for each track:\n\n```\n## Split the data\ndfs <- split(t1,t1$A)\n\n## Find hypotenuse between first and last rows for each A\nlapply( dfs , function(x){\n  j <- nrow(x)\n  str <- x[1,c(\"X\",\"Y\")]\n  end <- x[j,c(\"X\",\"Y\")]\n  dist <- sqrt( sum( (end - str)^2 ) )\n  return( dist )\n} )\n```\n\n\nHow do I edit the code, so that it will not just have the Euclidean distance from start to end, but from every X,Y position?\nThanks again!\n\nEDIT:\nAnd also: How to visualize the results as a matrix. Thank you\n    ", "Answer": "\r\nI would use the ```\ndist```\n function in the ```\nstats```\n package. You can apply it to your data easily enough:\n\n```\nlapply( dfs , function(x) dist( x[,c(\"X\",\"Y\")] , diag = TRUE )[1:nrow(x)] )\n```\n\n\nThe idea being that we operate on each dataframe, applying the ```\ndist```\n function to the ```\n\"X\"```\n and ```\n\"Y\"```\n columns of each data frame. The subsetting at the end ( ```\n[1:nrow(x)]```\n )is used to return only the distances between the first point and all the other points. Remove this subsetting if you want a full distance matrix for each track.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Manhattan Distance for two geolocations\r\n                \r\nLet's say I have two locations represented by latitude and longitude.\nLocation 1 : ```\n37.5613```\n , ```\n126.978```\n\nLocation 2 : ```\n37.5776```\n , ```\n126.973```\n\n\nHow can I calculate the distance using Manhattan distance ?\n\nEdit : I know the formula for calculating Manhattan distance like stated by ```\nEmd4600```\n on the answer which is ```\n|x1-x2| - |y1-y2|```\n but I think it's for Cartesian. If it is can be applied that straight forward ```\n|37.5613-37.5776| + |126.978-126.973|```\n what is the distance unit of the result ?\n    ", "Answer": "\r\nGiven a plane with ```\np1```\n at ```\n(x1, y1)```\n and ```\np2```\n at ```\n(x2, y2)```\n, it is, the formula to calculate the Manhattan Distance is ```\n|x1 - x2| + |y1 - y2|```\n. (that is, the difference between the latitudes and the longitudes). So, in your case, it would be:\n\n```\n|126.978 - 126.973| + |37.5613 - 37.5776| = 0.0213\n```\n\n\nEDIT: As you have said, that would give us the difference in latitude-longitude units. Basing on this webpage, this is what I think you must do to convert it to the metric system. I haven't tried it, so I don't know if it's correct:\n\nFirst, we get the latitude difference:\n\n```\nΔφ = |Δ2 - Δ1|\nΔφ = |37.5613 - 37.5776| = 0.0163\n```\n\n\nNow, the longitude difference:\n\n```\nΔλ = |λ2 - λ1|\nΔλ = |126.978 - 126.973| = 0.005\n```\n\n\nNow, we will use the ```\nhaversine```\n formula. In the webpage it uses ```\na = sin²(Δφ/2) + cos φ1 ⋅ cos φ2 ⋅ sin²(Δλ/2)```\n, but that would give us a straight-line distance. So to do it with Manhattan distance, we will do the latitude and longitude distances sepparatedly.\n\nFirst, we get the latitude distance, as if longitude was 0 (that's why a big part of the formula got ommited):\n\n```\na = sin²(Δφ/2)\nc = 2 ⋅ atan2( √a, √(1−a) )\nlatitudeDistance = R ⋅ c // R is the Earth's radius, 6,371km\n```\n\n\nNow, the longitude distance, as if the latitude was 0:\n\n```\na = sin²(Δλ/2)\nc = 2 ⋅ atan2( √a, √(1−a) )\nlongitudeDistance = R ⋅ c // R is the Earth's radius, 6,371km\n```\n\n\nFinally, just add up ```\n|latitudeDistance| + |longitudeDistance|```\n.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "What to return when exiting early from a recursive function?\r\n                \r\nI have a homework assignment in which I need to compute the edit distance between two strings. I got the initial function to work but I've been having trouble with this part\n\n\n  Now add the cutoff into the edit distance. This shouldn't change what result are produced, but will drastically speed up the performance.\n\n\nHere's my original function:\n\n```\nstatic unsigned int compute_edit_distance(const char *const a,\n                                      const char *const b)\n{\n    if (strcmp(a, b) == 0) return 0;\n    if (a[0] == '\\0') return strlen(b);\n    if (b[0] == '\\0') return strlen(a);\n\n    unsigned int remove_from_a =\n        compute_edit_distance(a + 1, b) + 1;\n    unsigned int remove_from_b =\n        compute_edit_distance(a, b + 1) + 1;\n\n    unsigned int remove_from_both =\n        compute_edit_distance(a + 1, b + 1);\n    if (tolower(a[0]) != tolower(b[0])) ++remove_from_both;\n\n    return get_min(get_min(remove_from_a, remove_from_b),\n               remove_from_both);\n}\n```\n\n\nI've tried a few things, but none of them work. My latest change is this\n\n```\nif (depth == MAX_EDIT_DISTANCE_DEPTH)\n{\n    size_t a_length = strlen(a);\n    size_t b_length = strlen(b);\n    size_t max_length = (a_length > b_length) ? a_length : b_length;\n    return MAX_EDIT_DISTANCE_DEPTH + max_length;\n}\n```\n\n\nwith a new function signature\n\n```\nstatic unsigned int compute_edit_distance(const char *const a,\n                                      const char *const b, unsigned int depth)\n```\n\n\nbut that doesn't work either.\n\nCan I get a hint on how to do this right? Thanks!\n    ", "Answer": "\r\nThe easiest way is to pass in the \"depth remaining\" as a parameter. That is, the first call gets passed the cut-off depth, and all the recursive calls get passed a smaller number, which you determine by the type of edit made.\n\nThe fundamental idea is that in your first solution, the depth is calculated after the branch is explored recursively. That is, the calls are all made down the branch, then the numbers added together on the way back up the branch.\n\nYou can still do this to calculate the depth, but to prevent the branch from going too far, you pass in a running total of the editing budget you have already used in the calls on the way down the branch, or equivalently the editing budget that remains.\n\nYou'll need some trick to pass back a number from the failing branch to make sure the number will be rejected. For example return a number that you know will be too large, then check the result at the end. E.g., return MAX_DEPTH + 1, or similar.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein distance igonore overhanging bases\r\n                \r\nI would like an algorithm that reports that the edit distance between these two sequences is 2:\n\nGCGGCTCCTCTGGGGCGTTCCC\n\nGCGGCTCCTCTGGGGGGCGTTC\n\nThe first can be converted to the second with an insertion of 2 characters like so:\n\nGCGGCTCCTCTGGGGGGCGTTCCC\n\nGCGGCTCCTCTGGGGGGCGTTC\n\nThe lengths of the original two strings were 22.  The first 22 characters in these two strings are now identical.  The levenshtein distance between these two strings is 4, and I'd like a way of reporting an edit distance of 2 for these two strings. \n\nIs there a way to do this with the python package Levenshtein_distance function or Levenshtein python package I'm already using?\n\nMore details:\n\nI'm applying this to Next Generation Sequencing data.  I'd like to compare 2 sequences generated from a portion of each sequencing read.  The sequences are obtained from the start of the full length sequencing read and should be a unique sequence per sequencing read. \n\nExample:\nRead A: ATCGAACCGGTT\nRead B: ATGAACCGGTT\n\nWhere the first four bases of the strings will be used as the unique identifier of each read.  Sequence ATCG is the unique identifier for Read A and ATGA is the unique identifier for Read B.  Both reads contain the identical sequence \"AACCGGTT\".  When comparing the unique identifiers (ATCG and ATGA), I'd like a metric that returns an edit distance of 1 between the two sequences.\n\nRead A unique identifier:         ATCG\nRead B unique id after insertion: AT_GA\n\nThe reasons I think the overhanging bases on the right side of the string (end of the sequencing) should not be penalized, but they should be penalized on the left side of the sequence, are as follows:\n\n\nThe first and most important reason is that just because there are overhanging characters on the right side of the string (AKA the end of the sequence), that doesn't mean the characters don't align between the two sequences being compared.  It only means we don't have the corresponding characters from the other sequence to compare them to.  The same is not true for the left side of the string. \nUsually, the characters at the left side of the string (AKA the start of the sequencing read) are more confidently identified (have higher quality scores) than those on the right side.  \n\n    ", "Answer": "\r\nAlthough it is not difficult to write a customized function to calculate the 'distance', you can try ```\nedlib```\n first. Cause it is a very efficient tool to do this job.\n\nInput Read A:\n\n```\nATCGAACCGGTT\n```\n\n\nInput Read B:\n\n```\nATGAACCGGTTATG\n```\n\n\nAfter Alignment:\n\n```\nATCGAACCGGTT---   # these tailing gap will be ignored\nAT-GAACCGGTTATG   # the internal gap is meaningful\n```\n\n\nThere is python tag in your question, so I post a solution using edlib python wrapper.\n\n```\n>>> import edlib\n>>> edlib.align(\"ATCGAACCGGTT\",\"ATGAACCGGTTATG\", mode=\"SHW\")['editDistance']\n1 \n```\n\n\n\n  ```\nSHW```\n mode: gap at query end is not penalized\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Algorithm to compute edit set for transforming one string into another?\r\n                \r\nI'd like to compute the edits required to transform one string, A, into another string B using only inserts and deletions, with the minimum number of operations required.\n\nSo something like \"kitten\" -> \"sitting\" would yield a list of operations something like (\"delete at 0\", \"insert 's' at 0\", \"delete at 4\", \"insert 'i' at 3\", \"insert 'g' at 6\")\n\nIs there an algorithm to do this, note that I don't want the edit distance, I want the actual edits.\n    ", "Answer": "\r\nI had an assignment similar to this at one point. Try using an A* variant. Construct a graph of possible 'neighbors' for a given word and search outward using A* with the distance heuristic being the number of letter needed to change in the current word to reach the target. It should be clear as to why this is a good heuristic-it's always going to underestimate accurately. You could think of a neighbor as a word that can be reached from the current word only using one operation. It should be clear that this algorithm will correctly solve your problem optimally with slight modification.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Lower bound of recursive Levinshtein algorithm\r\n                \r\nWhat is the lower bound (Omega) of Levinshtein distance algorithm in terms of time complexity? Algorithm are as described: \n\n```\n// len_s and len_t are the number of characters in string s and t respectively\nint LevenshteinDistance(string s, int len_s, string t, int len_t)\n{\n  /* base case: empty strings */\n  if (len_s == 0) return len_t;\n  if (len_t == 0) return len_s;\n\n  /* test if last characters of the strings match */\n  if (s[len_s-1] == t[len_t-1])\n      cost = 0;\n  else\n      cost = 1;\n\n  /* return minimum of delete char from s, delete char from t, and delete char from both */\n  return minimum(LevenshteinDistance(s, len_s - 1, t, len_t    ) + 1,\n                 LevenshteinDistance(s, len_s    , t, len_t - 1) + 1,\n   `             LevenshteinDistance(s, len_s - 1, t, len_t - 1) + cost));\n}\n```\n\n\nI know this has been answered here: Complexity of edit distance (Levenshtein distance) recursion top down implementation. But I don't understand how Omega(2^(max(m,n))) is derived? I seek a derivation by either some kind of rule, example or mathematical derivation.\n    ", "Answer": "\r\nThe recursion depth is bounded below by max(m, n) for the two first calls as they decrease the length of exactly one string each time; the recursion depth is bounded below by min(m, n) for the three calls they decrease the length of the two strings (in the best case). This explains the powers of 2 or 3.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Total distance calculation from LatLng List\r\n                \r\nIm using dart/flutter and the ```\n'package:latlong/latlong.dart'```\n to parse a GPX file into a list of LatLng objects. That is working fine, but the next step is to find the total distance of the route.\n\nThe question here is\n\n\n  how can I get the total distance from a list of LatLng objects in\n  Dart?\n\n\nI have used Haversine formula before in Java, but not sure the best way to implement it using Dart. Any help of suggestion would be greatly appreciated.\n\nEdit\n\nBelow is an attempt to get total distance based on the answer below. For some reason though it doesn't add to ```\ntotalDistance```\n and doesn't return a double.\n\nThe below is suppose to iterate through a list of 60+ LatLng objects and find distance between each one, adding to a totalDistance double which is returned at the end of the loop.\n\n```\n static double getDistanceFromGPSPointsInRoute(List<LatLng> gpsList) {\n    double totalDistance = 0.0;\n\n    for (var i = 0; i < gpsList.length; i++) {\n      var p = 0.017453292519943295;\n      var c = cos;\n      var a = 0.5 -\n          c((gpsList[i + 1].latitude - gpsList[i].latitude) * p) / 2 +\n          c(gpsList[i].latitude * p) *\n              c(gpsList[i + 1].latitude * p) *\n              (1 - c((gpsList[i + 1].longitude - gpsList[i].longitude) * p)) /\n              2;\n      double distance = 12742 * asin(sqrt(a));\n      totalDistance += distance;\n      print('Distance is ${12742 * asin(sqrt(a))}');\n    }\n    print('Total distance is $totalDistance');\n    return totalDistance;\n  }\n```\n\n\nOutput\n\n```\nflutter: Distance is 0.004143962775784678\nflutter: Distance is 0.0041439635323316775\nflutter: Distance is 0.007796918986828574\nflutter: Distance is 0.007285385943437824\nflutter: Distance is 0.006890844300938902\nflutter: Distance is 0.006353952460010352\nflutter: Distance is 0.005560051252981138\n```\n\n\nAs you can see above, there is no mention of Total Distance.\n\nExample of LatLng List\n\n```\nflutter: -36.84975 , 174.646685\nflutter: -36.849692 , 174.646497\nflutter: -36.84967 , 174.646436\nflutter: -36.849578 , 174.646264\nflutter: -36.849502 , 174.646164\nflutter: -36.849367 , 174.646038\nflutter: -36.849209 , 174.645959\nflutter: -36.849155 , 174.64594\nflutter: -36.849107 , 174.645932\nflutter: -36.849058 , 174.645922\nflutter: -36.848952 , 174.645895\nflutter: -36.84886 , 174.645906\nflutter: -36.84879 , 174.645913\nflutter: -36.848748 , 174.645836\nflutter: -36.848744 , 174.645802\n```\n\n    ", "Answer": "\r\nTry this please. I tested it with Google Maps and works accurately. You can do a loop and find total distance in KM by using 2 points each time. I added some random dummy data to show how it works. Copy this code to https://dartpad.dartlang.org/ and test easily.\n```\nimport 'dart:math' show cos, sqrt, asin;\n\nvoid main() {\n  double calculateDistance(lat1, lon1, lat2, lon2){\n    var p = 0.017453292519943295;\n    var c = cos;\n    var a = 0.5 - c((lat2 - lat1) * p)/2 + \n          c(lat1 * p) * c(lat2 * p) * \n          (1 - c((lon2 - lon1) * p))/2;\n    return 12742 * asin(sqrt(a));\n  }\n  \n  List<dynamic> data = [\n    {\n      \"lat\": 44.968046,\n      \"lng\": -94.420307\n    },{\n      \"lat\": 44.33328,\n      \"lng\": -89.132008\n    },{\n      \"lat\": 33.755787,\n      \"lng\": -116.359998\n    },{\n      \"lat\": 33.844843,\n      \"lng\": -116.54911\n    },{\n      \"lat\": 44.92057,\n      \"lng\": -93.44786\n    },{\n      \"lat\": 44.240309,\n      \"lng\": -91.493619\n    },{\n      \"lat\": 44.968041,\n      \"lng\": -94.419696\n    },{\n      \"lat\": 44.333304,\n      \"lng\": -89.132027\n    },{\n      \"lat\": 33.755783,\n      \"lng\": -116.360066\n    },{\n      \"lat\": 33.844847,\n      \"lng\": -116.549069\n    },\n  ];\n  double totalDistance = 0;\n  for(var i = 0; i < data.length-1; i++){\n    totalDistance += calculateDistance(data[i][\"lat\"], data[i][\"lng\"], data[i+1][\"lat\"], data[i+1][\"lng\"]);\n  }\n  print(totalDistance);\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Dijkstra path (not only distance)\r\n                \r\nI have the code that calculates the minimum distance of Dijkstra path. Can you help me edit my code in order to not only print the minimum distance, but the full path too?\n\n```\n% min_dist(+Graph,+Start,-MinDist)\nmin_dist(Graph,Start,MinDist):-\n   dijkstra(Graph,[],[Start-0],MinDist).\n\nedge(g(Es,Vs),V1,V2,Value):-\n   member(e(V1,V2,Value),Vs) ;\n   member(e(V2,V1,Value),Vs).\n\nneighbourhood(Graph,V,NB):-\n   setof(V1-E,edge(Graph,V,V1,E),NB).\n\n% dijkstra(+Graph,+ClosedVertices,+OpenVertices,-Distances)\ndijkstra(_,MinDist,[],MinDist).\ndijkstra(Graph,Closed,Open,MinDist):-\n   choose_v(Open,V-D,RestOpen),\n   neighbourhood(Graph,V,NB),  % NB is a list of adjacent vertices+distance to V\n   diff(NB,Closed,NewNB),\n   merge(NewNB,RestOpen,D,NewOpen),\n   dijkstra(Graph,[V-D|Closed],NewOpen,MinDist).\n\n% choose_v(+OpenVertices,-VertexToExpand,-RestOpenVertices)\nchoose_v([H|T],MinV,Rest):-\n   choose_minv(T,H,MinV,Rest).\nchoose_minv([],MinV,MinV,[]).\nchoose_minv([H|T],M,MinV,[H2|Rest]):-\n   H=V1-D1, M=V-D,\n   (D1<D -> NextM=H,H2=M\n          ; NextM=M,H2=H),\n   choose_minv(T,NextM,MinV,Rest).\n\n% diff(+ListOfVertices,+Closed,-ListOfNonClosedVertices)\ndiff([],_,[]).\ndiff([H|T],Closed,L):-\n   H=V-D,\n   (member(V-_,Closed) -> L=NewT ; L=[H|NewT]),\n   diff(T,Closed,NewT).\n\n% merge(+ListOfVertices,+OldOpenVertices,-AllOpenVertices)\nmerge([],L,_,L).\nmerge([V1-D1|T],Open,D,NewOpen):-\n   (remove(Open,V1-D2,RestOpen)\n      -> VD is min(D2,D+D1)\n       ; RestOpen=Open,VD is D+D1),\n   NewOpen=[V1-VD|SubOpen],\n   merge(T,RestOpen,D,SubOpen).\n\nremove([H|T],H,T).\nremove([H|T],X,[H|NT]):-\n   H\\=X,\n   remove(T,X,NT).\n```\n\n\nThanks!\nEDIT: I have edited my code because I have forgot to add the neighbourhood and edge predicates.\n    ", "Answer": "\r\nNice source, well organized and well commented.\n\nI suggest you to modify your \"merge\" statements in order to, not only update the minimal distances, but include a third field with the vertice that gives this minimum.\n\n(warning: comment for theses statements lacks one argument).\n\nSomething like:\n\n```\nmerge([V1-D1|T],Open,V-D-_,[V1-VD-O|SubOpen]):-\n   (remove(Open,V1-D2-O2,RestOpen)\n      -> ( D2<D+D1 -> VD=D2, O=O2 ; VD is D+D1, O=V)\n      ; RestOpen=Open,VD is D+D1,O=V),\n   merge(T,RestOpen,D,SubOpen).\n```\n\n\nthat means you must adapt all remainder to pass terms of the form \"Vertice-Distance-Origin\".\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Fast fuzzy/approximate search in dictionary of strings in Ruby\r\n                \r\nI have a dictionary of 50K to 100K strings (can be up to 50+ characters) and I am trying to find whether a given string is in the dictionary with some \"edit\" distance tolerance. (Levenshtein for example). I am fine pre-computing any type of data structure before doing the search.\n\nMy goal to run thousands of strings against that dictionary as fast as possible and returns the closest neighbor. I would be fine just getting a boolean that say whether a given is in the dictionary or not if there was a significantly faster algorithm to do so\n\nFor this, I first tried to compute all the Levenshtein distances and take the minimum and it was obviously horribly slow. So I tried to implement a Levenshtein Trie based on this article http://stevehanov.ca/blog/index.php?id=114\n\nSee my gist here for reproducing the benchmark: https://gist.github.com/nicolasmeunier/7493947\n\nHere are a few benchmarks I got on my machine:\n\nEdit distance of 0 (perfect match)\n\n```\nBenchmark.measure { 10.times { dictionary.search(random_word, 0) } }\n<Benchmark::Tms:0x007fa59bad8908 @label=\"\", @real=0.010889, @cstime=0.0, @cutime=0.0, @stime=0.0, @utime=0.00999999999999801, @total=0.00999999999999801> \n```\n\n\n*Edit distance of 2, it becomes a LOT slower *\n\n```\nBenchmark.measure { 10.times { dictionary.search(random_word, 2) } }\n<Benchmark::Tms:0x007fa58c9ca778 @label=\"\", @real=3.404604, @cstime=0.0, @cutime=0.0, @stime=0.020000000000000018, @utime=3.3900000000000006, @total=3.4100000000000006>\n```\n\n\nAnd it goes downhill from there and become extremely slow for edit distance larger than 2. (1+ second on average per tested string).\n\nI would like to know how/if I could speed this up significantly. If there are existing solutions already implemented in ruby/gems, I also don't want to reinvent the wheel...\n\nEDIT 1: In my case, I expect most of the strings I am matching against the dictionary NOT to be in there. So if there are any algorithm to quickly discard a string, that could really help.\n\nThanks,\nNicolas\n    ", "Answer": "\r\nI wrote a pair of gems, fuzzily and blurrily which do trigrams-based fuzzy matching.\nGiven your (low) volume of data Fuzzily will be easier to integrate and about as fast, in with either you'd get answers within 5-10ms on modern hardware.\n\nGiven both are trigrams-based (which is indexable), not edit-distance-based (which isn't), you'd probably have to do this in two passes:\n\n\nfirst ask either gem for a set of best matches trigrams-wise\nthen compare results with your input string, using Levenstein\nand return the min for that measure.\n\n\nIn Ruby (as you asked), using Fuzzily + the Text gem, obtaining the records withing the edit distance threshold would look like:\n\n```\nMyRecords.find_by_fuzzy_name(input_string).select { |result|\n  Text::Levenshtein.distance(input_string, result.name)] < my_distance_threshold\n}\n```\n\n\nThis performas a handful of well optimized database queries and a few\n\nCaveats:\n\n\nif the \"minimal\" edit distance you're looking for is high, you'll still be doing lots of Levenshteins.\nusing trigrams assumes your input text is latin text or close to (european languages basically).\nthere probably are edge cases since nothing garantees that \"number of matching trigrams\" is a great general approximation to \"edit distance\".\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Tree edit distance, python code for Tree bracket format, Converting 2 dimensional tree to one string\r\n                \r\nCan someone help me with the code to breakdown a decision tree into a  1 dimensional tree bracket format in python. The algorithm to do so is \n\nINPUT: x—decision tree\n\nOUTPUT: TBF is the tree x in the tree bracket format\n\nFunction UpdateNodes(x):\n\nif NumberOfChildrenOfNM (x) is not null then\n\nNM (x) is the list of node attribute names of x\n\nSet TBF = TBF + ’{ ’+ NM (x)\n\nfor j = 1 to NumberOfChildrenOfNM (x) do\n\nCall UpdateNodes(x)\n\nSet TBF = TBF+ ’}’\n\nend for\n\nend if\n\nBut when I am trying to implement this I am getting error\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Scalding: Compare strings pairwise?\r\n                \r\nWith Scalding I need to:\n\n\nGroup string fields by first 3 chars\nCompare strings in all pairs in every group using ```\nedit-distance```\n metric ( http://en.wikipedia.org/wiki/Edit_distance)\nWrite results in CSV file where record is ```\nstring; string; distance```\n\n\n\nTo group strings I use ```\nmap```\n and ```\ngroupBy```\n as in the following example:  \n\n```\nimport cascading.tuple.Fields\nimport com.twitter.scalding._\n\nclass Scan(args: Args) extends Job(args) {\n  val output = TextLine(\"tmp/out.txt\")\n\n  val wordsList = List(\n    (\"aaaa\"),\n    (\"aaabb\"),\n    (\"aabbcc\"),\n    (\"aaabccdd\"),\n    (\"aaabbccdde\"),\n    (\"aaabbddd\"),\n    (\"bbbb\"),\n    (\"bbbaaa\"),\n    (\"bbaaabb\"),\n    (\"bbbcccc\"),\n    (\"bbbddde\"),\n    (\"ccccc\"),\n    (\"cccaaa\"),\n    (\"ccccaabbb\"),\n    (\"ccbbbddd\"),\n    (\"cdddeee\")\n    )\n\n  val orderedPipe =\n    IterableSource[(String)](wordsList, ('word))\n        .map('word -> 'key ){word:String => word.take(3)}\n    .groupBy('key) {_.toList[String]('word -> 'x) }\n        .debug\n        .write(output)\n}\n```\n\n\nAs a result I get:\n\n```\n['aaa', 'List(aaabbddd, aaabbccdde, aaabccdd, aaabb, aaaa)']\n['aab', 'List(aabbcc)']\n['bba', 'List(bbaaabb)']\n['bbb', 'List(bbbddde, bbbcccc, bbbaaa, bbbb)']\n['ccb', 'List(ccbbbddd)']\n['ccc', 'List(ccccaabbb, cccaaa, ccccc)']\n['cdd', 'List(cdddeee)']\n```\n\n\nNow, in this example, I need to comute edit-distance for strings with ```\naaa```\n key in this list:\n\n```\nList(aaabbddd, aaabbccdde, aaabccdd, aaabb, aaaa)\n```\n\n\nnext for all strings with 'bbb' key in this list:\n\n```\nList(bbbddde, bbbcccc, bbbaaa, bbbb)\n```\n\n\netc.\n\nTo compute edit-distance between all strings in every group I need to replace ```\ntoList```\n with my own function, how can I do this? And also how can I write results of my function to a CSV file?\n\nThanks!\n\nUpdate\n\nHow to get ```\nList```\n from Scalding ```\nPipe```\n?\n\n```\ntoList```\n just returns another ```\nPipe```\n so I can't use it all:\n\n```\n  val orderedPipe =\n    IterableSource[(String)](wordsList, ('word))\n        .map('word -> 'key ){word:String => word.take(3)}\n        .groupBy('key) {_.toList[String]('word -> 'x) }\n        .combinations(2) //---ERROR! Pipe has no such method!\n        .debug\n        .write(output)\n```\n\n    ", "Answer": "\r\nThe edit-distance can be calculated as described in wikipedia:\n\n```\ndef editDistance(a: String, b: String): Int = {\n\n    import scala.math.min\n\n    def min3(x: Int, y: Int, z: Int) = min(min(x, y), z)\n\n    val (m, n) = (a.length, b.length)\n\n    val matrix = Array.fill(m + 1, n + 1)(0)\n\n    for (i <- 0 to m; j <- 0 to n) {\n\n        matrix(i)(j) = if (i == 0) j\n                       else if (j == 0) i\n                       else if (a(i-1) == b(j-1)) matrix(i-1)(j-1)\n                       else min3(\n                                 matrix(i - 1)(j) + 1,\n                                 matrix(i)(j-1) + 1,\n                                 matrix(i - 1)(j - 1) + 1) \n    }\n\n    matrix(m)(n)\n}\n```\n\n\nFor finding pairwise edit-distance of elements of a list:\n\n```\ndef editDistances(list: List[String]) = {\n\n    list.combinations(2).toList.map(x => (x(0), x(1), editDistance(x(0), x(1))))\n}\n```\n\n\nuse this in groupBy:\n\n```\n  val orderedPipe =\n      IterableSource[(String)](wordsList, ('word))\n      .map('word -> 'key ){word:String => word.take(3)}\n      .groupBy('key) {_.mapList[String, List[(String, String, Int)]]('word -> 'x)(editDistances)}\n      .debug\n      .write(output)    \n```\n\n\nAs far as writing to csv format is concerned, you can simply use the ```\ncom.twitter.scalding.Csv```\n class.\n\n```\nwrite(Csv(outputFile))```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "data mining cluster in Non-Euclidean Spaces\r\n                \r\nConsider the space of strings with edit distance as the distance\nmeasure. Give an example of a set of strings such that if we choose the clustroid\nby minimizing the sum of the distances to the other points we get one point\nas the clustroid, but if we choose the clustroid by minimizing the maximum\ndistance to the other points, another point becomes the clustroid.\n\nI meet the challenge of this topic. Can anyone help me out?\n    ", "Answer": "\r\nTry this set of strings:\n\n```\nbadger\nbadger\nbadger\nbadger\nbadger\nbanana\nnanana\n```\n\n\nWhat string has the minimum sum, what string the smallest maximum distance?\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Clustering string data with ELKI\r\n                \r\nI need to cluster a large number of strings using ELKI based on the Edit Distance / Levenshtein Distance. Since the data set is too large, I'd like to avoid file based precomputed distance matrices. How can I\n\n(a) load string data in ELKI from a file (only \"Labels\")?\n\n(b) implement a distance function accessing the labels (extend AbstractDBIDDistanceFunction, but how to get the labels?)\n\nSome code snippets or example input files would be helpful.\n    ", "Answer": "\r\nIt's actually pretty straightforward:\n\nA) write a ```\nParser```\n that is adequate for your input file format (why try to reuse a parser written for numerical vectors with labels?), probably subclassing ```\nAbstractStreamingParser```\n, producing a relation of the desired data type (probably you can just use ```\nString```\n. If you want to be a bit more general ```\nTokenSequence```\n may be a more appropriate concept for these distances. Strings are just the simplest case.\n\nB) implement a ```\nDistanceFunction```\n based on this vector type instead of DBIDs, i.e. a ```\nPrimitiveDistanceFunction<String>```\n. Again, subclassing ```\nAbstractPrimitiveDistanceFunction```\n may be the easiest thing to do.\n\nFor performance reasons, you may also want to look into indexing algorithms to retrieve e.g. the k most similar strings efficiently. I'm not sure which index structures exist for string edit distance and levenshtein distance.\n\nA colleague has a student that apparently has some working token edit distances, but I have not seen or reviewed the code yet. As he is processing log files, he will probably be using a token based approach instead of characters.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Clustering string data with ELKI\r\n                \r\nI need to cluster a large number of strings using ELKI based on the Edit Distance / Levenshtein Distance. Since the data set is too large, I'd like to avoid file based precomputed distance matrices. How can I\n\n(a) load string data in ELKI from a file (only \"Labels\")?\n\n(b) implement a distance function accessing the labels (extend AbstractDBIDDistanceFunction, but how to get the labels?)\n\nSome code snippets or example input files would be helpful.\n    ", "Answer": "\r\nIt's actually pretty straightforward:\n\nA) write a ```\nParser```\n that is adequate for your input file format (why try to reuse a parser written for numerical vectors with labels?), probably subclassing ```\nAbstractStreamingParser```\n, producing a relation of the desired data type (probably you can just use ```\nString```\n. If you want to be a bit more general ```\nTokenSequence```\n may be a more appropriate concept for these distances. Strings are just the simplest case.\n\nB) implement a ```\nDistanceFunction```\n based on this vector type instead of DBIDs, i.e. a ```\nPrimitiveDistanceFunction<String>```\n. Again, subclassing ```\nAbstractPrimitiveDistanceFunction```\n may be the easiest thing to do.\n\nFor performance reasons, you may also want to look into indexing algorithms to retrieve e.g. the k most similar strings efficiently. I'm not sure which index structures exist for string edit distance and levenshtein distance.\n\nA colleague has a student that apparently has some working token edit distances, but I have not seen or reviewed the code yet. As he is processing log files, he will probably be using a token based approach instead of characters.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to choose the proper maximum value for Damerau-Levenshtein distance?\r\n                \r\nI am using the Damerau-Levenshtein code available from here in my similarity measurements. The problem is that when I apply the Damerau-Levenshtein on two strings such as ```\ncat sat on a mat```\n and ```\ndog sat mat```\n, I am getting edit distance as 8. This similarity results can get any number regarding insertion, deletion or substitution like any range from 0, 1, 2, ... . Now I am wondering if there is any way that we can assume or find a maximum of this distance (similarity) and converted between 0 and 1 or how can we set the max value that at least I can say: ```\ndistance =1 - similarity```\n.\nThe reason for this post is that I am setting a threshold for a few distance metrics like cosine, Levenstein and damerau levenstein and outputs of all should be betweeb zero and 1.  \n    ", "Answer": "\r\n```\nLevenshtein Distance score = number of insertion + number of deletion + number of substitution.```\n\nSo the maximum value is 3 X(multiplied) the maximum length string in your data set.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Leaflet screen distance to LatLng distance\r\n                \r\nI want to combine markers based on the zoom level. I'm not using Markercluster but my own algorithm to detect which markers should be combined. This algorithm works perfectly fine. The only thing I have to add is a zoom-based condition when the markers should be combined. Currently, all markers within a distance of 0.001 get combined. What I want is that every marker within a distance of 0.5cm on the screen gets combined. So I need a function that converts a distance in cm, px, or something else into a distance in degree.\nAn example: On zoom-level 18, two markers with a distance of 0.00005 in the longitude have a distance of nearly 0.3cm on my screen.\nEDIT:\nSo this is what I did:\n```\nfunction in_range(location1, location2, map) {\n    return get_distance_in_px(location1, location2, map) < 25;\n}\n\nfunction get_distance_in_px(location1, location2, map) {\n    var p1 = map.latLngToContainerPoint(L.latLng(location1[0], location1[1]));\n    var p2 = map.latLngToContainerPoint(L.latLng(location2[0], location2[1]));\n\n    var a = p1.x - p2.x;\n    var b = p1.y - p2.y;\n\n    return Math.sqrt(a * a + b * b);\n}\n```\n\n    ", "Answer": "\r\nYou can get the containerPoint of the latlngs and then calculate the distance:\n```\n\nmap.on('zoomend',(e)=>{\n    console.log(e);\n    recalcZoom();\n});\n\nvar DISTANCE = 20; //px\nfunction recalcZoom() {\n    var layers = findLayers(map);\n    var resultLayers = []; //sturcture: {main: layer, childs: [];\n    layers.forEach((layer)=>{\n        if(resultLayers.length === 0){ // set the first layer as main layer\n            resultLayers.push({\n                main: layer,\n                childs: []\n            });\n        }else{\n            var found = false;\n            var mainObj = null;\n            var lastDis = null;\n            resultLayers.forEach((rLayer)=>{\n                var main = rLayer.main;\n                var p1 = map.latLngToContainerPoint(main.getLatLng());\n                var p2 = map.latLngToContainerPoint(layer.getLatLng());\n                var dis = p1.distanceTo(p2);\n                if(dis <= DISTANCE){ // distance between main layer and current marker is lower then DISTANCE\n                    if(lastDis == null || dis < lastDis) { // a main layer is found, where the distance between them is lower\n                        if(mainObj && mainObj.childs.indexOf(layer) > -1){ // remove the layer from the old main layer childs array\n                            mainObj.splice(mainObj.childs.indexOf(layer),1);\n                        }\n\n                        rLayer.childs.push(layer);\n                        found = true;\n                        mainObj = rLayer;\n                    }\n                }\n            });\n            if(!found){ // if no main layer is found, add it as new main layer\n                resultLayers.push({\n                    main: layer,\n                    childs: []\n                });\n            }\n        }\n    });\n\n    console.log(resultLayers);\n    // Logic to find center of all childs + main\n    // remove the old layers and add the new clustered layer\n    // keep in mind, that you have to store somewhere the original markers, else you can't recalc the clusters\n}\n\nfunction findLayers(map) {\n// your logic to get the original layers\n    let layers = [];\n    map.eachLayer(layer => {\n        if (layer instanceof L.Marker) {\n            layers.push(layer);\n        }\n    });\n    return layers;\n}\n```\n\nYou have to implement by yourself the logic to find center of all childs + main layer, then remove the old layers and add the new clustered layer.\nBut keep in mind, that you have to store somewhere the original markers, else you can't recalc the clusters.\nLittle Example: https://jsfiddle.net/falkedesign/ny9s17cb/ (look into the console)\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Norvig's spell checker, how he implements the conditional probability?\r\n                \r\nWhen defining the conditional probability, he took a shortcut: \n\n\n  So I took a shortcut: I defined a trivial model that says all known words of edit distance 1 are infinitely more probable than known words of edit distance 2, and infinitely less probable than a known word of edit distance 0. By \"known word\" I mean a word that we have seen in the language model training data -- a word in the dictionary. We can implement this strategy as follows:\n\n\n```\ndef known(words): return set(w for w in words if w in NWORDS)\ndef correct(word):\n    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n    return max(candidates, key=NWORDS.get)\n```\n\n\nI don't see how this code implements his strategy. To me the last line of return is just returing the word has a highest counts/prior, instead of the priority list in his model.\n\nand also in defining his word counting dictionary: \n\n```\ndef train(features):\nmodel = collections.defaultdict(lambda: 1)\nfor f in features:\n    model[f] += 1\nreturn model\n```\n\n\nWhy didn't he start from 0? I mean shouldn't the default_factory be (lambda:0) or (int)? \n\nCan anyone explain? You can find the full article here: http://norvig.com/spell-correct.html\n\nThanks \n    ", "Answer": "\r\nThe priority list is implemented by the ```\nor```\n. If ```\nknown([word])```\n is non-empty set, its value is the value of the expression. If it's empty, the right-hand side\n\n```\nknown(edits1(word)) or known_edits2(word) or [word]\n```\n\n\nis evaluated. E.g.\n\n```\n>>> [1, 2, 3] or [4, 5, 6]\n[1, 2, 3]\n>>> [] or [4, 5, 6]\n[4, 5, 6]\n```\n\n\n\n  Why didn't he start from 0?\n\n\nThat's Laplace smoothing. It's actually explained in the article.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "how to get Distance of Object from iPhone camera using image Exif meta data?\r\n                \r\n\n\nEdit: Sorry for late Edit, Without two parameter you cannot calculate it, \n So first need to fill user Camera height from ground.\n\n\n\nI have check a number of solutions but none of them helpful!\n\nI know that Working Distance = (Sensor Height + Subject Height) * Focal Length / Sensor Height\n\nand\n\n```\ndistance to object (mm) = focal length (mm) * real height of the object (mm) * image height (pixels)\n                          ----------------------------------------------------------------\n                                object height (pixels) * sensor height (mm)\n```\n\n\nAnd I want to get distance from this:\n\nImage Formation by Lenses and the Eye\n\nHello I get the following info using image Exif  ALAssetsLibrary\n\nAnd I got following meta data :\n\n```\nSave image metadata.\n {\n    DPIHeight = 72;\n    DPIWidth = 72;\n    FaceRegions =     {\n        Regions =         {\n            HeightAppliedTo = 2448;\n            RegionList =             (\n                                {\n                    AngleInfoRoll = 270;\n                    AngleInfoYaw = 0;\n                    ConfidenceLevel = 376;\n                    FaceID = 1;\n                    Height = \"0.1413399\";\n                    Timestamp = 5996166864910;\n                    Type = Face;\n                    Width = \"0.1060049\";\n                    X = \"0.3560049\";\n                    Y = \"0.4746732\";\n                }\n            );\n            WidthAppliedTo = 3264;\n        };\n    };\n    Orientation = 6;\n    \"{Exif}\" =     {\n        ApertureValue = \"2.526068811667587\";\n        BrightnessValue = \"1.291629806962232\";\n        ColorSpace = 1;\n        DateTimeDigitized = \"2014:03:25 15:43:36\";\n        DateTimeOriginal = \"2014:03:25 15:43:36\";\n        ExposureMode = 0;\n        ExposureProgram = 2;\n        ExposureTime = \"0.05\";\n        FNumber = \"2.4\";\n        Flash = 24;\n        FocalLenIn35mmFilm = 33;\n        FocalLength = \"4.12\";\n        ISOSpeedRatings =         (\n            160\n        );\n        LensMake = Apple;\n        LensModel = \"iPhone 5 back camera 4.12mm f/2.4\";\n        LensSpecification =         (\n            \"4.12\",\n            \"4.12\",\n            \"2.4\",\n            \"2.4\"\n        );\n        MeteringMode = 5;\n        PixelXDimension = 3264;\n        PixelYDimension = 2448;\n        SceneType = 1;\n        SensingMethod = 2;\n        ShutterSpeedValue = \"4.321956949076723\";\n        SubjectArea =         (\n            1631,\n            1223,\n            1795,\n            1077\n        );\n        SubsecTimeDigitized = 261;\n        SubsecTimeOriginal = 261;\n        UserComment = hoge;\n        WhiteBalance = 0;\n    };\n    \"{GPS}\" =     {\n        Altitude = \"196.008\";\n        AltitudeRef = 0;\n        DateStamp = \"2014:03:25\";\n        Latitude = \"28.61772\";\n        LatitudeRef = N;\n        Longitude = \"77.38891\";\n        LongitudeRef = E;\n        TimeStamp = \"10:13:37.439000\";\n    };\n    \"{MakerApple}\" =     {\n        1 = 0;\n        3 =         {\n            epoch = 0;\n            flags = 1;\n            timescale = 1000000000;\n            value = 249840592070541;\n        };\n        4 = 0;\n        5 = 179;\n        6 = 139;\n        7 = 1;\n    };\n    \"{TIFF}\" =     {\n        DateTime = \"2014:03:25 15:43:36\";\n        Make = Apple;\n        Model = \"iPhone 5\";\n        Software = \"7.0.6\";\n        XResolution = 72;\n        YResolution = 72;\n    };\n}\n```\n\n\nI need to calculate the distance of the object from the camera, using the above details; using iphone4s,iphone5, or iphone5s.\nIs it possible?\n\n\n\nModified Need to know formula used by this app any idea:\n\nNeed to know any method \n\nhttp://www.youtube.com/watch?v=eCStIagorx8 \n\nhow this App working ???\nAll Help are welcome\n    ", "Answer": "\r\n@iphonemaclover. As many people have pointed out there's insufficient information to calculate this purely using trigonometry.  However, depending on how accurate you need to be, or what it is you're trying to measure and other data you can expect to recover from an iphone and/or you're willing to make some assumptions, it is possible to make some inroads. \n\na) +1 to Martin R, if you assume a flat earth, a phone height (which as per the app you quoted) a user could update for calibration purposes, can recover pitch information and know where a point on the ground at the base of your object is then this is simple trig. Once there's a estimate for the distance and assuming the thing you're measuring is close to vertical (or sits at a known angle) then its height can also be calculated.\n\nb) Your exif file contains face region information. If you're interested in people and are happy to make an assumption that they're an adult then you could use an average head size assumption to estimate distance using the method you've outlined already.\n\nc) If you can recover a series of images and camera positions and the camera / object positions vary then I believe that 3d information can be recovered using projective geometry. \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "elasticsearch distance sorting and calculation incorrect and inaccurate\r\n                \r\nI'm using elasticsearch for a search functionality on a website. The search must provide a search by distance from a location in an - for example - 25km radius.\n\nThat's no problem, I'm using the following elasticsearch query array:\n\nedit 3: updated the query with function_score\n\n```\n{\n    \"index\": \"kasd9i9021profiles\",\n    \"type\": \"profile\",\n    \"size\": 30,\n    \"from\": 0,\n    \"body\": {\n    \"query\": {\n        \"function_score\": {\n            \"functions\": [\n                {\n                    \"linear\": {\n                        \"location\": {\n                            \"origin\": \"49.449919468911,11.073560787681\",\n                            \"offset\": \"2km\",\n                            \"scale\": \"1km\"\n                        }\n                    }\n                }\n            ],\n            \"score_mode\": \"avg\",\n            \"boost_mode\": \"replace\",\n            \"query\": {\n                \"bool\": {\n                    \"must\": [\n                        {\n                            \"term\": {\n                                \"published\": \"1\"\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"country\": \"DE\"\n                            }\n                        }\n                    ],\n                    \"filter\": {\n                        \"geo_distance\": {\n                            \"distance\": \"25km\",\n                            \"distance_type\": \"arc\",\n                            \"_cache\": true,\n                            \"location\": {\n                                \"lat\": 49.449919468911,\n                                \"lon\": 11.073560787681\n                            },\n                            \"unit\": \"km\"\n                        }\n                    }\n                }\n            }\n        }\n    },\n    \"aggs\": {\n        \"rings\": {\n            \"geo_distance\": {\n                \"field\": \"location\",\n                \"origin\": \"49.449919468911,11.073560787681\",\n                \"distance_type\": \"arc\",\n                \"unit\": \"km\",\n                \"ranges\": [\n                    {\n                        \"to\": 25\n                    }\n                ]\n            }\n        }\n    },\n    \"script_fields\": {\n        \"distance\": {\n            \"lang\": \"groovy\",\n            \"params\": {\n                \"lat\": 49.449919468911,\n                \"lon\": 11.073560787681\n            },\n            \"script\": \"doc['location'].distanceInKm(lat,lon)\"\n        }\n    },\n    \"sort\": [\n        {\n            \"upgrade_sort\": {\n                \"order\": \"desc\"\n            }\n        },\n        {\n            \"has_siegel\": {\n                \"order\": \"desc\"\n            }\n        },\n        {\n            \"_geo_distance\": {\n                \"location\": {\n                    \"lat\": 49.449919468911,\n                    \"lon\": 11.073560787681\n                },\n                \"order\": \"desc\",\n                \"unit\": \"km\"\n            }\n        }\n    ]\n    },\n    \"fields\": [\n    \"_source\",\n    \"distance\"\n    ]\n}\n```\n\n\nThe value of \"upgrade_sort\" can be between 0 and 3.\nThe value of \"has_siegel\" can be true or false.\n\nThe problems are:\n\n\nThe results are not sorted by distance\nThe distance in the result is between 0 and ~30km, not between 0 and 25km.\n\n\nIs that a bug or a wrong query?\n\nEDIT 1\n\nMapping:\n\n```\n$params = [\n    'index' => $name,\n    'body' => [\n        'mappings' => [\n            'profile' => [\n                'properties' => [\n                    'name' => [\n                        'type' => 'string',\n                    ],\n                    'logo_url' => [\n                        'type' => 'string',\n                    ],\n                    'foto_url' => [\n                        'type' => 'string',\n                    ],\n                    'info_text' => [\n                        'type' => 'string',\n                    ],\n                    'cPerson' => [\n                        'type' => 'string',\n                    ],\n                    'street' => [\n                        'type' => 'string',\n                    ],\n                    'city' => [\n                        'type' => 'string',\n                    ],\n                    'country' => [\n                        'type' => 'string',\n                    ],\n                    'website' => [\n                        'type' => 'string',\n                    ],\n                    'location' => [\n                        'type' => 'geo_point'\n                    ],\n                    'upgrade_sort' => [\n                        'type' => 'integer'\n                    ],\n                ]\n            ]\n        ]\n    ]\n];\n```\n\n\nResults: For privacy reasons I can not publish the full results. Here are the distance details:\n\nEDIT 2\n\nCoordinates addet\n\n```\ndistance: 9,82km\nhas_siegel: 0\nupgrade_sort: 3\nlon: 10.988270100000022\nlat: 49.4724229\n\ndistance: 10,87km\nhas_siegel: 0\nupgrade_sort: 2\nlon: 10.980907500000058\nlat: 49.4808363\n\ndistance: 15,71km\nhas_siegel: 0\nupgrade_sort: 1\nlon: 11.017770000000041\nlat: 49.5795\n\ndistance: 0,15km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.072768300000007\nlat: 49.4488594\n\ndistance: 0,32km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.07072740000001\nlat: 49.44968069999999\n\ndistance: 0,32km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.07072740000001\nlat: 49.44968069999999\n\ndistance: 0,47km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.077651400000036\nlat: 49.4487752\n\ndistance: 0,60km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.078965499999981\nlat: 49.4501188\n\ndistance: 0,61km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.078960000000052\nlat: 49.45074\n\ndistance: 0,56km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.076950099999976\nlat: 49.4536002\n\ndistance: 0,83km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.080846599999973\nlat: 49.4483038\n\ndistance: 0,70km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.07790239999997\nlat: 49.45442389999999\n\ndistance: 0,70km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.07790239999997\nlat: 49.45442389999999\n\ndistance: 0,94km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.065464399999996\nlat: 49.4475953\n\ndistance: 0,72km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.075771300000042\nlat: 49.4560251\n\ndistance: 1,14km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.066973200000007\nlat: 49.4578074\n\ndistance: 1,40km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.084297399999969\nlat: 49.4563874\n\ndistance: 1,23km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.077985799999965\nlat: 49.4397852\n\ndistance: 1,41km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.081709400000022\nlat: 49.440276\n\ndistance: 1,51km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.067300100000011\nlat: 49.4619894\n\ndistance: 2,48km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.092409999999973\nlat: 49.4618\n\ndistance: 3,65km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.104663200000005\nlat: 49.4602779\n\ndistance: 3,74km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.100480000000061\nlat: 49.47002\n\ndistance: 4,12km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.043360000000007\nlat: 49.42859\n\ndistance: 4,16km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.100729999999999\nlat: 49.47564\n\ndistance: 5,61km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.032510600000023\nlat: 49.4207607\n\ndistance: 5,41km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.093795699999987\nlat: 49.4057095\n\ndistance: 6,60km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.033873800000038\nlat: 49.4058283\n\ndistance: 7,51km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.016179100000045\nlat: 49.4143705\n\ndistance: 7,49km\nhas_siegel: 0\nupgrade_sort: 0\nlon: 11.130335599999967\nlat: 49.41382410000001 \n```\n\n    ", "Answer": "\r\nYour filter only excludes results that are not within the 25km radius circle. If you want to sort the results by distance you must use a function score query with a decay function.\n\nFor the distance, if you want it to be more accurate, as mentioned in the doc you should use the arc distance_type.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Algorithm to compute similarity of two strings in javascript [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nIs there any text similarity algorithm in javascript? I want to compare too essays to determine how similar they are. I was thinking about edit distance, but I don't know how to translate it into percentage.\n    ", "Answer": "\r\nTake a look at jsdifflib, a javascript implementation of python's SequenceMatcher. You can get the similar percentage:\ndifflib.ratio(string1, string2) * 100. Here is the demo. Hope this is what you want. \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Optimize the code [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is off-topic. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\r\n                \r\n                    \r\n                        Closed 12 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI am new to python. I would like all the python gurus to suggest some ways to make the following code more pythonic and hence more efficient .\nIts a simple code to find the edit distance between two words.\n\nP.S. I would like improvements in code, not in the logic or algorithm optimization.\n\n```\nclass test:\n    def __init__(self,a,b,I=1,D=1,R=1):\n        self.a = a\n        self.b = b\n        self.mem = dict()\n        self.la = len(a)\n        self.lb = len(b)\n        self.I = I\n        self.D = D\n        self.R = R\n\n    def diff(self,i=0,j=0):\n        T = self.diff\n        memo = self.mem\n        if j == self.lb: return self.D * i\n        if i == self.la: return self.D * j\n        if (i,j) in memo:\n            return memo[(i,j)]\n        if self.a[i] == self.b[j]:\n            memo[(i,j)] = T( i+1,j+1 )\n            return memo[ (i,j) ]\n        memo[(i,j)] = min(self.R + T(i+1,j+1) , self.D + T(i+1,j) , self.I + T(i,j+1) ,\n         self.D + T(i,j+1) , self.I + T(i+1,j) )\n        return memo[(i,j)]\n```\n\n\nVariable explanation:\n\na,b are two string whose edit distance is to be found.\nI,D,R Insertion Deletion and Replace cost of a single letter.\nmem is dictionary used to memoize the recursive calls.\ni and j are the pointers of the string a and b respectively\n    ", "Answer": "\r\nPythonic would be:\n\n\nWrite lots of unit tests.\nDon't reinvent the wheel: search online for previous solutions to the problem. See the comments.\nDon't optimise prematurely: profile your code to work out whether this really is\n a bottleneck and if it is improve the algorithm. \nMeaningful variable names\nDon't start local variable names with capital letters\nUse normal whitespace inside parentheses ```\nT(i+1, j+1)```\n not ```\nT( i+1,j+1 )```\n\nDon't use spurious parentheses round tuples: ```\nmemo[i,j]```\n not ```\nmemo[(i,j)]```\n\nDon't optimise prematurely: ```\nself.diff(i+1, j+1)```\n not ```\nT(i+1,j+1)```\n\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Finding minimum of a set of recursion results\r\n                \r\nI am encoding the edit distance recurrence equation in Dafny.\n\nI think I've got it verified, but I am curious if there's a more succinct way of representing the choice the recurrence takes among the three edit choices:\n\n```\nE(i,j) := min { 1 + E(i-1,j), 1 + E(i, j-1), diff(i, j) + E(i-1,j-1) }\n```\n\n\nIn Dafny:\n\n```\nif 1 + recEdDist'(a, b, ai+1, bi) < 1 + recEdDist'(a, b, ai, bi+1) \n        && 1 + recEdDist'(a, b, ai+1, bi) < diff(a[ai], b[bi]) + recEdDist'(a, b, ai+1, bi+1)\nthen 1 + recEdDist'(a, b, ai+1, bi) else\nif 1 + recEdDist'(a, b, ai, bi+1) < 1 + recEdDist'(a, b, ai+1, bi) \n        && 1 + recEdDist'(a, b, ai, bi+1) < diff(a[ai], b[bi]) + recEdDist'(a, b, ai+1, bi+1)\nthen 1 + recEdDist'(a, b, ai, bi+1) else\ndiff(a[ai], b[bi]) + recEdDist'(a, b, ai+1, bi+1)\n```\n\n\nFull source here.\n    ", "Answer": "\r\nIf you define\n\n```\nfunction min(a: int, b: int) : int\n{ if a < b then a else b }\n```\n\n\nthe following expression gives the minimum of your three arguments:\n\n```\nmin(1 + E(i-1,j), (min(1 + E(i, j-1), diff(i, j) + E(i-1,j-1))\n```\n\n\nand Dafny proves for you things like this, in case you would be worried:\n\n```\n  assert min(a, min(b, c)) <= a; \n  assert min(a, min(b, c)) <= b; \n  assert min(a, min(b, c)) <= c; \n  assert min(a, min(b, c)) == a || min(a, min(b, c)) == b || min(a, min(b, c)) ==c;\n```\n\n\nYou say that you think you've got it verified. But what I see is a Dafny file with only a function definition and a few tests. What is verified is then only the well-definedness of the inductive function and these few test cases. Did you mean that by verification, or do you also have an implementation that is verified against this function?  \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Remove Pandas rows that contain approximate string matches\r\n                \r\nI want to remove rows from a Pandas dataframe that have the same value in column A and are an approximate match in column B (by edit distance). Ex:\n\n\n\n\nIndex\nA\nB\n\n\n\n\n0\n'Apple'\n'bicycle'\n\n\n1\n'Apple'\n'gigantic bicycle'\n\n\n2\n'Apple'\n'a bicycle'\n\n\n3\n'Peach'\n'bicycle'\n\n\n4\n'Apple'\n'~bicycle**'\n\n\n5\n'Peach'\n'airplane'\n\n\n6\n'Apple'\n'car'\n\n\n7\n'Apple'\n'cars'\n\n\n\n\nIn this case I want the resulting data frame to be:\n\n\n\n\nIndex\nA\nB\n\n\n\n\n0\n'Apple'\n'bicycle'\n\n\n1\n'Apple'\n'gigantic bicycle'\n\n\n3\n'Peach'\n'bicycle'\n\n\n5\n'Peach'\n'airplane'\n\n\n6\n'Apple'\n'car'\n\n\n\n\nRows 0 and 3 both remain because the value in column A is different, and row 1 remains because 'gigantic bicycle' is different enough from 'bicycle'. We want to remove matches closer than say an edit distance of 3.\nI found the get_close_matches() in the difflib library has a decent matching system but doesn't seem to compare by edit distance so it's not quite what I need.\n    ", "Answer": "\r\nI would suggest looking into using Levenshtein distance. I believe there is a Python library for calculating this.\nhttps://pypi.org/project/python-Levenshtein/\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to use levenshtein function in Sequelize ORM?\r\n                \r\nI am trying to perform a search query in PostgreSQL database using Sequelize orm.\nThe Levenshtein distance (a.k.a edit distance) is a measure of similarity between two strings. It is defined as the minimum number of changes required to convert string a into string b (this is done by inserting, deleting or replacing a character in string a). The smaller the Levenshtein distance, the more similar the strings are.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Importing Distance Matrix from Excel to Python\r\n                \r\nI have the following distance matrix in Excel format.\nExcel Distance Matrix\nI need to convert it to python as a distance matrix. However, I can not achieve it right now.\nCould you help me, please?\nThank you\nEdit: This is my code and how the output looks like\nI need to use this output as distance matrix.\n    ", "Answer": "\r\nInstall ```\npandas```\n:\n\nAnaconda environment: ```\nconda install pandas```\n\nPython environment: ```\npip install pandas```\n\n\nAnd load your file:\n```\nimport pandas\n\ndf = pd.read_excel('myfile.xlsx')\n```\n\nRead more:\n\nhttps://pandas.pydata.org/docs/user_guide/io.html#reading-excel-files\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein distance and triangle inequality\r\n                \r\nI am confused about Levenshtein distance and triangle inequality. Wikipedia and other articles say that Levenshtein distance follows triangle inequality. \n\nTriangle inequality states ```\nx+y>z```\n, but for Levenshtein distance, it appears to me that ```\nx+y```\n can be equal to ```\nz```\n. For example, ```\nkitten-> sitting=3```\n, ```\nkitten->sittin=2```\n and ```\nsittin->sitting=1```\n. What am I missing here?\n\nEDIT\n\nThe triangle inequality is not in eucledian space, but metric space. In a metric space, the triangle inequality is ```\nd(x,z)<= d(x,y)+d(y,z)```\n\n    ", "Answer": "\r\nTriangle inequality states ```\nx+y>=z```\n.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Find the closest string using LD (Levenshtein Distance) against a large number of strings in a parallel IronPython script using more than one core?\r\n                \r\nI have the algorithm working in a single thread keeping the shortest edit distance returned.  I understand the built in LD function is written in C already but I have thousands of string to search for the best match and the strings can average 50 characters in length.  So this sounds like a good application for parallel where I could divide the list in 2 and portion the calls out to two different threads running on 2 separate cores.  Ideally I think it would be better to have a special version of LD (Levenshtein Distance) algorithm that would take the best match so far as a parameter also and return early (before the whole distance is calculated) if the distance so far exceed the best match obtained on a prior sting in the list.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Taking altitude into account when calculating geodesic distance\r\n                \r\ni´m currently dealing with gps data combined with precise altitude measurement. \nI want to calculate the distance between two consecuting points. There is a lot\nof information out there about calculating distance between two points using the WGS84 ellipsoid and so on.\n\nhowever, i did not find any information that takes Altitude changes into account for this\ndistance calculation.\n\ndoes anyone know about some websites, papers, books etc. that describes such a method?\nthanks\n\nedit: Sql Server 2008 geographic extensions also neglect altitude information when calculating distance.\n    ", "Answer": "\r\nI implemented a WGS84 distance function using the average of the start and end altitude as the constant altitude. If you are certain that there will be relatively little altitude variation along your path this works acceptably well (error is relative to the altitude difference of your two LLA points).\n\nHere's my code (C#):\n\n```\n    /// <summary>\n    /// Gets the geodesic distance between two pathpoints in the current mode's coordinate system\n    /// </summary>\n    /// <param name=\"point1\">First point</param>\n    /// <param name=\"point2\">Second point</param>\n    /// <param name=\"mode\">Coordinate mode that both points are in</param>\n    /// <returns>Distance between the two points in the current coordinate mode</returns>\n    public static double GetGeodesicDistance(PathPoint point1, PathPoint point2, CoordMode mode) {\n        // calculate proper geodesics for LLA paths\n        if (mode == CoordMode.LLA) {\n            // meeus approximation\n            double f = (point1.Y + point2.Y) / 2 * LatLonAltTransformer.DEGTORAD;\n            double g = (point1.Y - point2.Y) / 2 * LatLonAltTransformer.DEGTORAD;\n            double l = (point1.X - point2.X) / 2 * LatLonAltTransformer.DEGTORAD;\n\n            double sinG = Math.Sin(g);\n            double sinL = Math.Sin(l);\n            double sinF = Math.Sin(f);\n\n            double s, c, w, r, d, h1, h2;\n            // not perfect but use the average altitude\n            double a = (LatLonAltTransformer.A + point1.Z + LatLonAltTransformer.A + point2.Z) / 2.0;\n\n            sinG *= sinG;\n            sinL *= sinL;\n            sinF *= sinF;\n\n            s = sinG * (1 - sinL) + (1 - sinF) * sinL;\n            c = (1 - sinG) * (1 - sinL) + sinF * sinL;\n\n            w = Math.Atan(Math.Sqrt(s / c));\n            r = Math.Sqrt(s * c) / w;\n            d = 2 * w * a;\n            h1 = (3 * r - 1) / 2 / c;\n            h2 = (3 * r + 1) / 2 / s;\n\n            return d * (1 + (1 / LatLonAltTransformer.RF) * (h1 * sinF * (1 - sinG) - h2 * (1 - sinF) * sinG));\n        }\n\n        PathPoint diff = new PathPoint(point2.X - point1.X, point2.Y - point1.Y, point2.Z - point1.Z, 0);\n        return Math.Sqrt(diff.X * diff.X + diff.Y * diff.Y + diff.Z * diff.Z);\n    }\n```\n\n\nIn practice we've found that the altitude difference rarely makes a large difference, our paths are typically 1-2km long with altitude varying on the order of 100m and we see about ~5m change on average versus using the WGS84 ellipsoid unmodified.\n\nEdit:\n\nTo add to this, if you do expect large altitude changes, you can convert your WGS84 coordinates to ECEF (earth centered earth fixed) and evaluate straight-line paths as shown at the bottom of my function. Converting a point to ECEF is simple to do:\n\n```\n    /// <summary>\n    /// Converts a point in the format (Lon, Lat, Alt) to ECEF\n    /// </summary>\n    /// <param name=\"point\">Point as (Lon, Lat, Alt)</param>\n    /// <returns>Point in ECEF</returns>\n    public static PathPoint WGS84ToECEF(PathPoint point) {\n        PathPoint outPoint = new PathPoint(0);\n\n        double lat = point.Y * DEGTORAD;\n        double lon = point.X * DEGTORAD;\n        double e2 = 1.0 / RF * (2.0 - 1.0 / RF);\n        double sinLat = Math.Sin(lat), cosLat = Math.Cos(lat);\n\n        double chi = A / Math.Sqrt(1 - e2 * sinLat * sinLat);\n        outPoint.X = (chi + point.Z) * cosLat * Math.Cos(lon);\n        outPoint.Y = (chi + point.Z) * cosLat * Math.Sin(lon);\n        outPoint.Z = (chi * (1 - e2) + point.Z) * sinLat;\n\n        return outPoint;\n    }\n```\n\n\nEdit 2:\n\nI was asked about some of the other variables in my code:\n\n```\n// RF is the eccentricity of the WGS84 ellipsoid\npublic const double RF = 298.257223563;\n\n// A is the radius of the earth in meters\npublic const double A = 6378137.0;\n```\n\n\n```\nLatLonAltTransformer```\n is a class I used to convert from LatLonAlt coordinates to ECEF coordinates and defines the constants above.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Efficient algorithm for shortest distance between two line segments in 1D\r\n                \r\nI can find plenty formulas for finding the distance between two skew lines. I want to calculate the distance between two line segments in one dimension.\n\nIt's easy to do with a bunch of IF statements. But I was wondering if their is a more efficient math formula.\n\nE.g. 1:\n\n```\n----L1x1-------L2x1-------L1x2------L2x2----------------------------\n```\n\n\nL1 = line segment 1, L2 = line segment 2;\nthe distance here is 0 because of intersection\n\nE.g. 2:\n\n```\n----L1x1-------L1x2-------L2x1------L2x2----------------------------\n```\n\n\nthe distance here is L2x1 - L1x2\n\nEDIT:\n\nThe only assumption is that the line segments are ordered, i.e. x2 is always > x1.\n\nLine segment 1 may be to the left, right, equal to etc. of line segment 2. The algorithm has to solve for this.\n\nEDIT 2:\n\nI have to implement this in T-SQL (SQL Server 2008). I just need the logic... I can write the T-SQL.\n\nEDIT 3:\n\nIf a line segment is a line segment of the other line, the distance is 0.\n\n```\n----L1x1-------L2x1-------L2x2------L1x2----------------------------\n```\n\n\nLine segment 2 is a segment of line segment 1, making the distance 0.\n\nIf they intersect or touch, the distance is 0.\n    ", "Answer": "\r\nThis question is the same as the question \"Do two ranges intersect, and if not then what is the distance between them?\"  The answer depends slightly on whether you already know which range is smallest already, and whether the points in the ranges are ordered correctly (that is, whether the lines have the same direction).\n\n```\nif (a.start < b.start) {\n  first = a;\n  second = b;\n} else {\n  first = b;\n  second = a;\n}\n```\n\n\nThen:\n\n```\ndistance = max(0, second.start - first.end);\n```\n\n\nDepending on where you're running this, your compiler should optimise it nicely.  In any case, you should probably profile to make sure that your code is a bottleneck before making it less readable for a theoretical performance improvement.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "detect the total added up scroll distance\r\n                \r\nI want to detect the total distance the user scrolled on a website. Therefore I want to add up the scroll distance downwards as well as the scroll distance upwards.\n\nSo for example: the user scrolls 150px downwards and scroll back to the top of the page the result should be 300px.\n\nWith ```\nwindow.pageYOffset```\n I can detect the distance downwards. How can I add both directions up?\n\n// edit:\n\n\n    ", "Answer": "\r\nYou need a counter – ```\ntotalOffset```\n. You need to check current scroll position –  ```\ncurrOffset```\n. You need a function that fires on scroll and calculates the distance between current and cached position and that updates the counter and the cached position.\n\n\r\n\r\n```\nlet totalOffset = 0;\r\nlet currOffset = window.pageYOffset;\r\nwindow.addEventListener(\r\n  \"scroll\",\r\n  () => {\r\n    let addedOffset = Math.abs(currOffset - window.pageYOffset);\r\n    totalOffset += addedOffset;\r\n    currOffset = window.pageYOffset;\r\n    console.log('the total scroll in px is: ', totalOffset);\r\n  },\r\n  false\r\n);```\n\r\n```\n<div style=\"min-height:2000px\">\r\n<div>```\n\r\n\r\n\r\n\n\nIt might not work ideally in SO snippet runner, but it seems to work fine in a browser.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Spelling Correction Algorithm\r\n                \r\nThis is generic algorithms stuff too so please dont stop reading if you see solr in text (please skip first 3 lines)\n\nIn Solr, For spell checking component I set extendedResults to get the frequencies of the corrected word and then select the word with the best frequency. I understand the spell check algorithm based on Edit Distance. For an example:\n\nQuery to Solr: Marien\n\nSpell Check Text Returned: Marine (Freq: 120), Market (Freq: 900) and others. My dictionary here is based on indexed words.\n\nSo I chose Market (more frequency) however which is wrong as my intent was marine. Both have Edit Distance of 2.\n\nNow how can I improve this Algorithm to select marine instead of market (based on something more than edit distance and frequency stuff)?\n\nDo I have to incorporate some \"soundex\" algorithms too?\n\nI am looking for simple stuff which I can quickly implement.\n\nI even tried using Peter Norvig's spell corrector Algorithm (which is great) but again I ran in same problems.\n    ", "Answer": "\r\nIn this particular case, you could improve the results by using a metric which recognises transpositions - 'marien' differs from 'marine' by two substitutions, but only one transposition, so if you do that, it seems closer than 'market'.\n\nThe classic Levenshtein edit distance measure only deals with insertions, deletions, and substitutions. However, the Damerau–Levenshtein distance deals with transposition as well.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Increase line height based on the x distance\r\n                \r\nI have 4 lines and want to increase the height by 5 px but based on the x distance. It will be simple if the x distance have the same value but in my case it can be different.\nFor example if the x distance are so:\n\n```\nline.x: 50, 80, 110, 130\nline.y: 20, 20, 20, 20\n```\n\n\nI could do\n\n```\nline.x: 50, 80, 110, 130\nline.height: 20, 25, 30, 35\n```\n\n\nBut how I can do the math when the x distance is so:\n\n```\nline.x: 50, 80, 95, 130\n```\n\n\nEDIT: Here is an example image:\n\n    ", "Answer": "\r\nI think that you want an equation of a line that rises 15 units between ```\nline.x[0]```\n and ```\nline.x[3]```\n. If so, the following should work:\n\n```\nlet line_x = [50,80,95,130];\nlet m = 15/(line_x[3]-line_x[0]);\nline_y = line_x.map(x => m*(x-line_x[0]) + 20);\nconsole.log(line_y);\n//[ 20, 25.625, 28.4375, 35 ]\n```\n\n\nIn simplified notation, the key step is calculating ```\ny```\n as ```\ny = m(x-x[0]) + 20```\n where ```\nm = 15/(x[3]-x[0])```\n (which is a simple ```\nslope = rise/run```\n calculation).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Add my custom loss function to torch\r\n                \r\nI want to add a loss function to torch that calculates the edit distance between predicted and target values.\nIs there an easy way to implement this idea?\nOr do I have to write my own class with backward and forward functions?\n    ", "Answer": "\r\nIf your criterion can be represented as a composition of existing modules and criteria, it's a good idea to simply construct such composition using containers. The only problem is that standard containers are designed to work with modules only, not criteria. The difference is in ```\n:forward```\n method signature:\n\n```\nmodule:forward(input)\ncriterion:forward(input, target)\n```\n\n\nLuckily, we are free to define our own container which is able work with criteria too. For example, sequential:\n\n```\nlocal GeneralizedSequential, _ = torch.class('nn.GeneralizedSequential', 'nn.Sequential')\n\nfunction GeneralizedSequential:forward(input, target)\n    return self:updateOutput(input, target)\nend\n\nfunction GeneralizedSequential:updateOutput(input, target)\n    local currentOutput = input\n    for i=1,#self.modules do\n        currentOutput = self.modules[i]:updateOutput(currentOutput, target)\n    end\n    self.output = currentOutput\n    return currentOutput\nend\n```\n\n\nBelow is an illustration of how to implement ```\nnn.CrossEntropyCriterion```\n having this generalized sequential container:\n\n```\nfunction MyCrossEntropyCriterion(weights)\n    criterion = nn.GeneralizedSequential()\n    criterion:add(nn.LogSoftMax())\n    criterion:add(nn.ClassNLLCriterion(weights))\n    return criterion\nend\n```\n\n\nCheck whether everything is correct:\n\n```\noutput = torch.rand(3,3)\ntarget = torch.Tensor({1, 2, 3})\n\nmycrit = MyCrossEntropyCriterion()\n-- print(mycrit)\nprint(mycrit:forward(output, target))\nprint(mycrit:backward(output, target))\n\ncrit = nn.CrossEntropyCriterion()\n-- print(crit)\nprint(crit:forward(output, target))\nprint(crit:backward(output, target))\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Approximate Estimation of Distance Matrices\r\n                \r\nI have a set of N objects, and I'd like to compute a NxN distance matrix. Sometimes my set of N objects is very large, and I'd like to compute an approximation to the NxN distance matrix by only computing a subset of the distance comparisons. \n\nCan anyone point me in the direction of something that calculates approximations to a full distance matrix? I have some ideas in mind, but I'd like to avoid re-inventing the wheel. \n\nEdit: An example of the type of algorithm would take advantage of the fact that if there is a very small distance between object A and object B, and there is a very small distance between object B and object C, there has to be a somewhat short distance between objects A and C. \n    ", "Answer": "\r\nI had this same question and ended up writing Python code for it:\n\nhttps://github.com/jpeterbaker/lazyDistance\n\nREADME.md explains how the triangle inequality can be used to update upper and lower bounds for each distance.\n\nJust run the Python file as a script for an example in 2-dimensional space. The plotted lines are the only distances that were actually calculated.\n\nIn my version, the time savings aren't about having a large number of objects. As I've written it, it's a O(n^4) algorithm, so it's actually worse than just calculating all distances if the number of objects is large. But my method will save time when you have a modest number of objects and the distance function is very expensive to calculate. It assumes that it is faster to do several O(n^2) operations rather than a single distance measurement.\n\nIf n is large, you could look for cheaper methods to decide which distance to calculate next (that don't involve arithmetic with n^2 entries of distance bounds matrices). You also may not need to update all 2*n^2 bounds every time that this code does.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "calculate positional proximity of two multiword exact phrases inside a large text in Python\r\n                \r\nHow to calculate minimum positional distance between two multi-word, exact phrases inside a large text (e.g. an article), by using Python?\nAssumption is that there might be multiple occurrences of both phrases.\nTo avoid miss-understanding, this is not a question about fuzzy string matching, edit distance, word lists etc.This is question about calculating positional proximity/distance between two exact phrases inside a text.\nEDIT (modified solution by https://stackoverflow.com/users/2359945/razzle-shazl):\n```\ndef str_to_raw(s):\n    raw_map = {8:r'\\b', 7:r'\\a', 12:r'\\f', 10:r'\\n', 13:r'\\r', 9:r'\\t', 11:r'\\v'}\n    return r''.join(i if ord(i) > 32 else raw_map.get(ord(i), i) for i in s)\n\ndef find_smallest_distance1(sentence, word1, word2):\n    distances = []\n    dist = float('inf')\n    p1 = str_to_raw(word1)\n    p2 = str_to_raw(word2)\n    s = sentence\n    \"\"\"\n    f1 = re.finditer(r'\\bred fox\\b', s, re.I)\n    f2 = re.finditer(r'\\bblue hen\\b', s, re.I)\n    \"\"\"\n    f1 = re.finditer(p1, s, re.I)\n    f2 = re.finditer(p2, s, re.I)    \n    _f1 = _f2 = None\n    while True:\n        try:\n            _f1 = next(f1)\n        except StopIteration:\n            break\n    \n        if _f2 == None:\n            try:\n                _f2 = next(f2)\n            except StopIteration:\n                break\n            \n        if _f1.span()[0] > _f2.span()[0]:\n            # we want f1 to always be closer to start / lower start index\n            f1, f2 = f2, f1\n            _f1, _f2 = _f2, _f1\n        \n        dist = min(dist, _f2.span()[0] - _f1.span()[1])\n    return dist\n```\n\nI was wondering, how can it be modified, so that distance of phrase2 (word2) is calculated only to left or only to right direction from position of phrase1 (word1)?\n    ", "Answer": "\r\nLet's find the indices for both substrings.  Then we can walk both lists of indices and calculate minimum distance in one pass.\nI would use regular expressions as they are flexible (think about future maintainers) and powerful.\nWe create two iterators that return matches for both substrings.  Then we pop the iterator that has the lower value (in this case, lowest start index).\nWhen this \"shorter\" iterator is finally exhausted, we can skip checking the remainder of the other iterator as those indices will have a worse distance than already obtained.\nShortest Distance\n```\nimport re\n\ndef positionalProximity(re1: str, re2: str, s: str, bidir: bool = True, regexFlags: int = 0) -> int:\n    # returns shortest positional distance between re1 and re2\n    # when not bidirectional, then search for re1 only to the left of re2\n    dist = float('inf')\n    f1 = re.finditer(re1, s, regexFlags)\n    f2 = re.finditer(re2, s, regexFlags)\n    _f1 = _f2 = None\n    while True:\n        try:\n            _f1 = next(f1)\n        except StopIteration:\n            break\n\n        if _f2 == None:\n            try:\n                _f2 = next(f2)\n            except StopIteration:\n                break\n\n        if bidir and _f1.span()[0] > _f2.span()[0]:\n            # we want f1 to always be closer to start / lower start index\n            f1, f2 = f2, f1\n            _f1, _f2 = _f2, _f1\n\n        if bidir or _f2.span()[0] > _f1.span()[1]:\n            dist = min(dist, _f2.span()[0] - _f1.span()[1])\n    return dist\n\n\ns = 'The red fox took stock of the blue hen,\\\n    and the blue hen took stock of the red fox.\\\n    \"Blue hen!\" cried red fox.  \"Blue hen!\"'\nre1 = r'\\bred fox\\b'\nre2 = r'\\bblue hen\\b'\n\nprint(f'dist = {positionalProximity(re1, re2, s, False)}')\nprint(f'dist = {positionalProximity(re1, re2, s, regexFlags = re.I)}')\n```\n\nOutput:\n```\ndist = 19\ndist = 4\n```\n\nIf you are curious about ```\nspan()```\n, it returns the start-inclusive and end-exclusive indices of your match:\n```\nprint([f.span() for f in f1])\nprint([f.span() for f in f2])\n```\n\nOutput:\n```\n(4, 11)\n(75, 82)\n(105, 112)\n(30, 38)\n(48, 56)\n(88, 96)\n(116, 124)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "distance between two signed numbers\r\n                \r\nThe distance between two numbers is often calculated like that:\n\n```\nlong distance(long x, long y)\n{\n     return x > y ? x - y : y - x;\n}\n```\n\n\nHowever with signed ```\nx```\n and ```\ny```\n these subtractions there may overflow and so that function can invoke undefined behavior both in C and C++.\n\nOne way out of that issue is to use unsigned type to represent resulting distance. Distance can not be negative so signed type is not needed. Distance between minimum and maximum of signed type should fit into unsigned type of same size. (Edit: As chux answered it was not entirely correct assumption.) So I did modify the first function like that:\n\n```\nunsigned long distance(long x, long y)\n{\n    return (x > y) ? (unsigned long)x - (unsigned long)y\n                   : (unsigned long)y - (unsigned long)x;\n}\n```\n\n\nDoes it now correctly calculate the distance between two signed longs in standard conforming and portable manner? If it does not, what would be the fix?\n    ", "Answer": "\r\n\n  Does it now correctly calculate the distance between two signed longs in standard conforming and portable manner?\n\n\nYes.  \n\nRare exception1 would oblige using a wider type.\n\n\n\nConsider the 3 cases when ```\nx > y```\n\n\nx >= 0, y >= 0\n\nFollowing is trivially correct as the cast does not change value.\n\n```\n(unsigned long)x - (unsigned long)y\n```\n\n\nx < 0, y < 0\n\nBoth x,y values are increased by ```\nULONG_MAX + 1```\n due to the ```\n(unsigned long)```\n and the subtraction cancels that out.\n\n```\n// is akin to \n((unsigned long)(x + ULONG_MAX + 1) - (unsigned long)(y + ULONG_MAX + 1))\n// or\nx - y // with unsigned math.\n```\n\n\nx >= 0, y < 0\n\n```\n(unsigned long)y```\n has the value of ```\ny + ULONG_MAX + 1```\n, which is more than ```\nx```\n.  (Assuming ```\nULONG_MAX/2 >= LONG_MAX```\n1) The difference is negative.  Yet unsigned math wraps around, and adds back ```\nULONG_MAX + 1```\n.\n\n```\n// is akin to \n((unsigned long)x - (unsigned long)(y + ULONG_MAX + 1)) + (ULONG_MAX + 1).\n// or\nx - y // with unsigned math.\n```\n\n\nx < 0, y >= 0\n\nThis case not possible as ```\nx > y```\n.\n\n\n\n1: C does not specify ```\nULONG_MAX/2 == LONG_MAX```\n even though that is exceedingly common.  I've only come across this once long ago where it did not apply. It that case it was ```\nULONG_MAX == LONG_MAX```\n. ```\nULONG_MAX/2 == LONG_MAX```\n is so expected that I doubt a modern platform would risk not doing so.  C does specify ```\nULONG_MAX >= LONG_MAX```\n.\n\n\n  The range of nonnegative values of a signed integer type is a subrange of the\n  corresponding unsigned integer type, and the representation of the same value in each type is the same. ... C11dr §6.2.5 9\n\n\nCode could use the below to detect these rare platforms.\n\n```\n#if ULONG_MAX/2 < LONG_MAX\n  #error `unsigned long` too narrow.  Need new approach.\n#endif\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Compute total distance from an ordering of a path\r\n                \r\nI have a distance matrix and a data.frame of orderings that I want to compute the total distances for each ordering (row). \n\ndistance matrix (generated by ```\nas.matrix(dist(x, upper=TRUE, diag=TRUE))```\n):\n\n```\n               FOV5.1.T4.C1 FOV5.1.T4.C1.1 FOV5.1.T4.C2 FOV5.1.T4.C2.1\nFOV5.1.T4.C1      0.0000000     11.5454430    0.3431676     13.2814257\nFOV5.1.T4.C1.1   11.5454430      0.0000000   11.5625031      2.8374444\nFOV5.1.T4.C2      0.3431676     11.5625031    0.0000000     13.2407547\nFOV5.1.T4.C2.1   13.2814257      2.8374444   13.2407547      0.0000000\n```\n\n\nordering (generated by ```\nexpand.grid()```\n):\n\n```\n            Var1           Var2\n1   FOV5.1.T4.C2   FOV5.1.T4.C1\n2 FOV5.1.T4.C2.1   FOV5.1.T4.C1\n3   FOV5.1.T4.C2 FOV5.1.T4.C1.1\n4 FOV5.1.T4.C2.1 FOV5.1.T4.C1.1\n```\n\n\nExpected Output:\n\n```\n            Var1           Var2          Dist\n1   FOV5.1.T4.C2   FOV5.1.T4.C1      0.3431676\n2 FOV5.1.T4.C2.1   FOV5.1.T4.C1     13.2814257\n3   FOV5.1.T4.C2 FOV5.1.T4.C1.1       ...\n4 FOV5.1.T4.C2.1 FOV5.1.T4.C1.1       ...\n```\n\n\nI would like a total distance column appended to the end of the ordering data frame that would give the total distance from ```\nVar1```\n to ```\nVarN```\n. \n\nEDIT: The ultimate goal is to generalize this problem for many orderings (rows) and elements (columns) of the ordering data frame. For example: \n\ndistance matrix:\n\n```\n                FOV10.5.T1.C1 FOV10.5.T1.C1.1 FOV10.5.T6.C1 FOV10.5.T6.C1.1 FOV10.5.T7.C2 FOV10.5.T7.C2.1 FOV10.5.T7.C4 FOV10.5.T7.C4.1\nFOV10.5.T1.C1        0.000000        9.259314      9.525777        4.920990      8.520076        3.246356     10.429007       12.771907\nFOV10.5.T1.C1.1      9.259314        0.000000      2.903446        6.485444      2.604540        6.943048      2.962850       12.658076\nFOV10.5.T6.C1        9.525777        2.903446      0.000000        8.185294      1.095356        8.058659      5.763981        9.949294\nFOV10.5.T6.C1.1      4.920990        6.485444      8.185294        0.000000      7.233955        1.724583      6.384782       15.156368\nFOV10.5.T7.C2        8.520076        2.604540      1.095356        7.233955      0.000000        7.054426      5.528189       10.060419\nFOV10.5.T7.C2.1      3.246356        6.943048      8.058659        1.724583      7.054426        0.000000      7.488958       13.938926\nFOV10.5.T7.C4       10.429007        2.962850      5.763981        6.384782      5.528189        7.488958      0.000000       15.570799\nFOV10.5.T7.C4.1     12.771907       12.658076      9.949294       15.156368     10.060419       13.938926     15.570799        0.000000\n```\n\n\nordering: \n\n```\n              Var1            Var2            Var3            Var4   Dist\n1    FOV10.5.T1.C1   FOV10.5.T7.C4   FOV10.5.T7.C2   FOV10.5.T6.C1   sum(Var1 --> Var2, Var2 --> Var3, Var3 --> Var4)\n2  FOV10.5.T1.C1.1   FOV10.5.T7.C4   FOV10.5.T7.C2   FOV10.5.T6.C1   ...\n3    FOV10.5.T1.C1 FOV10.5.T7.C4.1   FOV10.5.T7.C2   FOV10.5.T6.C1   ...\n4  FOV10.5.T1.C1.1 FOV10.5.T7.C4.1   FOV10.5.T7.C2   FOV10.5.T6.C1   ...\n5    FOV10.5.T1.C1   FOV10.5.T7.C4 FOV10.5.T7.C2.1   FOV10.5.T6.C1\n6  FOV10.5.T1.C1.1   FOV10.5.T7.C4 FOV10.5.T7.C2.1   FOV10.5.T6.C1\n7    FOV10.5.T1.C1 FOV10.5.T7.C4.1 FOV10.5.T7.C2.1   FOV10.5.T6.C1\n8  FOV10.5.T1.C1.1 FOV10.5.T7.C4.1 FOV10.5.T7.C2.1   FOV10.5.T6.C1\n9    FOV10.5.T1.C1   FOV10.5.T7.C4   FOV10.5.T7.C2 FOV10.5.T6.C1.1\n10 FOV10.5.T1.C1.1   FOV10.5.T7.C4   FOV10.5.T7.C2 FOV10.5.T6.C1.1\n11   FOV10.5.T1.C1 FOV10.5.T7.C4.1   FOV10.5.T7.C2 FOV10.5.T6.C1.1\n12 FOV10.5.T1.C1.1 FOV10.5.T7.C4.1   FOV10.5.T7.C2 FOV10.5.T6.C1.1\n13   FOV10.5.T1.C1   FOV10.5.T7.C4 FOV10.5.T7.C2.1 FOV10.5.T6.C1.1\n14 FOV10.5.T1.C1.1   FOV10.5.T7.C4 FOV10.5.T7.C2.1 FOV10.5.T6.C1.1\n15   FOV10.5.T1.C1 FOV10.5.T7.C4.1 FOV10.5.T7.C2.1 FOV10.5.T6.C1.1\n16 FOV10.5.T1.C1.1 FOV10.5.T7.C4.1 FOV10.5.T7.C2.1 FOV10.5.T6.C1.1\n```\n\n    ", "Answer": "\r\n```\nrequire(dplyr)\nrequire(reshape2)\n\npoints <- replicate(5, sample(1:100, 2, T)) %>% \n            `colnames<-`(letters[1:5]) \n\ndists <-  points %>% \n            t %>% \n            dist %>% \n            as.matrix %>% \n            melt(value.name = 'dist') %>% \n            mutate_if(is.factor, as.character)\n\npaths <- replicate(5, sample(colnames(points), 4, T)) %>% \n            as.data.frame %>% \n            mutate(tot.dist = NA)\n\nfor(i in 1:nrow(paths)){\n    d <- numeric(ncol(paths) - 2)\n    for(j in 2:(ncol(paths) - 1)){\n        d[j - 1] <- dists %>% \n                        filter(Var1 == paths[i, j - 1] & Var2 == paths[i, j]) %>% \n                        select(dist) %>% \n                        unlist\n    }\n    paths$tot.dist[i] <- sum(d)\n}\n\npaths\n\n#   V1 V2 V3 V4 V5  tot.dist\n# 1  c  c  e  e  c  99.29753\n# 2  a  b  e  d  a 173.82135\n# 3  d  e  e  d  a  87.30152\n# 4  a  b  a  b  e 251.46679\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Stringdist distance unexpectedly large\r\n                \r\nThe following data has the surprising result that it does not match. I was expecting the distance to be ```\n5```\n, but even at ```\n7```\n I get no match\n```\nlibrary(fuzzyjoin)\none <- as.data.frame(\"Other field crops (non-organic)\")\nnames(one) <- \"A\"\ntwo <-  as.data.frame(\"other_field_crops_non_organic\")\nnames(two) <- \"A\"\n\nstringdist_left_join(one, two, by = \"A\", method = \"lcs\", max_dist = 7, ignore_case=TRUE)\n\n                              A.x  A.y\n1 Other field crops (non-organic) <NA>\n```\n\nOnly at ```\n10```\n I get a match..\n```\nstringdist_left_join(one, two, by = \"A\", method = \"lcs\", max_dist = 10, ignore_case=TRUE)\n                              A.x                           A.y\n1 Other field crops (non-organic) other_field_crops_non_organic\n```\n\nCould someone explain to me why this distance larger than ```\n9```\n? Does it have to do with the brackets? And if so how can I circumvent this issue without removing the brackets?\nEDIT\n```\nlibrary(fuzzyjoin)\none <- as.data.frame(\"Other field crops non-organic\")\nnames(one) <- \"A\"\ntwo <-  as.data.frame(\"other_field_crops_non_organic\")\nnames(two) <- \"A\"\n\nstringdist_left_join(one, two, by = \"A\", method = \"lcs\", max_dist = 5, ignore_case=TRUE)\n                            A.x  A.y\n1 Other field crops non-organic <NA>\n```\n\nEven without the brackets I cannot get the distance within ```\n5```\n.\n    ", "Answer": "\r\nThe problem comes down to the method you are using to calculate the string distance. You are using the ```\nlcs```\n (longest common substring) method, which in effect only allows deletions and insertions rather than substitutions. From the docs:\n\nThe longest common substring (method='lcs') is defined as the longest string that can be obtained by pairing characters from a and b while keeping the order of characters intact. The lcs-distance is defined as the number of unpaired characters. The distance is equivalent to the edit distance allowing only deletions and insertions, each with weight one.\n\nSo when we convert spaces to underscores, we incur a weighting of 2 per substitution:\n```\nstringdist('abc def', 'abc_def', method = 'lcs')\n#> [1] 2\n```\n\nThis is in contrast to the default 'osa' method, which like the Levenshtein distance and the R function ```\nadist```\n allows direct substitutions, with only a 1-point weighting:\n```\nstringdist('abc def', 'abc_def', method = 'osa')\n#> [1] 1\n```\n\nYou can compare how the different ```\nstringdist```\n methods compare on your two strings. To further simplify, let's make both lowercase since you are already specifying ```\nignore_case```\n in your left join:\n```\nlibrary(stringdist)\n\na <- \"other field crops (non-organic)\"\nb <- \"other_field_crops_non_organic\"\nmethods <- c(\"osa\", \"lv\", \"dl\", \"hamming\", \"lcs\", \n             \"qgram\", \"cosine\", \"jaccard\", \"jw\", \"soundex\")\n\nsapply(methods, function(x) stringdist(a, b, method = x))\n#>        osa         lv         dl    hamming        lcs      qgram     cosine \n#>  6.0000000  6.0000000  6.0000000        Inf 10.0000000 10.0000000  0.2025635 \n#>    jaccard         jw    soundex \n#>  0.2500000  0.1104931  0.0000000\n```\n\nYou can see that the Hamming distance is infinite, since your strings are of different length, and ```\nosa```\n (the default method) is only 6, but ```\nlcs```\n requires 10 (4 removals of underscores, 3 additions of spaces, one addition of a hyphen, and two additions of parentheses). If this string pair is representative of your data, you might want to switch to \"osa\"\nCreated on 2022-04-14 by the reprex package (v2.0.1)\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Why is greedy decoding outperforming beam decoding for a CTC network?\r\n                \r\nI trained a TensorFlow model using the CTC loss. \n\nI decoded the network output using ```\ntf.nn.ctc_greedy_decoder```\n, and got an average edit distance of 0.437 over a batch of 1000 sequences.\n\nI decoded the network output using ```\ntf.nn.ctc_beam_search_decoder```\n, and for the following beam widths, got the following average edit distances:\n\nwidth 1: 0.48953804\n\nwidth 4: 0.4880197\n\nwidth 100: 0.48801425\n\nMy question is: since beam width = 1 is equivalent to greedy decoding, shouldn't the accuracy be the same? And why is beam search decoding performing worse than greedy decoding?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Distance between two line segments\r\n                \r\nEDIT: PROBLEM SOLVED. This code works properly !\nI translated this python code Shortest distance between two line segments (answered by Fnord) to Objective-C in order to find the shortest distance between two line segments. However the distances are incorrect. Could you please help me find what is wrong?\nMatrix.h:\n```\n#import <Cocoa/Cocoa.h>\n\n@interface Matrix : NSObject\n\n@property NSUInteger m;\n@property NSUInteger n;\n@property NSMutableArray *rows;\n\n- (id)initWithSizeM:(NSUInteger)m N:(NSUInteger)n;\n- (CGFloat)determinant3x3;\n\n@end\n\n```\n\nMatrix.m:\n```\n#import <Cocoa/Cocoa.h>\n#import \"Matrix.h\"\n\n@implementation Matrix;\n\n- (id)initWithSizeM:(NSUInteger)m N:(NSUInteger)n {\n    self = [super init];\n    _m = m;\n    _n = n;\n    _rows = [NSMutableArray new];\n    for (int i = 0; i < m; i ++) {\n        [_rows addObject:[NSMutableArray new]];\n        for (int j = 0; j < n; j ++) {\n            CGFloat value = 0.0;\n            [_rows[i] addObject:[NSNumber numberWithFloat:value]];\n        }\n    }\n    return self;\n}\n\n- (CGFloat)determinant3x3 {\n    CGFloat a = [_rows[0][0] floatValue];\n    CGFloat b = [_rows[0][1] floatValue];\n    CGFloat c = [_rows[0][2] floatValue];\n    CGFloat d = [_rows[1][0] floatValue];\n    CGFloat e = [_rows[1][1] floatValue];\n    CGFloat f = [_rows[1][2] floatValue];\n    CGFloat g = [_rows[2][0] floatValue];\n    CGFloat h = [_rows[2][1] floatValue];\n    CGFloat i = [_rows[2][2] floatValue];\n    return a*e*i + b*f*g + c*d*h - c*e*g - b*d*i - a*f*h;\n}\n\n\n@end\n```\n\nPointCartesian.h:\n```\n#import <Cocoa/Cocoa.h>\n#import \"Vector.h\"\n\n@class Vertex;\n@interface PointCartesian: NSObject\n\n@property CGFloat x;\n@property CGFloat y;\n@property CGFloat z;\n\n@property NSString *name;\n\n- (id)initWithCoordinatesX:(CGFloat)x Y:(CGFloat)y Z:(CGFloat)z;\n- (Vector*)toVector;\n+ (CGFloat)shortestDistanceBetweenTwoSegmentsA0:(PointCartesian*)a0 A1:(PointCartesian*)a1 B0:(PointCartesian*)b0 B1:(PointCartesian*)b1;\n\n@end\n\n```\n\nPointCartesian.m:\n```\n#import <Cocoa/Cocoa.h>\n#import \"PointCartesian.h\"\n#import \"Vector.h\"\n#import \"Matrix.h\"\n\n@implementation PointCartesian;\n\n- (id)initWithCoordinatesX:(CGFloat)x Y:(CGFloat)y Z:(CGFloat)z {\n    _x = x;\n    _y = y;\n    _z = z;\n    return self;\n}\n\n- (Vector*)toVector {\n    return [[Vector alloc] initWithX:_x Y:_y Z:_z];\n}\n\n+ (CGFloat)shortestDistanceBetweenTwoSegmentsA0:(PointCartesian*)a0 A1:(PointCartesian*)a1 B0:(PointCartesian*)b0 B1:(PointCartesian*)b1 {\n    BOOL clampA0 = YES;\n    BOOL clampA1 = YES;\n    BOOL clampB0 = YES;\n    BOOL clampB1 = YES;\n    Vector *A = [Vector vectorFromA:a0 B:a1];\n    Vector *B = [Vector vectorFromA:b0 B:b1];\n    CGFloat magA = [A length];\n    CGFloat magB = [B length];\n    Vector *_A = [[Vector alloc] initWithX:A.x/magA Y:A.y/magA Z:A.z/magA];\n    Vector *_B = [[Vector alloc] initWithX:B.x/magB Y:B.y/magB Z:B.z/magB];\n    Vector *cross = [Vector crossProductA:_A B:_B];\n    CGFloat denom = pow([cross length], 2);\n    \n    if (denom == 0) {\n        CGFloat d0 = [Vector dotProductA:_A B:[Vector vectorFromA:a0 B:b0]];\n        if (clampA0 || clampA1 || clampB0 || clampB1) {\n            CGFloat d1 = [Vector dotProductA:_A B:[Vector vectorFromA:a0 B:b1]];\n            if (d0 <= 0 && 0 >= d1) {\n                if (clampA0 && clampB1) {\n                    if (floor(d0) < floor(d1)) {\n                        return [[Vector vectorFromA:b0 B:a0] length];\n                    }\n                   return [[Vector vectorFromA:b1 B:a0] length];\n                }\n            }\n            if (d0 >= magA && magA <= d1) {\n                if (clampA1 && clampB0) {\n                    if (floor(d0) < floor(d1)) {\n                        return [[Vector vectorFromA:b0 B:a1] length];\n                    }\n                    return [[Vector vectorFromA:b1 B:a1] length];\n                }\n            }\n        }\n        return [[[Vector alloc] initWithX:d0*_A.x + a0.x - b0.x Y:d0*_A.y + a0.y - b0.y Z:d0*_A.z + a0.z - b0.z] length];\n    }\n    \n    Vector *t = [Vector vectorFromA:a0 B:b0];\n    \n    Matrix *MA = [[Matrix alloc] initWithSizeM:3 N:3];\n    MA.rows[0][0] = [NSNumber numberWithFloat:t.x];\n    MA.rows[0][1] = [NSNumber numberWithFloat:t.y];\n    MA.rows[0][2] = [NSNumber numberWithFloat:t.z];\n    MA.rows[1][0] = [NSNumber numberWithFloat:_B.x];\n    MA.rows[1][1] = [NSNumber numberWithFloat:_B.y];\n    MA.rows[1][2] = [NSNumber numberWithFloat:_B.z];\n    MA.rows[2][0] = [NSNumber numberWithFloat:cross.x];\n    MA.rows[2][1] = [NSNumber numberWithFloat:cross.y];\n    MA.rows[2][2] = [NSNumber numberWithFloat:cross.z];\n    \n    Matrix *MB = [[Matrix alloc] initWithSizeM:3 N:3];\n    MB.rows[0][0] = [NSNumber numberWithFloat:t.x];\n    MB.rows[0][1] = [NSNumber numberWithFloat:t.y];\n    MB.rows[0][2] = [NSNumber numberWithFloat:t.z];\n    MB.rows[1][0] = [NSNumber numberWithFloat:_A.x];\n    MB.rows[1][1] = [NSNumber numberWithFloat:_A.y];\n    MB.rows[1][2] = [NSNumber numberWithFloat:_A.z];\n    MB.rows[2][0] = [NSNumber numberWithFloat:cross.x];\n    MB.rows[2][1] = [NSNumber numberWithFloat:cross.y];\n    MB.rows[2][2] = [NSNumber numberWithFloat:cross.z];\n    \n    CGFloat detA = [MA determinant3x3];\n    CGFloat detB = [MB determinant3x3];\n    \n    CGFloat t0 = detA / denom;\n    CGFloat t1 = detB / denom;\n    \n    PointCartesian *pA = [[PointCartesian alloc] initWithCoordinatesX:_A.x*t0 + a0.x Y:_A.y*t0 + a0.y Z:_A.z*t0 + a0.z];\n    PointCartesian *pB = [[PointCartesian alloc] initWithCoordinatesX:_B.x*t1 + b0.x Y:_B.y*t1 + b0.y Z:_B.z*t1 + b0.z];\n\n    if (clampA0 || clampA1 || clampB0 || clampB1) {\n        if (clampA0 && t0 < 0) {\n            pA = a0;\n        } else if (clampA1 && t0 > magA) {\n            pA = a1;\n        }\n        if (clampB0 && t1 < 0) {\n            pB = b0;\n        } else if (clampB1 && t1 > magB) {\n            pB = b1;\n        }\n        if ((clampA0 && t0 < 0) || (clampA1 && t0 > magA)) {\n            CGFloat dot = [Vector dotProductA:_B B:[Vector vectorFromA:b0 B:pA]];\n            if (clampB0 && dot < 0) {\n                dot = 0;\n            } else if (clampB1 && dot > magB) {\n                dot = magB;\n            }\n            pB = [[PointCartesian alloc] initWithCoordinatesX:b0.x + _B.x*dot Y:b0.y + _B.y*dot Z:b0.z + _B.z*dot];\n        }\n        if ((clampB0 && t1 < 0) || (clampB1 && t1 > magB)) {\n            CGFloat dot = [Vector dotProductA:_A B:[Vector vectorFromA:a0 B:pB]];\n            if (clampA0 && dot < 0) {\n                dot = 0;\n            } else if (clampA1 && dot > magA) {\n                dot = magA;\n            }\n            pA = [[PointCartesian alloc] initWithCoordinatesX:a0.x + _A.x*dot Y:a0.y + _A.y*dot Z:a0.z + _A.z*dot];\n        }\n    }\n    \n    return [[[Vector alloc] initWithX:pA.x-pB.x Y:pA.y-pB.y Z:pA.z - pB.z] length];\n}\n\n\n@end\n```\n\nVector.h:\n```\n#import <Cocoa/Cocoa.h>\n\n@class PointCartesian;\n@interface Vector: NSObject\n\n@property CGFloat x;\n@property CGFloat y;\n@property CGFloat z;\n\n@property NSString *name;\n\n- (id)initWithX:(CGFloat)x Y:(CGFloat)y Z:(CGFloat)z;\n- (CGFloat)length;\n- (void)normalize;\n+ (Vector*)vectorFromA:(PointCartesian*)a B:(PointCartesian*)b;\n- (PointCartesian*)toPoint;\n+ (CGFloat)dotProductA:(Vector*)a B:(Vector*)b;\n+ (Vector*)crossProductA:(Vector*)a B:(Vector*)b;\n\n@end\n\n```\n\nVector.m:\n```\n#import <Cocoa/Cocoa.h>\n#import \"Vector.h\"\n#import \"PointCartesian.h\"\n\n@implementation Vector\n\n- (id)initWithX:(CGFloat)x Y:(CGFloat)y Z:(CGFloat)z {\n    _x = x;\n    _y = y;\n    _z = z;\n    return self;\n}\n- (CGFloat)length {\n    return sqrt(pow(_x,2)+pow(_y,2)+pow(_z,2));\n}\n\n- (void)normalize {\n    _x = _x / [self length];\n    _y = _y / [self length];\n    _z = _z / [self length];\n}\n\n+ (Vector*)vectorFromA:(PointCartesian*)a B:(PointCartesian*)b {\n    Vector *AB = [[Vector alloc] initWithX:b.x-a.x Y:b.y-a.y Z:b.z-a.z];\n    return AB;\n}\n\n- (PointCartesian*)toPoint {\n    return [[PointCartesian alloc] initWithCoordinatesX:_x Y:_y Z:_z];\n}\n\n+ (CGFloat)dotProductA:(Vector*)a B:(Vector*)b {\n    return a.x*b.x + a.y*b.y + a.z*b.z;\n}\n\n+ (Vector*)crossProductA:(Vector*)a B:(Vector*)b {\n    return [[Vector alloc] initWithX:a.y*b.z - a.z*b.y Y:a.z*b.x - a.x*b.z Z:a.x*b.y - a.y*b.x];\n}\n\n\n\n\n@end\n\n```\n\nmain.m:\n```\n\n#import <Cocoa/Cocoa.h>\n#import \"PointCartesian.h\"\n\n\nint main(int argc, const char * argv[]) {\n    PointCartesian *a0 = [[PointCartesian alloc] initWithCoordinatesX:27.83 Y:31.74 Z:-26.60];\n    PointCartesian *a1 = [[PointCartesian alloc] initWithCoordinatesX:13.43 Y:21.77 Z:46.81];\n    PointCartesian *b0 = [[PointCartesian alloc] initWithCoordinatesX:77.54 Y:7.53 Z:6.22];\n    PointCartesian *b1 = [[PointCartesian alloc] initWithCoordinatesX:26.99 Y:12.39 Z:11.18];\n    CGFloat d = [PointCartesian shortestDistanceBetweenTwoSegmentsA0:a0 A1:a1 B0:b0 B1:b1];\n    NSLog(@\"%f\", d);\n    \n    return NSApplicationMain(argc, argv);\n}\n\n```\n\noutput:\n```\n2020-06-26 17:57:30.762912+0200 Distance[2235:111429] 15.651394\n```\n\nThe distance should be 15.826771412132246 according to the python code in link above.\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Time series distance metric\r\n                \r\nIn order to clusterize a set of time series I'm looking for a smart distance metric. \nI've tried some well known metric but no one fits to my case.\n\nex: Let's assume that my cluster algorithm extracts this three centroids [s1, s2, s3]:\n\n\nI want to put this new example [sx] in the most similar cluster:\n\n\n\nThe most similar centroids is the second one, so I need to find a distance function d that gives me ```\nd(sx, s2) < d(sx, s1)```\n and ```\nd(sx, s2) < d(sx, s3)```\n \n\nedit\n\nHere the results with metrics [cosine, euclidean,  minkowski, dynamic type warping]\n]3\n\nedit 2\n\nUser Pietro P suggested to apply the distances on the cumulated version of the time series\nThe solution works, here the plots and the metrics:\n\n    ", "Answer": "\r\nnice question! using any standard distance of R^n (euclidean, manhattan or generically minkowski) over those time series cannot achieve the result you want, since those metrics are independent of the permutations of the coordinate of R^n (while time is strictly ordered and it is the phenomenon you want to capture).\n\nA simple trick, that can do what you ask is using the cumulated version of the time series (sum values over time as time increases) and then apply a standard metric. Using the Manhattan metric, you would get as a distance between two time series the area between their cumulated versions.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Time series distance metric\r\n                \r\nIn order to clusterize a set of time series I'm looking for a smart distance metric. \nI've tried some well known metric but no one fits to my case.\n\nex: Let's assume that my cluster algorithm extracts this three centroids [s1, s2, s3]:\n\n\nI want to put this new example [sx] in the most similar cluster:\n\n\n\nThe most similar centroids is the second one, so I need to find a distance function d that gives me ```\nd(sx, s2) < d(sx, s1)```\n and ```\nd(sx, s2) < d(sx, s3)```\n \n\nedit\n\nHere the results with metrics [cosine, euclidean,  minkowski, dynamic type warping]\n]3\n\nedit 2\n\nUser Pietro P suggested to apply the distances on the cumulated version of the time series\nThe solution works, here the plots and the metrics:\n\n    ", "Answer": "\r\nnice question! using any standard distance of R^n (euclidean, manhattan or generically minkowski) over those time series cannot achieve the result you want, since those metrics are independent of the permutations of the coordinate of R^n (while time is strictly ordered and it is the phenomenon you want to capture).\n\nA simple trick, that can do what you ask is using the cumulated version of the time series (sum values over time as time increases) and then apply a standard metric. Using the Manhattan metric, you would get as a distance between two time series the area between their cumulated versions.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "edit_distance with 'findall' in pandas\r\n                \r\nI have a list of tokens and need to find them in a text. I'm using ```\npandas```\n to store my text. However, I noticed that sometimes the tokens I am looking for are misspelled and thus I am thinking about adding the Levenshtein distance to pick those misspelled tokens. At the moment, I implemented a very simple approach:\n\n```\ndf_texts['Text'].str.findall('|'.join(list_of_tokens))\n```\n\n\nThat works perfectly find. My question is how to add ```\nedit_distance```\n to account for misspelled tokens? ```\nNLTK```\n packages offers a nice function to compute edit distance:\n\n```\nfrom nltk.metrics import edit_distance\n\n>> edit_distance('trazodone', 'trazadon')\n>> 2\n```\n\n\nIn the above example, ```\ntrazodone```\n is the correct token, while ```\ntrazadon```\n is misspelled one and should be retrieved from my text.\n\nIn theory, I can check every single word in my texts and measure the edit distance to decided on whether they are similar or not, but it would be very inefficient. Any pythonian ideas?\n    ", "Answer": "\r\nI would start by using a \"spell check\" function to get a list of all words in the corpus which are not spelled correctly.  This will cut down the data set massively.  Then you can brute-force the misspelled words using ```\nedit_distance```\n against all the search tokens whose length is similar enough (say, within one or two characters of the same length).\n\nYou can pre-compute a dict of the search tokens keyed by their length, so when you find a misspelled word like \"portible\" you can check its edit distance from all your search tokens having 7, 8, or 9 characters.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How To Show String Alignment?\r\n                \r\nI am nearly done with this program I am just a little stuck on the last part here. \nwhen run the program will ask for two strings. These are then compared to see the minimum edit distance. Deletions and insertions are a cost of 1, substitutions (delete and insert) are then a cost of 2. \n\nIE quickly and quicker have a distance of 4 because the last two letters would need to be substituted to go from one to the other. \n\nWhat I am having trouble with is showing the alignment. I want to show like:\n\n```\nq u i c k l y\n          s s\nq u i c k e r\n```\n\n\nto show the two substitutions and where they would be. this would also go for deletions and insertions. \n\nHere it is so far:\n\n```\n#!/usr/bin/env python\nimport sys\nfrom sys import stdout\n\ndef  Edit_Distance(target, source):\n\n\nn = len(target)\nm = len(source)\n\ndistance = [[0 for i in range(m+1)] for j in range(n+1)]\n\nfor i in range(1,n+1):\n    distance[i][0] = distance[i-1][0] + insertCost(target[i-1])\n\nfor j in range(1,m+1):\n    distance[0][j] = distance[0][j-1] + deleteCost(source[j-1])\n\nfor i in range(1,n+1):\n    for j in range(1,m+1):\n       distance[i][j] = min(distance[i-1][j]+1,\n                            distance[i][j-1]+1,\n                            distance[i-1][j-1]+substCost(source[j-1],target[i-1]))\nreturn distance[n][m]\n\ndef substCost(x,y):\n    if x == y:\n        return 0\n    else:\n        return 2\n\ndef insertCost(x):\n    return 1\n\ndef deleteCost(x):\n    return 1\n\n\n# User inputs the strings for comparison\nword1 = raw_input(\"Enter A Word: \")\nword2 = raw_input(\"Enter The Second Word: \")\n\n# Simple conditional that will set the length of the range loop below based on the longest string\nif (word2 >= word1):\n    x = len(word2)\nelse:\n        x = len(word1)\n\n# x is then the longest string length so that we have the perfect length range loop\n# stdout.write allows us to print multiple things on the same line, instead of tabbing down a line each time\nprint (\"The minimum edit distance between S1 and S2 is: \", Edit_Distance(word1,word2))\n\nprint list(word1)\nfor i in range(x):\n    if(word1[i] != word2[i]):\n        print(\"D\")\n\nprint list(word2)\n```\n\n    ", "Answer": "\r\nif your question is only how to print the \"D\" aligned, you should look at the \n\nhttp://docs.python.org/2/library/stdtypes.html#string-formatting-operations \n\npage of the python documentation, where you can find how to format a string (which is what you are asking for, if I understand).\n\nBtw, just an hint on how to print aligned (given that the code work well): why don't you use a list to store the \"D\" in the loop ?\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Sorting strings on a circle\r\n                \r\nI have a list of strings like \"AAAA\",\"BAAA\", \"BBAA\", \"BBBA\", \"BBBAA\" etc. (In reality they are longer, more complicated strings) I would like to sort them on a circle. All strings on a circle are a fixed edit distance, by substitution or insertion (say L) from some reference string (not in the list), but the distance to one another can be up to 2L. \n\nI want to organize the strings on this circle such that the sequences are -roughly- sorted by their distance to each other. Meaning closer points (in radians) on the circle, have smaller edit distance with high probability. What is the best approach to do this? (Perfect sort cannot be possible for arbitrary input). I'm thinking insertion sort may be the best approach. \n\nNote: This is not about translating to radial coordinates, my question is about the order of points. The last point on the circle would be neighboring the first point, so a simple distance sort using some point on the circle as a reference is not what I'm looking for. \n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Overload C++ set insert\r\n                \r\nIs it possible to overload the compare operator in a string set so that it defines two elements with edit distance <= 1 as identical?\n\nThis is my failed attempt:\n\n```\n#include <string>\n#include <set>\n#include <vector>\n#include <iostream>\n\nusing namespace std;\n\nint min(int x, int y, int z) \n{\n   return min(min(x, y), z);\n}\n\nint getEditDist(string& str1, string& str2, int m, int n)\n{\n    if (str1[m-1] == str2[n-1])\n        return getEditDist(str1, str2, m-1, n-1);\n    return 1 + min ( getEditDist(str1,  str2, m, n-1),\n                     getEditDist(str1,  str2, m-1, n),\n                     getEditDist(str1,  str2, m-1, n-1)\n        );\n}\n\n\nclass editDist\n{\npublic:\n    bool operator () (string& str1, string& str2)\n        {\n            return(getEditDist(str1, str2, str1.length(), str2.length()) <= 1);\n        }\n};\n\n\nint main(int argc, char* argv[])\n{\n\n    string id1 = \"AAA\";\n    string id2 = \"BBB\";\n    string id3 = \"BAB\";\n    set<string,editDist> my_set;\n    //set<string> my_set;\n    set<string,editDist>::const_iterator itr;\n    //set<string>::const_iterator itr;\n    my_set.insert(id1);\n    my_set.insert(id2);\n    my_set.insert(id3);\n    for(itr = my_set.begin();itr != my_set.end();++itr) cout<<*itr<<endl;\n    return(0);\n}\n```\n\n\nI'm not even sure it's coded correctly since it doesn't compile.\n    ", "Answer": "\r\nTLDR: no it's not possible to do what you want.\n\nDetails:\n\n\nyou can define a comparator for which d(a,b) ≤ 1 ⇒ a ∼ b;\nthe only valid comparator, is ∀(a,b), a ∼ b which is very useless;\nhowever, it is not possible to define a comparator for which d(a,b) ≤ 1 ⇔ a ∼ b which is really what you want.\n\n\nThe comparator defines an ordering and classes of equivalence of strings. If you want any two elements with edit distance ≤ 1 in the same equivalence class, this implies that any elements with edit distance ≤ 2 are in the same equivalence class as well. We can continue the reasoning for any possible edit distance so all the strings must be in the same class of equivalence:\n\n```\n struct always_equal_less {\n   bool operator()(std::string const& x, std::string const& y) const {\n     return false;\n   }\n };\n```\n\n\nMore formal explanation\n\nThe ```\nCompare```\n argument of ```\nstd:set<Key,Compare,Allocator>```\n must follow the Compare concept i.e. it must define a strict weak ordering relation. It must have the following properties:\n\n\ntransitivity, (a ≺ b ∧ b ≺ c) ⇒ a ≺ c;\nirreflexivity, ¬(a ≺ a);\nassymetry, (a ≺ b) ⇒ ¬(b ≺ a);\ntransitivity of incomparability (a ∼ b) ∧ (b ∼ c) ⇒ (a ∼ c).\n\n\nI'm using a ∼ b for ¬(a ≺ b) ∧ ¬(b ≺ a).\n\nLet's suppose we have such a relationship ≺ which has the additional property that for any two elements with the edit distance is lower or equals than 1 are equals: d(a,b) ≤ 1 ⇒ a ∼ b.\n\nWe can show that the only relation for which this is true compares all strings as equals: ∀(a,b), a ∼ b:\n\n\nLet's take two elements with edit distance 2, d(a,b) = 2. We can find a third element c such as: d(a,c) = 1 and d(c,b) = 1. We have a ∼ c and c ∼ b. The transitivity of incomparability gives: a ∼ b. This means that any two elements with edit distance 2 are considered equal as well: ∀(a,b), d(a,b) = 2, a ~ b.\nYou can continue the reasoning for d(x,y)=3, d(x,y)=4. This shows that any given pair of strings must compare as equal.\n\n\nWe get the (useless) relationship ∀(a,b), a ~ b.\n\nIt is thus not possible to define a comparator for which d(a,b) ≤ 1 ⇔ a ∼ b.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "knitr/Rmd: page break after n lines/n distance\r\n                \r\nLet me caveat by saying that this may be better suited as an html/css question but (a) I'm not too familiar with those techniques and (b) I wanted to keep it all in the family (R family) if possible.\n\nI would like to use ```\nknitr```\n to write academic style reports (APA 6 type guidelines) using Rmarkdown.  I've got most aspects worked out but not page breaking.  I can manually set page breaks with something like:\n\n```\n# report\n\n```{r setup, include=FALSE}\n# set global chunk options\nopts_chunk$set(cache=TRUE)\n```\n------\n## Page 1\n\n```{r plot1}\nplot(1:10, 1:10)\n```\n------\n## Page 2\n```\n\n\nIn the following .Rmd I'd like to programatically set those breaks after n lines/n distance.  So let's say after 8 inches or 140 lines. \n\n```\n# report\n\n```{r setup, include=FALSE}\n# set global chunk options\nopts_chunk$set(cache=TRUE)\n```\nInitial Text. Yay!\n\n```{r plot1}\nplot(1:10, 1:10)\n```\n\nMore Text.  Outstanding.  What Hadley's not calling it plyr2?\n\n```{r plot2, fig.width=4, fig.height=4}\nplot(1:10, 1:10)\n```\n\n`r paste(rep(\"So much text so little time!\", 10000))`\n```\n\n\nHow can I programatically set page breaks after n distance.  This is similar to how ```\nLaTeX```\n would break a file into pages so if a figure takes too much space it would be forced to the next page.\n\nEDIT Found this from a friend: http://www.w3.org/TR/css3-page/  may be helpful.\n    ", "Answer": "\r\nProgramatically. Create an HTML div. Set this div's width and height to a fixed amount and the overflow to scroll. \n\n```\n<div style=\"height:1000px; width: 500px; overflow-y: scroll;\">\n    ...\n</div>\n```\n\n\nProcess your markdown into HTML elements. I have 5 ```\nh1```\n tags that are 300px tall each.\n\n```\n<h1 style=\"height:300px;\">First</h1>\n<h1 style=\"height:300px;\">Second</h1>\n<h1 style=\"height:300px;\">Third</h1>\n<h1 style=\"height:300px;\">Fourth</h1>\n<h1 style=\"height:300px;\">Fifth</h1>\n```\n\n\nThese 5 ```\nh1```\n wont all fit on the same page. The page is only 1,000 pixels tall. Only 3 ```\nh1```\n tags will fit on this page. We'll need to insert a pagebreak after the third element. \n\nIncrementally add each new item into the DOM. After inserting each item check to see if the browser's scroll bar is present. If it is, then we know that the item we just inserted was too big for this page; remove the item and insert a page break.\n\nBefore:\n\n```\n### First\n### Second\n### Third\n### Fourth\n### Fifth\n```\n\n\nAfter:\n\n```\n### First\n### Second\n### Third\n------\n### Fourth\n### Fifth\n```\n\n\nThis would work for any element and you wouldn't have to worry about an item's height. Because, if the item you just inserted made the HTML div scroll then we need a page break. Images, videos, h1, h2, p, custom/dynamic css, anything.\n\nUPDATE\n\nYou could also calculate the height of each div element. http://api.jquery.com/height/ That way recalculating a 54 page document would be much easier. \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein distance from index 0\r\n                \r\nI've been working through \"The Algorithm Design Manual\" section 8.2.1 Edit Distance by Recursion. In this section Skiena writes, \"We can define a recursive algorithm using the observation that the last character in the string must either be matched, substituted, inserted, or deleted.\" That got me wondering, why the last character? This is true for any character based on the problem definition alone. The actual Levenshtein distance algorithm makes recursive calls from the back of the strings. Why? There's no reason you couldn't do the opposite, right? Is it just a simpler, more elegant syntax?\n\nI'm flipping the algorithm around, so it iterates from the front of the string. My attempt is below. I know my implementation doesn't work completely (ex: ```\nminDistance(\"industry\", \"interest\")```\n returns 5 instead of 6). I've spent a couple hours trying to figure out what I'm doing wrong, but I'm not seeing it. Any help would be much appreciated.\n\n```\nvar matchChar = (c,d) => c === d ? 0 : 1;\nvar minDistance = function(word1, word2) {\n    var stringCompare = function(s, t, i, j) {\n        if(i === s.length) return Math.max(t.length-s.length-1,0)\n        if(j === t.length) return Math.max(s.length-t.length-1,0)\n        if(cache[i][j] !== undefined) {\n            return cache[i][j]\n        }\n        let match = stringCompare(s,t,i+1,j+1) + matchChar(s[i], t[j]);\n        let insert = stringCompare(s,t,i,j+1) + 1;\n        let del = stringCompare(s,t,i+1,j) + 1;\n\n        let lowestCost = Math.min(match, insert, del)\n        cache[i][j] = lowestCost\n\n        return lowestCost\n    };\n\n    let s = word1.split('')\n    s.push(' ')\n    s = s.join('')\n\n    let t = word2.split('')\n    t.push(' ')\n    t = t.join('')\n\n    var cache = []\n    for(let i = 0; i < s.length; i++) {\n        cache.push([])\n        for(let j = 0; j < t.length; j++) {\n            cache[i].push(undefined)\n        }\n    }\n\n    return stringCompare(s, t, 0, 0)\n}\n```\n\n    ", "Answer": "\r\nThe lines\n\n```\n    if(i === s.length) return Math.max(t.length-s.length-1,0)\n    if(j === t.length) return Math.max(s.length-t.length-1,0)\n```\n\n\nlook wrong to me. I think they should be\n\n```\n    if(i === s.length) return t.length-j\n    if(j === t.length) return s.length-i\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Generic Computation of Distance Matrices in Pytorch [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 2 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI have two tensors ```\na```\n & ```\nb```\n of shape ```\n(m,n)```\n, and I would like to compute a distance matrix ```\nm```\n using some distance metric ```\nd```\n. That is, I want ```\nm[i][j] = d(a[i], b[j])```\n. This is somewhat like ```\ncdist(a,b)```\n but assuming a generic distance function ```\nd```\n which is not necessarily a p-norm distance. Is there a generic way to implement this in PyTorch?\nAnd a more specific side question: Is there an efficient way to perform this with the following metric\n```\nd(x,y) = 1 - cos(x,y)\n```\n\nedit\nI've solved the specific case above using this answer:\n```\ndef metric(a, b, eps=1e-8):\n    a_norm, b_norm = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n    a_norm = a / torch.max(a_norm, eps * torch.ones_like(a_norm))\n    b_norm = b / torch.max(b_norm, eps * torch.ones_like(b_norm))\n    similarity_matrix = torch.mm(a_norm, b_norm.transpose(0, 1))\n    return 1 - similarity_matrix\n```\n\n    ", "Answer": "\r\nI'd suggest using broadcasting: since ```\na,b```\n both have shape ```\n(m,n)```\n you can compute\n```\nm = d( a[None, :, :], b[:, None, :])\n```\n\nwhere ```\nd```\n needs to operate on the last dimension, so for instance\n```\ndef d(a,b): return 1 - (a * b).sum(dim=2) / a.pow(2).sum(dim=2).sqrt() / b.pow(2).sum(dim=2).sqrt()\n```\n\n(here I assume that ```\ncos(x,y)```\n represents the normalized inner product between ```\nx```\n and ```\ny```\n)\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "C# array distance function\r\n                \r\nIs there any simple way to calculate some sort of distance function on 2 arrays of the same length so as to check their difference? The arrays are both float and each bin may be empty or contain a value. I am doing this since I need to compare two color histograms of 2 different images.\nThank you\n\nEdit: By distance function I mean something like Levenshtein distance on the two arrays so I can check the 'difference' between. I was hoping to check whether the object is in the image according to the distance calculated.\n    ", "Answer": "\r\nIf you just want the sum of the differences between the individual values, you could use:\n\n```\nvar distance = array1.Zip(array2, (a,b) => Math.Abs(a-b)).Sum();\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "What's wrong with my Euclidean Distance Calculation? (Julia)\r\n                \r\nI'm trying to compute the Perceptually Important Points by using three different methods. \n\n\nEuclidean Distance; \nPerpendicular Distance; \nVertical Distance.\n\n\nMethod 2 and 3 gives me the same Point, but Euclidean distance not. Can't find the mistake I made. Hope someone can help me.\n\n```\npt = 7.6 #pt\n_t = 1 #t\nptT = 10.7 #p(t+T)\n_T = 253 #t+T\n\n# Distances\ndE = Float64[] #Euclidean Distances\ndP = Float64[] #Perpendicular Distances\ndV = Float64[] #Vertical Distances\nxi = Float64[] #x values\n\nfor i in 2:length(stockdf[:Price])-1\n_de = sqrt((_t - i)^2 + (pt - stockdf[:Price][i])^2) + sqrt((_T - i)^2 + (ptT - stockdf[:Price][i])^2)\npush!(dE,_de)\n\n_dP = abs(_s*i+_c-stockdf[:Price][i])/sqrt(_s^2+1)\npush!(dP,_dP)\n\n_dV = abs(_s*i+_c-stockdf[:Price][i])\npush!(dV,_dV)\n\npush!(xi,i)\nend\n```\n\n\nBoth method 2 and 3 give me the max point indexed at 153, but method 1 gives me a point, which is not the max point and is indexed at 230.\n\nFormula for the 3rd PIP with Euclidean Distance is:\n\n```\ndE = sqrt((t-i)^2 + (pt-pi)^2) + sqrt((t+T-i)^2+(pt+T-pi)^2)\n```\n\n\nEDIT:\nFor a better understanding I reproduced the code with other variables which you can test for yourself.\n\n```\nxs = Array(1:10)\nys = rand(1:1:10,10)\n\ndde = Float64[]\nddP = Float64[]\nddV = Float64[]\nxxi = Float64[]\n\n# Connecting Line of first 2 PIPs\n_ss = (ys[end]-ys[1])/10\n_cc = ys[1]-(1*(ys[end]-ys[1]))/10\n_zz = Float64[]\n\nfor i in 1:length(dedf[:Price])\n    push!(_zz,_ss*i+_cc)\nend\n\n\nfor i in 2:length(xs)-1\n    _dde = sqrt((1-i)^2+(ys[1]-ys[i])) + sqrt((10-i)^2 + (ys[end]-    ys[i])^2)\n    push!(dde,_dde)\n\n    _ddP = abs(_ss*i+_cc-ys[i])/sqrt(_ss^2+1)\n    push!(ddP,_ddP)\n\n    _ddV = abs(_ss*i+_cc-ys[i])\n    push!(ddV,_ddV)\n\n    push!(xxi,i)\nend\nprintln(dde)\n\nfor i in 1:length(dde)\n    if ddV[i] == maximum(ddV)\n        println(i)\n    end\nend\n```\n\n\nFor Euclidean Distance I get index 7 \nfor Perpendicular and Vertical Distance I get index 5. Look at the graphs\nEuclidean Distance on graph\n\nPerpendicular Distance on graph\n\nEDIT:\nI'm working through a book about pattern recognition in financial time series. Now I downloaded the same data, which the book used and the now the results are the same. All of the 3 methods gave me the same index. But with different data sets, method 1 differs from 2 and 3. I don't know why. \n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Algorithm used for auto-suggestion\r\n                \r\nWhich algorithms or data structures are used in auto-suggest features?\n\nIt seems that edit-distance will be used, but again a frequency or score associated with each word should also be considered. For example, consider the tags option on SO's Ask Question page.\n    ", "Answer": "\r\nYou can use a trie:\n\n\nevery node of the trie has all the children that begins with the value itself, for example: from \"in\" node you can visit the subtree of all strings starting with \"in\"\nin your case you have to consider score so you can first gather all children (traversing the tree) and then sort them according to the score or whatever\nif you really want to keep Hamming Distance (edit-distance) you can adapt the trie to build children according to it\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to return just distance from a google maps distance matrix\r\n                \r\nI want to determine the distance between two addresses using the google maps distance matrix API. How can I edit my code such that the output I get is only distance rather than the entire JSON dictionary?\n\nI am currently using json which returns destination, origin, rows(```\ndistance(text, value)```\n and ```\nduration(text, value)```\n) and status. I can get it down to just displaying 'rows' but want to isolate ```\ndistance(value)```\n. ```\npick[i]```\n and ```\ndrop[i]```\n are inputs from a csv file.\n\n```\nfor i in range(1,10):\n    source = pick[i]\n\n    dest = drop[i]\n\n    url ='https://maps.googleapis.com/maps/api/distancematrix/json?'\n\n    r = requests.get(url + 'origins=' + source +\n                       '&destinations=' + dest +\n                       '&key=myapi') \n\n    x = r.json()\n\n    print(i,x['rows'],\"\\n\")\n```\n\n\nMy current output is:\n\n```\n[{'elements': [{'distance': {'text': '18.2 km', 'value': 18218}, 'duration': {'text': '21 mins', 'value': 1264}, 'status': 'OK'}]}]\n```\n\n    ", "Answer": "\r\nHere you have it. I just tested it and it works:\n```\ngmaps = googlemaps.Client(key=api_key)\ndistance_result = gmaps.distance_matrix(origin, destination, units=\"metric\")\nprint(distance_result[\"rows\"][0][\"elements\"][0]['distance']['text'])\n```\n\nlet me know if this is what you need.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Modify Levenshtein-Distance to ignore order\r\n                \r\nI'm looking to compute the the Levenshtein-distance between sequences containing up to 6 values. The order of these values should not affect the distance.\n\nHow would I implement this into the iterative or recursive algorithm?\n\nExample:\n\n```\n# Currently \n>>> LDistance('dog', 'god')\n2\n\n# Sorted\n>>> LDistance('dgo', 'dgo')\n0\n\n# Proposed\n>>> newLDistance('dog', 'god')\n0\n```\n\n\n'dog' and 'god' have the exact same letters, sorting the strings before hand will return the desired result. However this doesn't work all the time:\n\n```\n# Currently \n>>> LDistance('doge', 'gold')\n3\n\n# Sorted\n>>> LDistance('dego', 'dglo')\n2\n\n# Proposed\n>>> newLDistance('doge', 'gold')\n1\n```\n\n\n'doge' and 'gold' have 3/4 matching letters and so should return a distance of 1.\nHere is my current recursive code:\n\n```\ndef mLD(s, t):\n    memo = {}\n    def ld(s, t):\n        if not s: return len(t)\n        if not t: return len(s)\n        if s[0] == t[0]: return ld(s[1:], t[1:])\n        if (s, t) not in memo:\n            l1 = ld(s, t[1:])\n            l2 = ld(s[1:], t)\n            l3 = ld(s[1:], t[1:])\n            memo[(s,t)] = 1 + min(l1, l2, l3)\n        return memo[(s,t)]\n    return ld(s, t)\n```\n\n\nEDIT: Followup question: Adding exceptions to Levenshtein-Distance-like algorithm\n    ", "Answer": "\r\nYou don't need the Levenshtein machinery for this.\n\n```\nimport collections\ndef distance(s1, s2):\n    cnt = collections.Counter()\n    for c in s1:\n        cnt[c] += 1\n    for c in s2:\n        cnt[c] -= 1\n    return sum(abs(diff) for diff in cnt.values()) // 2 + \\\n        (abs(sum(cnt.values())) + 1) // 2   # can be omitted if len(s1) == len(s2)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to calculate distance between two cities using google maps api V3 [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs debugging details. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     Edit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nHow to calculate the driving distance ?\n\nI'm unable to find a working example, can anybody help me ?\n\nThank you.\n\nEDIT: \nMore info:\n\nI have two text inputs to type the cities. On change I just want to display/update the distance in another div.\n    ", "Answer": "\r\nWhat you want is the Distance Matrix API.  That is, if you are really using it in combination google maps.  Note that Google prohibits the use of this API if you are not using it with a Google Map somewhere on the page.\n\nHere's a basic example of the Distance Matrix API:\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n<script type=\"text/javascript\" src=\"http://maps.google.com/maps/api/js?sensor=false\"></script>\n<script type=\"text/javascript\">\nvar origin = new google.maps.LatLng(55.930385, -3.118425),\n    destination = \"Stockholm, Sweden\",\n    service = new google.maps.DistanceMatrixService();\n\nservice.getDistanceMatrix(\n    {\n        origins: [origin],\n        destinations: [destination],\n        travelMode: google.maps.TravelMode.DRIVING,\n        avoidHighways: false,\n        avoidTolls: false\n    }, \n    callback\n);\n\nfunction callback(response, status) {\n    var orig = document.getElementById(\"orig\"),\n        dest = document.getElementById(\"dest\"),\n        dist = document.getElementById(\"dist\");\n\n    if(status==\"OK\") {\n        orig.value = response.destinationAddresses[0];\n        dest.value = response.originAddresses[0];\n        dist.value = response.rows[0].elements[0].distance.text;\n    } else {\n        alert(\"Error: \" + status);\n    }\n}\n</script>\n</head>\n<body>\n    <br>\n    Basic example for using the Distance Matrix.<br><br>\n    Origin: <input id=\"orig\" type=\"text\" style=\"width:35em\"><br><br>\n    Destination: <input id=\"dest\" type=\"text\" style=\"width:35em\"><br><br>\n    Distance: <input id=\"dist\" type=\"text\" style=\"width:35em\">\n</body>\n</html>\n```\n\n\nThis is a version of google's example adapted to your personal taste which I found in the Google Maps Api v3 - Distance Matrix section\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Fast Infix Searching\r\n                \r\nI'm building an autocomplete that will have to query a 10+ million words/phrases quickly, and am running into some problems.  My first idea was to go through some sort of trie/ternary tree structure, but those are strictly prefix matching, which isn't good enough for my application (i want full infix matching).  I then moved to some of the bigger solutions,  SqlServer FullText Indexing, Lucene, Solr, Sphinx, but Lucene and SqlServer FullText Indexing aren't actually fulltext, but prefix with nifty features (soundex, proximity, etc).  I tried to think of a way Levenshtein edit distance could help, but couldn't find a way to be both at least reasonably accurate as well as supporting words with high edit distances (i.e. google and ogl.  edit distance of 3, but 3 is way to high a threshold a general case).  \n\nMy question is, how do powerhouses like Google/bing etc do it?  Do they just brute force it after a bit?  I would imagine no, but I can't find any support of that.\n\nAny help would be appreciated!\n    ", "Answer": "\r\nIf you enable ```\nqueryParser.setAllowLeadingWildcard(true);```\n in Lucene, you can use leading and trailing wildcards like:\n\n```\n*talli*\n```\n\n\nThat would pick up all single-word terms that contain \"talli\" including \"Metallica\".\n\nThis may not be fast enough for you, but in some cases (prefix-only wildcard searches to be precise) if you can preprocess the query string you might be able to get by with the old \"reverse the term and index that also\" trick:\n\n```\nacillateM\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to sort array of objects(latitude, longitude) by nearest distance?\r\n                \r\nI've an array from core data and I'm trying to think how can I sort the array by the nearest distance:\n\n```\nfor (int i=0; i<allTasks.count; i++) {\n        id singleTask = allTasks[i];\n        double latitude = [singleTask[@\"latitude\"] doubleValue];\n        double longitude = [singleTask[@\"longitude\"] doubleValue];\n    }\n```\n\n\nEDIT:\nThe distance between current location and all the locations in the array.\nI know how to calculate the distance, I don't know how to sort them.\n    ", "Answer": "\r\nSo do you want to sort your allTasks array?\n\nThe best thing to do would be to add a distance key/value pair to each singleTask object, holding a double NSNumber.\n\nIn a first pass, loop through your allTasks array, fetch each lat/long, use it to create a CLLocation, and use the CLLocation method distanceFromLocation: to calculate the distance between each location and your target (current?) location. Save the result into each singleTask object in your array.\n\nOnce your allTasks array contains a distance property, simply use one of the sort methods like sortUsingComparator to sort the array based on the distance value. (In the sortUsingComparator family of methods, you provide a comparator block that the system uses to compare pairs of objets. It then runs a sort algorithm on your array, using your comparator to decide on the sort order.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Fuzzy match in proc sql, select minimum distance\r\n                \r\nI have a list with some fuzzy variables and a list with some clean variables. I want to want to (fuzzy) match both of them where the compged distance is lowest and some rules regarding their edit distances are satisfied (f.ex. compged < 100 and spedis < 50). I tried the following code \n\n```\nproc sql noprint;\ncreate table out as\nselect min(compged(fuzzy.fuzzy_title,clean.cleaned_title,100)) \nas comp,\nfuzzy.fuzzy_title, clean.cleaned_title\nfrom fuzzy inner join clean\non (compged(fuzzy.fuzzy_title,clean.cleaned_title,100) < 100 and \nspedis(clean.cleaned_title,fuzzy.fuzzy_title) < 50);\nquit;\n```\n\n\nThe datasets fuzzy and clean basically just contain the titles that I want to match. The code I use just gives me the minimum compged score of the whole dataset and then some arbitrary match where my condition regarding the distances are satisfied. Is there a way to choose exactly the clean_title with the minimum compged score for a given fuzzy_title? I might have searched wrong, but I couldn't find the answer to this.\n    ", "Answer": "\r\nI think you are looking for ```\ngroup by```\n + ```\nhaving```\n:\n\n```\n   proc sql;\n    create table out as\n    select \n         compged(fuzzy.fuzzy_title,clean.cleaned_title,100)as comp\n        ,fuzzy.fuzzy_title\n        ,clean.cleaned_title\n    from fuzzy inner join clean\n        on (compged(fuzzy.fuzzy_title,clean.cleaned_title,100) < 100 \n        and spedis(clean.cleaned_title,fuzzy.fuzzy_title) < 50)\n    group by fuzzy.fuzzy_title\n    having calculated comp = min(compged(fuzzy.fuzzy_title,clean.cleaned_title,100))\n    ;quit;\n```\n\n\nIf there are more ```\nfuzzy_title```\n + ```\ncleaned_title```\n pairs having the same ```\ncomp```\n value, all of them will be in the output. You can select only one of them in a single query. However, I think it's easier to keep those steps separated and select one row for each ```\nfuzzy_title```\n in another query (e.g. using ```\nfirst```\n data step variable).\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Deleting a word from a specific implementaion of trie in Python\r\n                \r\nI am kinda new to datastructures and I am implementing a trie to disambiguate a database of names using edit distance. I am using the following implementation of the trie:\n\nhttp://stevehanov.ca/blog/index.php?id=114\n\nwhich is basically:\n\n```\nclass TrieNode:\n\n    def __init__(self):\n       self.word = None\n       self.children = {}\n\n       global NodeCount\n       NodeCount += 1\n\n    def insert( self, word ):\n       node = self\n       for letter in word:\n            if letter not in node.children: \n                node.children[letter] = TrieNode()\n\n            node = node.children[letter]\n\n       node.word = word\n\n# read dictionary file into a trie\ntrie = TrieNode()\nfor name in names:\n    WordCount += 1\n    trie.insert( name )\n```\n\n\nThis does the job beautifully as it inserts all the names into a trie. Now, I go through the list of names I have one by one, and use the trie to return a list of all names that are at a certain edit distance from the passed name. I want to then delete all the names from the trie that were returned in the list.\n\nIs there a fast way to do that?\n\nThanks!\n    ", "Answer": "\r\nThere are two ways to do this, depending on whether you want to check whether you're removing the last path through any internal node (which makes removes slightly slower, but potentially makes searches after the removes slightly faster). Both ways are trivial to do recursively, but if you want to unroll it iteratively (as your ```\ninsert```\n does), not checking is easier, so I'll do that.\n\n```\ndef delete(self, word):\n    node = self\n    for letter in word[:-1]:\n        if letter not in node.children:\n            return False\n        node = node.children[letter]\n    if word[-1] in node.children:\n        del node.children[letter]\n        return True\n    return False\n```\n\n\n\n\nCan you make this faster? Yes, but it may not matter.\n\nFirst, you know that the nodes will always exist, so you can remove some of the error checking. More importantly, if you can make the search function return the nodes, instead of just their values, that will make things a little faster. If you can add backlinks up the trie, that means you can erase the node in constant time instead of repeating the search. If you don't want backlinks up the trie, you can get the exact same benefit by returning a zipper instead of a node—or, more simply, just returning a stack of nodes.\n\nBut really, the worst case here is just doubling the work, not increasing the algorithmic complexity or multiplying by a large factor, so simple probably wins.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Calculating distance between dots java\r\n                \r\nMy task is to load coordinates from a file, and calculate perimeter of the shape.\nexample of dots.txt\n```\n-3, 9\n-8, 7\n-12, 4\n-6, -2\n-4, -6\n2, -8\n6, -5\n10, -3\n8, 5\n4, 8\n```\n\nI have created a Dot.class which is quite simple, with an method to calculate distance between 2 dots.\n```\npublic class Dot {\n    int x;\n    int y;\n\n    public Dot(int x, int y)\n    {\n        this.x = x;\n        this.y = y;\n    }\n\n\n    public double calcDistance(Dot anotherDot)\n    {\n        int dx = x - anotherDot.x;\n        int dy = y - anotherDot.y;\n        return Math.sqrt((dx*dx) + (dy*dy));\n//EDIT: WORKS SAME AS\n//return Math.hypot(x - anotherDot.x, y - anotherDot.y);\n    }\n\n    public String toString(){\n        return \"\" + x + y;\n    }\n}\n```\n\nHere is my Main method:\n```\nimport java.io.*;\nimport java.util.*;\n\npublic class Main {\n\n    public static void main(String[] args) throws Exception {\n        //read file\n        FileReader fr = new FileReader(\"src/com/company/dots.txt\");\n        Scanner inFile = new Scanner(fr);\n        ArrayList<Dot> dots = new ArrayList<>();\n\n        while (inFile.hasNext())\n        {\n            // Read the next line.\n            String [] line = inFile.nextLine().split(\",\");\n            // Display the line.\n            int x = Integer.parseInt(line[0]);\n            int y = Integer.parseInt(line[1].replace(\" \", \"\"));\n            System.out.println(x + \" \" + y);\n            dots.add(new Dot(x, y));\n        }\n\n        System.out.println(\"size of the ArrayList: \" + dots.size());\n\n\n        // Close the file.\n        inFile.close();\n\n    }\n}\n```\n\nPlease help me with calculating perimeter because I am pretty stuck with this one..\nThanks in advance\nEDIT:\nManaged to write a simple for-loop to calculate distance between dots and then sum it up.\n```\nint lastDot = dots.size() - 1;\ndouble perimeter = 0;\n    for(int i = 0; i< dots.size(); i++)\n    {\n//check if its the last dot, then calculate distance between the last dot and the first one.\n        if(i==lastDot){ \n            perimeter += dots.get(lastDot).calcDistance(dots.get(0));\n        } else\n        perimeter += dots.get(i).calcDistance(dots.get(i+1));\n    }\n```\n\n    ", "Answer": "\r\nHave you tried something like:\n```\ndouble dist=0;\nfor(int i=0;i<dots.size();i++){\n   dist+= dots.get(i%dots.size()).calcDistance((i+1)%dots.size());\n}\n```\n\nor you mean something more complex like not having crossing of lines?\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to modify Levenshtein algorithm, to know if it inserted, deleted, or substituted a character in XQuery?\r\n                \r\nSo I am trying to devise a spin off of the Levenshtein algorithm, where I keep track of what transformations I did in the string(insert a, replace a for b, or delete) in strictly XQuery only. \n\nExample:\n\nBasically, say I am computing the edit distance of \"cat\" and \"cbt\", and am trying to transform \"cbt\" into \"cat.\"\n\nThe edit distance will be 1 and the transformation will be \"replace (letter at index 1) for a\"\n\nP.S. This same post was made 5 years ago, but I wish to have a solution in XQuery instead of Python, and I am finding it quite hard to do so as I am unable to use 2-D arrays (sequences).    \n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Similarity between more than two strings Java\r\n                \r\nI know there are several algorithms (Soundex Algorithm, Levenshtein distance/ Edit Distance, and Longest Common Substring)  that can be used to compare two strings. Is there a known way to compare more than two strings and eliminate ones that are the least similar based on say a Max Levenshtein distance you've set?\n\nExample: \n\n```\n String -> {sam, ham, jam, stackoverflow}\n```\n\n\nAssuming I set my Levenshtein distance to 2 the output should have only three surviving items. \n\n```\nOutput -> {sam, ham, jam}\n```\n\n\nThe implementation I have has an O(n^2) Time Complexity. Is there a better/clever way of doing this?\n\nMethod signature:\n\n```\npublic String[] bestMatches (String[] s, int levenshteinDistanceMax){\n\n\n}\n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Java Sort by Distance\r\n                \r\nI have a 2-dimensional array like this:\n\n```\n{1,3,5,6,2,2}\n\n{6,2,4,7,2,1}\n\n{17,28,32,1,35,45}\n\n...\n```\n\n\nI have a function that can calculate the euclidean distance between any two arrays.  Assuming that the distance function operates correctly, what's the best way to sort the 2-dimensional array so that the arrays within the 2-dimensional array are swapped such that the one's closest to each other(distance wise) would be close to each other in the 2-dimensional array?\n\nEdit\n\nIs this boiling down to the travelling salesman problem again?\n    ", "Answer": "\r\nI'd use a ```\nTreeSet```\n and a comparator.\n\n```\npublic class EuclidComparator implements Comparator<int[]> {\n\n    @Override\n    public int compare(int[] o1, int[] o2) {\n\n        return euclid_distance(o1, o2);\n\n    }\n\n}\n```\n\n\nSort with:\n\n```\n    TreeSet<int[]> sort = new TreeSet<int[]>(new EuclidComparator());\n    sort.addAll(Arrays.asList(arrays));\n```\n\n\nOr even easier:\n\n```\n    Arrays.sort(arrays, new EuclidComparator());\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Breaking XOR repeated key\r\n                \r\nI want to break XOR repeated key, I dont now anything about the key nor the message, only thing I know that it is using repeated key. Encoded message s beenbase64'd after being encrypted with repeating-key XOR so I converted base 64 to base16 first so it is easier. I have instructions but I don't understand this very good.\n\n\nLet KEYSIZE be the guessed length of the key; try values from 2 to (say) 40.\nWrite a function to compute the edit distance/Hamming distance between two strings. \nFor each KEYSIZE, take the first KEYSIZE worth of bytes, and the second KEYSIZE worth of bytes, and find the edit distance between them. Normalize this result by dividing by KEYSIZE.\nThe KEYSIZE with the smallest normalized edit distance is probably the key. You could proceed perhaps with the smallest 2-3 KEYSIZE values. Or take 4 KEYSIZE blocks instead of 2 and average the distances.\n\n\nNow that you probably know the KEYSIZE: break the ciphertext into blocks of KEYSIZE length, etc, I got this and the rest fine, for now, I should now exactly when I found out if this is good and try to decode..\n\nI wrote a code for this in Python, it is working, but I am not completely sure if I have done this correctly\n\n```\n    def compute_distance(str1,str2,keysize):\n      count=0\n      str1=str1.replace(\"\\n\", \"\")\n      str2=str2.replace(\"\\n\", \"\")\n      keysize=str(keysize*8)\n      sbin1=format(int(str1,16),'0'+keysize+'b')\n      sbin2=format(int(str2,16),'0'+keysize+'b')\n\n      for c1,c2 in zip(sbin1, sbin2):\n        if  c1!=c2:\n          count+=1\n\n      return count\n\n\n    def keysize_dist(filelocation):\n      f=open(filelocation,'r')\n      lines=[]\n      for line in f.readlines():\n        line=line.strip('\\n')\n        lines.append(line)\n      lines=''.join(lines).strip('\\n')\n      normalized=[]\n      for keysize in range(2,40):\n        count=compute_distance(lines[0:keysize*2],lines[keysize*2:keysize*4],keysize)\n\n        normalized.append(float(count)/keysize)\n\n\n      return lines,int(min(normalized))\n```\n\n    ", "Answer": "\r\nThis is way i understood from your post.\nI did a python program that generate the ciphered xor stream with cycling key and that try to apply hamming string distance normalized method to find the best potential cycling keysize.\nI don't convert things into base64 and i apply directly string distance not binary distance.\n\n```\n#!/usr/bin/python\n\nimport sys\nfrom itertools import cycle\n\ndef xor_file_with_cycling_strkey(filelocation,outfile,key):\n  print filelocation\n  f=open(filelocation,'r')\n  f2=open(outfile,'w')\n  lines=[]\n  text=f.read()\n  if text != '':\n    for c,k in zip(text,cycle(key)):\n      r=chr(ord(c)^ord(k))\n      f2.write(r)\n  f2.close()\n  f.close()\n\n# not used here, see compute_distance_char based on same idea.\ndef compute_distance(str1,str2,keysize):\n  count=0\n  print '%s %s' % (str1,str2)\n  str1=str1.replace(\"\\n\", \"\")\n  str2=str2.replace(\"\\n\", \"\")\n  keysize=str(keysize*8)\n  sbin1=format(int(str1,16),'0'+keysize+'b')\n  sbin2=format(int(str2,16),'0'+keysize+'b')\n  return hamming_distance_str(sbin1,sbin2)\n\n#do preferer hamming_distance_bin which quicker.\ndef compute_distance_char(str1,str2,keysize):\n  count=0\n  str1=str1.replace(\"\\n\", \"\")\n  str2=str2.replace(\"\\n\", \"\")\n  keysize=str(keysize*8)\n  sbin1=''\n  sbin2=''\n  for c in str1:\n    sbin1=sbin1 + format(ord(c),'0'+keysize+'b')\n  for c in str2:\n    sbin2=sbin2 + format(ord(c),'0'+keysize+'b')\n  return hamming_distance_str(sbin1,sbin2)\n\ndef hamming_distance_str(str1,str2):\n  count=0\n  for c1,c2 in zip(str1, str2):\n    if  c1!=c2:\n      count+=1\n  return count\n\ndef hamming_distance_bin(str1,str2):\n  count=0\n  for c1,c2 in zip(str1, str2):\n    if  c1!=c2:\n      # quick hamming distance, counting number of differing bits.\n      s=ord(c1)^ord(c2)\n      # count number of bits sets using Wegner algorithm\n      while s !=0:\n        s&=(s-1);\n        count+=1\n  return count\n\ndef keysize_dist(filelocation):\n  potential_keysize=0\n  min_dist=40.0\n  f=open(filelocation,'r')\n  lines=[]\n  for line in f.readlines():\n    line=line.strip('\\n')\n    lines.append(line)\n  lines=''.join(lines).strip('\\n')\n  normalized=[]\n  for keysize in range(2,40):\n# should first create base16 entries for that one , then don't use it : count_bin1=compute_distance(lines[0:keysize*2],lines[keysize*2:keysize*4],keysize)\n    # proof that both functions compute same value :\n    count_bin1=compute_distance_char(lines[0:keysize*2],lines[keysize*2:keysize*4],keysize)\n    count_bin2=hamming_distance_bin(lines[0:keysize*2],lines[keysize*2:keysize*4])\n    if ( count_bin1 != count_bin2 ):\n      print 'Discrepency between compute_distance_char->%i and hamming_distance_bin->%i' % (count_bin1,count_bin2)\n    count=hamming_distance_str(lines[0:keysize*2],lines[keysize*2:keysize*4])\n\n    normalized_distance=float(count)/keysize\n    print '%s %f' % (keysize,normalized_distance)\n    if ( normalized_distance < min_dist ):\n      potential_keysize=keysize\n      min_dist=normalized_distance\n#  we are more interested in keysize corresponding to minimal distance, tha n to minimal distance itself.\n  return potential_keysize,min_dist\n\ndef main(args=sys.argv):\n if ( len(args) < 2 ):\n   print 'Please enter cleartext origin file to be ciphered then checked an optionaly a key string ( max length 40 )'\n   return 1\n if ( len(args) > 2):\n   key=args[2]\n else:\n   # on purpose default to key with a KEYSIZE char length 5.\n   key='12345'\n xor_file_with_cycling_strkey(args[1],args[1]+'.ciphered',key)\n xor_file_with_cycling_strkey(args[1]+'.ciphered',args[1] + '.cleartext',key)\n\n # raw non base64 encoded.\n print keysize_dist(args[1] + '.ciphered')\n\nif __name__ == \"__main__\":\n    main()\n```\n\n\nWith that code your can get all inputs needed to fully resolve your problem.\n\n./hamming_detect_xor_cycle.py cleartext 123456789ABCDE\n...\n(14, 1.7857142857142858)\n\nIt does not detect correctly all size, but i think this is a statistical effect and depends on cleartext that itself can have cycling properties. as your subject tells : using with more blocks can give a better result.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "iOS: distance of route between two locations (route, not straight line)\r\n                \r\nI have a MKMapview with multiple annotations. When the user chooses an annotation, the \"callout\" view shows the location's name and the distance between the location and the user location.\n\nI'm stuck at getting the distance. Prior to iOS 5, I could use Google direction API to get the distance, but now I can't. Is there any alternative solution?\n\nThank you!\n\nEDIT:\n\nThe distance I refer is the distance of the route between two locations. The route has many routes. It's not the distance of the straight line as the result of \"distanceFromLocation:\"\n    ", "Answer": "\r\nIf you have two CLLocations, you can use the ```\ndistanceFromLocation:```\n method to get the distance in meters between them.\n\nExample:\n\n```\nCLLocation* first = [[CLLocation alloc] initWithLatitude:firstLatitude longitude:firstLongitude];\nCLLocation* second = [[CLLocation alloc] initWithLatitude:secondLatitude longitude:secondLongitude];\n\nCGFloat distance = [first distanceFromLocation:second];\n```\n\n\nYou can get the latitude and longitude of your annotation (not your annotation view) from its ```\ncoordinate```\n or similar property on your annotation class.\n\nIf you have a list of annotations and want to get the distance of a path between them, simply:\n\n```\nCGFloat distance = 0;\nfor(int idx = 0; idx < [mapView.annotations count] - 1; ++idx) {\n    CLLocationCoordinate2D firstCoord = [mapView.annotations[idx] coordinate];\n    CLLocationCoordinate2D secondCoord = [mapView.annotations[idx + 1] coordinate];\n\n    CLLocation* first = [[CLLocation alloc] initWithLatitude:firstCoord.latitude longitude:secondCoord.latitude];\n    CLLocation* second = [[CLLocation alloc] initWithLatitude:secondCoord.latitude longitude:secondCoord.longitude];\n\n    distance += [first distanceFromLocation:second];\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Google maps: Discrepancy between distance calculated for original path and distance calculated for ElevationResult path\r\n                \r\nI have a path consisting of several LatLngs.\nI calculate its length using the following code:  \n\n```\nfor (i = 0; i < polyPath.length-1; i++) {  \n      totalDistance2 = totalDistance2 +   \n         google.maps.geometry.spherical.computeDistanceBetween(polyPath[i],polyPath[i+1]);\n}\n```\n\n\nNow i send a request to ElevationService for this path and compute the distance of the path provided by the result.\n\n```\nfor (i = 0; i < elevations.length-1; i++) {\n      totalDistance = totalDistance + \n         google.maps.geometry.spherical.computeDistanceBetween(elevations[i].location,elevations[i+1].location);\n} \n```\n\n\nFor any path that i use there is always a discrepancy between the two distances reported, with the original path distance being always greater (by about 1%).\n\nI have used 512 as the sample number for the elevation request. Lowering this increases the difference in distances.\n\nedit: the path contains around 200-300 vertices\n\nedit2: jsfiddle for working example: http://jsfiddle.net/wz3pup5o/ and the corresponding console output:\ninitial path length: 255\ndistance iterating over path latlngs: 8334.314359218815\ndistance using computelength(path): 8334.314359218815\ndistance iterating over elevation results: 8199.077136099133\n    ", "Answer": "\r\nThe elevation points are equally spaced along the path.  The path created by connecting those points takes some shortcuts.  Examine the blue polyline in this fiddle or the code snippet below.  Particularly around the sharp turns around point 213/214 and 218/219\n\ncode snippet:\n\n\r\n\r\n```\nvar path;\r\nvar map;\r\n$(window).load(function() {\r\n  map = new google.maps.Map(document.getElementById('map'), {\r\n    center: {\r\n      lat: 20.0094671,\r\n      lng: 73.3826942\r\n    },\r\n    zoom: 12,\r\n    streetViewControl: false\r\n  });\r\n\r\n  var polyString = \"iibyBody~L|MiOs@eAiAw@YYO]Mk@Ca@IkAHu@r@_C`@s@^c@dA_Ax@sAdB{Az@u@Xs@\\Wf@I`BYVIRUvA_CX_@JWBg@Ga@Qe@i@]Wa@@{AFaAIe@KQKKGOCk@Cc@IWQs@@aAB{AH{ANmBPs@Zw@`@}@Di@Cw@Ae@HaAHuBF_@\\aALa@b@g@JSBOAIQQ}@u@KUCg@?aANgAp@oBbAmB\\gAPm@Dm@FgDK{@Pm@Vu@d@eCFy@T{APQn@[x@i@JYGa@kA}Cy@_Cu@aCIy@@QFMx@y@zAqB\\{A`@o@d@g@f@w@HMYJgB`@gA\\_CbAo@n@YHQ?i@SOKIMCY@kANs@`@eBb@uAf@{@FQEMKEoADy@BQVOnAIVSJS@a@Gy@OSMi@mAg@e@gB_Aq@i@s@s@}AcBoBmA}Ag@k@YQYK[UYu@Yg@E_@?UEa@Om@K_Ce@oA[{@[oDaBoDuAsAa@Y?MFGNWP[Iu@a@e@c@i@Mi@AYBSRMRE`@KXsA`AU@SGY]SKMHAX@LHR`@~@VLj@PR?TKNEF?DFDPER_@j@ERB`@JhA?l@G`@Kb@ORm@^]FYA]Ok@Qc@MgBOgBM]Ig@[q@_BKw@Ay@EU[Mm@_@U[AYZmAFe@CKIAODWd@Sp@MRMAIMB]HMD_@?{@CIGCQ@[V_A~@MDO?OKe@iAWq@UUk@OOMC]A}Aj@_CNWRQj@UfA?^?HMLs@\\gBz@mCv@iCvA_CnA{BcBRgGj@w@Ro@FsBPo@Lk@PPn@\";\r\n\r\n  path = google.maps.geometry.encoding.decodePath(polyString);\r\n\r\n  var polyline = new google.maps.Polyline({\r\n    path: path,\r\n    strokeColor: 'purple',\r\n    map: map,\r\n    strokeWeight: 4\r\n  });\r\n\r\n  var elevator = new google.maps.ElevationService();\r\n\r\n  elevator.getElevationAlongPath({\r\n    path: path,\r\n    samples: 512\r\n  }, calcdistance);\r\n\r\n});\r\n\r\nfunction calcdistance(results, status) {\r\n  if (status == google.maps.ElevationStatus.OK) {\r\n\r\n    var totalDistance2 = 0;\r\n    console.log('initial path length:' + path.length);\r\n    for (i = 0; i < path.length - 1; i++) {\r\n      totalDistance2 = totalDistance2 + google.maps.geometry.spherical.computeDistanceBetween(path[i], path[i + 1]);\r\n    }\r\n    console.log('distance iterating over path latlngs:' + totalDistance2);\r\n    console.log('distance using computelength(path):' + google.maps.geometry.spherical.computeLength(path));\r\n\r\n    var totalDistance = 0;\r\n    var epath = [];\r\n    var old_loc = results[0].location;\r\n    var bounds = new google.maps.LatLngBounds();\r\n    var new_loc;\r\n    console.log(\"results.length=\" + results.length);\r\n    for (i = 0; i < results.length; i++) {\r\n      new_loc = results[i].location;\r\n      epath.push(new_loc);\r\n      bounds.extend(new_loc);\r\n      var m = new google.maps.Marker({\r\n        map: map,\r\n        title: \"\" + i,\r\n        position: new_loc,\r\n        icon: {\r\n          url: \"https://maps.gstatic.com/intl/en_us/mapfiles/markers2/measle.png\",\r\n          size: new google.maps.Size(7, 7),\r\n          anchor: new google.maps.Point(3.5, 3.5)\r\n        }\r\n      });\r\n      totalDistance = totalDistance + google.maps.geometry.spherical.computeDistanceBetween(old_loc, new_loc);\r\n      old_loc = new_loc;\r\n    }\r\n    map.fitBounds(bounds);\r\n    var polyline = new google.maps.Polyline({\r\n      path: epath,\r\n      strokeColor: 'blue',\r\n      map: map,\r\n      strokeWeight: 1\r\n    });\r\n    console.log('distance iterating over elevation results:' + totalDistance);\r\n    console.log('distance using computelength(epath):' + google.maps.geometry.spherical.computeLength(epath));\r\n  }\r\n}```\n\r\n```\nhtml,\r\nbody,\r\n#map {\r\n  width: 100%;\r\n  height: 100%;\r\n  border: 1px solid black;\r\n}```\n\r\n```\n<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js\"></script>\r\n<script src=\"http://maps.googleapis.com/maps/api/js?libraries=geometry\"></script>\r\n<div id=\"map\"></div>```\n\r\n\r\n\r\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "haversine formula. Total distance\r\n                \r\nI have problem with use of haversine formula in my project.\nIt is calculating distance from starting point to my current point ok.  But when I go back to my starting point distance getting smaller.\nHow do I get total travelled distance? In case like I walk in circle or go back and forward?\n\n(edit)\n\nThat’s full code except XML.\nI have other issues with it buy I trying step by step. So I don’t want to ask too many questions.\nMy main problem is that’s this formula haversine formula. Counting distance from my current point to my starting point.\nI need real time total distance I travelled. \n\n```\nimport java.io.IOException;\nimport java.util.List;\nimport java.util.Locale;\n\nimport android.app.Activity;\nimport android.content.Context;\nimport android.location.Address;\nimport android.location.Criteria;\nimport android.location.Geocoder;\nimport android.location.Location;\nimport android.location.LocationListener;\nimport android.location.LocationManager;\nimport android.os.Bundle;\nimport android.widget.TextView;\npublic class MainActivity extends Activity {\n\n    String addressString = \"No address found\";\n    String latLongString = \"Your Current Position is: \";\n\n    double latitude = 10;\n    double longitude = 10;\n    double sLatitude;\n    double sLongitude;\n    double eLatitude;\n    double eLongitude;\n    double distance = 10;\n\n    double el1 = 0;\n    double el2 = 0;\n\n    double counter = 2;\n\n    TextView myLocationText;\n    TextView myLatitude;\n    TextView myLongitude;\n    TextView myAddress;\n    TextView startingLatitude;\n    TextView startingLongitude;\n    TextView endingLatitude;\n    TextView endingLongitude;\n    TextView myDistance;\n    TextView myLogOut;\n\n    @Override\n    public void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        LocationManager locationManager;\n        String context = Context.LOCATION_SERVICE;\n        locationManager = (LocationManager) getSystemService(context);\n\n        Criteria criteria = new Criteria();\n        criteria.setAccuracy(Criteria.ACCURACY_FINE);\n        criteria.setAltitudeRequired(false);\n        criteria.setBearingRequired(false);\n        criteria.setCostAllowed(true);\n        criteria.setPowerRequirement(Criteria.POWER_LOW);\n\n        String provider = locationManager.getBestProvider(criteria, true);\n        Location location = locationManager.getLastKnownLocation(provider);\n\n        myLatitude = (TextView) findViewById(R.id.Latitude);\n        myLongitude = (TextView) findViewById(R.id.Longitude);\n        myLocationText = (TextView) findViewById(R.id.myLocationText);\n        myAddress = (TextView) findViewById(R.id.Address);\n        startingLatitude = (TextView) findViewById(R.id.startingLatitude);\n        startingLongitude = (TextView) findViewById(R.id.startingLongitude);\n        endingLatitude = (TextView) findViewById(R.id.endingLatitude);\n        endingLongitude = (TextView) findViewById(R.id.endingLongitude);\n        myDistance = (TextView) findViewById(R.id.distance);\n        myLogOut = (TextView) findViewById(R.id.logOut);\n\n        if (location != null) {\n            sLatitude = location.getLatitude();\n            sLongitude = location.getLongitude();\n\n        }\n\n        updateWithNewLocation(location);\n\n        locationManager.requestLocationUpdates(provider, 1000, 2,\n                locationListener);\n\n    }\n\n    private final LocationListener locationListener = new LocationListener() {\n        @Override\n        public void onLocationChanged(Location location) {\n            updateWithNewLocation(location);\n        }\n\n        @Override\n        public void onProviderDisabled(String provider) {\n            updateWithNewLocation(null);\n        }\n\n        @Override\n        public void onProviderEnabled(String provider) {\n        }\n\n        @Override\n        public void onStatusChanged(String provider, int status, Bundle extras) {\n        }\n    };\n\n    private void updateWithNewLocation(Location location) {\n\n        if (location != null) {\n            latitude = location.getLatitude();\n            longitude = location.getLongitude();\n\n            eLatitude = location.getLatitude();\n            eLongitude = location.getLongitude();\n            distance = distance(sLatitude, eLatitude, sLongitude, eLongitude,\n                    el1, el2);\n\n            Geocoder gc = new Geocoder(this, Locale.getDefault());\n            try {\n\n                List<Address> addresses = gc.getFromLocation(latitude,\n                        longitude, 1);\n                StringBuilder sb = new StringBuilder();\n                if (addresses.size() > 0) {\n                    Address address = addresses.get(0);\n\n                    for (int i = 0; i < address.getMaxAddressLineIndex(); i++)\n                        sb.append(address.getAddressLine(i)).append(\"\\n\");\n                    sb.append(address.getLocality()).append(\"\\n\");\n                    sb.append(address.getPostalCode()).append(\"\\n\");\n                    sb.append(address.getCountryName());\n                }\n                addressString = sb.toString();\n            } catch (IOException e) {\n            }\n        } else {\n            latLongString = \"No location found\";\n        }\n\n        myLocationText.setText(latLongString);\n        myLatitude.setText(\"Latitude: \" + latitude);\n        myLongitude.setText(\"Longitude: \" + longitude);\n        myAddress.setText(addressString);\n        startingLatitude.setText(\"Starting latitude: \" + sLatitude);\n        startingLongitude.setText(\"Starting longitude: \" + sLongitude);\n        endingLatitude.setText(\"Last latitude: \" + eLatitude);\n        endingLongitude.setText(\"Last longitude: \" + eLongitude);\n        myDistance.setText(\"Your distance to starting point: \" + distance);\n\n    }\n\n    public static double distance(double lat1, double lat2, double lon1,\n            double lon2, double el1, double el2) {\n\n        final int R = 6371; // Radius of the earth\n\n        double latDistance = Math.toRadians(lat2 - lat1);\n        double lonDistance = Math.toRadians(lon2 - lon1);\n        double a = Math.sin(latDistance / 2) * Math.sin(latDistance / 2)\n                + Math.cos(Math.toRadians(lat1))\n                * Math.cos(Math.toRadians(lat2)) * Math.sin(lonDistance / 2)\n                * Math.sin(lonDistance / 2);\n        double c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a));\n        double distance = R * c * 1000; // convert to meters\n\n        double height = el1 - el2;\n\n        distance = Math.pow(distance, 2) + Math.pow(height, 2);\n        return Math.sqrt(distance);\n    }\n\n}\n```\n\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Optimizing travel double for loop using swift\r\n                \r\nI used minimum edit distance algorithm to find the bundle of the most similar strings in an array.\n\nSo, I have to travel double ```\nfor```\n loop to compare all element.\n\nIf the data is large enough, this algorithm is Inefficient.\n\nIs there a way to optimize?\n\n```\nlet data = [\n  \"10000\", // count\n  \"asdfqwerty\", \"asdfzxcvgh\", \"asdfpoiuyt\",\n  ...\n]\n\nfor i in 1..<data.count {\n  let string = data[i]\n  for j in (i + 1)..<data.count {\n    let newMin = string.minimumEditDistance(other: data[j])\n\n    if min >= newMin {\n      // some logic\n    }\n  }\n}\n```\n\n\n```\nextension String {\n  public func minimumEditDistance(other: String, `default`: Int = 10) -> Int {\n    let m = self.count\n    let n = other.count\n\n    if m == 0 || n == 0 {\n      return `default`\n    }\n\n    var matrix = [[Int]](repeating: [Int](repeating: 0, count: n + 1), count: m + 1)\n\n    // initialize matrix\n    for index in 1...m {\n      // the distance of any first string to an empty second string\n      matrix[index][0] = index\n    }\n\n    for index in 1...n {\n      // the distance of any second string to an empty first string\n      matrix[0][index] = index\n    }\n\n    // compute Levenshtein distance\n    for (i, selfChar) in self.enumerated() {\n      for (j, otherChar) in other.enumerated() {\n        if otherChar == selfChar {\n          // substitution of equal symbols with cost 0\n          matrix[i + 1][j + 1] = matrix[i][j]\n        } else {\n          // minimum of the cost of insertion, deletion, or substitution\n          // added to the already computed costs in the corresponding cells\n          matrix[i + 1][j + 1] = Swift.min(matrix[i][j] + 1, matrix[i + 1][j] + 1, matrix[i][j + 1] + 1)\n        }\n      }\n    }\n    return matrix[m][n]\n  }\n}\n\n```\n\n    ", "Answer": "\r\nYou can achieve desired behaviour by sorting your array using your ```\nminimumEditDistance```\n as a sorting function and then taking first or last element (depends on how you define sorting) and what you need - min or max. It will likely run in ```\nO(N*log(N))```\n time. Which is already better than exponential.  \n\nAs @Sultan mentioned, it will work not for all distances, as transitivity is applicable only to Metrics (functions that define a distance between each element of the set). You're using Levenstain distance as an editing distance algorithm, which is indeed a metric. The solution I mentioned should help to optimise in some circumstances.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Django multiple queries optimization\r\n                \r\nI have a Django view that return a map of the edit distance between thousands of strings. These strings are arguments of the ```\nMyModel```\n class. I calculate the distance in the ```\nmyView```\n function.\n\nI profiled this code and realized that the ```\nqueryset```\n inside the loop consumes a lot of time.\n\nHow could I optimize this?\n\n```\n# models.py\nclass MyModel(models.Model):\n    str1 = models.CharField(max_length=300)\n    str2 = models.CharField(max_length=300)\n\n# views.py\ndef compare(a, b):\n    return Levenshtein.distance(a, b) / max(len(a), len(b))\n\ndef myView(request):    \n    query_set = MyModel.objects.filter(....)\n    size = query_set.count()\n\n    arr = numpy.zeros(size ** 2).reshape(size, size)\n\n    for i in range(size):\n        m1 = query_set[i].str1\n        for j in range(size):\n            m2 = query_set[j].str1\n            arr[i][j] = compare(m1, m2)\n\n    json_out = json.dumps({'data': arr.tolist()})\n    return HttpResponse(json_out, content_type=\"application/json\")\n```\n\n\nEDIT\n\nI think the problem is related to the database access because I tried a similar approach, but using an external txt file to store the data and it was much faster:\n\n```\n# file.txt\n[{'par1': ....}, {'par1': ....}, ...]\n\n# views.py\ndef myView(request):\n    with open('file.txt', 'r') as out:\n        data = out.read()\n    size = len(data)\n\n    arr = numpy.zeros(size ** 2).reshape(size, size)\n\n    for i in range(size):\n        for j in range(size):\n            m1 = data[i]['par1']\n            m2 = data[j]['par1']\n            arr[i][j] = compare(m1, m2)\n\n    json_out = json.dumps({'data': arr.tolist()})\n    return HttpResponse(json_out, content_type=\"application/json\")\n```\n\n    ", "Answer": "\r\nHow many queries is myView actually doing? It should be doing 1 - or possibly 2 for count() and then the actual data. But I would start by verifying that. I use https://github.com/dobarkod/django-queryinspect but most folks use https://github.com/jazzband/django-debug-toolbar to find out how many queries are being done. \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "R Interclass distance matrix\r\n                \r\nThis question is sort of a follow-up to how to extract intragroup and intergroup distances from a distance matrix? in R. In that question, they first computed the distance matrix for all points, and then simply extracted the inter-class distance matrix. I have a situation where I'd like to bypass the initial computation and skip right to extraction, i.e. I want to directly compute the inter-class distance matrix. Drawing from the linked example, with tweaks, let's say I have some data in a dataframe called ```\ndf```\n:\n\n```\nvalues<-c(0.002,0.3,0.4,0.005,0.6,0.2,0.001,0.002,0.3,0.01)\nclass<-c(\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"B\",\"A\",\"B\",\"A\")\ndf<-data.frame(values, class)\n```\n\n\nWhat I'd like is a distance matrix:\n\n```\n    1    2    3    8   10\n4 .003 .295 .395 .003 .005\n5 .598 .300 .200 .598 .590\n6 .198 .100 .200 .198 .190\n7 .001 .299 .399 .001 .009\n9 .298 .000 .100 .298 .290\n```\n\n\nDoes there already exist in R an elegant and fast way to do this?\n\nEDIT After receiving a good solution for the 1D case above, I thought of a bonus question: what about a higher-dimensional case, say if instead ```\ndf```\n looks like this:\n\n```\nvalues1<-c(0.002,0.3,0.4,0.005,0.6,0.2,0.001,0.002,0.3,0.01)\nvalues2<-c(0.001,0.1,0.1,0.001,0.1,0.1,0.001,0.001,0.1,0.01)\nclass<-c(\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"B\",\"A\",\"B\",\"A\")\ndf<-data.frame(values1, values2, class)\n```\n\n\nAnd I'm interested in again getting a matrix of the Euclidean distance between points in class ```\nB```\n with points in class ```\nA```\n.\n    ", "Answer": "\r\nFor general ```\nn```\n-dimensional Euclidean distance, we can exploit the equation (not R, but algebra):\n\n```\nsquare_dist(b,a) = sum_i(b[i]*b[i]) + sum_i(a[i]*a[i]) - 2*inner_prod(b,a)\n```\n\n\nwhere the sums are over the dimensions of vectors ```\na```\n and ```\nb```\n for ```\ni=[1,n]```\n. Here, ```\na```\n and ```\nb```\n are one pair from ```\nA```\n and ```\nB```\n. The key here is that this equation can be written as a matrix equation for all pairs in ```\nA```\n and ```\nB```\n.\n\nIn code:\n\n```\n## First split the data with respect to the class\nn <- 2   ## the number of dimensions, for this example is 2\ntmp <- split(df[,1:n], df$class)\n\nd <- sqrt(matrix(rowSums(expand.grid(rowSums(tmp$B*tmp$B),rowSums(tmp$A*tmp$A))),\n                 nrow=nrow(tmp$B)) - \n          2. * as.matrix(tmp$B) %*% t(as.matrix(tmp$A)))\n```\n\n\nNotes:\n\n\nThe inner ```\nrowSums```\n compute ```\nsum_i(b[i]*b[i])```\n and ```\nsum_i(a[i]*a[i])```\n for each ```\nb```\n in ```\nB```\n and ```\na```\n in ```\nA```\n, respectively.\n```\nexpand.grid```\n then generates all pairs between ```\nB```\n and ```\nA```\n.\nThe outer ```\nrowSums```\n computes the ```\nsum_i(b[i]*b[i]) + sum_i(a[i]*a[i])```\n for all these pairs.\nThis result is then reshaped into a ```\nmatrix```\n. Note that the number of rows of this matrix is the number of points of class ```\nB```\n as you requested.\nThen subtract two times the inner product of all pairs. This inner product can be written as a matrix multiply ```\ntmp$B %*% t(tmp$A)```\n where I left out the coercion to matrix for clarity.\nFinally, take the square root.\n\n\nUsing this code with your data:\n\n```\nprint(d)\n##          1         2         3         8         10\n##4 0.0030000 0.3111688 0.4072174 0.0030000 0.01029563\n##5 0.6061394 0.3000000 0.2000000 0.6061394 0.59682493\n##6 0.2213707 0.1000000 0.2000000 0.2213707 0.21023796\n##7 0.0010000 0.3149635 0.4110985 0.0010000 0.01272792\n##9 0.3140143 0.0000000 0.1000000 0.3140143 0.30364453\n```\n\n\nNote that this code will work for any ```\nn > 1```\n. We can recover your previous 1-d result by setting ```\nn```\n to ```\n1```\n and not perform the inner ```\nrowSums```\n (because there is now only one column in ```\ntmp$A```\n and ```\ntmp$B```\n):\n\n```\nn <- 1   ## the number of dimensions, set this now to 1\ntmp <- split(df[,1:n], df$class)\n\nd <- sqrt(matrix(rowSums(expand.grid(tmp$B*tmp$B,tmp$A*tmp$A)),\n                 nrow=length(tmp$B)) - \n          2. * as.matrix(tmp$B) %*% t(as.matrix(tmp$A)))\nprint(d)\n##      [,1]  [,2]  [,3]  [,4]  [,5]\n##[1,] 0.003 0.295 0.395 0.003 0.005\n##[2,] 0.598 0.300 0.200 0.598 0.590\n##[3,] 0.198 0.100 0.200 0.198 0.190\n##[4,] 0.001 0.299 0.399 0.001 0.009\n##[5,] 0.298 0.000 0.100 0.298 0.290\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is there any implementation of this string matching method in python?\r\n                \r\nI am trying to work out which entries in my data store are near-duplicates using approximate string matching.\n\nIs there any implementation of the following approach in python, or do i need to try and roll my own?\n\nThanks :)\n\nfrom wikipedia:\n\n\n  ...\n  \n  A brute-force approach would be to\n  compute the edit distance to P for all\n  substrings of T, and then choose the\n  substring with the minimum distance.\n  However, this algorithm would have the\n  running time O(n3 m)\n  \n  A better solution[3][4], utilizing\n  dynamic programming, uses an\n  alternative formulation of the\n  problem: for each position j in the\n  text T and each position i in the\n  pattern P, compute the minimum edit\n  distance between the i first\n  characters of the pattern, Pi, and any\n  substring Tj',j of T that ends at\n  position j.\n\n\nWhat is the most efficient way to apply this to many strings?\n    ", "Answer": "\r\nYes. \n\n```\ngoogle(\"python levenshtein\")\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "levenshtein matrix cell calculation\r\n                \r\nI do not understand how the values in the levenshtein matrix is calculated According to this article. I do know how we arrive at the edit distance of 3. Could someone explain in lay man terms how we arrive at each value in each cell?\n\n\n    ", "Answer": "\r\nHi I just had a look at the link of the Wikipedia article you shared:\n\nThe way the matrix is built is described in \"Definition\".\nNow I will just translate that into what it means and what you need to do to built the matrix all by yourself:\n\nJust to be sure that no basic information is missing: i denotes the row number and j denotes the column number.\n\nSo lets start with the first definition line of the matrix:\nIt says that the matrix is max(i, j), if min(i,j) = 0\nThe condition will be fulfilled only for elements of the 0-th row and the 0-th column.  (Then min(0, j) is 0 and min(i, 0) is 0). So for the 0-th row and the 0-th column you enter the value of max(i,j), which corresponds to the row number for the 0-th column and the column number for the 0-th row.\nSo far so good:\n\n```\n    k i t t e n\n  0 1 2 3 4 5 6\ns 1\ni 2\nt 3\nt 4\ni 5\nn 6\ng 7\n```\n\n\nAll the other values are built as the minimum of one of these three values:\n\n```\nlev(i-1, j) + 1\nlev(i, j-1) + 1\nlev(i-1, j-1) + 1_(a_i != b_i)\n```\n\n\nWhere lev corresponds to the already existing levenshtein matrix elements.\nThe lev(i, j-1) is simply the matrix component to the left of the one, that we want to determine. lev(i-1, j) is the component above and lev(i-1, j-1) is the element left and above. Here, 1_(a_i != b_i) means, that if the letters on this space do not equal 1 is added, otherwise 0.\n\nIf we jump right into the matrix element (1, 1), wich corresponds to letters (s, k): We determine the 3 components:\n\n```\nlev(i-1, j) + 1 = 2     [1 + 1 = 2]\nlev(i, j-1) + 1 = 2     [1 + 1 = 2]\nlev(i-1, j-1) + 1 = 1   [0 + 1 = 1]  + 1 because k is clearly not s\n```\n\n\nNow, we take the minimum of these three values and we found the next entry of the Levenshtein matrix.\n\nDo this evaluation for each single element row OR columnwise and the result is the full Levenshtein matrix.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to calculate distance in km between two points using Geocoder\r\n                \r\nI'm trying to figure out a way to show calculated distance between two point on a map using Geocoder. I have start_address and destination_address in my table which I ask for in my view, my model looks like this:\n\n```\nclass Ride < ActiveRecord::Base\n    geocoded_by :start_address\n    geocoded_by :destination_address\n    reverse_geocoded_by :latitude, :longitude\n    after_validation :geocode\n\nend\n```\n\n\nHow can I get calculated distance between the two points and show in view. No need for longitude and latitude.\n\nEdit:\nThe default unit is 'miles'. To change default unit set it in your ActiveRecord model:\n\n```\nVenue.near([40.71, -100.23], 20, :units => :km)\n```\n\n\nor change it in:\n\n```\n# config/initializers/geocoder.rb\n# set default units to kilometers:\n:units => :km,\n```\n\n\nEdit:\nI've managed to resolve this like so:\n\n```\nride = Ride.new(params)\nstart_address_coordinates = Geocoder.coordinates(params[:start_address])\ndestination_coordinates = Geocoder.coordinates(params[:destination])\nride.distance = Geocoder::Calculations.distance_between(start_address_coordinates, destination_coordinates)\n```\n\n    ", "Answer": "\r\nIn ```\nGeocoder::Calculations```\n module, there is a method called ```\ndistance_between(lat1, lon1, lat2, lon2, options = {})```\n\nyou pass the longitude and latitude of two points and it gives you back the distance between these two points.\nFor further info please check this link out\nquoted from the```\nGem's docs```\n:\nlook up coordinates of some location (like searching Google Maps)\n```\nGeocoder.coordinates(\"25 Main St, Cooperstown, NY\")\n=> [42.700149, -74.922767]\n```\n\nSo you can use this above method to get the coordinates of a specific location entered by the user, then you can calculate the difference in distance between two points by the below method.\ndistance between Eiffel Tower and Empire State Building\n```\nGeocoder::Calculations.distance_between([47.858205,2.294359], [40.748433,-73.985655])\n=> 3619.77359999382 # in configured units (default miles)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Write out results of for-loop of distance measures in matrix form in R\r\n                \r\nSuppose I have something like the following vector:\n\n```\ntext <- as.character(c(\"string1\", \"str2ing\", \"3string\", \"stringFOUR\", \"5tring\", \"string6\", \"s7ring\", \"string8\", \"string9\", \"string10\"))\n```\n\n\nI want to execute a loop that does pair-wise comparisons of the edit distance of all possible combinations of these strings (ex: string 1 to string 2, string 1 to string 3, and so forth). The output should be in a matrix form with rows equal to number of strings and columns equal to number of strings.\n\nI have the following code below:\n\n```\n#Matrix of pair-wise combinations\nm <- expand.grid(text,text)\n\n#Define number of strings\nn <- c(1:10)\n\n#Begin loop; \"method='osa'\" in stringdist is default\nfor (i in 1:10) {\n  n[i] <- stringdist(m[i,1], m[i,2], method=\"osa\")\n  write.csv(data.frame(distance=n[i]),file=\"/File/Path/output.csv\",append=TRUE)\n  print(n[i])\n  flush.console()\n}\n```\n\n\nThe stringdist() function is from the stringdist{} package but the function is also bundled in the base utils package as adist()\n\nMy question is, why is my loop not writing the results as a matrix, and how do I stop the loop from overwriting each individual distance calculation (ie: save all results in matrix form)?\n    ", "Answer": "\r\nI would suggest using ```\nstringdistmatrix```\n instead of ```\nstringdist```\n\n(especially if you are using ```\nexpand.grid```\n)\n\n```\n res <- stringdistmatrix(text, text)\n dimnames(res) <- list(text, text)  \n write.csv(res, \"file.csv\")\n```\n\n\n\n\nAs for your concrete question:  \"My question is, why is my loop not writing the results as a matrix\"\nIt is not clear why you would expect the output to be a matrix?  You are calculating an element at a time, saving it to a vector and then writing that vector to disk.  \n\nAlso, you should be aware that the arugments of ```\nwrite.csv```\n are mostly useless (they are there, I believe, just to remind the user of what the defaults are).  Use ```\nwrite.table```\n instead\n\nIf you want to do this iteratively, I would do the following: \n\n```\n# Column names, outputted only one time\nwrite.table(rbind(names(data.frame(i=1, distance=n[1])))\n            ,file=\"~/Desktop/output.csv\",append=FALSE   # <~~ Don't append for first run.\n             , sep=\",\", col.names=FALSE, row.names=FALSE)\n\nfor (i in 1:10) {\n  n[[i]] <- stringdist(m[i,1], m[i,2], method=\"osa\")\n  write.table(data.frame(i=i, distance=n[i]),file=\"~/Desktop/output.csv\"\n              ,append=TRUE, sep=\",\", col.names=FALSE, row.names=FALSE)\n  print(n[[i]])\n  flush.console()\n}\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Manhattan distance is over estimating and making me crazy\r\n                \r\nI'm implementing a-star algorithm with Manhattan distance to solve the 8-puzzle (in C). It seems to work very well and passes a lot of unit tests but it fails to find the shortest path in one case (it finds 27 steps instead of 25).\n\nWhen I change the heuristic function to Hamming distance it finds in 25 steps.\nAlso finds in 25 steps when I make the Manhattan distance function to return a half of the actual cost.\n\nThat's why I believe the problem lies somewhere in Manhattan distance function and it is over estimating the cost (hence inadmissible). I thought maybe something else is going wrong in the C program so I wrote a little Python script to test and verify the output of the Manhattan distance function only and they both produce the exact same result.\n\nI'm really confused because the heuristic function seems to be the only point of failure and it seems to be correct at the same time.\n\n\n\nYou can try this solver and put the tile order like \"2,6,1,0,7,8,3,5,4\"\nChoose the algorithm Manhattan distance and it finds in 25 steps.\nNow change it to Manhattan distance + linear conflict and it finds 27 steps.\n\nBut my Manhattan distance (without linear conflict) finds in 27 steps.\n\nHere's my general algorithm:\n\n```\nmanhattan_distance = 0\niterate over all tiles\nif the tile is not the blank tile:\nfind the coordinates of this tile on the goal board\nmanhattan_distance += abs(x - goal_x) + abs(y - goal_y)\n```\n\n\nI think if there was something very badly wrong with some important part it wouldn't pass all 25+ previous tests so this might be some sort of edge case.\n\nHere's commented Manhattan distance function in C:\n\n```\nint ManhattanDistance(Puzzle p, State b){\n   State goal = getFinalState(p);\n   int size = getSize(b);\n   int distance = 0;\n   if (getSize(goal) == size){ // both states are the same size\n      int i, j;\n      for(i=0; i<size; i++){\n         for(j=0; j<size; j++){ // iterate over all tiles\n            int a = getStateValue(b, i, j); // what is the number on this tile?\n            if (a != 'B'){ // if it's not the blank tile\n               int final_cordinates[2];\n               getTileCoords(goal, a, final_cordinates); // find the coordinates on the other board\n               int final_i = final_cordinates[0];\n               int final_j = final_cordinates[1];\n               distance +=  abs(i - final_i) + abs(j - final_j);\n            }\n         }\n      }\n   }\n   return distance;\n}\n```\n\n\nPlease help me.\n\nEDIT: As discussed in comments, the code provided for opening nodes can be found here\n    ", "Answer": "\r\nThe problem seems to be not in your heuristic function, but in the algorithm itself. From your description of the problem, and the fact that it occures only on some specific cases, I believe it has to do with the re-opening of a closed vertice, once you find a better path to it.\n\nWhile reading the code you have provided [in comments], I think I understood where the problem lays, in line 20:\n\n```\nif(getG(current) + 1 < getG(children[i])){\n```\n\n\nThis is wrong! You are checking if ```\ng(current) + 1 < g(children[i])```\n, you actually want to check for: ```\nf(current) + 1 + h(children[i]) < g(children[i])```\n, since you want to check this value with the heuristic function of ```\nchildren[i]```\n, and not of ```\ncurrent```\n! \nNote that it is identical as to set ```\nf(children[i]) = min{f(children[i]),f(current)+1}```\n, and then adding ```\nh(children[i])```\n to get the ```\ng```\n value.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Proper way to manually install a python library [osx-mavericks]\r\n                \r\nI'm trying to install a Python library (https://github.com/haakondr/graph-edit-distance-python) on my system. The issue is that the distribution is lacking a setup.py file. In this kind of situations, what is the proper way to install a Python library on osx?\n    ", "Answer": "\r\nThis isn't actually just an issue on OSX. This would be true anywhere you wanted to use this \n\nThe short answer is you don't actually need to install it.\n\nYou can just treat it as a set of files. If you download the .zip file from GitHub or clone the repo --- either way --- you can then access it directly. \n\nNote that in doing that, you'll have to install all the dependencies and what not yourself. \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "string match by tokens - concept check\r\n                \r\nThis question is for a concept check. I have a string 000.00-010.0.0.0 that I'd like to find the closest match to from the list {000.00-012.0.0.0 and 000.00-008.0.0.0} (include with the edit measure a numerical distance measure) I'd like to take '012', '010' and '008' as tokens and measure the distance between these. \n\nThe standard approach to string match will look for a change in each char position, sum the changes and return a distance. A modified distance will also measure the ASCII distance between the CHARS - G is farther from E than D.\n\nTo measure that '012' is to '010' as '008' is, requires bundling three chars into a token. Can this token be easily measured for edit distance and distance? The problem seems more complicated by the removal of delimiters in the tree database.\n\nMy proposed solution I want a reality check on is to convert '012', '010', and '008' into single CHAR ASCII symbols, say ), *, and +, measure the char distance and string edit distance, then on print convert back into '012', '010', and '008'.\n\nSample string: ```\nMER99.C0.00M.14.006.00.060.350```\n\n\nAnd, there are wildcards:\n\n\n```\nMER99.*.006.00.060.350```\n\n```\nMER99.C0.00M.??.006.00.060.350```\n\n\n\nSince the strings are the same length (some need dummy char for length, '00M' is actually 'M') matching is with the Hamming distance.\n\nI do not need help with the match algorithm, the Hamming distance approach, wildcards, or the dummy char, I added this for context to the question. Right now, I treat the token as separate char and get good results, but know they are not as exact as could be if handled as a token. The limiting factor is probably the inconsistency within the coding schema.  But, I'd like to have that as the limit and not my algorithm.\n    ", "Answer": "\r\nYour strings contains alpha-numerical characters, ie base 36 number. Furthermore, these characters are grouped in 'tokens'. It cannot be stored in a ```\nchar```\n, but you can store it in an ```\nint```\n.\n\nInstead of storing ints in your tree, you can store a pair, where the char tells the type of the value:\n\n\n```\n0```\n for a numeric value\n```\n1```\n for ```\n*```\n\n```\n2```\n for ```\nxxxx?```\n (mask)\netc...\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Change css :onhover distance\r\n                \r\nI was wondering if there was a way to make css ```\n:onhover```\n occur sooner (i.e. I'm 10px away from a link, and ```\n:onhover```\n calls, while still 10px away).\n\nEdit: I'm trying to figure out a way to change the ```\n:onhover```\n distance for a scrollbar.\n\nEdit 2: Here's the code:\n\n```\nhtml {\n    overflow: auto;\n}\nbody {\n    position: absolute;\n    top: 20px;\n    left: 20px;\n    bottom: 20px;\n    right: 20px;\n    padding: 30px; \n    overflow-y: scroll;\n    overflow-x: hidden;\n}\n::-webkit-scrollbar {\n    width: 5px;\n}\n\n/* Track */\n::-webkit-scrollbar-track {\n    -webkit-border-radius: 10px;\n    border-radius: 10px;\n    border: none;\n}\n\n/* Handle */\n::-webkit-scrollbar-thumb{\n    -webkit-border-radius: 10px;\n    border-radius: 10px;\n    background: rgba(200,200,200,0.1);\n}\n::-webkit-scrollbar-thumb:hover {\n    -webkit-border-radius: 10px;\n    border-radius: 10px;\n    background: rgba(200,200,200,0.8); \n}\n```\n\n    ", "Answer": "\r\nAdd ```\nCSS border property```\n to the element which then will receive the hover event.\n\nReference 1: jsFiddle Proximity Detector\n\nEdit: I now see you've re-written a major edit to your Question.\n\nI've looked at your code and have come up with a method that you might like for Chrome's scrollbar!\n\nReference 2: jsFiddle Scrollbar Proximity Detector\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Yii2: Selecting distance and mapping to model\r\n                \r\nIn my ProjectController, i got the following function:\n\n```\npublic function actionFindNearest($latitude, $longitude, $amount){\n    $projects = Project::findNearest($latitude, $longitude, $amount);\n    $html = '';\n    foreach($projects as $project){\n        $html .= $this->renderPartial( '/project/preview', array('model'=>$project), true );\n    }\n    return $html;\n}\n```\n\n\nThe method in the model looks like that:\n\n```\npublic static function findNearest($latitute, $longitude, $amount){\n    $sql = 'SELECT SQRT(\n            POW(69.1 * (latitude - '.$latitute.'), 2) +\n            POW(69.1 * ('.$longitude.' - longitude) * COS(latitude / 57.3), 2)) AS distance, p.*\n            FROM project as p\n            ORDER BY distance LIMIT '.$amount;\n\n    $command = Yii::$app->db->createCommand($sql);\n    return $command->queryAll();\n}\n```\n\n\nWhat i get now is an array with 3 objects containing all model attributes plus the distance I want - perfect!\nIn the controller, I pass it to the renderPartial now:\n\n```\nforeach($projects as $project){\n        $html .= $this->renderPartial( '/project/preview', array('model'=>$project), true );\n    }\n```\n\n\nIn the preview.php template, the distance attribute is lost because now i only have the model object, where the distance isn't an official field.\n\nEdit: the model properties of the project class:\n\n```\n/**\n * This is the model class for table \"project\".\n *\n * @property integer $id\n * @property string $updated\n * @property string $name\n * @property string $description\n * @property string $teaserimage\n * @property string $goal\n * @property string $current\n * @property string $startdate\n * @property string $enddate\n * @property string $created\n * @property string $headerimage\n * @property integer $category_id\n * @property integer $address_id\n * @property integer $user_id\n * @property string $website_url\n *\n * @property Comment[] $comments\n * @property Donation[] $donations\n * @property Goodie[] $goodies\n * @property Address $address\n * @property Category $category\n * @property User $user\n */\n```\n\n\nHow could I achieve to use the distance in my template file? \n\nThank you for your suggestions!\n    ", "Answer": "\r\nYou need to add ```\npublic $distance```\n property into your project model, and then it will be populated automatically for each record\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Constructing a triangular distance matrix dataframe in pyspark?\r\n                \r\nI want to construct a distance matrix using values from a dataframe in pyspark. What I have right now is\n\n```\n+----+-------------+\n| id | list        |\n+----+-------------+\n| 1  | [a, b, ...] |\n+----+-------------+\n| 2  | [c, d, ...] |\n+----+-------------+\n| 3  | [e, f, ...] |\n+----+-------------+\n```\n\n\nI want to use my own distance function and do something like\n\n```\nfor i in range(len(ids)):\n    for j in range(i + 1, len(ids)):\n        dist = calculate_distance(features[i], features[j])\n        add_row_to_distance_df([ids[i], ids[j], dist])\n```\n\n\nEDIT: Expected output is\n\n```\n+-----+-----+-----------------------------+\n| id1 | id2 | dist                        |\n+-----+-----+-----------------------------+\n| 1   | 2   | d([a, b, ...], [c, d, ...]) |\n+-----+-----+-----------------------------+\n| 1   | 3   | d([a, b, ...], [e, f, ...]) |\n+-----+-----+-----------------------------+\n| 2   | 3   | d([c, d, ...], [e, f, ...]) |\n+-----+-----+-----------------------------+\n```\n\n\nHow do I go about doing this? \n    ", "Answer": "\r\nYou can use ```\ncartesian()```\n and ```\nfilter()```\n just the necessary triangle, e.g.:\n\n```\nIn []:\ndef calculate_distance(a, b):\n    return f'd({a}, {b})'  # Py 3.6\n\nrdd = sc.parallelize([(1, ['a', 'b', 'c']), (2, ['c', 'd', 'e']), (3, ['e', 'f', 'g'])])\n\n(rdd.cartesian(rdd)\n .filter(lambda x: x[0][0] < x[1][0])\n .map(lambda x: (x[0][0], x[1][0], calculate_distance(x[0][1], x[1][1])))\n .collect())\n\nOut[]:\n[(1, 2, \"d(['a', 'b', 'c'], ['c', 'd', 'e'])\"),\n (1, 3, \"d(['a', 'b', 'c'], ['e', 'f', 'g'])\"),\n (2, 3, \"d(['c', 'd', 'e'], ['e', 'f', 'g'])\")]\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Why does indexing a string inside of a recursive call yield a different result?\r\n                \r\nIn my naive implementation of edit-distance finder, I have to check whether the last characters of two strings match:\n```\nulong editDistance(const string a, const string b) {\n    if (a.length == 0)\n        return b.length;\n    if (b.length == 0)\n        return a.length;\n\n    const auto delt = a[$ - 1] == b[$ - 1] ? 0 : 1;\n\n    import std.algorithm : min;\n\n    return min(\n        editDistance(a[0 .. $ - 1], b[0 .. $ - 1]) + delt, \n        editDistance(a, b[0 .. $ - 1]) + 1, \n        editDistance(a[0 .. $ - 1], b) + 1\n    );\n}\n```\n\nThis yields the expected results but if I replace ```\ndelt```\n with its definition it always returns 1 on non-empty strings:\n```\nulong editDistance(const string a, const string b) {\n    if (a.length == 0)\n        return b.length;\n    if (b.length == 0)\n        return a.length;\n\n    //const auto delt = a[$ - 1] == b[$ - 1] ? 0 : 1;\n\n    import std.algorithm : min;\n\n    return min(\n        editDistance(a[0 .. $ - 1], b[0 .. $ - 1]) + a[$ - 1] == b[$ - 1] ? 0 : 1, //delt, \n        editDistance(a, b[0 .. $ - 1]) + 1, \n        editDistance(a[0 .. $ - 1], b) + 1\n    );\n}\n```\n\nWhy does this result change?\n    ", "Answer": "\r\n\nThe operators have different precedence from what you expect. In ```\nconst auto delt = a[$ - 1] == b[$ - 1] ? 0 : 1;```\n there is no ambiguity, but in ```\neditDistance(a[0 .. $ - 1], b[0 .. $ - 1]) + a[$ - 1] == b[$ - 1] ? 0 : 1```\n, there is (seemingly).\n\nSimplifying:\n\n```\nauto tmp = editDistance2(a[0..$-1], b[0..$-1]);\nreturn min(tmp + a[$-1] == b[$-1] ? 0 : 1),\n    //...\n);\n```\n\n\nThe interesting part here is parsed as ```\n(tmp + a[$-1]) == b[$-1] ? 0 : 1```\n, and ```\ntmp + a[$-1]```\n is not equal to ```\nb[$-1]```\n. The solution is to wrap things in parentheses:\n\n```\neditDistance(a[0 .. $ - 1], b[0 .. $ - 1]) + (a[$ - 1] == b[$ - 1] ? 0 : 1)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Issue in handling NaN for distance calculation?\r\n                \r\nI have a ```\nDataFrame```\n as following(for simplicity) with points as index column:\n\n```\n import numpy as np\nimport pandas as pd\na = {'a' : [0.6,0.7,0.4,np.NaN,0.5,0.4,0.5,np.NaN],'b':['cat','bat','cat','cat','bat',np.NaN,'bat',np.nan]}\ndf = pd.DataFrame(a,index=['x1','x2','x3','x4','x5','x6','x7','x8'])\ndf\n```\n\n\nSince it has ```\nNaN```\n, I wanted the column to treat as a Number and did the following :\n\n```\nfor col in df.select_dtypes(include=['object']):\n        s = pd.to_numeric(df[col], errors='coerce')\n        if s.notnull().any():\n            df[col] = s\n```\n\n\nAfter converting the column to numeric type, I wanted to calculate distance matrix as following:\n\n```\ndef distmetric(x,y):\n    numeric5=x.select_dtypes(include=[\"number\"])\n    others5=x.select_dtypes(exclude=[\"number\"])\n    numeric6=y.select_dtypes(include=[\"number\"])\n    others6=y.select_dtypes(exclude=[\"number\"])\n    numnp5=numeric5.values\n    catnp5=others5.values\n    numnp6=numeric6.values\n    catnp6=others6.values\n    result3=np.around((np.repeat(numnp5, len(numnp6),axis=0) - np.tile(numnp6,(len(numnp5),1)))**2,3)\n    catres3=~(np.equal((np.repeat(catnp5,len(catnp6),axis=0)),(np.tile(catnp6,(len(catnp5),1)))))\n    sumtogeth3=result3.sum(axis=1)\n    sumcattoget3=catres3.sum(axis=1)\n    sum_result3=sumtogeth3+sumcattoget3\n    final_result3=np.around(np.sqrt(sum_result3),3)\n    final_result20=np.reshape(final_result3, (len(x.index),len(y.index)))\n    return final_result20\n\nmetric=distmetric(df,df)\nprint(metric)\n```\n\n\nI got a distance matrix as following:\n\n```\n [[0.    1.005 0.2     nan 1.005 1.02  1.005   nan]\n [1.005 0.    1.044   nan 0.2   1.044 0.2     nan]\n [0.2   1.044 0.      nan 1.005 1.    1.005   nan]\n [  nan   nan   nan   nan   nan   nan   nan   nan]\n [1.005 0.2   1.005   nan 0.    1.005 0.      nan]\n [1.02  1.044 1.      nan 1.005 1.    1.005   nan]\n [1.005 0.2   1.005   nan 0.    1.005 0.      nan]\n [  nan   nan   nan   nan   nan   nan   nan   nan]]\n```\n\n\nI would like to get an output like:\n\n```\n            x1       x2       x3      x4      x5       x6       x7       x8\nx1         0.0      1.005    0.2     1.0     1.005    1.02     1.005   1.414\nx2         1.005    0.0     1.044   1.414    0.2      1.044    0.2     1.414\nx3         0.2      1.044    0.0     1.0     1.005    1.0      1.005   1.414\nx4         1.0      1.414    1.0     0.0     1.414    1.414    1.414    1.0\nx5         1.005    0.2     1.005   1.414    0.0      1.005    0.0     1.414\nx6         1.02     1.044    1.0    1.414    1.005    0.0      1.005    1.0\nx7         1.005    0.2     1.005   1.414    0.1      1.005    0.0     1.414\nx8         1.414    1.414   1.414    1.0     1.414     1.0     1.414    0.0\n```\n\n\nI wanted to calculate distance between two ```\nNaN```\n which should result as 0 and distance between ```\nNaN```\n to any number or any string should result 1. Is there any method or way of doing it?\n\nEDIT:\nI am calculating distance in the following form:\n\n```\nfor each row:\n     if col is numerical: \n         then calculate (x1 element)-(x2 element)**2 and return this value to squareresult\n     if col is categorical:\n         then compare x1 element and x2 element.\n         if they are equal then cateresult=0 \n         else cateresult=1\n     totaldistanceresultforrow=sqrt(squareresult+cateresult)\n```\n\n\nNote: ```\nNaN```\n-```\nNaN```\n=0 and ```\nNaN```\n-any Num or string=1 (here '-' is subtract)\n    ", "Answer": "\r\nThis helped me :\n\n```\nsquare_res = (df['a'].values - df['a'][:, None]) ** 2\nnumeric=pd.DataFrame(square_res)\nidx = numeric.isnull().all()\nalltrueindices=np.where(idx)\n\nfor index in alltrueindices:\n    numeric.loc[index, index] = 0\nnumeric = numeric.fillna(1)\ndf['b']=df['b'].replace(np.nan, '?')\ncat_res = (df['b'].values != df['b'][:, None])\nres = (numeric + cat_res) ** .5\n\nprint(res.round(3))\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "minimum distance from an array [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                This question is unlikely to help any future visitors; it is only relevant to a small geographic area, a specific moment in time,  or an extraordinarily narrow situation that is not generally applicable to the worldwide audience of the internet. For help making  this question more broadly applicable, visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI followed this method from my other post [distance between a point and a curve[(find the distance between a point and a curve python) but something is wrong.  The values aren't accurate.\n\nI plotted this same trajectory in Mathematica and checked a few distances and I have found distances as low as ```\n18000```\n where python is returning a minimum of ```\n209000```\n.\n\nWhat is going wrong in the code at the bottom?\n\nEDIT  There was an error in this code everything checks out now.  Thanks.      \n\n```\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nme = 5.974 * 10 ** (24)  #  mass of the earth                                     \nmm = 7.348 * 10 ** (22)  #  mass of the moon                                      \nG = 6.67259 * 10 ** (-20)  #  gravitational parameter                             \nre = 6378.0  #  radius of the earth in km                                         \nrm = 1737.0  #  radius of the moon in km                                          \nr12 = 384400.0  #  distance between the CoM of the earth and moon                 \nM = me + mm\n\npi1 = me / M\npi2 = mm / M\nmue = 398600.0  #  gravitational parameter of earth km^3/sec^2                    \nmum = G * mm  #  grav param of the moon                                           \nmu = mue + mum\nomega = np.sqrt(mu / r12 ** 3)\nnu = -129.21 * np.pi / 180  #  true anomaly angle in radian                       \n\nx = 327156.0 - 4671\n#  x location where the moon's SOI effects the spacecraft with the offset of the  \n#  Earth not being at (0,0) in the Earth-Moon system                              \ny = 33050.0   #  y location                                                       \n\nvbo = 10.85  #  velocity at burnout                                               \n\ngamma = 0 * np.pi / 180  #  angle in radians of the flight path                   \n\nvx = vbo * (np.sin(gamma) * np.cos(nu) - np.cos(gamma) * np.sin(nu))\n#  velocity of the bo in the x direction                                          \nvy = vbo * (np.sin(gamma) * np.sin(nu) + np.cos(gamma) * np.cos(nu))\n#  velocity of the bo in the y direction                                          \n\nxrel = (re + 300.0) * np.cos(nu) - pi2 * r12\n#  spacecraft x location relative to the earth         \nyrel = (re + 300.0) * np.sin(nu)\n\n#  r0 = [xrel, yrel, 0]                                                           \n#  v0 = [vx, vy, 0]                                                               \nu0 = [xrel, yrel, 0, vx, vy, 0]\n\n\ndef deriv(u, dt):\n    n1 = -((mue * (u[0] + pi2 * r12) / np.sqrt((u[0] + pi2 * r12) ** 2\n                                               + u[1] ** 2) ** 3)\n        - (mum * (u[0] - pi1 * r12) / np.sqrt((u[0] - pi1 * r12) ** 2\n                                              + u[1] ** 2) ** 3))\n    n2 = -((mue * u[1] / np.sqrt((u[0] + pi2 * r12) ** 2 + u[1] ** 2) ** 3)\n        - (mum * u[1] / np.sqrt((u[0] - pi1 * r12) ** 2 + u[1] ** 2) ** 3))\n    return [u[3],  #  dotu[0] = u[3]                                              \n            u[4],  #  dotu[1] = u[4]                                              \n            u[5],  #  dotu[2] = u[5]                                              \n            2 * omega * u[5] + omega ** 2 * u[0] + n1,  #  dotu[3] = that         \n            omega ** 2 * u[1] - 2 * omega * u[4] + n2,  #  dotu[4] = that         \n            0]  #  dotu[5] = 0                                                    \n\n\ndt = np.arange(0.0, 320000.0, 1)  #  200000 secs to run the simulation            \nu = odeint(deriv, u0, dt)\nx, y, z, x2, y2, z2 = u.T\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot(x, y, z)\nplt.show()\n\n\nmy_x, my_y, my_z = (384400,0,0)\n\ndelta_x = x - my_x\ndelta_y = y - my_y\ndelta_z = z - my_z\ndistance = np.array([np.sqrt(delta_x ** 2 + delta_y ** 2 +\n           delta_z ** 2)])\n\nprint(distance.min())\n```\n\n    ", "Answer": "\r\nCorrected code\n\n```\nimport numpy as np\nfrom scipy.integrate import odeint\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nme = 5.974 * 10 ** (24)  #  mass of the earth                                     \nmm = 7.348 * 10 ** (22)  #  mass of the moon                                      \nG = 6.67259 * 10 ** (-20)  #  gravitational parameter                             \nre = 6378.0  #  radius of the earth in km                                         \nrm = 1737.0  #  radius of the moon in km                                          \nr12 = 384400.0  #  distance between the CoM of the earth and moon                 \nM = me + mm\n\npi1 = me / M\npi2 = mm / M\nmue = 398600.0  #  gravitational parameter of earth km^3/sec^2                    \nmum = G * mm  #  grav param of the moon                                           \nmu = mue + mum\nomega = np.sqrt(mu / r12 ** 3)\nnu = -129.21 * np.pi / 180  #  true anomaly angle in radian                       \n\nx = 327156.0 - 4671\n#  x location where the moon's SOI effects the spacecraft with the offset of the  \n#  Earth not being at (0,0) in the Earth-Moon system                              \ny = 33050.0   #  y location                                                       \n\nvbo = 10.85  #  velocity at burnout                                               \n\ngamma = 0 * np.pi / 180  #  angle in radians of the flight path                   \n\nvx = vbo * (np.sin(gamma) * np.cos(nu) - np.cos(gamma) * np.sin(nu))\n#  velocity of the bo in the x direction                                          \nvy = vbo * (np.sin(gamma) * np.sin(nu) + np.cos(gamma) * np.cos(nu))\n#  velocity of the bo in the y direction                                          \n\nxrel = (re + 300.0) * np.cos(nu) - pi2 * r12\n#  spacecraft x location relative to the earth         \nyrel = (re + 300.0) * np.sin(nu)\n\n#  r0 = [xrel, yrel, 0]                                                           \n#  v0 = [vx, vy, 0]                                                               \nu0 = [xrel, yrel, 0, vx, vy, 0]\n\n\ndef deriv(u, dt):\n    n1 = -((mue * (u[0] + pi2 * r12) / np.sqrt((u[0] + pi2 * r12) ** 2\n                                               + u[1] ** 2) ** 3)\n        - (mum * (u[0] - pi1 * r12) / np.sqrt((u[0] - pi1 * r12) ** 2\n                                              + u[1] ** 2) ** 3))\n    n2 = -((mue * u[1] / np.sqrt((u[0] + pi2 * r12) ** 2 + u[1] ** 2) ** 3)\n        - (mum * u[1] / np.sqrt((u[0] - pi1 * r12) ** 2 + u[1] ** 2) ** 3))\n    return [u[3],  #  dotu[0] = u[3]                                              \n            u[4],  #  dotu[1] = u[4]                                              \n            u[5],  #  dotu[2] = u[5]                                              \n            2 * omega * u[4] + omega ** 2 * u[0] + n1,  #  dotu[3] = that         \n            omega ** 2 * u[1] - 2 * omega * u[3] + n2,  #  dotu[4] = that         \n            0]  #  dotu[5] = 0       \n\n\ndt = np.arange(0.0, 320000.0, 1)  #  200000 secs to run the simulation            \nu = odeint(deriv, u0, dt)\nx, y, z, x2, y2, z2 = u.T\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot(x, y, z)\nplt.show()\n\n\nmy_x, my_y, my_z = (384400,0,0)\n\ndelta_x = x - my_x\ndelta_y = y - my_y\ndelta_z = z - my_z\ndistance = np.array([np.sqrt(delta_x ** 2 + delta_y ** 2 +\n           delta_z ** 2)])\n\nprint(distance.min())\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to develop a spell checker and suggester [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\n\n  Possible Duplicate:\n  What algorithm gives suggestions in a spell checker?  \n\n\n\n\nI need to develop a Bengali spell checker and correct spell suggester. I had developed one using Edit Distance Algorithm, which does not give good prediction. Can anyone guide me to some better approach?\n    ", "Answer": "\r\nPeter Norvig wrote a popular article on how to build a statistical spelling corrector.  If you can find a bunch of probably-correct Bengali text, the approach might work for you.  The examples are in Python, but the approach is not language specific.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Image Thresholding with Distance measure\r\n                \r\nI'm new to OpenCV, and I'm currently wondering if there's a way of image thresholding or clustering to be applied to pixel distance (euclidean distance) to figure out if an object exists in an image.\n\nSay, I have 3 images, one with the main object, the other one with a arbitrary background with the main object, and finally one with the arbitrary background without the object. \n\nI managed to calculate the distance between corner points using ```\ncv2.goodFeaturesToTrack()```\n\n\nBut is there an image thresholding method that I can use for distance?\n\nI have read many thresholding methods on the net, but they seem to apply to only grayscale images. The ones like ```\ncv2.threshold```\n and ```\ncv2.adaptiveThreshold```\n\n\nWhat other image thresholding methods can I use for distance? \n\n\n\nEDIT\n\nThe below are three images:\n\n\n\nAs you can see, the Rubik's Cube is the main object benchmark. I managed to get the corners as well as measure the distance between corners using Euclidean's Distance formula. \n\nWhat thresholding/clustering technique can I consider to apply to the distance value to determine if the object exists in the images?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Apache solr fuzzy search with distance parameter as 2\r\n                \r\nEnviornment- java version \"11.0.12\" 2021-07-20 LTS, solr-8.9.0\nI have loaded a csv file in solr. csv file has a field 'name'. Type of 'Name' column in solr is defined as 'text_general'.\nI understand that to perform a fuzzy search, tilde ~ symbol at the end of a single-word term is used. Default value of distance parameter is 2.\nI have used following fuzzy-search query\n```\nhttp://localhost:8983/solr/startsolr/select?indent=on&wt=json&q=(Name:'Ellyse~') AND (Name:'Perry~')&sort=field(Name) asc\n```\n\nAbove fuzzy search query is resulting following name as 'Ellysea Perry', 'Ellys Perry'\nBut why above query is not giving document having follwoing name 'Elly Perry' (as default distance parameter is 2 and 2 characters (se) are not present.)\nStrings having editDistance as '2' , should come in output(Eg. 'Elly Perry').\nI understand that \"with max edit distance 2 i can have up to 2 insertions, deletions or substitutions.\"\n```\nName available in loaded data - 'Elly Perry'\nInput query parameter - (Name:'Ellyse~') AND (Name:'Perry~')\n```\n\nSince after deleting 2 characters from name 'Ellyse', It becomes 'Elly'. so it should result in output. Could someone help me find the missing piece?\nhttps://en.wikipedia.org/wiki/Levenshtein_distance\nI expect the following row to match:\n```\n'Ellysea Perry',\n'Ellys Perry', \n'Elly Perry'\n```\n\nBut only get following two\n```\n'Ellysea Perry',\n'Ellys Perry'\n```\n\n'Name' field is configured in managed-schema as follows-\n```\n<field name=\"Name\" type=\"text_general\" multiValued=\"false\" indexed=\"true\" stored=\"true\" required=\"true\"/>\n```\n\nFieldType 'text_general' description is as follows-\n```\n<fieldType name=\"text_general\" class=\"solr.TextField\" positionIncrementGap=\"100\" multiValued=\"true\">\n      <analyzer type=\"index\">\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n      </analyzer>\n      <analyzer type=\"query\">\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n```\n\nAs per documentation ===> \"To perform a fuzzy search, use the tilde ~ symbol at the end of a single-word term.\".\nReferences - https://solr.apache.org/guide/8_5/the-standard-query-parser.html#TheStandardQueryParser-FuzzySearches\ntry to be more clear, i'm in lack of ideas in this problem, even it sounds like a classic :I have spend hours trying to play around with this but have got nowhere.\nI have resolved the error, Now it is working on edit-distance:2\nModified query is as follows :-\n```\nhttp://localhost:8983/solr/startsolr/select?indent=on&wt=json&q=(Name:Ellyse~) AND (Name:Perry~)&sort=field(Name) asc\n```\n\nBut further - I have indexed 16 milion records in solr, but fuzzy search is not working for a specific token having 40K records.\nIn rest of the cases its working.\nShould i have to configure some parameters in solr-config.xml file?\n    ", "Answer": "", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Use Absolute Pearson Correlation as Distance in K-Means Algorithm (MATLAB)\r\n                \r\nI need to do some clustering using a correlation distance but instead of using the built-in 'distance' 'correlation' which is defined as d=1-r I need the absolute Pearson distance. In my application anti-correlated data should get the same cluster ID. And now when using the kmeans() function I'm getting centroids that are highly anticorrelated which I would like to avoid by combining them. Now, I'm not that fluent in matlab yet and have some problems reading the kmeans function. Would it be possible to edit it for my purpose?\nExample: \nRow 1 and 2 should get the same cluster ID  when using the correlation distance as metrics.\nI did some attempts to edit the built-in matlab function ( open kmeans- >line 775)\nbut what's weird - when I change the distance function I'm getting a valid distance matrix but wrong cluster indexes, can't find the reason for it.\nWould love to get some tips! all best!\n    ", "Answer": "\r\nThis is a good example of why you should not use k-means with other distance functions.\n\nk-means does not minimize distances. It minimizes the sum of squared 1-dimensional deviations (SSQ).\n\nWhich is mathematically equivalent to squared Euclidean distance, so it does minimize Euclidean distances, as a mathematical side effect. It does not minimize arbitrary other distances, which are not equivalent to variance minimization.\n\nIn your case, it's pretty nice to see why it fails; I have to remember this as a demo case.\n\nAs you may know, k-means (Lloyds, that is) consists of two steps: assign by minimum squared deviation and then recompute the means.\n\nNow the problem is, recomputing the mean is not consistent with absolute pearson correlation.\n\nLet's take two of your vectors, which are -1 correlated:\n\n```\n+1 +2 +3 +4 +5\n-1 -2 -3 -4 -5\n```\n\n\nand compute the mean:\n\n```\n 0  0  0  0  0\n```\n\n\nBoom. They are not at all correlated to their mean. In fact, Pearson correlation is not even well-defined for this vector anymore, because it has zero variance...\n\nWhy does this happen? Because you misinterpreted k-means as distance based. It's actually as much arithmetic mean based. The arithmetic mean is a least-squares (!!) estimator - it minimizes the sum of squared deviations. And that is why squared Euclidean distance works: it optimizes the same quantity as recomputing the mean. Optimizing the same objective in both steps makes the algorithm converge.\n\nSee also this counter-example for Earth-movers distance, where the mean step of k-means yields suboptimal results (although probably not as bad as with absolute pearson)\n\nInstead of using k-means, consider using k-medoids aka PAM, which does work for arbitrary distances. Or one of the many other clustering algorithms, including DBSCAN and OPTICS.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Python Hamming distance rewrite countless for cycles into recursion\r\n                \r\nI have created a code generating strings which have hamming distance n from given binary string. Though I'm not able to rewrite this in a simple recursive function. There are several sequences (edit: actually only one, the length change) in the for loops logic but I don't know how to write it into the recursive way (the input for the function is string and distance (int), but in my code the distance is represented by the count of nested for cycles. Could you please help me?\n\n(e.g. for string ```\n'00100'```\n and distance 4, code returns ```\n['11010', '11001', '11111', '10011', '01011']```\n,\nfor string ```\n'00100'```\n and distance 3, code returns ```\n['11000', '11110', '11101', '10010', '10001', '10111', '01010', '01001', '01111', '00011']```\n)\n\n```\ndef change(string, i):\n    if string[i] == '1':\n        return string[:i] + '0' + string[i+1:]\n    else: return string[:i] + '1' + string[i+1:] #'0' on input\n\n\ndef hamming_distance(number):\n\n    array = []\n    for i in range(len(number)-3): #change first bit\n        a = number\n        a = change(a, i) #change bit on index i\n        for j in range(i+1, len(number)-2): #change second bit\n            b = a\n            b = change(b, j)\n            for k in range(j+1, len(number)-1): #change third bit\n                c = b\n                c = change(c, k)\n                for l in range(k+1, len(number)): #change fourth bit\n                    d = c\n                    d = change(d, l)\n                    array.append(d)\n    return array\n\nprint(hamming_distance('00100'))\n```\n\n\nThank you!\n    ", "Answer": "\r\nVery briefly, you have three base cases:\n\n```\nlen(string) == 0:    # return; you've made all the needed changes\ndist == 0            # return; no more changes to make\nlen(string) == dist  # change all bits and return (no choice remaining)\n```\n\n\n... and two recursion cases; with and without the change:\n\n```\nham1 = [str(1-int(string[0])) + alter \n            for alter in change(string[1:], dist-1) ]\nham2 = [str[0] + alter for alter in change(string[1:], dist) ]\n```\n\n\nFrom each call, you return a list of strings that are ```\ndist```\n from the input ```\nstring```\n.  On each return, you have to append the initial character to each item in that list.\n\nIs that clear?\n\nCLARIFICATION\n\nThe above approach also generates only those that change the string.  \"Without\" the change refers to only the first character.  For instance, given input ```\nstring=\"000\", dist=2```\n, the algorithm will carry out two operations:\n\n```\n'1' + change(\"00\", 2-1)  # for each returned string, \"10\" and \"01\"\n'0' + change(\"00\", 2)    # for the only returned string, \"11\"\n```\n\n\nThose two ```\nham```\n lines go in the recursion part of your routine.  Are you familiar with the structure of such a function?  It consists of base cases and recursion cases.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How compute lucene FuzzyQuery on top GraphDB lucene index?\r\n                \r\nGraphDB supports FTS Lucene plugin to build RDF 'molecule' to index texts efficiently. However, when there is a typo (missspell) in the word your are searching, Lucene would not retrieve a result. I wonder if it is possible to implement a FuzzyQuery based on the Damerau-Levenshtein algorithm on top the Lucene Index in GraphDB for FTS. That way even if the word is not correctly spell you can get a list of more 'closed' words based on an edit distance similarity.   \n\nThis is the index I have created for indexing labels of NounSynset in WordNet RDF.  \n\n```\nPREFIX wn20schema: <http://www.w3.org/2006/03/wn/wn20/schema/>\nINSERT DATA {\n    luc:index luc:setParam \"uris\" .\n    luc:include luc:setParam \"literals\" .\n    luc:moleculeSize luc:setParam \"1\" .\n    luc:includePredicates luc:setParam \"http://www.w3.org/2000/01/rdf-schema#label\" .\n    luc:includeEntities luc:setParam wn20schema:NounSynset.\n    luc:nounIndex luc:createIndex \"true\".\n}\n```\n\n\nWhen running the query\n\n```\nselect * where {\n    {?id luc:nounIndex \"credict\"}\n    ?id luc:score ?score.  \n}\n```\n\n\nThe result is empty and I would like to get at least the word \"credit\" as the edit distance is 1.\n\nThank you!!!\n    ", "Answer": "\r\nIf you use the ```\n~```\n it should give you a fuzzy match.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Levenshtein implementation in Clojure with memoization\r\n                \r\nThis is a minimal Levenshtein (edit distance) implementation using Clojure with recursion:\n\n```\n(defn levenshtein [s1, i1, s2, i2]\n  (cond\n    (= 0 i1) i2\n    (= 0 i2) i1\n    :else\n    (min (+ (levenshtein s1 (- i1 1) s2 i2) 1)\n         (+ (levenshtein s1 i1 s2 (- i2 1)) 1)\n         (+ (levenshtein s1 (- i1 1) s2 (- i2 1)) (if (= (subs s1 i1 (+ i1 1)) (subs s2 i2 (+ i2 1))) 0 1))\n         )\n    )\n  )\n\n(defn levenshteinSimple [s1, s2]\n  (levenshtein s1, (- (count s1) 1), s2, (- (count s2) 1)))\n```\n\n\nWhich can be used like this:\n\n```\n(println (levenshteinSimple \"hello\", \"hilloo\"))\n(println (levenshteinSimple \"hello\", \"hilloo\"))\n(println (levenshteinSimple \"bananas\", \"bananas\"))\n(println (levenshteinSimple \"ananas\", \"bananas\"))\n```\n\n\nAnd prints this:\n\n```\n2\n2\n0\n1\n```\n\n\nHow can you add memoize to this implementation to improve performance?\n\nPlease note: I am a Clojure beginner. These are my first lines in Clojure\n    ", "Answer": "\r\nThe simplest way is to just make use of the ```\nmemoize```\n function. It takes a function, and returns a memoized function:\n\n```\n(let [mem-lev (memoize levenshteinSimple]\n  (println (mem-lev \"hello\", \"hilloo\"))\n  (println (mem-lev \"hello\", \"hilloo\"))\n  (println (mem-lev \"bananas\", \"bananas\"))\n  (println (mem-lev \"ananas\", \"bananas\")))\n```\n\n\n```\nmem-lev```\n will memorize every argument you give it and the result of what your function returns, and will return the cached result if it's already seen the arguments that you gave it. \n\nNote that this will not cause the recursive calls to become memoized, but it's unlikely that any recursive calls would benefit from memoization anyways.\n\nThis will also not cause your original function to become memoized. In this example, only ```\nmem-lev```\n will be memoized. If you really wanted to have your global function memoized, you could change your definition to something like:\n\n```\n(def levenshteinSimple\n  (memoize\n    (fn [s1, s2]\n      ...\n```\n\n\nBut I wouldn't recommend doing this. This causes the function itself to hold state, which isn't ideal. It'll also hold onto that state for the length of the program, which could cause memory issues if abused.\n\n(As a great exercise, try writing your own version of ```\nmemoize```\n. I learned a lot by doing that). \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "package for geocoding (street location!) and next calculation of distance\r\n                \r\nEDIT: And package for not geographical distance between two lan-long points, but distance by car, or foot (like this http://code.google.com/intl/sk/apis/maps/documentation/directions/) Travel mode - driving, walking. No map, just distance. Thanks for any idea.\n\n\n\nHello. I search package in R, what gives me longtitude and latitude for data frame with 10000 locations like this \"STREET, town, Europe\".\n\nAnd then package, what gives me distance per haversine formula for those locations. But mainly package for lat-long.. Distance should be easy.\n\nI searched a lot but found nothing useful for me. Thanks for help! I hope that R knows it :)\n    ", "Answer": "\r\nPackage ```\ndismo```\n has address level ```\ngeocode()```\n, though installation can be difficult on some platforms. \n\nPackage ```\nsp```\n has WGS84 ellipsoidal distance in ```\nspDistsN1()```\n. \n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Set the last letters of a string equal PostgreSQL\r\n                \r\nI have a column 'names' and 'ids' in table \"OG\" and want to find those names where the last letter is different and the total edit distance is two. So far I have:\n\n```\nSELECT\nz1.names as names1, z2.names as names2, z1.ids, z2.ids\nFROM (SELECT t.names, SUBSTRING(t.names for Length(t.names-1) AS newnames\nfrom \"OG\" t) z1, (SELECT r.names, SUBSTRING(r.names for Length(r.names-1) AS\nnewnames1 FROM \"OG\" r) z2\nWHERE levenshtein(z1.newnames, z2.newnames1) = 2 AND z1.id != z2.id\n```\n\n\nUnfortunetly, this doesn't ensure the last letters are different. Any ideas for a fix?\n    ", "Answer": "\r\nCheck the last characters as well:\n\n```\nWHERE levenshtein(z1.newnames, z2.newnames1) = 2 AND z1.id != z2.id\nAND substring(z1.names,Length(z1.names)) <> substring(z2.names,Length(z2.names))\n```\n\n\nNote that using ```\nSUBSTRING(t.names for length(t.names)-1)```\n in your query will fail when the string is empty (not null)\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "How to calculate distance between two points using geosparql\r\n                \r\nI'm trying to calculate the distance between two points using ```\ngeosparql```\n. I have objects like the following image (same properties, different values): \n\n\n\nAnd I'm executing that query in ```\nsparql```\n:\n\n```\nPREFIX geos: <http://www.opengis.net/ont/geosparql#>\nPREFIX geosf: <http://www.opengis.net/def/function/geosparql/>\nPREFIX : <http://www.semanticweb.org/frubi/ontologies/2017/10/puntsWIFI#>\n\nSELECT ?distance\nWHERE {\n    ?wifipoint1 :hasGeometry ?geo1 .\n    ?geo1 geos:asWKT ?wpoint1 .\n    FILTER sameterm(?wifipoint1, <http://www.semanticweb.org/frubi/ontologies/2017/10/puntsWIFI#NYWifiFree103>) \n    ?wifipoint2 :hasGeometry ?geo2 .\n    ?geo2 geos:asWKT ?wpoint2 .\n    FILTER sameterm(?wifipoint2, <http://www.semanticweb.org/frubi/ontologies/2017/10/puntsWIFI#NYWifiFree105>) .\n    ?distance geosf:distance(?wpoint1 ?wpoint2 <http://qudt.org/vocab/unit#Kilometer>)\n}\n```\n\n\nWithout adding the distance, I'm able to get the following result: \n\n\n\nBut at the moment I add the distance I get empty rows. Any idea?\n\nNotes: \n\n\nI need to calculate the distance between two wifipoints (NYWifiFree103 and NYWifiFree105) which have each one a point. \nI'm executing that queries in stardog.\n\n\n** EDIT **\n\nI simplified the query:\n\n```\nPREFIX geos: <http://www.opengis.net/ont/geosparql#>\nPREFIX geof: <http://www.opengis.net/def/function/geosparql/>\nPREFIX : <http://www.semanticweb.org/frubi/ontologies/2017/10/puntsWIFI#>\n\nSELECT (geof:distance(?wpoint1, ?wpoint2, <http://qudt.org/vocab/unit#Kilometer>) as ?distance)\nWHERE {\n    ?wifipoint1 :hasGeometry ?geo1 .\n    ?geo1 geos:asWKT ?wpoint1 .\n    FILTER sameterm(?wifipoint1, <http://www.semanticweb.org/frubi/ontologies/2017/10/puntsWIFI#NYWifiFree103>) .\n    ?wifipoint2 :hasGeometry ?geo2 .\n    ?geo2 geos:asWKT ?wpoint2 .\n    FILTER sameterm(?wifipoint2, <http://www.semanticweb.org/frubi/ontologies/2017/10/puntsWIFI#NYWifiFree105>)\n}\n```\n\n\nWhen I set in ```\ngeof:distance```\n two harcoded ```\nwktLiteral```\n returns me correct distance, but using Points does not return nothing.\n    ", "Answer": "\r\nThe ```\ngeof:distance```\n function accepts Geometries as its first two parameters. So with your simplified query, using ```\ngeof:distance(?geo1, ?geo2, unit:Kilometer)```\n should give you the result you desire. The Stardog Blog has a geospatial primer post that you may find useful.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Minimize distance between two latitude-longitude points?\r\n                \r\nI'm looking for a way to obtain a new columns reporting the minimale distance (km) under condition.\n\nIt will be more clear with an example : \n\n```\nSer_Numb        LAT      LONG   VALUE   MIN\n       1  74.166061 30.512811       1\n       2  72.249672 33.427724       1\n       3  67.499828 37.937264       0\n       4  84.253715 69.328767       1\n       5  72.104828 33.823462       0\n       6  63.989462 51.918173       0\n       7  80.209112 33.530778       0\n       8  68.954132 35.981256       1\n       9  83.378214 40.619652       1\n       10 68.778571 6.607066        0\n```\n\n\nSo when ```\nvalue=0```\n, I have to find the closest other city (latitude/longitude) to compute the distance to this city who presents a ```\nVALUE=1```\n.\n\nWith this stack we can have the formula, but how can I adapt it to take the minimal distance ? \n\n```\nfrom math import radians, cos, sin, asin, sqrt\ndef haversine(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    \"\"\"\n    # convert decimal degrees to radians \n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    # haversine formula \n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * asin(sqrt(a)) \n    # Radius of earth in kilometers is 6371\n    km = 6371* c\n    return km\n```\n\n\nEDIT\nHere is what I try:\n\n```\ndf['dist_VALUE']=0\n\nfor i in range(len(df[df['VALUE']<1])):\n    for j in range(len(df[df['VALUE']>0])):\n        (df[df['VALUE']<1].reset_index(drop=True).loc[i,'dist_VALUE'] =\n         min(haversine(df[df['VALUE']<1].reset_index(drop=True).loc[I,'LONG'], \n         df[df['VALUE']<1].reset_index(drop=True).loc[i,'LAT'],\n         df[df['VALUE']>0].reset_index(drop=True).loc[j,'LONG'], \n         df[df['VALUE']>0].reset_index(drop=True).loc[j,'LAT'])))\n```\n\n\n```\nVALUE```\n is integer and ```\nLAT```\n or ```\nLONG```\n are float. \n    ", "Answer": "\r\nMaybe this can help you:\n\n```\nimport pandas as pd\n\ndf = pd.DataFrame(\n    data=[\n        [74.166061, 30.512811, 1],\n        [72.249672, 33.427724, 1],\n        [67.499828, 37.937264, 0],\n        [84.253715, 69.328767, 1],\n        [72.104828, 33.823462, 0],\n        [63.989462, 51.918173, 0],\n        [80.209112, 33.530778, 0],\n        [68.954132, 35.981256, 1],\n        [83.378214, 40.619652, 1],\n        [68.778571,  6.607066, 0],\n    ],\n    columns=['lat', 'long', 'val'])\ndf['min'] = 0\nprint(df)\n# print(df.shape)\n# print(df.index)\n# print(df.columns)\n\ndestination_cities = [\n    {\n        'i': i,\n        'lat': row['lat'],\n        'long': row['long'],\n    }\n    for i, row in df.iterrows()\n    if row['val'] == 1]\nprint('destination_cities')\nprint(destination_cities)\n\nfor i in df.index:\n    row = df.iloc[i, :]\n    # print(type(row))\n    # print(row)\n\n    if row['val'] == 0:\n        target_distances = [\n            {\n                'destination_i': i,\n                'distance': haversine(\n                    lon1=row['long'],\n                    lat1=row['lat'],\n                    lon2=destination['long'],\n                    lat2=destination['lat']),\n            }\n            for destination in destination_cities]\n        elem = min(target_distances, key=lambda x: x['distance'])\n        row = df.loc[i, 'min'] = elem['distance']\n\nprint(df)\n```\n\n\n\n\nAnother approach could be to pre-compute the shortest distance for each city and the use ```\ndf.apply()```\n to assign the values; maybe this is a little bit faster for you:\n\n```\ndf = pd.DataFrame(\n    data=[\n        [ 1, 74.166061, 30.512811, 1],\n        [ 2, 72.249672, 33.427724, 1],\n        [ 3, 67.499828, 37.937264, 0],\n        [ 4, 84.253715, 69.328767, 1],\n        [ 5, 72.104828, 33.823462, 0],\n        [ 6, 63.989462, 51.918173, 0],\n        [ 7, 80.209112, 33.530778, 0],\n        [ 8, 68.954132, 35.981256, 1],\n        [ 9, 83.378214, 40.619652, 1],\n        [10, 68.778571,  6.607066, 0],\n    ],\n    columns=['i', 'lat', 'long', 'val'])\n\n# precompute closest distance for each city with val=0 to all cities with val=1\ndistances = {}\nfor _, row_orig in df.iterrows():\n    if row_orig['val'] == 0:\n        distances[row_orig['i']] = min(\n            haversine(\n                lon1=row_orig['long'],\n                lat1=row_orig['lat'],\n                lon2=row_dest['long'],\n                lat2=row_dest['lat'])\n            for _, row_dest in df.iterrows()\n            if row_dest['val'] == 1])\n\ndf['min'] = df.apply(lambda row: distances.get(row['i'], 0), axis=1)\nprint(df)\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "calculating distance in consecutive records\r\n                \r\nI'm working on an application which keeps track of refuels.\nEvery record stores data about a visit at the gasstation.\nI've decided to store as little information in the database as possible and calculate things in the controller.\n\nThis is my model:\n\n```\npublic class Refuel\n{\n    #region Properties\n\n        public int id { get; set; }\n        public int mileage { get; set; }\n        public DateTime date { get; set; }\n        public Nullable<double> volume { get; set; }\n        public Nullable<double> price { get; set; }\n\n        public Nullable<int> imageId { get; set; }\n        public Nullable<int> gasstationId { get; set; }\n\n        public virtual Gasstation Gasstation { get; set; }\n        public virtual Image Image { get; set; }\n\n    #endregion\n\n    #region Methods\n\n        public int distance { get; set; }\n\n        [DisplayFormat(DataFormatString = \"€ {0:0.00}\")]\n        public Nullable<double> totalPrice\n        {\n            get\n            {\n                return volume * price;\n            }\n        }\n\n        [DisplayFormat(DataFormatString = \"{0:0.00}\")]\n        public Nullable<double> consumption\n        {\n            get\n            {\n                if (distance > 0)\n                    return volume / distance * 100;\n                else\n                    return 0;\n            }\n        }\n\n        [DisplayFormat(DataFormatString = \"{0:0}\")]\n        public Nullable<double> consumption2\n        {\n            get\n            {\n                if (distance > 0)\n                    return distance / volume;\n                else\n                    return 0;\n            }\n        }\n\n        [DisplayFormat(DataFormatString = \"{0:0.00}\")]\n        public Nullable<double> mileageLeft\n        {\n            get \n            {\n                return volumeLeft * consumption2;\n            }\n        }\n\n        [DisplayFormat(DataFormatString = \"{0:0.00}\")]\n        public Nullable<double> volumeLeft\n        {\n            get\n            {\n                double? d = 45 - volume;\n                return d;\n            }\n        }\n\n    #endregion\n}\n```\n\n\nAs you can see I don't do anything with the distance here.\nI calculate the distance in the details page of one specific record.\n\nWhat I want to do now is make a summary on the index page summing the distance and calculate the average consumption.\nThis is all relative to the distance, but this is an empty field at that point.\n\nI've got the same problem with showing a list of all records on the index page.\nThe distance and consumption columns are empty.\n\nWhat would be the best way to get the distance filled at that point?\nOr would it be better to store the distance in the database after all?\n\nThis is my index function:\n\n```\n    [HttpGet]\n    [Authorize]\n    public ActionResult Index(string sortOrder, int? page)\n    {\n        ViewBag.CurrentSort = sortOrder;\n        ViewBag.NameSortParm = String.IsNullOrEmpty(sortOrder) ? \"asc\" : \"\";\n\n        List<Refuel> rf = db.Refuels.ToList();\n        var refuels = from t in db.Refuels select t;\n\n        switch (sortOrder)\n        {\n            case \"asc\":\n                refuels = refuels.OrderBy(q => q.mileage);\n                break;\n            default:\n                refuels = refuels.OrderByDescending(q => q.mileage);\n                break;\n        }\n\n        // statistieken\n        ViewBag.count = rf.Count();\n        ViewBag.distance = rf.Sum(q => q.distance);\n        ViewBag.volume = rf.Sum(q => q.volume);\n        ViewBag.consumption = String.Format(\"{0:0.0}\", rf.Where(q => q.consumption > 0).Average(q => q.consumption));\n        ViewBag.consumption2 = String.Format(\"{0:0.0}\", rf.Where(q => q.consumption2 > 0).Average(q => q.consumption2));\n        ViewBag.cost = String.Format(\"{0:c2}\", rf.Sum(q => q.totalPrice));\n\n        int pageSize = 10;\n        int pageNumber = (page ?? 1);\n        return View(refuels.ToPagedList(pageNumber, pageSize));\n    }\n```\n\n\nEDIT:\nManaged to fix the summary part, of course I could just use the last and first mileage to get the total distance driven:\n\n```\nViewBag.count = tb.Count();\nViewBag.distance = tb.Last().mileage - tb.First().mileage;\nViewBag.volume = tb.Sum(q => q.volume);\nViewBag.consumption = String.Format(\"{0:0.0}\", ViewBag.volume / ViewBag.distance * 100);\nViewBag.consumption2 = String.Format(\"{0:0.0}\", ViewBag.distance / ViewBag.volume);\nViewBag.cost = String.Format(\"{0:c2}\", tb.Sum(q => q.totalPrice));\n```\n\n\nAll I need to do now is display the distance per record in the index table.\nThis is the loop in my index.cshtml:\n\n```\n@foreach (var item in Model)\n{\n    <tr>\n        <td>\n            @Html.ActionLink(Html.DisplayFor(modelItem => item.mileage).ToString(), \"Details\", new { id = item.id })\n        </td>\n        <td>\n            @Html.DisplayFor(modelItem => item.date)\n        </td>\n        <td>\n            @Html.DisplayFor(modelItem => item.distance)\n        </td>\n        <td>\n            @Html.DisplayFor(modelItem => item.volume)\n        </td>\n        <td>\n            @Html.DisplayFor(modelItem => item.consumption)\n        </td>\n        <td>\n            @Html.DisplayFor(modelItem => item.totalPrice)\n        </td>\n        <td>\n            @Html.DisplayFor(modelItem => item.Gasstation.fullname)\n        </td>\n    </tr>\n}\n```\n\n    ", "Answer": "\r\nwhatever you are doing in the detail page of the single record to calculate the distance have to be done in this case also. If you think that operation is costlier than storing the distance field in database - you got the answer.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Hamming distance on binary strings in SQL\r\n                \r\nI have a table in my DB where I store SHA256 hashes in a BINARY(32) column. I'm looking for a way to compute the Hamming distance of the entries in the column to a supplied value, i.e. something like:\n\n```\nSELECT * FROM table \n  ORDER BY HAMMINGDISTANCE(hash, UNHEX(<insert supplied sha256 hash here>)) ASC \n  LIMIT 10\n```\n\n\n(in case you're wondering, the Hamming distance of strings A and B is defined as ```\nBIT_COUNT(A^B)```\n, where ^ is the bitwise XOR operator and BIT_COUNT returns the number of 1s in the binary string).\n\nNow, I know that both the ^ operator and BIT_COUNT function only work on INTEGERs and so I'd say that probably the only way to do it would be to break up the binary strings in substrings, cast each binary substring to integer, compute the Hamming distance substring-wise and then add them. The problem with this is that it sounds terribly complicated, not efficient and definitely not elegant. My question therefore is: could you suggest any better way? (please note that I'm on shared hosting and therefore I can't modify the DB server or load libraries)\n\nedit(1): Obviously loading the whole table in PHP and doing the computations there would be possible but I'd rather avoid it because this table will probably grow quite large.\n\nedit(2): The DB server is MySQL 5.1\n\nedit(3): My answer below contains the code that I just described above.\n\nedit(4): I just found out that using 4 BIGINTs to store the hash instead of a BINARY(32) yields massive speed improvements (more than 100 times faster). See the comments to my answer below.\n    ", "Answer": "\r\nIt appears that storing the data in a ```\nBINARY```\n column is an approach bound to perform poorly. The only fast way to get decent performance is to split the content of the ```\nBINARY```\n column in multiple ```\nBIGINT```\n columns, each containing an 8-byte substring of the original data.\n\nIn my case (32 bytes) this would mean using 4 ```\nBIGINT```\n columns and using this function:\n\n```\nCREATE FUNCTION HAMMINGDISTANCE(\n  A0 BIGINT, A1 BIGINT, A2 BIGINT, A3 BIGINT, \n  B0 BIGINT, B1 BIGINT, B2 BIGINT, B3 BIGINT\n)\nRETURNS INT DETERMINISTIC\nRETURN \n  BIT_COUNT(A0 ^ B0) +\n  BIT_COUNT(A1 ^ B1) +\n  BIT_COUNT(A2 ^ B2) +\n  BIT_COUNT(A3 ^ B3);\n```\n\n\nUsing this approach, in my testing, is over 100 times faster than using the ```\nBINARY```\n approach.\n\n\n\nFWIW, this is the code I was hinting at while explaining the problem. Better ways to accomplish the same thing are welcome (I especially don't like the binary > hex > decimal conversions):\n\n```\nCREATE FUNCTION HAMMINGDISTANCE(A BINARY(32), B BINARY(32))\nRETURNS INT DETERMINISTIC\nRETURN \n  BIT_COUNT(\n    CONV(HEX(SUBSTRING(A, 1,  8)), 16, 10) ^ \n    CONV(HEX(SUBSTRING(B, 1,  8)), 16, 10)\n  ) +\n  BIT_COUNT(\n    CONV(HEX(SUBSTRING(A, 9,  8)), 16, 10) ^ \n    CONV(HEX(SUBSTRING(B, 9,  8)), 16, 10)\n  ) +\n  BIT_COUNT(\n    CONV(HEX(SUBSTRING(A, 17, 8)), 16, 10) ^ \n    CONV(HEX(SUBSTRING(B, 17, 8)), 16, 10)\n  ) +\n  BIT_COUNT(\n    CONV(HEX(SUBSTRING(A, 25, 8)), 16, 10) ^ \n    CONV(HEX(SUBSTRING(B, 25, 8)), 16, 10)\n  );\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Is it possible to find the distance between ticks in D3.js?\r\n                \r\nIs there a way to find out the distance between the tick marks on the x axis? I'm using the ordinal scale with rangeRoundBands with tells me it doesn't have a tick function.\n\n```\nvar x= d3.scale.ordinal().rangePoints([_margin.left, cWidth]);\nx.domain(['Dec','Jan']);\nvar testTicks = x.ticks(2);\n```\n\n\nIt generates the axis fine (can't post an image) but I can't figure out how to get the distance \n\n(edit: added x.domain)\n    ", "Answer": "\r\n```\nvar data         = [45, 31, 23], // whatever your data is\n    graphHeight  = 400,\n    // however many ticks you want to set\n    numberTicksY = 4,\n    // set y scale\n    // (hardcoded domain in this example to min and max of data vals > you should use d3.max real life)\n    y            = d3.scale.linear().range(graphHeight, 0]).domain(23, 45), \n    yAxis        = d3.svg.axis().scale(y).orient(\"left\").ticks(numberTicksY),\n    // eg returns -> [20, 30, 40, 50]\n    tickArr      = y.ticks(numberTicksY),\n    // use last 2 ticks (cld have used first 2 if wanted) with y scale fn to determine positions\n    tickDistance = y(tickArr[tickArr.length - 1]) - y(tickArr[tickArr.length - 2]);\n```\n\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Find distance from a point to a polygon\r\n                \r\nI have a polygon that contains latitude and longitude as:\n```\npolygon= [[latitude1, longitude1], [[latitude2, longitude2]]....[latitudeN, longitudeN]]\n```\n\nThe result of the shape is a circle, I want to calculate the distance from a point to the polygon.\nI know how to find the distance from one point to another, so I can iterate over all points in the polygon and find distance against my point to find minimum distance but is there another way?\nEdit-1: I have some satellites footprints over a map, those footprints are presented as a polygons. I have some other points (locations) and I want to see which satellite is closer to each point and calculate the distance to that satellite\n    ", "Answer": "\r\nIn the case that the polygon indeed describes a circle, you could save the polygon as a center location (x,y) coordinate and the radius of the circle. The distane of a point to the polygon can be computed as the distance from the center of the circle to the wanted point, and then reduce the raidus size. As a bonus, if the resulted distance is negative, your point is inside the circle.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
{"Question": "Distance between two convex polygons in 3D\r\n                \r\nI have two convex polygons in 3D. They are both flat on different planes, so they're a pair of faces. \n\nWhat's the simplest way to calculate the closest distance between these two polygons?\n\nEdit: The length of the shortest possible line that has an endpoint in the first polygon and another other endpoint in the second polygon. The distance that I'm looking for is the length of this shortest possible line. \n    ", "Answer": "\r\nWell, there are only a few possibilities; the shortest line between the two polygons could be:\n\n\nBetween two vertices\nBetween an edge and a vertex\nBetween two edges (imagine two polygons on perpendicular planes)\nBetween a vertex and the inside of the polygon\nOr the polygons intersect\n\n\nCases 1-3 are all taken care of by treating each edge + vertex-pair as a line segment, and enumerating the distance between all line-segment pairs.\n\nFor case 4, find the distance between each vertex and the other polygon's plane.  Check to make sure that the line (spanning from the vertex to the nearest point on the plane) is inside the other polygon; if it's not, then the shortest line to the other polygon will be on its perimeter, which was already taken care of in case 1 or 2.\n(make sure to do this check for both polygons)\n\nFor case 5, at least one line segment must intersect with the area of the other polygon - if they intersected on their perimeters, we would have caught it already in cases 1-3, and if a vertex intersected the area, we would have caught it in case 4.  So simply check the intersection of each edge with the other polygon's plane and see if the intersection point is inside the other polygon.\n(make sure to do this check for both polygons)\n\nTake the minimum distance found in all of that, and we're done.\n    ", "Knowledge_point": "Edit Distance", "Tag": "算法分析"}
