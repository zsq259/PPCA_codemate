{"Question": "Universal hashing misunderstanding\r\n                \r\nI am trying to understand how universal hashing works. It is defined ```\nh(x) = [(a*x + b) mod p] mod m```\n where ```\na,b```\n - random numbers, ```\nm```\n - size of hash table, ```\nx```\n - key, and ```\np```\n - prime number. For example, I have several different keys:\n\n```\n92333\n23347\n20313\n```\n\n\nAnd in order to create a universal hash function I have to to following:\n\n```\nLet a = 10, b = 22, p = 313, m = 100\nh(92333) = [(10 * 92333 + 22) mod 313] mod 100 = 2 mod 100 = 2\nh(23347) = [(10 * 23347 + 22) mod 313] mod 100 = 307 mod 100 = 7\n...\n```\n\n\nBut probably every time I will get number in range from 0 to 99 and there might be lots of collisions. \n\nSo my question is: I understood and applied universal hashing correctly?\n    ", "Answer": "\r\nAssuming that the numbers you're hashing have a uniform distribution, your function is biased toward buckets 0 through 12.\n\nAssume that the hash operation up to and including the ```\nmod 313```\n operation occurs. The result of that operation gets you a value in the range 0..312. Again, if the result of this operation is even distributed, then take the ```\nmod 100```\n you get the following effect:\n\n```\n result of       Occurs for these\n  mod 100        mod 313 results\n-----------     ------------------\n     0           0, 100, 200, 300\n     1           1, 101, 201, 301\n     2           2, 102, 202, 302\n     3           3, 103, 203, 303\n     4           4, 104, 204, 304\n     5           5, 105, 205, 305\n     6           6, 106, 206, 306\n     7           7, 107, 207, 307\n     8           8, 108, 208, 308\n     9           9, 109, 209, 309\n    10          10, 110, 210, 310\n    11          11, 111, 211, 311\n    12          12, 112, 212, 312\n    13          13, 113, 213\n    14          14, 114, 214\n    15          15, 115, 215\n```\n\n\nNotice how the number of opportunities to get a particular result drop after 12?  There's your bias.  Here's more evidence of that effect taken from counting the results of hashing the numbers 0 through 5,000,000:\n\n```\ncounts[0]: 63898\ncounts[1]: 63896\ncounts[2]: 63899\ncounts[3]: 63900\ncounts[4]: 63896\ncounts[5]: 63896\ncounts[6]: 63900\ncounts[7]: 63896\ncounts[8]: 63896\ncounts[9]: 63900\ncounts[10]: 63898\ncounts[11]: 63896\ncounts[12]: 63899\ncounts[13]: 47925\ncounts[14]: 47922\ncounts[15]: 47922\ncounts[16]: 47925\n\n... elided similar counts ...\n\ncounts[97]: 47922\ncounts[98]: 47922\ncounts[99]: 47925\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Universal hashing\r\n                \r\nI do not quite understand how universal hashing works. For example, when I insert an item into my hash table, I have to choose a random function from my universal family of hash functions. Now I want to retrieve said item. How will my hash table know which function it has to use to calculate the hash?\n    ", "Answer": "\r\nBecause you'll use the same hash function for all the items in the table.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Universal Hashing Integers\r\n                \r\nThis is my first thread here and I would like to ask you a couple of questions for universal hashing of integers.\nA universal hashing algorithm is supposed to use this:\n\n```\nequation =\n((a*x+b)mod p) mod m\n\na=random number from 1 to p-1\n\nb=random number from 0 to p-1\n\nx= the Key \n\np= a prime number >=m\n\nm=the size of the array\n```\n\n\nI know the numbers I am going to hash are on the range of 1-2969.\nBut I cannot understand how to use this equation in order to make as low collisions as possible.\nAt the time a and b are random I cannot do anything about it.\nMy question is how I am supposed to pick the prime if I have more than one choice, the range of primes I can use are from 2 to 4999.\nI tried to pick the first available that corresponds the requirements for the function but sometimes it can return negative numbers. I have searched on Google and Stackoverflow but I could not figure out what I am not doing wrong.\n\nI am coding in C. Also, I can use only universal hashing.\n\nThank your for your time.\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Why doesn't Java's hashCode support universal hashing?\r\n                \r\nSome hash table schemes, such as cuckoo hashing or dynamic perfect hashing, rely on the existence of universal hash functions and the ability to take a collection of data exhibiting collisions and resolve those collisions by picking a new hash function from the family of universal hash functions.\n\nA while ago I was trying to implement a hash table in Java backed by cuckoo hashing and ran into trouble because while all Java objects have a ```\nhashCode```\n function, the value that ```\nhashCode```\n returns is fixed for each object (unless, of course, the objects change).  This means that without the user providing an external family of universal hash functions, it's impossible to build a hash table that relies on universal hashing.  \n\nInititially I thought that I could get around this by applying a universal hash function to the object's ```\nhashCode```\ns directly, but this doesn't work because if two objects have the same ```\nhashCode```\n, then any deterministic function you apply to those hash codes, even a randomly-chosen hash function, will result in the same value and thus cause a collision.\n\nIt seems like this would be detrimental to Java's design.  It means that ```\nHashMap```\n and other hash containers are completely prohibited from using tables based on universal hashing, even if the language designers may think that such tables would be appropriate in the language design.  It also makes it harder for third-party library designers to build hash tables of this sort as well.\n\nMy question is: is there a reason that Java opted to design ```\nhashCode```\n without considering the possibility of hashing objects with multiple hash functions?  I understand that many good hashing schemes like chained hashing or quadratic probing don't require it, but it seems as though the decision makes it hard to use certain classes of algorithms on Java objects.\n    ", "Answer": "\r\nSimplicity. Java allows class designers to provide their own ```\nhashCode```\n, which as you mention is good enough for \"ordinary\" hash tables, and can be hard enough to understand.\n\nBesides, when the Java Collections API was designed, having generic hash tables in the standard library was bold enough a move already. C has never had them. C++ had them in the STL as ```\nhash_set```\n and ```\nhash_map```\n, but those didn't make it into the standard. Only now, in C++0x, are hash tables being considered for standardization again.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Use of universal hashing\r\n                \r\nI'm trying to understand the usefullness of universal hashing over normal hashing, other than the function is randomly produced everytime, reading Cormen's book.\n\nFrom what i understand in universal hashing we choose the function to be\n\n```\nH(x)=[(ax+b)mod p]mod m\n```\n\n\nwith p being a prime number larger than all the keys, m the size of the data table, and a,b random numbers.\n\nSo for example if i want to read the ID of 80 people, and each ID has a value between [0,200], then m would be 80 and p would be 211(next prime number). Right?\nI could use the function lets say \n\n```\nH(x)=[(100x+50)mod 211]mod 80\n```\n\n\nBut why would this help? There is a high chance that i'm going to end up having a lot of empty slots of the table, taking space without reason. Wouldn't it be more usefull to lower the number m in order to get a smaller table so space isn't used wtihout reason?\n\nAny help appreciated\n    ", "Answer": "\r\nI think the best way to answer your question is to abstract away from the particulars of the formula that you're using to compute hash codes and to think more about, generally, what the impact is of changing the size of a hash table.\n\nThe parameter m that you're considering tuning adjusts how many slots are in your hash table. Let's imagine that you're planning on dropping n items into your hash table. The ratio n / m is called the load factor of the hash table and is typically denoted by the letter α.\n\nIf you have a table with a high load factor (large α, small m), then you'll have less wasted space in the table. However, you'll also increase the cost of doing a lookup, since with lots of objects distributed into a small space you're likely to get a bunch of collisions that will take time to resolve.\n\nOn the other hand, if you have a table with a low load factor (small α, large m), then you decrease the likelihood of collisions and therefore will improve the cost of performing lookups. However, if α gets too small - say, you have 1,000 slots per element actually stored - then you'll have a lot of wasted space.\n\nPart of the engineering aspect of crafting a good hash table is figuring out how to draw the balance between these two options. The best way to see what works and what doesn't is to pull out a profiler and measure how changes to α change your runtime.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Confusion related to universal hashing\r\n                \r\nI was reading this video lecture related to universal hashing. It shows the example of hashing IP addresses. Each IP address consists of 4, 32 bit integers, (x1,x2,x3,x4) with any xi having the maximum value of 255.\n\nThe tutorial says that the size of the hash table should be greater than 255 or any of the xis. Why is it so?\n    ", "Answer": "\r\n(For those of you who haven't seen the video, this comes up around 20:45).\n\nThe class of functions defined this way are functions of the form\n\n\n  ha(x1, x2, x3, x4) = a1 x1 + a2 x2 + a3 x3 + a4 x4 (mod n)\n\n\nwhere n is the number of buckets, each xi is in the range 0 to n - 1, each ai is in the range 0 to n - 1, and n is prime.\n\nYour question is why all the xi's have to be less than n. The reason has to do with the proof that this family of hash functions is universal. As Tim explains in the video, one of the ways that you can prove that the hash function is universal is to consider two distinct inputs (call them x and y). That means that they have to differ in some component, and the idea is to assume without loss of generality that they differ in the fourth component. That is, x4 ≠ y4. With a bit of math, under that assumption, you can show that the probability that you get a collision is equal to the probability that this statement is true:\n\n\n  a4(x4 - y4) = a1(x1 - y1) + a2(x2 - y2) + a3(x3 - y3) (mod n)\n\n\nHere, since we chose the hash function randomly, all of the ai's are random. The key insight is that if you treat a1, a2, and a3 as fixed values, then the right-hand side of this equation just some fixed number k. You're then interested in the probability that\n\n\n  a4(x4 - y4) = k (mod n)\n\n\nBecause we're assuming that n > x4, that n > y4, that x4 ≠ y4, and that n is prime. This tells us two very important facts:\n\n\nx4 - y4 ≠ 0 mod n. This is the main reason that we need n to be larger than the xi's. We'll see why in a minute.\nx4 - y4 is coprime with n. Why is that? Well, we know that n is a prime number. Since n > x4 and n > y4, we know that x4 - y4 must be strictly between n-1 and -(n-1). Because we're assuming that n is prime, there are non nontrivial divisors of n in that range, so x4 - y4 and n are coprime.\n\n\nTo see why these facts matter, let's consider two cases for k. First, it's possible that k = 0. In that case, in what case will a4(x4 - y4) = k (mod n)? Since x4 - y4 ≠ 0, we know that this only happens if a4 = 0. Since a4 takes on a uniformly-random value in the range 0 through n-1 inclusive, the probability that we get a collision in this case is 1/n.\n\nNext, suppose that k ≠ 0. In that case, what has to happen for a4(x4 - y4) = k (mod n) to be true? Well, we know that x4 - y4 ≠ 0, so it must have a multiplicative inverse modulo n because x4 - y4 is coprime with n. In fact, it has exactly one multiplicative inverse. The only possible choice of a4 that will cause this to be true is the choice where a4 is the multiplicative inverse of x4 - y4 modulo n multiplied by k. There's exactly one choice of number in the range 0 through n-1 that works, so the probability of picking it is 1/n.\n\nNotice that if x4 - y4 were equal to zero modulo n, the above reasoning wouldn't work. In the first case, every choice of a4 would cause a collision, so the collision probability would be 1. In the second case, no choice of a4 could cause a collision, so the collision probability would be 0. These conditions would invalidate the proof.\n\nHope this helps!\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Universal hashing performs worse than modulo hashing, what is wrong?\r\n                \r\nIn case you are not familiar with universal hashing, it's mainly an attempt to guarantee a low number of collisions (as opposed, say with using plain old modulo), using some rather simple math involving randomness. The problem is it doesn't work for me:\n\n```\nsize_t hash_modulo(const int value) {\n    return (size_t) (value % TABLE_SIZE);\n}\n\n// prime 491 is used because its > 128, which is the size of the hash table\nsize_t hash_universal(const int value) {\n    const size_t a = (size_t) (rand() % 491 + 1);\n    const size_t b = (size_t) (rand() % 491);\n    //printf(\"a: %zu, b:%zu\\n\", a, b);\n    return ((a * value + b) % 491) % TABLE_SIZE;\n}\n```\n\n\nI test modulo hashing first and determine the longest chain length (chain length means a hash bucket size):\n\n```\nsize_t get_max_chain_length(int input[TABLE_SIZE], size_t (*hash_function)(const int)) {\n    HashTable *hash_table = hash_table_create(hash_function);\n    if (!hash_table) {\n        return 0;\n    }\n\n    for (size_t i = 0; i < TABLE_SIZE; ++i) {\n        hash_table_add(hash_table, input[i]);\n    }\n\n    size_t maximum_chain_length = 0;\n    for (int j = 0; j < TABLE_SIZE; ++j) {\n        const size_t length = length_of_(hash_table->rows[j]);\n        maximum_chain_length = (length > maximum_chain_length) ? length : maximum_chain_length;\n    }\n\n    //hash_table_print(hash_table);\n    hash_table_destroy(hash_table);\n\n    return maximum_chain_length;\n}\n```\n\n\nI pick one of the inputs which led to a really big chain (id est one which performs bad using plain modulo) and throw this one against universal hashing. Universal hashing uses randomness so I can take a constant input and still get varying results. \n\nAnd here comes the problem. I try 100 random input arrays of size 128 each and calculate the average longest chain and the total longest chain, but both algorithms perform similar.\n\nYou can check my main in my repo.\n\nMy question is: Is that result to be expected? Does universal hashing perform not any better with input which already performed poor using modulo? Or did I just screw up my implementation (more likely).\n\nThanks a lot in advance!\n    ", "Answer": "\r\nWell, why do you think modulo is bad? If the input is random and sufficiently large, the modulo should yield a uniformly distributed result. Uniform hashing (as your link states) provides protection against non-random (i.e., malicious) input, which isn't the case here.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "understanding Universal hashing chapter on CLRS\r\n                \r\nHi I am reading the chapter about universal hashing on CLRS.\n\nOn page 234\n\nCorollary 11.4 \n\n\n  Using universal hashing and collision resolution by chaining in a\n  table with m  slots, it takes expected time Theta(n) to handle any\n  sequence of n INSERT, SEARCH  and DELETE operations containing O(m)\n  INSERT operations.\n\n\nI kinda get the idea but I have a hard time to understand this English sentence. What does the end \"containing O (m) INSERT operations\" mean?\n\nDoes it mean n = O(m) insertion was performed already. Then, .... I don't know. I can't parse this sentence. What is the difference between the 1st INSERT and 2nd INSERT?\n\nI would like to hear your opinion.\n\nThanks!\n    ", "Answer": "\r\nI think that there is only one sequence of n insert, search, and delete operations, but the parameter m is used to limit the number of insert operations you are allowed to put within those n operations. Suppose that you had a table of size 10, so m=10, and then you set n=1 000 000, with the first 500 000 operations inserts, and the next 500 000 searches for an item not in the table. Then the performance would be very poor, because the table would have chains about 100 000 items long.\n\nSo if you have a table with m slots, the theorem only allows you about m insert operations, so that the table never holds more than about m items, and the chains won't get too long, and all the operations are pretty much O(1) - in the example above you can only have about 10 insert operations, so the other 999 990 operations must be either search or delete.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Dynamic perfect hashing and universal hash functions - explanation please?\r\n                \r\nSo I'm reading up about hash tables, hash functions etc.  I was intrigued to read on wikipedia about how \"dynamic perfect hashing\" involves using a second hash table as the data structure to store multiple values within a particular bucket.\n\nWhere I get lost however, is when it comes to how a universal hash function is selected to perform the hashing for that second hash table.  Can anyone explain how this universal hash function is determined from the values being stored in the bucket?  I vaguely following the reasoning and logic in wikipedia's \"universal hash function\" page, but am struggling to have any intuition on it.  In particular, how do these functions guarantee no clashes?  Or atleast, if they're disposed of and a new one generated if a clash is detected, how do we know this can be done in a realistic amount of time if at all?\n\nLadybird book explanation please?\n    ", "Answer": "\r\nPerfect hashing means that read access takes constant time even in the worst case.\n\nFor inserting keys there are no worst-case guarantees, the time bounds are only true on average (or maybe amortized).\n\nTo make insertion fast enough the second level hash table is chosen very large for the number of keys (k2), large enough so that collisions become sufficiently unlikely. This is not a problem w.r.t. size because the first level hash distributes keys evenly so that on average second level hash tables are still small.\n\nThe hash function for the second level tables are chosen at random from a set of parameterized hash functions.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is it possible to implement universal hashing for the complete range of integers?\r\n                \r\nI am reading about Universal hashing on integers. The prerequisite and mandatory precondition seems to be that we choose a prime number ```\np```\n greater than the set of all possible keys.\n\nI am not clear on this point.\n\nIf our set of keys are of type ```\nint```\n then this means that the prime number needs to be of the next bigger data type e.g. ```\nlong```\n.\n\nBut eventually whatever we get as the hash would need to be down-casted to an int to index the hash table. Doesn't this down-casting affect the quality of the Universal Hashing (I am referring to the distribution of the keys over the buckets) somehow?\n    ", "Answer": "\r\n\n  If our set of keys are integers then this means that the prime number\n  needs to be of the next bigger data type e.g. long.\n\n\nThat is not a problem. Sometimes it is necessary otherwise the hash family cannot be universal. See below for more information.\n\n\n  But eventually whatever we get as the hash would need to be\n  down-casted to an ```\nint```\n to index the hash table.\n  \n  Doesn't this down-casting affect the quality of the Universal Hashing\n  (I am referring to the distribution of the keys over the buckets)\n  somehow?\n\n\nThe answer is no. I will try to explain.\n\nWhether ```\np```\n has another data type or not is not important for the hash family to be universal. Important is that ```\np```\n is equal or larger than ```\nu```\n (the maximum integer of the universe of integers). It is important that ```\np```\n is big enough (i.e. ```\n>= u```\n).\n\n\n  A hash family is universal when the collision probability is equal or\n  smaller than ```\n1/m```\n.\n\n\nSo the idea is to hold that constraint.\n\nThe value of ```\np```\n, in theory, can be as big as a ```\nlong```\n or more. It just needs to be an integer and prime.\n\n\n```\nu```\n is the size of the domain/universe (or the number of keys). Given the universe ```\nU = {0, ..., u-1}```\n, ```\nu```\n denotes the size ```\n|U|```\n.\n```\nm```\n is the number of bins or buckets\n```\np```\n is a prime which must be equal or greater than ```\nn```\n\nthe hash family is defined as ```\nH = {h(a,b)(x)}```\n with ```\nh(a,b)(x) = ((a * x + b) mod p) mod m```\n. Note that ```\na```\n and ```\nb```\n are randomly chosen integers (from all possible integers, so theoretically can be larger than ```\np```\n) modulo a prime ```\np```\n (which can make them either smaller or larger than ```\nm```\n, the number of bins/buckets); but here too the data type (domain of values does not matter). See Hashing integers on Wikipedia for notation.\nFollow the proof on Wikipedia and you conclude that the collision probability is ```\n_p/m_ * 1/(p-1)```\n (the underscores mean to truncate the decimals). For ```\np >> m```\n (```\np```\n considerably bigger than ```\nm```\n) the probability tends to ```\n1/m```\n (but this does not mean that the probability would be better the larger ```\np```\n is).\n\n\nIn other terms answering your question: ```\np```\n being a bigger data type is not a problem here and can be even required. ```\np```\n has to be equal or greater than ```\nu```\n and ```\na```\n and ```\nb```\n have to be randomly chosen integers modulo ```\np```\n, no matter the number of buckets ```\nm```\n. With these constraints you can construct a universal hash family.\n\n\n\nMaybe a mathematical example could help\n\n\nLet U be the universe of integers that correspond to ```\nunsigned char```\n (in C for example). Then ```\nU = {0, ..., 255}```\n\nLet ```\np```\n be (next possible) prime equal or greater than ```\n256```\n. Note that ```\np```\n can be any of these types (```\nshort```\n, ```\nint```\n, ```\nlong```\n be it signed or unsigned). The point is that the data type does not play a role (In programming the type mainly denotes a domain of values.). Whether ```\n257```\n is ```\nshort```\n, ```\nint```\n or ```\nlong```\n doesn't really matter here for the sake of correctness of the mathematical proof. Also we could have chosen a larger ```\np```\n (i.e. a bigger data type); this does not change the proof's correctness.\n\n\nThe next possible prime number would be ```\n257```\n. \nWe say we have ```\n25```\n buckets, i.e. ```\nm = 25```\n. This means a hash family would be universal if the collision probability is equal or less than ```\n1/25```\n, i.e. approximately ```\n0.04```\n.\nPut in the values for ```\n_p/m_ * 1/(p-1)```\n: ```\n_257/25_ * 1/256 = 10/256 = 0.0390625```\n which is smaller than ```\n0.04```\n. It is a universal hash family with the chosen parameters.\n\n\n\nWe could have chosen ```\nm = u = 256```\n buckets. Then we would have a collision probability of ```\n0.003891050584```\n, which is smaller than ```\n1/256 = 0,00390625```\n. Hash family is still universal.\n\nLet's try with ```\nm```\n being bigger than ```\np```\n, e.g. ```\nm = 300```\n. Collision probability is 0, which is smaller than ```\n1/300 ~= 0.003333333333```\n. Trivial, we had more buckets than keys. Still universal, no collisions.\n\n\n\nImplementation detail example\n\nWe have the following:\n\n\n```\nx```\n of type ```\nint```\n (an element of ```\n|U|```\n)\n```\na```\n, ```\nb```\n, ```\np```\n of type ```\nlong```\n\n```\nm```\n we'll see later in the example\n\n\nChoose ```\np```\n so that it is bigger than the max ```\nu```\n (element of ```\n|U|```\n),  ```\np```\n is of type ```\nlong```\n.\nChoose ```\na```\n and ```\nb```\n (modulo ```\np```\n) randomly. They are of type ```\nlong```\n, but always ```\n< p```\n.\nFor an ```\nx```\n (of type ```\nint```\n from ```\nU```\n) calculate ```\n((a*x+b) mod p)```\n. ```\na*x```\n is of type ```\nlong```\n, ```\n(a*x+b)```\n is also of type ```\nlong```\n and so ```\n((a*x+b) mod p```\n is also of type ```\nlong```\n. Note that ```\n((a*x+b) mod p)```\n's result is ```\n< p```\n. Let's denote that result ```\nh_a_b(x)```\n.\n```\nh_a_b(x)```\n is now taken ```\nmodulo m```\n, which means that at this step it depends on the data type of ```\nm```\n whether there will be downcasting or not. However, it does not really matter. ```\nh_a_b(x)```\n is ```\n< m```\n, because we take it ```\nmodulo m```\n. Hence the value of ```\nh_a_b(x) modulo m```\n fits into ```\nm```\n's data type. In case it has to be downcasted there won't be a loss of value. And so you have mapped a key to a bin/bucket.\n\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Why hash code computed using division method instead of universal hashing method?\r\n                \r\nI find the following code for compute hashcode: \n\n```\nint hashCode = comparer.GetHashCode(key) & 0x7FFFFFFF;\nint index = hashCode % buckets.Length;\n```\n\n\nWhy engineers didn't choose a universal hashing method:\n\n```\nint index = [(ak + b) mod p)mod buckets.Length]\n```\n\n\nwhere ```\na,b```\n are random numbers between ```\n0...p-1```\n (p is prime) ?\n    ", "Answer": "\r\nA complete answer to the question would require consulting with the individual(s) who wrote that code. So I don't think you're going to get a complete answer.\n\nThat said:\n\n\nThe \"universal hashing method\", as you call it, is hardly the only possible implementation of a good hash code. People implement hash code computations in a variety of ways for a variety of reasons.\n\n\nMore important though…\n\n\nThe computation to which you refer is not actually computing a hash code. The variable name is a bit misleading, because while the value is based on the hash code of the item in question, it's really an implementation detail of the class's internal hash table. By sacrificing the highest bit from the actual hash code, the ```\nEntry```\n value for the hash table can be flagged as unused using that bit. Masking the bit off as opposed to, for example, just special-casing an element with a hash code value of ```\n-1```\n, preserves the distribution qualities of the original hash code implementation (which is determined outside the ```\nDictionary<TKey, TValue>```\n class).\n\n\nIn other words, the code you're asking about is simply how the author of that code implemented a particular optimization, in which they decreased the size of the ```\nEntry```\n value by storing a flag they needed for some other purpose — i.e. the purpose of indicating whether a particular table ```\nEntry```\n is used or not — in the same 32-bit value where part of the element's hash code is stored.\n\nStoring the hash code in the ```\nEntry```\n value is in turn also an optimization. Since the ```\nEntry```\n value includes the ```\nTKey key```\n value for the element, the implementation could in fact just have always called the ```\nkey.GetHashCode()```\n method to get the hash code. This is a trade-off in acknowledging that the ```\nGetHashCode()```\n method is not always optimized itself (indeed, most implementations, including .NET's implementation for the ```\nSystem.String```\n class, always recompute the hash code from scratch), and so the choice was (apparently) made to cache the hash code value within the ```\nEntry```\n value, rather than asking the ```\nTKey```\n value to recompute it every time it's needed.\n\nDon't confuse the caching and subsequent usage of some other object's hash code implementation with an actual hash code implementation. The latter is not what's going on in the code you're asking about, the former is.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Integer overflow in universal hashing implementation\r\n                \r\nFor one of my pet projects I would like to create an arbitrary number of different integer hashes. After a bit of research I figured that universal hashing is the way to go. However, I struggle with the (numpy) implementation. Say I'm trying to use the hash family ```\nh(a,b)(x) = ((a * x + b) mod p) mod m```\n and my x can be anywhere in the range of uint32. Then choosing ```\np >= max(x)```\n and ```\na, b < p```\n means that in the worst case ```\na * x + b```\n overflows not only uint32 but also uint64. I tried to find an implementation which solves this problem but only found clever ways to speed up the modulo operation and nothing about overflow.\n\nAny suggestions for how to implement this are greatly appreciated. Thanks :-)\n    ", "Answer": "\r\n```\n(x + y) % z == ((x % z) + (y % z)) % z```\n. So you could take the modulus before doing the sum:\n\n\nCast ```\na```\n and ```\nx```\n to uint64. (Multiply two uint32 will never overflow uint64).\nCompute ```\nh = (a * x) % p + b```\n\nReturn ```\n(h - p) if h > p else h```\n. (Alternatively: return ```\nh % p```\n)\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Basics in Universal Hashing, how to ensure accessibility\r\n                \r\nto my current understanding Universal Hashing is a method whereby the hash function is chosen randomly at runtime in order to guarantee reasonable performance for any kind of input. \n\nI understand we may do this in order to prevent manipulation by somebody choosing malicious input deliberately (a possibility of a deterministic hash function is know).\n\nMy Question is the following: Is it not true, that we still need to guarantee that a key will be mapped to the same address every time we hash it ? For instance if we want to retrieve information, but the hash function is chosen at random, how do we guarantee we can get back at our data ?\n    ", "Answer": "\r\nA universal hash function is a family of different hash functions that have the property that with high probability, two randomly-chosen elements from the universe will not collide no matter which hash function is chosen.  Typically, this is implemented by having the implementation pick a random hash function from a family of hash functions to use inside the implementation.  Once this hash function is chosen, the hash table works as usual - you use this hash function to compute a hash code for an object, then put the object into the appropriate location.  The hash table has to remember the choice of the hash function it made and has to use it consistently throughout the program, since otherwise (as you've noted) it would forget where it mapped each element.\n\nHope this helps!\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Why do we select random hash function in Universal Hashing\r\n                \r\nAs per the definition of Universal Hashing , a random hash function is selected to to have a good worst case garuntee. But I am unable to understand how it works. \n\nSuppose if I select some random Hash Function h , still there is a chance to ending up with the worst set of elements possbile.\n\nPleas explain in simple words.\n\nI have seen the video https://www.youtube.com/watch?v=s7QSM_hlS1U . But it is difficult to understand\n    ", "Answer": "\r\nYou are right: using a random hash function does not 100% prevent you from ending up with a worst case set. But in the lecture you provided, the main concern is that an enemy might be able to predict an input that always yields to a worst case scenario. \n\nAs an example he used a competitor that has to choose a benchmark for your hash table. Without using a random hash function at runtime, he would know the hash function you use and could predict which keys would yield to the same hash value and thus transform the hash table to a linked list (since each key is assigned to the same bucket). A deterministic hash function carries this risk of a predictable worst-case outcome, which is especially bad in an adversary setting. \n\nBy using a random hash function at runtime, even if the enemy chooses the benchmark, you are guaranteed with a certain probability that there are no collisions. \nMore specific, when you have the values x and y (with x != y) and you choose a function h from m different hash functions H, then (quite intuitively) the probability that h(x) = h(y) is AT LEAST smaller than 1/m, i.e., 1/m sets an upper probability bound. A deterministic hash function cannot give you this property.\n\nAlso see here\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Universal hashing, should get the same hash value for the same key?\r\n                \r\nI mean, I have implemented an universal hashing function using this expression:\n\nh(k) = ((a*k + b)mod p)mod m; (from Cormen)\n\nwhere:\n-p is big prime number greater than k;\n-a and b are two numbers that are randomly choosen the first in the range [1, p-1] and the second one [0, p-1].\n\nNow, I implemented this, and for the random function I have choosen the seed equal to k. That's because, if I don't do this, when I insert a value with the key k, it will generate a hash value, that will depends on the default seed of Random function (maybe the time). So if I want to search the key again, I can't do this, because now the universal hashing function returns me another value. So, I would appreciate you to tell me if my reasoning is correct or not.\nMy doubt is that now, doing so, if two elements have the same key, they will be irrimediably stored in the same linked list (thing that I didn't understand if it is correct or not).\n\nThanks in advance.\n    ", "Answer": "\r\nI think you have a slight misunderstanding about how universal hashing works. Rather than choosing ```\na```\n and ```\nb```\n at random every time you compute the hash, instead, before you do any hashing at all, select a random ```\na```\n and ```\nb```\n. Once you've done that, every time you need to compute the hash, go and compute it using the formula above based on the input value ```\nk```\n and the values ```\na```\n and ```\nb```\n that you chose initially.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Describe an Explicit Universal Hash Function Family\r\n                \r\nIn this problem, I was given the follow mapping\n\n```\n U = {0, 1, 2, 3, 4, 5, 6, 7} to {0, 1}\n```\n\n\nFrom this, there is an explicit universal hashing function that must be derived, with the hint that this can be done with a set of 4 functions.  Unfortunately, despite searching through articles on how to do this, I am still confused.  Any help in understanding how to find this hashing function and moving towards the right direction is greatly appreciated!\n\nEDIT:\n\nAfter some deliberation, this is what I came up with; would this be correct?\n\n```\n     0  1  2  3  4  5  6  7\n---------------------------\nh1 | 1  1  0  0  0  0  0  0 \nh2 | 0  0  1  1  0  0  0  0\nh3 | 0  0  0  0  1  1  0  0\nh4 | 0  0  0  0  0  0  1  1\n```\n\n    ", "Answer": "\r\nUsing the definition from Wikipedia:\n\n\n  A family of functions H = {h: U → [m]} is called a universal family if, ∀ x,y ∈ U,   x ≠ y :     Prh∈H[h(x) = h(y)] ≤ 1/m.\n\n\nIn your case, this means that for any two values x and y in the set {0, 1, 2, 3, 4, 5, 6, 7}, at most two of your four hash functions can map them to the same bit.\n\nYour suggestion:\n\n```\n     0  1  2  3  4  5  6  7\n---------------------------\nh1 | 1  1  0  0  0  0  0  0 \nh2 | 0  0  1  1  0  0  0  0\nh3 | 0  0  0  0  1  1  0  0\nh4 | 0  0  0  0  0  0  1  1\n```\n\n\ndoes not work, because there are four pairs (x, y) — namely (0,1), (2,3), (4,5), and (6,7) — where all four hash functions map them to the same bit.\n\nInstead, here are some options that do work:\n\n```\n     0  1  2  3  4  5  6  7\n---------------------------\nh1 | 0  0  0  0  1  1  1  1\nh2 | 0  0  1  1  0  0  1  1\nh3 | 0  1  0  1  0  1  0  1\nh4 | 0  1  1  0  1  0  0  1\n\n     0  1  2  3  4  5  6  7\n---------------------------\nh1 | 0  0  0  1  0  1  1  1\nh2 | 0  0  1  0  1  0  1  1\nh3 | 0  1  0  0  1  1  0  1\nh4 | 1  0  0  0  1  1  1  0\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Python: Faster Universal Hashing function with built in libs\r\n                \r\nI am trying to implement the universal hashing function with only base libs:\n\n\nI am having issues because I am unable to run this in an effective time. I know % is slow so I have tried the following:\n\n```\n((a * x + b) % P) % n\n\ndivmod(divmod(a * x + b, P)[1], n)[1]\n\nsubeq = pow(a * x + b, 1, P)\nhash = pow(subeq, 1, self.n)\n```\n\n\nAll of these function are too slow for what I am trying to do. Is there a faster way to do mod division only using the base libs that I am unaware of?\n\nEdit To elaborate, I will be running this function about 200000 times (or more) and I need for all 200000 runs to complete in under 4 seconds. None of these methods are even in that ball park (taking minutes)\n    ", "Answer": "\r\nYou're not going to do better than ```\n((a * x + b) % P) % m```\n in pure Python code; the overhead of the Python interpreter is going to bottleneck you more than anything else; yes, if you ensure the ```\nm```\n is a power of two, you can precompute ```\nmm1 = m - 1```\n and change the computation to ```\n((a * x + b) % P) & mm1```\n, replacing a more expensive remaindering operation with a cheaper bitmasking operation, but unless ```\nP```\n is huge (hundreds of bits minimum), the interpreter overhead will likely outweigh the differences between remainder and bitmasking.\n\nIf you really need the performance, and the types you're working with will fit in C level primitive type, you may benefit from writing a Python C extension that converts all the values to ```\nsize_t```\n, ```\nPy_hash_t```\n, ```\nuint64_t```\n, or whatever suits your problem and performs the math as a set of bulk conversions to C types, C level math, then a single conversion back to Python ```\nint```\n, saving a bunch of byte code and intermediate values (that are promptly tossed).\n\nIf the values are too large to fit in C primitives, GMP types are an option (look at ```\nmpz_import```\n and ```\nmpz_export```\n for efficient conversions from ```\nPyLong```\n to ```\nmpz_t```\n and back), but the odds of seeing big savings go down; GMP does math faster in general, and can mutate numbers in place rather than creating and destroying lots of temporaries, but even with ```\nmpz_import```\n and ```\nmpz_export```\n, the cost of converting between Python and GMP types would likely eat most of the savings.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Universal Hash Function Implementation for Strings in Java\r\n                \r\nI have some troubles understanding the implementation of a universal hash function in Java.\nThe hash function ```\n(ax + b) mod p```\n should be implemented in base of H_1.\nAlso this implementation should work for strings.\nI recently worked on this:\n```\npublic class UniversalHashing {\n    // the hash function is a linear function (h(x) = (ax + b) mod p)\n    // where a and b are chosen randomly\n    // p is a prime number\n\n    private int a;\n    private int b;\n    private int p;\n\n    public UniversalHashing(int a, int b, int p) {\n        this.a = a;\n        this.b = b;\n        this.p = p;\n    }\n\n    public int hash(String string) {\n        int hash = 0;\n        for (int i = 0; i < string.length(); i++) {\n            hash = (hash * a + string.charAt(i)) % p;\n        }\n        return (hash + b) % p;\n    }\n\n    public static void main(String[] args) {\n        UniversalHashing h = new UniversalHashing(5, 3, 11);\n        System.out.println(h.hash(\"hello\"));\n        System.out.println(h.hash(\"world\"));\n        System.out.println(h.hash(\"hello\"));\n        System.out.println(h.hash(\"world\"));\n    }\n}\n```\n\nIs this implementation correct or am I on the completely wrong path to implement a universal hash function for String.\nthanks for helping me out for this\ngreez\n    ", "Answer": "\r\nOr something like this?\n```\nimport java.math.BigInteger;\n\npublic class UniversalHashing {\n    private final BigInteger a,b;\n    private final long p = 1000000007;\n\n    public UniversalHashing(long m) {\n        a = BigInteger.valueOf((long) (Math.random() * p));\n        b = BigInteger.valueOf((long) (Math.random() * (p - 1L)));\n    }\n\n    public int hashCode(String string) {\n        BigInteger hash = BigInteger.valueOf(0);\n        for (int i = 0; i < string.length(); i++) {\n            hash = hash.add(BigInteger.valueOf(string.charAt(i)).multiply(a.pow(i)));\n        }\n        return (int) (hash.mod(BigInteger.valueOf(p)).add(b).mod(BigInteger.valueOf(p)).longValue());\n    }\n\n    public static void main(String[] args) {\n        UniversalHashing uh = new UniversalHashing(1000000007);\n        System.out.println(uh.hashCode(\"abc\"));\n        System.out.println(uh.hashCode(\"abcd\"));\n        System.out.println(uh.hashCode(\"abcde\"));\n    }\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Finding items in an universal hash table?\r\n                \r\nIf items are organized randomly, how does the table know where to start looking?\n\nIn a non-random table items are organized according to some characteristic. (i.e. name). So if the table needs to look up some arbitrary information about \"John\", it can start looking in the 'J' bucket. \n\nIn a universal hash table though, items are arranged randomly. There's no defining characteristic. Therefore to find some arbitrary info about \"John\", wouldn't the table have to look through every bucket?\n\nIsn't that a huge waste of time? It's like looking through every cabinet in your house to find a spoon. \n    ", "Answer": "\r\nWhile the previous answers are essentially correct, they don't directly address the random part of a universal hashing algorithm. Universal hashing algorithms do not use randomness when calculating a hash for a key. Random numbers are only used during the initialization of the hash table to choose a hash function from a family of hash functions.  This prevents an adversary with access to the details of the hash function from devising a worst case set of keys. \n\nIn other words, during the lifetime of the hash table, the bucket for a given key is consistent. However, a different instance (such as next time the program runs) may place that same key in a different bucket.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Universal hash with collisions\r\n                \r\nI have a homework but I really don´t know where start.\n\nWrite a program in Java that implements the two families of universal hash functions we saw in class today .\nBoth families of functions depend on various parameters , so that Java should use two classes whose constructures receive the appropriate parameters and C should make some kind of structure that contains a pointer to a function.\nTo test their implementations , use the following sets : U = f0 ; 1; 2; ::: 10008g and D = f0 ; 1; 2; ::: 2052g . Write an application at random choose a function of each type and insert U 500 random number in a table containing the addresses of D. The output of your program should be the number of collisions obtained during insertions using buckets policy to solve collisions.\n\nCan you please tell me how can I start or what should I implement for this.\nThanks\n    ", "Answer": "\r\n\nCreate the two hash functions you have been given\nGenerate and insert into your hash table, 500 numbers of type U which map to an address of type D. The insertion should also randomly choose a hash function to use for inserting\nIterate over your hash table and count the number of buckets containing more than one item of type U\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Does universal hashing choose a new hash function again after each operation has been done?\r\n                \r\nI have been reading \"Introduction to Algorithms\". I was wondering if the universal hashing choose a new one from the collection of hash functions to do the next mapping. For example, given an empty table, and a sequence of operation: [insert, insert, search, delete, insert,...], first, the algorithm choose a function from the collection and do the first operation, insert. Then, does the algorithm choose a new hash function to do the second operation, insert, or use the function chosen at the beginning of the algorithm? Thanks in advance!\n    ", "Answer": "\r\nNo, the hash function is not chosen separately for each inserted element -- if it were, you would need a way of knowing which hash function to use if someone asked you \"does the element ```\nfoo```\n exist in the hash table\"? This would be done with a deterministic algorithm by necessity, since you cannot possibly maintain a randomized 1-to-1 mapping between possible inputs and hash functions. And that in turn means that an attacker could use knowledge of this algorithm to choose inputs that ultimately result in collisions, effectively undoing the advantage of universal hashing.\n\nSo: the hash function is chosen from the universal family when initializing the hash table, and it is possible (although not necessary) to change it whenever a rehash occurs -- but not after adding or inserting single elements.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "universal hash function implementation for bloom filters in C\r\n                \r\nim simulating set intersection approximation using bloom filters. i have tried a lot of simple hash functions to hash the values to the filter. but its not good at avoiding collisions. so somebody suggested a universal hash function. but im not sure of how it works. my program is designed to pass just the key to the hash function and the hash function returns the hash. can anyone help me with the code? \nthanks\n    ", "Answer": "\r\ndon't worry about collision of hash functions when used with bloom filters. you don't have to handle collision in this case. just get k different has functions which set k bits in an array of m-bits when you are inserting an element. at the time of query, you again use all k hash functions to check all the k-bits; if any one of them is not set then the search is false. if all of them is set, you can't conclude anything (false positive results). This is clearly explained in wiki:\n\nhttp://en.wikipedia.org/wiki/Bloom_filter\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to get a family of independent universal hash function?\r\n                \r\nI am trying to implement the hyperloglog counting algorithm using stochastic averaging. To do that, I need many independent universal hash functions to hash items in different substreams.\n\nI found that there are only a few hash function available in hashlib\nand there seems to be no way for me to provide a seed or something? I am thinking using different salts for different substreams.\n    ", "Answer": "\r\nYou probably DON'T need different hash functions.  A common solution to this problem is to use only part of the hash to compute the HyperLogLog rho statistic, and the other part to select the substream.  If you use a good hash function (e.g. murmur3), it effectively behaves as multiple independent ones.\n\nSee the \"stochastic averaging\" section here for an explanation of this:\nhttps://research.neustar.biz/2012/10/25/sketch-of-the-day-hyperloglog-cornerstone-of-a-big-data-infrastructure/\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Very fast universal hash function for 128 bit keys\r\n                \r\nI need a very fast universal hash function for a 128-bit key. The returned value needs to be about 32 bit (well, 16 bit would be sufficient; in most cases I only need 1-4 bits actually). \n\nUniversal hash means, there are two parameters: key (128 bit) and index (64 bit). For two keys, the universal hash function needs to return different result eventually, if called with different indexes. So with a different index, the universal hash should behave like a different hash function. For ```\nx = universalHash(k, i)```\n and ```\ny = universalHash(k, i + 1)```\n, it would be best if on average 50% of all bits are different between x and y (randomly). The same for the case if the method is called with different keys. In practise, 5% off is OK for me.\n\nIt needs to be very fast (one or two multiplications at most). It is called millions of times. Please don't say: no, you won't need it to be fast. It also needs to return different values eventually.\n\nWhat I have so far (Java code, but C is (due to the lack of a 128 bit data type, the key is the composite of a and b, which are 64 bit each):\n\n```\nint universalHash(long a, long b, long index) {\n    long x = a ^ Long.rotateLeft(b, (int) index) ^ index;\n    int y = (int) ((x >>> 32) ^ x);\n    y = ((y >>> 16) ^ y) * 0x45d9f3b;\n    y = ((y >>> 16) ^ y) * 0x45d9f3b;\n    y = (y >>> 16) ^ y;\n    return y;\n}\n\nint universalHash2(long a, long b, long index) {\n    long x = Long.rotateLeft(a, (int) index) ^ \n            Long.rotateRight(b, (int) index) ^ index;\n    x = (x ^ (x >>> 32)) * 0xbf58476d1ce4e5b9L;\n    return (int) ((x >>> 32) ^ x);\n}\n```\n\n\n(The second method is actually broken for some values.)\n\nI would like to have a hash function that is faster than those above, and is guaranteed to work in all cases (if possible provably correct, even thought that's not a strict requirement; it doesn't need to be cryptographically secure however).\n\nI will call the universalHash method with incrementing index (first index 0, then index 1, and so on) for the same keys. It would be best if the next result could be calculated faster (e.g. without multiplication) from the previous result. But I also need to have a fast \"direct access\" if the index is some value (as in the example code).\n\nBackground\n\nThe problem I'm trying to solve is finding a MPHF (minimal perfect hash function) for a relatively small set of keys (up to 16 keys by directly mapping, and up to about 1024 keys by splitting into smaller subsets). For details on the algorithm, see my MinPerf project, specially the RecSplit algorithm. To support set of size 10^12 (like BBHash), I'm trying to internally use 128 bit signatures, which would simplify the algorithm.\n    ", "Answer": "\r\nYou need a hash function that outputs 32 bits for 128 bits of inputs. \n\nA simple way would be to just return \"some\" 32 bits out of the original 128 bits. There are many ways of choosing 32 bits and every choice will have collisions. But the index can decide which 32 bits to choose.\n\n128/32 = 4, so 4 indices are enough to find at least one different bit. \n\n\nFor key 0 you choose the lower most 32 bits\nFor key 1 you choose the next 32 bits \nand so on .. \n\n\nThe C implementation would be \n\n```\nuint32_t universal_hash(uint64_t key_higher, uint64_t key_lower, int index) {\n    // For a lack of portable 128 bit datatype we take the key in parts.\n    return 0xFFFFFFFF & ( index >=2 ? key_higher >> ((index - 2)*32) : key_lower >> (index*32));\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Expected bounds on universal hashing\r\n                \r\nI am sure this is a simple problem to work out, but I don't see an obvious solution... If I have a hash table with m bins and hash into this n < m keys, what is the probability that no bin receives more than k hash-keys. I'm trying to figure out how many rehash operations I should expect if I fill up a table to load n / m and then rehash until I see no more than k collisions in any bin (obviously with k > n / m).\n    ", "Answer": "\r\nWith uniform distribution, this is the same as throwing balls into bins, which has been studied in \"Balls into Bins - A Simple and Tight Analysis\" from M. Raab and A. Steger. \n\nThis is a bit related to cuckoo hashing, but here you just use one hash function.\n\nAs this is stackoverflow.com, I give you a simulation program that can be used to verify your formula. According to this, it also depends on the number of balls / buckets, and not just on the the average number of balls per bucket.\n\n```\npublic static void main(String... args) throws InterruptedException {\n    for (int k = 1; k < 4; k++) {\n        test(10, 30, k);\n        test(100, 300, k);\n    }\n}\n\npublic static void test(int ballCount, int binCount, int k) {\n    int rehashCount = 0;\n    Random r = new Random(1);\n    int testCount = 100000000 / ballCount;\n    for(int test = 0; test < testCount; test++) {\n        long[] balls = new long[ballCount];\n        int[] bins = new int[binCount];\n        for (int i = 0; i < ballCount; i++) {\n            balls[i] = r.nextLong();\n        }\n        // it's very unlikely to get duplicates, but test\n        Arrays.sort(balls);\n        for (int i = 1; i < ballCount; i++) {\n            if (balls[i - 1] == balls[i]) {\n                throw new AssertionError();\n            }\n        }\n        int universalHashId = 0;\n        boolean rehashNeeded = false;\n        for (int i = 0; i < ballCount; i++) {\n            long x = balls[i];\n            // might as well do y = x\n            long y = supplementalHashWeyl(x, universalHashId);\n            int binId = reduce((int) y, binCount);\n            if (++bins[binId] > k) {\n                rehashNeeded = true;\n                break;\n            }\n        }\n        if (rehashNeeded) {\n            rehashCount++;\n        }\n    }\n    System.out.println(\"balls: \" + ballCount + \" bins: \" + binCount +\n            \" k: \" + k + \" rehash probability: \" + (double) rehashCount / testCount);\n}\n\npublic static int reduce(int hash, int n) {\n    // http://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/\n    return (int) (((hash & 0xffffffffL) * n) >>> 32);\n}\n\npublic static int supplementalHashWeyl(long hash, long index) {\n    long x = hash + (index * 0xbf58476d1ce4e5b9L);\n    x = (x ^ (x >>> 32)) * 0xbf58476d1ce4e5b9L;\n    x = ((x >>> 32) ^ x);\n    return (int) x;\n}\n```\n\n\nOutputs:\n\n```\nballs: 10 bins: 30 k: 1 rehash probability: 0.8153816\nballs: 100 bins: 300 k: 1 rehash probability: 1.0\nballs: 10 bins: 30 k: 2 rehash probability: 0.1098305\nballs: 100 bins: 300 k: 2 rehash probability: 0.777381\nballs: 10 bins: 30 k: 3 rehash probability: 0.0066018\nballs: 100 bins: 300 k: 3 rehash probability: 0.107309\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Notation from some text about Universal Hashing\r\n                \r\nHere is a quote from some lecture on teh topic. I do not understand this part ```\nh : {1,...,M} -> {0,...,m-1}```\n (the notation). Could someone please explain what it means? E.g. \"a hash function h selected from M hash function, which returns values between 1 and m-1\"??\n\nThanks.\n\n\n  Hashing\n  \n  We assume that all the basics about hash tables have been covered in 61B.\n  \n  We will make the simplifying assumption that the keys that we want to hash have been\n  encoded as integers, and that such integers are in the range ```\n{1,...,M}```\n. We also assume that\n  collisions are handled using linked lists.\n  \n  Suppose that we are using a table of size m, that we have selected a hash function\n  ```\nh : {1,...,M} -> {0,...,m-1}```\n and that, at some point, the keys ```\nY1,...,Yn```\n have been\n  inserted in the data structure, and that we want to find, or insert, or delete, the key x.\n  The running time of such operation will be a big-Oh of the number of elements Yi such that\n  ```\nh(yi) = h(x)```\n.\n  \n  ...........\n  \n  ...........\n\n\nSource: www.cs.berkeley.edu/~luca/cs170/notes/lecture9.pdf\n    ", "Answer": "\r\nIt says: h is a function from the input set {1,...,M} to the target set {0,...,m-1}\nMore specifically it doesn't say how the function is formed.\nIt simply says that it deals with certain range of inputs and some other range of outputs and that it exists.  \n\nEDIT: it's a function, not a relation.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "What hashing method is implemented in standard unordered containers? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nSince language standards rarely mandate implementation methods, I'd like to know what is the real world hashing method used by C++ standard library implementations (libc++, libstdc++ and dinkumware).\n\nIn case it's not clear, I expect the answer to be a method like these : \n\n\nHashing with chaining\nHashing by Division / Multiplication\nUniversal hashing \nPerfect hashing (static, dynamic)\nHashing with open addressing (linear/quadratic probing or double hashing)\nRobin-Hood hashing\nBloom Filters\nCuckoo hashing\n\n\nKnowing why a particular method was chosen over the others would be a good thing as well. \n    ", "Answer": "\r\n\nlibstdc++: Chaining, only power-of-two table size, default (if it is even configurable) load threshold for rehashing is 1.0, buckets are all separate allocations. Outdated. I don't know current state of things.\nRust: Robin Hood, default load threshold for rehashing is 0.9 (too much for open addressing, BTW)\nGo: table slots point to \"bins\" of 5(7?) slots, not sure what happens if bin is full, AFAIR it is growing in a ```\nvector```\n/```\nArrayList```\n manner\nJava: chaining, only power-of-two table size, default load threshold is 0.75 (configurable), buckets (called entries) are all separate allocations.  In recent versions of Java, above a certain threshold, chains are changed to binary search trees.\nC#: chaining, buckets are allocated from a flat array of bucket structures. If this array is full, it is rehashed (with the table, I suppose) in a ```\nvector```\n/```\nArrayList```\n manner.\nPython: open addressing, with own unique collision-resolution scheme (not very fortunate, IMHO), only power-of-two table sizes, load threshold for rehashing is 0.666.. (good). However, slot data in a separate array of structures (like in C#), i. e. hash table operations touch at least two different random memory locations (in the table and in the array of slot data)\n\n\nIf some points missed in descriptions, it doesn't mean they are absent, it means I don't know/remember details.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Universal Hashfunctions for string\r\n                \r\nI am trying to implement two different universal hash functions for strings.\nBut I have the problem that sometimes the hash value is 0.\nWith this I can´t use the hash function because I want to implement double hashing and have to implement this function: hash_func1(string s) + i * hash_func2(string s) to go through the hash table.\nBut if one hash function is 0 nothing changes and I get an endless loop.\nThis is for collision detection in a hash table.\nI need two different universal hash functions for doing that.\n\nI have tried different hash functions but cant find anything that works.\n\nCan anyone help me with this problem?\n\nThis are some of the functions I have tried.\n\n```\nint h = 0 , r1 = 31415 , r2 = 27183;\n for (int i =0; i < key.length (); i ++) {\n h = ( r1 * h + key.charAt ( i )) % capacity ;\n r1 = r1 * r2 % (capacity -1);\n}\nreturn h ;\n```\n\n\nOr this one\n\n```\nint seed = 131; \nlong hash = 0;\nfor(int i = 0; i < key.length(); i++)\n{\nhash = (hash * seed) + key.charAt(i);\n}\nreturn (int) (hash % capacity);\n```\n\n    ", "Answer": "\r\nThe wikipedia article on double hashing suggests that you modify your hash function to avoid that it becomes zero, the easiest way to do that being to simply add ```\n1```\n:\n\n```\nint h1 = hash_func1(s);\nint h2 = (hash_func2(s) % (capacity - 1)) + 1;\n\n// loop over (h1 + i * h2) % capacity\n```\n\n\nEDIT: Oops, I guess you also need to bound it by ```\ncapacity - 1```\n, otherwise with ```\nh2 == capacity```\n, you would still run into an endless loop...\nOr, even better, have ```\nhash_func2()```\n already return a value less than ```\ncapacity - 1```\n, then adding ```\n1```\n is sufficient.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to generate 64 bit random numbers?\r\n                \r\nI'm implementing universal hashing and using the following universal hash function :\n\n\n  h(k)=((A*k)mod 2^64) rsh 64-r\n\n\nwhere A is a random number between \n\n\n  2^61 and 2^62.\n\n\nThe ```\nrand()```\n function in C++ has return type integer and it can't generate that big numbers. So how can i generate random numbers in this range? (numbers should be very random i.e. every number should have equal probability to be selected)\n\nNote: \n\n```\nlong long int random=rand();\n```\n\n\ndoesn't work as the number returned by ```\nrand```\n is int.\n    ", "Answer": "\r\nIn C++11 you can use the random header and std::uniform_int_distribution along with a 64-bit instance of std::mersenne_twister_engine this should do what you want (see it live):\n\n```\n#include <iostream>\n#include <random>\n#include <cmath>\n\nint main()\n{\n    std::random_device rd;\n\n    std::mt19937_64 e2(rd());\n\n    std::uniform_int_distribution<long long int> dist(std::llround(std::pow(2,61)), std::llround(std::pow(2,62)));\n\n    std::cout << std::llround(std::pow(2,61)) << std::endl; \n    std::cout << std::llround(std::pow(2,62)) << std::endl; \n\n    for (int n = 0; n < 10; ++n) {\n            std::cout << dist(e2)<< \", \" ;\n    }\n    std::cout << std::endl ;\n}\n```\n\n\nIf C++11 is not an option then it seems there is source code available for several 64-bit Mersenne Twister implementations.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "For hashing, what happens to number of empty slots when n tends to inifinity?\r\n                \r\nUnder universal hashing assumption, if i have hash table of size m=cn, c>0, and as n tends to infinity, what does the number of empty slots tend to? \n\nI'm a bit stuck on how to do this because m is a function of n...(the answer I get for different values of m too) is always tending to infinity and I'm not exactly sure if that is accurate...\n    ", "Answer": "\r\nTake a specific cell i. The probability that all keys missed it is \n\n((m - 1) / m)n = (1 - 1/m)n = (1 - 1/m)m/c ~ e-1/c. \n\n(For the last approximation, see representations of e.)\n\nThe event that all keys missed some other specific cell j is not independent, but by linearity of expectation, that doesn't matter. The expected number of empty bins will be m multiplied by the previous expression. \n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "SQL bigint hash to match c# int64 hash [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has an answer here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        SQL Server varbinary bigint with BitConverter.ToInt64 values are different\r\n                            \r\n                                (1 answer)\r\n                            \r\n                    \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI am trying to create a universal hashing alogrithim that hashes a string as a 64 bit int.\n\nI am able to hash the strings correctly:\nsql:\n\n```\nselect  \n    convert\n    (\n        varchar(64),\n        HASHBYTES\n        (\n            'SHA1',\n            'google.com'\n        ),\n        2\n    )\n```\n\n\nreturns ```\nBAEA954B95731C68AE6E45BD1E252EB4560CDC45```\n\n\nC#\n\n```\n    System.Security.Cryptography.SHA1 c = System.Security.Cryptography.SHA1.Create();\n    System.Text.StringBuilder sb = new StringBuilder();\n    byte[] b = c.ComputeHash(Encoding.UTF8.GetBytes(\"google.com\"));\n    for (int i = 0; i < b.Length;i++ )\n    {\n        byte by = b[i];\n        sb.Append(by.ToString(\"x2\").ToUpper());\n    }\n\n    return sb.ToString();\n```\n\n\nretruns ```\nBAEA954B95731C68AE6E45BD1E252EB4560CDC45```\n\n\nHowever when I convert to a bigint/long the values do not match:\nsql:\n\n```\nselect  \n    convert\n    (\n        bigint,\n        HASHBYTES\n        (\n            'SHA1',\n            'google.com'\n        )\n    )\n```\n\n\nreturns ```\n2172193747348806725```\n\n\nc#:\n\n```\n    System.Security.Cryptography.SHA1 c = System.Security.Cryptography.SHA1.Create();\n    byte[] b = c.ComputeHash(Encoding.UTF8.GetBytes(\"google.com\"));\n    return BitConverter.ToInt64(b, 0);\n```\n\n\nreturns ```\n7501998164347841210```\n\n\nAny ideas on how to get these numbers to match?\n    ", "Answer": "\r\nYour SQL bigint takes the last 8 bytes while the c# implementation takes the first 8 bytes (and reverses them because its running on little endian).\n\nTake the proper Range of the array in C# and reverse it. Then you should be fine.\n\nDid some coding:\n\n```\nSystem.Security.Cryptography.SHA1 c = System.Security.Cryptography.SHA1.Create();\nbyte[] b = c.ComputeHash(Encoding.UTF8.GetBytes(\"google.com\"));\nlong value = BitConverter.ToInt64(b, 12);\nvalue = IPAddress.HostToNetworkOrder(value);\n\nDebug.WriteLine(value);\n// writes 2172193747348806725\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Are there any SHA-256 javascript implementations that are generally considered trustworthy? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed last year.\r\n                    \r\n                \r\n            The community reviewed whether to reopen this question last year and left it closed:\r\n            \r\n                    Original close reason(s) were not resolved\r\n            \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI am writing a login for a forum, and need to hash the password client side in javascript before sending it on to the server. I'm having trouble figuring out which SHA-256 implementation I can actually trust. I was expecting there to be some kind of authoritative script that everyone used, but I'm finding loads of different projects all with their own implementations.\n\nI realize using other people's crypto is always a leap of faith unless you're qualified to review it yourself, and that there is no universal definition of \"trustworthy\", but this seems like something common and important enough that there ought to be some kind of consensus on what to use. Am I just naive?\n\nEdit since it comes up a lot in the comments: Yes, we do a more stringent hash again on the server side. The client side hashing is not the final result that we save in the database. The client side hashing is because the human client requests it. They have not given a specific reason why, probably they just like overkill.\n    ", "Answer": "\r\nOn https://developer.mozilla.org/en-US/docs/Web/API/SubtleCrypto/digest I found this snippet that uses internal js module:\n```\nasync function sha256(message) {\n    // encode as UTF-8\n    const msgBuffer = new TextEncoder().encode(message);                    \n\n    // hash the message\n    const hashBuffer = await crypto.subtle.digest('SHA-256', msgBuffer);\n\n    // convert ArrayBuffer to Array\n    const hashArray = Array.from(new Uint8Array(hashBuffer));\n\n    // convert bytes to hex string                  \n    const hashHex = hashArray.map(b => b.toString(16).padStart(2, '0')).join('');\n    return hashHex;\n}\n```\n\nNote that ```\ncrypto.subtle```\n in only available on ```\nhttps```\n or ```\nlocalhost```\n - for example for your local development with ```\npython3 -m http.server```\n you need to add this line to your ```\n/etc/hosts```\n:\n```\n0.0.0.0 localhost```\n\nReboot - and you can open ```\nlocalhost:8000```\n with working ```\ncrypto.subtle```\n.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "parse uniVerse hash / data files in R\r\n                \r\nI have inherited a uniVerse database (link to Rocketsoftware site) and would like to know if it's possible to read/parse the underlying data files (which I believe are hash tables?) into 'R'?\n\nI'm aware there are ```\nODBC```\n drivers as well as ```\n.NET```\n libraries, but I'm interested in parsing the files in ```\nR```\n (if possible) without these drivers?\n\n(I've searched and seen a few topics on parsing hash tables in Java and C#, but nothing in R yet)\n    ", "Answer": "\r\nIt's a propriety format, so unless you want to reverse engineer it and re-implement in R that isn't the path forward. Also note that it isn't a single hash-table format either, aside from the standard modulo and bucket sizes, there are several different formats you'll encounter.\n\nIf you don't want work with any of the native APIs of the database to read the data, you can issue database commands that will dump it to CSV or XML flat files. Take a look into the RetrieVe query language manuals to learn more.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Choose the universe of keys\r\n                \r\nI need to hash a sequence S of numbers of length n^2 where each number is a sum of two numbers each of them is an element on one of the sequences: ```\n{x_1,..., x_n},{y_1,..., y_n}```\n.\n\nI am using universal hashing so the question is how do I find the universe of keys U, if there are infinitely many possibilities for members of S.\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is it better to build a hash table from an existing array than to create a hash table first then insert all the elements?\r\n                \r\nAre there any implementations which would pick several hash functions in a universal hashing and try these functions to reduce the total collisions to an acceptable level and return the best result with the least collisions ?\n\nIf there is ,building a hash table from an existing array is much reliable than creating a hash table first then insert all the elements ,isn't it ?\n\nThe following paragraphs are from introduction to algorithms.\n\n\"If a malicious adversary chooses the keys to be hashed by some fixed hash function,then the adversary can choose n keys that all hash to the same slot, yielding an average retrieval time of ‚.n/. Any fixed hash function is vulnerable to such terrible worst-case behavior; the only effective way to improve the situation is to choose the hash function randomly in a way that is independent of the keys that are actually going to be stored. This approach, called universal hashing, can yield provably good performance on average, no matter which keys the adversary chooses.\n\nIn universal hashing, at the beginning of execution we select the hash function\nat random from a carefully designed class of functions. As in the case of quicksort,randomization guarantees that no single input will always evoke worst-case behavior. Because we randomly select the hash function, the algorithm can behave differently on each execution, even for the same input, guaranteeing good\naverage-case performance for any input. Returning to the example of a compiler’s\nsymbol table, we find that the programmer’s choice of identifiers cannot now cause consistently poor hashing performance. Poor performance occurs only when the compiler chooses a random hash function that causes the set of identifiers to hash poorly, but the probability of this situation occurring is small and is the same for any set of identifiers of the same size.\"\n    ", "Answer": "\r\nIf you know the keys in advance, you can use perfect hashing to avoid any collisions. So, if you have all the elements somewhere (as in your example, in an array), and there won't be new inserts, then sure, you can do a lot better.\n\nThe thing is, in real apps, keys usually come and go. The table is constantly changing.\n\nI don't know about implementations, but as always it boils down to trade-offs. You're trying to trade extra safety for quick lookups, and you'll pay with extra code complexity and slowdown and a potentially costly insert that will recreate the hash when there are lot of collisions. But do you really need that safety? And if you have a lot of collisions, why don't you simply increase the size of the table?\n\n\n  reduce the total collisions to an acceptable level\n\n\nThe chances of a lot of collisions are really really small (with a good implementation that keeps the table not to dense) and you already defended the algorithm against malicious inputs (as the attacker doesn't know how to abuse keys). For real life applications this is already way better than an \"acceptable level\".\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "hashing algorithms for strings\r\n                \r\nI am reading hashing by Algorithms in C++ by Robert Sedwick\n\nThis implementation of a hash function for string keys involves one multiplication and one addition per character in the key. If we were to replace the constant 127 by 128, the program would simply compute the remainder when the number corresponding to the 7-bit ASCII representation of the key was divided by the table size, using Horner's method. The prime base 127 helps us to avoid anomalies if the table size is a power of 2 or a multiple of 2.\n\n```\nint hash(char *v, int M)\n    { int h = 0, a = 127;\n      for (; *v != 0; v++)\n        h = (a*h + *v) % M;\n      return h;\n    }\n```\n\n\nA theoretically ideal universal hash function is one for which the chance of a collision between two distinct keys in a table of size M is precisely 1/M. It is possible to prove that using a sequence of different random values, instead of a fixed arbitrary value, for the coefficient a in Program 14.1 turns modular hashing into a universal hash function. However, the cost of generating a new random number for each character in the key is likely to be prohibitive. Program 14.2 demonstrates a practical compromise: We vary the coefficients by generating a simple pseudorandom sequence.\n\nIn summary, to use hashing for an abstract symbol-table implementation, the first step is to extend the abstract type interface to include a hash operation that maps keys into nonnegative integers less than M, the table size. The direct implementation\n\ninline int hash(Key v, int M)\n  { return (int) M*(v-s)/; }\n\ndoes the job for floating-point keys between the values s and t; for integer keys, we can simply return v % M. If M is not prime, the hash function might return\n\n```\n(int) (.616161 * (float) v) % M\n```\n\n\nor the result of a similar integer computation such as\n\n```\n(16161 * (unsigned) v) % M.\n```\n\n\nMy question how author mean prime base 127 helpus us to avoid anomalies if the table size is power of 2 or a multiple of 2?\n\n\nWhat does authorm mean by \"for integer keys, we can simply return v % M.\"?\nWhat does author mean by \"If M is not prime, the hash function might return\n\n\n(int) (.616161 * (float) v) % M\n\nor the result of a similar integer computation such as\n\n(16161 * (unsigned) v) % M.\n\nHow author came up with .616161 and 16161?\n\nRequest help to understand with simple example\n    ", "Answer": "\r\nFirst, the base has to be co-prime with the word size, otherwise the first characters of the string do not contribute to the hash value;\n\ne.g.  a=256;  strings \"AABA\", \"AAABA\", \"XYZAABA\" will all produce the same hash, since the intermediate variable h is calculated modulo 2^32.\n\nIt's slightly more complex when only 7 bits are used per character, but the principle holds.\n\n\nFor integer keys, 0<= v <2^32, M = table_size (M is prime), v % M maps the complete range of v approximately uniformly taking account all bits in v.\nMultiplying the integer v by a float 0.616161 before taking the modulo M, guarantees somekind of scrambling of the bits in v. Otherwise, just as in the first example, having M=100, would map all the values 1001, 101, 201, 3412341234101 to the same slot, ignoring up to 90% of the bits of the variable.\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to create a universal identity-based hashing dict?\r\n                \r\nContext\nI want to implement a language on top of python where I need to add meta-data to existing python objects, but I don't want to modify the underlying representation of all python objects. I can compute and store the id of EVERY object, then use as key for a traditional Python dict. but this is an incredible waste, since those IDs are very quickly derivable from the objects themselves.  Other programming languages (Lisp, Java, etc.) have an \"identity hash\" which hashes based on the identity of the key object with ever calling its hash function.\n\nCan I do this in Python without without writing my own hash from scratch?\n    ", "Answer": "\r\n@Paritosh said it plainly: just add use the ```\nid```\n as a key:\n\n```\nmydict[id(myobj)] = myvalue\n```\n\n\nor the hash of the id: \n\n```\nmydict[hash(id(myobj))] = myvalue #Note you could just hash(myobj) for many objects.\n```\n\n\nYou could wrap this up in some class, but that seems like over kill to me, though it depends on your exact use case. This is not a true hash map though - so collisions may happen and override existing objects. You can add logic to check if the key exists, and then create a list there:\n\n```\nnewhash = hash(id(myobj))\nif newhash in mydict: #O(1)\n    try: mydict[newhash].append(myvalue)\n    except AttributeError: mydict[newhash] = [mydict[newhash],myvalue]\nelse: mydict[newhash] = myvalue\n```\n\n\nor just use a ```\ndefaultdict```\n. This may be a better candidate for a new class.\n\n@Patrick raises the concern that what you want to do may make no sense - you would have no idea how to reference the object, without already knowing what ```\nid```\n the object is associated with. If you keep some list of ids you added it may make some sense, again depending on use case.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is Universal family of hash functions only to prevent enemy attack?\r\n                \r\nIf my intention is only to have a good hash function that spreads data evenly into all of the buckets, then I need not come up with a family of hash functions, I could just do with one good hash function, is that correct? \n\nThe purpose of having a family of hash functions is only to make it harder for the enemy to build a pathological data set as when we pick a hash function randomly, he/she has no information about which hash function is employed. Is my understanding right?\n\nEDIT:\nSince someone is trying to close as unclear; This question is to know the real purpose of employing a Universal family of hash functions.\n    ", "Answer": "\r\n\n  I could just do with one good hash function, is that correct?\n\n\nAs you note later in your question, an \"enemy\" who knows which hash function you're using could prepare a pathological data set.\n\nFurther, hashing is just the first stage in storing data into your table's buckets - if you're implementing open addressing / closed hashing, you also need to select alternative buckets to probe after collisions: simple approaches like linear and quadratic probing generally provide adequate collision avoidance, and are likely mathematically simpler and therefore faster than rehashing, but they don't maintain a probability of the next probe finding an unused bucket at the load factor.  Rehashing with another good hash function (including another from a family of such functions) does, so if that's important to you you may prefer to use a family of hash functions.\n\nNote too that sometimes an in-memory hash table is used to say at which offsets/sectors on disk data is stored, so extra rehashing calculations with already-in-memory data may be far more appealing than a higher probability (with linear/quadratic probing) of waiting on disk I/O only to find another collision.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to find the value for a given key if a hash function is chosen randomly from Universal family of hash functions?\r\n                \r\nI am taking a course on data structures in coursera and I read recently about Universal family of hash functions. If i choose a hash function randomly from a universal family of hash functions, How will i exactly remap it to look up for a value. If i have to remember the function chosen for each key, then i should maintain a list for it. And this evaluation of finding the correct hash function for a key itself will take linear time violating the constant time look up of  hash tables. How should i proceed implementing it?\n    ", "Answer": "\r\nWhen making one hash map, you use one function from the family. When you rehash the entire map (typically because of lack of capacity or too many collisions) or create a separate map, you can then choose a different hashing function from the family. You wouldn't use two different functions to attempt to create the same hash map.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Java Library providing perfect hashing? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 6 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI've done some searching and found a few useful posts about supporting perfect (i.e. no collision) hashing in Java.\n\nWhy doesn't Java's hashCode support universal hashing?\n\nIs it possible in java make something like Comparator but for implementing custom equals() and hashCode()\n\nBut I am looking for a practical solution, hopefully in the form of a tested library.  I have a situation which is suitable for perfect hashing: essentially, we can assume that the set of keys is fixed, and the program runs for a long time and does a lot of lookups.  (This is not exactly true, but keys are added rarely enough that it is a close enough approximation, and if I have to re-hash periodically or something to deal with that, that's OK).\n\nBasically, I would like to be able to increase the load factor and also reduce collisions.  In other words, the objectives are to reduce memory use, and increase throughput (i.e. number of lookups per second).\n\nThere are some issues.  Obviously, there is the problem that if ```\nhashCode()```\n doesn't return distinct values, then perfect hashing is impossible.  And there are other considerations besides the hashing algorithm, like the complexity of ```\nhashCode()```\n (or whether I should cache the hashcodes on the key objects, etc) or whatever other function I use to initially map my objects to integers or longs.\n\nWhat I am envisioning is being able to re-hash in a background thread, trying different hash functions to find a perfect one or at least a good one.  I'm open to another solution though.  And I would like to use tested code rather than write it myself, though I am open to that too.\n    ", "Answer": "\r\nYou don't need perfect hashing if your data is sufficiently random. Mitzenmacher has a neat paper explaining both why perfect hashing is hard in practice, and why it's (usually) unnecessary in practice. I'll give you a link and paste in the header so you can find it if the link vanishes.\n\nhttp://people.seas.harvard.edu/~salil/research/streamhash-Jun10.pdf\n\nWhy Simple Hash Functions Work:\nExploiting the Entropy in a Data Stream\n\nMichael Mitzenmacher Salil Vadhan\nSchool of Engineering & Applied Sciences\n\nJune 23, 2010\n\nHashing is fundamental to many algorithms and data structures widely used in practice. For theoretical analysis of hashing, there have been two main approaches. First, one can assume that the hash function is truly random, mapping each data item independently and uniformly to the range. This idealized model is unrealistic because a truly random hash function requires an exponential number of bits to describe. Alternatively, one can provide rigorous bounds on performance when explicit families of hash functions are used, such as 2-universal or O(1)-wise independent families. For such families, performance guarantees are often noticeably weaker than for ideal hashing.\n\nIn practice, however, it is commonly observed that simple hash functions, including 2-universal hash functions, perform as predicted by the idealized analysis for truly random hash functions. In this paper, we try to explain this phenomenon. We demonstrate that the strong performance of universal hash functions in practice can arise naturally from a combination of the randomness of the hash function and the data. Specifically, following the large body of literature on random sources and randomness extraction, we model the data as coming from a “block source,” whereby each new data item has some “entropy” given the previous ones. As long as the (Renyi) entropy per data item is sufficiently large, it turns out that the performance when choosing a hash function from a 2-universal family is essentially the same as for a truly random hash function. We describe results for several sample applications, including linear probing, balanced allocations, and Bloom filters.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "open addressing vs chained hashing\r\n                \r\nOpen addressing is usually faster than chained hashing. I am testing my code with successful researches with a low load factor (0.1) but I keep getting best time results for the chained hashing instead of the open addressing. The difference is very very little and sometimes open address is even faster but on an average of 100 inserts the time is better with chained hasing.\n\nI used a ```\nvector<string>```\n as hash table for open addressing and a ```\nvector<list<string> >```\n for chained hasing and universal hasing for both.\n\nThis is the part of the code about successful researches that I am timing:\n\nChained hashing research:\n\n```\nfor (auto it_ric = this->hash[key].begin(); it_ric != this->hash[key].end(); it_ric++)\n{\n    if ((*it_ric) == string)\n    {\n        found = true;\n    }\n}\n```\n\n\nOpen addressing:\n\n```\nwhile (found == false && key != hash.size())\n{\n    if (hash[key] == string)\n    {\n        key = hash.size();\n        found = true;\n    }\n    else if (hash[key] == \"\")\n    {\n        key = hash.size();\n        found = true;\n    }\n    else\n    {\n        key++;\n    }\n}\n```\n\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Why is encrypting necessary for security after hashing in the UMAC (Universal Message Authentication Code) algorithm?\r\n                \r\nOn the Wikipedia for UMAC, https://en.wikipedia.org/wiki/UMAC, it states:\n\n\n  The resulting digest or fingerprint is then encrypted to hide the\n  identity of the hash function used.\n\n\nFurther, in this paper, http://web.cs.ucdavis.edu/~rogaway/papers/umac-full.pdf, it states:\n\n\n  A message is authenticated by hashing it with the shared hash function\n  and then encrypting the resulting hash (using the encryption key).\n\n\nMy question is, if the set of hash functions ```\nH```\n is large enough, and the number of hash buckets ```\n|B|```\n is large enough, why do we need to encrypt -- isn't the secret hash secure enough?\n\nFor example, take the worst case scenario where every client is sending the same, short content, like \"x\". If we hash to 32 bytes and our hash depends on a secret 32 byte hash key, and the hashes exhibit uniform properties, how could an attacker ever hope to learn the secret hash key of any individual client, even without encryption?\n\nAnd, if the attacker doesn't learn the key, how could the attacker ever hope to maliciously alter the message contents?\n\nThank you!\n    ", "Answer": "\r\nI don't know much about UMAC specifically but:\n\n\nHaving a rainbow table for a specific hash function defeats any encryption you have put on the message so instead of having a single attack surface, you now have two\nAs computational powers increase with time you will be more and more likely to figure out the plaintext of the message so PFS (https://en.wikipedia.org/wiki/Forward_secrecy) will never be possible if you leave the MAC unencrypted. \nOn top of all this, if you can figure out a single plaintext message from a MAC value, you exponentially get closer to decrypting the rest of the message by getting some information about the PRNG, context of the other data, IV, etc.\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Analysis of open addressing\r\n                \r\nI am currently learning hash tables from \"An introduction of algorithms 3th\". Get quite confused while trying to understand open addressing from statistical point of view. Linear probing and quadratic probing can only generate m possible probe sequence, assuming m is hash table length. However, as defined in open addressing, the possible key value number is greater than the number of hash values, i.e. load factor n/m< 1. In reality, if the hash function is predefined, there exists only n possible probe sequence, which is less than m. The same thing applies to double hashing. If the book says, one hash function is randomly chosen from a set of universal hash functions, then, I can understand. Without introducing randomness in open addressing analysis, the analysis of its performance based on universal hashing is obscured. I have never used hash table in practice, maybe I dive too much into the details. But I also have such doubt in hash table's practical usage:  \n\nQ: In reality, if the load factor is less than 1, why would we bother open addressing ? Why not project each key to an integer and arrange them in an array ? \n    ", "Answer": "\r\n\n  Q: In reality, if the load factor is less than 1, why would we bother open addressing? Why not project each key to an integer and arrange them in an array ? \n\n\nBecause in many situations when hash tables are used, there's no good O(1) way to \"project each key to an [distinct, not-absurdly-sparse] integer\" array index.\n\nA simple thought experiment illustrates this: say you expect the user to type four three-uppercase-letter keys, and you want to store them somewhere in an array with dimension 10.  You have 264 possible inputs, so no matter what your logic is, on average 264/10 of them will \"project... to an integer\" indicating the same array position.  When you realise the \"project[ion]\" can't avoid potential \"collisions\", and that projection is a logically identical operation to \"hashing\" and modding to a \"bucket\", then some collision-handling logic will be needed, your proposed \"alternative\" morphs back into a hash table....\n\n\n  Linear probing and quadratic probing can only generate m possible probe sequence, assuming m is hash table length. However, as defined in open addressing, the possible key value number is greater than the number of hash values, i.e. load factor n/m< 1.\n\n\nThey are very confusing statements.  The \"number of hash values\" is not arbitrarily limited - you could use a 32 bit hash generating any of ~4 billion hash values, a 512-bit hash, or whatever other size you feel like.  Given the structure of your statement is \"a > b, i.e. load factor n/m < 1\", and \"n/m < 1\" can be rewritten as \"n < m\" or \"m > n\", you imply \"a\" and \"m\" are meant to be the same thing, as are \"b\" and \"n\":\n\n\nyou're referring to ```\nm```\n - which \"load factor n/m\" requires be the number of buckets in the hash table - as \"the possible key value number\": it's not, and what could that even mean?\nyou're referring to ```\nn```\n - which \"load factor n/m\" requires be the number of keys stored in the hash table - as \"the number of hash values\": it's not, except in the trivial sense of that many (not necessarily distinct) hash values being generated when the keys are hashed\n\n\n\n  In reality, if the hash function is predefined, there exists only n possible probe sequence, which is less than m.\n\n\nAgain, that's a very poorly defined statement.  The hashing of ```\nn```\n keys can identify at most ```\nn```\n distinct buckets from which collision-handling would kick in, but those ```\nn```\n could begin pretty much anywhere within the ```\nm```\n buckets, given the hash function's job is to spray them around.  And, so what?\n\n\n  The same thing applies to double hashing. If the book says, one hash function is randomly chosen from a set of universal hash functions, then, I can understand.\n\n\nUnderstand what? \n\n\n  Without introducing randomness in open addressing analysis, the analysis of its performance based on universal hashing is obscured.\n\n\nFor sure.  \"Repeatable randomness\" of hashing is a very convenient and tangible benchmark against which specific implementations can be compare.\n\n\n  I have never used hash table in practice, maybe I dive too much into the details. But I also have such doubt in hash table's practical usage:\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "chained hash table keys with universal hasing,does it need a rehash?\r\n                \r\nI am implementing a chained hash table using a vector < lists  >. I resized my vector to a prime number, let's say 5. To choose the key I am using the universal hasing.\n\nMy question is, do I need to rehash my vector? I mean this code will generate always a key in a range between 0 and 5 because it depends from the size of my hashtable, causing collisions of course but the new strings will be all added in the lists of every position in the vector...so it seems I don't need to resize/rehash the whole thing. What do you think? Is this a mistake?\n    ", "Answer": "\r\nYes, you do. Otherwise objects will be in the wrong hash bucket and when you search for them, you won't find them. The whole point of hashing is to make locating an object faster -- that won't work if objects aren't where they're supposed to be.\n\nBy the way, you probably shouldn't be doing this. There are people who have spent years developing efficient hashing algorithms. Trying to roll your own will result in poor performance. Start with the article on linear hashing in Wikipedia.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to serve images in Angular Universal\r\n                \r\nI import my hashed image urls:\n\nComponent:\n\n```\nimport basicSmartLogo from \"../../assets/img/smart_logo_basic.png\"\n\n@Component({\n  selector: \"home\",  // <home></home>\n  styleUrls: [\"./home.component.scss\"],\n  templateUrl: \"./home.component.html\"\n})\nexport class HomeComponent {\n\n  basicSmartLogo = basicSmartLogo;\n}\n```\n\n\nHTML:\n\n```\n<img src=\"{{basicSmartLogo}}\">\n```\n\n\nThis seems to work on in SPA builds but in universal the  urls are very mangled. How do I use hashed resource urls in angular universal?\n    ", "Answer": "\r\nI solved this issue as follows.\n\n\nCreate a ```\nconfiguration.production```\n object in the ```\nserver```\n object in ```\nangular.json```\n as described in https://github.com/angular/angular-cli/issues/10417#issuecomment-390491021.\nAdd ```\n\"outputHashing\": \"media\"```\n to the ```\nconfiguration.production```\n object\nExecute ```\nng run {PROJECT_NAME}:server:production```\n instead of ```\nng run {PROJECT_NAME}:server```\n to build server-side code.\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "why does message authentication using 2-universal family of hash functions require a prime number of possible hash values?\r\n                \r\nI am self-studying the book Intro to Algorithms 3ed by CLRS. One of the problems seems to give a piece of information that is not necessary, Problem 11-4 in the book states\n\n\n  Let H be class of hash functions in which each hash function h ∈ H maps the universe U of keys to {0, 1...m-1}. We say H is k-universal if, for every fixed sequence of k distinct keys {x1, x2...xk} and for any h chosen at random from H, the sequence  is equally likely to be any of the m^k sequences of length k with elements drawn from {0, 1...m-1}\n  \n  part d (previous parts are unrelated): suppose that Alice and Bob secretly agree on a hash function h from a 2-universal family H of hash functions. Each h ∈ H maps from a universe of keys U to Zp = {0, 1, 2...p-1}, where p is prime. Later, Alice sends a message m to Bob over the Internet, where m ∈ H. She authenticates this message to Bob by also sending an authentication tag t = h(m), and Bob checks that the pair (m,t) he receives indeed satisfies t = h(m). Suppose that an adversary intercepts (m,t) en route and tries to fool Bob by replacing the pair (m,t) with a different pair (m',t'). Argue that the probability that the adversary succeeds in fooling Bob into accepting (m', t') is at most 1/p, no matter how much computing power the adversary has, and even if the adversary knows the family of H of hash functions used.\n\n\nSo the best thing the adversary can do is find all h' such that h'(m) = t, then find some m', t' pair that matches as many of those h' as possible. However, since the universe H is 2-universal, that means even if the adversary has narrowed it down to the smaller set of h', for each m', the value of t' is still equally likely to be any of the p possible values. Hence, the probability of the adversary's pair m', t' matching Alice and Bob's function h is exactly 1/p, assuming that the adversary has narrowed it down at first.\n\nHowever, the problem stated that p is prime. In the argument I gave, at no part did I use that fact, so either my argument is wrong, or the problem has made a somewhat unnecessary condition?\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Generating k pairwise independent hash functions\r\n                \r\nI'm trying to implement a Count-Min Sketch algorithm in Scala, and so I need to generate k pairwise independent hash functions.\n\nThis is a lower-level than anything I've ever programmed before, and I don't know much about hash functions except from Algorithms classes, so my question is: how do I generate these k pairwise independent hash functions?\n\nAm I supposed to use a hash function like MD5 or MurmurHash? Do I just generate k hash functions of the form ```\nf(x) = ax + b (mod p)```\n, where p is a prime and a and b are random integers? (i.e., the universal hashing family everyone learns in algorithms 101)\n\nI'm looking more for simplicity than raw speed (e.g., I'll take something 5x slower if it's simpler to implement).\n    ", "Answer": "\r\nScala already has ```\nMurmurHash```\n implemented (it's ```\nscala.util.MurmurHash```\n).  It's very fast and very good at distributing values.  A cryptographic hash is overkill--you'll just take tens or hundreds of times longer than you need to.  Just pick ```\nk```\n different seeds to start with and, since it's nearly cryptographic in quality, you'll get ```\nk```\n largely independent hash codes.  (In 2.10, you should probably switch to using ```\nscala.util.hashing.MurmurHash3```\n; the usage is rather different but you can still do the same thing with mixing.)\n\nIf you only need near values to be mapped to randomly far values this will work; if you want to avoid collisions (i.e. if A and B collide using hash 1 they will probably not also collide using hash 2), then you'll need to go at least one more step and hash not the whole object but subcomponents of it so there's an opportunity for the hashes to start out different.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is client side routing recommended along with the server side routing provided angular-universal\r\n                \r\nI'm just starting out with Angular Universal and completely new to 'isomorphic' javascript.\n\nI'm a little confused as to how to have a logical routing system, are you meant to build out angular universal as a typical node rest api? Or is it literally just meant to serve client side code instantly, and retrieve data from an outside source/rest api?\n\nAs to take the angular universal starter kit for example it starts with these routes;\n\n```\napp.get('/', ngApp);\napp.get('/about', ngApp);\napp.get('/about/*', ngApp);\napp.get('/home', ngApp);\napp.get('/home/*', ngApp);\n```\n\n\nis it then logical to include a node/express/mongo api along side this to get/post data? ie;\n\n```\nconst bearRoutes = require('./routes/bear')\napp.use('/', bearRoutes)\n```\n\n\nIf it is, would you then use a hashing strategy in order to differentiate the URLS?\n\nWould there be a noticeable performance difference between serving data on an angular2 app using universal from an external api compared to having everything hosted together?  \n\nThanks for any advice!\n    ", "Answer": "\r\n\n  I'm a little confused as to how to have a logical routing system, are you meant to build out angular universal as a typical node rest api?\n\n\nYes. ```\nangular-universal```\n takes that philosophy. You build the routes on server side because everything is built out server side.\n\n\n  is it then logical to include a node/express/mongo api along side this to get/post data? \n\n\nYes for your data services.\ne.g. Hello World example with github services makes API requests\nNo for ```\napp.use```\n because this is essentially your Node REST server.\n\n\n  If it is, would you then use a hashing strategy in order to differentiate the URLS?\n\n\nFirst thing is anything after ```\n#```\n in the URL is not going to be seen by the server side. But the problem you describe is about naming the URLs and whether or not to differentiate between the URLs that serves the angular compiled/rendered HTML content vs the data/API services. \n\nAnswer is Yes. This is for your own good in managing your server routes. For example if your routes are \n\n```\napp.get('/', ngApp);\napp.get('/about', ngApp);\napp.get('/about/*', ngApp);\napp.get('/home', ngApp);\napp.get('/home/*', ngApp);\n```\n\n\nyou could just add\n\n```\napp.get('/api/profile', serverAPIAppOrprofileJSON); \n```\n\n\n\n  Would there be a noticeable performance difference between serving data on an angular2 app using universal from an external api compared to having everything hosted together?\n\n\nThink about a scenario where a small section of your page changes due to certain user interaction and that needs you to bring some data. You may want to just update this section instead of rendering the whole page. So performance will be better in this situation. \n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How does the universal hash function that randomly generates an integer perform lookup after insertion?\r\n                \r\nThe following demonstration was taken from the Algorithms: Design and Analysis video lecture on Coursera provided by Tim Roughgarden (phenomenal explanations btw). From what I understand, this hash function effectively returns an existing index position between 0 to n in the hash table. However, I am confounded by how this unique computation can be reverse engineered for constant time look-ups when ```\na```\n (below) is a randomized integer. \n\n```\nLet U = IP addresses (of the form (x1, x2, x3, x4)\n                       with each subscript of x in  {0,1,2,...255})\nLet n = a prime number\nDefine one hash function per 4-tuple a = (a1, a2, a3, a4)\n                         with each subscript of a in {0, 1, 2,...n-1}\nDefine: h<sub>a : IP addresses ---> buckets        *n^4 such functions*\nby h<sub>a(x1,x2,x3,x4) = (a1x1 + a2x2 + a3x3 + a4x4)\n\n<sub> = subscript notation\n```\n\n\nThe above equation is then computed with ```\nmodulo n```\n to return a position within the hash table that has a 1/n probability chance of recurrence for any different inputs within ```\nU```\n. How do I retrieve that IP address location with the original IP address?\n    ", "Answer": "\r\nHashes lose information. So you cannot work backwards from the hash to the original. You store the original at the index given by its hash; you can verify that some value x is in the table by seeing whether the item at H(x) is x.\n\nOf course, that won't work if two objects in the table have the same hash (a \"collision\"). Different hash-table algorithms have different strategies to deal with collisions, and for the strategy your lecturer is probably explaining, it will be useful to have another independent hash function.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Using C++ class as mere bit container\r\n                \r\nI'm currently implementing a HashTable in C++ using universal hashing (matrix hashing). The way I implement the matrix is by making an array of pointers (which are merely random bits, they do not \"work\" as pointers but as a 32x64 bit matrix). In order to hash the key, I multiply the pointer key to the matrix (using bit operations), which makes a 32 bits column (our hashed key). This raises a big question:\n\nIs it possible to use a class (more properly, a C++ string) to fill with random bits and do bit operations? I do not care if the data in the string is pure garbage, I just use it to hash. Or, as an alternative, how can I make a 32-byte type and cast a string into one?\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "identifier \"a\" is undefined in c++\r\n                \r\nI am writing c++ code to implememt universal hash function, but it doesn't recognize the parameters, here is the code:\n\n```\n#include <math.h>\n#include <cmath>\n#include \"hash.h\"\n#include <iostream>\nusing namespace std;\n#include <string>\n\nHASH::HASH(){\n    a=23;\n    b=88;\n    n=100;\n    p=997;\n    products=new product[n];\n    }  \n\nHASH::~HASH(){\n    delete []products;\n\n}  \n\nHASH::HASH(int aa,int bb,int nn, int pp){\n    a=aa;\n    b=bb;\n    n=nn;\n    p=pp;\n    products=new product[n];\n\n}  \n\n\nint HF(int key){\n\n    int index;\n    int h;\n\n   h=((((a*key)+b)% p)% n);\n\n}\n```\n\n\nin HF function,it tells me that the parameters a,b,n,p are undefined. I defined them in the header file like this:\n\n```\nclass HASH {\nprivate: \n    int a,b,n,p;\n    product* products;\n\npublic:\n\n        HASH();\n        HASH(int aa,int bb, int nn, int pp);\n        ~HASH();\n        bool insert(product s);\n        bool retrieve(int id,product &product);\n        bool updateName(int id);\n        bool updateCost(int id);\n        bool updateQuantity(int id);\n        bool remove(product &d);\n        int getNumberOfProducts();\n\n};\nint HF(int key);\n```\n\n    ", "Answer": "\r\nFirst, never define a variable with a single char, that you will use a lot, or in class. If you wanna find it in code, it will be impossible - you will stop one each \"a\" char. Name it something readable.\n\nSecond, HF is not a member of class HASH, it can't see private variables of other classes.\n\n\nAdd to public part of HASH class \nint HF(int key);\nchange procedure to\nint HASH::HF(int key){ ...\nand it will work\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is perfect hash function achieveable?\r\n                \r\nI am working on a universal hash function ```\nh(k)=((a*k+b)mod p)mod m```\n, by picking ```\na```\n and ```\nb```\n randomly using boost library so that a perfect hash function may achieve. I am using chaining technique to detect number of collisions using array of singly linked lists. Loop will keep on going as it will not get ```\ncount_collisions=0;```\n. Here is the piece of code: \n\n```\nvoid hashFunction()\n{\nlong long int m=0;\nlong long int val=0;\ncout<<\"Enter table size: \";\ncin>>m;\nm=m*m;\nlong long int *p=generateDistinctRandomKeys(m);\n\n\nlong long int pno=generateSingleRandomNo(); // prime No 'P'\n\nwhile((pno/1!=pno) && (pno/pno!=1) && (pno<m))\n{\n    if((pno/1==pno) && (pno/pno==1) && (pno>m))\n        break;\n    else\n        pno=generateSingleRandomNo();\n}\n\n//intializing linked list !\n\nint cc=0; \nstartt_:\n\nLinkedList **l=new LinkedList*[m];\nfor(int i=0;i<m;i++)\n{\n    l[i]=new LinkedList;\n\n    //l[i]=NULL;\n}\n\n\ncc++;\nint count_collisions=0;\nlong long int a=generateSingleRandomNo(1,pno-1); \nlong long int b=generateSingleRandomNo(0,pno-1);\nfor(int j=0;j<m;j++)\n{\n    //applying hash function !!!\n\n    val=(((a*p[j])+b)%(pno))%(m);\n    if(val<0)\n        val=val*(-1);\n\n    bool f=l[val]->insert(p[j]);\n    //cout<<val<<endl;\n    //cc=j;\n}\n\nfor(int s=0;s<m;s++)\n{\n    //l[s]->display();\n    count_collisions=count_collisions+l[s]->countCollisions();\n}\nif(count_collisions==0)\n{\n    cout<<\"Perfect function achieved after \"<<cc<<\" Iterations!\"<<endl;\n    cout<<\"Values of a and b are: \"<<a<<\" and \"<<b<<endl;\n    exit(0);\n}\nelse\n{/*\n    if(cc==5)\n    {\n        cout<<\"STOP!!!!!!!!!!!!!\"<<endl;\n        exit(0);\n    }*/\n    //j=0;\n    for(int n=0;n<m;n++)\n    {\n        l[n]->~LinkedList();\n    }\n    delete [] l;\n    l=0;\n    goto startt_ ;\n}\n\n\n\n\n    /*cout<<\"Collisions are:\"<<count_collisions<<endl;*/\n```\n\n\n}\n    ", "Answer": "\r\nPerect hash functions are usually constructed, not \"discovered by chance\".\n\nOnly few things lend themselves for the \"Monte Carlo\" approach (which means, rolling the dice often enough to arrive at a good approximation).\n\nThe suggestion to use gperf is excellent, see e.g. \n\n\nIs it possible to map string to int faster than using hashmap?\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Why are hash table expansions usually done by doubling the size?\r\n                \r\nI've done a little research on hash tables, and I keep running across the rule of thumb that when there are a certain number of entries (either max or via a load factor like 75%) the hash table should be expanded.\n\nAlmost always, the recommendation is to double (or double plus 1, i.e., 2n+1) the size of the hash table.  However, I haven't been able to find a good reason for this.\n\nWhy double the size, rather than, say, increasing it 25%, or increasing it to the size of the next prime number, or next k prime numbers (e.g., three)?\n\nI already know that it's often a good idea to choose an initial hash table size which is a prime number, at least if your hash function uses modulus such as universal hashing.  And I know that's why it's usually recommended to do 2n+1 instead of 2n (e.g., http://www.concentric.net/~Ttwang/tech/hashsize.htm)\n\nHowever as I said, I haven't seen any real explanation for why doubling or doubling-plus-one is actually a good choice rather than some other method of choosing a size for the new hash table.\n\n(And yes I've read the Wikipedia article on hash tables :) http://en.wikipedia.org/wiki/Hash_table\n    ", "Answer": "\r\nHash-tables could not claim \"amortized constant time insertion\" if, for instance, the resizing was by a constant increment. In that case the cost of resizing (which grows with the size of the hash-table) would make the cost of one insertion linear in the total number of elements to insert. Because resizing becomes more and more expensive with the size of the table, it has to happen \"less and less often\" to keep the amortized cost of insertion constant.\n\nMost implementations allow the average bucket occupation to grow to until a bound fixed in advance before resizing (anywhere between 0.5 and 3, which are all acceptable values). With this convention, just after resizing the average bucket occupation becomes half that bound. Resizing by doubling keeps the average bucket occupation in a band of width *2.\n\nSub-note: because of statistical clustering, you have to take an average bucket occupation as low as 0.5 if you want many buckets to have at most one elements (maximum speed for finding ignoring the complex effects of cache size), or as high as 3 if you want a minimum number of empty buckets (that correspond to wasted space).\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Hash mark in universal links in React Native (EXPO)\r\n                \r\nI've got a problem with Deep linking. Example link to my web app: https://example.com/#/create-password/:id\nDuring open this URI, my app doesn't start.\nMy app.json:\n```\n\"android\": {\n...\n      \"intentFilters\": [\n        {\n          \"action\": \"VIEW\",\n          \"autoVerify\": true,\n          \"data\": [\n            {\n                  \"scheme\": \"https\",\n                  \"host\": \"*example.com\",\n                  \"pathPrefix\": \"/#/create-password\"\n                }\n              ],\n              \"category\": [\n                \"BROWSABLE\",\n                \"DEFAULT\"\n              ]\n            }\n          ]\n    ...\n }\n```\n\nWhen I remove hash mark form app.json and run https://example.com/create-password/:id, my react native app starts.\nCould you help me resolve this problem?\n    ", "Answer": "\r\nThe problem you're experiencing seems to be directly related to Android not React Native - Intent filter pathPrefix with '#' not working. I suppose you can work around that using ```\npathPattern```\n instead of ```\npathPrefix```\n.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "What is the simplest, poor performance hash class for standard library containers?\r\n                \r\nIn situations where performance can be ignored, some standard library containers still require a custom hash class for most of the types. Assume I have a type ```\nSomeType```\n defined somewhere, I want to use ```\nstd::unordered_set```\n and I don't care about performance. The minimum I have to write is something like this:\n\n```\ntemplate<typename T>\nstruct Hash42 {\n  size_t operator()(T const& e) {return 42;}\n};\n\nstd::unordered_set<SomeType, Hash42<SomeType>> s;\n```\n\n\nIs there a shorter version? Is there a universal hash class defined somewhere?\n    ", "Answer": "\r\n\n  Is there a shorter version?\n\n\nYou can make the operator itself a template instead of the class, thereby allowing the type argument to be deduced:\n\n```\nstruct Hash0 {\n  std::size_t operator()(auto const& e) {return 0;}\n};\n\nstd::unordered_set<SomeType, Hash0> s;\n```\n\n\n\n  Is there a universal hash class defined somewhere?\n\n\nNow there is. There isn't one in the standard library.\n\n\n\nOf course, one should use this only if their goal is to make the program slow.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "HashTable Double Hashing function implementation problem\r\n                \r\nso I have assignment for my university course and I encountered very outstandingly peculiar thing. So I wrote unit tests for abstract class HashTableOpenAdresing. And here is the code:\n```\npublic abstract class HashOpenAdressing<T extends Comparable<T>> implements HashTable<T> {\n\n    private final Elem nil = null;\n    private final double correctLoadFactor;\n    private int size;\n    private int nElems;\n    private Elem[] hashElems;\n\n    HashOpenAdressing() {\n        this(2039); // initial size as random prime number\n    }\n\n    HashOpenAdressing(int size) {\n        validateHashInitSize(size);\n\n        this.size = size;\n        this.hashElems = new Elem[this.size];\n        this.correctLoadFactor = 0.75;\n    }\n\n    @Override\n    public void put(T newElem) {\n        validateInputElem(newElem);\n        resizeIfNeeded();\n\n        int key = newElem.hashCode();\n        int i = 0;\n        int hashId = hashFunc(key, i);\n\n        while (isNotDel(hashElems[hashId]) && hashElems[hashId].compareTo(newElem) != 0) {\n            i = (i + 1) % size;\n            hashId = hashFunc(key, i);\n        }\n\n        if(isDel(hashElems[hashId])){\n            int hashIdN = hashId;\n\n            while (hashElems[hashIdN] != nil ) {\n                if(hashElems[hashIdN].compareTo(newElem) == 0){\n                    break;\n                }\n                i = (i + 1) % size;\n                hashIdN = hashFunc(key, i);\n            }\n\n            if (hashElems[hashIdN].compareTo(newElem) == 0){\n                nElems--;\n            }\n        }\n\n        hashElems[hashId] = new Elem<>(newElem);\n        hashElems[hashId].isDel = false;\n        nElems++;\n    }\n\n    @Override\n    public T get(T searchedValue) {\n        ...\n        //Not relevant to the problem.\n    }\n\n    @Override\n    public void delete(T elem) {\n        validateInputElem(elem);\n\n        int key = elem.hashCode();\n        int i = 0;\n        int hashId = hashFunc(key, i);\n\n        while (hashElems[hashId] != nil && hashElems[hashId].compareTo(elem) != 0) {\n            i = (i + 1) % size;\n            hashId = hashFunc(key, i);\n        }\n\n        if (hashElems[hashId] != nil) {\n            hashElems[hashId].value = null;\n            hashElems[hashId].isDel = true;\n            nElems--;\n        }\n    }\n\n    abstract int hashFunc(int key, int i);\n\n    int getSize() {\n        return size;\n    }\n\n    private void resizeIfNeeded() {\n        ...\n    }\n\n    private boolean isDel(Elem elem){\n        if(elem == nil) {\n            return false;\n        }\n        return elem.isDel;\n    }\n\n    private boolean isNotDel(Elem elem){\n        if(elem == nil) {\n            return false;\n        }\n        return !elem.isDel;\n    }\n\n    private class Elem<T extends Comparable<T>> implements Comparable{\n        private T value;\n        private boolean isDel;\n\n        Elem(T value) {\n            this.value = value;\n        }\n\n        @Override\n        public int compareTo(Object o) {\n            ...\n            // can compare to Object of type Elem or T with respect to this.value\n        }\n    }\n}\n```\n\nMost of the code here are getter or methods that was just helping me understand the code.\nHere is the test that is not passed by class Double Hashing:\n```\n@Test\npublic void shouldPassWhenDoesNotAddDuplicateOfTheElementThatIsAfterDel(){\n    HashTable table = getNewHashTable(16);\n    setElemsWithSameHashForGivenTable(3, table);\n    for (Comparable el : ElemsWithSameHash) {\n        table.put(el);\n    }\n    table.delete(ElemsWithSameHash[1]);\n    //when\n    table.put(ElemsWithSameHash[2]);\n    //then\n    assertEquals(2, getNumOfElems(table));\n}\n```\n\nWhere getNumOfElems returns number of elements that is in the table right now.\nAnd at last here is hashFunc for Double Hash and for comparison here is Linear probing hashFunc:\nDouble Hashing:\n```\n@Override\nint hashFunc(int key, int i) {\n    int m = getSize();\n    int hash = (f(key, m) + i * g(key, m));\n    hash = hash % m;\n\n    if (hash == Integer.MIN_VALUE + 1) {\n        hash += 1;\n    }\n\n    hash = hash < 0 ? -hash : hash;\n\n    return hash;\n}\n\nint g(int key, int m) {\n    return 1 + (key % (m - 3));\n}\n\nint f(int key, int m) {\n    return key % m;\n}\n```\n\nLinear Probing:\n```\n@Override\nint hashFunc(int key, int i) {\n    int m = getSize();\n\n    int hash = (key + i) % m;\n\n    if (hash <= Integer.MIN_VALUE + 1) {\n        hash += 1;\n    }\n\n    hash = hash < 0 ? -hash : hash;\n\n    return hash;\n}\n```\n\nNow here is the problem, the test is passing for Linear Probing and is not passed for Double Hashing. I have now idea why and I am becoming desperate for answers.\nI tried to rebuild hashFunc for Double Hashing multiple times now. The problem is this is the implementation that I am required to have, so I can't change it in any ways. Please just tell me why this is not working (ignore the fact that size of table can be m = 3, Double Hashing).\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Find all possible sums from two number sequences of length n and insert them into a hash table in O(n) time?\r\n                \r\nSo the question I'm trying to solve is as follows:\n\n\n  The input is a sequence of n numbers {x1, x2, . . . , xn}, another\n  sequence of n numbers {y1, y2, . . . , yn}, and a number z. Your\n  algorithm should determine whether or not z ∈ {xi + yj | 1 ≤ i, j ≤\n  n}. You should use universal hashing families, and your algorithm\n  should run in expected time O(n). \n  \n  Provide justification that your algorithm is correct and runs in the\n  required time. Be very clear about which theorems from class and/or\n  the text you are using, and how.\n\n\nSo far, I've come up with this algorithm for finding all possible sums, inserting them into the hash table and then searching for ```\nz```\n:\n\n```\nfor (i in x; i++) {\n    for (j in y; j++) {\n        sum = xi + yj;\n        insert_into_hash_table(T, sum);\n    }\n}\n\nsearch_hash_table(T, z);\n```\n\n\nThe only problem is that the worst-case time here is ```\nO(n^2)```\n.\n\nHow do I do this in ```\nO(n)```\n?? =S\n    ", "Answer": "\r\nJust put all ```\nYi```\n  to a ```\nmap```\n. \n\nNow once you have ```\nZ```\n:\n\n```\n for all values from Xi\n     find if Z - Xi os present in map\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Find Pair with Difference less than K with O(n) complexity on average\r\n                \r\nI have an unsorted array of ```\nn```\n positive numbers and a parameter ```\nk```\n, I need to find out if there is a pair of numbers in the array that the difference between than is less than ```\nk```\n and I need to do so in time complexity of ```\nO(n)```\n on probable average and in space complexity of ```\nO(n)```\n.\n\nI believe it requires the use of a universal hash table but I'm not sure how, any ideas?\n    ", "Answer": "\r\nThis answer works even on unbounded integers and floats (doing some assumptions on the nicety of the hashmap you'll be using - the java implementation should work for instance):\n\n\nkeep a ```\nhashmap<int, float> all_divided_values```\n. For each key ```\ny```\n,\nif ```\nall_divided_values[y]```\n exists, it will contain a value ```\nv```\n that\nis in the array such that ```\nfloor(v/k) = y```\n.\nFor each value ```\nv```\n in the original array ```\nA```\n, if ```\nv/k```\n is in  all_divided_values's keys, output ```\n(v, all_divided_values[v/k])```\n\n(they are distant by less than k). Else, store ```\nv```\n in\n```\nall_divided_values[v/k]```\n\nOnce ```\nall_divided_values```\n is filled, go through ```\nA```\n again. For each ```\nv```\n, test whether ```\nall_divided_values[v/k - 1]```\n exists, and if so,\noutput the pair ```\n(v, all_divided_values[v/k - 1])```\n if and only if ```\nabs(v-all_divided_values[v/k - 1])<=k```\n\n\n\nInserting in a hashmap is usually (with Java hashmap for instance) ```\nO(1)```\n in average, so the total time is ```\nO(n)```\n. But please note that technically this could be false, for instance if your language's implementation does not have a nice strategy about the hashmap. \n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "what are the options for obtaining k pair-wise independent hash functions that are fast\r\n                \r\nI ran into the needs of k pair-wise independent hash functions, each takes as input an integer, and produces a hash value in the range of 0-N. Need this for count-min sketch, which is similar to Bloom filter.\n\nFormally, I need h_1,h_2, ..., h_k hash functions, pair-wise independent.\n\n(h_i(n) mod N ) will give the hash value of n in the range of 0-N. The hashing needs to be time-efficient as I am working with a large set of data. At the same time, they should be as pair-wise independent as possible.\n\nWhat I have tried so far:\n\n1) xxhash: It is efficient, but it is not good in terms of pair-wise independent, meaning there are hash collisions between hash functions (meaning h1(n1)=h1(n2) then some h_k(n1) also = h_k(n2)) and the result i got was bad due to this.\n\n2) Similarly, the famous integer hashing method ((a*n+b) mod p) mod N also has the same problem as xxhash. I believe this is called Universal hashing\n\n3) The other one introduced in count-min-sketch produces quite good results, but takes too much time for a large input.\n\n4) Also tried Murmur3, sha1 with similar problems in collisions. \n\nAny idea would be greatly appreciated. C/C++ preferred, but Java would also be fine, or simply algorithm.\nThanks\n    ", "Answer": "\r\nI suspect that your problem with method 2 is that you tossed a_i and b_i that were correlated.\nWork in large field (somewhere near 2^64) and for starters make sure all a_i and b_i are different (i.e., you get 2*k different numbers). If they are uniformly distributed inside the field this also wouldn't hurt :)   \n\nYou might encountered the same problem in method 4 with SHA. Most cryptographic hash functions (including even the broken and older ones) are much more than enough for data structures needs, be it k-wise independence for any reasonable k, or almost any other property.\nI would recheck - how you used it?\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Hash tables - Perfect hashing\r\n                \r\nI want to create a hash table with perfect hashing for a dictionary. The thing is I still don't quite get it. What I understand is that you have two universal function h1 and h2. Then you map the keys to a table with h1, and then you create a new table for every slot in the first table.\n\nThat's all good. The thing is:\n1) h1 is a function that picks a random vector 'a' = {a1,a2,...,an}, then you dot-product this with the key(which is also a vector).\nQuestion 1:\nLet's say I have a String as my key, how would I pick vector 'a', and how would I pick the key?\n\nQuestion 2:\nSupposedly if h2 collides, you pick another h2 from the same family of functions. But then how would I find it when I search for it? I would have to go through the same process again?\n\nIf someone decides to help me, examples (they don't have to be complex) would be very appreciated.\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to deploy angular 4 universal app to firebase\r\n                \r\nWith the introduction to angular 4 of universal, I can't figure out how to deploy the app successfully to a firebase hosting. I followed the steps here \nhttps://github.com/angular/angular-cli/wiki/stories-universal-rendering\n\nI can't figure out this part of it though:\n\n\"The bundle produced has a hash in the filename from webpack. When deploying this to a production server, you will need to ensure the correct bundle is required, either by renaming the file or passing the bundle name as an argument to your server.\"\n\nUsually we just use ng build --prod \n\nThen firebase deploy the dist directory.\n\nWith this universal inclusion, which folder do I deploy to firebase?\n\ndist-server or dist?\n    ", "Answer": "\r\nThere is a youtube video from Google on this subject here: https://youtu.be/gxCu5TEmxXE\n\nBasically, as far as I can tell at the moment there is no way of linking your functions folder with your 'dist' folder, so what we have to do is overwrite the firebase.json settings to serve your app as a function. This basically means that your function (expressJS code) is serving the app rather than the static files in dist/\n\nAfter following the youtube video your firebase.json should be as follows:\n\n\r\n\r\n```\n{\r\n  \"hosting\": {\r\n    \"public\": \"dist\",\r\n    \"ignore\": [\r\n      \"firebase.json\",\r\n      \"**/.*\",\r\n      \"**/node_modules/**\"\r\n    ],\r\n    \"rewrites\": [\r\n      {\r\n        \"source\": \"**\",\r\n        \"function\": \"ssrapp\"\r\n      }\r\n    ]\r\n  },\r\n  \"functions\": {\r\n    \"predeploy\": \"npm --prefix functions run build\",\r\n    \"source\": \"functions\"\r\n  }\r\n}```\n\r\n\r\n\r\n\n\nThen you have to copy your dist files into your function directory, so that the function can bootstrap your app. If your setup is correct then when people load your URL, the server function in /functions will then redirect them to the browser & server files that are hosted in the functions directory. \n\nWe actually have to delete the index.html file in the dist/ folder so that  firebase doesn't accidently serve this instead of rerouting the traffic through the server you have configured as a firebase function:\n\nindex.ts (server in functions folder)\n\n\r\n\r\n```\nimport * as functions from 'firebase-functions';\r\nimport * as angularUniversal from 'angular-universal-express-firebase';\r\n\r\nexport let ssrapp = angularUniversal.trigger({\r\n\tindex: __dirname + '/browser/index.html',\r\n\tmain: __dirname + '/server/main.bundle',\r\n\tenableProdMode: true,\r\n\tbrowserCacheExpiry: 1200,\r\n\tcdnCacheExpiry: 600\r\n});```\n\r\n\r\n\r\n\n\nRegards your question about the hashing, basically your angular-cli should be building 2 apps, the normal one and the 'SSR' Server rendered app. An example configuration is here: \n\n\r\n\r\n```\n  \"apps\": [\r\n    {\r\n      \"root\": \"src\",\r\n      \"outDir\": \"dist/browser\",\r\n      \"assets\": [\r\n        \"assets\",\r\n        \"favicon.ico\"\r\n      ],\r\n      \"index\": \"index.html\",\r\n      \"main\": \"main.ts\",\r\n      \"polyfills\": \"polyfills.ts\",\r\n      \"test\": \"test.ts\",\r\n      \"tsconfig\": \"tsconfig.app.json\",\r\n      \"testTsconfig\": \"tsconfig.spec.json\",\r\n      \"prefix\": \"app\",\r\n      \"styles\": [\r\n        \"styles.scss\",\r\n        \"trexco.scss\"\r\n      ],\r\n      \"scripts\": [\r\n        \"../node_modules/moment/min/moment.min.js\"\r\n      ],\r\n      \"environmentSource\": \"environments/environment.ts\",\r\n      \"environments\": {\r\n        \"dev\": \"environments/environment.ts\",\r\n        \"prod\": \"environments/environment.prod.ts\"\r\n      }\r\n    },\r\n      {\r\n        \"platform\": \"server\",\r\n        \"root\": \"src\",\r\n        \"outDir\": \"dist/server\",\r\n        \"assets\": [\r\n          \"assets\",\r\n          \"favicon.ico\"\r\n        ],\r\n        \"index\": \"index.html\",\r\n        \"main\": \"main.server.ts\",\r\n        \"test\": \"test.ts\",\r\n        \"tsconfig\": \"tsconfig.server.json\",\r\n        \"testTsconfig\": \"tsconfig.spec.json\",\r\n        \"prefix\": \"app\",\r\n        \"styles\": [\r\n          \"styles.scss\",\r\n          \"trexco.scss\"\r\n        ],\r\n        \"scripts\": [],\r\n        \"environmentSource\": \"environments/environment.ts\",\r\n        \"environments\": {\r\n          \"dev\": \"environments/environment.ts\",\r\n          \"prod\": \"environments/environment.prod.ts\"\r\n        }\r\n      }\r\n\r\n  ],```\n\r\n\r\n\r\n\n\nBecause the function in our /functions folder should always link to the server bundle, we just need to make sure that when we compile the server app we do so without hashes, this is as simple as this in your package.json\n\n\r\n\r\n```\n  \"scripts\": {\r\n    \"ng\": \"ng\",\r\n    \"start\": \"ng serve\",\r\n    \"build\": \"ng build\",\r\n    \"build:client-and-server-bundles\": \"ng build --prod && ng build --prod --app 1 --output-hashing=false\",\r\n    \"build:prerender\": \"npm run build:client-and-server-bundles && npm run webpack:server && npm run generate:prerender\",\r\n    \"build:ssr\": \"npm run build:client-and-server-bundles && npm run webpack:server\",\r\n    \"generate:prerender\": \"cd dist && node prerender\",\r\n    \"webpack:server\": \"webpack --config webpack.server.config.js --progress --colors\",\r\n    \"serve:prerender\": \"cd dist/browser && http-server\",\r\n    \"serve:ssr\": \"node dist/server\"\r\n  },```\n\r\n\r\n\r\n\n\nIf you build the server app (app 1) with the argument --output-hashing=false then the output will always be main.bundle, meaning that your server function can always find the correct file without extra logic.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Generate a unique id\r\n                \r\nI am a student at university and our task is to create a search engine. I am having difficulty generating a unique id to assign to each url when added into the frontier. I have attempted using the SHA-256 hashing algorithm as well as Guid. Here is the code that i used to implement the guid:\n\n```\npublic string generateID(string url_add)\n{\n    long i = 1;\n\n    foreach (byte b in Guid.NewGuid().ToByteArray())\n    {\n        i *= ((int)b + 1);\n    }\n\n    string number = String.Format(\"{0:d9}\", (DateTime.Now.Ticks / 10) % 1000000000);\n\n    return number;\n}\n```\n\n    ", "Answer": "\r\nWhy not just use ToString?\n\n```\npublic string generateID()\n{\n    return Guid.NewGuid().ToString(\"N\");\n}\n```\n\n\nIf you would like it to be based on a URL, you could simply do the following:\n\n```\npublic string generateID(string sourceUrl)\n{\n    return string.Format(\"{0}_{1:N}\", sourceUrl, Guid.NewGuid());\n}\n```\n\n\nIf you want to hide the URL, you could use some form of SHA1 on the sourceURL, but I'm not sure what that might achieve.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Angular4 Universal Server Side Rendering would return more HTML rendered on request\r\n                \r\ni'm tryingo to use universal and angular4.. everything works, accessing directly via hash routes as well.. but the problem is the request always shows ```\n<body><demo-app></demo-app></body>```\n\n\nI expect to have some html tags and info inside ```\n<demo-app></demo-app>```\n\n\nHere is how I implemented that:\n\n```\n@NgModule({\n  bootstrap: [AppComponent],\n  imports: [\n    BrowserModule.withServerTransition({\n      appId: 'my-app-id'\n    }),\n    ServerModule,\n    ServerTransferStateModule,\n    AppModule\n  ]\n})\n\nexport class ServerAppModule {\n\n  constructor(private transferState: TransferState,\n  private translateService: TranslateService){\n    translateService.use('en');\n  }\n  // Gotcha\n  ngOnBootstrap = () => {\n    this.transferState.inject();\n  }\n}\n\n\napp.engine('html', ngExpressEngine({\n  bootstrap: ServerAppModule\n}));\n\napp.set('view engine', 'html');\napp.set('views', 'src');\n\n\napp.get(\"*\", (req, res) => {\n  console.time(`GET: ${req.originalUrl}`);\n  res.render('../dist/index', {\n    req: req,\n    res: res\n  });\n  console.timeEnd(`GET: ${req.originalUrl}`);\n});\n```\n\n\nMy versions are \"@angular/common\": \"^4.0.0\"\n\nTested using https://github.com/angular/universal/blob/master/modules/ng-express-engine/src/main.ts and ```\nimport { ngExpressEngine } from '@nglibs/universal-express-engine'```\n\n\nFollowing this discussion i could see that is a new implementation for that\n\n\nhttps://github.com/angular/universal/pull/681/files\n\n\nTryed to import as it suggest, but does not work (namespace does not exists)\n\n\nimport { ngExpressEngine } from '@universal/ng-express-engine'\n\n\nTested with version rc.5 as suggested in this pull request.\n\nFinally , following this link \n\nhttps://github.com/angular/universal/tree/master/modules/ng-express-engine\n\nTested with this lib gives me same results:\n\n```\nimport { ngExpressEngine } from '@nguniversal/express-engine';```\n\n\nAt least the project is running, so i can keep coding and using it.. but looking forward to have a full server side rendering.\n\nThanks!\n    ", "Answer": "\r\nFor Angular 4+ with platform-server, use the official ```\n@nguniversal/express-engine```\n, that other one looks like an older version of the express-engine that someone published for some reason?\n\nThe namespace is now going to be ```\n@nguniversal```\n though so give that one a shot and you should be fine!\n\nYou can find documentation on the newest universal express-engine (for Angular 4+ and platform-server) here:\n\nhttps://github.com/angular/universal/tree/master/modules/ng-express-engine\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "hashing file in custom pattern\r\n                \r\nFairly simple. How can I make a single function to create a multi-valued hashed url.\n\nSo for example *getfile.mysite.ltd/file/dl.php?file_id=10101* is alright but I do not want to disclose file_id. For such reason I am fetching files via universal file hash that is stored in SQL. Which is a MD5 of the files name.\n\nHere is how the thing looks in my SQL record\n\n```\n my-vacation-image.jpg -> b16f48c6cfd73bb9bfb058c95175537f\n```\n\n\nbut I wan't to make it bit more complex as examle below\n\n```\n my-vacation-image.jpg -> ce591714-d1e8-46d5-bbfa-23b45bea81e2\n```\n\n\nAny ideas or suggestions as to how?\n    ", "Answer": "\r\n```\n$dashedHash = implode('-', str_split($hash, 5));\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "values for MAD compression method?\r\n                \r\nI am stuck trying to implement the perfect hashing technique using universal hashing at each level from Cormen. Specifically, with the compression method (at least, I think here is my problem).\n\nI am working on strings, I think short strings (between 8 and 150), and for that, I have my set of hash functions with Murmur3/2, xxhash, FNV1, Cityhash and Spookyhash, using the 64-bits keys (for those hash functions like spookyhash I am getting the lower 64-bits), the problem is that there exist collissions with only three unique strings (two of 10 characters and one of 11 characters) in 9 buckets.\n\nI am using the Cormen's hash compression method for that:\n\n\n  h_ab(k) = ((ak+b)mod p) mod m\n\n\nwith a = 3, p = 4294967291 (largest 32-bit prime), b = 5 and m = 9 (because m_j should be the square of n_j). As \"k\" I am using the hash value returned by the hash function (like murmur).\n\nIf for example, I am using a hash function like murmur2 (64-bit version), the p number should be the largest 64-prime number? I that way, I am covering all possibles hashes that murmur could return, is that right?\n\nWhich other hash compressions techniques (apart of division) exist and do you recommend?\n\nAny reference, hint, book, paper, help is pretty welcome.\nSorry for the silly question, I am pretty newbie with hash functions and hash tables.\n\nThanks in advance.\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Understanding the Count Sketch data structure and associated algorithms\r\n                \r\nWorking on wrapping my head around the CountSketch data structure and its associated algorithms.  It seems to be a great tool for finding common elements in streaming data, and the additive nature of it makes for some fun properties with finding large changes in frequency, perhaps similar to what Twitter uses for trending topics.\n\nThe paper is a little difficult to understand for someone that has been away from more academic approaches for a while, and a previous post here did help some, for me at least it still left quite a few questions.\n\nAs I understand it, the Count Sketch structure is similar to a bloom filter. However the selection of hash functions has me confused.   The structure is an N by M table with N hash functions with M possible values determining the \"bucket\" to alter, and another hash function s for each N that is \"pairwise independent\"\n\nAre the hashes to be selected from a universal hashing family, say something of the h(x) = ((ax+b) % some_prime) % M?  \n\nAnd if so, where are the s hashes that return either +1 or -1 chosen from? And what is the reason for ever subtracting from one of the buckets?\n    ", "Answer": "\r\nThey subtract from the buckets to make average effect of additions/subtractions caused by other occurrences to be 0.  If half the time I add the count of 'foo', and half the time I subtract the count of 'foo', then in expectation, the count of 'foo' does not influence the estimate of the count for 'bar'.  \n\nPicking a universal hash function like you describe will indeed work, but it's mostly important for the theory rather than the practice.  Salting your favorite reasonable hash function will work too, you just can't meaningfully write proofs based on the expected values using a few fixed hash functions.  \n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Issue while integrating PayU in windows universal apps\r\n                \r\nI am new to Windows universal apps development. Now I am developing an app in which I have to integrate PayU. I tried a lot but everytime the transaction error is thrown from the server.\n\n```\nstring temp1 = \"key=xxxxxx&txnid=xxxxxx&hash=hashValue&amount=xxx&firstname=abc\" + \n  \"&email=a@a.com&phone=80xxxxxxxx&productinfo=xxxxxxxxxxxx\" + \n  \"&surl=https://www.google.com&furl=https://www.twitter.com\" + \n  \"&udf1=a&udf2=b&udf3=c&udf4=d&udf5=e&pg=CC&bankcode=CC\" + \n  \"&ccardtype=CC&ccnum=1234xxxxxxxxx&ccname=xxx&ccvv=xxx\" +\n  \"&ccexpmon=xx&ccexpyr=xxxx\";\n\nvar httpClient = new Windows.Web.Http.HttpClient();\nWindows.Web.Http.HttpRequestMessage httpRequestMessage = new Windows.Web.Http.HttpRequestMessage(Windows.Web.Http.HttpMethod.Post, theUri);\nWindows.Web.Http.IHttpContent content = new Windows.Web.Http.HttpStringContent(temp1, Windows.Storage.Streams.UnicodeEncoding.Utf8);\nhttpRequestMessage.Content = content;\ntry\n{\n    webView.NavigateWithHttpRequestMessage(httpRequestMessage);\n}\ncatch(Exception f)\n{\n    new MessageDialog(f.ToString()).ShowAsync();\n}\n```\n\n\nAnd I am creating the hashValue by using method :\n\n```\n public String SampleHashMsg(String strMsg)\n    {\n        // Convert the message string to binary data.\n        string strAlgName = HashAlgorithmNames.Sha512;\n        IBuffer buffUtf8Msg = CryptographicBuffer.ConvertStringToBinary(strMsg, BinaryStringEncoding.Utf8);\n\n        // Create a HashAlgorithmProvider object.\n        HashAlgorithmProvider objAlgProv = HashAlgorithmProvider.OpenAlgorithm(strAlgName);\n\n        // Demonstrate how to retrieve the name of the hashing algorithm.\n        String strAlgNameUsed = objAlgProv.AlgorithmName;\n\n        // Hash the message.\n        IBuffer buffHash = objAlgProv.HashData(buffUtf8Msg);\n\n        // Verify that the hash length equals the length specified for the algorithm.\n        if (buffHash.Length != objAlgProv.HashLength)\n        {\n            throw new Exception(\"There was an error creating the hash\");\n        }\n\n        // Convert the hash to a string (for display).\n        //String strHashBase64 = CryptographicBuffer.EncodeToBase64String(buffHash);\n\n        String strHashBase64 = CryptographicBuffer.EncodeToHexString(buffHash);\n        // Return the encoded string\n\n        return strHashBase64;\n    }\n```\n\n\nI should load the request to the webview. But I am getting an error \"Transaction Error\" in that.\n\nI am getting transaction error, txnid is not provided. Well at PayU side the sent hash key will be used for verify a transaction. May be my txnid and the txnid contained by hash does not match and payu server denies the transaction saying that provide txnid.\n\nI am using Microsoft Visual Studio 2013 Universal apps for the app development.\n\nBut still I am not getting correct result. Please if anyone can help me out, then please reply immediately. Thanks in advance.\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to overload 'std::string operator()' for hashing?\r\n                \r\nI am implementing a hashmap for a university project, and was wondering if its possible to overload ```\nstd::string operator ()```\n for hashing (using ```\noperator ()```\n is a design choice).\nThe hashmap is general and requires the key to have an ```\noperator()```\n, this is fine for any user-defined types, but is it possible for built-in types?\nAn explanation would be helpful, but just code will also work (my project choice is already overkill).\nI really don't want to reinvent the wheel by re-implimenting my own version of ```\nstring```\n.\nPS: I know that hashmaps already exist, and it has a way to hash strings, but using that would defeat the purpose of this project.\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Hashing passwords for on-disk storage (More details inside)\r\n                \r\nI need to store hashes of passwords on disk. I am not entirely sure which hash function to use (they all seem somewhat troubled at the moment), but I am leaning towards SHA-256.\n\nMy plan is to take the user's password and combine it with their user ID, a random user-specific salt, and a universal site-wide salt. Should I concatenate these values together and then hash the single resulting string, or should I hash each separately, concatenate the hashes, and then hash that? Also, does the order (password, user id, user salt, site salt) matter? Can I rearrange them however I like, or is it a bad idea to have something that doesn't change (site salt) or something completely predictable (user id/user salt) first?\n\nThanks.\n    ", "Answer": "\r\nSHA-256 seems to be one of the better options available right now.\n\nConcatenating everything should be fine and order isn't all that important. Just make sure that you are using a significantly long salt value.\n\nThis post has some good recommendations-\nWhat algorithm should I use to hash passwords into my database?\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Can't upgrade settings when updating a WPF Desktop Bridge Universal app\r\n                \r\nMy app is written in WPF C# and I export it as Universal app using MSIX Application Project straight from Visual Studio.\nI just can't get the settings to persist between updates. I'm using the following code in the ```\nMainWindow_Loaded```\n event:\n```\nSettings.Default.Upgrade();\nSettings.Default.Save();\nSettings.Default.Reload();\n```\n\nI tried keeping assembly information versions the same and just increment the version in the appx.manifest but it doesn't work.\nI've noticed that each time the app updates it creates a new uniquely named parent settings folder (with a new hash every time) and the subfolder name is the version from the assembly. The folder structure is like this:\n```\nApp.exe_Url_dfvfmfjs1qo33zsag1ay5w1s0rwg0u53/0.2.10.0/user.config```\n\n```\nApp.exe_Url_tazrvujdga5ujjarnahpkoscv5zbkgl0/0.2.10.0/user.config```\n\nI believe it might have to do with the fact that it keeps generating new hashes instead of just placing the new version as a subfolder and that's why Upgrade doesn't do anything.\nThe only information I've found so far is to use ```\nSettings.Default.Upgrade()```\n\nHow am I supposed to transfer the old version settings to the new version when my universal desktop bridge app updates?\n    ", "Answer": "\r\nAs far as I researched these settings do not transfer to UWP updates using Desktop Bridge. So I started using UWP's native ```\nApplicationData.Settings```\n\nAs a workaround I created 2 methods to update the newly created WPF settings using ```\nLocalSettings```\n which is the UWP equivalent and vice versa. UWP's ```\nLocalSettings```\n transfer on update. I call ```\nUpdate()```\n when I save my WPF settings and I call ```\nLoad()```\n when the application starts. It works.\nHere's the code, only caveat I've found so far is you should use the basic types as these methods will fail transferring something like a ```\nList<string>```\n or a ```\nStringCollection```\n, for that I'm using serialization, although you can always adapt them to do that too:\n```\nstatic class UWPSettings\n    {\n        public static void Update()\n        {\n            if (Startup.IsUniversalPlatform)\n            {\n                foreach (SettingsPropertyValue value in Properties.Settings.Default.PropertyValues)\n                {\n                    ApplicationData.Current.LocalSettings.Values[value.Name] = value.PropertyValue;\n                }\n            }\n        }\n\n        public static void Load()\n        {\n            if (Startup.IsUniversalPlatform)\n            {\n                foreach (var setting in ApplicationData.Current.LocalSettings.Values)\n                {\n                    foreach (SettingsPropertyValue s in Properties.Settings.Default.PropertyValues)\n                    {\n                        if (s.Name == setting.Key)\n                        {\n                            s.PropertyValue = setting.Value;\n                        }\n                    }\n                }\n\n                Properties.Settings.Default.Save();\n            }\n        }\n    }\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Passing parameters via GET to any url in universal way\r\n                \r\nI have ```\nback_url```\n given to me from the outside. I need to generate a hash and to make redirect to this ```\nback_url```\n with this param: ```\nheader(\"Location: $back_url?hash=123sdf\")```\n. But the problem is that I don't know the format of ```\nback_url```\n.\n\n\nIt may be ```\nwww.example.com```\n and I do ```\nheader(\"Location: $back_url/?hash=123sdf\")```\n sj that's fine.\nIt maybe ```\nwww.example.com/?param=value```\n and if I do h```\neader(\"Location: $back_url/?hash=123sdf\")```\n it's wrong, because it would be ```\nwww.example.com/?param=value/?hash=123asd```\n.\n\n\nAnd so on. The question is: what's the universal way to pass params to ```\nback_url```\n ignoring its format?\n    ", "Answer": "\r\nA complex but very clean way is \n\n\nUse ```\nparse_url()```\n to extract the query string  (if any) from the URL into an array\nAdd ```\nhash```\n to the resulting array: ```\n$params[\"hash\"] = \"1234\";```\n\nUse ```\nhttp_build_query()```\n to glue the parameters back into a query string\nTake all the components returned by ```\nparse_url()```\n and glue them back into a full URL\n\n\none thing to note is that this dissects the URL into it components and glues it back together, so it's likely it won't work with URLs that are broken in the first place.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "iOS-Xamarin, branch.io, universal links Integration issue\r\n                \r\nAm integrating branch.io to my iOS app which is being developed in Xamarin, without Forms.\nFollowed the steps mentioned in SDK integration.\n\n\nAdded the Branch Xamarin SDK as a NuGet package\nbranch_key added in Info.plist\nAdded URL scheme in Xamarin\nEnabled Universal Links in branch.io portal\nAdded code for initialization and delegates for branch.io\nEnabled Associated Domains in developer portal\nEnabled Associated Domains in xamarin studio and added domains applinks:bnc.lt\n\nEnsure that the correct build target is checked in the right sidebar.\nAm not sure how to do that in xamarin. By default the Entitlements.plist file is specified in configuration.\nEnabled Universal Links in branch dashboard\nSpecified bundle identifier and Apple App prefix properly.\nCreated a marketing link, and tapped the link from messages app.\nWhich routes to safari with a dummy page from bnc.lt, with option of 'Get the app'\n\n\nAny information on this would be greatly appreciable. Thanks!\n\nTested in\n\nDevice: iPhone6S+, iOS9.1\n\nDevice: iPhone6s, iOS9.4\n\nUpdate\n\nWe had a custom link label for the all the marketing links, which should be kept empty to support Universal Links. The format of the market link would be bnc.lt/«four-letter-identifier»/«link-hash»\n    ", "Answer": "\r\nI'm adding a note here to help anyone coming across this ticket and thinking they may have a similar issues:\n\nThe default Branch link domain for all apps until early 2016 was \"bnc.lt.\" To support Universal Linking, all Branch apps using this bnc.lt domain had entries in a single Apple-App-Site-association (AASA) file. To uniquely identify apps in this AASA file, Branch relied on a four-letter alpha identifier in each link: https://bnc.lt/«four-letter-identifier»/«link-hash»\n\nBranch allows partners to replace the \"«four-letter-identifier»/«link-hash»\" portion of bnc.lt links with custom aliases (\"https://bnc.lt/mylink,\" for example), however doing so makes it impossible for the app to be identified in the AASA file - meaning that such links cannot function as Universal Links.\n\nBranch introduced app.link domains in early 2016 to replace bnc.lt. The limitation regarding Universal Links and link aliases described here does not affect app.link domains.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Migrating users to Universal Membership Provider\r\n                \r\nI'm trying to migrate to the new Universal Membership providers (from a home brew solution). I've migrated our old User table to the Users/Memberships table.\n\nWhen I run ```\nMembership.ValidateUser(txtUsername.Text.Trim(), txtPassword.Text.Trim())```\n, it always returns false, even though I know the username/password is correct. \n\nHere is how I generated the password, hash, and salt: \n\n```\nvar salt = Crypto.GenerateSalt();\nvar hashedPassword = this.GenerateHashWithSalt(password, salt);\n```\n\n\nThis is the GenerateHashWithSalt method I'm using\n\n```\nprivate string GenerateHashWithSalt(string password, string salt)\n{\n    string hashWithSalt = password + salt;\n    byte[] saltedHashBytes = Encoding.UTF8.GetBytes(hashWithSalt);\n\n    HashAlgorithm algo = HashAlgorithm.Create(Membership.HashAlgorithmType);\n\n    byte[] hash = algo.ComputeHash(saltedHashBytes);\n\n    return Convert.ToBase64String(hash);\n}\n```\n\n\nI've also gone with: \n\n```\nvar salt = Crypto.GenerateSalt();\nvar saltedPassword = password + salt;\nvar hashedPassword = Crypto.HashPassword(saltedPassword);\n```\n\n\nNeither of these seem to work. What am I missing? \n\nScott\n    ", "Answer": "\r\nBah, answered it. \n\nSo step 1 of all of this was migrating users from our 2-way encrypted passwords to 1 way hash. \n\nEvery example I saw of doing that included hashing the password manually. Turns out ```\nMembership.CreateUser(username,password);```\n hashes the password (and salts), so I was hashing a hashed password, which is why auth was failing.\n\nI simply had to call  Membership.CreateUser, passing in the username & plain text pass, and it worked. Weee!\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is there a universal function F that F(sha(a),sha(b)) = sha(ab)\r\n                \r\nI am faced with a need to send my data in parts, and at the same time I am expected to provide sha256 for my WHOLE data.\n\nSomething like this ```\ncat large file | chunker | receiver```\n\nwhere receiver is an application that is expected to receive the data, possibly in chunks having in the header sha256 of the payload, and then following payload. After collecting all chunks, it is supposed to store the whole transmitted data, and the sha256 of all data (particular sha256 will be used only to rehash and confirm integrity of the data.)\n\nOf course, the simplest thing would be if the receiver generated sha256 from whole the streamed data, but I was wondering if there is a simpler way by collecting all hashes of all chunks, and combine them to generate one final hash, which would be the same as the hash calculated from all the data.\n\nIn other words - and I copy this from the title - I wonder if there is a function F that would receive a list of hashes of chunks of data, and then generated final hash that would be equal to the hash generated on all the data.\n\nAnd again, in other words, having this formula:\nF(sha256(data[0]), sha256(data[1]), ... sha256(data[N])) = sha256(data[0..N])\n\nWhat would be the function F?\nWould it be a universal function or there is no such thing for the way hashing is calculated?\n\nI suspect there is no such function or this is too complicated question to answer.\n    ", "Answer": "\r\nAFAIK there are still no known collisions for SHA-256 but I bet that once some is found, i.e. someone finds two messages ```\nm1```\n and ```\nm2```\n such that ```\nSHA-256(m1) = SHA-256(m2)```\n, then for almost any prefix ```\na```\n hashes ```\nSHA-256(a || m1)```\n and ```\nSHA-256(a || m2)```\n will be different i.e. the function you ask is actually not a function (has different outputs for the same inputs). Or to put it otherwise SHA-2 is susceptible to length extension attacks but AFAIK not to prefixing attacks. Still even if this actually a function, it is not enough for you for such a function to exist, you also want it to be fast. And I believe there is no such fast to compute function. \n\nOn the other hand SHA-256 works by splitting the original message into 512-bit chunks and processing them using a well defined process (which is based on the state from all the previous chunks) so theoretically you can modify some implementation of ```\nSHA-256```\n to compute two hashes at the same time (by applying the same logic to different initial states):\n\n\nHash of your application-defined chunk (using standard initial state)\nHash of all chunks up to this point (using the state passed from the previous output of the same step as the initial state).\n\n\nThis probably will be slightly faster than doing those things independently but I don't know whether it will be so much faster to justify such a custom implementation.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Angular with universal does not work\r\n                \r\nHi I am using angular 2 with universal.\n\nI am trying to this.\n\nFirst I create a new project :\n\n```\nng new universaldemo.\n```\n\n\nI installed these packages :\n\n```\nnpm install --save @angular/platform-server @nguniversal/module-map-ngfactory-loader ts-loader @nguniversal/express-engine```\n.\n\nHere is my ```\napp.module.ts```\n :\n\n```\nimport { BrowserModule } from '@angular/platform-browser';\nimport { NgModule } from '@angular/core';\n\nimport { AppComponent } from './app.component';\n\n@NgModule({\n  declarations: [\n    AppComponent\n  ],\n  imports: [\n    BrowserModule.withServerTransition({ appId: 'universaldemo' }),\n  ],\n  providers: [],\n  bootstrap: [AppComponent]\n})\nexport class AppModule { }\n```\n\n\nHere is my ```\napp.server.module.ts```\n\n\n```\nimport { NgModule } from '@angular/core';\nimport { ServerModule } from '@angular/platform-server';\nimport { ModuleMapLoaderModule } from '@nguniversal/module-map-ngfactory-loader';\n\nimport { AppModule } from './app.module';\nimport { AppComponent } from './app.component';\n\n@NgModule({\n  imports: [\n    AppModule,\n    ServerModule,\n    ModuleMapLoaderModule\n  ],\n  providers: [\n    // Add universal-only providers here\n  ],\n  bootstrap: [ AppComponent ],\n})\nexport class AppServerModule {}\n```\n\n\nHere is my ```\nserver.ts```\n :\n\n```\n// These are important and needed before anything else\nimport 'zone.js/dist/zone-node';\nimport 'reflect-metadata';\n\nimport { enableProdMode } from '@angular/core';\n\nimport * as express from 'express';\nimport { join } from 'path';\n\n// Faster server renders w/ Prod mode (dev mode never needed)\nenableProdMode();\n\n// Express server\nconst app = express();\n\nconst PORT = process.env.PORT || 4000;\nconst DIST_FOLDER = join(process.cwd(), 'dist');\n\n// * NOTE :: leave this as require() since this file is built Dynamically from webpack\nconst { AppServerModuleNgFactory, LAZY_MODULE_MAP } = require('./dist/server/main.bundle');\n\n// Express Engine\nimport { ngExpressEngine } from '@nguniversal/express-engine';\n// Import module map for lazy loading\nimport { provideModuleMap } from '@nguniversal/module-map-ngfactory-loader';\n\napp.engine('html', ngExpressEngine({\n  bootstrap: AppServerModuleNgFactory,\n  providers: [\n    provideModuleMap(LAZY_MODULE_MAP)\n  ]\n}));\n\napp.set('view engine', 'html');\napp.set('views', join(DIST_FOLDER, 'browser'));\n\n// TODO: implement data requests securely\napp.get('/api/*', (req, res) => {\n  res.status(404).send('data requests are not supported');\n});\n\n// Server static files from /browser\napp.get('*.*', express.static(join(DIST_FOLDER, 'browser')));\n\n// All regular routes use the Universal engine\napp.get('*', (req, res) => {\n  res.render(join(DIST_FOLDER, 'browser', 'index.html'), { req });\n});\n\n// Start up the Node server\napp.listen(PORT, () => {\n  console.log(`Node server listening on http://localhost:${PORT}`);\n});\n```\n\n\nHere is my ```\nwebpack.server.config.js```\n :\n\n```\nconst path = require('path');\nconst webpack = require('webpack');\n\nmodule.exports = {\n  entry: { server: './server.ts' },\n  resolve: { extensions: ['.js', '.ts'] },\n  target: 'node',\n  // this makes sure we include node_modules and other 3rd party libraries\n  externals: [/(node_modules|main\\..*\\.js)/],\n  output: {\n    path: path.join(__dirname, 'dist'),\n    filename: '[name].js'\n  },\n  module: {\n    rules: [{ test: /\\.ts$/, loader: 'ts-loader' }]\n  },\n  plugins: [\n    // Temporary Fix for issue: https://github.com/angular/angular/issues/11580\n    // for 'WARNING Critical dependency: the request of a dependency is an expression'\n    new webpack.ContextReplacementPlugin(\n      /(.+)?angular(\\\\|\\/)core(.+)?/,\n      path.join(__dirname, 'src'), // location of your src\n      {} // a map of your routes\n    ),\n    new webpack.ContextReplacementPlugin(\n      /(.+)?express(\\\\|\\/)(.+)?/,\n      path.join(__dirname, 'src'),\n      {}\n    )\n  ]\n};\n```\n\n\nFinally i create ```\ntsconfig.server.json```\n :\n\n```\n{\n  \"extends\": \"../tsconfig.json\",\n  \"compilerOptions\": {\n    \"outDir\": \"../out-tsc/app\",\n    \"baseUrl\": \"./\",\n    \"module\": \"commonjs\",\n    \"types\": []\n  },\n  \"exclude\": [\n    \"test.ts\",\n    \"**/*.spec.ts\"\n  ],\n  \"angularCompilerOptions\": {\n    \"entryModule\": \"app/app.server.module#AppServerModule\"\n  }\n}\n```\n\n\nBelow lines are added to my ```\npackage.json```\n :\n\n```\n\"scripts\": {\n    ...\n    \"build:universal\": \"npm run build:client-and-server-bundles && npm run webpack:server\",\n    \"serve:universal\": \"node dist/server.js\",\n    \"build:client-and-server-bundles\": \"ng build --prod && ng build --prod --app 1 --output-hashing=false\",\n    \"webpack:server\": \"webpack --config webpack.server.config.js --progress --colors\"\n    ...\n}\n```\n\n\nHere is my ```\nangular-Cli.json```\n :\n\n```\n{\n  \"$schema\": \"./node_modules/@angular/cli/lib/config/schema.json\",\n  \"project\": {\n    \"name\": \"angularuniversaldemo\"\n  },\n  \"apps\": [\n    {\n      \"root\": \"src\",\n      \"outDir\": \"dist\",\n      \"assets\": [\n        \"assets\",\n        \"favicon.ico\"\n      ],\n      \"index\": \"index.html\",\n      \"main\": \"main.ts\",\n      \"polyfills\": \"polyfills.ts\",\n      \"test\": \"test.ts\",\n      \"tsconfig\": \"tsconfig.app.json\",\n      \"testTsconfig\": \"tsconfig.spec.json\",\n      \"prefix\": \"app\",\n      \"styles\": [\n        \"styles.css\"\n      ],\n      \"scripts\": [],\n      \"environmentSource\": \"environments/environment.ts\",\n      \"environments\": {\n        \"dev\": \"environments/environment.ts\",\n        \"prod\": \"environments/environment.prod.ts\"\n      }\n    }\n  ],\n  \"e2e\": {\n    \"protractor\": {\n      \"config\": \"./protractor.conf.js\"\n    }\n  },\n  \"lint\": [\n    {\n      \"project\": \"src/tsconfig.app.json\",\n      \"exclude\": \"**/node_modules/**\"\n    },\n    {\n      \"project\": \"src/tsconfig.spec.json\",\n      \"exclude\": \"**/node_modules/**\"\n    },\n    {\n      \"project\": \"e2e/tsconfig.e2e.json\",\n      \"exclude\": \"**/node_modules/**\"\n    }\n  ],\n  \"test\": {\n    \"karma\": {\n      \"config\": \"./karma.conf.js\"\n    }\n  },\n  \"defaults\": {\n    \"styleExt\": \"css\",\n    \"component\": {}\n  }\n}\n```\n\n\nI try to create build and serve my app :\n\n```\nnpm run build:universal\nnpm run serve:universal.\n```\n\n\nDoes not create build and run app.\n\nI am getting these errors :\n\n1.\n\n```\nERROR in Error: Metadata version mismatch for module F:/anitha/projects/Universal/universaldemo/node_modules/@angular/platform-server/platform-server.d.ts, found version 4, expected 3...\nnpm ERR! Windows_NT 10.0.15063....\n```\n\n\n2.\n\n```\n Cannot find name 'process' and Cannot find name 'require'.\n```\n\n\n3.\n\n```\n Unable to find app with name or index. Verify the configuration in `.angular-cli.json`\nnpm ERR! code ELIFECYCLE\nnpm ERR! errno 1\nnpm ERR! angularuniversaldemo@0.0.0 build:client-and-server-bundles: `ng build --prod && ng build --prod --app 1 --output-hashing=false`\nnpm ERR! Exit status 1\n```\n\n\nI need run and build my app. How can I do this?\n\nKindly advice me,\n\nThanks.\n    ", "Answer": "\r\n1. Version missmatch\n\nIt looks like you're using a wrong version of the platform-server module.\n\nYou can try to force the dependency version to the one needed in your project directly in this file\n\n2. ts-node for typescript scripts\n\nYou also have to you start your server.ts script with ```\nts-node```\n rather than only ```\nnode```\n :\n\n```\nnpm install ts-node --save```\n \n\nand change your ```\npackage.json```\n script with :\n\n```\n\"serve:universal\": \"ts-node dist/server.ts\n```\n\n\n3. angular-cli.json multiple apps\n\nTo differenciate builds, you can define multiple apps in your ```\nangular-cli.json```\n file, like this :\n\n```\n\"apps\": [\n{\n  \"name\": \"app\",\n  \"root\": \"src\",\n  \"outDir\": \"dist\",\n  ...\n},\n{\n  \"name\": \"universal\",\n  \"root\": \"src\",\n  \"outDir\": \"dist-universal\",\n  ...\n},\n{\n  \"name\": \"server\",\n  \"platform\": \"server\",\n  \"root\": \"src\",\n  ...\n}\n],\n```\n\n\nAnd then, you can choose which app to launch with the ```\n--app```\n flag :\n\n```\nng build/serve --app app/universal/server\n```\n\n\nThe name must match the ones in your ```\nangular-cli.json```\n file, so in your case your app must be named ```\n1```\n.\nOr, if you need only one app/config, you can run :```\nng build (--prod/aot...)```\n without ```\n--app```\n\n\n4. ./dist path error\n\nBe careful about the path of your ```\ndist```\n folder. Check these parameters :\n\ntsconfig.**.json:\n\n```\n...\n\"genDir\": \"../dist\"\n...\n```\n\n\nangular-cli.json:\n\n```\n...\n\"outDir\": \"./dist\",\n...\n```\n\n\nThese path are relative and have to match with path defined in your ```\nserver.ts```\n file\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Angular Universal renders page in wrong way without events\r\n                \r\nI migrated this page: https://www.beta.rohub.org to Angular 7  and started to make this page served on Server side rendering by using Angular universal.\nThe page is rendered, template works, some requests are working, and the page looks normal.\n\nThere are some things which are not working.\n\nFor Example:\n\n\nclick events - are not working totally, there are only buttons not\nworking\nview child\nDirectives\nNgModel - values from ngmodel are not working after change\nlocal storage and cache\nrouting - some components can't initialize after going to example on component for showing research Object.\nHost Listeners\n\n\nThe Dist folder is generating correctly.\n\nHere is my code. I don't know what is wrong with this all configs. \n\nangular.json\n\n```\n{\n  \"$schema\": \"./node_modules/@angular/cli/lib/config/schema.json\",\n  \"version\": 1,\n  \"newProjectRoot\": \"projects\",\n  \"projects\": {\n    \"ng-universal-demo\": {\n      \"projectType\": \"application\",\n      \"root\": \"\",\n      \"sourceRoot\": \"src\",\n      \"schematics\": {\n        \"@schematics/angular:component\": {\n          \"style\": \"css\"\n        }\n      },\n      \"prefix\": \"app\",\n      \"architect\": {\n        \"build\": {\n          \"builder\": \"@angular-builders/custom-webpack:browser\",\n          \"options\": {\n            \"outputPath\": \"dist/browser\",\n            \"index\": \"src/index.html\",\n            \"main\": \"src/main.browser.ts\",\n            \"tsConfig\": \"tsconfig.browser.json\",\n            \"polyfills\": \"src/polyfills.ts\",\n            \"aot\": true,\n            \"assets\": [\n              {\n                \"glob\": \"**/*\",\n                \"input\": \"src/assets\",\n                \"output\": \"/assets\"\n              },\n              {\n                \"glob\": \"favicon.ico\",\n                \"input\": \"src\",\n                \"output\": \"/\"\n              }\n            ],\n            \"styles\": [\n              \"src/styles.css\",\n              \"src/bootstrap.min.css\",\n              \"./node_modules/ng2-tree/styles.css\"\n            ],\n            \"scripts\": [\"./node_modules/quill/dist/quill.min.js\"],\n            \"customWebpackConfig\": {\n              \"path\": \"./extra-webpack.config.js\",\n              \"replaceDuplicatePlugins\": true,\n              \"mergeStrategies\": {\n                \"externals\": \"prepend\"\n              }\n            },\n            \"serviceWorker\": true\n          },\n          \"configurations\": {\n            \"production\": {\n              \"optimization\": true,\n              \"outputHashing\": \"all\",\n              \"sourceMap\": false,\n              \"extractCss\": true,\n              \"namedChunks\": false,\n              \"aot\": true,\n              \"extractLicenses\": true,\n              \"vendorChunk\": false,\n              \"buildOptimizer\": true,\n              \"fileReplacements\": [\n                {\n                  \"replace\": \"src/environments/environment.ts\",\n                  \"with\": \"src/environments/environment.prod.ts\"\n                }\n              ]\n            }\n          }\n        },\n        \"serve\": {\n          \"builder\": \"@angular-devkit/build-angular:dev-server\",\n          \"options\": {\n            \"browserTarget\": \"ng-universal-demo:build\"\n          },\n          \"configurations\": {\n            \"production\": {\n              \"browserTarget\": \"ng-universal-demo:build:production\"\n            }\n          }\n        },\n        \"extract-i18n\": {\n          \"builder\": \"@angular-devkit/build-angular:extract-i18n\",\n          \"options\": {\n            \"browserTarget\": \"ng-universal-demo:build\"\n          }\n        },\n        \"test\": {\n          \"builder\": \"@angular-devkit/build-angular:karma\",\n          \"options\": {\n            \"main\": \"src/test.ts\",\n            \"karmaConfig\": \"./karma.conf.js\",\n            \"polyfills\": \"src/polyfills.ts\",\n            \"tsConfig\": \"src/tsconfig.spec.json\",\n            \"scripts\": [],\n            \"styles\": [\n              \"src/styles.css\"\n            ],\n            \"assets\": [\n              {\n                \"glob\": \"**/*\",\n                \"input\": \"src/assets\",\n                \"output\": \"/assets\"\n              },\n              {\n                \"glob\": \"favicon.ico\",\n                \"input\": \"src\",\n                \"output\": \"/\"\n              }\n            ]\n          }\n        },\n        \"lint\": {\n          \"builder\": \"@angular-devkit/build-angular:tslint\",\n          \"options\": {\n            \"tsConfig\": [\n              \"src/tsconfig.app.json\"\n            ],\n            \"exclude\": [\n              \"**/node_modules/**\"\n            ]\n          }\n        },\n        \"server\": {\n          \"builder\": \"@angular-devkit/build-angular:server\",\n          \"options\": {\n            \"outputPath\": \"dist/server\",\n            \"main\": \"src/main.server.ts\",\n            \"tsConfig\": \"tsconfig.server.json\"\n          },\n          \"configurations\": {\n            \"production\": {\n              \"fileReplacements\": [\n                {\n                  \"replace\": \"src/environments/environment.ts\",\n                  \"with\": \"src/environments/environment.prod.ts\"\n                }\n              ]\n            }\n          }\n        }\n      }\n    },\n    \"ng-universal-demo-e2e\": {\n      \"root\": \"e2e\",\n      \"projectType\": \"application\",\n      \"cli\": {},\n      \"schematics\": {},\n      \"architect\": {\n        \"e2e\": {\n          \"builder\": \"@angular-devkit/build-angular:protractor\",\n          \"options\": {\n            \"protractorConfig\": \"./protractor.conf.js\",\n            \"devServerTarget\": \"ng-universal-demo:serve\"\n          }\n        },\n        \"lint\": {\n          \"builder\": \"@angular-devkit/build-angular:tslint\",\n          \"options\": {\n            \"tsConfig\": [],\n            \"exclude\": [\n              \"**/node_modules/**\"\n            ]\n          }\n        }\n      }\n    }\n  },\n  \"cli\": {},\n  \"schematics\": {\n    \"@schematics/angular:class\": {\n      \"spec\": false\n    },\n    \"@schematics/angular:component\": {\n      \"spec\": false,\n      \"inlineStyle\": true,\n      \"inlineTemplate\": true,\n      \"prefix\": \"app\",\n      \"styleext\": \"css\"\n    },\n    \"@schematics/angular:directive\": {\n      \"spec\": false,\n      \"prefix\": \"app\"\n    },\n    \"@schematics/angular:guard\": {\n      \"spec\": false\n    },\n    \"@schematics/angular:module\": {\n      \"spec\": false\n    },\n    \"@schematics/angular:pipe\": {\n      \"spec\": false\n    },\n    \"@schematics/angular:service\": {\n      \"spec\": false\n    }\n  }\n```\n\n\npackage.json:\n\n```\n\n  \"name\": \"ng-universal-demo\",\n  \"version\": \"0.0.0\",\n  \"license\": \"MIT\",\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"https://github.com/angular/universal-starter.git\"\n  },\n  \"contributors\": [],\n  \"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"ng serve\",\n    \"build\": \"ng build\",\n    \"lint\": \"ng lint ng-universal-demo\",\n    \"build:client-and-server-bundles\": \"ng build --prod  --output-hashing=none && ng run ng-universal-demo:server:production\",\n    \"build:prerender\": \"npm run build:client-and-server-bundles && npm run compile:server && npm run generate:prerender\",\n    \"build:ssr\": \"npm run build:client-and-server-bundles && npm run webpack:server\",\n    \"compile:server\": \"tsc -p server.tsconfig.json\",\n    \"webpack:server\": \"webpack --config webpack.server.config.js --progress --colors\",\n    \"generate:prerender\": \"cd dist && node prerender\",\n    \"serve:prerender\": \"cd dist/browser && http-server\",\n    \"serve:\nssr\": \"node dist/server\"\n  },\n```\n\n\nserver.ts\n\n```\nimport 'zone.js/dist/zone-node';\nimport 'reflect-metadata';\nimport { enableProdMode } from '@angular/core';\nimport * as express from 'express';\nimport { join } from 'path';\nimport { readFileSync } from 'fs';\nenableProdMode();\n\nconst app = express();\nconst PORT = process.env.PORT || 3000;\nconst DIST_FOLDER = join(process.cwd(), 'dist');\n\nconst template = readFileSync(join(DIST_FOLDER, 'browser', 'index.html')).toString();\nconst { AppServerModuleNgFactory, LAZY_MODULE_MAP } = require( './dist/server/main');\nimport { ngExpressEngine } from '@nguniversal/express-engine';\nimport { provideModuleMap } from '@nguniversal/module-map-ngfactory-loader';\n\napp.engine('html', ngExpressEngine({\n  bootstrap: AppServerModuleNgFactory,\n  providers: [\n    provideModuleMap(LAZY_MODULE_MAP)\n  ]\n}));\n\napp.set('view engine', 'html');\napp.set('views', join(DIST_FOLDER, 'browser'));\n\n\napp.get('*.*', express.static(join(DIST_FOLDER, 'browser')));\n\napp.get('*', (req, res) => {\n  res.render('index', { req });\n});\n\napp.listen(PORT, () => {\n  console.log(`Node Express server listening on http://localhost:${PORT}`);\n});\n```\n\n\ntsconfig.json\n\n```\n{\n  \"compileOnSave\": false,\n  \"compilerOptions\": {\n    \"baseUrl\": \"./\",\n    \"outDir\": \"./dist/out-tsc\",\n    \"sourceMap\": true,\n    \"declaration\": false,\n    \"module\": \"esnext\",\n    \"moduleResolution\": \"node\",\n    \"emitDecoratorMetadata\": true,\n    \"experimentalDecorators\": true,\n    \"importHelpers\": true,\n    \"target\": \"es5\",\n    \"typeRoots\": [\n      \"node_modules/@types\"\n    ],\n    \"lib\": [\n      \"es2017\",\n      \"dom\"\n    ],\n    \"skipLibCheck\": true\n  }\n}\n```\n\n\ntsconfig.app.json\n\n```\n{\n  \"extends\": \"../tsconfig.json\",\n  \"compilerOptions\": {\n    \"outDir\": \"../out-tsc/app\",\n    \"baseUrl\": \"./\",\n    \"module\": \"es2015\",\n    \"types\": [\n      \"node\"\n    ]\n  },\n  \"exclude\": [\n    \"test.ts\",\n    \"**/*.spec.ts\"\n  ],\n  \"angularCompilerOptions\": {\n    \"entryModule\": \"app/app.server.module#AppServerModule\"\n  }\n}\n\n```\n\n\nwebpack.server.config.js\n\n```\nvar path = require('path');\nvar webpack = require('webpack');\n\nmodule.exports = {\n  entry: { server: './server.ts' },\n  resolve: { extensions: ['.js', '.ts'] },\n  target: 'node',\n  mode: 'none',\n  // this makes sure we include node_modules and other 3rd party libraries\n  externals: [/node_modules/],\n  output: {\n    path: path.join(__dirname, 'dist'),\n    filename: '[name].js'\n  },\n  module: {\n    rules: [{ test: /\\.ts$/, loader: 'ts-loader' }]\n  },\n  plugins: [\n    // Temporary Fix for issue: https://github.com/angular/angular/issues/11580\n    // for 'WARNING Critical dependency: the request of a dependency is an expression'\n    new webpack.ContextReplacementPlugin(\n      /(.+)?angular(\\\\|\\/)core(.+)?/,\n      path.join(__dirname, 'src'), // location of your src\n      {} // a map of your routes\n    ),\n    new webpack.ContextReplacementPlugin(\n      /(.+)?express(\\\\|\\/)(.+)?/,\n      path.join(__dirname, 'src'),\n      {}\n    )\n  ]\n};\n```\n\n\nmain.server.ts\n\n```\nexport {AppServerModule} from './app/app.server.module';\nimport { enableProdMode } from '@angular/core';\n\nenableProdMode();\n\n\n```\n\n    ", "Answer": "\r\nI solved problem.\nA firts thing was docuemnt.cookies and window. on my ts code. So I modificated cookies by ngx-cookies-service like this:\ncommons.service.ts\n\n```\nimport {CookieService} from 'ngx-cookie-service';\n\n...\n\nconstructor(private configurationService: ConfigurationService,\n              private cookies: CookieService) {\n\n  }\n\n....\n\nif (name === 'accountType') {\n      const cookiesArray = Object.keys(this.cookies.getAll());\n      console.log(cookiesArray);\n      if (cookiesArray.length >= 1) {\n        console.log('COOKIES', cookiesArray, 'END OF COOKIES');\n        const objectOfCookies = this.cookies.getAll(),\n          arrayOfCookies = Object.keys(objectOfCookies),\n          typeIndex = arrayOfCookies.indexOf(name);\n\n        if (typeIndex !== -1) {\n          const arrayOfAccounts = ['ORCID', 'GOOGLE', 'EVEREST'];\n          for (const currentElement of arrayOfAccounts) {\n            if (arrayOfCookies.indexOf(currentElement) !== -1) {\n              this.properCookieValue = currentElement;\n              break;\n            }\n          }\n        } else {\n          this.properCookieValue = null;\n        }\n      }\n    } else {\n\n```\n\n\nSecond thing: Everything with my files was ok, the only thing on server.ts needed a change. \nSo server.ts looks like that:\n\n```\nimport 'zone.js/dist/zone-node';\nimport 'reflect-metadata';\nimport { enableProdMode } from '@angular/core';\nimport * as express from 'express';\nimport * as cookieParser from 'cookie-parser';\nimport { join } from 'path';\nimport { readFileSync } from 'fs';\nenableProdMode();\nconst domino = require('domino');\nconst app = express();\nconst PORT = process.env.PORT || 3000;\nconst DIST_FOLDER = join(process.cwd(), 'dist');\n\n\nconst { AppServerModuleNgFactory, LAZY_MODULE_MAP } = require( './dist/server/main');\nimport { ngExpressEngine } from '@nguniversal/express-engine';\nimport { provideModuleMap } from '@nguniversal/module-map-ngfactory-loader';\n\napp.engine('html', ngExpressEngine({\n  bootstrap: AppServerModuleNgFactory,\n  providers: [\n    provideModuleMap(LAZY_MODULE_MAP)\n  ]\n}));\n\n\nconst template = readFileSync(join(DIST_FOLDER, 'browser', 'index.html')).toString();\nconst win = domino.createWindow(template);\nglobal['window'] = win;\nglobal['document'] = win.document;\napp.set('view engine', 'html');\napp.set('views', join(DIST_FOLDER, 'browser'));\n\napp.use(cookieParser('Your private token'));\napp.get('*.*', express.static(join(DIST_FOLDER, 'browser')));\n\napp.get('*', (req, res) => {\n  res.render('index', { req });\n});\n\napp.listen(PORT, () => {\n  console.log(`Node Express server listening on http://localhost:${PORT}`);\n});\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "how to add dynamic link tag prefetch with hash files on angular8 SSR\r\n                \r\nI'm using angular 8 as SSR or angular universal, we found very difficult adding dynamic prefetch link tag with the main angular files.\n\nAfter building as ssr, the code inject at the end of the index.html a line like this,\n\n```\n<script src=\"runtime.58b180e8c30580ec9ea9.js\" defer></script><script src=\"polyfills-es5.ea80a4cb53fa86a5c510.js\" nomodule defer></script><script src=\"polyfills.c8c0431bcdc0afdb2644.js\" defer></script><script src=\"scripts.2bbb0f7e81eb835e1cea.js\" defer></script><script src=\"vendor.e7551457890efc085d4b.js\" defer></script><script src=\"main.80bdfc18f9240a3efe5c.js\" defer></script></body>\n\n</html>\n```\n\n\nHowever, on the head tag we need to have something like this\n\n```\n  <link rel=\"prefetch\" as=\"script\" href=\"/runtime.js\">\n  <link rel=\"prefetch\" as=\"script\" href=\"/polyfills-es5.js\">\n  <link rel=\"prefetch\" as=\"script\" href=\"/polyfills.js\">\n  <link rel=\"prefetch\" as=\"script\" href=\"/scripts.js\">\n  <link rel=\"prefetch\" as=\"script\" href=\"/vendor.js\">\n```\n\n\nThe problem is that every build will have a different hash so we need an option to detect the hashing on the server before rendering the page.\n\nAny ideas, how we can do this?\n\nAngular 8 as SSR, with express.js\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Easily Memorable Hash (what 3 words)\r\n                \r\nI was hoping to create an easily memorable hash something like 3 random words (what3words), so the idea would be to hash a java object and the result was three random words.\nUse case: I have objects with many fields and I need to compress the field into 24 chars (this is the size of a varchar primary key column in a database where these objects are stored, and cannot be changed), the resulting compressed value should also be easily memorable.\nInitially, I decided to use 3 different hash functions (namely FNV1a64Hash, CRC32Hash and DJB2) to create 3 pre-hashes and then use those values as indexes into the dictionary however this resulted in a lot of collisions (```\nRandom Words tried: 10000000 No of collisions: 9272419```\n). Note my dictionary size is approx 50k words.\nNext, I decided to just call ```\nhashCode()```\n on the object, then pad the resulting int and finally split it into 3 chunks of 5 digits, unfortunately again a lot of collisions occurred (```\nRandom Words tried: 10000000 No of collisions: 9999900```\n). I think this in part could be down to the max size of an int being 2^31 which is only a 10 digit number so the first index was always 00000.\nI've also used universal hashing but again I got a fair amount of collisions (```\nRandom Words tried: 10000000 No of collisions: 9996436```\n)\nI am wondering if I am missing something obvious here or if anyone knows of any well-known algorithms that might be able to help here? Apologies in advance for the noob question, this is the first time I've encountered hashing and there is a whole lot to learn.\nI've pasted my code & test code below in case there is something obviously wrong with that.\n```\npublic static String generate3Words1(Object obj) {\n    BigInteger input = BigInteger.valueOf(obj.hashCode());\n    int index1 = indexFor(CRC32Hash(input.toByteArray()));\n    int index2 = indexFor(FNV1a64Hash(input.toByteArray()));\n    int index3 = indexFor(DBJ2(input.toByteArray()));\n    return dictionary.get(index1) + \"-\" + dictionary.get(index2) + \"-\" + dictionary.get(index3);\n}\n\n\npublic String generate3Words2(Object obj) {\n       int h = (h = obj.hashCode()) ^ (h >>> 16);\n       String i = String.format(\"%015d\", h);      \n       String s = dictionary.get(indexFor(Integer.parseInt(i.substring(0, 5)))) + \"-\" + dictionary.get(indexFor(Integer.parseInt(i.substring(5, 10)))) + \"-\" + dictionary.get(indexFor(Integer.parseInt(i.substring(10, 15))));\n       return s.length() > MAX_LEN ? s.substring(0, MAX_LEN) : s;\n   }\n\nprivate static int indexFor(long h) {\n    return (int) (h & (ThreeWordHash.dictionary.size() - 1));\n}\n\nprivate static long FNV1a64Hash(byte[] data) {\n    long hash = 0xcbf29ce484222325L;\n    for (byte datum : data) {\n        hash ^= (datum & 0xff);\n        hash *= 1099511628211L;\n    }\n    return hash;\n}\n\nprivate static long CRC32Hash(byte[] data) {\n    CRC32.reset();\n    CRC32.update(data);\n    return CRC32.getValue();\n}\n\n\nprivate static long DBJ2(byte[] data) {\n    long hash = 5381;\n    for (byte datum : data) {\n        hash = ((hash << 5) + hash) + datum;\n    }\n    return hash;\n}\n\nprivate static String universalHashing(Object data) {\n     int[] hashCodes = new int[NO_OF_WORDS];\n     int hashCodeSizeDiff = WORD_SIZE - (WORD_SIZE / 2);\n     int hstart = data.hashCode();\n     int bmax = 1 << hashCodeSizeDiff;\n     for (int i = 0; i < NO_OF_WORDS; i++) {\n         hashCodes[i] = (((hstart * (i * 2 + 1)) + RAND.nextInt(bmax)) >> hashCodeSizeDiff) & (ThreeWordHash.dictionary.size() - 1);\n     }\n     String s = ThreeWordHash.dictionary.get(hashCodes[0]) + \" \" + ThreeWordHash.dictionary.get(hashCodes[1]) + \" \" + ThreeWordHash.dictionary.get(hashCodes[2]);\n     return s.length() > MAX_LEN ? s.substring(0, MAX_LEN) : s;\n }\n\n```\n\nTest code:\n```\n@Test\nvoid generate3Words() {\n    List<String> words = new ArrayList<>(TestDictionary.WORDS);\n    words.addAll(TestDictionary.WORDS);\n\n    Random random = new Random(1);\n    HashSet<String> seen1 = new HashSet<>();\n    HashSet<String> seen2 = new HashSet<>();\n    \n    int count = 0;\n    int noOfIterations = 10000000;\n    \n    //NOTE test dict size approx 4k words\n    for (int j = 0; j < noOfIterations; j++) {\n        String randomWord =  new StringBuilder()\n                .append(words.get(random.nextInt(TestDictionary.WORDS.size())))\n                .append(words.get(random.nextInt(TestDictionary.WORDS.size())))\n                .append(words.get(random.nextInt(TestDictionary.WORDS.size())))\n                .append(words.get(random.nextInt(TestDictionary.WORDS.size())))\n                .append(words.get(random.nextInt(TestDictionary.WORDS.size()))).toString();\n      \n        String res = ThreeWordHash.generate3Words(randomWord);\n        \n        if (seen1.contains(res) && !seen2.contains(randomWord)) {\n            count++;\n        }\n\n        seen2.add(randomWord);\n        seen1.add(res);\n    }\n    System.out.println(\"Random Words tried: \" + seen2.size() + \" No of collisions: \" + count);\n}\n```\n\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Angular Universal - Server rendered app not functioning as expected\r\n                \r\nI've followed several workthroughs https://youtu.be/gxCu5TEmxXE on deploying Angular Universal and am currently doing so on firebase. I've successfully got the server rendering the app, BUT for some reason the app isn't bootstrapping from the server code. What I mean to say, is that when I load the app rendered by the server, the actual app itself doesn't function correctly.\n\nI'm not sure if this is because of incompatibility for example with Angular Material https://github.com/angular/universal#in-progress or if I've made some errors in the configuration.\n\nFor example, if I load up my homepage via the normal compiled app and navigate to a form, I see the 'expected' form. However, if I load same page via the server rendering then I see the rendered form but I cannot interact with it.\n\nRendered form that you cannot interact with\n\nExpected result if you navigate through the app\n\nIt feels like I am missing a key part of in the integration to make the server side rendered HTML transition into the normal app. What am I missing to make sure that the server rendered HTML transitions and functions in the same way as the normal app?\n\napp.server.module\n\r\n\r\n```\nimport {NgModule} from '@angular/core';\r\nimport {ServerModule} from '@angular/platform-server';\r\n\r\nimport {AppModule} from './app.module';\r\nimport {AppComponent} from './app.component';\r\n\r\n@NgModule({\r\n  imports: [\r\n    // The AppServerModule should import your AppModule followed\r\n    // by the ServerModule from @angular/platform-server.\r\n    AppModule,\r\n    ServerModule\r\n  ],\r\n  // Since the bootstrapped component is not inherited from your\r\n  // imported AppModule, it needs to be repeated here.\r\n  bootstrap: [AppComponent],\r\n})\r\nexport class AppServerModule {}```\n\r\n\r\n\r\n\n\nserver.ts\n\n\r\n\r\n```\nimport * as functions from 'firebase-functions';\r\nimport * as angularUniversal from 'angular-universal-express-firebase';\r\n\r\nexport let ssrapp = angularUniversal.trigger({\r\n\tindex: __dirname + '/browser/index.html',\r\n\tmain: __dirname + '/server/main.bundle',\r\n\tenableProdMode: true,\r\n\tbrowserCacheExpiry: 1200,\r\n\tcdnCacheExpiry: 600\r\n});```\n\r\n\r\n\r\n\nfirebase.json\n\n\r\n\r\n```\n{\r\n  \"hosting\": {\r\n    \"public\": \"dist\",\r\n    \"ignore\": [\r\n      \"firebase.json\",\r\n      \"**/.*\",\r\n      \"**/node_modules/**\"\r\n    ],\r\n    \"rewrites\": [\r\n      {\r\n        \"source\": \"**\",\r\n        \"function\": \"ssrapp\"\r\n      }\r\n    ]\r\n  },\r\n  \"functions\": {\r\n    \"predeploy\": \"npm --prefix functions run build\",\r\n    \"source\": \"functions\"\r\n  }\r\n}```\n\r\n\r\n\r\n\n\npackage.json\n\r\n\r\n```\n  \"scripts\": {\r\n    \"ng\": \"ng\",\r\n    \"start\": \"ng serve\",\r\n    \"build\": \"ng build\",\r\n    \"build:client-and-server-bundles\": \"ng build --prod && ng build --prod --app 1 --output-hashing=false\",\r\n    \"build:prerender\": \"npm run build:client-and-server-bundles && npm run webpack:server && npm run generate:prerender\",\r\n    \"build:ssr\": \"npm run build:client-and-server-bundles && npm run webpack:server\",\r\n    \"generate:prerender\": \"cd dist && node prerender\",\r\n    \"webpack:server\": \"webpack --config webpack.server.config.js --progress --colors\",\r\n    \"serve:prerender\": \"cd dist/browser && http-server\",\r\n    \"serve:ssr\": \"node dist/server\"\r\n  },\r\n  \"private\": true,\r\n  \"dependencies\": {\r\n    \"@angular/animations\": \"^5.1.0\",\r\n    \"@angular/cdk\": \"5.0.1\",\r\n    \"@angular/common\": \"^5.1.0\",\r\n    \"@angular/compiler\": \"^5.1.0\",\r\n    \"@angular/core\": \"^5.1.0\",\r\n    \"@angular/flex-layout\": \"git+https://github.com/angular/flex-layout-builds.git\",\r\n    \"@angular/forms\": \"^5.1.0\",\r\n    \"@angular/http\": \"^5.1.0\",\r\n    \"@angular/material\": \"5.0.1\",\r\n    \"@angular/material-moment-adapter\": \"^5.0.0\",\r\n    \"@angular/platform-browser\": \"^5.1.0\",\r\n    \"@angular/platform-browser-dynamic\": \"^5.1.0\",\r\n    \"@angular/platform-server\": \"^5.2.0-beta.0\",\r\n    \"@angular/router\": \"^5.1.0\",\r\n    \"@angular/service-worker\": \"^5.1.0\",\r\n    \"@nguniversal/express-engine\": \"^5.0.0-beta.5\",\r\n    \"@nguniversal/module-map-ngfactory-loader\": \"^5.0.0-beta.5\",\r\n    \"@types/moment\": \"^2.13.0\",\r\n    \"angular-ssr\": \"^0.10.40\",\r\n    \"angular2-universal\": \"^2.1.0-rc.1\",\r\n    \"body-parser\": \"^1.18.2\",\r\n    \"braintree-web\": \"^3.26.0\",\r\n    \"core-js\": \"^2.4.1\",\r\n    \"moment\": \"^2.20.1\",\r\n    \"rxjs\": \"^5.5.2\",\r\n    \"zone.js\": \"^0.8.14\"\r\n  },```\n\r\n\r\n\r\n\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "hashing function guaranteed to be unique?\r\n                \r\nIn our app we're going to be handed png images along with a ~200 character byte array.  I want to save the image with a filename corresponding to that bytearray, but not the bytearray itself, as i don't want 200 character filenames.  So, what i thought was that i would save the bytearray into the database, and then MD5 it to get a short filename.  When it comes time to display a particular image, i look up its bytearray, MD5 it, then look for that file.  \n\nSo far so good.  The problem is that potentially two different bytearrays could hash down to the same MD5. Then, one file would effectively overwrite another.  Or could they?  I guess my questions are\n\n\nCould two ~200 char bytearrays MD5-hash down to the same string?\nIf they could, is it a once-per-10-ages-of-the-universe sort of deal or something that could conceivably happen in my app?\nIs there a hashing algorithm that will produce a (say) 32 char string that's guaranteed to be unique?\n\n    ", "Answer": "\r\nIt's logically impossible to get a 32 byte code from a 200 byte source which is unique among all possible 200 byte sources, since you can store more information in 200 bytes than in 32 bytes.\n\nThey only exception would be that the information stored in these 200 bytes would also fit into 32 bytes, in which case your source date format would be extremely inefficient and space-wasting.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Deploying angular-universal app to heroku\r\n                \r\nIve been trying to deploy my angular-universal start application to heroku. I've set up a few things as I normally do with angular node applications but for this I might be doing something wrong. \n\nHere is the error I am getting\n\n```\n2017-12-18T07:16:23.453463+00:00 heroku[web.1]: Error R10 (Boot timeout) -> Web process failed to bind to $PORT within 60 seconds of launch\n```\n\n\nBut I am dynamically setting PORT.\n\nHere is my folder structure\n\n\n\nI believe it may be something wrong with my settings.\nPackage.json\n\n```\n{\n  \"name\": \"ng-universal-demo\",\n  \"version\": \"0.0.0\",\n  \"license\": \"MIT\",\n  \"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"ng serve\",\n    \"build\": \"ng build\",\n    \"build:client-and-server-bundles\": \"ng build --prod && ng build --prod --app 1 --output-hashing=false\",\n    \"build:prerender\": \"npm run build:client-and-server-bundles && npm run webpack:server && npm run generate:prerender\",\n    \"build:ssr\": \"npm run build:client-and-server-bundles && npm run webpack:server\",\n    \"generate:prerender\": \"cd dist && node prerender\",\n    \"webpack:server\": \"webpack --config webpack.server.config.js --progress --colors\",\n    \"serve:prerender\": \"cd dist/browser && http-server\",\n    \"serve:prerender:prod\": \"cd dist/browser && node ./bin/www\",\n    \"serve:ssr\": \"node dist/server\",\n    \"postinstall\": \"npm run build:ssr\",\n    \"deploy\": \"git push origin master && git push heroku master\"\n  },\n  \"private\": true,\n  \"dependencies\": {\n    \"@angular/animations\": \"^5.0.0\",\n    \"@angular/cli\": \"^1.5.0\",\n    \"@angular/common\": \"^5.0.0\",\n    \"@angular/compiler\": \"^5.0.0\",\n    \"@angular/compiler-cli\": \"^5.0.0\",\n    \"@angular/core\": \"^5.0.0\",\n    \"@angular/forms\": \"^5.0.0\",\n    \"@angular/http\": \"^5.0.0\",\n    \"@angular/language-service\": \"^5.0.0\",\n    \"@angular/platform-browser\": \"^5.0.0\",\n    \"@angular/platform-browser-dynamic\": \"^5.0.0\",\n    \"@angular/platform-server\": \"^5.0.0\",\n    \"@angular/router\": \"^5.0.0\",\n    \"@nguniversal/express-engine\": \"^5.0.0-beta.5\",\n    \"@nguniversal/module-map-ngfactory-loader\": \"^5.0.0-beta.5\",\n    \"@types/node\": \"^8.0.30\",\n    \"body-parser\": \"^1.18.2\",\n    \"cookie-parser\": \"^1.4.3\",\n    \"core-js\": \"^2.4.1\",\n    \"cpy-cli\": \"^1.0.1\",\n    \"express\": \"^4.15.2\",\n    \"http-server\": \"^0.10.0\",\n    \"morgan\": \"^1.9.0\",\n    \"reflect-metadata\": \"^0.1.10\",\n    \"rxjs\": \"^5.5.2\",\n    \"ts-loader\": \"^2.3.7\",\n    \"typescript\": \"~2.4.2\",\n    \"zone.js\": \"^0.8.14\"\n  },\n  \"devDependencies\": {}\n}\n```\n\n\nBin \n\n```\nvar app = require('../server');\nvar debug = require('debug')('test:server');\nvar http = require('http');\n\nvar port = normalizePort(process.env.PORT || '3000');\napp.set('port', port);\n\nvar server = http.createServer(app);\n\n//models.sequelize.sync({alter: true}).then(function() {\n  server.listen(port, function() {\n    console.log('Express server listening on port ' + server.address().port);\n    });\n  server.on('error', onError);\n  server.on('listening', onListening);\n//});\n\n\n\n\nfunction normalizePort(val) {\n  var port = parseInt(val, 10);\n\n  if (isNaN(port)) {\n    // named pipe\n    return val;\n  }\n\n  if (port >= 0) {\n    // port number\n    return port;\n  }\n\n  return false;\n}\n\nfunction onError(error) {\n  if (error.syscall !== 'listen') {\n    throw error;\n  }\n\n  var bind = typeof port === 'string'\n    ? 'Pipe ' + port\n    : 'Port ' + port;\n\n  // handle specific listen errors with friendly messages\n  switch (error.code) {\n    case 'EACCES':\n      console.error(bind + ' requires elevated privileges');\n      process.exit(1);\n      break;\n    case 'EADDRINUSE':\n      console.error(bind + ' is already in use');\n      process.exit(1);\n      break;\n    default:\n      throw error;\n  }\n}\n\n\nfunction onListening() {\n  var addr = server.address();\n  var bind = typeof addr === 'string'\n    ? 'pipe ' + addr\n    : 'port ' + addr.port;\n  debug('Listening on ' + bind);\n}\n```\n\n\nAnd server.ts\n\n```\nimport 'zone.js/dist/zone-node';\nimport 'reflect-metadata';\nimport { renderModuleFactory } from '@angular/platform-server';\nimport { enableProdMode } from '@angular/core';\n\nimport * as express from 'express';\nimport { join } from 'path';\nimport { readFileSync } from 'fs';\nimport * as cookieParser from 'cookie-parser';\nimport * as bodyParser from \"body-parser\";\nimport * as logger from 'morgan';\n\n// Faster server renders w/ Prod mode (dev mode never needed)\nenableProdMode();\n\n// Express server\nconst app = express();\n\nconst DIST_FOLDER = join(process.cwd(), 'dist');\n\n// Our index.html we'll use as our template\nconst template = readFileSync(join(DIST_FOLDER, 'browser', 'index.html')).toString();\n\n// * NOTE :: leave this as require() since this file is built Dynamically from webpack\nconst { AppServerModuleNgFactory, LAZY_MODULE_MAP } = require('./dist/server/main.bundle');\n\n// Express Engine\nimport { ngExpressEngine } from '@nguniversal/express-engine';\n// Import module map for lazy loading\nimport { provideModuleMap } from '@nguniversal/module-map-ngfactory-loader';\n\n// Our Universal express-engine (found @ https://github.com/angular/universal/tree/master/modules/express-engine)\napp.engine('html', ngExpressEngine({\n  bootstrap: AppServerModuleNgFactory,\n  providers: [\n    provideModuleMap(LAZY_MODULE_MAP)\n  ]\n}));\n\napp.set('view engine', 'html');\napp.set('views', join(DIST_FOLDER, 'browser'));\n\napp.use(logger('dev'));\n\napp.use(bodyParser.json({limit: '50mb'}));\napp.use(bodyParser.urlencoded({extended: true}, {limit: '50mb'}));\napp.use(cookieParser());\n\n/* - Example Express Rest API endpoints -\n  app.get('/api/**', (req, res) => { });\n*/\n\n// Server static files from /browser\napp.get('*.*', express.static(join(DIST_FOLDER, 'browser'), {\n  maxAge: '1y'\n}));\n\n// ALl regular routes use the Universal engine\napp.get('*', (req, res) => {\n  res.render('index', { req });\n});\n\n\nmodule.exports = app;\n```\n\n\nAside from the bin setup Ive used everything the repo started with. Not sure what I have to do in order to get it deployed on heroku\n    ", "Answer": "\r\n```\n\"ng\": \"ng\",\n\"start\": \"npm run serve:ssr\",\n\"build\": \"npm run build:ssr\",\n\"test\": \"ng test\",\n\"lint\": \"ng lint\",\n\"e2e\": \"ng e2e\",\n\"dev:ssr\": \"ng run universal:serve-ssr\",\n\"serve:ssr\": \"node dist/universal/server/main.js\",\n\"build:ssr\": \"ng build --prod && ng run universal:server:production\",\n\"prerender\": \"ng run universal:prerender\"\n```\n\nuse this change start and build script\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Angular 4 with universal\r\n                \r\nI am using angular 2 with a universal.\n\nI am trying to this.\n\nFirst I create a new project :\n\n```\nng new universaldemo.\n```\n\n\nI installed these packages :\n\n```\nnpm install --save @angular/platform-server @nguniversal/module-map-ngfactory-loader ts-loader @nguniversal/express-engine.\n```\n\n\nHere is my app.module.ts :\n\n```\nimport { BrowserModule } from '@angular/platform-browser';\nimport { NgModule } from '@angular/core';\n\nimport { AppComponent } from './app.component';\n\n@NgModule({\n  declarations: [\n    AppComponent\n  ],\n  imports: [\n    BrowserModule.withServerTransition({ appId: 'universaldemo' }),\n  ],\n  providers: [],\n  bootstrap: [AppComponent]\n})\nexport class AppModule { }\n```\n\n\nHere is my app.server.module.ts\n\n```\nimport { NgModule } from '@angular/core';\nimport { ServerModule } from '@angular/platform-server';\nimport { ModuleMapLoaderModule } from '@nguniversal/module-map-ngfactory-loader';\n\nimport { AppModule } from './app.module';\nimport { AppComponent } from './app.component';\n\n@NgModule({\n  imports: [\n    AppModule,\n    ServerModule,\n    ModuleMapLoaderModule\n  ],\n  providers: [\n    // Add universal-only providers here\n  ],\n  bootstrap: [ AppComponent ],\n})\nexport class AppServerModule {}\n```\n\n\nHere is my server.ts :\n\n// These are important and needed before anything else\n\n```\nimport 'zone.js/dist/zone-node';\nimport 'reflect-metadata';\n\nimport { enableProdMode } from '@angular/core';\n\nimport * as express from 'express';\nimport { join } from 'path';\n\n// Faster server renders w/ Prod mode (dev mode never needed)\nenableProdMode();\n\n// Express server\nconst app = express();\n\nconst PORT = process.env.PORT || 4000;\nconst DIST_FOLDER = join(process.cwd(), 'dist');\n\n// * NOTE :: leave this as require() since this file is built Dynamically from webpack\nconst { AppServerModuleNgFactory, LAZY_MODULE_MAP } = require('./dist/server/main.bundle');\n\n// Express Engine\nimport { ngExpressEngine } from '@nguniversal/express-engine';\n// Import module map for lazy loading\nimport { provideModuleMap } from '@nguniversal/module-map-ngfactory-loader';\n\napp.engine('html', ngExpressEngine({\n  bootstrap: AppServerModuleNgFactory,\n  providers: [\n    provideModuleMap(LAZY_MODULE_MAP)\n  ]\n}));\n\napp.set('view engine', 'html');\napp.set('views', join(DIST_FOLDER, 'browser'));\n\n// TODO: implement data requests securely\napp.get('/api/*', (req, res) => {\n  res.status(404).send('data requests are not supported');\n});\n\n// Server static files from /browser\napp.get('*.*', express.static(join(DIST_FOLDER, 'browser')));\n\n// All regular routes use the Universal engine\napp.get('*', (req, res) => {\n  res.render(join(DIST_FOLDER, 'browser', 'index.html'), { req });\n});\n\n// Start up the Node server\napp.listen(PORT, () => {\n  console.log(`Node server listening on http://localhost:${PORT}`);\n});\nHere is my webpack.server.config.js :\n\nconst path = require('path');\nconst webpack = require('webpack');\n\nmodule.exports = {\n  entry: { server: './server.ts' },\n  resolve: { extensions: ['.js', '.ts'] },\n  target: 'node',\n  // this makes sure we include node_modules and other 3rd party libraries\n  externals: [/(node_modules|main\\..*\\.js)/],\n  output: {\n    path: path.join(__dirname, 'dist'),\n    filename: '[name].js'\n  },\n  module: {\n    rules: [{ test: /\\.ts$/, loader: 'ts-loader' }]\n  },\n  plugins: [\n    // Temporary Fix for issue: https://github.com/angular/angular/issues/11580\n    // for 'WARNING Critical dependency: the request of a dependency is an expression'\n    new webpack.ContextReplacementPlugin(\n      /(.+)?angular(\\\\|\\/)core(.+)?/,\n      path.join(__dirname, 'src'), // location of your src\n      {} // a map of your routes\n    ),\n    new webpack.ContextReplacementPlugin(\n      /(.+)?express(\\\\|\\/)(.+)?/,\n      path.join(__dirname, 'src'),\n      {}\n    )\n  ]\n};\n```\n\n\nFinally i create tsconfig.server.json :\n\n```\n{\n  \"extends\": \"../tsconfig.json\",\n  \"compilerOptions\": {\n    \"outDir\": \"../out-tsc/app\",\n    \"baseUrl\": \"./\",\n    \"module\": \"commonjs\",\n    \"types\": []\n  },\n  \"exclude\": [\n    \"test.ts\",\n    \"**/*.spec.ts\"\n  ],\n  \"angularCompilerOptions\": {\n    \"entryModule\": \"app/app.server.module#AppServerModule\"\n  }\n}\n```\n\n\nBelow lines are added to my package.json :\n\n```\n\"scripts\": {\n    ...\n    \"build:universal\": \"npm run build:client-and-server-bundles && npm run webpack:server\",\n    \"serve:universal\": \"node dist/server.js\",\n    \"build:client-and-server-bundles\": \"ng build --prod && ng build --prod --app 1 --output-hashing=false\",\n    \"webpack:server\": \"webpack --config webpack.server.config.js --progress --colors\"\n    ...\n}\n```\n\n\nI try to create build and serve my app :\n\n```\nnpm run build:universal\n```\n\n\nIt works.\n\nI try to run my app.using this command\n\n```\nnpm run serve:universal.\n```\n\n\nI got this error.\n\n\n  ReferenceError: webpackJsonp is not defined at Object\n\n\nHow can I fix this issue?\n    ", "Answer": "\r\nYou are missing the ```\nwebpack.server.config.js```\n file as described in the Universal Wiki : https://github.com/angular/angular-cli/wiki/stories-universal-rendering\n\nDo you Angular 5 or the older 2/4 version? Maybe your package.json is outdated?!\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How does std::unordered_map actually use hash functions?\r\n                \r\nIt is not very clear to me how the standard ```\nstd::unordered_map```\n container uses hashing.\nI am pretty new to hashing, and right now I'm trying to pass my university data structure exams.\nI understand that if I have a collection of objects, I have to group their keys as random as possible by a criteria so that they lie as uniformly as possible in some buckets, and I can afterwards search/insert/delete in constant time by looking into the bucket associated with the hashed key (this is mainly what hashing with chaining does, correct me if I am wrong).\nBut, how does ```\nstd::unordered_map```\n use hashing? How does it set a new (key, value) pair using hashing? I mean, I know that hashing will group the keys by some criteria, but it is not clear at all how it sets a new (key, value) pair using hashing.\n    ", "Answer": "\r\nFor most standard library containers, the answer would be: However it feels like, it's an implementation detail left up to the writer of the library.\nHowever, ```\nunordered_map```\n is a little peculiar in that respect because it not only has to behave in a certain way, but it also has contraints applied to how it's implemented.\nFrom the standard: http://eel.is/c++draft/unord.req#general-9\n\nThe elements of an unordered associative container are organized into buckets. Keys with the same hash code appear in the same bucket. The number of buckets is automatically increased as elements are added to an unordered associative container, so that the average number of elements per bucket is kept below a bound. Rehashing invalidates iterators, changes ordering between elements, and changes which buckets elements appear in, but does not invalidate pointers or references to elements. For unordered_­multiset and unordered_­multimap, rehashing preserves the relative ordering of equivalent elements.\n\nIn short, the map has ```\nN```\n buckets at any given time. The result of the hash function is used to pick a bucket by doing something along the lines of ```\nbucket_id = hash_value % N```\n. If the buckets start to get too \"full\", the map will increase ```\nN```\n, and reorganize its contents.\nHow are things organized within a bucket is not really specified. It's typically a linked list.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Angular Universal Not rendering data from the server\r\n                \r\nI have implemented Angular Universal for Angular 5. Everything has compiled well enough. The only places where I see the universal work ( html with content in view source) is in static pages i.e where no data is fetched from the server. Views having data fetched from resolve or any other means is not shown in the view source of the page. I am not quite sure what I am missing. Can you please help.\n\nBelow is the screen shot of the view source of page having data from server.\n\n\n\nCan anyone look into this issue. Have been stuck in this for past week. No results.\n\nBelow are my files .\n\n1. angular-cli.json\n\n```\n{\n   \"$schema\":\"./node_modules/@angular/cli/lib/config/schema.json\",\n   \"project\":{\n      \"name\":\"project\"\n   },\n   \"apps\":[\n      {\n         \"root\":\"src\",\n         \"outDir\":\"dist/browser\",\n         \"assets\":[\n            \"assets\",\n            \"favicon.ico\"\n         ],\n         \"index\":\"index.html\",\n         \"main\":\"main.ts\",\n         \"polyfills\":\"polyfills.ts\",\n         \"test\":\"test.ts\",\n         \"tsconfig\":\"tsconfig.app.json\",\n         \"testTsconfig\":\"tsconfig.spec.json\",\n         \"prefix\":\"app\",\n         \"styles\":[\n\n         ],\n         \"scripts\":[\n\n         ],\n         \"environmentSource\":\"environments/environment.ts\",\n         \"environments\":{\n            \"dev\":\"environments/environment.ts\",\n            \"stag\":\"environments/environment.stag.ts\",\n            \"prod\":\"environments/environment.prod.ts\"\n         }\n      },\n      {\n         \"platform\":\"server\",\n         \"root\":\"src\",\n         \"outDir\":\"dist/server\",\n         \"assets\":[\n            \"assets\",\n            \"favicon.ico\"\n         ],\n         \"index\":\"index.html\",\n         \"main\":\"main.server.ts\",\n         \"test\":\"test.ts\",\n         \"tsconfig\":\"tsconfig.server.json\",\n         \"testTsconfig\":\"tsconfig.spec.json\",\n         \"prefix\":\"app\",\n         \"styles\":[\n            \"styles.css\"\n         ],\n         \"scripts\":[\n\n         ],\n         \"environmentSource\":\"environments/environment.ts\",\n         \"environments\":{\n            \"dev\":\"environments/environment.ts\",\n            \"stag\":\"environments/environment.stag.ts\",\n            \"prod\":\"environments/environment.prod.ts\"\n         }\n      }\n   ]\n}\n```\n\n\n2. server.ts\n\n```\nimport \"zone.js/dist/zone-node\";\nimport \"reflect-metadata\";\nimport {\n    renderModuleFactory\n} from \"@angular/platform-server\";\nimport {\n    enableProdMode\n} from \"@angular/core\";\n\nimport * as express from \"express\";\nimport {\n    join\n} from \"path\";\nimport {\n    readFileSync\n} from \"fs\";\n\n// Faster server renders w/ Prod mode (dev mode never needed)\nenableProdMode();\n\n// Express server\nconst app = express();\n\nconst PORT = process.env.PORT || 4000;\nconst DIST_FOLDER = join(process.cwd(), \"dist\");\n\n// Our index.html we'll use as our template\nconst template = readFileSync(\n    join(DIST_FOLDER, \"browser\", \"index.html\")\n).toString();\n\n// * NOTE :: leave this as require() since this file is built Dynamically from webpack\nconst {\n    AppServerModuleNgFactory,\n    LAZY_MODULE_MAP\n} = require(\"./dist/server/main.bundle\");\n\n// Express Engine\nimport {\n    ngExpressEngine\n} from \"@nguniversal/express-engine\";\n// Import module map for lazy loading\nimport {\n    provideModuleMap\n} from \"@nguniversal/module-map-ngfactory-loader\";\n\n// Our Universal express-engine (found @ https://github.com/angular/universal/tree/master/modules/express-engine)\napp.engine(\n    \"html\",\n    ngExpressEngine({\n        bootstrap: AppServerModuleNgFactory,\n        providers: [provideModuleMap(LAZY_MODULE_MAP)]\n    })\n);\n\napp.set(\"view engine\", \"html\");\napp.set(\"views\", join(DIST_FOLDER, \"browser\"));\n\n/* - Example Express Rest API endpoints -\n  app.get('/api/**', (req, res) => { });\n*/\n\n// Server static files from /browser\napp.get(\n    \"*.*\",\n    express.static(join(DIST_FOLDER, \"browser\"), {\n        maxAge: \"1y\"\n    })\n);\n\n// ALl regular routes use the Universal engine\napp.get(\"*\", (req, res) => {\n    res.render(\"index\", {\n        req\n    });\n});\n\n// Start up the Node server\napp.listen(PORT, () => {\n    console.log(`Node Express server listening on http://localhost:${PORT}`);\n});\n```\n\n\n3. prerender.ts\n\n```\n// Load zone.js for the server.\nimport \"zone.js/dist/zone-node\";\nimport \"reflect-metadata\";\nimport {\n    readFileSync,\n    writeFileSync,\n    existsSync,\n    mkdirSync\n} from \"fs\";\nimport {\n    join\n} from \"path\";\nimport {\n    chdir\n} from \"process\";\n\nimport {\n    enableProdMode\n} from \"@angular/core\";\nconst domino = require(\"domino\");\nconst fs = require(\"fs\");\nconst path = require(\"path\");\nconst templateA = fs\n    .readFileSync(path.join(\"..\", \"dist/browser\", \"index.html\"))\n    .toString();\nconst win = domino.createWindow(templateA);\nwin.Object = Object;\nwin.Math = Math;\n\nglobal[\"window\"] = win;\nglobal[\"document\"] = win.document;\nglobal[\"branch\"] = null;\nglobal[\"object\"] = win.object;\n// Faster server renders w/ Prod mode (dev mode never needed)\nenableProdMode();\n\n// Express Engine\nimport {\n    ngExpressEngine\n} from \"@nguniversal/express-engine\";\n// Import module map for lazy loading\nimport {\n    provideModuleMap\n} from \"@nguniversal/module-map-ngfactory-loader\";\n\nimport {\n    renderModuleFactory\n} from \"@angular/platform-server\";\n\n// * NOTE :: leave this as require() since this file is built Dynamically from webpack\nconst {\n    AppServerModuleNgFactory,\n    LAZY_MODULE_MAP\n} = require(\"./dist/server/main.bundle\");\n\n// Get route paths to prerender only static pages\nconst PATHS = require(\"./static.paths\");\n\nconst BROWSER_FOLDER = join(process.cwd(), \"browser\");\n\n// Load the index.html file containing referances to your application bundle.\nconst index = readFileSync(join(\"browser\", \"index.html\"), \"utf8\");\n\nlet prom = Promise.resolve();\n\n// Iterate each route path\nPATHS.forEach(function(route) {\n    // Changes current directory to ./dist/browser\n    chdir(BROWSER_FOLDER);\n\n    // Creates new directories (if not exists) and changes current directory for the nested one\n    route\n        .split(\"/\")\n        .filter(val => val !== \"\")\n        .forEach(function(dir) {\n            if(!existsSync(dir)) {\n                mkdirSync(dir);\n            }\n            chdir(dir);\n        });\n\n    // Writes rendered HTML to index.html, replacing the file if it already exists.\n    prom = prom\n        .then(_ =>\n            renderModuleFactory(AppServerModuleNgFactory, {\n                document: index,\n                url: route,\n                extraProviders: [\n                    [provideModuleMap(LAZY_MODULE_MAP)]\n                ]\n            })\n        )\n        .then(html =>\n            writeFileSync(join(BROWSER_FOLDER, route, \"index.html\"), html)\n        );\n});\n```\n\n\n4. package.json\n\n```\n{\n   \"name\":\"sirved\",\n   \"version\":\"0.0.0\",\n   \"license\":\"MIT\",\n   \"scripts\":{\n      \"ng\":\"ng\",\n      \"start\":\"ng serve\",\n      \"build\":\"ng build --prod\",\n      \"test\":\"ng test\",\n      \"lint\":\"ng lint\",\n      \"e2e\":\"ng e2e\",\n      \"build:client-and-server-bundles\":\"ng build --prod && ng build --prod --app 1 --output-hashing=false\",\n      \"build:static\":\"npm run build:client-and-server-bundles && npm run webpack:server && npm run generate:static\",\n      \"build:dynamic\":\"npm run build:client-and-server-bundles && npm run webpack:server\",\n      \"generate:static\":\"cd dist && node prerender\",\n      \"webpack:server\":\"webpack --config webpack.server.config.js --progress --colors\",\n      \"serve:static\":\"cd dist/browser && http-server\",\n      \"serve:dynamic\":\"node dist/server\"\n   },\n   \"private\":true,\n   \"dependencies\":{\n      \"@agm/core\":\"^1.0.0-beta.3\",\n      \"@angular/animations\":\"^5.2.11\",\n      \"@angular/common\":\"^5.2.0\",\n      \"@angular/compiler\":\"^5.2.0\",\n      \"@angular/core\":\"^5.2.0\",\n      \"@angular/forms\":\"^5.2.0\",\n      \"@angular/http\":\"^5.2.0\",\n      \"@angular/platform-browser\":\"^5.2.0\",\n      \"@angular/platform-browser-dynamic\":\"^5.2.0\",\n      \"@angular/platform-server\":\"^5.2.0\",\n      \"@angular/router\":\"^5.2.0\",\n      \"@nguniversal/express-engine\":\"^5.0.0-beta.1\",\n      \"@nguniversal/module-map-ngfactory-loader\":\"^5.0.0-beta.1\",\n      \"@types/jquery\":\"^3.3.4\",\n      \"core-js\":\"^2.4.1\",\n      \"zone.js\":\"^0.8.19\"\n   },\n   \"devDependencies\":{\n      \"@angular/cli\":\"~1.7.4\",\n      \"@angular/compiler-cli\":\"^5.2.0\",\n      \"@angular/language-service\":\"^5.2.0\",\n      \"@types/googlemaps\":\"^3.30.11\",\n      \"@types/jasmine\":\"~2.8.3\",\n      \"@types/jasminewd2\":\"~2.0.2\",\n      \"@types/node\":\"~6.0.60\",\n      \"codelyzer\":\"^4.0.1\",\n      \"jasmine-core\":\"~2.8.0\",\n      \"jasmine-spec-reporter\":\"~4.2.1\",\n      \"karma\":\"~2.0.0\",\n      \"karma-chrome-launcher\":\"~2.2.0\",\n      \"karma-coverage-istanbul-reporter\":\"^1.2.1\",\n      \"karma-jasmine\":\"~1.1.0\",\n      \"karma-jasmine-html-reporter\":\"^0.2.2\",\n      \"protractor\":\"~5.1.2\",\n      \"ts-node\":\"~4.1.0\",\n      \"tslint\":\"~5.9.1\",\n      \"typescript\":\"~2.5.3\",\n      \"ts-loader\":\"^2.3.7\"\n   }\n}\n```\n\n\nI have done exactly same as the angular universal starter project is done but I am not getting the required results. Could anyone please look into this!\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Algorithm for hashing/encoding multiple values into a single integer value\r\n                \r\nThere is this algorithm for \"hashing\" or encoding multiple values into a single integer, by assigning exponentially increasing numeric value to individual values. This approach was in particular used in Windows DLLs.\nA possible use case can be a client application requesting a list of items matching certain status codes from an API.\nFor example, if we have the following values:\n```\n* open\n* assigned\n* completed\n* closed\n```\n\n...we assign a numeric value to each:\n```\n* open - 1\n* assigned - 2\n* completed - 4\n* closed - 8\n```\n\netc. where each following value is 2x the previous.\nEncoding\nWhen we need to pass a combination of any of these values, we add up the corresponding numeric values. For example, for \"open, assigned\" it is ```\n3```\n, for \"assigned, completed, closed\" it is ```\n14```\n. This covers all of the unique combinations. As we can see, the \"encoding\" part is very straightforward.\nDecoding\nTo decode the value, the only way I can think of is switch..case statements, like so (pseudocode):\n```\n1 = open\n2 = assigned\n3 = open + assigned\n4 = completed\n5 = open + completed\n6 = assigned + completed\n7 = open + assigned + completed\n8 = closed\n9 = open + closed\n10 = assigned + closed\n11 = open + assigned + closed\n12 = completed + closed\n13 = open + completed + closed\n14 = assigned + completed + closed\n15 = open + assigned + completed + closed\n```\n\nThis algorithm obviously works under the following assumptions:\n\nonly works when each value is used only once\nonly works when both sides know the matching numeric values\n\nQuestions:\n\nWhat is a more optimal way/algorithm to \"decode\" the values instead of the very elaborate switch..case statements?\nIs there a name for this algorithm?\n\nNote: the question is tagged with ```\nwinapi```\n mostly for discoverability. The algorithm is fairly universal.\n    ", "Answer": "\r\nWhat you are describing is formally known as a bit mask, where each bit in an integer is assigned a meaning.  Bits are assigned numeric values that are powers of 2 in binary (bit0=20=1, bit1=21=2, bit2=22=4, bit3=23=8, etc).\nYou can use the ```\nOR```\n and ```\nAND```\n logical bitwise operators to set/query individual bits in an integer, eg:\n```\nconst DWORD State_Open = 1;\nconst DWORD State_Assigned = 2;\nconst DWORD State_Completed = 4;\nconst DWORD State_Closed = 8;\n\nvoid DoSomething(DWORD aStates)\n{\n  ...\n\n  if (aStates & State_Open)\n    // open is present\n  else\n    // open is not present\n\n  if (aStates & State_Assigned)\n    // assigned is present\n  else\n    // assigned is not present\n\n  if (aStates & State_Completed)\n    // completed is present\n  else\n    // completed is not present\n\n  if (aStates & State_Closed)\n    // closed is present\n  else\n    // closed is not present\n\n  ...\n}\n```\n\n```\nDWORD lState = State_Open | State_Assigned | State_Completed | State_Closed;\n// whatever combination you need ...\nDoSomething(lState);\n```\n\nIn Delphi/Pascal, this is better handled using a ```\nSet```\n instead, which is internally implemented as a bit mask, eg:\n```\ntype\n  State = (State_Open, State_Assigned, State_Completed, State_Closed);\n  States = Set of State;\n\nprocedure DoSomething(aStates: States);\nbegin\n  ...\n\n  if State_Open in aStates then\n    // open is present\n  else\n    // open is not present\n\n  if State_Assigned in aStates then\n    // assigned is present\n  else\n    // assigned is not present\n\n  if State_Completed in aStates then\n    // completed is present\n  else\n    // completed is not present\n\n  if State_Closed in aStates then\n    // closed is present\n  else\n    // closed is not present\n\n  ...\nend;\n```\n\n```\nvar\n  lState: States;\nbegin\n  ...\n  lState := [State_Open, State_Assigned, State_Completed, State_Closed];\n  // whatever combination you need ...\n  DoSomething(lState);\n  ...\nend;\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is r.uuid() guaranteed to be unique?\r\n                \r\nIs ```\nr.uuid()```\n guaranteed to be unique?\n\n\n  Return a UUID (universally unique identifier), a string that can be used as a unique ID.\n\n\nHow universal is ```\nr.uuid()```\n? Is it scoped to a table/database/instance of RethinkDB? Or is it simply computing the hash of a random byte sequence (e.g. ```\n/dev/rand```\n)? Or does it hash nano-unix time?\n    ", "Answer": "\r\nYou can check the answers to a related question in here.\n\nUUIDs are supposed to be uniques because of the very low probability of colisitions. Although in theory they may not be uniques as it's a random algorithm that generates the UUIDs, you will hardly generate a duplicate.\n\nFrom the Wikipedia they say that for ```\n68,719,476,736```\n generated UUIDs (Which it's a very huge number for a common application) you have ```\n0.0000000000000004```\n for an accidental clash. It's almost impossible..\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "HashTable problems Complexity implementation\r\n                \r\nI coded a java implementation of Hashtable, and I want to test the complexity. The hash table is structured as ad array of double linked list(always implemented by me). The dimension of array is m.  I implemented a division hashing function, multiplication one and universal one. For now I'm testing the first one hashing. \nI've developed a testing suite made this way:\n\n```\nU (maximum value for a key) = 10000;\nm (number of position in the hashkey) = 709;\nn (number of element to be inserted) = variable.\n```\n\n\nSo I made multiple insert, where gradually I inserted array with different n. I checked the time of execution with the ```\nSystem.nanoTime()```\n.\n\nThe graph that comes out is the next:\nhttps://i.stack.imgur.com/pp7Bi.jpg\n\nSupposed that insert is O(1), n insert are O(n). So should this graph be a O(n)? \n\nIf I change my values like this:\n\n```\nU = 1000000\nm = 1009\nn = variable-> ( I inserted once for time, array with incrementally dimension by 25000 elements, from the one with 25000 elements to the one with 800000 elements ). \n```\n\n\nThe graph i got looks like a little strange:\n\nhttps://i.stack.imgur.com/XL1P1.jpg\n\nThe unique key of elements to be inserted are chosen pseudo randomly between the universe of key U.\nBut, with different executions, also if I store the same keys in a file, the behavior of the graph always changes with some peaks.\n\nHope you may help me. If someone needs code, can comment and I will be pleasure to show.\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How do you avoid collision in Hash if a malicious hacker is trying to force collisions on Hash key?\r\n                \r\nRecently in one of the interviews, I was asked following,\n\nWe have our own hash implementation. Let's say we open source our Hash implementation where the logic of generating hashcode is visible. Now, a hacker can force collision as he can send same key with different values. What can be done to prevent such scenario?\n\nI was stuck, I tried to discussing following,\n\nOn hash function to be universal hash function like ```\n(ax + b) mod p```\n where ```\na```\n, ```\nb```\n and ```\np```\n are big primes hidden in environment variable, which can reduce the probability of collision. But they argued that as software is open source, we can't hide it. (bad coding pattern, but still for sake of argument it is valid).\nI even even suggested, Hash pointing to another hash if we see too many collisions. But it is just delaying the eventuality of multiple collisions in the next hash.\nBlock DDos attack from hacker\n\nBut none of the answers seemed to satisfy the interviewer. Now, I'm confused and want to know. What are the other ways to handle forceable collisions in ```\nHash```\n?\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to efficiently check if variable is Array or Object (in NodeJS & V8)?\r\n                \r\nIs there any way to efficiently check if the variable is Object or Array, in NodeJS & V8? \n\nI'm writing a Model for MongoDB and NodeJS, and to traverse the object tree I need to know if the object is simple (Number, String, ...) or composite (Hash, Array).\n\nIt seems that V8 has fast built-in ```\nArray.isArray```\n, but how to check if object is an Object? I mean complex object like hash ```\n{}```\n or instance of class, not something like ```\nnew String()```\n?\n\nUsually it may be done as this:\n\n```\nObject.prototype.toString.call(object) == \"[object Object]\"\n```\n\n\nor this:\n\n```\nobject === Object(object)\n```\n\n\nBut it seems that this operations aren't cheap, maybe there's some more efficient? It's ok if it's not universal and doesn't works on all engines, I need it only to work on V8.\n    ", "Answer": "\r\nFor simply checking against Object or Array without additional function call (speed).\nisArray()\n```\nlet isArray = function(a) {\n    return (!!a) && (a.constructor === Array);\n};\nconsole.log(isArray(        )); // false\nconsole.log(isArray(    null)); // false\nconsole.log(isArray(    true)); // false\nconsole.log(isArray(       1)); // false\nconsole.log(isArray(   'str')); // false\nconsole.log(isArray(      {})); // false\nconsole.log(isArray(new Date)); // false\nconsole.log(isArray(      [])); // true\n```\n\nisObject()\n```\nlet isObject = function(a) {\n    return (!!a) && (a.constructor === Object);\n};\nconsole.log(isObject(        )); // false\nconsole.log(isObject(    null)); // false\nconsole.log(isObject(    true)); // false\nconsole.log(isObject(       1)); // false\nconsole.log(isObject(   'str')); // false\nconsole.log(isObject(      [])); // false\nconsole.log(isObject(new Date)); // false\nconsole.log(isObject(      {})); // true\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is it a secure method to generate token?\r\n                \r\nI wan't to generate token to verify the email of users, I learn about universal ```\nhashing```\n (selecting a hash function at random from a family of hash functions) and I wrote this code in ```\nPHP```\n\n\nIs it a secure method to generate token ?\n\n```\n$string ='';\n$length = 60;\n$pattern = 'abcdefghijklmnpqrstuvwxyABCDEFGHIJKLMNPQRSTUVWXY0123456789';\n$hashList = array('sha256','sha384','sha512','ripemd256','ripemd320','openssl_random_pseudo_bytes');\n$randNumber = mt_rand(0, 6);\n\nfor($i=0; $i<$length; $i++)\n{\n    $string .= $pattern[rand()%strlen($pattern)];\n}\n\nswitch ($randNumber) {\n\n    case 0:\n    return substr(hash($hashList[$randNumber],$string),0,$length);    \n    break;\n\n    case 1:\n    return substr(hash($hashList[$randNumber],$string),0,$length);\n    break;\n\n    case 2:\n    return substr(hash($hashList[$randNumber],$string),0,$length);\n    break;\n\n    case 3:\n    return substr(hash($hashList[$randNumber],$string),0,$length);\n    break;\n\n    case 4:\n    return substr(hash($hashList[$randNumber],$string),0,$length);\n    break;\n\n    case 5:\n    return substr(bin2hex($hashList[$randNumber]($length)),0,$length);\n    break;\n\n    default:\n    return $string;\n    break;\n\n}\n```\n\n    ", "Answer": "\r\nIt seems a bit complicated for me, verifying users email may not be the most important thing to secure, generating hash trough that function. It's unlikely to guess hash value.\n\nI would choose only one hash function, don't just create hash value from username, email values, add some salt or random stuff into the string. Next option would be controlling how many invalid verifications has been processed and just block user to verify for some period.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "sha256.TransformBlock in Win10 Universal App\r\n                \r\nI have this working code in .NET 4.5:\n```\nvar sha256 = System.Security.Cryptography.SHA256.Create();\nvar message = new byte[] {1, 2, 3};\nvar s = new byte[32];\nvar m = sha256.ComputeHash(message);\nsha256.TransformBlock(m, 0, m.Length, m, 0);\nsha256.TransformFinalBlock(s, 0, s.Length);\nvar x = sha256.Hash;  // x = {236, 196, 174, 128, 243....}\n```\n\nAnd I'm trying to replicate it in an universal Windows 10 App.\nHowever I cannot find TransformBlock / TransformFinalBlock functions on the SHA256-object in the new .NET libraries.\nI've added a dependency to version 4.0.0-beta-23409 of System.Security.Cryptography.Algorithms. And the errors I'm getting are:\n\nerror CS1061: 'SHA256' does not contain a definition for 'TransformBlock' and no extension method 'TransformBlock' accepting a first argument of type 'SHA256' could be found (are you missing a using directive or an assembly reference?)\nerror CS1061: 'SHA256' does not contain a definition for 'TransformFinalBlock' and no extension method 'TransformFinalBlock' accepting a first argument of type 'SHA256' could be found (are you missing a using directive or an assembly reference?)\nerror CS1061: 'SHA256' does not contain a definition for 'Hash' and no extension method 'Hash' accepting a first argument of type 'SHA256' could be found (are you missing a using directive or an assembly reference?)\n\nHow do I achieve the same result as in .NET 4.5?\n    ", "Answer": "\r\nThe solution was found in another class, IncrementalHash.\nApparently, Microsoft wanted to separate the stateful (TransformBlock and TransformFinalBlock) and \"stateless\" (ComputeHash) parts of HashAlgorithm, because they didn't have good isolation.\n\nAnyway, here's how to replicate the code in a Universal Windows 10 App:\n\n```\nvar message = new byte[] { 1, 2, 3 };\nvar s = new byte[32];\nbyte[] m;\nbyte[] x;\n\nusing (HashAlgorithm sha256 = SHA256.Create())\n{\n    m = sha256.ComputeHash(message);\n}\n\nusing (IncrementalHash sha256 = IncrementalHash.CreateHash(HashAlgorithmName.SHA256))\n{\n    sha256.AppendData(m);\n    sha256.AppendData(s);\n    x = sha256.GetHashAndReset();\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to hash file (MD5, SHA..v.v..) in UWP\r\n                \r\nI'm coding an Universal App, how can I hash a file with ```\nmd5```\n or ```\nSHA```\n algorithm ?\n\nI searched, found this: ```\nsystem.security.cryptography```\n, but it's not available in my project.\n\nI'm using Visual Studio 2015.\n    ", "Answer": "\r\nIn UWP, it is Windows.Security.Cryptography namespace and Windows.Security.Cryptography.Core namespace. \n\nIn the CryptographicBuffer class there is sample showing how to use this class.\n\nHere is my demo about getting MD5 hash:\n\n```\nprivate string strAlgNameUsed;\n\npublic string GetMD5Hash(String strMsg)\n{\n    string strAlgName = HashAlgorithmNames.Md5;\n    IBuffer buffUtf8Msg = CryptographicBuffer.ConvertStringToBinary(strMsg, BinaryStringEncoding.Utf8);\n\n    HashAlgorithmProvider objAlgProv = HashAlgorithmProvider.OpenAlgorithm(strAlgName);\n    strAlgNameUsed = objAlgProv.AlgorithmName;\n\n    IBuffer buffHash = objAlgProv.HashData(buffUtf8Msg);\n\n    if (buffHash.Length != objAlgProv.HashLength)\n    {\n        throw new Exception(\"There was an error creating the hash\");\n    }\n\n    string hex = CryptographicBuffer.EncodeToHexString(buffHash);\n\n    return hex;\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Highly Repeating one way Hashing algorithm\r\n                \r\nDisclaimer This will not be used in any kind of security\n\nI need to design/implement a one-way hash function that given a string and a number it returns me a hash of the string. The method signature should be like ```\nstring GetHash(string input, int universe)```\n with the following limitations:\n\n\n```\nuniverse```\n will be bigger than lets say 3 and will never decrease (eg, if a hash was calculated using 8 as universe, no other hash can be calculated using a universe smaller than 8)\nThe hash for a static string should always be the same, regardless of the ```\nuniverse```\n parameter, eg:\nGetHash(\"ABC\",3) => H1,\nGetHash(\"ABC\",4) => H1,\nGetHash(\"ABC\",5) => H1,\nGetHash(\"ABC\",6) => H1\nGiven N strings and a fixed ```\nuniverse```\n, the maximum number of generated different hashes cannot be bigger than ```\nuniverse```\n, eg:\nGetHash(\"A\",2) => H1,\nGetHash(\"B\",2) => H2,\nGetHash(\"C\",2) => H2,\nGetHash(\"D\",2) => H2,\nGetHash(\"E\",2) => H1,\nGetHash(\"F\",2) => H1,\nGetHash(\"G\",2) => H2,\nNOTE increased universe\nGetHash(\"A\",3) => H1,\nGetHash(\"B\",3) => H2,\nGetHash(\"C\",3) => H2,\nGetHash(\"D\",3) => H2,\nGetHash(\"E\",3) => H1,\nGetHash(\"F\",3) => H1,\nGetHash(\"G\",3) => H2,\nNOTE new strings\nGetHash(\"Z\",3) => H1,\nGetHash(\"W\",3) => H2,\nGetHash(\"Y\",3) => H3,\nGetHash(\"T\",3) => H2,\nGetHash(\"S\",3) => H1,\nGetHash(\"R\",3) => H3,\nGetHash(\"Q\",3) => H2\n\n    ", "Answer": "\r\nThe only solution is to return the same hash for all strings and universes.\n\nConsidering this requirement:\n\n\n  Given N strings and a fixed universe, the maximum number of generated different hashes cannot be bigger than universe.\n\n\nGiven a ```\nuniverse```\n of 1, the hashes for all strings need to be the same.\n\nAnd, given this requirement:\n\n\n  The hash for a static string should always be the same, regardless of the universe parameter\n\n\nThe hashes for some given string needs to be the same across all values of ```\nuniverse```\n.\n\nThis requirement actually makes the universe parameter obsolete (beyond what its lower bound is). Given some strings and a universe, that can't produce difference hashes than those strings and a smaller universe, simply because each individual string needs to have the same hash across all universes.\n\n\n\nI'm assuming that any string can be valid for any ```\nuniverse```\n parameter. If this is not the case, the scenario may be different (although the exact rules for where a string can appear will need to be included in the problem statement).\n\nThis is assuming ```\nuniverse```\n can be 1. If the lower bound on ```\nuniverse```\n is 2, for example, just separate the strings into 2 parts (using whatever criteria you see fit) and return one hash for the one and another hash for the other (these could also be the same hash) - this can be extended to any lower bound of ```\nuniverse```\n by just increasing the number of parts appropriately.\n\n\n\nIf we ignore the first requirement, we could perhaps consider using any hash function to hash each string to a number, after which we can mod (take the remainder of, after division by) each such number with ```\nuniverse```\n - this will limit the return values to ```\n[0, universe)```\n, thus there can be no more than ```\nuniverse```\n different hash values.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "can json-ld be used to build a unique hash signature of a json object?\r\n                \r\nThis is a near duplicate of How to reliably hash JavaScript objects?, where someone wants to reliably hash javascript objects ;\n\nNow that the json-ld specification has been validated, I saw that there is a normalization procedure that they advertise as a potential way to normalize a json object :\n\n\n  normalize the data using the RDF Dataset normalization algorithm, and then dump the output to normalized NQuads format. The NQuads can then be processed via SHA-256, or similar algorithm, to get a deterministic hash of the contents of the Dataset.\n\n\nBuilding a hash of a json object has always been a pain because something like \n\n```\nsha1(JSON.stringify(object))\n```\n\n\ndoes not work or is not guaranteed to work the same across implementations (the order of the keys is not defined of example).\n\nDoes json-ld work as advertized ? Is it safe to use it as universal json normalization procedure for hashing objects ? Can those objects be standard json objects or do they need some json-ld decorations (@context,..) to be normalized ?\n    ", "Answer": "\r\nYes, normalization works with JSON-LD, but the objects do need to be given context (via the @context property) in order for them to produce any RDF. It is the RDF that is deterministically output in NQuads format (and that can then be hashed, for example).\n\nIf a property in a JSON-LD document is not defined via @context, then it will be dropped during processing. JSON-LD requires that you provide global meaning (semantics) to the properties in your document by associating them with URLs. These URLs may provide further machine-readable information about the meaning of the properties, their range, domain, etc. In this way data becomes \"linked\" -- you can both understand the meaning of a JSON document from one API in the context of another and you can traverse documents (via HTTP) to find more information.\n\nSo the short answer to the main question is \"Yes, you can use JSON-LD normalization to build a unique hash for a JSON object\", however, the caveat is that the JSON object must be a JSON-LD object, which really constitutes a subset of JSON. One of the main reasons for the invention of the normalization algorithm was for hashing and digitally-signing graphs (JSON-LD documents) for comparison.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Hash table O(1) amortized or O(1) average amortized?\r\n                \r\nThis question may seem a bit pedantic but i've been really trying to dive deeper into Amortized analysis and am a bit confused as to why insert for a hash table is O(1) amortized.(Note: Im not talking about table doubling, I understand that)\n\nUsing this definition, \"Amortized analysis gives the average performance (over time) of each operation in the worst case.\" It seems like the worst case for N inserts into a hashtable would result in a collision for every operation. I believe universal hashing guarantees collision at a rate of 1/m when the load balance is kept low, but isn't it still theoretically possible to get a collision for every insert? \n\nIt seems like technically the average amortized analysis for hashtable's insert is O(1). \n\nEdit: You can assume the hashtable uses basic chaining where the element is placed at the end of the corresponding linked list. The real meat of my question refers to amortized analysis on probabilistic algorithms.\n\nEdit 2: \n I found this post on quicksort, \n\"Also there’s a subtle but important diﬀerence between amortized running time and expected running time. Quicksort with random pivots takes O(n log n) expected running time, but its worst-case running time is in Θ(n^2). This means that there is a small possibility that quicksort will cost (n^2) dollars, but the probability that this will happen approaches zero as n grows large.\" I think this probably answers my question.\n    ", "Answer": "\r\nYou could theoretically get a collision every insert but that would mean that you had a  poor performing hashing function that failed to space out values across the \"buckets\" for keys. A theoretically perfect hash function would always put a new value into a new bucket so that each key would refer to it's own bucket. (I am assuming a chained hash table and referring to the chain field as a \"bucket\", just how I was taught). A theoretically worst case function would stick all keys into the same bucket leading to a chain in that bucket of length N. \n\nThe idea behind the amortization is that given a reasonably good hashing function you should end up with a linear time for insert because the amount of times that insertion is > O(1) would be greatly dwarfed by the number of times that insertion is simple and O(1). That is not to say that insertion is without any calculation (the hash function still has to be calculated and in some special cases hash functions can be more calc heavy than just looking through a list).\n\nAt the end of the day this brings us to an important concept in big-O which is the idea that when calculating time complexity you need to look at the most frequently executed action. In this case that is the insertion of a value that does not collide with another hash. \n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Hash different for the same object, Swift, Hashable\r\n                \r\nInside of the ```\nHashable```\n we can see:\n\n```\n/// Hash values are not guaranteed to be equal across different executions of\n/// your program. Do not save hash values to use during a future execution.\n```\n\n\nWhy it is so? Getting different hash for the same object on every run confuses me because in the university I studied that ```\nhash function```\n return same value for same object. What algorithm is Apple using for hashing?\n\nE.G. (this will print the different value on every run)\n\n```\nstruct HashTesting: Hashable {\n    var a = 10\n    var b = 20\n    var str = \"str\"\n}\n\nclass ViewController: UIViewController {\n    override func viewDidLoad() {\n        super.viewDidLoad()\n\n        let obj = HashTesting(a: 10, b: 10, str: \"str\")\n        print(\"\\(obj.hashValue)\")\n    }\n}\n```\n\n    ", "Answer": "\r\nHash randomization was enforced in Swift 4.2, with the implementation of SE 0206 Hashable Enhancements. From the proposal:\n\n\n  However, Hasher may generate entirely different hash values in other executions, even if it is fed the exact same byte sequence. This randomization is a critical feature, as it makes it much harder for potential attackers to predict hash values. Hashable has always been documented to explicitly allow such nondeterminism.\n\n\nIn addition, it allows the actual implementation to be changed (e.g. improved) in the Swift standard library, without breaking compatibility.\n\nFor debugging purposes the hash randomization can be disabled by defining the SWIFT_DETERMINISTIC_HASHING environment variable with a value of 1.\n\nThe implementation of the Swift standard hasher can be found in the open source repository:\n\n\nhttps://github.com/apple/swift/blob/master/stdlib/public/core/Hasher.swift\nhttps://github.com/apple/swift/blob/master/stdlib/public/core/SipHash.swift\n\n\nIt is based on SipHash.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Microsoft Dynamics CRM - password hashing algorithm\r\n                \r\nI work at a University and our application for admissions is a product that is built on top of Microsoft Dynamics CRM 4.0.  (The application is called 'Recruiter'.)\n\nThere is a table with usernames and hashed passwords.  I would like to write an app (in Ruby on Rails) that queries the username and hashed password to validate a login.\n\nUnfortunately, I do not know how the hashed password is generated.  It appears to be a Base64-encoded MD5 checksum.\n\nI have read elsewhere that the hashed password is generated using a combination of:\n\n\nthe plaintext password\nthe primary key of the record (a UUID string, not an integer)\nthe database server name\n\n\nI have tried variations of this to create a hashed password that matches the record in the database, but I cannot get this to work.\n\nCan anyone tell me how to generate a matching hashed password from a plaintext password?\n\nThank you!\n    ", "Answer": "\r\nThis is not a customization based on the capabilities of Dynamics CRM. It looks like Dynamics CRM is used as a framework for this. You have to contact the original creator of this application.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "BigCommerce University Unable to GET https://api.bigcommerce.com/stores/{{store-hash}}/v2/payments/methods\r\n                \r\nI'm currently enrolled in the BigCommerce University course, I've set up postman following the instructions of the course when trying to get https://api.bigcommerce.com/stores/%7B%7Bstore-hash%7D}/v2/payments/methods I receive a 403 error:\n{\n\"status\": 403,\n\"title\": \"You don't have a required scope to access the endpoint\",\n\"type\": \"https://developer.bigcommerce.com/api-docs/getting-started/api-status-codes\",\n\"errors\": {}\n}\nI've checked again my presets. I have Accept, Content-Type and X-Auth-Token\n    ", "Answer": "\r\nThis error is referring to the API key you are using does not have proper permissions to view payment methods.\nAssuming you are just messing around for learning, go back to your store and create a new API key and select read/write for all the different options.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Angular + Node + Universal remove # with working refresh\r\n                \r\nI am using Angular 13 + Angular Universal + Node.js.\nI have deployed app on server, but I have big problem with removing # from URL.\nI have built app using\n```\nnpm run build:ssr\n```\n\nAnd I am serving it using node.js app on my server using :\n```\napp.use(express.static(__dirname + '/dist/app-name/browser'));\n\napp.get('/', (req, res) => res.sendFile(path.join(__dirname)));\n```\n\nWhen I use HashLocationStrategy, then everything works fine. Problem appears when I am trying to remove hash from link.\nI changed it to PathLocationStrategy and I am getting error 404 on refresh on other path than '/'.\nI was trying to add different routes in node.js file to sendFile (f.e.)\n```\napp.get('/subpage', (req, res) => res.sendFile(path.join(__dirname)));\n```\n\nBut it didn't worked for me.\nI was also trying to overwite # with node.js, but as it comes from angular app it's impossible to do it from node.js code.\nMy files :\napp.server.module.ts\n```\n    import { NgModule } from '@angular/core';\n    import { ServerModule } from '@angular/platform-server';\n    \n    import { AppModule } from './app.module';\n    import { AppComponent } from './app.component';\n    import {UniversalInterceptor} from './universal-interceptor.service';\n    import {HTTP_INTERCEPTORS} from '@angular/common/http';\n    import {NgxSsrTimeoutModule} from '@ngx-ssr/timeout';\n    \n    @NgModule({\n      imports: [\n        AppModule,\n        ServerModule,\n        NgxSsrTimeoutModule.forRoot({ timeout: 5000 }),\n      ],\n      providers: [\n        {\n          provide: HTTP_INTERCEPTORS,\n          useClass: UniversalInterceptor,\n          multi: true,\n        }\n      ],\n      bootstrap: [AppComponent],\n    })\n    export class AppServerModule {}\n```\n\nserver.ts\n```\nimport 'zone.js/dist/zone-node';\n\nimport {ngExpressEngine} from '@nguniversal/express-engine';\nimport * as express from 'express';\nimport {join} from 'path';\n\nimport {AppServerModule} from './src/main.server';\nimport {APP_BASE_HREF} from '@angular/common';\nimport {existsSync} from 'fs';\n\n// The Express app is exported so that it can be used by serverless Functions.\nexport function app(): express.Express {\n  const server = express();\n  const distFolder = join(process.cwd(), 'dist/app-name/browser');\n  const indexHtml = existsSync(join(distFolder, 'index.original.html')) ? 'index.original.html' : 'index';\n\n  // Our Universal express-engine (found @ https://github.com/angular/universal/tree/master/modules/express-engine)\n  server.engine('html', ngExpressEngine({\n    bootstrap: AppServerModule,\n  }));\n\n  server.set('view engine', 'html');\n  server.set('views', distFolder);\n\n  // Example Express Rest API endpoints\n  server.get('/api/**', (req, res) => {\n    res.status(404).send('data requests are not yet supported');\n  });\n  // Serve static files from /browser\n  server.get('*.*', express.static(distFolder, {\n    maxAge: '1y'\n  }));\n\n  // All regular routes use the Universal engine\n  server.get('*', (req, res) => {\n    res.render(indexHtml, {req, providers: [{provide: APP_BASE_HREF, useValue: req.baseUrl}]});\n  });\n\n  return server;\n}\n\nfunction run(): void {\n  const port = process.env['PORT'] || 4000;\n\n  // Start up the Node server\n  const server = app();\n  server.listen(port, () => {\n    console.log(`Node Express server listening on http://localhost:${port}`);\n  });\n}\n\n// Webpack will replace 'require' with '__webpack_require__'\n// '__non_webpack_require__' is a proxy to Node 'require'\n// The below code is to ensure that the server is run only when not requiring the bundle.\ndeclare const __non_webpack_require__: NodeRequire;\nconst mainModule = __non_webpack_require__.main;\nconst moduleFilename = mainModule && mainModule.filename || '';\nif (moduleFilename === __filename || moduleFilename.includes('iisnode')) {\n  run();\n}\n\nexport * from './src/main.server';\n```\n\nHow to to deploy app without '#' in url with working refresh?\n    ", "Answer": "\r\nOkay, so I finally found solution for this. My problem was, that I needed to give absolute path to my index.html file for all angular routes. So finally i changed\n```\napp.get('/', (req, res) => res.sendFile(path.join(__dirname)));\n```\n\nto\n```\napp.get('/**', (req, res) => {\n  res.sendFile(path.join(__dirname + '/dist/app-name/browser/index.html'))\n});\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Storing duplicate values in mongodb\r\n                \r\nWhat's the correct way to deal with inserting records with common values? e.g. a university field, or state etc? \n\nDoes mongo apply any sort of value-based hashing to optimize space, or should I do that manually?\n\nThanks\n    ", "Answer": "\r\nCreating a hashed index on the duplicated field will result in the field being stored in hashed form. Mongo does not apply this index automatically, but it should optimize space if you do it manually. This page in the MongoDB documentation shows you how to do it and I found that this article explained it nicely as well.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Hash table complexity?\r\n                \r\nSay there is hash table with 'n' entries and let 'h' be a randomized hash function. What is the worst case time complexity of inserting m keys into this hash table using h. The keys are drawn from a universal set 'U' where absoulte value of U >= mn.\n\nMy best guess would be O(n). Any suggestions?\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How can you use Auth0 with existing ASP.NET Core Identity database?\r\n                \r\nI'm working on Auth0 integration for an application. I have an existing application that uses ASP.NET Core Identity and a SQL Database currently with the usual tables (AspNetUsers, AspNetRoles, etc).\n\nThe problem is that Auth0 currently has custom database templates for working with ASP.NET Membership Provider (MVC3 Universal Providers, and MVC4 Simple Membership) database, but not ASP.NET Core Identity. Since I don't know exactly how to code the Password Hashing necessary to be compatible with my ASP.NET Core Identity database, this is a big problem.\n\nDoes anyone have an example of Auth0 custom database scripts for working with an existing ASP.NET Core Identity database? Or, at least the hashing algorithm used to code it myself?\n\nFor more info, the ASP.NET Core Identity database has a ```\nAspNetUsers```\n table with the following columns that would be of interest for this integration:\n\n\n```\nId```\n (PK, nvarchar, not null)\n```\nEmail```\n (nvarchar, null)\n```\nPasswordHash```\n (nvarchar, null)\n```\nSecurityStamp```\n (nvarchar, null)\n```\nUserName```\n (nvarchar, not null)\n\n\nJust to be clear, I'm asking about how to setup the Custom Database scripts within the Auth0 configurations written in JavaScript; not the ASP.NET web app code. The web app is pretty straight forward according to the docs to get setup. It's just the Auth0 built-in custom database templates don't include an example to use for the ASP.NET Core Identity database schema and password hashing.\n\nAny help would be greatly appreciated!\n    ", "Answer": "\r\nYou need to figure out the hashing algorithm used and modify the scripts they have in the templates, using the Auth0 documentation too, to get it correct. You can find code for the algorithm in the aspnet-identity-pw project.\n\nHere's a sample Login database action script for Auth0 that works with an ASP.NET Core Identity 2.0 database stored in Azure SQL Database:\n\n```\nfunction login (username, password, callback) {\n\n  var Connection = require('tedious@1.11.0').Connection;\n  var Request = require('tedious@1.11.0').Request;\n  var TYPES = require('tedious@1.11.0').TYPES;\n\n  var connection = new Connection({\n    userName:  configuration.db_username + '@' + configuration.db_server,\n    password:  configuration.db_password,\n    server:    configuration.db_server, //'dbserver.database.windows.net',\n    options: {\n      database:  configuration.db_database,\n      encrypt: true\n    }\n  });\n\n  connection.on('debug', function(text) {\n    // if you have connection issues, uncomment this to get more detailed info\n    //console.log(text);\n  }).on('errorMessage', function(text) {\n    // this will show any errors when connecting to the SQL database or with the SQL statements\n    //console.log(JSON.stringify(text));\n  });\n\n  connection.on('connect', function (err) {\n    if (err) {\n      console.log('error: ' + JSON.stringify(err));\n      return callback(err);\n    }\n\n    getMembershipUser(username, function(err, user) {\n      if (err) {\n        return callback(err); // this will return a 500\n      }\n      if (!user.profile) {\n        return callback(); // this will return a 401\n      }\n\n      validatePassword(password, user.password.hash, function(err, isValid) {\n        if (!isValid) {\n          return callback(); // unauthorized\n        }\n\n        callback(null, user.profile);\n      });\n\n    });\n  });\n\n\n  // Membership Provider implementation used with ASP.NET Core Identity database\n\n  /**\n   * getMembershipUser\n   *\n   * This function gets a username or email and returns a user info, password hashes and salt\n   *\n   * @usernameOrEamil   {[string]}    the username or email, the method will do a query\n   *                                  on both with an OR\n   * @callback          {[Function]}  first argument will be the Error if any, and second\n   *                                  argument will be a user object\n   */\n  function getMembershipUser(usernameOrEmail, callback) {\n    var user = {};\n    var query =\n      'SELECT Id, UserName, Email, PasswordHash, SecurityStamp from AspNetUsers ' +\n      'WHERE UserName = @UserName';\n\n    var getMembershipQuery = new Request(query);\n\n    getMembershipQuery.addParameter('UserName', TYPES.VarChar, usernameOrEmail);\n\n    getMembershipQuery.on('row', function (fields) {\n      user.profile = {};\n      user.password = {};\n      for(var f in fields) {\n        var item = fields[f];\n        if (item.metadata.colName === 'Id') {\n          user.profile.user_id = item.value;\n        } else if (item.metadata.colName === 'UserName') {\n          user.profile.nickname = item.value;\n        } else if (item.metadata.colName ==='Email') {\n          user.profile.email = item.value;\n        } else if (item.metadata.colName ==='PasswordHash') {\n          user.password.hash = item.value;\n        }\n      }\n\n      //console.log('User: ' + JSON.stringify(user));\n      callback(null, user);\n    });\n\n    connection.execSql(getMembershipQuery);\n  }\n\n  /**\n   * validatePassword\n   *\n   * This function gets the password entered by the user, and the original password\n   * hash and salt from database and performs an HMAC SHA256 hash.\n   *\n   * @password      {[string]}      the password entered by the user\n   * @originalHash  {[string]}      the original password hashed from the database\n   *                                (including the salt).\n   * @return        {[bool]}        true if password validates\n   */\n  function validatePassword(password, originalHash, callback) {\n    aspnet_identity_pw.validatePassword(password, originalHash, function(result, isValid) {\n      console.log('Is Password Valid: ' + isValid);\n\n      callback(null, isValid);\n    });\n  }\n\n  var aspnet_identity_pw = {\n    validatePassword: function(password, hashedPassword, callback) {\n      // Original Source:\n      //   https://github.com/Syncbak-Git/aspnet-identity-pw/blob/master/lib/aspnet-identity-pw.js\n      //   https://www.npmjs.com/package/aspnet-identity-pw\n      //   There were some slight modifications to make it run well in Auth0\n\n      var done = false;\n      var error = null;\n      var result = null;\n\n      if(!hashedPassword) {\n\n          if(callback) {\n              callback(null, false);\n          }\n\n          return false;\n      }\n\n      if(!password) {\n\n          error = new Error(\"Password is required.\");\n\n          if(callback) {\n              callback(error);\n              return;\n          }\n\n          throw error;\n      }\n\n      var src = new Buffer(hashedPassword, 'base64');\n\n      if(src.length !== 49 || src[0] !== 0) {\n          return false;\n      }\n\n      var salt = new Buffer(16);\n      src.copy(salt, 0, 1, 17);\n\n      var bytes = new Buffer(32);\n      src.copy(bytes, 0, 17, 49);\n\n      var hashed = crypto.pbkdf2Sync(password, salt, 1000, 32, 'sha1');\n      result = true;\n\n          for(var i = 0; i < 32; i++) {\n              if(bytes[i] !== hashed[i]) {\n                  result = false;\n                  break;\n              }\n          }\n\n          done = true;\n\n          if(callback) {\n              callback(null, result);\n          }\n\n      if(!callback) {\n        throw 'callback required!';\n      }\n\n      }\n  };\n\n  }\n```\n\n\nThis took what seemed like forever to get fully figured out. Especially to get the password hashing algorithm coded, until stumbling upon the js project listed with the code for it.\n\nHope this helps others!\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Issue with double hashing Java\r\n                \r\nI've been having an issue with a project at university, we have to write a double hash method in a class, which returns a double hashed string object. I thought this would be relatively simple considering the fact that there's an inbuilt hashCode() method in Java, but it seems when you iterate over a hashCode a second time it returns the exact same value. For example:\n\nStringHashCode.java:\n\n```\npublic class StringHashCode implements HashCode{\n    @Override\n    public int giveCode(Object obj) {\n        return obj.hashCode();\n    }\n}\n```\n\n\nspell.java\n\n```\nwhile(dict_fwr.hasNextWord())\n{\n   String derp = dict_fwr.nextWord();\n   System.out.print(derp + \"(hash value = \" + impl.giveCode(derp) + \")\" + \"(Double hashed = \" + impl.giveCode(impl.giveCode(derp)) + \")\\n\" );\n}\n```\n\n\nExcerpt From Output:\n\n```\nmidwest(hash value = 1055712247)(Double hashed = 1055712247)\npartakes(hash value = 1188620491)(Double hashed = 1188620491)\ninterspersed(hash value = 486466540)(Double hashed = 486466540)\nmarginally(hash value = 1971567014)(Double hashed = 1971567014)\nbemoans(hash value = -223340895)(Double hashed = -223340895)\nbankrupt(hash value = -1858199613)(Double hashed = -1858199613)\ntranspire(hash value = 1052958868)(Double hashed = 1052958868)\nconspire(hash value = -567922531)(Double hashed = -567922531)\n```\n\n\nDict is a file that contains a list of words and the class is a class that was created by out lecturer, it returns the next string and checks whether there is another word.\n    ", "Answer": "\r\nYou pass an ```\nint```\n to ```\ngiveCode()```\n. That gets autoboxed as an Integer. The hash code of the hash code value is the same, because the hash code of an Integer is the integer value. From the docs:\n\n\n  Returns: a hash code value for this object, equal to the primitive int value represented by this Integer object.\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is there a \"good enough\" hash function for the average programmer?\r\n                \r\nWe are told that we should implement hashCode() for our classes but most people like me have no real idea of how to do this or what happens if we get it \"wrong\". For example I have a need for a hash function for indexing nodes in a tree (Finding the most frequent subtrees in a collection of (parse) trees). In this case I need to recursively generate hashcodes based on ordered child nodes, e.g.\n\n```\nhashCode = function(child1.hashCode, child2.hashCode, ...)\n```\n\n\nIn a recent discussion of hashCodes answers included a hash for strings (based on a long prime and 31) and also bitshifting. The String hash is:\n\n```\n// adapted from String.hashCode()\npublic static long hash(String string) {\n  long h = 1125899906842597L; // prime\n  int len = string.length();\n\n  for (int i = 0; i < len; i++) {\n    h = 31*h + string.charAt(i);\n  }\n  return h;\n}\n```\n\n\nI'm not interested in security and don't mind collisions. Is there a \"universal function\" for combining hashcodes of ordered objects that will do more good than harm (and more good than not calling it at all)?\n\nAlso is there a site where we can look up common cases? strings, lists, etc.)\n\nI didn't specify a language as I was hoping there were universal approaches. But if it is seriously language-specific then please indicate the language and why it's not universal.\n\nUPDATE Two suggestions are to use the IDE's hashCode generator. That seems an excellent default;  Here's Netbeans:\n\n```\npublic int hashCode() {\n    int hash = 5;\n// objects\n    hash = 97 * hash + (this.rootElement != null ? this.rootElement.hashCode() : 0);\n    hash = 97 * hash + (this.tableElement != null ? this.tableElement.hashCode() : 0);\n// a string\n    hash = 97 * hash + (this.tag != null ? this.tag.hashCode() : 0);\n    return hash;\n}\n```\n\n    ", "Answer": "\r\nThere is an excellent hashCode() in Joshua Bloch's Effective Java.  Sample Chapter 3, \"Methods Common to All Objects\" was free (well, it used to be back when there was a page on Sun's old site for it. If you search you might still find a PDF version of that chapter lying around somewhere).\n\nYou could also look at the source for HashCodeBuilder in Apache Commons Lang, but just don't copy it for class without citing it.  It is time well spent to actually learn about this -- it will make you a better person.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Why Angular Universal Server Side Rendering get an error after deployment in Firebase Hosting?\r\n                \r\nI am currently working on Angular using its 3rd party library such as Angular CLI/Angular Universal following this link https://github.com/angular/angular-cli/wiki/stories-universal-rendering / and firebase hosting / and real-time database. And it working great on my local machine listening in, localhost:4000. I see it render server-side rendering smoothly and I see the HTML which I need for SEO friendly I use also lazy loading which improves the loading of web application performance very fast. Now, my problem is, after I deploy my Angular application with Universal Server-side Rendering also with lazy loading. I get this error saying, Error: could not handle the request and also if I just deploy without lazy loading I get this 3 errors, \n\n```\n1. Uncaught SyntaxError: Unexpected token < inline.318b50c57b4eba3d437b.bundle.js:1 \n\n2. Uncaught SyntaxError: Unexpected token < polyfills.bf95165a1d5098766b92.bundle.js:1\n\n3. Uncaught SyntaxError: Unexpected token < main.4871dfc6be55ccbf4c0b.bundle.js:1 \n```\n\n\nHERE'S THE FOLDER STRUCTURE:\n\n\ndist => this is the build folder of the angular application\n\n\nbrowser\nserver\nserver.js\n\ne2e\nfunctions => this is the build folder and I use to upload into firebase hosting\n\n\nbrowser\nnode_modules\nserver\n.eslintrc.json\nindex.js\npackage-lock.json\npackage.json\nserver.js\n\nnode_modules\nserver => I use to transpile index.ts to index.js\n\n\nindex.ts\ntsconfig.functions.json\n\nsrc => main directory for the angular application\n\n\napp\n\n\ndemo\n\n\nbuttons\n\n\nbuttons.component.css\nbuttons.component.html\nbuttons.component.spec.ts\nbuttons.component.ts\n\ndemo-routing.module.ts\ndemo.module.ts\n\napp.component.css\napp.component.html\napp.component.spec.ts\napp.component.ts\napp.module.ts\napp.server.module.ts\n\nassets\nenvironments\nfavicon.ico\nindex.html\nmain.server.ts\nmain.ts\npolyfills.ts\nstyles.css\ntest.ts\ntsconfig.app.json\ntsconfig.server.json\ntypings.d.ts\n\n.angular-cli.json\n.editorconfig\n.firebaserc\n.gitignore\nfirebase.json\nkarma.conf.js\npackage-lock.json\npackage.json\nprotractor.conf.js\nREADME.md\nserver.ts\ntsconfig.json\ntslint.json\nwebpack.server.config.js\n\n\nFOR, src/app/app.module.ts\n\n```\nimport { BrowserModule } from '@angular/platform-browser';\nimport { NgModule } from '@angular/core';\n\nimport { Routes, RouterModule } from '@angular/router';\n\nimport { AppComponent } from './app.component';\n\nimport { ButtonsComponent } from './demo/buttons/buttons.component';\n\nconst routes: Routes = [\n  { path: 'buttons', loadChildren: './demo/demo.module#DemoModule' },\n  { path: '**', redirectTo: 'buttons' }\n];\n\n@NgModule({\n  declarations: [\n    AppComponent\n  ],\n  imports: [\n    BrowserModule.withServerTransition({ appId: 'something-unique-id' }),\n    RouterModule.forRoot(routes)\n  ],\n  providers: [],\n  bootstrap: [\n    AppComponent,\n    ButtonsComponent\n  ]\n})\n\nexport class AppModule { }\n```\n\n\nFOR, src/app/app.server.module.ts\n\n```\n import {NgModule} from '@angular/core';\n    import {ServerModule} from '@angular/platform-server';\n    import {ModuleMapLoaderModule} from '@nguniversal/module-map-ngfactory-loader';\n\n    import {AppModule} from './app.module';\n    import {AppComponent} from './app.component';\n\n    @NgModule({\n      imports: [\n        // The AppServerModule should import your AppModule followed\n        // by the ServerModule from @angular/platform-server.\n        AppModule,\n        ServerModule, \n        ModuleMapLoaderModule // <-- *Important* to have lazy-loaded routes work\n      ],\n      // Since the bootstrapped component is not inherited from your\n      // imported AppModule, it needs to be repeated here.\n      bootstrap: [AppComponent],\n    })\n    export class AppServerModule {}\n```\n\n\nFOR, src/main.server.ts\n\n```\nexport { AppServerModule } from './app/app.server.module';\n```\n\n\nFOR, src/tsconfig.server.json\n\n```\n{\n  \"extends\": \"../tsconfig.json\",\n  \"compilerOptions\": {\n    \"outDir\": \"../out-tsc/app\",\n    \"baseUrl\": \"./\",\n    // Set the module format to \"commonjs\":\n    \"module\": \"commonjs\",\n    \"types\": []\n  },\n  \"exclude\": [\n    \"test.ts\",\n    \"**/*.spec.ts\"\n  ],\n  // Add \"angularCompilerOptions\" with the AppServerModule you wrote\n  // set as the \"entryModule\".\n  \"angularCompilerOptions\": {\n    \"entryModule\": \"app/app.server.module#AppServerModule\"\n  }\n}\n```\n\n\nFOR, .angular-cli.json\n\n```\n{\n  \"$schema\": \"./node_modules/@angular/cli/lib/config/schema.json\",\n  \"project\": {\n    \"name\": \"kuntento\"\n  },\n  \"apps\": [\n    {\n      \"root\": \"src\",\n      \"outDir\": \"dist/browser\",\n      \"assets\": [\n        \"assets\",\n        \"favicon.ico\"\n      ],\n      \"index\": \"index.html\",\n      \"main\": \"main.ts\",\n      \"polyfills\": \"polyfills.ts\",\n      \"test\": \"test.ts\",\n      \"tsconfig\": \"tsconfig.app.json\",\n      \"testTsconfig\": \"tsconfig.spec.json\",\n      \"prefix\": \"app\",\n      \"styles\": [\n        \"styles.css\"\n      ],\n      \"scripts\": [],\n      \"environmentSource\": \"environments/environment.ts\",\n      \"environments\": {\n        \"dev\": \"environments/environment.ts\",\n        \"prod\": \"environments/environment.prod.ts\"\n      }\n    },\n    {\n      \"platform\": \"server\",\n      \"root\": \"src\",\n      \"outDir\": \"dist/server\",\n      \"assets\": [\n        \"assets\",\n        \"favicon.ico\"\n      ],\n      \"index\": \"index.html\",\n      \"main\": \"main.server.ts\",\n      \"test\": \"test.ts\",\n      \"tsconfig\": \"tsconfig.server.json\",\n      \"testTsconfig\": \"tsconfig.spec.json\",\n      \"prefix\": \"app\",\n      \"styles\": [\n        \"styles.css\"\n      ],\n      \"scripts\": [],\n      \"environmentSource\": \"environments/environment.ts\",\n      \"environments\": {\n        \"dev\": \"environments/environment.ts\",\n        \"prod\": \"environments/environment.prod.ts\"\n      }\n    }\n  ],\n  \"e2e\": {\n    \"protractor\": {\n      \"config\": \"./protractor.conf.js\"\n    }\n  },\n  \"lint\": [\n    {\n      \"project\": \"src/tsconfig.app.json\",\n      \"exclude\": \"**/node_modules/**\"\n    },\n    {\n      \"project\": \"src/tsconfig.spec.json\",\n      \"exclude\": \"**/node_modules/**\"\n    },\n    {\n      \"project\": \"e2e/tsconfig.e2e.json\",\n      \"exclude\": \"**/node_modules/**\"\n    }\n  ],\n  \"test\": {\n    \"karma\": {\n      \"config\": \"./karma.conf.js\"\n    }\n  },\n  \"defaults\": {\n    \"styleExt\": \"css\",\n    \"component\": {}\n  }\n}\n```\n\n\nFOR, ./server.ts\n\n```\n    import 'zone.js/dist/zone-node';\n    import 'reflect-metadata';\n\n    import { renderModuleFactory } from '@angular/platform-server';\n    import { enableProdMode } from '@angular/core';\n\n    import * as express from 'express';\n    import { join } from 'path';\n    import { readFileSync } from 'fs';\n\n    enableProdMode();\n\n    const app = express();\n\n    const PORT = process.env.PORT || 4000;\n    const DIST_FOLDER = join(process.cwd(), 'dist');\n\n    const template = readFileSync(join(DIST_FOLDER, 'browser', 'index.html')).toString();\n\n    const { AppServerModuleNgFactory, LAZY_MODULE_MAP } = require('./dist/server/main.bundle');\n\n    const { provideModuleMap } = require('@nguniversal/module-map-ngfactory-loader');\n\n    app.engine('html', (_, options, callback) => {\n        renderModuleFactory(AppServerModuleNgFactory, {\n            document: template,\n            url: options.req.url,\n            extraProviders: [\n                provideModuleMap(LAZY_MODULE_MAP)\n            ]\n        }).then(html => {\n            callback(null, html);\n        });\n    });\n\n    app.set('view engine', 'html');\n    app.set('views', join(DIST_FOLDER, 'browser'));\n\n    app.get('*.*', express.static(join(DIST_FOLDER, 'browser')));\n\n    app.get('*', (req, res) => {\n        res.render(join(DIST_FOLDER, 'browser', 'index.html'), { req });\n    });\n\n    app.listen(PORT, () => {\n        console.log(`Node server listening on http://localhost:${PORT}`);\n    });\n\nFOR, ./webpack.server.config.js\n\nconst path = require('path');\nconst webpack = require('webpack');\n\nmodule.exports = {\n    entry: { server: './server.ts' },\n    resolve: { extensions: ['.js', '.ts'] },\n    target: 'node',\n    externals: [/(node_modules|main\\..*\\.js)/],\n    output: {\n        path: path.join(__dirname, 'dist'),\n        filename: '[name].js'\n    },\n    module: {\n        rules: [\n            { test: /\\.ts$/, loader: 'ts-loader' }\n        ]\n    },\n    plugins: [\n        new webpack.ContextReplacementPlugin(\n            /(.+)?angular(\\\\|\\/)core(.+)?/,\n            path.join(__dirname, 'src'),\n            {}\n        ),\n        new webpack.ContextReplacementPlugin(\n            /(.+)?express(\\\\|\\/)(.+)?/,\n            path.join(__dirname, 'src'),\n            {}\n        )\n    ]\n}\n```\n\n\nFOR, package.json\n\n```\n{\n  \"name\": \"universalrendering\",\n  \"version\": \"0.0.0\",\n  \"license\": \"MIT\",\n  \"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"ng serve\",\n    \"build\": \"ng build --prod\",\n    \"build:ssr\": \"npm run build:client-and-server-bundles && npm run webpack:server\",\n    \"serve:ssr\": \"node dist/server.js\",\n    \"build:client-and-server-bundles\": \"ng build --prod && ng build --prod --app 1 --output-hashing=false\",\n    \"webpack:server\": \"webpack --config webpack.server.config.js --progress --colors\",\n    \"test\": \"ng test\",\n    \"lint\": \"ng lint\",\n    \"e2e\": \"ng e2e\"\n  },\n  \"private\": true,\n  \"dependencies\": {\n    \"@angular/animations\": \"^5.2.0\",\n    \"@angular/common\": \"^5.2.0\",\n    \"@angular/compiler\": \"^5.2.0\",\n    \"@angular/core\": \"^5.2.0\",\n    \"@angular/forms\": \"^5.2.0\",\n    \"@angular/http\": \"^5.2.0\",\n    \"@angular/platform-browser\": \"^5.2.0\",\n    \"@angular/platform-browser-dynamic\": \"^5.2.0\",\n    \"@angular/platform-server\": \"^5.2.6\",\n    \"@angular/router\": \"^5.2.0\",\n    \"@nguniversal/module-map-ngfactory-loader\": \"^5.0.0-beta.5\",\n    \"angular-universal-express-firebase\": \"0.0.4\",\n    \"core-js\": \"^2.4.1\",\n    \"express\": \"^4.16.2\",\n    \"firebase-functions\": \"^0.8.1\",\n    \"rxjs\": \"^5.5.6\",\n    \"ts-loader\": \"^3.5.0\",\n    \"zone.js\": \"^0.8.19\"\n  },\n  \"devDependencies\": {\n    \"@angular/cli\": \"^1.7.1\",\n    \"@angular/compiler-cli\": \"^5.2.0\",\n    \"@angular/language-service\": \"^5.2.0\",\n    \"@types/jasmine\": \"~2.8.3\",\n    \"@types/jasminewd2\": \"~2.0.2\",\n    \"@types/node\": \"~6.0.60\",\n    \"codelyzer\": \"^4.0.1\",\n    \"jasmine-core\": \"~2.8.0\",\n    \"jasmine-spec-reporter\": \"~4.2.1\",\n    \"karma\": \"~2.0.0\",\n    \"karma-chrome-launcher\": \"~2.2.0\",\n    \"karma-coverage-istanbul-reporter\": \"^1.2.1\",\n    \"karma-jasmine\": \"~1.1.0\",\n    \"karma-jasmine-html-reporter\": \"^0.2.2\",\n    \"protractor\": \"~5.1.2\",\n    \"ts-node\": \"~4.1.0\",\n    \"tslint\": \"~5.9.1\",\n    \"typescript\": \"^2.7.2\"\n  }\n}\n```\n\n\nTHIS IS FOR Firebase Hosting:\n\n\nserver forlder\n\n\nFOR, index.html\n\n```\nimport * as functions from 'firebase-functions';\nimport * as angularUniversal from 'angular-universal-express-firebase';\n\nexport let server_rendering = angularUniversal.trigger({\n    index: __dirname + '/browser/index.html',\n    main: __dirname + '/server/main.bundle',\n    enableProdMode: true,\n    browserCacheExpiry: 1200,\n    cdnCacheExpiry: 600\n});\n```\n\n\nFOR, tsconfig.functions.json\n\n```\n{\n    \"compilerOptions\": {\n        \"module\": \"commonjs\",\n        \"target\": \"es2015\",\n        \"rootDir\": \".\",\n        \"outDir\": \"../functions\"\n    },\n    \"files\": [\n        \"index.ts\"\n    ]\n}\n```\n\n\n\nfunctions folder\nfunctions => this is the build folder and I use to upload into firebase hosting\n\n\nbrowser\nnode_modules\nserver\n.eslintrc.json\nindex.js\npackage-lock.json\npackage.json\nserver.js\n\n\n\nFOR, package.json\n\n```\n{\n  \"name\": \"functions\",\n  \"description\": \"Cloud Functions for Firebase\",\n  \"scripts\": {\n    \"lint\": \"./node_modules/.bin/eslint .\",\n    \"serve\": \"firebase serve --only functions\",\n    \"shell\": \"firebase experimental:functions:shell\",\n    \"start\": \"npm run shell\",\n    \"deploy\": \"firebase deploy --only functions\",\n    \"logs\": \"firebase functions:log\"\n  },\n  \"dependencies\": {\n    \"firebase-admin\": \"~5.8.1\",\n    \"firebase-functions\": \"^0.8.1\",\n    \"@angular/animations\": \"^5.2.0\",\n    \"@angular/common\": \"^5.2.0\",\n    \"@angular/compiler\": \"^5.2.0\",\n    \"@angular/core\": \"^5.2.0\",\n    \"@angular/forms\": \"^5.2.0\",\n    \"@angular/http\": \"^5.2.0\",\n    \"@angular/platform-browser\": \"^5.2.0\",\n    \"@angular/platform-browser-dynamic\": \"^5.2.0\",\n    \"@angular/platform-server\": \"^5.2.6\",\n    \"@angular/router\": \"^5.2.0\",\n    \"@nguniversal/module-map-ngfactory-loader\": \"^5.0.0-beta.5\",\n    \"angular-universal-express-firebase\": \"0.0.4\",\n    \"core-js\": \"^2.4.1\",\n    \"express\": \"^4.16.2\",\n    \"rxjs\": \"^5.5.6\",\n    \"ts-loader\": \"^3.5.0\",\n    \"zone.js\": \"^0.8.19\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"^4.12.0\",\n    \"eslint-plugin-promise\": \"^3.6.0\"\n  },\n  \"private\": true\n}\n```\n\n\nFOR, index.js\n\n```\n\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nconst angularUniversal = require(\"angular-universal-express-firebase\");\nexports.server_rendering = angularUniversal.trigger({\n    index: __dirname + '/browser/index.html',\n    main: __dirname + '/server/main.bundle',\n    enableProdMode: true,\n    browserCacheExpiry: 1200,\n    cdnCacheExpiry: 600\n});\n```\n\n    ", "Answer": "\r\nPlease check the same issue here, usually that error when your redirect rules are sending the index page instead of the javascript.\n\nUpdate your ```\npackage.json```\n ```\nscripts```\n as like follows:\n\n```\n        \"scripts\": {\n          \"ng\": \"ng\",\n          \"start\": \"ng serve\",\n          \"build\": \"ng build --prod\",\n          \"build.server\": \"ng build -aot -app ssr\",\n          \"test\": \"ng test\",\n          \"lint\": \"ng lint\",\n          \"e2e\": \"ng e2e\"\n        },\n```\n\n\nAnd then ```\n.angular-cli.json```\n ```\n\"main\": \"main.ts\"```\n to  ```\n\"main\": \"main.server.ts\"```\n.\n\nBuild browser and server versions of the app\n\n```\nnpm run build\nnpm run build.server\n```\n\n\nPlease check the tutorial to here Deploy Angular Universal w/ Firebase and demo.\n\nHopes this will help you !!\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How best to display a file hash\r\n                \r\nSimple question. What is the best (most universal) way to display a file hash? Below are two SHA256 hashes for the same file. One is displayed as base64 and one is...something else. The file hash will be used for auditing to make sure the file we send is the same as the file the auditor received. If the hash needs to be verified, I want to make sure I provide the hash that is the most easily verifiable.\n\n```\nSHA256          55461e72cccb74b475278189956b9db307bf44945e1639af93c34b224b7fcfd\nSHA256 Base 64  VUYecszLdLR1J4GJlWudswe/RJReFjmvk8NLIkt/z9s=\n```\n\n    ", "Answer": "\r\n55461e72cccb74b475278189956b9db307bf44945e1639af93c34b224b7fcfd\n\nThe point of Base64 is to constrain the character set to displayable characters. The hash is in hexadecimal which is even more constrained.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Reconstructing trees from a \"fingerprint\"\r\n                \r\nI've done my SO and Google research, and haven't found anyone who has tackled this before, or at least, anyone who has written about it.\n\nMy question is, given a \"universal\" tree of arbitrary height, with each node able to have an arbitrary number of branches, is there a way to uniquely (and efficiently) \"fingerprint\" arbitrary sub-trees starting from the \"universal\" tree's root, such that given the universal tree and a tree's fingerprint, I can reconstruct the original tree?\n\nFor instance, I have a \"universal\" tree (forgive my poor illustrations), representing my universe of possibilities:\n\n\n                Root\n        /  /  /  |  \\  \\ ... \\\n       O  O  O   O   O  O     O  (Level 1)\n      /|\\/|\\...................\\ (Level 2)\n\n\netc.\n\nI also have tree A, a rooted subtree of my universe\n\n\n        Root\n      / /|\\ \\\n     O O O O O\n    /\n\n\nEtc.\n\nIs there a way to \"fingerprint\" the tree, so that given that fingerprint, and the universal tree, I could reconstruct A?\n\nI'm thinking something along the lines of a hash, a compression, or perhaps a functional/declarative construction?  Big-O analysis (in time or space) is a plus.\n\nAs a for-instance, a nested expression like: ```\n{{(Root)},{(1),(2),(3)},{(2,3),(1),(4,5)}...}```\n representing the actual nodes present at each level in the tree is probably valid, but can it be done more efficiently?\n    ", "Answer": "\r\nI would use a list of lists, where each element in the list states how many children you have:\n\n```\n[[2][1,2][0,0,0]]```\n\n\nIs a tree with two nodes on the first level and the left child has one child node, and the right child node has 2 of its own.\n\nRun that output through a lossless compression algorithm of your choice.\n\nYou could also use a depth-first traversal of the tree, or any other type of traversal really. Whatever is easiest for you to reconstruct from.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Universal-Image-Loader Custom FileNameGenerator based on local file name\r\n                \r\nI am using Universal Image Loader to display images downloaded from a URI or already available in a disk cache implementation.\n\nI want to display music album covers, but more than one track might have the same URI for a cover (i.e. tracks from the same album). I want that even if the image is the same it's stored each time for each different track with the track name, because I want users to be able to replace the default covers with a custom one, even for each single track.\n\nFor instance\n\n\n01 - Track 01.mp3\n02 - Track 02.mp3 \n\n\nBelong to the same album and the cover URI is ```\nhttp://something/img.jpg```\n, on disk cache I want to have \n\n\n01 - Track 01.jpg\n02 - Track 02.jpg\n\n\neven if it's the same image.\n\nSo I've coded a ```\nFileNameGenerator```\n that stores a ```\nSet```\n of hashes for each ```\nUri```\n, where the hash is the SHA-1 of the absolute path of the file. \n\nHere is my implementation: \n\n```\npublic MyFileNameGenerator(String ext) {\n    super();\n    this.ext = ext;\n}\n\nHashMap<String,HashSet<String>> names = new HashMap<String, HashSet<String>>();\n\n@Override\npublic String generate(String imageUri) {\n    if(imageUri==null) return null;\n    if (imageUri.startsWith(\"file:///\")) {\n        return FilenameUtils.removeExtension(Uri.parse(imageUri)\n                .getLastPathSegment()) + \".\"+ext;\n    }\n    //How to recognize the correct hash?\n    //return FilenameUtils.removeExtension(Data.currentFiles\n    //      .get(names.get(imageUri)).getName()) + \".\" + ext;\n}\n\npublic void setTrackData(String uri, String hash) {\n    if(!names.containsKey(uri))\n        names.put(uri, new HashSet<String>());\n    names.get(uri).add(hash);\n}\n```\n\n\nBut I'm at a dead end, because it's impossible to understand for which file I'm displaying the image, as generate only takes imageURI as parameter and more hashes can belong to the same uri. \n\nHow could I circumvent this issue? \n    ", "Answer": "\r\nI think I've found a solution for this.\n\nWhenever I call my ```\nImageLoader```\n instance to show an image, I use\n\n```\nif (uri != null && !uri.isEmpty()) {\n    uri = Uri.parse(uri).buildUpon()\n        .appendQueryParameter(\"myhashkeyparameter\", \"myHashValue\").toString();\n}\nmImageLoader.displayImage(uri,mImageView);\n```\n\n\nThis code will append to a vaild uri a query parameter, the uri will become:\n\n```\nhttp://someuri/image.jpg?myhashkeyparameter=myHashValue\n```\n\n\nThen in the ```\nFileNameGenerator```\n's ```\ngenerate```\n method I can use \n\n```\nString hash = Uri.parse(imageUri).getQueryParameter(\"myhashkeyparameter\");\n```\n\n\nto retrieve the wanted file without relying on using the imageUri as key. \n\nFull code:\n\n```\n@Override\npublic String generate(String imageUri) {\n    if(imageUri==null||imageUri.isEmpty()) return \"\";\n    if (imageUri.startsWith(\"file:///\")) {\n        return FilenameUtils.removeExtension(Uri.parse(imageUri)\n                .getLastPathSegment()) + \".\"+ext;\n    }\n    String hash = Uri.parse(imageUri).getQueryParameter(\"myhashkeyparameter\");\n    if(null==hash||hash.isEmpty()) return \"\";\n    if(Data.currentFiles.containsKey(hash)){\n     return FilenameUtils.removeExtension(Data.currentFiles.get(hash).getName())+\".png\";\n    }\n    else return \"\";\n}\n```\n\n\nYou just have to be careful that the string used as queryparameter is not already used in the http URL, so avoid traditional names like name,hash,h,title and so on. \n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "how do facebook android sample apps work without debug hash?\r\n                \r\nWhen I create a facebook app to authenticate android devices, I have to put in a debug hash for testing as well as a hash for my production keystore.\n\nFacebook samples apps on the other hand are intended to work out of the box, this implies that there is a universal android debug hash that exists, or there is an exemption for these apps (based on package name?) on Facebook\n\ninsight appreciated, MingLi?\n    ", "Answer": "\r\nThere is no universal debug hash for Android.\n\nI have not personally tried the sample Facebook apps, but Facebook likely makes an exemption for them on the server side of things by some kind of identifier (package name or some kind of authentication done via code).\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to track Google Analytics anchor clicks using Universal Analytics\r\n                \r\nI'm trying to implement anchor (index.html#anchor) tracking according to this related question:\nHow to track anchor tags with Google Analytics\n\nI use anchors to launch javascript easter eggs upon entering the website and I would like to track all the anchors as they would be a different pages (urls). \n\nSo I would have tracked:\n\n```\nlink.html                 20 pageviews\nlink.html#anchor1         5 pageviews\nlink.html#anchor2         8 pageviews\n```\n\n\nThe javascript easter eggs functions are currently loaded like this:\n\n```\n    $(function () // on page load\n    {\n      if(window.location.hash.slice(1) != \"\")\n        launchEasterEgg(window.location.hash.slice(1));\n    });\n\n    $(window).bind('hashchange', function ()  //hash change detected\n    {\n      var hash = window.location.hash.slice(1);\n      launchEasterEgg(hash);\n      return true;\n    });\n```\n\n\nI'm not sure whether I should launch the tracking code on:\n\n\n\"hash change detected\" + \"on page load\"\nOR add directly to the links like: ```\n<a href=\"#anchor\" onclick=\"eventTrackingFunction()\">link</a>```\n\nOR just somehow set: ```\n_gaq.push(['_setAllowAnchor', true]);```\n as I did in the previous version of GA code\n\n\nAlso I don't quite understand what type of Google Analytics Event should I track (https://developers.google.com/analytics/devguides/collection/analyticsjs/events)\n\nApart from the sample question at the top, I'm using Universal Analytics so my tracking code is different:\n\n```\n  <script>\n    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');\n    ga('create', 'UA-XXXXXXXX-X', 'mydomain.com');\n    ga('send', 'pageview');\n  </script>\n```\n\n    ", "Answer": "\r\nI've managed to get it working by myself: I now track visitors when Easter egg is launched:\n\n```\nfunction launchEasterEgg(hash)\n{\n  ga('send', 'pageview', '/#'+hash); // track hash\n  // + my function code\n}\n```\n\n\nFor tracking visitors that don't launch Easter egg I use:\n\n```\n$(function () // on page load\n{\n  if(window.location.hash.slice(1) != \"\")\n    launchEasterEgg(window.location.hash.slice(1));\n  else\n    ga('send', 'pageview'); // track no hash pageview\n});\n```\n\n\nOf course I load the Universal analytics code before (without the \"send pageview\" line).\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Rules for using # sign in a custom URL\r\n                \r\nIn an HTTP URL, the hash sign (#) signifies an anchor within  a page and may only appear once.\n\nIs this a universal rule for all URLs? If I want to implement a custom URL protocol, can I use the following as a legal URL?\n\n```\nmyprotocol://zoo#1/cage#30/lion#11```\n\n    ", "Answer": "\r\nIn your own protocol you may do what ever you please. However if you want common parsers to be able to parse your URL you'll have to follow RFC3986 You may want to take a look at section 3 syntaxe component as for rules for using \"#\", \"?\", \":\" and \"/\".\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Pairwise independent hash functions in Java\r\n                \r\nI'm looking for a quick and easy way to use a (universal) family of pairwise independent hash functions in my Java projects.\n\nIdeally, I would have some object ```\nUniversalFamily```\n (representing the Family) which would return me objects with a method ```\nhash()```\n which hashes integers.\n\nExample usage:\n\n```\n// use this object to generate pairwise independent hash functions\nUniversalFamily family = new UniversalFamily();\n\n// these objects represent the pairwise independent hash functions\nHashF hashF1 = fam.getHashFunction();\nHashF hashF2 = fam.getHashFunction();\n// ...\n\n/* here the hash functions are being used to hash the integers 1, 2 and \n   1337, the return values (not stored) are the results of the \n   corresponding hash functions. */\nhashF1.hash(1);\nhashF1.hash(2);\nhashF2.hash(1337);\n// ...\n```\n\n\nBefore I start tinkering around, is there anything like this already available? \n    ", "Answer": "\r\nUse something like this:\n\n```\n/* Used to generate and encapsulate pairwise independent hash functions.\nSee see https://people.csail.mit.edu/ronitt/COURSE/S12/handouts/lec5.pdf , claim 5 for more information.\n */\nprivate static class HashF {\n\n    private final int a;\n    private final int b;\n    private final int p = 1610612741; // prime\n\n    HashF(int a, int b) {\n        Random rand = new Random();\n\n        this.a = rand.nextInt(p);\n        this.b = rand.nextInt(p);\n    }\n\n    // hashes integers\n    public int hash(int x) {\n        return (a*x + b) % p;\n    }\n\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Database Password Decryption in UWP\r\n                \r\nI'm a beginner at Universal Windows Platform. I am using Php My admin for my database using web services. Now I'm struck at login. In database, The password is in Bcrypted or Hashed. I don't know how to decrypt it in C# or UWP. \n    ", "Answer": "\r\nYou shouldn't decrypt it at all (or \"dehash\" - which isn't really possible, kind of the whole point of it). Instead encrypt the user input too and check if the hashes match. \n\nC# comes with a wide range of hashing algorithms (can be found in the ```\nWindows.Security.Cryptography```\n or ```\nSystem.Security.Cryptography```\n namespace) that you can use to hash the user input.\n\nIf the algorithm that you need isn't available there, you could try Bouncy Castle instead.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Does md5 have any uniqueness guarantee for short strings (finite number of strings)?\r\n                \r\nSo I understand that there is proof that MD5 can not guarantee uniqueness as there are more strings in the universe than there are MD5 hash strings, but is there any inverse proof for a finite number of strings?\nBasically, if I have strings of maximum length of X, is there an X for which MD5 is guaranteed to be unique? if yes, then what is that X? and if there are more than one values for X, what is the maximum value of X?\nor is there such an X for any other hashing algorithm, SHA-1, etc.?\n    ", "Answer": "\r\nSummarizing the excellent answers here: What's the shortest pair of strings that causes an MD5 collision?\n\nThe shortest known attack on MD5 requires 2 input blocks, that is, 128 bytes or 1024 bits.\n\nFor any hash algorithm that outputs N bits, assuming it distributes inputs approximately randomly, you can assume a collision is more than 50% likely in about ```\nsqrt(2^N)```\n inputs. For example, MD5 hashes to 128 bits, so you can expect a collision among all 64 bit inputs. This assumes a uniformly random hash. Any weaknesses would reduce the number of inputs before a collision can be expected to occur.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Angular Universal, getting node:internal/modules/cjs/loader:988 throw err; error when deploying to Heroku\r\n                \r\nI'm trying to deploy my Angular Universal 14 app to Heroku and getting this error. I have a Javascript script in my project saved in src/assets to autogenerate translation flies. What I don't understand is that the program is working well on both my local machines(Windows 11 and Ubuntu Linux) but is giving this error when I deploy it on Heroku. Below logs from ```\nheroku logs --tail```\n\n```\n                                             Starting process with command `npm start`\n2022-10-22T11:19:24.467581+00:00 app[web.1]: \n2022-10-22T11:19:24.467583+00:00 app[web.1]: > spaceman@0.0.0 start\n2022-10-22T11:19:24.467584+00:00 app[web.1]: > npm run hash:i18n && npm run serve:ssr\n2022-10-22T11:19:24.467584+00:00 app[web.1]:    \n2022-10-22T11:19:24.782970+00:00 app[web.1]: \n2022-10-22T11:19:24.782972+00:00 app[web.1]: > spaceman@0.0.0 hash:i18n\n2022-10-22T11:19:24.782973+00:00 app[web.1]: > node src/assets/hash-translations.js\n2022-10-22T11:19:24.782973+00:00 app[web.1]: \n2022-10-22T11:19:24.833752+00:00 app[web.1]: node:internal/modules/cjs/loader:988\n2022-10-22T11:19:24.833754+00:00 app[web.1]: throw err;\n2022-10-22T11:19:24.833755+00:00 app[web.1]: ^\n2022-10-22T11:19:24.833755+00:00 app[web.1]: \n2022-10-22T11:19:24.833756+00:00 app[web.1]: Error: Cannot find module 'md5'\n2022-10-22T11:19:24.833756+00:00 app[web.1]: Require stack:\n2022-10-22T11:19:24.833757+00:00 app[web.1]: - /app/src/assets/hash-translations.js\n2022-10-22T11:19:24.833759+00:00 app[web.1]: at Function.Module._resolveFilename \n(node:internal/modules/cjs/loader:985:15)\n2022-10-22T11:19:24.833760+00:00 app[web.1]: at Function.Module._load \n(node:internal/modules/cjs/loader:833:27)\n2022-10-22T11:19:24.833760+00:00 app[web.1]: at Module.require \n(node:internal/modules/cjs/loader:1057:19)\n2022-10-22T11:19:24.833761+00:00 app[web.1]: at require \n(node:internal/modules/cjs/helpers:103:18)\n2022-10-22T11:19:24.833761+00:00 app[web.1]: at Object.<anonymous> \n(/app/src/assets/hash-translations.js:4:13)\n2022-10-22T11:19:24.833762+00:00 app[web.1]: at Module._compile \n(node:internal/modules/cjs/loader:1155:14)\n2022-10-22T11:19:24.833762+00:00 app[web.1]: at Object.Module._extensions..js \n(node:internal/modules/cjs/loader:1209:10)\n2022-10-22T11:19:24.833762+00:00 app[web.1]: at Module.load \n(node:internal/modules/cjs/loader:1033:32)\n2022-10-22T11:19:24.833762+00:00 app[web.1]: at Function.Module._load \n(node:internal/modules/cjs/loader:868:12)\n2022-10-22T11:19:24.833763+00:00 app[web.1]: at Function.executeUserEntryPoint [as \nrunMain] (node:internal/modules/run_main:81:12) {\n2022-10-22T11:19:24.833763+00:00 app[web.1]: code: 'MODULE_NOT_FOUND',\n2022-10-22T11:19:24.833763+00:00 app[web.1]: requireStack: [ '/app/src/assets/hash- \ntranslations.js' ]\n2022-10-22T11:19:24.833763+00:00 app[web.1]: }\n2022-10-22T11:19:25.292927+00:00 heroku[web.1]: State changed from starting to crashed\n2022-10-22T11:19:25.297021+00:00 heroku[web.1]: State changed from crashed to starting\n2022-10-22T11:19:24.982215+00:00 heroku[web.1]: Process exited with status 1\n2022-10-22T11:19:45.429157+00:00 heroku[web.1]: Starting process with command `npm \nstart`\n2022-10-22T11:19:47.225261+00:00 app[web.1]: \n2022-10-22T11:19:47.225262+00:00 app[web.1]: > spaceman@0.0.0 start\n2022-10-22T11:19:47.225263+00:00 app[web.1]: > npm run hash:i18n && npm run serve:ssr\n2022-10-22T11:19:47.619162+00:00 app[web.1]: - /app/src/assets/hash-translations.js\n2022-10-22T11:19:47.619162+00:00 app[web.1]: - /app/src/assets/hash-translations.js\n2022-10-22T11:19:47.619163+00:00 app[web.1]: at Function.Module._resolveFilename \n(node:internal/modules/cjs/loader:985:15) \n2022-10-22T11:19:47.619164+00:00 app[web.1]: at Function.Module._load \n(node:internal/modules/cjs/loader:833:27)\n2022-10-22T11:19:47.619165+00:00 app[web.1]: at Module.require \n(node:internal/modules/cjs/loader:1057:19)\n2022-10-22T11:19:47.619166+00:00 app[web.1]: at require \n(node:internal/modules/cjs/helpers:103:18)\n2022-10-22T11:19:47.619166+00:00 app[web.1]: at Object.<anonymous> \n(/app/src/assets/hash-translations.js:4:13)\n2022-10-22T11:19:47.619166+00:00 app[web.1]: at Module._compile \n(node:internal/modules/cjs/loader:1155:14)\n2022-10-22T11:19:47.619167+00:00 app[web.1]: at Object.Module._extensions..js \n(node:internal/modules/cjs/loader:1209:10)   \n2022-10-22T11:19:47.619167+00:00 app[web.1]: at Module.load \n(node:internal/modules/cjs/loader:1033:32)\n2022-10-22T11:19:47.619167+00:00 app[web.1]: at Function.Module._load \n(node:internal/modules/cjs/loader:868:12)2022-10-22T11:19:47.619168+00:00 app[web.1]: at \nFunction.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:81:12)\n```\n\nHere is my package.json script that's running the script\n```\n\"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"npm run hash:i18n && npm run serve:ssr\",\n    \"build\": \"npm run build:ssr\",\n    \"test\": \"ng test\",\n    \"lint\": \"ng lint\",\n    \"e2e\": \"ng e2e\",\n    \"dev:ssr\": \"ng run spaceman:serve-ssr\",\n    \"serve:ssr\": \"node dist/spaceman/server/main.js\",\n    \"build:ssr\": \"npm run hash:i18n && ng build && ng run spaceman:server\",\n    \"prerender\": \"ng run spaceman:prerender\",\n    \"hash:i18n\": \"node src/assets/hash-translations.js\"\n  },\n```\n\nAnyone with any Idea how I can get rid of this error? So far I've tried to change the location of the Javascript file, initial it was in a folder at the root of the project scripts/hash-translations.js then I moved the file to the assets folder and the error is still persisting when I deploy it on Heroku. Your help will be greatly appreciated.\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "ASP.NET Universal Providers & Azure - Users Created Through ASP.NET Configuration Not Working\r\n                \r\nDescribing this in detail is going to be tough but here goes:\n\nConfiguration\n\n\n\n\nASP.NET MVC 3.0 Website Project\nVisual Studio 2010 SP1\nWindows Azure Deployment including, Compute, Storage and SQL\nSQL Azure has two sets of DB's; Development & Production\n\n\nProblem\n\n\n\nUsing the ASP.NET Universal Providers I've setup a Development_ApplicationServices DB in my SQL Azure account.  I've then gone in to the ASP.NET Configuration Website and created users for Development and assigned them to roles.  As there are multiple developers working on this project this works well so we consistently have the same default accounts available to test role related use cases.  Development has been running smoothly without any issues with the Providers.\n\nI posted the Application to an Azure Compute Instance today for the 1st time and tried to login.  Suddenly, the accounts we've been using, despite pointing at the same database are not working.  If I register a new account it works fine while I'm in the Azure environment however, if I run in the development environment and then the account I created on the Azure instance using the default Register function of MVC in the Account Controller isn't accessible though I can see it in the DB if I query directly.\n\nQuestion\n\n\n\n\nDoes the Universal Provider embed something about the Site Context (for lack of a better term) into the hash for UserID or Password?\nIs there a best practice for the way I've configured by Dev/Prod environment, as it relates to SQL Azure and Membership Services, that I can use as reference?\n\n\nHappy to answer questions to make this more clear but I'm pretty stumped at this point and don't know what would be relevant to include since this seems odd, to me at least.\n\nThanks in advance,\nK\n    ", "Answer": "\r\nI was also getting this exact same behavior.  I posted a question on the official Azure forums ( http://social.msdn.microsoft.com/Forums/en-US/windowsazuredevelopment/thread/e8944c4d-5e22-4844-82fc-2e6863f0901a ) and was directed to the answer in another post here on stackoverflow I didn't find during my initial searching.\n\nYou'll find the answer here:\n\nASP.NET Membership - login works locally, fails on Azure\n\nTo summarize, the hashing type used on Azure is different from that which is now the default for .NET 4.0.  You have to explicitely override the default machineKey element and the hashAlgorithmType attribute of the membership element in the Web.config to specify the hashing method to use.\n\nIt worked for me!\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Kentico 9 pagination and anchor links\r\n                \r\nI have a page with three webparts:\n\n\nPages Data Source\nBasic Repeater\nUniversal Pager\n\n\nThings are working as expected, but i was wonder if there is a way to scroll to the top of the Basic Repeater when a new page is selected. There isn't an anchor tag passed through the URL, just a parameter for the page selected. I've tried adding in an hash value to the QueryString, but that caused issues. Setting paging mode to PostBack cause the pagination to no longer work.\n\nI've set paging mode to PostBack, and enabled Use update panel, all within the Universal Pager webpart. With this set, the repeater doesn't update.\n    ", "Answer": "\r\nWhat about Reset scroll position property in Pager properties section?\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Hash function for a String based on scalar product with random numbers\r\n                \r\nI'm doing a college assignment that involves creating a Distributed Hash Table with a custom hash function to distribute the keys amongst the stores (nodes).\n\nIn class we learned the formal definition for a 'good' universal family of hash functions, so that the probability of collision is 1 over the size of the table (that size being prime).\n\nThe proceeding to produce the hash is the following (m size of the table):\n\n\nw = log2(m)\nDivide the key in parts of w bits each\nk = m / w\nInterpret each part as a number in [0,2^w -1]\nGenerate a vector of length k of random numbers\nThe hash is the scalar product of the parts as a vector and the vector of random numbers mod m\n\n\nThis procedure is proven to have said properties.\n\nIn the assignment I have to implement it for Strings as keys. However, Strings can only be (easily) divided into chars/bytes and not bits, so I cannot use w = log2(m) but rather w' = log256(m). If needed I can post the code, but I think the idea is understandable.\n\nIs the hash function still universal using w' or do I have to divide the String into parts of w bits?\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Windows 10 SDK 10.01.14393 installation problems\r\n                \r\nI fresh install windows 10 home edition and then start to install Visual Studio 2017 Community Edition. \n\nI only select universal windows platform development, .net desktop development and asp.net web development but \"setup completed with warnings\".\n\nI want to share the log file above. \n\nDo you have any ideas for this issue ? \n\nI also asked this question to Microsoft Community but no one answered.\n\n\nThe product failed to install the listed workloads and components due to one or more package failures.\n\nIncomplete workloads\n    Universal Windows Platform development (Microsoft.VisualStudio.Workload.Universal,version=15.0.26206.0)\n\nIncomplete components\n    Universal Windows Platform tools (2.0) (Microsoft.VisualStudio.Component.UWP.Support,version=15.0.26206.0)\n    Universal Windows Platform tools for Cordova (2.0) (Microsoft.VisualStudio.ComponentGroup.UWP.Cordova,version=15.0.26206.0)\n    Universal Windows Platform tools for Xamarin (2.0) (Microsoft.VisualStudio.ComponentGroup.UWP.Xamarin,version=15.0.26206.0)\n    Windows 10 SDK (10.0.14393.0) (Microsoft.VisualStudio.Component.Windows10SDK.14393,version=15.0.26127.0)\n\nYou can search for solutions using the information below, modify your selections for the above workloads and components and retry the installation, or remove the product from your machine.\n\nFollowing is a collection of individual package failures that led to the incomplete workloads and components above. To search for existing reports of these specific problems, please copy and paste the URL from each package failure into a web browser. If the issue has already been reported, you can find solutions or workarounds there. If the issue has not been reported, you can create a new issue where other people will be able to find solutions or workarounds.\n\nPackage 'Win10SDK_10.0.14393.795,version=10.0.14393.79501' failed to install.\n    Search URL: https://aka.ms/VSSetupErrorReports?q=PackageId=Win10SDK_10.0.14393.795;PackageAction=Install;ReturnCode=-2146889721\n    Impacted workloads\n        Universal Windows Platform development (Microsoft.VisualStudio.Workload.Universal,version=15.0.26206.0)\n    Impacted components\n        Universal Windows Platform tools (2.0) (Microsoft.VisualStudio.Component.UWP.Support,version=15.0.26206.0)\n        Universal Windows Platform tools for Cordova (2.0) (Microsoft.VisualStudio.ComponentGroup.UWP.Cordova,version=15.0.26206.0)\n        Universal Windows Platform tools for Xamarin (2.0) (Microsoft.VisualStudio.ComponentGroup.UWP.Xamarin,version=15.0.26206.0)\n        Windows 10 SDK (10.0.14393.0) (Microsoft.VisualStudio.Component.Windows10SDK.14393,version=15.0.26127.0)\n    Log\n        C:\\Users\\emrek\\AppData\\Local\\Temp\\dd_setup_20170307010944_177_Win10SDK_10.0.14393.795.log\n    Details\n        Command executed: \"c:\\windows\\syswow64\\\\windowspowershell\\v1.0\\powershell.exe\" -NoLogo -NoProfile -ExecutionPolicy Unrestricted -InputFormat None -Command \"& \"\"\"C:\\ProgramData\\Microsoft\\VisualStudio\\Packages\\Win10SDK_10.0.14393.795,version=10.0.14393.79501\\WinSdkInstall.ps1\"\"\" -SetupExe sdksetup.exe -SetupLogFolder standalonesdk -PackageId Win10SDK_10.0.14393.795 -LogFile \"\"\"C:\\Users\\emrek\\AppData\\Local\\Temp\\dd_setup_20170307010944_177_Win10SDK_10.0.14393.795.log\"\"\" -SetupParameters \"\"\"/features OptionId.AvrfExternal OptionId.WindowsSoftwareDevelopmentKit OptionId.WindowsSoftwareLogoToolkit OptionId.NetFxSoftwareDevelopmentKit /quiet /norestart\"\"\"; exit $LastExitCode\"\n        Return code: -2146889721\n        Return code details: The hash value is not correct.\n\n    ", "Answer": "\r\nI solved this problem by vpn connection. \nIf you have any problem installing visual studio 2017 or windows 10 sdk try to connect vpn and continue process. \nI do not know why this kind of situation we have but you can fix this issue connecting to the net via vpn. \n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "CCHmac with kCCHmacAlgSHA1 has inconsistent output length\r\n                \r\nI'm working on an oauth library for iOS and about 10% to 15% of my requests are failing because the ```\noauth_signature```\n generated by my library is incorrect. I've traced the issue down to ```\nCCHmac()```\n returning unexpected results. The signature fails when the hmac hash has an incorrect length. To verify this problem, I ran this code:\n\n```\nNSString *key = @\"25f108b539761bd43b6c66b64fb191c8\";\n\nfor (int i = 0; i < 25; i++) {\n    unsigned int chunks[4] = {\n        arc4random() % ((int) pow(256, 4)),\n        arc4random() % ((int) pow(256, 4)),\n        arc4random() % ((int) pow(256, 4)),\n        arc4random() % ((int) pow(256, 4))\n    };\n\n    // Generate a random input string of 32 hex chars\n    NSString *input = [NSString stringWithFormat:@\"%08x%08x%08x%08x\", chunks[0], chunks[1], chunks[2], chunks[3]];\n\n    unsigned char output[CC_SHA1_DIGEST_LENGTH];\n\n    CCHmac(kCCHmacAlgSHA1, key.UTF8String, key.length, input.UTF8String, input.length, output);\n\n    NSLog(@\"HMAC Hash Length: %02lu\", strlen(output));\n}\n```\n\n\n... and got this output:\n\n```\n2013-12-06 16:05:24.596 ODB[98281:70b] HMAC Hash Length: 40\n2013-12-06 16:05:24.596 ODB[98281:70b] HMAC Hash Length: 20\n2013-12-06 16:05:24.596 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.596 ODB[98281:70b] HMAC Hash Length: 35\n2013-12-06 16:05:24.596 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.597 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.597 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.597 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.597 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.597 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.597 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.597 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.598 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.598 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.598 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.598 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.795 ODB[98281:70b] HMAC Hash Length: 20\n2013-12-06 16:05:24.795 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.795 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.795 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.796 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.796 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.796 ODB[98281:70b] HMAC Hash Length: 41\n2013-12-06 16:05:24.796 ODB[98281:70b] HMAC Hash Length: 24\n2013-12-06 16:05:24.797 ODB[98281:70b] HMAC Hash Length: 41\n```\n\n\nThis method of hmac hashing seems to be pretty universally accepted so I would expect to see consistent output length. What am I missing?\n    ", "Answer": "\r\nIf I remember correctly, CCHmac outputs its result in binary form, so when this function finishes, output[] should always be filled with exactly 20 bytes.\n\nRecall that strlen() just iterates through memory until it finds a 'null' byte, e.g.\n\n```\nint strlen(char *s)\n{\n  for(int n = 0; s[n] != 0; s++)\n  {\n    n++;\n  }\n  return n;\n}\n```\n\n\nSo, if it happens that one of the bytes in output[] is 0, then your NSLog statement will print out a value less than 20. Otherwise, strlen() will continue searching into undefined regions of memory until it happens to find a value of 0 somewhere. You will get largely unpredictable results, and this could actually make your program crash.\n\nInstead, it looks like you may be expecting a hex-string representation of the HMAC. To get one, you could do something like:\n\n```\nNSMutableString *hexOutput = [NSMutableString string];\nfor (int i = 0; i < CC_SHA1_DIGEST_LENGTH; i++) {\n    [hexOutput appendFormat:@\"%02x\", output[i]];\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Build in ubuntu docker's container is different than in mac os\r\n                \r\nI think there is a problem in my docker's configuration but I can not find that out.\n\nI tried to use Angular Universal by following the angular tutorial on my project. I used also hammerjs that calls window in its init function. The server-side rendering does not support that kind of object so it caused:\n\n```\n})(window, document, 'Hammer');\nwindow is not defined\n```\n\n\nThanks to @cyrilletuzi for his hotfix, I moved ```\nimport 'hammerjs'```\n from app.module.ts to main.ts and it worked! Until now, it is ok on mac os.\n\nBut because of the limit of memory of my VPS on ubuntu, I have to build my project in local, using Docker. This is my Dockerfile:\n\n```\nFROM ubuntu:14.04\n\nRUN apt-get update\nRUN apt-get install curl -y\n\nWORKDIR /home\nRUN curl -sL https://deb.nodesource.com/setup_8.x -o node.sh && chmod +x node.sh\nRUN ./node.sh && apt-get install -y nodejs\nRUN apt-get install -y build-essential\nRUN chmod 755 $(which node)\n\nRUN npm config set registry http://registry.npmjs.org/\nRUN npm install -g @angular/cli@1.6.2 --unsafe-perm\n\nCOPY package.json /home\nRUN rm -rf node_modules package-lock.json\nRUN npm install\nCOPY . /home\nRUN cd /home\nRUN ng build --prod \nRUN ng build --prod --app mpp-universal --output-hashing=none\nRUN webpack --config webpack.server.config.js\n```\n\n\nThis is how my build folder looks like:\n\n```\n- browser\n- server\n    + main.bundle.js\n    + ...\n- server.js\n```\n\n\nThe problem is, it seems like I have a problem about cache. import 'hammerjs' is still in my server/main.bundle.js in docker ubuntu, but not anymore in the same file on my mac.\n\nMaybe in docker, the build didn't take the main.server.ts but main.ts\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to determine correspondence between two lists of names?\r\n                \r\nI have: \n1 million university student names and \n3 million bank customer names\n\nI manage to convert strings into numerical values based on hashing (similar strings have similar hash values). I would like to know how can I determine correlation between these two sets to see if values are pairing up at least 60%? \n\nCan I achieve this using ICC? How does ICC 2-way random work?\n\nPlease kindly answer ASAP as I need this urgently.\n    ", "Answer": "\r\nThis kind of entity resolution etc is normally easy, but I am surprised by the hashing approach here.  Hashing loses information that is critical to entity resolution.  So, if possible, you shouldn't use hash, rather the original strings.\n\nAssuming using original strings is an option, then you would want to do something like this:\n\nList A (1M), List B (3M)\n\n```\n// First, match the entities that match very well, and REMOVE them.\nfor a in List A\n  for b in List B\n    if compare(a,b) >= MATCH_THRESHOLD   // This may be 90% etc\n       add (a,b) to matchedList\n       remove a from List A\n       remove b from List B\n\n// Now, match the entities that match well, and run bipartite matching\n// Bipartite matching is required because each entity can match \"acceptably well\"\n// with more than one entity on the other side\nfor a in List A\n  for b in List B\n    compute compare(a,b)\n    set edge(a,b) = compare(a,b)\n    If compare(a,b) < THRESHOLD // This seems to be 60%\n       set edge(a,b) = 0\n\n// Now, run bipartite matcher and take results\n```\n\n\nThe time complexity of this algorithm is O(n1 * n2), which is not very good.  There are ways to avoid this cost, but they depend upon your specific entity resolution function.  For example, if the last name has to match (to make the 60% cut), then you can simply create sublists in A and B that are partitioned by the first couple of characters of the last name, and just run this algorithm between corresponding list.  But it may very well be that last name \"Nuth\" is supposed to match \"Knuth\", etc.  So, some local knowledge of what your name comparison function is can help you divide and conquer this problem better.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Hash password in a Laravel app and check in another one\r\n                \r\nI have a system where several Laravel apps upload information to individuals databases, the information of these databases could be accessed with a universal Laravel API which is the core of the system.\nI use hash::make in my Laravel apps to hash my passwords but I want to check them in my Laravel API but when I try to \\hash::check the passwords don't match. \n\nThis is my hash code in my Laravel app: \n\n```\n            $patient = new patient();\n            $patient->username = $request->input('username');\n            $patient->password = \\Hash::make($request->input('password'));\n            $patient->fullname = $request->input('name');\n            $patient->note = $request->input('note');\n            $patient->save();\n\n```\n\n\nAnd this is my API Login code: \n\n```\n        $username  = $request->username;\n        $password  = $request->password;\n        $patient = Patients::where('username','=',$username)->get();\n        if (\\Hash::check($password, $patient[0]->password))\n        {\n            return response()->json($patient[count($patient)-1]);\n        }else {\n            return 0;\n        }  \n```\n\n\nAm I doing something wrong or I just can't do something like that? \n\nThanks :) \n\n\n  I'm using Laravel 5.8\n\n    ", "Answer": "\r\nMy guess is that your ```\nconfig/hashing.php```\n could be configured differently between these apps. There are multiple options (e.g. Bcrypt, Argon2i, Argon2id, etc...) for this configuration and which one is used when the hashed password is stored, will matter when checking it. Make sure they are consistent across the different apps.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "AppleScript error \"Can’t make \\\"\" into type integer.\" number -1700\r\n                \r\nI know this error has been covered here but I have been searching through the answers for days now and when I think I have cracked it I end up breaking the code somewhere else.\n\nI am trying to run AppleScript in Xcode to make a backup of a folder on a set network machine.\n\nThe problem I am having is that the backup is date stamped and in a sub folder on the users Desktop.\nI am trying to avoid using hard POSIX paths to make the final app universal.\nAll the solutions I have found don't take this in to account.\n\nI apologize for the messy code but I am still learning AppleScript and so end up hashing different bits of code together. my code is\n\n```\nset catFolder to \"Catalyst AA\"\nset bckup to \"Backups\"\nset myDesktop to path to desktop folder as alias\nset main_folder to ((path to desktop folder) & \"Backups\") as string\n\ntell application \"Finder\"\nif not (exists folder (bckup) in myDesktop) then\n    set main_folder to (make new folder at desktop with properties {name:bckup}) as      alias\nelse\n\n    if not (exists folder (catFolder) in main_folder) then\n        set cat_folder to (make new folder at folder main_folder with properties {name:catFolder}) as alias\n    end if\nend if\n\nend tell\n\n\n\ntry\nmount volume \"afp://10.0.0.1/Content SSD AA/Catalyst/Catalyst 4.40_m373_HD\"\n\nset d to (year of (current date) as text) & \"-\" & (month of (current date) as integer as text) & \"-\" & (day of (current date) as text) & \"-\" & (time string of (current date))\ntell application \"Finder\"\n    set f to make new folder at POSIX file \"~/Desktop/Backups/Catalyst AA\" with properties {name:d}\n    duplicate POSIX file \"/Volumes/Content SSD AA/Catalyst\" to f\nend tell\ntry\n    tell application \"Finder\"\n        eject disk \"Content SSD AA\"\n    end tell\nend try\nend try\n```\n\n\nthe error file is\n\n```\ntell application \"Finder\"\npath to desktop as alias\n    --> alias \"Macintosh HD:Users:ben:Desktop:\"\npath to desktop\n    --> alias \"Macintosh HD:Users:ben:Desktop:\"\nexists folder \"Backups\" of alias \"Macintosh HD:Users:ben:Desktop:\"\n    --> true\nResult:\nerror \"Can’t make \\\"Catalyst AA\\\" into type integer.\" number -1700 from \"Catalyst AA\" to   integer\n```\n\n\nI appreciate it if someone can point out where I'm going wrong.\n\nThanks for your time\n\nBen\n    ", "Answer": "\r\nTry this:\n\n```\nset folderPath to POSIX path of (path to desktop as text) & \"Backups/Catalyst AA\"\ndo shell script \"mkdir -p \" & quoted form of folderPath\n\ntry\n    mount volume \"afp://10.0.0.1/Content SSD AA/Catalyst/Catalyst 4.40_m373_HD\"\n    set d to (year of (current date) as text) & \"-\" & (month of (current date) as integer as text) & \"-\" & (day of (current date) as text) & \"-\" & (time string of (current date))\n    tell application \"Finder\"\n        set f to make new folder at POSIX file folderPath with properties {name:d}\n        duplicate POSIX file \"/Users/unimac/Desktop/output.csv\" to f\n    end tell\n    try\n        tell application \"Finder\"\n            eject disk \"Content SSD AA\"\n        end tell\n    end try\nend try\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Login accepts password with any extra characters appended\r\n                \r\nI don't know whether this is actually a problem or not. I did tell the IT administrators but I got no reply and nothing was done.\n\nAt my university, everyone has a logon account. I discovered that it would accept my password if I added a random string after it.\n\nSo if my password was \n\n```\npassword\n```\n\n\nIt accepts\n\n```\npassword123\npasswordhvgFghvjej36277\n```\n\n\nAnd so on. It does not accept anything like\n\n```\npasswor\n133password\n```\n\n\nIt only works if the password is correct and begin the phrase.\n\nIs this a big problem?\nSurely if they are hashing the passwords, adding an extra character will change the hash?\n    ", "Answer": "\r\nThis will happen if they cut off the password after a certain maximum length. Not really good practice, but unfortunately not uncommon.\n\nSo what gets hashed (hopefully they do hash!) is just a fixed-length prefix of what you entered.\n\nOf course, it could also mean that they just store a fixed-length truncated clear-text password. That would be terrible.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to replace Django's primary key with a different integer that is unique for that table\r\n                \r\nI have a Django web application that uses the default auto-incremented positive integers as the primary key. This key is used throughout the application and is frequently inserted into the URL. I don't want to expose this number to the public so that they can guess the number of users or other entities in my Database.\n\nThis is a frequent requirement and I have seen questions to similar mine with answers. Most solutions recommend hashing the original primary key value. However, none of those answers fit my need exactly. These are my requirements:\n\n\nI would like to keep the Primary Key field type as Integer. \nI also would prefer not to have to hash/unhash this value every time it is read or written or compared to the database. That seems wastefuly It would be nice to do it just once: When the record is initially inserted into the Database\nThe hashing/encryption function need not be reversible since I don't need to recover the original sequential key. The hashed value just needs to be unique.\nThe hashed value needs to be unique ONLY for that table -- not universally unique.\nThe hashed value should be as short as possible. I would like to avoid extremely long (20+ characters) URLs\n\n\nWhat is the best way to do achieve this? Would the following work?\n\n```\ndef hash_function(int):\n    return fancy-hash-function # What function should I use??\n\n\ndef obfuscate_pk(sender, instance, created, **kwargs):\n    if created:\n        logger.info(\"MyClass #%s, created with created=%s: %s\" % (instance.pk, created, instance))\n        instance.pk = hash_function(instance.pk)\n        instance.save()\n        logger.info(\"\\tNew Pk=%s\" % instance.pk)\n\nclass MyClass(models.Model):\n    blahblah = models.CharField(max_length=50, null=False, blank=False,)\n\n\npost_save.connect(obfuscate_pk, sender=MyClass)\n```\n\n    ", "Answer": "\r\nThe Idea\nI would recommend to you the same approach that is used by Instagram. Their requirements seems to closely follow yours.\n\nGenerated IDs should be sortable by time (so a list of photo IDs, for\nexample, could be sorted without fetching more information about the\nphotos) IDs should ideally be 64 bits (for smaller indexes, and better\nstorage in systems like Redis) The system should introduce as few new\n‘moving parts’ as possible—a large part of how we’ve been able to\nscale Instagram with very few engineers is by choosing simple,\neasy-to-understand solutions that we trust.\n\nThey came up with a system that has 41 bits based on the timestamp, 13 o the database shard and 10 for an auto increment portion. Sincce you don't appear to be using shards. You can just have 41 bits for a time based copmonent and 23 bits chosen at random. That does produce an extremely unlikely 1 in 8.3 million chance of getting a conflict if you insert records at the same time. But in practice you are never likely to hit this. Right so how about some code:\nGenerating IDs\n```\nSTART_TIME = a constant that represents a unix timestamp\n\ndef make_id():\n    '''\n    inspired by http://instagram-engineering.tumblr.com/post/10853187575/sharding-ids-at-instagram\n        '''\n    \n    t = int(time.time()*1000) - START_TIME\n    u = random.SystemRandom().getrandbits(23)\n    id = (t << 23 ) | u\n    \n    return id\n\n\ndef reverse_id(id):\n    t  = id >> 23\n    return t + START_TIME \n```\n\nNote, ```\nSTART_TIME```\n in the above code is some arbitary starting time. You can use time.time()*1000 , get the value and set that as ```\nSTART_TIME```\n\nNotice that the ```\nreverse_id```\n method I have posted allows you to find out at which time the record was created. If you need to keep track of that information you can do so without having to add another field for it! So your primary key is actually saving your storage rather than increasing it!\nThe Model\nNow this is what your model would look like.\n```\nclass MyClass(models.Model):\n   id = models.BigIntegerField(default = fields.make_id, primary_key=True)  \n```\n\nIf you make changes to your database outside django you would need to create the equivalent of ```\nmake_id```\n as an sql function\nAs a foot note. This is somewhat like the approach used by Mongodb to generate it's _ID for each object.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "What's a reliable way to check an RSS feed for new entries?\r\n                \r\nPart of an app I'm building needs to check RSS feeds for updates. I'm looking for a reliable way to know if a feed has new entries.\n\nI know that sometimes people make posts to the future and, after that, posts to the present time which could cause some entries to be hidden. It seems like there could be more complications than that, as well. I also know that hashing the title or content would result in poor performance and unreliable results since those can change and are not a sign of new entries. And I know that a few years ago when I was maintaining a podcast RSS feed manually I never changed the  item.\n\nSo, I need some way to reliably check RSS, Atom, etc feeds for new entries since they were lasted checked.\n\nSpecifically, this application will be written in Python for Google App Engine using Universal Feed Parser, but I doubt that matters too much in this case.\n    ", "Answer": "\r\nYou can use a conditional get by adding a if-modified-since header to your http request. Well behaved servers will return a 304 unmodified if there are no changes.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Silverlight WCF Ria Services app built with Visual Studio 2012 fails while creating Users table for membership\r\n                \r\nI am creating a Silverlight 5 application with RIA Services under Windows 8 and Visual Studio 2012. I added a model pointing to my database, created the domain service and everything works fine.\nNow I need to add membership functionality and so I was thinking of using the good'old aspnet_regsql to add the new tables, views, sprocs, etc. needed by the ASP.NET subsystem.\nI learned here http://dotnet.dzone.com/articles/stronger-password-hashing-net that aspnet_regsql is no longer used and that a Universal Provider is now used.\nHowever, my database already contained a table named Users and so after I tried to log in to the newly created app (as suggested here: ASP.NET Universal Providers and Session State), the other tables got created, but the Users table didn't (since it already existed).\nThen when I try to run the app and create a new user, the app crashes on line 62 of the default UserRegistrationService.cs file that says:\nMembership.CreateUser(user.UserName, password, user.Email, user.Question, user.Answer, true, null, out createStatus);\nThe inner exception of that error is:\nInvalid column name 'ApplicationId'.\\r\\nInvalid column name 'UserName'.\\r\\nInvalid column name 'ApplicationId'.\\r\\nInvalid column name 'UserName'.\\r\\nInvalid column name 'IsAnonymous'.\\r\\nInvalid column name 'LastActivityDate'.\n\nI think the problem is that the Users table didn't get created. But I don't know how to fix this problem.    \n\nAny suggestions?\nThanks to all!\nTrex\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Python with etc/Shadow\r\n                \r\nso I'm writing this program that needs to check the password hash in etc/shadow and compare it to the password the user entered. I tried encrypting the password with hashlib.sha512, but the result was not the same. I think it's salted some how, but I don't know if it uses a universal salt or how I can get the salt each time. \n\ntldr; I need a way for a user to enter a password, then have the program hash it and check it against the etc/shadow. Any ideas?\n    ", "Answer": "\r\nTry this https://pypi.python.org/pypi/pam . First link in google by ```\npython pam```\n.\nLook at distribution package manager for python-pam if exists. Else install with ```\npip```\n or ```\neasy_install```\n.\n\nSmall example:\n\n```\n>>> import pam\n>>> pam.authenticate('fred', 'fredspassword')\nFalse\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Arrays/Ptr Dynamic Allocation in C\r\n                \r\nI have a series of values I am \"hashing\" into 16000000 slots. I must increment the slots to indicate how many hashed into it.\nIf a slot is needed, it should be initialized to 0 before I begin incrementation.\nI'm unfamiliar with malloc() but I understand it may be needed (using C). Ptr/array conventions have always been confusing to me.\nI have the following:\n```\n\nint (*temp) [16000000]; // ptr to array of 16000000 (unallocated)\n\nfor (n)\n (*temp)[hashmap]++; // dereference the pointer, access into array at hashmap to increment\n\n...\n\nfor (n)\n if (temp[i] != NULL) // if the array location at the ith position is allocated\n   .... = (*temp)[i]; // set some thing equal to the value at that position\n\n```\n\nAre my usages correct so far? If so, how may I dynamically allocate to fulfill my needs?\nFor reference, I'm a second-year CS student in university.\n    ", "Answer": "\r\nThis would be the easiest way to create an initialized array of integers (or if your counts are positive use ```\nsize_t```\n instead of ```\nint```\n):\n```\n#include <stdlib.h>\n// ...\nint *hash_table = calloc(16000000, sizeof int);\n```\n\nThen you do:\n```\nsize_t hash_value = hash_function(key) % (sizeof tmp / sizeof *tmp);\nhash_table[hash_value]++;\n```\n\nwhere ```\nkey```\n is the thing you want to assign to a slot (you don't tell us what but could be a string or a number or whatever).\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Converting from MD5 Legacy Auth System to CakePHP\r\n                \r\nI have a site which runs off an MD5 hashing scheme for passwords. As a way of supporting this legacy system, I've this answer to manually override the login system for now. But this isn't really ideal, as MD5 is pretty much universally known to be awful at encryption. So in the interest of security, what's the best way to migrate users over to the safer CakePHP auth system without causing them undue grief?\n    ", "Answer": "\r\nFigured it out thanks to this answer (albeit lightly modified). Basically, it updates the user behind the scenes to use the new system if the current system doesn't match up with it.\n\n```\n/**\n *  Login method\n */\npublic function login() {\n    $this->layout = 'homepage';\n    // If the user is already logged in, redirect to their user page\n    if($this->Auth->user() != null) {\n        $this->redirect();\n    } else {\n        // If this is being POSTed, check for login information\n        if($this->request->is('post')) {\n            if($this->Auth->login($this->loginHelper($this->request->data))) {\n                // Redirect to origin path, ideally\n\n            } else {\n                $this->Session->setFlash('Invalid username or password, try again');\n            }\n        }           \n    }\n}\n\n/**\n *  Update password method\n *  @param array The user's data array\n *  @param Returns either a user object if the user is valid or null otherwise\n */\nprivate function loginHelper($data) {\n    $username = $this->data['User']['username'];\n    $plainText = $this->data['User']['password'];\n\n    $user = current($this->User->findByUsername($username));\n\n    $salted = Security::hash($plainText, null, true);\n\n    if ($salted === $user['password']) {\n        return $user; // user exists, password is correct\n    }\n\n    $md5ed = Security::hash($plainText, 'md5', null);\n\n    if ($md5ed === $user['password']) {\n                $this->User->id = $user['id'];\n        $this->User->saveField('password', $plainText);\n\n        return $user; // user exists, password now updated to blowfish\n    }\n\n    return null; // user's password does not exist.\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Verify signature generated with RSA 2048-bit key, SHA256 algorithm and PKCSv1.5 padding\r\n                \r\nI have a UWA (Universal Windows Application) signing some data with the KeyCredential.RequestSignAsync method.\n\nThe signature is created with:\n\n\nRSA 2048-bit key (public portion can be retrieved with KeyCredential.RetrievePublicKey)\nHashing algorithm used is SHA256\nPadding used with the signature is PKCSv1.5\n\n\nAnd can be validated in the same UWA with the following code:\n\n```\npublic static bool VerifySignature(\n    IBuffer buffPublicKey,\n    IBuffer buffMessageData,\n    IBuffer buffSignature)\n{\n    bool b = false;\n\n    // Open the algorithm provider\n    var algProv = AsymmetricKeyAlgorithmProvider.OpenAlgorithm(AsymmetricAlgorithmNames.RsaSignPkcs1Sha256);\n\n    // Import the public key\n    var ckey = algProv.ImportPublicKey(buffPublicKey);\n\n    // Verify\n    b = CryptographicEngine.VerifySignature(ckey, buffMessageData, buffSignature);\n\n    return b;\n}\n```\n\n\nI need to verify that signature but in a regular C# application (not UWA). The Public Key, message and signature are being encoded to base 64 with CryptographicBuffer.EncodeToBase64String before being transferred.\n\nSo according System.Security.Cryptography namespace I tried with:\n\n```\npublic static bool VerifySignature(string base64PublicKey, string base64Data, string base64Signature)\n{\n    bool b = false;\n\n    byte[] publicKey = Convert.FromBase64String(base64PublicKey);\n    byte[] data = Convert.FromBase64String(base64Data);\n    byte[] signature = Convert.FromBase64String(base64Signature);\n\n    using (var rsa = new RSACryptoServiceProvider(2048))\n    {\n        // Import public key\n        rsa.ImportCspBlob(publicKey);\n\n        // Create signature verifier with the rsa key\n        var signatureDeformatter = new RSAPKCS1SignatureDeformatter(rsa);\n\n        // Set the hash algorithm to SHA256.\n        signatureDeformatter.SetHashAlgorithm(\"SHA256\");\n\n        b = signatureDeformatter.VerifySignature(data, siganture);\n    }\n\n    return b;\n}\n```\n\n\nBut getting a System.Security.Cryptography.CryptographicException with Additional information: Bad Version of provider.  in:\n\n\n  rsa.ImportCspBlob(publicKey);\n\n\n¿How is the proper way to verify the signature with that public key?\n\nEDIT: Sample values (base64 encoded)\n\n\nPublicKey: MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAp6HzbSgZPkJPfZJWydFAKdzUWlQcGHCTZhghg8HwHOfRZp3QZ/iiDORVzdIlW6XYPz76aAn8Nxm/v4NbsQsFPbwIcc7CPOJe21VT+7f6ocZ4kef0dqxUOGuK1FynrqzsAeYoaeTW+w/HElXODOEzZs3CfyE3d4hy3TTM/mVyQGV1FO/hHWB/zXq7ryQ8hXP/ueJimmJvitB7UweemRxvEYfVx52VVAgzg1RqVWeRj8L/obfm0lwQtIAHdDOnIi/cwpsyKQNikjMsf4dFgt14fcOgFdSG06jB840GnOsRZM04CWZQ9ttwAvoNGK/zjriRYGySQ4Ey0K0l5G3UVr56mQIDAQAB\nData: dGF0b0Bmcm9td2luMzIuY29t\nSignature: lWKRRgWBA2lBAfUvBS+54s9kmHTH3nJwcvYYmjCg5QpWQ9joY7Rzpq0zZjOhyxASXoAN4Vz8+mqSqPWi/4DFH7947ZWZSbopPfxiI7jjDRMAVymG0B+dRVjiMow48ZvhgP/FGSZqeLAei77Z0aAmwN2TBxkClqBpt9uy+nkI7V/TJGAbbLcWfiPWNVOGsU0smoFDQLlJjkocahNSOqjj+9PPFVqbc/VVHQWsSoq1ZxtCPILFwPCCtUCDITXrU/riGMFJ282p/3rfhDJKYis9/izR98/zgBLRoCew8zu8Za4UNWaHaR3HP/6voQI2NiVSKtss1VjvwjwXYIOh56yeSw==\n\n    ", "Answer": "\r\nGiven that the PublicKey is ASN.1 encoded in X509SubjectPublicKeyInfo format and that rsa.ImportCspBlob(publicKey) expects a blob that is compatible with the unmanaged Microsoft Cryptographic API (CAPI), I've created a helper method based on this solution that extracts the public key parameters.\n\nWith the following code, the signature is verified successfuly:\n\n```\nusing System;\nusing System.IO;\nusing System.Security.Cryptography;\n\n\nnamespace ConsoleApplication2\n{\n    class Program\n    {\n\n        static void Main(string[] args)\n        {\n            var verified = false;\n\n            byte[] data = Convert.FromBase64String(\"dGF0b0Bmcm9td2luMzIuY29t\");\n            byte[] signature = Convert.FromBase64String(\"lWKRRgWBA2lBAfUvBS+54s9kmHTH3nJwcvYYmjCg5QpWQ9joY7Rzpq0zZjOhyxASXoAN4Vz8+mqSqPWi/4DFH7947ZWZSbopPfxiI7jjDRMAVymG0B+dRVjiMow48ZvhgP/FGSZqeLAei77Z0aAmwN2TBxkClqBpt9uy+nkI7V/TJGAbbLcWfiPWNVOGsU0smoFDQLlJjkocahNSOqjj+9PPFVqbc/VVHQWsSoq1ZxtCPILFwPCCtUCDITXrU/riGMFJ282p/3rfhDJKYis9/izR98/zgBLRoCew8zu8Za4UNWaHaR3HP/6voQI2NiVSKtss1VjvwjwXYIOh56yeSw==\");\n            byte[] publicKey = Convert.FromBase64String(\"MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAp6HzbSgZPkJPfZJWydFAKdzUWlQcGHCTZhghg8HwHOfRZp3QZ/iiDORVzdIlW6XYPz76aAn8Nxm/v4NbsQsFPbwIcc7CPOJe21VT+7f6ocZ4kef0dqxUOGuK1FynrqzsAeYoaeTW+w/HElXODOEzZs3CfyE3d4hy3TTM/mVyQGV1FO/hHWB/zXq7ryQ8hXP/ueJimmJvitB7UweemRxvEYfVx52VVAgzg1RqVWeRj8L/obfm0lwQtIAHdDOnIi/cwpsyKQNikjMsf4dFgt14fcOgFdSG06jB840GnOsRZM04CWZQ9ttwAvoNGK/zjriRYGySQ4Ey0K0l5G3UVr56mQIDAQAB\");\n\n            byte[] modulus;\n            byte[] exponent;\n            ExtractPublicKeyParameters(publicKey, out modulus, out exponent);\n\n            using (var rsa = new RSACryptoServiceProvider())\n            {\n                // Create parameters\n                var rsaParam = new RSAParameters()\n                {\n                    Modulus = modulus,\n                    Exponent = exponent\n                };\n\n                // Import public key\n                rsa.ImportParameters(rsaParam);\n\n                // Create signature verifier with the rsa key\n                var signatureDeformatter = new RSAPKCS1SignatureDeformatter(rsa);\n\n                // Set the hash algorithm to SHA256.\n                signatureDeformatter.SetHashAlgorithm(\"SHA256\");\n\n                // Compute hash\n                byte[] hash;\n                using (SHA256 sha256 = SHA256.Create())\n                {\n                    hash = sha256.ComputeHash(data);\n                }\n\n                verified = signatureDeformatter.VerifySignature(hash, signature);\n            } \n\n        }\n\n        // encoded OID sequence for  PKCS #1 rsaEncryption szOID_RSA_RSA = \"1.2.840.113549.1.1.1\"\n        static readonly byte[] SeqOid = { 0x30, 0x0D, 0x06, 0x09, 0x2A, 0x86, 0x48, 0x86, 0xF7, 0x0D, 0x01, 0x01, 0x01, 0x05, 0x00 };\n\n        public static void ExtractPublicKeyParameters(byte[] publicKey, out byte[] modulus, out byte[] exponent)\n        {\n            modulus = new byte[0];\n            exponent = new byte[0];\n\n            byte[] seq = new byte[15];\n\n            // ---------  Set up stream to read the asn.1 encoded SubjectPublicKeyInfo blob  ------\n            MemoryStream mem = new MemoryStream(publicKey);\n            BinaryReader binr = new BinaryReader(mem);    //wrap Memory Stream with BinaryReader for easy reading\n            byte bt = 0;\n            ushort twobytes = 0;\n\n            try\n            {\n\n                twobytes = binr.ReadUInt16();\n                if (twobytes == 0x8130) //data read as little endian order (actual data order for Sequence is 30 81)\n                    binr.ReadByte();    //advance 1 byte\n                else if (twobytes == 0x8230)\n                    binr.ReadInt16();   //advance 2 bytes\n                else\n                    return;\n\n                seq = binr.ReadBytes(15);       //read the Sequence OID\n                if (!CompareBytearrays(seq, SeqOid))    //make sure Sequence for OID is correct\n                    return;\n\n                twobytes = binr.ReadUInt16();\n                if (twobytes == 0x8103) //data read as little endian order (actual data order for Bit String is 03 81)\n                    binr.ReadByte();    //advance 1 byte\n                else if (twobytes == 0x8203)\n                    binr.ReadInt16();   //advance 2 bytes\n                else\n                    return;\n\n                bt = binr.ReadByte();\n                if (bt != 0x00)     //expect null byte next\n                    return;\n\n                twobytes = binr.ReadUInt16();\n                if (twobytes == 0x8130) //data read as little endian order (actual data order for Sequence is 30 81)\n                    binr.ReadByte();    //advance 1 byte\n                else if (twobytes == 0x8230)\n                    binr.ReadInt16();   //advance 2 bytes\n                else\n                    return;\n\n                twobytes = binr.ReadUInt16();\n                byte lowbyte = 0x00;\n                byte highbyte = 0x00;\n\n                if (twobytes == 0x8102) //data read as little endian order (actual data order for Integer is 02 81)\n                    lowbyte = binr.ReadByte();  // read next bytes which is bytes in modulus\n                else if (twobytes == 0x8202)\n                {\n                    highbyte = binr.ReadByte(); //advance 2 bytes\n                    lowbyte = binr.ReadByte();\n                }\n                else\n                    return;\n                byte[] modint = { lowbyte, highbyte, 0x00, 0x00 };   //reverse byte order since asn.1 key uses big endian order\n                int modsize = BitConverter.ToInt32(modint, 0);\n\n                int firstbyte = binr.PeekChar();\n                if (firstbyte == 0x00)\n                {   //if first byte (highest order) of modulus is zero, don't include it\n                    binr.ReadByte();    //skip this null byte\n                    modsize -= 1;   //reduce modulus buffer size by 1\n                }\n\n                modulus = binr.ReadBytes(modsize);   //read the modulus bytes\n\n                if (binr.ReadByte() != 0x02)            //expect an Integer for the exponent data\n                    return;\n                int expbytes = (int)binr.ReadByte();        // should only need one byte for actual exponent data (for all useful values)\n                exponent = binr.ReadBytes(expbytes);\n            }\n\n            finally\n            {\n                binr.Close();\n            }\n        }\n\n        private static bool CompareBytearrays(byte[] a, byte[] b)\n        {\n            if (a.Length != b.Length)\n                return false;\n            int i = 0;\n            foreach (byte c in a)\n            {\n                if (c != b[i])\n                    return false;\n                i++;\n            }\n            return true;\n        }\n    }\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Safely handling passwords in Python?\r\n                \r\nI want to try out a system where I use a key and salt it with the name of a website, then hash it and use the hash as my password on the site that it's salted with. But, I'd like to do this securely. My concerns are:\n\n\nThe hash (my password for a given site) being printed to the terminal\nThe hash, as well as my universal key used to generate the hash, being in memory.\n\n\nWould it be safe to print the password to the terminal, and just close the terminal after?\nWould the key and password be gone from memory and disk once Python has completed?\nI'm going to use getpass, but does that provide any actual security against anything but over-shoulder lookers? Is there a way to securely overwrite the raw key and the hash/password in RAM?\n    ", "Answer": "\r\nI wouldn't worry about these. If a hostile agent is on your machine, you have bigger issues to worry about than terminal buffers and private memory.\n\nI do know that there are already similar solutions that are much slicker than what you describe; browser plugins that combine a master password with the domain name to make a unique plugin, with nice auto-completion features.\n\nBut if this is mostly a programming exercise, go for it! \"Normal\" users won't be able to access your terminal buffer. They also shouldn't be able to examine the memory of your process. \n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Hash Function For Sequence of Unique Ids (UUID)\r\n                \r\nI am storing message sequences in the database each sequence can have up to ```\nN```\n number of messages. I want to create a hash function which will represent the message sequence and enable to check faster if message sequence exists.\n\nEach message has a case-sensitive alphanumeric universal unique id (UUID).\nConsider following messages ```\n(M1, M2, M3)```\n with ids-\n\n```\nM1 - a3RA0000000e0taBB\nM2 - a3RA00033000e0taC\nM3 - a3RA0787600e0taBB\n```\n\n\nMessage sequences can be\n\n```\nSequence-1 : (M1,M2,M3)\nSequence-2 : (M1,M3,M2)\nSequence-3 : (M2,M1,M3)\nSequence-4 : (M1,M2)\nSequence-5 : (M2,M3)\n```\n\n...etc...\n\nFollowing is the database structure example for storing message sequence \n\n\n\nGiven the message sequence, we need to check whether that message sequence exists in the database. For example, check if message sequence ```\nM1 -> M2 -> M3```\n i.e. with UIDs ```\n(a3RA0000000e0taBB -> a3RA00033000e0taC -> a3RA0787600e0taBB)```\n exists in the database. \n\nInstead of scanning the rows in the table, I want to create a hash function which represents the message sequence with a hash value. Using the hash value lookup in the table supposedly faster.\n\nMy simple hash function is-\n\n\nI am wondering what would be an optimal hash function for storing the message sequence hash for faster is exists check.\n    ", "Answer": "\r\nYou don't need a full-blown cryptographic hash, just a fast one, so how about having a look at FastHash: https://github.com/ZilongTan/Coding/tree/master/fast-hash. If you believe 32 or 64 bit hashes are not enough (i.e. produce too many collisions) then you could use the longer MurmurHash: https://en.wikipedia.org/wiki/MurmurHash (actually, the author of FastHash recommends this approach)\n\nThere's a list of more algorithms on Wikipedia: https://en.wikipedia.org/wiki/List_of_hash_functions#Non-cryptographic_hash_functions\n\nIn any case, hashes using bit operations (SHIFT, XOR ...) should be faster than the multiplication in your approach, even on modern machines.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Change default hash function for block hashing\r\n                \r\ni'm very new with HF and i'm supposed to study this platform for a university project. The first thing assigned is to understand Fabric sources, in particular find where the block creation is done and in which way may be possible to change the default hash function used for blocks creation.\n\nI'v found some files in which may be the interested part, but i'm new with the platform and i really don't know where to get such information.\n\nI've found this:\n\n\nhttps://github.com/hyperledger/fabric/blob/release-1.2/common/ledger/blockledger/util.go\nCreateNewBlock function\nhttps://github.com/hyperledger/fabric/blob/release-1.2/protos/common/block.go\n\n\nAny help? Thank you.\n    ", "Answer": "\r\nAccording to the document, currently SHA256 is the only one valid algorithm. And it was hardcoded some places. \n\nHowever, you can do some experiments, bellow are some configuration places you may want to investigate - \nThe BCCSP configuration for both peer and orderer nodes, and the channel configuration Hashing Algorithm part. \n\nBCCSP configures the blockchain crypto service providers.\n\n```\nBCCSP:\n    # Default specifies the preferred blockchain crypto service provider\n    # to use. If the preferred provider is not available, the software\n    # based provider (\"SW\") will be used.\n    # Valid providers are:\n    #  - SW: a software based crypto provider\n    #  - PKCS11: a CA hardware security module crypto provider.\n    Default: SW\n\n    # SW configures the software based blockchain crypto provider.\n    SW:\n        # TODO: The default Hash and Security level needs refactoring to be\n        # fully configurable. Changing these defaults requires coordination\n        # SHA2 is hardcoded in several places, not only BCCSP\n        Hash: SHA2\n        Security: 256\n        # Location of key store. If this is unset, a location will be\n        # chosen using: 'LocalMSPDir'/keystore\n        FileKeyStore:\n            KeyStore:\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Where and how are passwords stored in Magento?\r\n                \r\nIt would be a tremendous user experience bless to have a universal login across various apps of my website. For now, I have a storefront of Magento and a community of IPS board. And I'm trying to integrate them into one universal login for my users.\n\nIPS board offers a variety of login methods and one of them is External Database that enables me to integrate it with an external database for user details. \n\nFantastic! So I can link IPS with Magento's database for unified user credentials.\n\nHowever, thus far I can only find the email field that is customer_entity.email.\n\nMy questions are:\n\n\nWhat is the password hash field (table.field) in Magento?\nHow does Magento generate password hash? MD5? SHA1? What is the salt (I guess it's different by installation but where can I find it)?\n\n\nAs you can see from the attached images, I need the details of where and how Magento stores password to enable IPS to use Magento's database as external database for user login details.\n\nAttached:\n\n\n\n\n\nAny idea or suggestion on how to get this done would be greatly appreciated!\n    ", "Answer": "\r\nCustomer's password is stored in ```\ncustomer_entity_varchar```\n, it is an eav attribute. You can't use IPB external database functionality. You should use ```\nMage::getModel('customer/customer')->authenticate($logi, $password);```\n to authenticate customers in your code.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "how to get length of javascript \"array\"\r\n                \r\nWe have been using javascript \"hashes\" a lot lately, and we've been looking for a universal way to count the items contained in both arrays and hashes without having to \"know\" which we're dealing with except in the count method. As everyone knows .length is useless since it only returns the value of the highest index in the array. What we have below does not work because hashes test true for Array, but the length value returned is crap for hashes. We originally replaced .length all over our project with Object.keys().length, but this isn't supported in IE8 and lower.\n\nThis is such a stupid simple thing and we can't seem to get it working. Help me, Obi Wan. You're my only hope!\n\n```\nfunction isNullOrUndefined(aObject) {\n    \"use strict\";\n    return (typeof aObject === 'undefined' || aObject === null);\n}\n\nfunction count(aList) {\n    \"use strict\";\n    var lKey = null,\n        lResult = 0;\n    if (!isNullOrUndefined(aList)) {\n        if (aList.constructor == Array) {\n            lResult = aList.length;\n        } else if (!isNullOrUndefined(Object.keys)) {\n            lResult = Object.keys(aList).length;\n        } else {\n            for (lKey in aList) {\n                if (aList.hasOwnProperty(lKey)) {\n                    lResult++;\n                }\n            }\n        }\n    }\n    return lResult;\n}\n```\n\n    ", "Answer": "\r\n```\nObject.keys```\n polyfill copied verbatim from the ES5-shim\n\n```\n// ES5 15.2.3.14\n// http://es5.github.com/#x15.2.3.14\nif (!Object.keys) {\n    // http://whattheheadsaid.com/2010/10/a-safer-object-keys-compatibility-implementation\n    var hasDontEnumBug = true,\n        dontEnums = [\n            \"toString\",\n            \"toLocaleString\",\n            \"valueOf\",\n            \"hasOwnProperty\",\n            \"isPrototypeOf\",\n            \"propertyIsEnumerable\",\n            \"constructor\"\n        ],\n        dontEnumsLength = dontEnums.length;\n\n    for (var key in {\"toString\": null}) {\n        hasDontEnumBug = false;\n    }\n\n    Object.keys = function keys(object) {\n\n        if ((typeof object != \"object\" && typeof object != \"function\") || object === null) {\n            throw new TypeError(\"Object.keys called on a non-object\");\n        }\n\n        var keys = [];\n        for (var name in object) {\n            if (owns(object, name)) {\n                keys.push(name);\n            }\n        }\n\n        if (hasDontEnumBug) {\n            for (var i = 0, ii = dontEnumsLength; i < ii; i++) {\n                var dontEnum = dontEnums[i];\n                if (owns(object, dontEnum)) {\n                    keys.push(dontEnum);\n                }\n            }\n        }\n        return keys;\n    };\n\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "how to get length of javascript \"array\"\r\n                \r\nWe have been using javascript \"hashes\" a lot lately, and we've been looking for a universal way to count the items contained in both arrays and hashes without having to \"know\" which we're dealing with except in the count method. As everyone knows .length is useless since it only returns the value of the highest index in the array. What we have below does not work because hashes test true for Array, but the length value returned is crap for hashes. We originally replaced .length all over our project with Object.keys().length, but this isn't supported in IE8 and lower.\n\nThis is such a stupid simple thing and we can't seem to get it working. Help me, Obi Wan. You're my only hope!\n\n```\nfunction isNullOrUndefined(aObject) {\n    \"use strict\";\n    return (typeof aObject === 'undefined' || aObject === null);\n}\n\nfunction count(aList) {\n    \"use strict\";\n    var lKey = null,\n        lResult = 0;\n    if (!isNullOrUndefined(aList)) {\n        if (aList.constructor == Array) {\n            lResult = aList.length;\n        } else if (!isNullOrUndefined(Object.keys)) {\n            lResult = Object.keys(aList).length;\n        } else {\n            for (lKey in aList) {\n                if (aList.hasOwnProperty(lKey)) {\n                    lResult++;\n                }\n            }\n        }\n    }\n    return lResult;\n}\n```\n\n    ", "Answer": "\r\n```\nObject.keys```\n polyfill copied verbatim from the ES5-shim\n\n```\n// ES5 15.2.3.14\n// http://es5.github.com/#x15.2.3.14\nif (!Object.keys) {\n    // http://whattheheadsaid.com/2010/10/a-safer-object-keys-compatibility-implementation\n    var hasDontEnumBug = true,\n        dontEnums = [\n            \"toString\",\n            \"toLocaleString\",\n            \"valueOf\",\n            \"hasOwnProperty\",\n            \"isPrototypeOf\",\n            \"propertyIsEnumerable\",\n            \"constructor\"\n        ],\n        dontEnumsLength = dontEnums.length;\n\n    for (var key in {\"toString\": null}) {\n        hasDontEnumBug = false;\n    }\n\n    Object.keys = function keys(object) {\n\n        if ((typeof object != \"object\" && typeof object != \"function\") || object === null) {\n            throw new TypeError(\"Object.keys called on a non-object\");\n        }\n\n        var keys = [];\n        for (var name in object) {\n            if (owns(object, name)) {\n                keys.push(name);\n            }\n        }\n\n        if (hasDontEnumBug) {\n            for (var i = 0, ii = dontEnumsLength; i < ii; i++) {\n                var dontEnum = dontEnums[i];\n                if (owns(object, dontEnum)) {\n                    keys.push(dontEnum);\n                }\n            }\n        }\n        return keys;\n    };\n\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "What is the structure of AppxSignature.p7x?\r\n                \r\nUniversal Windows apps are in .appx file, which is simply a zip of a bunch of files and metadata. Most of the metadata files are extensively documented on the Microsoft website and are trivial to parse and/or regenerate. However AppxSignature.p7x remains a mystery.\n\nFrom this diagram (source):\n\n\nAppxSignature.p7x should have hashes of the AppxBlockMap.xml, content & directory hashes, and a signature. However I cannot find any documentation of the AppxSignature.p7x file itself. Ideally I would like to use an alternative tool to produce and verify this signature, e.g. openssl/gnutls or similar. A practical use for this is to update and repackage apps on Linux, and prepare .appxupload file for the Windows Store.\n    ", "Answer": "\r\nAs described in the blog post you link to, the AppxBlockMap.xml file stores cryptographic block hashes for every file in the package. This file is verified and secured with a digital signature when the package is signed using authenticode.\n\nSo, on windows, you have two tools:\n\n\nMakeAppx.exe that creates the package (.zip format) and the blockmap file at the same time. This is important, as what's in the block map corresponds closely to the .zip file bits, you can't just any zipping tool for this step, you must program the zip/app package creation using some ZIP API.\nSignTool.exe that adds the signature to the package using \"standard\" authenticode.\n\n\nWith the Windows API you can do the same as MakeAppx using the\nPackaging API and you can do the same as SignTool using The SignerSign function.\n\nThe whole MakeAppx process is not documented IMHO, but the blockmap schema is in fact described here: Package block map schema reference which is relatively easy to understand.\n\nThe Authenticode signature for PE document is documented here: Windows Authenticode Portable Executable Signature Format\n\nBut it's only for PE (.dll, .exe, etc.) files (note it's also possible to sign .CAB files), and I don't think how SignerSign builds AppxSignature.p7x is documented. However, there is an open source tool here that does it here: https://github.com/facebook/fb-util-for-appx. You will notice this file https://github.com/facebook/fb-util-for-appx/blob/master/PrivateHeaders/APPX/Sign.h that declares what should be used as input for signing. I have no idea where they got that information. \n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How can I get GA utm parameters in a secondary page in user's session\r\n                \r\nI am using GA universal analytics. I want to get the ```\nutm_content```\n parameter of the user, that had come to my page with some. But I want it to get it in any page user visit in that session.\n\nI just thought about reading cookie, but the data is hashed and I understood that Google did not want that way. Some analysis on cookie\n\ncan I do it by any API method in Analytics.js or with any other solution?\n    ", "Answer": "\r\nI think you might be able to get use the tracker and call the get method.\nIt returns the value currently associated with the given field. This should be called on an instantiated tracker object.\n\n```\nga(function(tracker) {\n  car content = ga('get', 'campaignContent', 'content');\n});\n```\n\n\nThis function will execute once the analytics.js library has loaded. When the function executes, the tracker parameter will have a reference to the default tracker (assuming it was created prior to the function being called). The tracker reference is then used to get the campaignContent value.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to automatically generate salt for crypt method with blowfish\r\n                \r\nI have just started learning PHP and I want to create a website with a login for my final year university project. I've read that blowfish is the best method for hashing in a number of places like here: openssl_digest vs hash vs hash_hmac? Difference between SALT & HMAC? \n\nEverywhere I read about the crypt method includes a string like ```\n$2y$07$usesomesillystringforsalt$```\n My main question is: how do I randomly generate this? I've read in places that time stamps and mt_rand() are not secure.\n\nAlso I've heard AES is the preferred technology recently but from what I can see it seems pretty tricky to implement in PHP! Is blowfish still an acceptable method to secure stored passwords?\n    ", "Answer": "\r\nA salt should be unique (for each password) and unpredictable. These two criterias are a bit difficult to fulfill with a deterministic computer, so the best thing you can do is, to use the random source of the operating system, to generate the salt.\n\nTime stamps, as well as the ```\nmt_rand()```\n function, are not ideal, because one can argue that they are predictable. At least an attacker can narrow down (and therefore precalculate) the possible combinations for a certain period. While this may not have a big impact in practice, why not do the best you can?\n\nSince PHP 5.3 you can safely use the ```\nmcrypt_create_iv()```\n function to read from the random source, then you will have to encode the binary string to the allowed alphabet. This is a possible implementation.\n\nPHP 5.5 will have it's own functions ```\npassword_hash()```\n and ```\npassword_verify()```\n ready, to simplify this task. There is also a compatibility pack for PHP 5.3/5.4 available, downloadable at password_compat.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "django static files versioning\r\n                \r\nI'm working on some universal solution for problem with static files and updates in it.\n\nExample: let's say there was site with ```\n/static/styles.css```\n file - and site was used for a long time - so a lot of visitors cached this file in browser\n\nNow we doing changes in this css file, and update on server, but some users still have old version (despite modification date returned by server)\n\nThe obvious solution is to add some version to file ```\n/static/styles.css?v=1.1```\n but in this case developer must track changes in this file and manually increase version\n\nA second solution is to count the md5 hash of the file and add it to the url ```\n/static/styels.css/?v={mdp5hashvalue}```\n which looks much better, but md5 should be calculated automatically somehow.\n\nthey possible way I see it - create some template tag like this\n\n```\n{% static_file  \"style.css\" %}\n```\n\n\nwhich will render \n\n```\n<link src=\"/static/style.css?v=md5hash\">\n```\n\n\nBUT, I do not want this tag to calculate md5 on every page load, and I do not want to store hash in django-cache, because then we will have to clear after updating file...\n\nany thoughts ?\n    ", "Answer": "\r\nDjango 1.4 now includes ```\nCachedStaticFilesStorage```\n which does exactly what you need (well... almost).\n\nSince Django 2.2 ```\nManifestStaticFilesStorage```\n should be used instead of ```\nCachedStaticFilesStorage```\n.\n\nYou use it with the ```\nmanage.py collectstatic```\n task.  All static files are collected from your applications, as usual, but this storage manager also creates a copy of each file with the MD5 hash appended to the name.  So for example, say you have a ```\ncss/styles.css```\n file, it will also create something like ```\ncss/styles.55e7cbb9ba48.css```\n.\n\nOf course, as you mentioned, the problem is that you don't want your views and templates calculating the MD5 hash all the time to find out the appropriate URLs to generate.  The solution is caching.  Ok, you asked for a solution without caching, I'm sorry, that's why I said almost.  But there's no reason to reject caching, really.  ```\nCachedStaticFilesStorage```\n uses a specific cache named ```\nstaticfiles```\n.  By default, it will use your existing cache system, and voilà!  But if you don't want it to use your regular cache, perhaps because it's a distributed memcache and you want to avoid the overhead of network queries just to get static file names, then you can setup a specific RAM cache just for ```\nstaticfiles```\n.  It's easier than it sounds: check out this excellent blog post.  Here's what it would look like:\n\n```\nCACHES = {\n  'default': {\n    'BACKEND': 'django.core.cache.backends.memcached.PyLibMCCache',\n    'LOCATION': '127.0.0.1:11211',\n  },\n  'staticfiles': {\n    'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n    'LOCATION': 'staticfiles-filehashes'\n  }\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to store passwords offline\r\n                \r\nAlthough this is focused on Windows Phone 7, I guess the principle is universal. I would like to have a password protected zone within my app. However, my application is completely offline and so I will have to store credential details on the phone. My initial idea is to store a hash of the password and the salt. Would this be the best way to go? If so, should the hash and salt be stored in plain text, or is there a way to ensure that even they are encrypted? I understand that having the entire scheme on the phone will eventually be cracked, but what would be the best way to raise the barrier? thanks for any suggestions\n    ", "Answer": "\r\nPersonally, I would encrypt the passwords with a salt that is based on a unique ID of the device (and, if possible, some custom user input like a really short password [dog, cat, bob] - that kind of thing).\nJust a suggestion.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Checking if hash value has a text\r\n                \r\nI have a hash:\n\n```\nuniversityname = e.university\ntopuniversities = CSV.read('lib/assets/topuniversities.csv',{encoding: \"UTF-8\", headers:true, header_converters: :symbol, converters: :all})\nhashed_topuniversities = topuniversities.map {|d| d.to_hash}\n\nhashed_topuniversities.any? {|rank, name| name.split(' ').include?(universityname) }.each do |s|\n  if s[:universityrank] <= 10\n    new_score += 10\n  elsif s[:universityrank] >= 11 && s[:universityrank] <= 25\n    new_score += 5\n  elsif s[:universityrank] >= 26 && s[:universityrank] <= 50\n    new_score += 3\n  elsif s[:universityrank] >= 51 && s[:universityrank] <= 100\n    new_score += 2\n  end\n```\n\n\nBasically what this is doing is looking at a hash and checking if the hash value contains a university name is an input.\n\nFor example the user input can be \"Oxford University\" and in the hash its stored as \"Oxford\". The User needs to type in as it stored in the hash to be able to be assigned a score, But I want it that if the user types in \"oxford university\" then the hash value \"Oxford\" should be selected and then go through. \n\nEverything else in this works fine but the ```\n.include?```\n does not work correctly, I still need to type the exact word.\n    ", "Answer": "\r\n```\nhashed_topuniversities = topuniversities.map &:to_hash\n\nuniv = hashed_topuniversities.detect do |rank, name|\n  name.downcase.split(' ').include?(universityname.downcase)\nend\n\nnew_score += case univ[:universityrank]\n             when -Float::INFINITY..10 then 10\n             when 11..25 then 5\n             when 26..50 then 3\n             when 50..100 then 2\n             else 0\n             end\n```\n\n\nBesides some code improvements in terms of being more idiomatic ruby, the main change is ```\ndowncase```\n called on both university name and user input. Now they are compared case insensitive.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to hash a row in Python?\r\n                \r\nI am trying to implement the following algorithm in Python [1]:\n\n\n  The problem: (row compression) Let ```\nA```\n be an ```\nn x m```\n array of bounded degree d (i.e., each element of ```\nA```\n is an integer, ```\na```\n, such that ```\n0<a<d```\n.), and let ```\nk```\n be the number of distinct rows of ```\nA```\n. Find a ```\nk x m```\n array, ```\nA'```\n, such that each row of ```\nA```\n appears in ```\nA'```\n. \n  \n  \n  \n  The Method: (Hashing) Hash the rows of ```\nA```\n into a table, skipping duplicates and adding the rest to ```\nA'```\n. \n\n\nI do not understand how to hash a row in Python? \n\nWell, I would like to understand what does it mean by \"hash the rows of A into a table\". What I understand is the following. Suppose I have a matrix like this:\n\n```\nA = [[1, 2, 3, 4], \n     [1, 2, 3, 4],\n     [6, 7, 5, 4]]\n```\n\n\nSo, I hash its rows (somehow) and I get:\n\n```\nB = [x1, x2, x3]\n```\n\n\nwhere ```\nxi```\n is the hash of row ```\ni```\n. Here I will have ```\nx1=x2```\n since the row 1 and the row 2 are equal. Since I get ```\nx1=x2```\n, I will keep one and I finally have:\n\n```\nA' = [[1, 2, 3, 4], \n      [6, 7, 5, 4]]\n```\n\n\nAm I right? If so, how can I implement such an algorithm in Python?\n\nThanks.\n\n\n\n[1] D. Comer, \"Removing Duplicate Rows of a Bounded Degree Array Using Tries\", Purdue University, 1977 .\n    ", "Answer": "\r\nFirst of all, you need to remove the duplicated rows. For that aim, you can use ```\nset```\n but first you need to convert all your rows to immutable object types.\n\nYou can convert them to ```\ntuple```\n :\n\n```\n>>> A = [[1, 2, 3, 4], \n...      [1, 2, 3, 4],\n...      [6, 7, 5, 4]]\n>>> \n>>> map(tuple,A)\n[(1, 2, 3, 4), (1, 2, 3, 4), (6, 7, 5, 4)]\n```\n\n\nThen you can use ```\nset```\n. And as ```\nset```\n uses hash function, the result will be hashed automatically :\n\n```\n>>> set(map(tuple,A))\nset([(1, 2, 3, 4), (6, 7, 5, 4)])\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Why does Git store (and hash) blob size in the blob file?\r\n                \r\nGit's blob object file format is ```\nblob <size string>\\0<data>```\n.\nThe blob-identifying SHA-1 hash is calculated not from the blob contents alone, but from the header-augmented blob data (as described above).\n\nAs a purist I do not like that architecture. It mixes the universal property of the data (its SHA1 hash) with some git-specific header.\n\nAnother advantage of pure-data blob storage is that the files can be added to the index using \"copy-on-write\" instead of copying the whole file. The required space could be halved and some operations could become faster.\n\nSo, why did Git developers choose to use the header-based format instead of the pure data format? \n\nP.S. AFAIK in the early days of Git the SHA-1 hash was based on the compressed data.\n    ", "Answer": "\r\n\n  AFAIK in the early days of Git the SHA-1 hash was based on the compressed data.\n\n\nYes, and that lead to all kind of \"optimizations\" like commit 65c2e0c, git 0.99, June 2015: \n\n\n  Find size of SHA1 object without inflating everything. \n\n\nBut that new format, illustrated in \"How does git compute file hashes?\", can be traced back to:\n\n\n```\ngit diff```\n, in commit 051308f (git 1.4.0-rc1, May 2006)\n```\ngit fast-import```\n, started in commit db5e523 (git 1.5.0, Aug. 2006)\n\n\nEach time, the length of the data is needed to do anything with the data itself.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "fish shell --- how to simulate or implement a hash table, associative array, or key-value store\r\n                \r\nI am migrating from ```\nksh```\n to ```\nfish```\n.  I am finding that I miss the ability to define an associative array, hash table, dictionary, or whatever you wish to call it.  Some cases can be simulated as in\n\n```\n\nset dictionary$key $value\neval echo '$'dictionary$key\n```\n\n\nBut this approach is heavily limited; for example, ```\n$key```\n may contain only letters, numbers, and underscores.\n\nI understand that the ```\nfish```\n approach is to find an external command when one is available, but I am a little reluctant to store key-value information in the filesystem, even in ```\n/run/user/<uid>```\n, because that limits me to \"universal\" scope.\n\nHow do ```\nfish```\n programmers work around the lack of a key-value store?  Is there some simple approach that I am just missing?\n\nHere's an example of the sort of problem I would like to solve: I would like to modify the ```\nfish_prompt```\n function so that certain directories print not using ```\nprompt_pwd```\n but using special abbreviations.  I could certainly do this with a ```\nswitch```\n command, but I would much rather have a universal dictionary so I can just look up a directory and see if it has an abbreviation.  Then I could change the abbreviations using ```\nset```\n instead of having to edit a function.\n    ", "Answer": "\r\nYou can store the keys in one variable and values in the other, and then use something like\n\n```\nif set -l index (contains -i -- foo $keys) # `set` won't modify $status, so this succeeds if `contains` succeeds\n    echo $values[$index]\nend\n```\n\n\nto retrieve the corresponding value.\n\nOther possibilities include alternating between key and value in one variable, though iterating through this is a pain, especially when you try to do it only with builtins. Or you could use a separator character and store a key-value pair as one element, though this won't work for directories because variables cannot contain \\0 (which is the only possible separator for paths).\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Quasar Framework how to store login hash so that it works on web and mobile ( using cordova ) application\r\n                \r\nI am working on a web and mobile app using https://quasar.dev which is VueJs framework. My main dilema is how do I write the code so universal that for example the login functionality work on both web and mobile. For my backend I am using Laravel with Passport, so after successful login I get a hash. The problem is where do I store this hash ? \n\nI have read this page https://quasar.dev/quasar-cli/developing-ssr/ssr-frequently-asked-questions and it says that Local Store doesn't work while SSR. And they say you can import Cookie and it will be sent on every request. Then where do I store the hash on mobile ? and how do i deploy in on cordova? using ssr or without ?\n\nSo to sum up my questions:\n\n\nWhere do I store the hash ? Different places on web and mobile ?\nWhen deploying on ssr I run npm ```\nrun build --mode ssr```\n and the ```\nnpm run start```\n in the build-ssr folder and on mobile ```\nnpm run build```\n ? \n\n    ", "Answer": "\r\nYou can store it in the local storage or use vuex-persist https://www.npmjs.com/package/vuex-persist to make a store that is stored in the local storage. also local storage works on Cordova! https://cordova.apache.org/docs/en/latest/cordova/storage/storage.html\n\nTo the second question if you are using the Quasar Cli you should use the Cli commands to build for mobile and web. https://quasar.dev/quasar-cli/developing-ssr/build-commands#Building-for-Production  https://quasar.dev/quasar-cli/developing-cordova-apps/build-commands#Introduction\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "What does the html-webpack-plugin mean when it says \"hash in the filename\"?\r\n                \r\nI feel like the word hash gets used in so many different places and can mean different things....\n\n\nWhat is an html template with a hash in the file name?\n\n\n\n  This is a webpack plugin that simplifies creation of HTML files to\n  serve your webpack bundles. This is especially useful for webpack\n  bundles that include a hash in the filename which changes every\n  compilation. You can either let the plugin generate an HTML file for\n  you, supply your own template using lodash templates or use your own\n  loader.\n\n\nsource: https://github.com/ampedandwired/html-webpack-plugin\n\nAs a minor follow-up question, is there a universal definition of a \"hash\"? \n\nI've seen it used talking about cookies, URLs, authentication, in redis and data structures (e.g. hash tables)... is there any method to this madness or is it just a popular word that people decide to use when naming anything they do? I've tried to do some research on this myself but its widespread use in so many situations has made it hard to google exactly what I'm wondering about.\n    ", "Answer": "\r\nIn this instance, 'hash in the filename' means that the contents of the file are hashed, which generates a sorta-unique string of characters like a1b2c3 or whatever, and is then inserted into the output filename, like ```\nindex.a1b2c3.js```\n. It's useful in web development because that hash will change if and only if the contents of the file change, meaning you can cache the file forever - if it changes, it'll be at a different filename.\n\n'hash' just means to take any data of arbitrary length (contents of a file, string value, or whatever else) and construct a fixed-length, non-reversible, and almost-unique-but-not-really string value representation thereof. Without getting to far into a compsci lesson, it is very useful in various aspects of programming and is used consistently across all the instances you mentioned, as far as I know.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to delete a dynamic array of objects with pointers to other objects (C++)\r\n                \r\nLong story short, I have to code a hashtable using linear hashing in C++ for university. The hashtable works, but the ressources are not being freed which is an issue, especially that the unit test tests the table with 100k+ values and the garbage left behind is huge.\nBasically when I'm creating the new hashtable I am doing the following:\n\n```\nhashTable = new Bucket[this->tableSize];\n        for (size_t i = 0; i < tableSize; i++) {\n            hashTable[i] = * new Bucket();\n        }\n```\n\n\nEach Bucket can contain a pointer to another overflow bucket, which may or may not be set.\n\n```\n    class Bucket {\n    private:\n        size_t bucketSize;                                                                   \n        size_t elementsInBucket;                                                              \n        E v[N];   // int v[N];                                                              \n        bool hasOverflow;                                                                     \n        Bucket * nextBucket = nullptr; \n```\n\n\nMy question is, how can I delete the whole hashtable including the buckets with their potential overflow buckets as the following only frees half of the occupied memory.\n\n```\ndelete[] hashTable;\nhashTable = nullptr;\n```\n\n\nThanks!\n    ", "Answer": "\r\nYou are immediately leaking one ```\nBucket```\n on this line:\n\n```\nhashTable[i] = * new Bucket();\n```\n\n\nWhat this does is:\n\n\nAllocate a new ```\nBucket```\n and return a pointer to it\nDereference the returned pointer and pass a reference to your new ```\nBucket```\n to ```\nBucket::operator=```\n\nCopy the empty ```\nBucket```\n into the already existing object ```\nhashTable[i]```\n\nDiscard the pointer to your newly allocated ```\nBucket```\n, thus leaking it\n\n\n```\nhashTable```\n is a pointer to the first element of an array of ```\ntableSize```\n ```\nBucket```\ns.  Those objects already exist, so you don't need to allocate new ones in your loop.\n\nAdditionally, you've not shown a destructor for ```\nBucket```\n, or shown how you allocate ```\nBucket::nextBucket```\n.  I'm assuming each ```\nBucket```\n should own its ```\nnextBucket```\n, so you should have a destructor that does something like\n\n```\nBucket::~Bucket()\n{\n    delete nextBucket;\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Angular 6 Server Side Errror: Module not found: Error: Can't resolve './dist/server/main.bundle'\r\n                \r\nI'm working on a project, updating it to Angular 6. After the update, I'm now getting this error when trying to run the server-side rendering build \n\n```\nModule not found: Error: Can't resolve './dist/server/main.bundle'\n```\n\n\nI tried going to https://angular.io/guide/universal and matching up my code to the SSR files in Angular Universal. This did not work for me. \n\nIt seems that the dist folder is not generating a /server folder but is creating a /browser. Im unsure why. \n\n\n\nHere's my angular.json file \n\n```\n{\n \"$schema\": \"./node_modules/@angular/cli/lib/config/schema.json\",\n  \"version\": 1,\n  \"newProjectRoot\": \"projects\",\n  \"projects\": {\n    \"xilo\": {\n      \"root\": \"\",\n      \"sourceRoot\": \"src\",\n      \"projectType\": \"application\",\n      \"architect\": {\n        \"build\": {\n          \"builder\": \"@angular-devkit/build-angular:browser\",\n          \"options\": {\n            \"outputPath\": \"dist/browser\",\n            \"index\": \"src/index.html\",\n            \"main\": \"src/main.ts\",\n            \"tsConfig\": \"src/tsconfig.app.json\",\n            \"polyfills\": \"src/polyfills.ts\",\n            \"assets\": [\n              \"src/assets\",\n              \"src/favicon.ico\"\n            ],\n            \"styles\": [\n              \"src/styles.css\",\n              \"node_modules/font-awesome/css/font-awesome.min.css\"\n            ],\n            \"scripts\": [\n              \"node_modules/moment/min/moment.min.js\"\n            ]\n          },\n          \"configurations\": {\n            \"production\": {\n              \"optimization\": true,\n              \"outputHashing\": \"all\",\n              \"sourceMap\": false,\n              \"extractCss\": true,\n              \"namedChunks\": false,\n              \"aot\": true,\n              \"extractLicenses\": true,\n              \"vendorChunk\": false,\n              \"buildOptimizer\": true,\n              \"fileReplacements\": [\n                {\n                  \"replace\": \"src/environments/environment.ts\",\n                  \"with\": \"src/environments/environment.prod.ts\"\n                }\n              ]\n            }\n          }\n        },\n        \"serve\": {\n          \"builder\": \"@angular-devkit/build-angular:dev-server\",\n          \"options\": {\n            \"browserTarget\": \"xilo:build\"\n          },\n          \"configurations\": {\n            \"production\": {\n              \"browserTarget\": \"xilo:build:production\"\n            }\n          }\n        },\n        \"extract-i18n\": {\n          \"builder\": \"@angular-devkit/build-angular:extract-i18n\",\n          \"options\": {\n            \"browserTarget\": \"xilo:build\"\n          }\n        },\n        \"test\": {\n          \"builder\": \"@angular-devkit/build-angular:karma\",\n          \"options\": {\n            \"main\": \"src/test.ts\",\n            \"karmaConfig\": \"./karma.conf.js\",\n            \"polyfills\": \"src/polyfills.ts\",\n            \"tsConfig\": \"src/tsconfig.spec.json\",\n            \"scripts\": [\n              \"node_modules/moment/min/moment.min.js\"\n            ],\n            \"styles\": [\n              \"src/styles.css\",\n              \"node_modules/font-awesome/css/font-awesome.min.css\"\n            ],\n            \"assets\": [\n              \"src/assets\",\n              \"src/favicon.ico\"\n            ]\n          }\n        },\n        \"lint\": {\n          \"builder\": \"@angular-devkit/build-angular:tslint\",\n          \"options\": {\n            \"tsConfig\": [\n              \"src/tsconfig.app.json\",\n              \"src/tsconfig.spec.json\"\n            ],\n            \"exclude\": [\n              \"**/node_modules/**\"\n            ]\n          }\n        },\n        \"server\": {\n          \"builder\": \"@angular-devkit/build-angular:server\",\n          \"options\": {\n            \"outputPath\": \"dist/server\",\n            \"main\": \"main.server.ts\",\n            \"tsConfig\": \"tsconfig.server.json\"\n          }\n        }\n      }\n    },\n    \"xilo-e2e\": {\n      \"root\": \"\",\n      \"sourceRoot\": \"e2e\",\n      \"projectType\": \"application\",\n      \"architect\": {\n        \"e2e\": {\n          \"builder\": \"@angular-devkit/build-angular:protractor\",\n          \"options\": {\n            \"protractorConfig\": \"./protractor.conf.js\",\n            \"devServerTarget\": \"xilo:serve\"\n          }\n        },\n        \"lint\": {\n          \"builder\": \"@angular-devkit/build-angular:tslint\",\n          \"options\": {\n            \"tsConfig\": [\n              \"e2e/tsconfig.e2e.json\"\n            ],\n            \"exclude\": [\n              \"**/node_modules/**\"\n            ]\n          }\n        }\n      }\n    }\n  },\n  \"defaultProject\": \"xilo\",\n  \"schematics\": {\n    \"@schematics/angular:class\": {\n      \"spec\": false\n    },\n    \"@schematics/angular:component\": {\n      \"spec\": false,\n      \"inlineStyle\": true,\n      \"inlineTemplate\": true,\n      \"prefix\": \"app\",\n      \"styleext\": \"css\"\n    },\n    \"@schematics/angular:directive\": {\n      \"spec\": false,\n      \"prefix\": \"app\"\n    },\n    \"@schematics/angular:guard\": {\n      \"spec\": false\n    },\n    \"@schematics/angular:module\": {\n      \"spec\": false\n    },\n    \"@schematics/angular:pipe\": {\n      \"spec\": false\n    },\n    \"@schematics/angular:service\": {\n      \"spec\": false\n    }\n  }\n}\n```\n\n\nserver.ts\n\n```\n// These are important and needed before anything else\nimport 'zone.js/dist/zone-node';\nimport 'reflect-metadata';\n\nimport { enableProdMode } from '@angular/core';\n\nimport * as express from 'express';\nimport { join } from 'path';\n\n// Faster server renders w/ Prod mode (dev mode never needed)\nenableProdMode();\n\n// Express server\nconst app = express();\n\nconst PORT = process.env.PORT || 4000;\nconst DIST_FOLDER = join(process.cwd(), 'dist');\n\n// * NOTE :: leave this as require() since this file is built Dynamically from webpack\nconst { AppServerModuleNgFactory, LAZY_MODULE_MAP } = require('./dist/server/main.bundle');\n\n// Express Engine\nimport { ngExpressEngine } from '@nguniversal/express-engine';\n// Import module map for lazy loading\nimport { provideModuleMap } from '@nguniversal/module-map-ngfactory-loader';\n\napp.engine('html', ngExpressEngine({\n  bootstrap: AppServerModuleNgFactory,\n  providers: [\n    provideModuleMap(LAZY_MODULE_MAP)\n  ]\n}));\n\napp.set('view engine', 'html');\napp.set('views', join(DIST_FOLDER, 'browser'));\n\n// TODO: implement data requests securely\napp.get('/api/*', (req, res) => {\n  res.status(404).send('data requests are not supported');\n});\n\n// Server static files from /browser\napp.get('*.*', express.static(join(DIST_FOLDER, 'browser')));\n\n// All regular routes use the Universal engine\napp.get('*', (req, res) => {\n  res.render('index', { req });\n});\n\n// Start up the Node server\napp.listen(PORT, () => {\n  console.log(`Node server listening on http://localhost:${PORT}`);\n});\n```\n\n\nEverything is set up exactly to the Angular Universal directions. \n\nAnyone know why my build script would not be generating a /server folder?\n\n```\n\"build:ssr\": \"npm run build:client-and-server-bundles && npm run webpack:server\"\n```\n\n\npackage.json\n\n```\n{\n  \"name\": \"XXX\",\n  \"version\": \"XXX\",\n  \"license\": \"MIT\",\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"XXXXX\"\n  },\n  \"engines\": {\n    \"node\": \"10.7.0\",\n    \"npm\": \"6.1.0\"\n  },\n  \"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"node dist/server\",\n    \"build\": \"ng build\",\n    \"build:client-and-server-bundles\": \"ng build --prod && ng build --prod --project xilo --output-hashing=all\",\n    \"build:prerender\": \"npm run build:client-and-server-bundles && npm run webpack:server && npm run generate:prerender\",\n    \"build:ssr\": \"npm run build:client-and-server-bundles && npm run webpack:server\",\n    \"deploy\": \"git push origin master && git push heroku master\",\n    \"generate:prerender\": \"cd dist && node prerender\",\n    \"postinstall\": \"npm run build:ssr\",\n    \"webpack:server\": \"webpack --config webpack.server.config.js --progress --colors\",\n    \"serve:prerender\": \"cd dist/browser && http-server\",\n    \"serve:ssr\": \"node dist/server\"\n  },\n  \"private\": true,\n  \"dependencies\": {\n    \"@angular/animations\": \"6.1.0\",\n    \"@angular/common\": \"6.1.0\",\n    \"@angular/compiler\": \"6.1.0\",\n    \"@angular/compiler-cli\": \"6.1.0\",\n    \"@angular/core\": \"^6.1.0\",\n    \"@angular/forms\": \"6.1.0\",\n    \"@angular/http\": \"6.1.0\",\n    \"@angular/language-service\": \"6.1.0\",\n    \"@angular/platform-browser\": \"6.1.0\",\n    \"@angular/platform-browser-dynamic\": \"6.1.0\",\n    \"@angular/platform-server\": \"6.1.0\",\n    \"@angular/router\": \"6.1.0\",\n    \"@nguniversal/express-engine\": \"^5.0.0-beta.5\",\n    \"@nguniversal/module-map-ngfactory-loader\": \"^5.0.0-beta.5\",\n    \"@nicky-lenaers/ngx-scroll-to\": \"^0.5.0\",\n    \"@types/moment\": \"^2.13.0\",\n    \"@types/node\": \"^8.10.21\",\n    \"angular-file\": \"^0.4.1\",\n    \"angular2-moment\": \"^1.9.0\",\n    \"classlist.js\": \"^1.1.20150312\",\n    \"core-js\": \"^2.5.7\",\n    \"cpy-cli\": \"^1.0.1\",\n    \"express\": \"^4.16.3\",\n    \"font-awesome\": \"^4.7.0\",\n    \"http-server\": \"^0.10.0\",\n    \"moment\": \"^2.22.2\",\n    \"ng-circle-progress\": \"^0.9.9\",\n    \"ng-hotjar\": \"0.0.13\",\n    \"ng-intercom\": \"^1.0.0-beta.5-2\",\n    \"ng2-google-charts\": \"^3.4.0\",\n    \"ng4-geoautocomplete\": \"^0.1.0\",\n    \"ngx-filter-pipe\": \"^1.0.2\",\n    \"ngx-loading\": \"^1.0.14\",\n    \"ngx-pagination\": \"^3.0.3\",\n    \"reflect-metadata\": \"^0.1.10\",\n    \"rxjs\": \"^6.2.2\",\n    \"rxjs-compat\": \"^6.2.2\",\n    \"ts-loader\": \"^4.4.2\",\n    \"typescript\": \"2.9.2\",\n    \"web-animations-js\": \"^2.3.1\",\n    \"zone.js\": \"^0.8.20\"\n  },\n  \"devDependencies\": {\n    \"@angular-devkit/build-angular\": \"~0.6.8\",\n    \"@angular/cli\": \"^6.0.8\",\n    \"webpack-cli\": \"^3.1.0\"\n  }\n}\n```\n\n    ", "Answer": "\r\nError it self say's what is missing, the SSR build requires the main.bundle.js\nSo update the commands in ```\npackage.json```\n and run the commands to build the main.bundle.js\n\n```\n\"build:client-and-server-bundles\": \"ng build --prod && ng run defaultProject:server && npm run webpack:server\"\n```\n\n\n```\ndefaultProject```\n name can be found from angular.json file.\nthen run \n\n```\n\"serve:ssr\": \"node dist/careercontroller/server/main.bundle.js\"\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Sign APK same as AAB\r\n                \r\nHow can I sign my APK same as AAB (android app bundle)?\n\nI have 2 problem\n\n1- I want to use sms-retriever and want has same hash string for both APK and AAB\n2- I upload my add in google play and unofficial android app stores , In google pplay I upload AAB file , and as the unofficial store cannot support AAB I upload universal APK  , the problem is If someone download app from play store , they cannot update it form unofficial android app stores and vise versa , because the sign is different\n    ", "Answer": "\r\nYou can check whether your app are using ```\nApp Signing```\n by accessing Release management > App signing. If you opted in, you can not disable it.\n\n```\nNote: Opting in to app signing by Google Play applies for the lifetime of your app\n```\n\n\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Sign APK same as AAB\r\n                \r\nHow can I sign my APK same as AAB (android app bundle)?\n\nI have 2 problem\n\n1- I want to use sms-retriever and want has same hash string for both APK and AAB\n2- I upload my add in google play and unofficial android app stores , In google pplay I upload AAB file , and as the unofficial store cannot support AAB I upload universal APK  , the problem is If someone download app from play store , they cannot update it form unofficial android app stores and vise versa , because the sign is different\n    ", "Answer": "\r\nYou can check whether your app are using ```\nApp Signing```\n by accessing Release management > App signing. If you opted in, you can not disable it.\n\n```\nNote: Opting in to app signing by Google Play applies for the lifetime of your app\n```\n\n\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "What stops impersonation of a certificate issuer on the blockchain?\r\n                \r\nVerifiable credentials are a big upside for using blockchain. However I am a little confused how this works. If a university gives a verifiable degree to student X, sure the student can share the hashID or degree ID with a potential employer. But what stops someone from pretending to be the university and issuing an identical degree with a different hash ID, which can be shared with a potential employer. In other words, what part of the degree establishes the \"actual\" university signed this degree, and not someone pretending to be the university?\nI am new to blockchain so I am still looking to understand the basics. Each approach i have explored only matches the hash with the issuer key. The problem is even if I am \"pretending to be the university\", my hash will match with my key. How do the people viewing the degree know if the degree was issued by address \"abcd....\", that this address belongs to the university?\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to implement angular server side rendering in laravel?\r\n                \r\nI created a single page application with angular 5. I use this application with laravel backend. In the laravel project I have an angular folder, this folder store all the angular files. I built the angular application outside the angular folder, in a public folder in the root laravel's project folder. I set the web.php file, so if I go to homestead.test url the index.html will loaded from the public folder. After I set up the serverside rendering with stories universal rendering, I built the client application and the server bundle. After I did this, and go to the homestead.test in the browser a I saw that my application doesn't look like before. With normal ng build (I'm hungarian by the way, so I used hungarian words):\n\nAfter i did the universan rendering builds (ng build --prod --extract-css, ng build --prod --extract-css --app 1 --output-hashing=false, I used the --extract-css, because I used scss in the angular and when i didn't used it i got a lot of warnings):\n\nAnd when I looked at the page source, it looked like the original angular source, but in the express server the source shows the real content.\nSo my question is how to set up laravel to act like the express server? The ng build --prod --extract-css create a lot of js file, the assets folder and some other files, but the ng build only create some of them, without the assets folder. So how can I import all the assets folder and the js files in laravel? The stories universal rendering create a server.js file too from Node server.ts. I don't know this file is necessary in laravel.\n\nI also got some addicitonal questions:\nIs it the best way to connect laravel to an angular application? If yes, can I host it with real web host?\nIs there any free web host to test my application?\nIf I succesfully display the angular app in laravel like I did with express server, than refresh the page in any url will works? Because now I got an error message.\n\nI created this application as a thesis app for my university and I've neved host app in the web, I only used local hosts.\n    ", "Answer": "\r\nThe idea of Angular Universal is to run the javascript on the server side to generate the page on the server side. Of course that is easy with Node (Express JS) and hence that is there already.\n\nRegarding Angular Universal on PHP, you can follow this Github issue. What I understand is, at least today, there is no support. But it's true that many people want to have the support and probably the support will be there in future.\n\nFor the time being, you have the following options:\n\n\nYou may keep doing thing the way you do now and you can host the app in any hosting platform. Although, you should probably improve your build mechanism to remove the manual intervention of copying angular files within Laraval. Your build mechanism should do all this copying things. The downside of the approach is that you won't have the benefits of server side rendering that Angular Universal provides. But if you don't care about those benefits like page loading speed and SEO, you are good.\nThe other option is to host your Angular part in the Node (Express JS) and route all API calls to Laraval. Thus you can get the benefit of Angular Universal. It means that you seend to run at lease two processes - one for the API and the other for serving the UI.\n\n\nBoth the options are used depending on the requirement. For a larger microservices based architecture the second approach is preferred as it makes the lifecycle of the UI part and the API part independent.\n\nHowever, for smaller projects where the user base is less and you don't really care about SEO, the first option makes sense.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Mapping Unique 16-Digit numeric ID to Unique Alphanumeric ID\r\n                \r\nIn a project I'm working on, I need to generate 16 character long unique IDs, consisting of 10 numbers plus 26 uppercase letters (only uppercase). They must be guaranteed to be universally unique, with zero chance of a repeat ever. \n\nThe IDs are not stored forever. An ID is thrown out of the database after a period of time and a new unique ID must be generated. The IDs can never repeat with the thrown out ones either. \n\nSo randomly generating 16 digits and checking against a list of previously generated IDs is not an option because there is no comprehensive list of previous IDs. Also, ```\nUUID```\n will not work because the IDs must be 16 digits in length.\n\nRight now I'm using 16-Digit Unique IDs, that are guaranteed to be universally unique every time they're generated (I'm using ```\ntimestamps```\n to generate them plus unique server ID). However, I need the IDs to be difficult to predict, and using ```\ntimestamps```\n makes them easy to predict. \n\nSo what I need to do is map the 16 digit numeric IDs that I have into the larger range of 10 digits + 26 letters without losing uniqueness. I need some sort of hashing function that maps from a smaller range to a larger range, guaranteeing a one-to-one mapping so that the unique IDs are guaranteed to stay unique after being mapped. \n\nI have searched and so far have not found any hashing or mapping functions that are guaranteed to be collision-free, but one must exist if I'm mapping to a larger space. Any suggestions are appreciated.\n    ", "Answer": "\r\nBrandon Staggs wrote a good article on Implementing a Partial Serial Number Verification System. The examples are written in Delphi, but could be converted to other languages.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Heroku nodejs Procfile never picked up\r\n                \r\nIt seems that my nodejs application is never deploying on Heroku. While my build successfully completes with the following message:\n\n```\n[154] ./server.ts 2.1 kB {0} [built]\n[238] ./src 160 bytes {0} [built]\n[244] (webpack)/buildin/module.js 517 bytes {0} [built]\n[259] ./dist/server/main.bundle.js 76.1 kB {0} [built]\n+ 301 hidden modules\nNode server listening on http://localhost:5000\n```\n\n\nI'm only just seeing Heroku's Blank app: \n\n\n\nOther symptoms:\n\nMy builds never end:\n\n\nAnd no dynos are being instantiated:\n\n\nThis is despite the fact that I'm providing a ```\nProcfile```\n, like so:\n\n```\nweb: npm start\n```\n\n\nSee my ```\npackage.json```\n:\n\n```\n{\n  \"name\": \"vivonslagrasse-app\",\n  \"version\": \"0.0.0\",\n  \"license\": \"MIT\",\n  \"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"node dist/server.js\",\n    \"build\": \"ng build --prod\",\n    \"test\": \"ng test\",\n    \"lint\": \"ng lint\",\n    \"e2e\": \"ng e2e\",\n    \"universal\": \"ng build --prod --aot && ng build --prod --app ssr --output-hashing=false && webpack --config webpack.server.config.js --progress --colors && node dist/server.js\",\n    \"postinstall\": \"npm run universal\"\n  },\n  \"private\": true,\n  \"dependencies\": {\n    \"@angular/animations\": \"^5.2.0\",\n    \"@angular/common\": \"^5.2.0\",\n    \"@angular/compiler\": \"^5.2.0\",\n    \"@angular/core\": \"^5.2.0\",\n    \"@angular/forms\": \"^5.2.0\",\n    \"@angular/http\": \"^5.2.0\",\n    \"@angular/platform-browser\": \"^5.2.0\",\n    \"@angular/platform-browser-dynamic\": \"^5.2.0\",\n    \"@angular/platform-server\": \"^5.2.9\",\n    \"@angular/router\": \"^5.2.0\",\n    \"@nguniversal/module-map-ngfactory-loader\": \"^5.0.0-beta.8\",\n    \"@ngx-translate/core\": \"^9.1.1\",\n    \"@ngx-translate/http-loader\": \"^2.0.1\",\n    \"@nicky-lenaers/ngx-scroll-to\": \"^0.6.0\",\n    \"ngx-cookie\": \"^2.0.1\",\n    \"core-js\": \"^2.4.1\",\n    \"foundation-sites\": \"^6.4.4-rc1\",\n    \"rxjs\": \"^5.5.6\",\n    \"ts-loader\": \"^3.5.0\",\n    \"xmlhttprequest\": \"^1.8.0\",\n    \"zone.js\": \"^0.8.19\"\n  },\n  \"devDependencies\": {\n    \"@angular/cli\": \"~1.7.0\",\n    \"@angular/compiler-cli\": \"^5.2.0\",\n    \"@angular/language-service\": \"^5.2.0\",\n    \"@types/jasmine\": \"~2.8.3\",\n    \"@types/jasminewd2\": \"~2.0.2\",\n    \"@types/jquery\": \"^3.3.1\",\n    \"@types/node\": \"~6.0.60\",\n    \"codelyzer\": \"^4.0.1\",\n    \"jasmine-core\": \"~2.8.0\",\n    \"jasmine-spec-reporter\": \"~4.2.1\",\n    \"karma\": \"~2.0.0\",\n    \"karma-chrome-launcher\": \"~2.2.0\",\n    \"karma-coverage-istanbul-reporter\": \"^1.2.1\",\n    \"karma-jasmine\": \"~1.1.0\",\n    \"karma-jasmine-html-reporter\": \"^0.2.2\",\n    \"protractor\": \"~5.1.2\",\n    \"ts-node\": \"~4.1.0\",\n    \"tslint\": \"~5.9.1\",\n    \"typescript\": \"~2.5.3\"\n  },\n  \"engines\": {\n    \"node\": \"7.7.4\",\n    \"npm\": \"5.6.0\"\n  }\n}\n```\n\n\nNote that I need to run a node server since this is an angular universal app and the initial page's contents need to be server-generated.\n\nEDIT: I'm adding my ```\nserver.ts```\n file (the generated ```\nserver.js```\n is too long to be reproduced here)\n\n```\n// These are important and needed before anything else\nimport 'zone.js/dist/zone-node';\nimport 'reflect-metadata';\nimport { renderModuleFactory } from '@angular/platform-server';\nimport { enableProdMode } from '@angular/core';\n\nimport * as express from 'express';\nimport { join } from 'path';\nimport { readFileSync } from 'fs';\n\n(global as any).XMLHttpRequest = require('xmlhttprequest').XMLHttpRequest;\n\n\n// Faster server renders w/ Prod mode (dev mode never needed)\nenableProdMode();\n\n// Express server\nconst app = express();\n\nconst PORT = process.env.PORT || 5000;\nconst DIST_FOLDER = join(process.cwd(), 'dist');\n\n// Our index.html we'll use as our template\nconst template = readFileSync(join(DIST_FOLDER, 'browser', 'index.html')).toString();\n\n// * NOTE :: leave this as require() since this file is built Dynamically from webpack\nconst {AppServerModuleNgFactory, LAZY_MODULE_MAP} = require('./dist/server/main.bundle');\n\nconst {provideModuleMap} = require('@nguniversal/module-map-ngfactory-loader');\n\napp.engine('html', (_, options, callback) => {\n  renderModuleFactory(AppServerModuleNgFactory, {\n    // Our index.html\n    document: template,\n    url: options.req.url,\n    // DI so that we can get lazy-loading to work differently (since we need it to just instantly render it)\n    extraProviders: [\n      provideModuleMap(LAZY_MODULE_MAP)\n    ]\n  }).then(html => {\n    callback(null, html);\n  });\n});\n\napp.set('view engine', 'html');\napp.set('views', join(DIST_FOLDER, 'browser'));\n\n// Server static files from /browser\napp.get('*.*', express.static(join(DIST_FOLDER, 'browser')));\n\n// All regular routes use the Universal engine\napp.get('*', (req, res) => {\n  res.render(join(DIST_FOLDER, 'browser', 'index.html'), {req});\n});\n\n// Start up the Node server\napp.listen(PORT, () => {\n  console.log(`Node server listening on http://localhost:${PORT}`);\n});\n```\n\n\nNeedless to say, this setup works perfectly on my development machine...\n\nYour help will be highly appreciated. I already spent 2,5 hours on this.\n    ", "Answer": "\r\nUse process.env.port in server.js to listen\n\n```\nconst port = process.env.port || 5000;\napp.listen(port);\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "SOLVED: Hash content access is inconsistent with different perl version\r\n                \r\nI came across an interesting problem with following piece of code in perl 5.22.1 and perl 5.30.0\n\n```\nuse strict;\nuse warnings;\nuse feature 'say';\n\n#use Data::Dumper;\n\nmy %hash;\nmy %seen;\nmy @header = split ',', <DATA>;\n\nchomp @header;\n\nwhile(<DATA>) {\n    next if /^\\s*$/;\n    chomp;\n    my %data;\n    @data{@header} = split ',';\n\n    push @{$hash{person}}, \\%data;\n    push @{$hash{Position}{$data{Position}}}, \"$data{First} $data{Last}\";\n    if( ! $seen{$data{Position}} ) {\n        $seen{$data{Position}} = 1;\n        push @{$hash{Role}}, $data{Position};\n    }\n}\n\n#say Dumper($hash{Position});\n\nmy $count = 0;\nfor my $person ( @{$hash{person}} ) {\n    say \"Person: $count\";\n    say \"Role: $person->{Position}\";\n}\n\nsay \"---- Groups ----\\n\";\n\nwhile( my($p,$m) = each %{$hash{Position}} ) {\n    say \"-> $p\";\n    my $members = join(',',@{$m});\n    say \"-> Members: $members\\n\";\n}\n\nsay \"---- Roles ----\";\n\nsay '-> ' . join(', ',@{$hash{Role}});\n\n__DATA__\nFirst,Last,Position\nJohn,Doe,Developer\nMary,Fox,Manager\nAnna,Gulaby,Developer\n```\n\n\nIf the code run as it is -- everything works fine\n\nNow it is sufficient to add ```\n$count++```\n increment as bellow and code produces errors\n\n```\nmy $count = 0;\nfor my $person ( @{$hash{person}} ) {\n    $count++;\n    say \"Person: $count\";\n    say \"Role: $person->{Position}\";\n}\n```\n\n\nErrors:\n\n```\nError(s), warning(s):\nUse of uninitialized value $data{\"Position\"} in hash element at source_file.pl line 22, <DATA> line 2.\nUse of uninitialized value $data{\"Position\"} in hash element at source_file.pl line 23, <DATA> line 2.\nUse of uninitialized value $data{\"Position\"} in hash element at source_file.pl line 24, <DATA> line 2.\nUse of uninitialized value $data{\"Position\"} in hash element at source_file.pl line 22, <DATA> line 3.\nUse of uninitialized value $data{\"Position\"} in hash element at source_file.pl line 23, <DATA> line 3.\nUse of uninitialized value $data{\"Position\"} in hash element at source_file.pl line 22, <DATA> line 4.\nUse of uninitialized value $data{\"Position\"} in hash element at source_file.pl line 23, <DATA> line 4.\nUse of uninitialized value in concatenation (.) or string at source_file.pl line 35, <DATA> line 4.\nUse of uninitialized value in concatenation (.) or string at source_file.pl line 35, <DATA> line 4.\nUse of uninitialized value in concatenation (.) or string at source_file.pl line 35, <DATA> line 4.\nUse of uninitialized value in join or string at source_file.pl line 48, <DATA> line 4.\n```\n\n\nThis problem does not manifest itself in perl 5.30.0 (Windows 10, Strawberry Perl) or Perl v5.24.2.\n\nNote: the problem manifests itself not only with ```\n$count++```\n but with any other access to content of the hash next to ```\nsay \"Person: $count\";```\n -- post# 60653651\n\nI would like to hear comments on this situation, what is the cause?\n\nCAUSE: input data have eol in DOS form ```\n\\r\\n```\n and when data processed in Linux ```\nchomp```\n removes only ```\n\\n```\n leaving ```\n\\r```\n as part of the field name (used as hash key). Thanks goes to Shawn for pointing out the source of the issue. \n\nSOLUTION: universal fix was implemented in form of ```\nsnip_eol($arg)```\n subroutine\n\n```\nuse strict;\nuse warnings;\nuse feature 'say';\n\nmy $debug = 0;\n\nsay \"\nPerl:  $^V\nOS: $^O\n-------------------\n\" if $debug;\n\nmy %hash;\nmy %seen;\nmy @header = split ',', <DATA>;\n\n$header[2] = snip_eol($header[2]);        # problem fix\n\nwhile(<DATA>) {\n    next if /^\\s*$/;\n\n    my $line = snip_eol($_);              # problem fix\n\n    my %data;\n    @data{@header} = split ',',$line;\n\n    push @{$hash{person}}, \\%data;\n    push @{$hash{Position}{$data{Position}}}, \"$data{First} $data{Last}\";\n    if( ! $seen{$data{Position}} ) {\n        $seen{$data{Position}} = 1;\n        push @{$hash{Role}}, $data{Position};\n    }\n}\n\n#say Dumper($hash{Position});\n\nmy $count = 0;\nfor my $person ( @{$hash{person}} ) {\n    $count++;\n    say \"-> Name:   $person->{First} $person->{Last}\";\n    say \"-> Role:   $person->{Position}\\n\";\n}\n\nsay \"---- Groups ----\\n\";\n\nwhile( my($p,$m) = each %{$hash{Position}} ) {\n    say \"-> $p\";\n    my $members = join(',',@{$m});\n    say \"-> Members: $members\\n\";\n}\n\nsay \"---- Roles ----\";\n\nsay '-> ' . join(', ',@{$hash{Role}});\n\nsub snip_eol {\n    my $data = shift;                      # problem fix\n\n    #map{ say \"$_ => \" . ord } split '', $data if $debug;\n    $data =~ s/\\r// if $^O eq 'linux';\n    chomp $data;\n    #map{ say \"$_ => \" . ord } split '', $data if $debug;\n\n    return $data;\n}\n\n__DATA__\nFirst,Last,Position\nJohn,Doe,Developer\nMary,Fox,Manager\nAnna,Gulaby,Developer\n```\n\n    ", "Answer": "\r\nI can replicate this behavior by (On linux) first converting the source file to have Windows-style ```\n\\r\\n```\n line endings and then trying to run it. I thus suspect that in your testing of various versions you're using Windows sometimes, and a Linux/Unix other times, and not converting the file's line endings appropriately.\n\n```\n@chomp```\n only removes a newline character (Well, the current value of ```\n$/```\n to be pedantic), so when used on a string with a Windows style line ending in it, it leaves the carriage return. The hash key is not ```\n\"Position\"```\n, it's ```\n\"Position\\r\"```\n, which is not what the rest of your code uses.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "SSMS and Active Directory Authentication doesn't work with Azure SQL and non-Azure SQL\r\n                \r\nSo we have Azure AD synced with our on-premise domain.  We have an Azure SQL Server configured with the active directory admin.  We also have a non Azure SQL server running on a virtual machine in Azure that is domain joined to this same domain.  The following are the results of using the various authentication mechanisms offered by SQL Management Studio (SSMS).  Can someone explain why the failures occur with the various options that should be supported?\n\nFacts about environment:\n\n\nPassthrough authentication is the sign-in method configured on AD Connect\nPassword hash sync is also enabled so password hashes are stored in Azure AD\nAzure SQL is configured with Active Directory admin\nLatest version of SSMS was downloaded when performing these test\nOn premise account was used to test all scenarios\n\n\nDomain joined client connecting to Azure SQL from SSMS\n\n\nActive Directory Password (PASS)\nActive Directory Universal (PASS)\nWindows Integrated (FAIL - not supported by Azure SQL)\nActive Directory Integrated (FAIL – see error below)\n\n\nFailure when client is standard domain joined client\n\n\nFailure when client is Azure domain joined client\n\n\nDomain joined client connecting to non-Azure SQL hosted on same domain\n\n\nActive Directory Integrated (PASS)\nWindows Integrated (PASS)\nActive Directory Password (FAIL – Login failed for user ‘’)\nActive Directory Universal (FAIL – Login failed for user ’’)\n\n\n\n    ", "Answer": "\r\nThis issue may be related with the AD Syncing options. Verify in your environment that AD is not syncing passwords into the tenant. This prevents AD Integration Authentication and AD Password Authentication. The only authentication that works on these cases is AD Universal Authentication, as your test shows.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Export users and passwords from Active directory into PHP\r\n                \r\nI am developing a web application for my university project this summer and I am looking for some advice, The web application is to be deployed in to a windows environment that has around 500 users. The users will need to login before they can access the web application, this is where my first problem appears.\n\nDo I need a way to export the users from active directory with their passwords on a daily basis. However I know that passwords are stored as a hash not clear text so id have to mimic the hashing in my web app. So do I use LDAP to authenticate the users for me? I would appreciate some advice of maybe a link to a resource that anyone knows that would be worth me reading. \n\nNote.\nThe web application will be made in PHP with a sql database running on a windows server inside the same domain. \n\nThanks\n    ", "Answer": "\r\nPHP has an LDAP module that allows you to interface the Active Directory without exporting it.\n\nHere is an example of implementing an LDAP authentication through php:\nhttp://code.activestate.com/recipes/101525-ldap-authentication/\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "git diff doesn't print an error for bad filenames\r\n                \r\nLet's say that I issue the following command:\n```\ngit diff hash1 hash2 -- MyFile.java\n```\n\nIf the versions of the file in the two hashes are identical, then git returns no output. That seems reasonable and expected. However, if instead I pass a bad filename like this:\n```\ngit diff hash1 hash2 -- MyFile.javaaaa\n```\n\ninstead of getting an error, git again returns no output. I know that this behavior isn't universal; there are other git commands that do complain about a bad filename. But that's not happening here. And it's a real problem, because I can't differentiate between the case where there really is no difference and the case where I've just fat fingered the filename.\nThis can't be right. What am I missing? Is there some setting I can change?\n    ", "Answer": "\r\nIn git diff's document, ```\n<path>```\n \"are used to limit the diff to the named paths\", which sounds like that it acted like a filename filter, rather than a filename specifier.\nThat is, if you enter a filename that this file not exists, you will filter out all files, remaining nothing in the output.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "AES128-ECB under UWP\r\n                \r\nI need help in retrieving AES128-EBC encrypted string under Universal Windows Application.\nI have a password in string that is used as a key. With it's 32 bits length MD5 hash value I would like to encrypt text with AES128-EBC. \nNow I am using this for creating MD5Hash:\n\n```\npublic string GetMD5Hash(String strMsg)\n{\n    string strAlgName = HashAlgorithmNames.Md5;\n    IBuffer buffUtf8Msg = CryptographicBuffer.ConvertStringToBinary(strMsg, BinaryStringEncoding.Utf8);\n\n    HashAlgorithmProvider objAlgProv = HashAlgorithmProvider.OpenAlgorithm\n    string strAlgNameUsed = objAlgProv.AlgorithmName;\n\n    IBuffer buffHash = objAlgProv.HashData(buffUtf8Msg);\n\n    if (buffHash.Length != objAlgProv.HashLength)\n    {\n        throw new Exception(\"There was an error creating the hash\");\n    }\n\n    string hex = CryptographicBuffer.EncodeToHexString(buffHash);\n\n    return hex;\n}\n```\n\n\nAnd this code for encryption:\n\n```\npublic string Encrypt(string input, string pass)\n{\n    SymmetricKeyAlgorithmProvider provider = SymmetricKeyAlgorithmProvider.OpenAlgorithm(SymmetricAlgorithmNames.AesEcbPkcs7);\n    CryptographicKey key;\n\n    string encrypted = \"\";\n\n    byte[] keyhash = Encoding.ASCII.GetBytes(GetMD5Hash(pass));\n    key = provider.CreateSymmetricKey(CryptographicBuffer.CreateFromByteArray(keyhash));\n\n    IBuffer data = CryptographicBuffer.CreateFromByteArray(Encoding.Unicode.GetBytes(input));\n    encrypted = CryptographicBuffer.EncodeToBase64String(CryptographicEngine.Encrypt(key, data, null));\n\n    return encrypted;\n}\n```\n\n\nThe cause why I am using ```\nSymmetricAlgorithmNames.AesEcbPkcs7```\n is when I am using ```\nSymmetricAlgorithmNames.AesEcb```\n the output string is empty. I don't understand why.\nMy question is: Does my code create an AES128-ECB encryption? Because I not really sure it does. Because the software that is waiting for that encrypted data not recognizes it, so it cannot decrypt it.\n    ", "Answer": "\r\n\n  My question is: Does my code create an AES128-ECB encryption? Because I not really sure it does. Because the software that is waiting for that encrypted data not recognizes it, so it cannot decrypt it.\n\n\nYes, your code create an AES encryption with ECB cipher mode and PKCS7 padding. If I correctly understand your problem, you said this works with ```\nAesEcbPkcs7```\n, but failed using ```\nAesEcb```\n, your software for decryption doesn't work for this.\n\nThe difference between ```\nAesEcbPkcs7```\n and ```\nAesEcb```\n is, ```\nAesEcbPkcs7```\n use PKCS#7 block padding modes, and PKCS #7 algorithms automatically pads the message to an appropriate length, so you don’t need to pad the cipher to a multiple of the block-size of the algorithm you are using. So if you insist to use ```\nAesEcb```\n to encrypt, I recommend to use `AesEcbPkcs7, otherwise an exception: The supplied user buffer is not valid for the requested operation.\n\nSo I guess, one possibility here in your decryption software, it may have the ability to use ```\nAesEcbPkcs7```\n, but it doesn't implement the decrytion of ```\nAesEcb```\n. Here I tested decryption based on your code, this code can decrypt ```\nAesEcb```\n correctly:\n\n```\npublic string Decrypt(string input, string pass)\n{\n    var keyHash = Encoding.ASCII.GetBytes(GetMD5Hash(pass));\n\n    // Create a buffer that contains the encoded message to be decrypted.\n    IBuffer toDecryptBuffer = CryptographicBuffer.DecodeFromBase64String(input);\n\n    // Open a symmetric algorithm provider for the specified algorithm.\n    SymmetricKeyAlgorithmProvider aes = SymmetricKeyAlgorithmProvider.OpenAlgorithm(SymmetricAlgorithmNames.AesEcb);\n\n    // Create a symmetric key.\n    var symetricKey = aes.CreateSymmetricKey(keyHash.AsBuffer());\n\n    var buffDecrypted = CryptographicEngine.Decrypt(symetricKey, toDecryptBuffer, null);\n\n    string strDecrypted = CryptographicBuffer.ConvertBinaryToString(BinaryStringEncoding.Utf8, buffDecrypted);\n\n    return strDecrypted;\n}\n```\n\n\nAnother possibility I think you catch the exception when using ```\nAesEcb```\n and the user buffer is not valid for the requested operation and handled it when you call your ```\nEncrypt(string input, string pass)```\n method, the encryption failed actually.      \n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "What are the implications of migrating from createWebHashHistory() to createWebHistory()?\r\n                \r\nI have an application that I released that uses ```\ncreateWebHashHistory()```\n to manage URLs. For example, a user visits the following URL to visit something called the earthquakes channel:\n```\nhttps://app.radar.chat/#/channel/earthquakes\n```\n\nI would like to switch over to using ```\ncreateWebHistory()```\n to manage URLs instead (SEO, social media previews, iOS universal links configuration, etc). With that in mind, I would like it if my new URL structure looks like this:\n```\nhttps://app.radar.chat/channel/earthquakes\n```\n\nI know that to support this change I need to make a server change. The easiest way is to have the server redirect incoming requests to an index.html file (this is documented extensively on the Vue website).\nHowever, there are URLs in the wild that link to these old pages, URLs that have been printed and that can never be updated.\nIs there a convenient mechanism to continue to support the old hash-based URLs while having the non-hashed URLs be the new default?\n    ", "Answer": "\r\nIn your router config, you could add a global ```\nbeforeEach```\n hook on the index path that resolves to the hash path in the URL if it exists:\n```\n// router.js\nimport { createRouter, createWebHistory } from 'vue-router'\n\nconst router = createRouter({\n  history: createWebHistory(),\n  routes: [⋯]\n})\n\nrouter.beforeEach((to, from, next) => {\n  if (to.path === '/' && to.hash.startsWith('#/')) {\n    next(to.hash.substr(1))\n  } else {\n    next()\n  }\n})\n\nexport default router\n```\n\ndemo\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Config file handling in Perl\r\n                \r\nThere are plenty of Modules in the Config:: Namespace on CPAN, but they are all limited in ond way or another.\n\nI'm currently using Config::Std, which is fine most of the time, however it makes certain things difficult:\n\n\nmore than two levels of nested directives\nhandling of multiple values per key\nconf.d directories, i.e. multiple config files which are merged into one big config hash\n\n\nConfig::Std generates a blessed hashref after parsing the config, so all my applications are coded to use a hashref for configuration. I'd prefer not having to change this.\n\nWhat I am looking for is a universal, lightweight Config Module that produces a hashref.\n\nMy Question is: Which Config Modules should I consider for replacing Config::Std?\n    ", "Answer": "\r\nConfig::Any (for loading several files and flattening to a hash) and its Config::General backend (for arbitrarily nested configuration items and multiple values per key à la Apache httpd)\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is it possible to install perl prerequisites before distribution testing and how?\r\n                \r\nI try to build a Perl distribution for a home-made module, from the Module::Starter base. Every test pass on my machine, but when I upload it to CPAN to get some more universal tests from cpantesters.org, some test failed on other architectures or OS, but I can't understand why. I can see in test reports that some of my prerequisites are not installed before testing but I would like it to. \n\nI've tried to list these dependencies into the ```\nMakefile.PL```\n ```\nPREREQ_PM```\n hash and then in the ```\nTEST_REQUIRES```\n hash, but it didn't changed a lot of results.\n\nThen, when I've removed the dependencies from my local machine and tried to install my module using ```\nCpanm```\n, it downloads dependencies first, test passed and install has been a success. \n\nThis is my first try for a module, so I think I am missing something, maybe I am too used of the ```\nCpanm```\n magic. Thanks for any help.\n    ", "Answer": "\r\nThe problem is something different. Andreas' smoker very probably built the dependency ```\nApp::Ack```\n (which looks in the fail reports like being absent) successfully. But here come at least two problems:\n\n\nWhen a distribution gets tested, then its dependencies may or may not be installed already. However, it's guaranteed that all dependent modules are made available through the ```\nPERL5LIB```\n environment variable, so ```\nmake test```\n usually works (To be more specific, if the ```\ninstall Module```\n command is used in the CPAN shell, then all dependencies are installed immediately. If the ```\ntest Module```\n command is used, then dependencies are only built, but not installed. The CPAN user can do the installation later using ```\ninstall_tested```\n). So it may be that ```\nApp::Ack```\n is not installed here, just built. Especially this means that the ```\nack```\n script is not installed in the final location.\nEven if it is installed, many smoke testers or users who have multiple perls installed in parallel use a non-standard directory for this perl. So ```\nack```\n wouldn't be installed in ```\n/usr/bin```\n or ```\n/usr/local/bin```\n, but in the bin directory belonging to this perl. This directory may or may not be in the user's ```\nPATH```\n at all. So you cannot assume that ```\ncan_run(\"ack\")```\n works here. A workaround here is to add ```\n$Config{scriptdir}```\n temporarily to ```\n$ENV{PATH}```\n. Another solution would be to use the App module instead of the script, if it's possible. Unfortunately it looks like ```\nack```\n can only be called as a script.\n\n\nIf you look at a sample fail report, then you can see that ```\nApp::Ack```\n was installed (it appears in the PREREQUISITES section both under requires and build_requires, you can also see which ```\nApp::Ack```\n version is installed in the \"HAVE\" column). You can also see the user's ```\nPATH```\n (in the ENVIRONMENT section). And you may guess about the scriptdir for this perl, it's usually the same directory where the perl binary itself is installed, and the path to current perl is visible in ```\n$^X```\n (under \"Perl special variables\").\n\nIf you want to reproduce the behavior, then you need to deinstall ```\nack```\n from your machine, build a custom perl using ```\n./configure.gnu --prefix=/path/to/custom/perl-5.X.Y```\n, and use this perl for tests.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "What are the practical limitations of a non-turing complete language like Coq?\r\n                \r\nAs there are non-Turing complete languages out there, and given I didn't study Comp Sci at university, could someone explain something that a Turing-incomplete language (like Coq) cannot do?\n\nOr is the completeness/incompleteness of no real practical interest (i.e. does it not make much difference in practice)?\n\nEDIT - I'm looking for an answer along the lines of you cannot build a hash table in a non-Turing complete language due to X, or something like that!\n    ", "Answer": "\r\nFirst, I assume you've already heard of the Church-Turing thesis, which states that anything we call “computation” is something that can be done with a Turing machine (or any of the many other equivalent models). So a Turing-complete language is one in which any computation can be expressed. Conversely, a Turing-incomplete language is one in which there is some computation that cannot be expressed.\n\nOk, that wasn't very informative. Let me give an example. There is one thing you cannot do in any Turing-incomplete language: you can't write a Turing machine simulator (otherwise you could encode any computation on the simulated Turing machine).\n\nOk, that still wasn't very informative. the real question is, what useful program cannot be written in a Turing-incomplete language? Well, nobody has come up with a definition of “useful program” that includes all the programs someone somewhere has written for a useful purpose, and that doesn't include all Turing machine computations. So designing a Turing-incomplete language in which you can write all useful programs is still a very long-term research goal.\n\nNow there are several very different kinds of Turing-incomplete languages out there, and they differ in what they can't do. However there is a common theme. If you're designing a language, there are two major ways to ensure that the language will be Turing-complete:\n\n\nrequire that the language has arbitrary loops (```\nwhile```\n) and dynamic memory allocation (```\nmalloc```\n)\nrequire that the language supports arbitrary recursive functions\n\n\nLet's look at a few examples of non-Turing complete languages that some people might nonetheless call programming languages.\n\n\nEarly versions of FORTRAN did not have dynamic memory allocation. You had to figure out in advance how much memory your computation would need and allocate that. In spite of that, FORTRAN was once the most widely used programming language.\n\nThe obvious practical limitation is that you have to predict the memory requirements of your program before running it. That can be hard, and can be impossible if the size of the input data is not bounded in advance. At the time, the person feeding the input data was often the person who had written the program, so it wasn't such a big deal. But that's just not true for most programs written today.\nCoq is a language designed for proving theorems. Now proving theorems and running programs are very closely related, so you can write programs in Coq just like you prove a theorem. Intuitively, a proof of the theorem “A implies B” is a function that takes a proof of theorem A as an argument and returns a proof of theorem B.\n\nSince the goal of the system is to prove theorems, you can't let the programmer write arbitrary functions. Imagine the language allowed you to write a silly recursive function that just called itself (pick the line that uses your favorite language):\n\n```\ntheorem_B boom (theorem_A a) { return boom(a); }\nlet rec boom (a : theorem_A) : theorem_B = boom (a)\ndef boom(a): boom(a)\n(define (boom a) (boom a))\n```\n\n\nYou can't let the existence of such a function convince you that A implies B, or else you would be able to prove anything and not just true theorems! So Coq (and similar theorem provers) forbid arbitrary recursion. When you write a recursive function, you must prove that it always terminates, so that whenever you run it on a proof of theorem A you know that it will construct a proof of theorem B.\n\nThe immediate practical limitation of Coq is that you cannot write arbitrary recursive functions. Since the system must be able to reject all non-terminating functions, the undecidability of the halting problem (or more generally Rice's theorem) ensures that there are terminating functions that are rejected as well. An added practical difficulty is that you have to help the system to prove that your function does terminate.\n\nThere is a lot of ongoing research on making proof systems more programming-language-like without compromising their guarantee that if you have a function from A to B, it's as good as a mathematical proof that A implies B. Extending the system to accept more terminating functions is one of the research topics. Other extension directions include coping with such “real-world” concerns as input/output and concurrency. Another challenge is to make these systems accessible to mere mortals (or perhaps convince mere mortals that they are in fact accessible).\nSynchronous programming languages are languages designed for programming real-time systems, that is, systems where the program must respond in less than n clock cycles. They are mainly used for mission-critical systems such as vehicle controls or signaling. These languages offer strong guarantees on how long a program will take to run and how much memory it may allocate.\n\nOf course, the counterpart of such strong guarantees is that you can't write programs whose memory consumption and running time you're not able to predict in advance. In particular, you can't write a program whose memory consumption or running time depends on the input data.\nThere are many specialized languages that don't even try to be programming languages and so can remain comfortably far from Turing completeness: regular expressions, data languages, most markup languages, ...\n\n\nBy the way, Douglas Hofstadter wrote several very interesting popular science books about computation, in particular Gödel, Escher, Bach: an Eternal Golden Braid. I don't recall whether he explicitly discusses limitations of Turing-incompleteness, but reading his books will definitely help you understand more technical material.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Safely generated encryption key from user's password?\r\n                \r\nI'm building a Django website with MySQL. I've already decided to use Django's built in pbkdf2-sha256 with randomly generated salt hash to store a user's password.\n\nHowever this website will also need to store third party login credentials for many other websites (who don't use oauth). So I was looking into AES-256 encryption and of course the issue becomes where to securely store the encryption key.\n\nNow here's my solution: Let each encryption key = the hash of the users actual password and a randomly generated salt (different from the salt already used for the password stored hash). The salt would be stored in the table, the actual password and the hash of it obviously not. So the encryption key would be generated on login and stored temporarily but expire on logout. Further someone compromising the server can't generate the encryption key without cracking the original pbkdf2-sha256 hash and even then, it would only be for that one user, not a universal key.\n\nThe downside is that if they change/reset password, they would have to re-enter their credentials for each site. But that's not a huge deal and seems to be way more secure than storing the key somewhere on the server or even a different server.\n\nBut I only learned what a hash is 24 hours ago so what do I know. Am I overlooking something or is this reasonably secure? Or is there a better way?\n    ", "Answer": "\r\nThe algorithm ```\nPBKDF2```\n which you mention is actually designed for this explicit purpose.\n\nSo, the workflow would be to generate a random salt. Then store that in the database for the user.\n\nUsing PBKDF2 with a high iteration count and the salt, generate 640 bits of key material (80 bytes).\n\nThe first 128 bits become the IV for the cipher\n\nThe next 256 bits become the cipher key (the key used for AES-256)\n\nThe final 256 bits become the MAC key (the key used to authenticate the encryption).\n\n```\nkey = PBKDF2-SHA256(password, salt, 50000, 80)\niv = key[0:128]\ncipherKey = key[128:384]\nmacKey = key[384:640]\n```\n\n\nThen, encrypt the data using those keys (pseudo-code):\n\n```\nciphertext = AES-256-CBC(data, cipherKey, iv)\nauthtext = SHA256-HMAC(ciphertext, macKey)\nresult = '{}{}'.format(authtext, ciphertext)\n```\n\n\nNow, on decryption, just walk back in reverse...\n\n```\nkey = PBKDF2-SHA256(password, salt, 50000, 80)\niv = key[0:128]\ncipherKey = key[128:384]\nmacKey = key[384:640]\n\nauthtext = result[0:32]\nciphertext = result[32:]\n\nif !timingSafeComparison(authtext, SHA256-HMAC(ciphertext, macKey)):\n    return false\n\nreturn AES-256-CBC-DECRYPT(ciphertext, cipherKey, iv)\n```\n\n\nYes, if your user forgets the password, all encrypted data is gone. But that's what you want, right?\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Safely generated encryption key from user's password?\r\n                \r\nI'm building a Django website with MySQL. I've already decided to use Django's built in pbkdf2-sha256 with randomly generated salt hash to store a user's password.\n\nHowever this website will also need to store third party login credentials for many other websites (who don't use oauth). So I was looking into AES-256 encryption and of course the issue becomes where to securely store the encryption key.\n\nNow here's my solution: Let each encryption key = the hash of the users actual password and a randomly generated salt (different from the salt already used for the password stored hash). The salt would be stored in the table, the actual password and the hash of it obviously not. So the encryption key would be generated on login and stored temporarily but expire on logout. Further someone compromising the server can't generate the encryption key without cracking the original pbkdf2-sha256 hash and even then, it would only be for that one user, not a universal key.\n\nThe downside is that if they change/reset password, they would have to re-enter their credentials for each site. But that's not a huge deal and seems to be way more secure than storing the key somewhere on the server or even a different server.\n\nBut I only learned what a hash is 24 hours ago so what do I know. Am I overlooking something or is this reasonably secure? Or is there a better way?\n    ", "Answer": "\r\nThe algorithm ```\nPBKDF2```\n which you mention is actually designed for this explicit purpose.\n\nSo, the workflow would be to generate a random salt. Then store that in the database for the user.\n\nUsing PBKDF2 with a high iteration count and the salt, generate 640 bits of key material (80 bytes).\n\nThe first 128 bits become the IV for the cipher\n\nThe next 256 bits become the cipher key (the key used for AES-256)\n\nThe final 256 bits become the MAC key (the key used to authenticate the encryption).\n\n```\nkey = PBKDF2-SHA256(password, salt, 50000, 80)\niv = key[0:128]\ncipherKey = key[128:384]\nmacKey = key[384:640]\n```\n\n\nThen, encrypt the data using those keys (pseudo-code):\n\n```\nciphertext = AES-256-CBC(data, cipherKey, iv)\nauthtext = SHA256-HMAC(ciphertext, macKey)\nresult = '{}{}'.format(authtext, ciphertext)\n```\n\n\nNow, on decryption, just walk back in reverse...\n\n```\nkey = PBKDF2-SHA256(password, salt, 50000, 80)\niv = key[0:128]\ncipherKey = key[128:384]\nmacKey = key[384:640]\n\nauthtext = result[0:32]\nciphertext = result[32:]\n\nif !timingSafeComparison(authtext, SHA256-HMAC(ciphertext, macKey)):\n    return false\n\nreturn AES-256-CBC-DECRYPT(ciphertext, cipherKey, iv)\n```\n\n\nYes, if your user forgets the password, all encrypted data is gone. But that's what you want, right?\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "I have a string with a package name. Is it possible to get a package variable?\r\n                \r\nin WithVar.pm:\n\n```\npackage Test::WithVar;\n\nour @desired_var = qw(1 2 3);\n```\n\n\nOtherPackage.pm:\n\n```\npackage Test::OtherPackage;\nuse Test::WithVar;\n\nsub test_get_var ($) {\n    my $package_name = shift;\n\n    #↓ How to do that ? Is it possible ? ↓\n    return @{$package_name::desired_var};\n}\n\n# I know that I can access the desired_var with @Test::WithVar::desired_var,\n# (explicitly typed package name, a 'bareword')\n# but what if I have package name in a string ?\n\ntest_get_var('Test::WithVar');\n```\n\n\nAnother solution (i guess) is to define a function in the WithVar package that will return our variable.\n\n```\npackage Test::WithVar;\n\nour @desired_var = qw(1 2 3);\n\nsub return_desired_var ($) {\n    my $self = shift;\n    our @desired_var;\n    return \\@desired_var;\n}\n```\n\n\nSo now I can:\n\n```\npackage Test::OtherPackage;\nuse Test::WithVar;\n\nmy $str = 'Test::WithVar';\nprint @{$str->return_desired_var()}, '\\n';\n```\n\n\nBut the problem is the inheritance. I need to inherit from WithVar and child modules will return WithVar::desired_var and I need $ChildPackage::desired_var.\n\npackage Test::ChildWithVar;\nuse parent 'Test::WithVar';\n\nour @desired_var = qw(4 5 6); # redefine variable in child module.\n\nAnd if I write\n\n```\npackage Test::OtherPackage;\nuse Test::ChildWithVar;\n\nmy $str = 'Test::ChildWithVar';\n\n# This will print 1 2 3 of course, not 4 5 6.\nprint @{$str->return_desired_var()}, '\\n'; \n```\n\n\nSo I need somehow to write universal method in Test::WithVar package:\n\n```\npackage Test::WithVar;\n\nour @desired_var = qw(1 2 3);\n\nsub return_desired_var_universal ($) {\n    my $self = shift;\n    my $class = ref $self || $self;\n    return $class::desired_var; #Somehow ?!\n}\n```\n\n\nI know that I can use a hash and store package-specific variables in it instead of using 'our' variables. I know. But I need a solution with 'our', or the clear understanding that this is not possible.\n\nThanks.\n    ", "Answer": "\r\nYou should ```\nuse strict```\n, for many reasons, the one concerning your case being explained here.\n\nYou can disable the warning, ```\nuse strict```\n emits for the following code by \n\n```\n# some code block\n{  \n   ...\n   no strict 'refs'; \n   return @{$package_name.'::desired_var'};\n}\n```\n\n\nYou should try to put what you really want to achieve in a seperate question and explain why you think, it could only be done with ```\nour```\n variables.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "pbkdf2 computation not consistent between C# and JavaScript\r\n                \r\nHi my question is I've Encrypted a password with crypto.pbkdf2 on windows azure server side Javascript I'm pretty sure that there is a public library which you can look up. The problem is I'm trying to encrypt the same password in C# on my system because I want the credentials to be universal but despite using Rfc2898DeriveBytes and the salt generated the first time I'm not able to get back to the same hashed password.\n\nThank you for your help :)\n\n```\nfunction hash(text, salt, callback) {\n  crypto.pbkdf2(text, salt, iterations, bytes, function(err, derivedKey){\n    if (err) { callback(err); }\n    else {\n      var h = new Buffer(derivedKey).toString('base64');\n      callback(null, h);\n    }\n  });\n}\n```\n\n\nAnd the C# code:\n\n```\nbyte[] salt = Convert.FromBase64String(user.salt);\nusing (var deriveBytes = new System.Security.Cryptography.Rfc2898DeriveBytes(password, salt, 1000))\n{\n  byte[] newKey = deriveBytes.GetBytes(32);\n\n  // user is the user object drawn from the database in existence\n  if (Convert.ToBase64String(newKey).Equals(user.password))\n  {\n    FormsAuthentication.RedirectFromLoginPage(Request.Form[\"username\"], false);\n  }\n}\n```\n\n\nhex generated by C# = 3lRSQF5ImYlQg20CGFy2iGUpWfdP5TD0eq2cTHhLono=\n\nhex generated by JS = w4PDh8K6YMKGwr3DgcObRsOsFFUgDMOJw5PCnkdAwrTCgcOOV8OCKMKFdcKRwrLCqMK2VA==\n\nSalt generated by JS and used at both = /Ij0hgDsvAC1DevM7xkdGUVlozdCxXVd0lgfK2xEh2A=\n\nAll the above info is in base64 format\n\nAnother thing that might be useful\n\n```\nitem.salt = new Buffer(crypto.randomBytes(bytes)).toString('base64'); crypto.pbkdf2(text, salt, iterations, bytes, function(err, derivedKey){\n```\n\n\nwhich means the JS function accepts a string\n    ", "Answer": "\r\n\n  I want the credentials to be universal but despite using Rfc2898DeriveBytes and the salt generated the first time I'm not able to get back to the same hashed password.\n\n\nThe obvious stuff is hash algorithm, salt, and iteration count. Can you confirm (for both languages):\n\n\nthe hash algorithm\nthe salt\nthe iteration count\n\n\nThe non-obvious is the encoding of the password and possibly salt. I included the salt because its often stored as a string.\n\nTo keep it portable among languages, you should use UTF-8. That's because you could encounter a default encoding, a UTF16-BE, UTF16-LE or any number of other encoding.\n\nIn C#, the setup would be:\n\n```\nbyte[] utf8_salt = Encoding.UTF8.GetBytes(salt);\nbyte[] utf8_pass = Encoding.UTF8.GetBytes(password);\n```\n\n\nYou would then pass ```\nutf8_salt```\n and ```\nutf8_pass```\n to the ```\nPBKDF2```\n function.\n\nI don't know how to do the same in Javascript.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to extract at runtime the AST or the bytecode or anything that uniquelly, deterministically and universally identifies a function* in Haskell?\r\n                \r\nI have managed to do it in Python, since the interpreter provide the bytecodes, as shown below.\nFrom the bytecodes it was easy to apply a hashing function.\n```\nimport dis\nfrom inspect import signature\n\n# Add signature.\nfargs = list(signature(f).parameters.keys())\nif not fargs:\n    raise Exception(f\"Missing function input parameters.\")\nclean = [fargs]\n\n# Clean line numbers.\ngroups = [l for l in dis.Bytecode(f).dis().split(\"\\n\\n\") if l]\nfor group in groups:\n    lines = [segment for segment in group.split(\" \") if segment][1:]\n    clean.append(lines)\n```\n\nHow could I extract a similar kind of deterministic universally unique ¹ function identifier in Haskell?\nAre there built-in libraries or GHC extensions for that?\n```\n1 -```\n unique, probabilistically speaking\n```\n* -```\n A function not in the math sense, since the possibilities to check programatically the equivalence of programs are very limited. So I am only interested in what is possible and costless attainable, i.e., generate a unique identity for functions with the exact same code (or bytecode, to avoid formatting noise). In my specific use case, even the signatures and parameter names should match, but that is not relevant for the point here, which is about how to inspect Haskell code at runtime.\n    ", "Answer": "\r\nWithin a single run of the program, you might consider ```\nStableName```\n. You can make a ```\nStableName```\n for any value (including a function/closure), get a hash value for a ```\nStableName```\n, and compare two ```\nStableName```\ns for equality. If two things have the same ```\nStableName```\n, then they're definitely the same. If they have the same ```\nStableName```\n hash, then they're probably the same. Note that if you just need to be able to check equality and don't need the hashes, one option is ```\nreallyUnsafePtrEquality#```\n, which just compares any two values by address.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is hashCode() considered as a hash function?\r\n                \r\nIm currently having a programming task on my university about hash tables, yet, the task specifically said that we cannot use any Hash functions from Java or any ADT libraries, if so, is hashCode() considered as a hash Function? I am deeply confused.\n\nThank you for your help :D\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Very high memory usage in these Digit Analysis (Hash) algorithms in C\r\n                \r\nI have an university exercise, where I have to compare some hashing methods with their number of colisions in the hash table. Then I made theses Digit Analysis algorithms, but they are using A LOT of memory (I can't even run the code until the end, because it kills my computer). You can ignore the comments, but fell free if you want and knows portuguese.\nDigit Analysis function 1 (Using dinamic matrix)\n```\nint hashUVDigitAnalysis(int len, int value, int numofdigits, int *arr){\n    int keydigits, *digits = getDigits(value, &keydigits);\n\n    if(numofdigits >= keydigits){\n        free(digits);                                                        //                                     -ATENÇÃO-\n        return value;                                                        //\n    }else{                                                                   // Essa função funciona, mas quando testei com o vetor *arr muito grande, ele rapidamente\n        int **numqntmatrix = (int **)(malloc(10 * sizeof(int *)));           //          consumiu maior parte da memória do meu pc, e o computador falhou por\n        int cont1, cont2, countdigits;                                       //                    falta de memória. Use essa função apenas para\n        float *detours = (float *)(malloc(keydigits * sizeof(float)));       //                            testar um único valor (UV).\n                                                                             // Como alternativa, tentei realizar a função abaixo desta, mas a mesma deu o mesmo problema.\n        for(cont1 = 0; cont1 < 10; cont1++){                                 //\n            numqntmatrix[cont1] = (int *)(malloc(keydigits * sizeof(int)));\n            for(cont2 = 0; cont2 < keydigits; cont2++){\n                numqntmatrix[cont1][cont2] = 0;\n            }\n        }\n\n        for(cont1 = 0; cont1 < len; cont1++){\n            digits = getDigits(arr[cont1], &countdigits);\n            for(cont2 = 0; cont2 < keydigits && cont2 < countdigits; cont2++){\n                numqntmatrix[digits[cont2]][cont2] += 1;\n            }\n        }\n\n        for(cont1 = 0; cont1 < keydigits; cont1++){\n            detours[cont1] = 0.0;\n        }\n\n        for(cont1 = 0; cont1 < keydigits; cont1++){\n            for(cont2 = 0; cont2 < 10; cont2++){\n                detours[cont1] += (numqntmatrix[cont2][cont1] - (len / 10.0)) * (numqntmatrix[cont2][cont1] - (len / 10.0));\n            }\n        }\n\n        for(cont1 = 0; cont1 < 10; cont1++){\n            free(numqntmatrix[cont1]);\n        }\n        free(numqntmatrix);\n\n        int *concatenate = (int *)(malloc(numofdigits * sizeof(int)));\n\n        for(cont1 = 0; cont1 < numofdigits; cont1++){\n            cont2 = 0;\n            concatenate[cont1] = cont2;\n            for(cont2 = 1; cont2 < keydigits; cont2++){\n                if(detours[cont2] < detours[concatenate[cont1]] && detours[cont2] != -1){\n                    concatenate[cont1] = cont2;\n                }\n            }\n            detours[concatenate[cont1]] = -1;\n        }\n\n        digits = getDigits(value, &countdigits);\n        int valueposition = 0;\n\n        for(cont1 = 0; cont1 < numofdigits; cont1++){\n            valueposition += digits[concatenate[cont1]] * pow(10, cont1);\n        }\n\n        free(detours);\n        free(digits);\n\n        return valueposition;\n    }\n}\n```\n\nDigit Analysis function 2 (Using only arrays)\n```\nint hashQuadraticDigitAnalysis(int len, int value, int numofdigits, int *arr){\n    int keydigits, *digits = getDigits(value, &keydigits);\n\n    if(numofdigits >= keydigits){\n        free(digits);\n        return value;\n    }else{\n        int cont1, cont2, countdigits;\n        int *numbers = (int *)(malloc(10 * sizeof(int)));\n        float *detours = (float *)(malloc(10 * sizeof(float)));\n\n        for(cont1 = 0; cont1 < 10; cont1++){\n            numbers[cont1] = 0;\n            detours[cont1] = 0.0;\n        }\n\n        for(cont1 = 0; cont1 < 10; cont1++){\n            for(cont2 = 0; cont2 < len; cont2++){\n                digits = getDigits(arr[cont2], &countdigits);\n                if(cont1 < countdigits){\n                    numbers[digits[cont1]] += 1;\n                }\n            }\n            for(cont2 = 0; cont2 < 10; cont2++){\n                detours[cont1] += (numbers[cont2] - (len / 10.0)) * (numbers[cont2] - (len / 10.0));\n                numbers[cont2] = 0;\n            }\n        }\n\n        int *concatenate = (int *)(malloc(numofdigits * sizeof(int)));\n\n        for(cont1 = 0; cont1 < numofdigits; cont1++){\n            cont2 = 0;\n            concatenate[cont1] = cont2;\n            for(cont2 = 1; cont2 < keydigits; cont2++){\n                if(detours[cont2] < detours[concatenate[cont1]] && detours[cont2] != -1){\n                    concatenate[cont1] = cont2;\n                }\n            }\n            detours[concatenate[cont1]] = -1;\n        }\n\n        digits = getDigits(value, &countdigits);\n        int valueposition = 0;\n\n        for(cont1 = 0; cont1 < numofdigits; cont1++){\n            valueposition += digits[concatenate[cont1]] * pow(10, cont1);\n        }\n\n        free(concatenate);\n        free(detours);\n        free(digits);\n\n        return valueposition;\n    }\n}\n```\n\ngetDigits function\n```\nint* getDigits(int value, int *addr_countdigits){\n    int copyofvalue = value;\n    *addr_countdigits = 0;\n\n    while(copyofvalue != 0){\n        copyofvalue = copyofvalue / 10;\n        *addr_countdigits = *addr_countdigits + 1;\n    }\n\n    int tmp = *addr_countdigits;\n    int *array = (int *)(malloc(tmp * sizeof(int)));\n    copyofvalue = value;\n\n    while(copyofvalue > 0){\n        array[tmp - 1] = copyofvalue % 10;\n        copyofvalue = copyofvalue / 10;\n        tmp = tmp-1;\n    }\n\n    return array;\n}\n```\n\nMain\n```\nint main(void){\n    int len = 100000, lenarr = 200000, cont, random, numcolision = 0, pos;\n\n    int *randomarr = (int *)(malloc(lenarr * sizeof(int)));\n\n    int *hash_division = (int *)(malloc(len * sizeof(int)));\n\n    for(cont = 0; cont < len; cont++){\n        hash_division[cont] = -1;\n    }\n\n    for(cont = 0; cont < lenarr; cont++){\n        random  = (((rand() & 255)<<8 | (rand() & 255))<<8 | (rand() & 255))<<7 | (rand() & 127);\n        random = random % 2000000001;\n        randomarr[cont] = random;\n    }\n\n    for(cont = 0; cont < lenarr; cont++){\n        pos = hashUVDigitAnalysis(lenarr, randomarr[cont], 5, randomarr);\n        if(hash_division[pos] == -1){\n            hash_division[pos] = randomarr[cont];\n        } else{\n            numcolision++;\n        }\n    }\n\n    printf(\"%d\", numcolision);\n    return 0;\n}\n```\n\nI have 14 GB of usable RAM memory (total of 16 GB).\nI tested both functions and there isn't any infinity loop. The code runs when I put ```\nlenarr = 10000```\n, but I have to test with ```\nlen = 100000```\n and ```\nlenarr```\n equals to 200 thousand, 400 thousand, 600 thousand, 800 thousand and 1 million, so only 10 thousand will not works for me. I'm doing something wrong? Is there any way to fix this? I never had a memory problem in any code before, so I'm not good with controlling my memory in code.\n    ", "Answer": "\r\nCause of leak\nI looked at this in valgrind, and it looks like you're missing five calls to free. This is the largest leak:\n```\n    for(cont1 = 0; cont1 < len; cont1++){\n        digits = getDigits(arr[cont1], &countdigits);\n        for(cont2 = 0; cont2 < keydigits && cont2 < countdigits; cont2++){\n            numqntmatrix[digits[cont2]][cont2] += 1;\n        }\n        // free memory returned by getDigits()\n        free(digits);\n    }\n```\n\nEvery loop through this calls ```\ngetDigits()```\n, which allocates memory which is never freed.\nOther sources of leaks:\n\n```\nint keydigits, *digits = getDigits(value, &keydigits);```\n\n```\nint *concatenate = (int *)(malloc(numofdigits * sizeof(int)));```\n\n```\nint *randomarr = (int *)(malloc(lenarr * sizeof(int)));```\n\n```\nint *hash_division = (int *)(malloc(len * sizeof(int)));```\n\n\nHow to use valgrind\nHere's how I performed the valgrind analysis:\nFirst, I reduced the number of loops the program makes from 100K to 10. This prevents it from running out of memory while valgrind is running.\nSecond, I compiled the program with ```\n-ggdb```\n, to enable debugging information. Here's the command I used:\n```\ngcc test027.c -Wall -Werror -lm -ggdb -o test027\n```\n\nThird, I ran valgrind with ```\n--leak-check=full```\n:\n```\nvalgrind --leak-check=full ./test027 \n```\n\nThis showed me the stacktraces of the sources of the memory leaks. Each one looks like this:\n```\n==174419== 1,501,600 bytes in 40,000 blocks are definitely lost in loss record 1 of 1\n==174419==    at 0x483C7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)\n==174419==    by 0x10925C: getDigits (test027.c:16)\n==174419==    by 0x10940D: hashUVDigitAnalysis (test027.c:50)\n==174419==    by 0x109988: main (test027.c:118)\n```\n\nThe first thing you see is the size of the memory leak. (1,501,600 bytes). You should solve the largest memory leak first. Next, you see how many allocations were made that were never freed. (40,000 blocks). Next, you see the line numbers of the part of the program which is responsible.\nSolve each memory leak one at a time, and re-run valgrind. Once you solve all the leaks, valgrind will show this output:\n```\n==174500== HEAP SUMMARY:\n==174500==     in use at exit: 0 bytes in 0 blocks\n==174500==   total heap usage: 43,003 allocs, 43,003 frees, 1,621,428 bytes allocated\n==174500== \n==174500== All heap blocks were freed -- no leaks are possible\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How exactly does std::distance calculate the return value? unclear problem with a selfmade set-iterator\r\n                \r\nFor an assignment in university I need to program my own ```\nset```\n that works with extendible hashing. I wrote the code for insertion, erase and whatever it takes and it worked fine. At last I added an ```\niterator```\n as a nested class and with my own test-program it worked well. now as I tested it with a program of my university the ```\nstd::distance(InputIterator first, InputIterator last)```\n function delivers a value that's about half the actual size of my set, if I use ```\nbegin()```\n and ```\nend()```\n as the iterators. When I regularly test my iterator it works perfectly all the time: reaches every element in the correct order and fulfills all the requirements of the requested ```\nforwarditerator```\n.\nSo my question is how the ```\nstd::distance()```\n function works internally to maybe find the problem in my program.\n\nI dont program for that long, only in second semester and new here. \nSo please be gentle and let me know  if you need something more to work with:)\ngreetings\n\nI've tried to overload the ```\noperator-```\n as well in case the ```\nstd::distance```\n function just subtract the iterators but it doesn't work as well.\n\n```\n//sorry i had to take the code out, because my university doesnt allow it for //this assignment in case of upcoming duplicates\n```\n\n\nno error messages, the output is just a bit more than half the actual size.\n    ", "Answer": "\r\n\n  How exactly does std::distance calculate the return value?\n\n\nDepends on what operations the iterator supports. Random access iterators support subtraction, and ```\nstd::distance```\n will use that. For non-random access iterators, it'll compare the first iterator to the second and if they are not equal, then increment to next one and repeat until they do compare equal, while using a counter to keep track of iterations.\n\nYou can see exactly what operations of your iterator are used with a debugger.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Angular page on server is blank after compiling and starting server\r\n                \r\nI builded the apllication on angular 7 with server-side rendering on my local enviroinment, on localhost, and everything works perfect. \nMy index.html was compiled as should be (on screen). \nBut when I tried to get this app on my server on ssh:\n-  I started the compilation on angular app by command \"npm run build:ssr && npm run serve:ssr\", so server.ts was started \n- then I started the server.js to start ssh server\n\nNow I have just blank page, and index.html looks only with . It shoudn't looks like that after compiling angular app.\nhere is the structure of files on ssh server: https://drive.google.com/file/d/11ailDCELJTLM_vut4mC6Bc2CWeRAmpYq/view?usp=sharing\n\nhere is the screen from localhost:\nhttps://drive.google.com/open?id=1E8DnLvvAt02uBhkuxIsREZKduuiPIeFW\n\nand from beta-server ssh:\nhttps://drive.google.com/open?id=1-CNrhaT9XgRo03igahEd_UwXpDAqRklH\n\nAs you can see, the root was not compiled and I don't know what i do wrong\n\nserver.js \n\n```\nconst express = require('express');\nconst path = require('path');\nconst http = require('http');\nconst bodyParser = require('body-parser');\n// Get our API routes\nconst api = require('./server/routes/api');\nvar timeout = require('connect-timeout'); //express v4\n\nconst app = express();\n\n// Parsers for POST data\napp.use(bodyParser.json());\napp.use(bodyParser.urlencoded({ extended: false }));\n\n// Point static path to dist\napp.use(express.static(path.join(__dirname, 'dist')));\n\n// Set our api routes\napp.use('/api', api);\n\n//TEST - timeout\napp.use(timeout(600000));\n\n// Catch all other routes and return the index file\napp.get('*', (req, res) => {\n  res.sendFile(path.join(__dirname, 'dist/index.html'));\n});\n\n/**\n * Get port from environment and store in Express.\n */\nconst port = process.env.PORT || '3000';\napp.set('port', port);\n\n/**\n * Create HTTP server.\n */\nconst server = http.createServer(app);\n\n/**\n * Listen on provided port, on all network interfaces.\n */\nserver.listen(port, () => console.log(`ROHUB Angular2 app is running on localhost:${port}`));\n```\n\n\nserver.ts\n\n```\nmport 'zone.js/dist/zone-node';\nimport { enableProdMode } from '@angular/core';\n\n// Express Engine\nimport { ngExpressEngine } from '@nguniversal/express-engine';\n// Import module map for lazy loading\nimport { provideModuleMap } from '@nguniversal/module-map-ngfactory-loader';\n\nimport * as express from 'express';\nimport { join } from 'path';\n\n// Faster server renders w/ Prod mode (dev mode never needed)\nenableProdMode();\n\n// Express server\nconst app = express();\n\nconst PORT = process.env.PORT || 4000;\nconst DIST_FOLDER = join(process.cwd(), 'dist');\n\n// * NOTE :: leave this as require() since this file is built Dynamically from webpack\nconst { AppServerModuleNgFactory, LAZY_MODULE_MAP } = require('./dist/server/main');\n\n// Our Universal express-engine (found @ https://github.com/angular/universal/tree/master/modules/express-engine)\napp.engine('html', ngExpressEngine({\n  bootstrap: AppServerModuleNgFactory,\n  providers: [\n    provideModuleMap(LAZY_MODULE_MAP)\n  ]\n}));\n\napp.set('view engine', 'html');\napp.set('views', join(DIST_FOLDER, 'browser'));\n\n// Example Express Rest API endpoints\n// app.get('/api/**', (req, res) => { });\n\n// Server static files from /browser\napp.get('*.*', express.static(join(DIST_FOLDER, 'browser'), {\n  maxAge: '1y'\n}));\n\n// All regular routes use the Universal engine\napp.get('*', (req, res) => {\n  res.render('index', { req });\n});\n\n// Start up the Node server\napp.listen(PORT, () => {\n  console.log(`Node Express server listening on http://localhost:${PORT}`);\n});\n```\n\n\npackage.json\n\n```\n{\n  \"name\": \"ng-universal-demo\",\n  \"version\": \"0.0.0\",\n  \"license\": \"MIT\",\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"https://github.com/angular/universal-starter.git\"\n  },\n  \"contributors\": [],\n  \"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"ng serve\",\n    \"build\": \"ng build\",\n    \"lint\": \"ng lint ng-universal-demo\",\n    \"build:client-and-server-bundles\": \"ng build --prod  --output-hashing=none && ng run ng-universal-demo:server:production\",\n    \"build:prerender\": \"npm run build:client-and-server-bundles && npm run compile:server && npm run generate:prerender\",\n    \"build:ssr\": \"npm run build:client-and-server-bundles && npm run webpack:server\",\n    \"compile:server\": \"tsc -p server.tsconfig.json\",\n    \"webpack:server\": \"webpack --config webpack.server.config.js --progress --colors\",\n    \"generate:prerender\": \"cd dist && node prerender\",\n    \"serve:prerender\": \"cd dist/browser && http-server\",\n    \"serve:ssr\": \"node dist/server\"\n  },\n  \"pre-commit\": [],\n  \"private\": true,\n  \"dependencies\": {\n[...]\n```\n\n\nI exprected the same look on browser both on browse and on browse console, but page from server is blank.\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Remove hash from url\r\n                \r\nI am ajax-ifying the pagination in one of me projects and since I want the users to be able to bookmarks the current page, I am appending the page number via hash, say:\n\n```\nonclick=\"callPage(2); window.location.hash='p=2'; return false;\"\n```\n\n\nand thats on the ```\nhyperlink```\n it works fine and everything, except, when the page number is 1, i dont want to ```\nURL```\n to be ```\n/products#p=1```\n, I just want it to be ```\n/products```\n\n\nI tried these variations:\n\n\n```\nwindow.location.hash=''```\n works but the url is now like ```\n/products#```\n and I dont quite the hash there.\nnot using window.location.hash at all, but when the user comes back to page 1 from, say page 3, he is in page one, but url is still ```\n/products#p=3```\n since I am not messing with the hash.\nGoogle search on this led me to several minutes (about 15) of silly forums where the question was asked right, but answers were suggesting that the page jumps because the thread creator had a hash in href like ```\n<a href=\"#\">```\n and he should use ```\njavascript:void(0)```\n instead. (had they never heard of Ajax?)\n\n\nSo finally, I decided to make this thread,  I found several similar threads here, but all the answers ls very similar to my second point.\n\nso my big question still remains a question: How to kick the hash out of the URL and possibly out of the universe? (only for the first page!)\n    ", "Answer": "\r\n```\nhistory.pushState(\"\", document.title, window.location.pathname);\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to use BCrypt / Bcryptjs in Angular?\r\n                \r\n```\nimport { Component, OnInit } from '@angular/core';\nimport { FormGroup, NgForm } from '@angular/forms';\nimport * as bcrypt from 'bcryptjs';\n\n@Component({\n  selector: 'app-user-profile',\n  templateUrl: './user-profile.component.html',\n  styleUrls: ['./user-profile.component.css']\n})\nexport class UserProfileComponent implements OnInit {\n\n  constructor() { }\n\n\n  val = {\n    firstName: '',\n    lastName: '',\n    phoneNumber: '',\n    username: '',\n    password: ''\n  }\n\n  ngOnInit(): void {\n  }\n\n  isDisplay = false;\n  toggleDisplay() {\n    this.isDisplay = !this.isDisplay;\n  }\n\n  onSubmit(v: NgForm) {\n    console.log(v.value, v.valid);\n    console.log(this.val);\n\n    // const salt = bcrypt.genSaltSync(10);\n    // const pass = bcrypt.hashSync(this.val.password, salt);\n    //delete const if not working\n  }\n}\n```\n\nWe are required to use BCrypt for our project in university. I would like to know how to console.log or to check if the password that is already hashed.\nI already installed ```\nnpm install bcryptjs```\n and ```\nnpm install @types/bcrypt```\n\nThank you!\n    ", "Answer": "\r\nAngular is a frontend framework, and hence can be spoofed by end users. You should NEVER EVER trust users' inputs, so Angular is NOT a place where you check the hash status of a password. Instead, you should do it in the server. The ```\nbcryptjs```\n library exposes ```\ncompare()```\n function which can be used to compare the plain password against hashed one. And if this functions returns ```\ntrue```\n, the given password is considered correct.\nExample:\n```\nconst bcrypt = require('bcryptjs');\nbcrypt.compare('<plain_password>', '<hashed_password>').then((isCorrect) => {\n  console.log(isCorrect); // should be true for correct password, false otherwise\n});\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Failed to fetch .... Hash Sum mismatch\r\n                \r\nEnvironment :\n- OS: Ubuntu 19.10\n- Docker Version:  19.03.8, build \n- Docker Compose Version: d 1.25.4,\n\nDocker Command Executed :\n\ndocker-compose -f development.yml build\n\nActual Result\n\nFailed to fetch http://archive.ubuntu.com/ubuntu/dists/xenial-updates/universe/source/by-hash/SHA256/f99962052ff8f88ecb0ca0a22b07aa069b984d346a0c9097b7d39f5ade9edb29 Hash Sum mismatch\n    ", "Answer": "\r\ni tried these two commands to resolve this issue\n```\nsudo rm -rf /var/lib/apt/lists/*\n\nsudo apt-get clean\n\nsudo sed -i -re 's/\\w+\\.archive\\.ubuntu\\.com/archive.ubuntu.com/g' /etc/apt/sources.list\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to organize log files from Jenkins builds?\r\n                \r\nCurrently I have a Jenkins job with console output that I would like to save to separate log files. I already know how to create the log files with UNIX commands, but I'm trying to figure out the best way to organize these files. Does Jenkins provide any sort of \"universal\" build number? I could use the $BUILD_NUMBER and $BUILD_TAG-$BUILD_ID, though I figured a hash or some other sort of label would be more durable. (What happens if past builds get deleted?)\n\nI could also label the log files based on the timestamp the Jenkins build was kicked off, but I'm unsure of how accurate it would be. Does Jenkins record a \"master time\" for when a build starts/ends?\n\nOther suggestions would be greatly appreciated, as well. How do you store your Jenkins log files?\n    ", "Answer": "\r\nI think $BUILD_TAG should be good enough, it's usually like \"Jenkins-$JOB_NAME-$BUILD_NUMBER\", of course you could append the UNIX timestamp to make it more unique, so it would become something like Jenkins-$JOB_NAME-$BUILD_NUMBER-1234567890.\n\nTo answer your 2 questions specifically :\n\n\nDoes Jenkins provide any sort of \"universal\" build number?\n\nIt is incremental automatically (still increase even if you delete the past builds) unless you reset it, so it could be regarded as universal in my opinion.\nDoes Jenkins record a \"master time\" for when a build starts/ends?\n\nJenkins records the build start time, you can find it in the build history.\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Problem with server-side rendering in angular: can not generate main.bundle.js\r\n                \r\nI started my application by client-side rendering and everything was ok. But now, when I try to compile and serve my application by server side rendering, I encountered some problems. \n\nFirst thing: When I run command \n```\nnpm run build:ssr && npm run serve:ssr```\n\nthe whole application can compile and run, And I see on the terminal communicate about serving this app on localhost:3000. I am going to localhost:3000 and see this:\n\n```\n    Error: You must pass in a NgModule or NgModuleFactory to be \n   bootstrapped\n    at View.engine (/home/ewelina/rohub-portal/portal- \n   angular/dist/server.js:196864:23)\n    at View.render (/home/ewelina/rohub-portal/portal- \n   angular/dist/server.js:46196:8)\n    at tryRender (/home/ewelina/rohub-portal/portal - \n   angular/dist/server.js:43892:10)\n    at Function.render (/home/ewelina/rohub-portal/portal- \n   angular/dist/server.js:43844:3)\n    at ServerResponse.render (/home/ewelina/rohub-portal/portal- \n   angular/dist/server.js:52737:7)\n    at /home/ewelina/rohub-portal/portal-angular/dist/server.js:149:9\n    at Layer.handle [as handle_request] (/home/ewelina/rohub- \n   portal/portal-angular/dist/server.js:45618:5)\n    at next (/home/ewelina/rohub-portal/portal- \n   angular/dist/server.js:45366:13)\n    at Route.dispatch (/home/ewelina/rohub-portal/portal- \n   angular/dist/server.js:45341:3)\n    at Layer.handle [as handle_request] (/home/ewelina/rohub- \n   portal/portal-angular/dist/server.js:45618:5)\n```\n\n\nI tried to learn about this problem and my conclusion is, my angular universal can not generate main.bundle.js on dist/server folder. I can not see any bundle files, and I think, this communicate appears, when angular can not generate html template because can not find AppServerModuleNgFactory on generated main.js from dist server. This is the only file I can add to my server.ts file.\n\nMy question is how can I generate proper main.bundle.js file on dist/server and start this app by server side rendering?\n\n```\nangular.json\n\n{\n  \"$schema\": \"./node_modules/@angular/cli/lib/config/schema.json\",\n  \"version\": 1,\n  \"newProjectRoot\": \"projects\",\n  \"projects\": {\n    \"ng-universal-demo\": {\n      \"root\": \"\",\n      \"sourceRoot\": \"src\",\n      \"projectType\": \"application\",\n      \"prefix\": \"app\",\n      \"schematics\": {},\n      \"architect\": {\n        \"build\": {\n          \"builder\": \"@angular-devkit/build-angular:browser\",\n          \"options\": {\n            \"outputPath\": \"dist/browser\",\n            \"index\": \"src/index.html\",\n            \"main\": \"src/main.ts\",\n            \"polyfills\": \"src/polyfills.ts\",\n            \"tsConfig\": \"src/tsconfig.server.json\",\n            \"assets\": [\n              \"src/assets\",\n              \"src/assets/favicon.ico\"\n            ],\n            \"styles\": [\n              \"src/bootstrap.min.css\",\n              \"src/styles.css\",\n              \"src/assets/custom.css\",\n              \"src/animate.min.css\"\n            ],\n            \"scripts\": []\n          },\n          \"configurations\": {\n            \"production\": {\n              \"optimization\": true,\n              \"outputHashing\": \"all\",\n              \"sourceMap\": false,\n              \"extractCss\": true,\n              \"namedChunks\": false,\n              \"aot\": true,\n              \"extractLicenses\": true,\n              \"vendorChunk\": false,\n              \"buildOptimizer\": true\n            }\n          }\n        },\n        \"serve\": {\n          \"builder\": \"@angular-devkit/build-angular:dev-server\",\n          \"options\": {\n            \"browserTarget\": \"ng-universal-demo:build\"\n          },\n          \"configurations\": {\n            \"production\": {\n              \"browserTarget\": \"ng-universal-demo:build:production\"\n            }\n          }\n        },\n        \"server\": {\n          \"builder\": \"@angular-devkit/build-angular:server\",\n          \"options\": {\n            \"outputPath\": \"dist-server\",\n            \"main\": \"src/main.server.ts\",\n            \"tsConfig\": \"src/tsconfig.server.json\"\n          }\n        },\n          \"test\": {\n            \"builder\": \"@angular-devkit/build-angular:karma\",\n            \"options\": {\n              \"main\": \"src/test.ts\",\n              \"polyfills\": \"src/polyfills.ts\",\n              \"tsConfig\": \"src/tsconfig.spec.json\",\n              \"karmaConfig\": \"src/karma.conf.js\",\n              \"styles\": [\n                \"src/styles.scss\",\n                \"src/theme.scss\"\n              ],\n              \"scripts\": [\n                \"node_modules/marked/lib/marked.js\"\n              ],\n              \"assets\": [\n                \"src/favicon.ico\",\n                \"src/assets\"\n              ]\n            }\n          },\n          \"lint\": {\n            \"builder\": \"@angular-devkit/build-angular:tslint\",\n            \"options\": {\n              \"tsConfig\": [\n                \"src/tsconfig.app.json\",\n                \"src/tsconfig.spec.json\"\n              ],\n              \"exclude\": [\n                \"**/node_modules/**\"\n              ]\n            }\n          }\n        }\n      }\n    }\n\n  }\n```\n\n\n```\npackage.json\n{\n  \"name\": \"ng-universal-demo\",\n  \"version\": \"0.0.0\",\n  \"license\": \"MIT\",\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"https://github.com/angular/universal-starter.git\"\n  },\n  \"contributors\": [\n    \"AngularClass <hello@angularclass.com>\",\n    \"PatrickJS <patrick@angularclass.com>\",\n    \"Jeff Whelpley <jeff@gethuman.com>\",\n    \"Jeff Cross <crossj@google.com>\",\n    \"Mark Pieszak <mpieszak84@gmail.com>\",\n    \"Jason Jean <jasonjean1993@gmail.com>\",\n    \"Fabian Wiles <fabian.wiles@gmail.com>\"\n  ],\n  \"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"ng serve\",\n    \"build\": \"ng build\",\n    \"build:client-and-server-bundles\": \"ng build --prod && ng build --prod  --output-hashing=none\",\n    \"build:static\": \"npm run build:client-and-server-bundles && npm run webpack:server && npm run generate:static\",\n    \"build:dynamic\": \"npm run build:client-and-server-bundles && npm run webpack:server\",\n    \"generate:static\": \"cd dist && node prerender\",\n    \"build:ssr\": \"npm run build:client-and-server-bundles && npm run webpack:server\",\n    \"serve:ssr\": \"node dist/server\",\n    \"webpack:server\": \"webpack --config webpack.server.config.js --progress --colors\",\n    \"serve:static\": \"cd dist/browser && http-server\",\n    \"serve:dynamic\": \"node dist/server\"\n  },\n  \"private\": true,\n  \"dependencies\": {\n    \"@agm/core\": \"^1.0.0-beta.5\",\n    \"@angular-devkit/build-angular\": \"^0.10.7\",\n    \"@angular/animations\": \"^6.1.10\",\n    \"@angular/cdk\": \"^7.3.7\",\n    \"@angular/common\": \"^6.1.10\",\n    \"@angular/compiler\": \"^6.1.10\",\n    \"@angular/core\": \"^6.1.10\",\n    \"@angular/flex-layout\": \"^7.0.0-beta.24\",\n    \"@angular/forms\": \"^6.1.10\",\n    \"@angular/http\": \"^6.1.10\",\n    \"@angular/platform-browser\": \"^6.1.10\",\n    \"@angular/platform-browser-dynamic\": \"^6.1.10\",\n    \"@angular/platform-server\": \"^6.1.10\",\n    \"@angular/router\": \"^6.1.10\",\n    \"@nguniversal/express-engine\": \"^6.1.0\",\n    \"@nguniversal/module-map-ngfactory-loader\": \"^6.1.0\",\n    \"@types/lodash\": \"^4.14.123\",\n    \"angular-in-memory-web-api\": \"^0.8.0\",\n    \"angular-tree-component\": \"8.0.0\",\n    \"animate.css\": \"^3.7.0\",\n    \"body-parser\": \"^1.18.3\",\n    \"bootstrap\": \"^4.3.1\",\n    \"cookie-parser\": \"^1.4.4\",\n    \"core-js\": \"^2.6.5\",\n    \"css-loader\": \"^1.0.1\",\n    \"express\": \"^4.15.2\",\n    \"font-awesome\": \"^4.7.0\",\n    \"jquery\": \"^3.3.1\",\n    \"mock-browser\": \"^0.92.14\",\n    \"moment\": \"^2.24.0\",\n    \"morgan\": \"^1.9.1\",\n    \"mydatepicker\": \"^1.10.2\",\n    \"ng2-cache\": \"^0.2.1\",\n    \"ng2-date-picker\": \"0.0.0\",\n    \"ng2-datetime-picker\": \"^0.9.10\",\n    \"ng2-nouislider\": \"^1.7.13\",\n    \"ngx-bootstrap\": \"^2.0.0-beta.9-1\",\n    \"ngx-cookie-service\": \"^1.0.10\",\n    \"nouislider\": \"^9.2.0\",\n    \"popper.js\": \"^1.14.7\",\n    \"postcss-loader\": \"^3.0.0\",\n    \"primeng\": \"^5.2.5\",\n    \"rxjs\": \"6.4.0\",\n    \"rxjs-compat\": \"^6.4.0\",\n    \"webpack-cli\": \"^3.3.5\",\n    \"xml2js\": \"^0.4.19\",\n    \"zone.js\": \"^0.8.29\"\n  },\n  \"devDependencies\": {\n    \"@angular/cli\": \"^6.2.9\",\n    \"@angular/compiler-cli\": \"^6.1.10\",\n    \"@angular/language-service\": \"^6.1.10\",\n    \"@nicky-lenaers/ngx-scroll-to\": \"^1.1.1\",\n    \"@types/file-saver\": \"0.0.1\",\n    \"@types/node\": \"^8.10.44\",\n    \"codelyzer\": \"0.0.28\",\n    \"cpy-cli\": \"^1.0.1\",\n    \"file-saver\": \"^1.3.8\",\n    \"http-server\": \"^0.10.0\",\n    \"jasmine-core\": \"2.4.1\",\n    \"jasmine-spec-reporter\": \"2.5.0\",\n    \"karma\": \"1.2.0\",\n    \"karma-chrome-launcher\": \"^2.2.0\",\n    \"karma-cli\": \"^1.0.1\",\n    \"karma-jasmine\": \"^1.1.2\",\n    \"karma-remap-istanbul\": \"^0.2.2\",\n    \"lodash\": \"^4.17.11\",\n    \"ng2-datepicker\": \"^2.3.1\",\n    \"ng2-slideable-directive\": \"1.0.13\",\n    \"ng2-slider-component\": \"1.0.9\",\n    \"ng2-styled-directive\": \"1.0.5\",\n    \"protractor\": \"4.0.9\",\n    \"reflect-metadata\": \"^0.1.10\",\n    \"rserv\": \"^1.1.3\",\n    \"ts-loader\": \"^4.0.0\",\n    \"ts-node\": \"1.2.1\",\n    \"tslint\": \"3.13.0\",\n    \"typescript\": \"^2.9.2\",\n    \"webpack\": \"^4.29.6\",\n    \"webpack-dev-server\": \"^3.2.1\"\n  }\n}\n```\n\n\n```\nserver.ts (piece of code)\nconst app = express();\n\nconst PORT = process.env.PORT || 3000;\nconst DIST_FOLDER = join(process.cwd(), 'dist');\n\nconst domino = require('domino');\nconst fs = require('fs');\nconst path = require('path');\n\nconst template = readFileSync(join(DIST_FOLDER, 'browser', 'index.html'))\n    .toString();\n\n\nconst MockBrowser = require('mock-browser').mocks.MockBrowser;\nconst mock = new MockBrowser();\nconst win = domino.createWindow(template);\nObject.assign(global, domino.impl);\n\nglobal['Event'] = null;\nglobal['KeyboardEvent'] = null;\nglobal['Element'] = {};\nglobal['Element'].prototype = {};\nglobal['Element'].prototype.remove = undefined;\n\nglobal['navigator'] = mock.getNavigator();\nglobal['window'] = mock.getWindow();\nglobal['document'] = mock.getDocument();\nglobal['localStorage'] = global['window'].localStorage;\n\n//here I add dist/server/mian because I hav an error that file doesn't exist during building\nconst { AppServerModuleNgFactory, LAZY_MODULE_MAP } = require('./dist/server/main');\n\n\nimport { ngExpressEngine } from '@nguniversal/express-engine';\n\nimport { provideModuleMap } from '@nguniversal/module-map-ngfactory-loader';\n\n\napp.engine('html', ngExpressEngine({\n  bootstrap: AppServerModuleNgFactory,\n  providers: [\n    provideModuleMap(LAZY_MODULE_MAP)\n  ]\n}));\n\napp.set('view engine', 'html');\napp.set('views', join(DIST_FOLDER, 'browser'));\n```\n\n\n```\nprerender.ts (piece of code)\nimport { provideModuleMap } from '@nguniversal/module-map-ngfactory-loader';\n\n//the same situation as on server.ts\nconst { AppServerModuleNgFactory, LAZY_MODULE_MAP } = require('./dist/server/main');\nconst PATHS = require('./static.paths');\n\nconst BROWSER_FOLDER = join(process.cwd(), 'browser');\nconst index = readFileSync(join('browser', 'index.html'), 'utf8');\n\n\nPATHS.forEach(function (route) {\n  chdir(BROWSER_FOLDER);\n\n  route.split('/').filter(val => val !== '')\n    .forEach(function (dir) {\n      if (!existsSync(dir)) {\n        mkdirSync(dir);\n      }\n      chdir(dir);\n    });\n\n  renderModuleFactory(AppServerModuleNgFactory, {\n    document: index,\n    url: route,\n    extraProviders: [\n      provideModuleMap(LAZY_MODULE_MAP)\n    ]\n  }).then(html => writeFileSync(join(BROWSER_FOLDER, route, 'index.html'), html));\n});\n```\n\n    ", "Answer": "\r\n```\n imports: [\n      ScrollToModule.forRoot(),\n      BrowserAnimationsModule,\n      BrowserModule.withServerTransition({appId: 'ng-angular-portal'}),\n      TreeModule,\n      ButtonModule,\n      MyDatePickerModule,\n      BsDropdownModule,\n      ButtonsModule,\n      TypeaheadModule.forRoot(),\n      PaginationModule.forRoot(),\n      AlertModule,\n      DatepickerModule,\n      CollapseModule,\n      AccordionModule,\n      ReactiveFormsModule,\n      BrowserTransferStateModule,\n      FormsModule,\n    ModuleMapLoaderModule,\n      //ServerTransferStateModule,\n      HttpClientModule,\n      //ServerModule,\n      RouterModule.forRoot(ROUTES),\n    AgmCoreModule.forRoot({\n      apiKey: \"AIzaSyCiNA4333VvLGmOtKvbg7y7gZ9ohe81hNI\",\n    }),\n  ],\n.....\n\nexport class AppModule {\n  constructor(private injector: Injector) {}\n\n    public ngDoBootstrap() {\n    if (isPlatformBrowser('my-app')) {\n      const {createCustomElement} = require('@angular/elements');\n\n        const AppElement = createCustomElement(AppComponent, {injector: this.injector });\n        customElements.define('my-app', AppElement);\n      }\n\n    }\n\n\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "using an iterator of a map<char,strring> to return a pointer map<char,strring>*\r\n                \r\nIm working on a university assignment hence the pedantic nature of my question.\n\nThe assignment is to implement a hash table, of size 26. The hash function is simply the last letter of the word {```\nhash(apple) = e, hash(car) = r```\n}. I have chosen to use a ```\nmap<char, string>```\n char for each letter of the alphabet (hash) and string to store the input words (key).\n\nI want to search the map and return a pointer to the position where a new key needs to be added. The search aspect I have working its the pointer I am having trouble with. any assistance would be much appreciated.\n\n```\nmap<char,string>* search(map<char,string>* hashTable, string key, char hashVal){\n\n    map<char,string>::iterator it;\n    for(it = hashTable->begin(); it != hashTable->end(); ++it){\n        if(I want to return this address of hashTable){\n            return &it->hashTable || something to that effect \n        }\n    }\n)\n```\n\n\nThe reason I want to do this is to fulfil the following use case in an add() or remove().\n\n```\nvoid remove||add(map<char,string>* hashTable, string key){\n    map<char, string>* temp = search(hashTable, key, hashVal);\n    temp->second == NULL||key;\n}\n```\n\n\nNOTE:\nI understand that the map library has functions like at() and find() however using them wont prove my understanding hash tables. This is pedantic nature of university data structures assignments 2020 I was referring to.\n\nps. let me know if you think there is a better library to base my hash table off. linked lists maybe?\n    ", "Answer": "\r\nOk, so I think you are attacking this the wrong way. ```\nstd::map```\n is already a fully fledged, hash-table-like container (cringe, yes I know it is not really, don't hurt me!). Using it will most likely lead you astray from what you should learn from this exercise.\n\n```\nstd::vector```\n is your friend.   If you have your ASCII table ready, you should be able to formulate a good hash function: ```\nhash(val) -> Index```\n.\n\nThe example might strike you as pedantically defined, but it will make all the properties of hash tables strikingly obvious (like in your face kind of obvious).\n\nAh, and to answer your question: std::next has a good overload for getting a pointer/iterator to an element.\n\nedit Addendum:\n\n(hint) The killer feature of hash tables would probably get lost when using linked lists...\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Password protection for php 5.3.3?\r\n                \r\nI'm doing an assignment for uni and I've been following guides as far as finding a way to hash a registered password onto the mysqli database but it seem's the university's myphp is only on 5.3.3 and MySQL 5.1.73.\n\nWhat can I use to hash it instead of using 5.5's password_hash() function? Don't suppose there's a handy tutorial out there for it?\n\nMany thanks!\n    ", "Answer": "\r\nYour best answer:\n\nHave them upgrade to PHP 5.5 or higher and use password_hash() with a high work factor.\n\nPoint them to Thomas Pornin's canonical Security.stackexchange answer to How to securely hash passwords? to let him help argue the case for good password security.\n\nYour next best answer:\n\nHave them upgrade to PHP 5.3.7 or higher and use the password_hash() compatibility pack\n\nSee above.\n\nYour not as good answer:\n\nYou can use crypt on your current PHP 5.3.3 version reasonably IF you change some of the options:\n\n```\ncrypt('password', '$6$rounds=150000$PerUserCryptoRandomSalt$')\n```\n\n\n\n$6 - use SHA-512, which has 64-bit operations that reduce the margin of advantage most GPU based attackers have over you as of early 2016.\n$rounds=150000 - set the number of iterations to hundreds of thousands or high tens of thousands of rounds.\nPerUserCryptoRandomSalt - unlike password_hash, you have to do this yourself.  You need to generate a unique, cryptographically random salt of 12-24 binary bytes (16 is very reasonable)\n\n\nNote that it's part of the result string, in cleartext, which is correct.\nThat's binary bytes!  The size in the crypt() function gets doubled if you convert to hex, or increased by 4/3rds if you Base64 it\n\n\n\nTo compare, you get the user's salt and number of rounds, and use crypt with those on the candidate password entered.  If you get the same answer, it's the same password.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Best way to debug the source-code of typescript dependencies? Npm link? How to remap `/src`?\r\n                \r\nI often find myself cross referencing the (un-transpiled) typescript code of my dependencies, and I would find it very useful to be able to step through the original (non-transpiled) source code with comments.\nIdeally, I'd like to have as universal of a process as possible.\nMy steps so far are:\n\n```\nnpm version <package>```\n and ```\nnpm repo <package>```\n\nClone the repo outside the project, and checkout the hash tagged with my version\n```\nnpm link <cloned-package-path>```\n inside of my repo.\n\nNow, the only problem I'm facing is that with some packages, the path for modules is not the same in the repo as it is in the built-and-published package. Often, I would need to replace ```\n<dependency>/<path>```\n with ```\n<dependency>/src/<path>```\n everywhere in my code.\nSo my questions are:\n\nIs there an easier way of doing this whole process? Installing source with npm, or some cli tool that does more of this work for me? Surely this is a common need.\nIs it possible to configure my local environment (I'm using next.js) to do this ```\n/```\n->```\n/src/```\n mapping for me?\n\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Serialization in Haskell\r\n                \r\nFrom the bird's view, my question is: Is there a universal mechanism for ```\nas-is```\n data serialization in Haskell?\nIntroduction\nThe origin of the problem does not root in Haskell indeed. Once, I tried to serialize a python dictionary where a hash function of objects was quite heavy. I found that in python, the default dictionary serialization does not save the internal structure of the dictionary but just dumps a list of key-value pairs. As a result, the de-serialization process is time-consuming, and there is no way to struggle with it. I was certain that there is a way in Haskell because, at my glance, there should be no problem transferring a pure Haskell type to a byte-stream automatically using BFS or DFS. Surprisingly, but it does not. This problem was discussed here (citation below)\n\nCurrently, there is no way to make HashMap serializable without modifying the HashMap library itself. It is not possible to make Data.HashMap an instance of Generic (for use with cereal) using stand-alone deriving as described by @mergeconflict's answer, because Data.HashMap does not export all its constructors (this is a requirement for GHC). So, the only solution left to serialize the HashMap seems to be to use the toList/fromList interface.\n\nCurrent Problem\nI have quite the same problem with ```\nData.Trie```\n bytestring-trie package. Building a trie for my data is heavily time-consuming and I need a mechanism to serialize and de-serialize this tire. However, it looks like the previous case, I see no way how to make ```\nData.Trie```\n an instance of Generic (or, am I wrong)?\nSo the questions are:\n\nIs there some kind of a universal mechanism to project a pure Haskell type to a byte string? If no, is it a fundamental restriction or just a lack of implementations?\n\nIf no, what is the most painless way to modify the bytestring-trie package to make it the instance of Generic and serialize with ```\nData.Store```\n\n\n\n    ", "Answer": "\r\n\nThere is a way using compact regions, but there is a big restriction:\n\n\nOur binary representation contains direct pointers to the info tables of objects in the region. This means that the info tables of the receiving process must be laid out in exactly the same way as from the original process; in practice, this means using static linking, using the exact same binary and turning off ASLR. This API does NOT do any safety checking and will probably segfault if you get it wrong. DO NOT run this on untrusted input.\n\nThis also gives insight into universal serialization is not possible currently. Data structures contain very specific pointers which can differ if you're using different binaries. Reading in the raw bytes into another binary will result in invalid pointers.\nThere is some discussion in this GitHub issue about weakening this requirement.\n\nI think the proper way is to open an issue or pull request upstream to export the data constructors in the internal module. That is what happened with ```\nHashMap```\n which is now fully accessible in its internal module.\n\nUpdate: it seems there is already a similar open issue about this.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Transferring a file from one host to another and checking its integrity\r\n                \r\nI've a university assignment to create a server-client pair, where the client can transfer a file to server. Also there should be an integrity check to see if the file is not corrupt. I've implemented the actual file transfer, using the ```\nDigestInputStream```\n and ```\nDigestOutputStream```\n classes, that calculate the hash code of the file at transfer time. Now my problem is to send the hash calculated by the client to server, where the server must compare it to the hash calculated by server. I need an idea of some kind of protocol to send the actual file data, its hash code and file name. Thanks in advance.\n    ", "Answer": "\r\nIt is proabably easiest to implement using the Socket API. \n\nThen you can take the following steps:\n\n\nConnect to the server from the client;\nHave the client send the calculated hash;\nServer validates the hash against its calculated one;\nServer sends back a \"success\" message to the client.\n\n\nThere are plenty of tutorials available on Socket and how to setup basic client-server communication.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Error in 2 files when checking disk\r\n                \r\nI used the media creating tool from here to delete all my partitions and install Windows on my computer.\n\nWhen I boot from an Ubuntu LiveUSB and use the option to check disk for errors, it says that there are errors in 2 files.\nIt does not give any details about what files or what errors.\n\nI checked the hash of the iso before installing it onto my USB drive and it was correct. I used the Universal USB Installer to install the iso onto my USB.\n\nWhen I use the option to try Ubuntu before installing it and use GParted, I get this error.\nWhen I click cancel, it has three drives that it detects, two of them are the 60GB storage drives that make up my RAID system, and the other is my Ubuntu USB.\n\nIt does not detect the virtual drive that my RAID system creates.\n\nWhat could be the problem?\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Proving a perfect hash function over a fixed length input\r\n                \r\nI have seen the answers on here stating to use gperf, however, I would prefer to roll my own based on the proof that I create for the domain of ```\nstrings```\n with a fixed length of ```\n<= 200```\n  Based on the calculations I have from wolfram I get ```\n~7.9 x 10^374```\n total permutations.  Therefore my line of thinking is if I have a ```\n2048```\n bit hash function (```\n3.2 x 10^616```\n) I should be able to handle the entire universe of strings that I need to process. My question is how can I prove that the hash implementation I end up producing will be perfect given the constraint of the universe of all strings of length 200 or less? \n    ", "Answer": "\r\nStrings with a length of 200 characters only have 200 * 8 = 1600 bits. If a 2048 bit hash is OK for your purpose, you could just use the string bits as a perfect hash. The identity hash function is perfect, as it maps each input to a distinct hash value (obviously, because there is no mapping).\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Why would OpenSSL be returning a different SHA1 hash output in my terminal?\r\n                \r\nI'm trying to hash a fairly small value using SHA1 for a university excercise.\n\nI'm running OpenSSL 1.1.1  11 Sep 2018. Operating System is Ubuntu 18.04.1, running through Windows Subsystem for Linux 1. \n\nRunning any of the following;\n\n```\necho \"361448504617\" | openssl dgst -SHA1\necho 361448504617 | openssl dgst -SHA1\nopenssl dgst -sha1 hash.txt\nopenssl SHA1 hash.txt\n```\n\n\nReturns:\n\n```\n(stdin)= f98a0e600cd960f6c414343748a8dabc5ae9ec0a\n(stdin)= f98a0e600cd960f6c414343748a8dabc5ae9ec0a\nSHA1(hash.txt)= f98a0e600cd960f6c414343748a8dabc5ae9ec0a\nSHA1(hash.txt)= f98a0e600cd960f6c414343748a8dabc5ae9ec0a\n\n```\n\n\nIf I go to an online SHA1 hash generator, such as https://passwordsgenerator.net/sha1-hash-generator/, it returns:\n\n```\nA599EBBA6735313C848118F6EDB63012163D7581\n```\n\n\nWhich is also the answer to the worksheet, and also what the labratory instructors terminal returns.\n\nCan anyone give me a hand in troubleshooting this?\n    ", "Answer": "\r\nAnnnd, I figured it out.\n\nOpenSSL was hashing the newline character also, pretty easy to solve using the -n argument for echo.\n\n```\necho -n 361448504617 | openssl SHA1\n```\n\n\nAlso, when OpenSSL was reading from file, I got the same error because vim was saving with an end of line character. Fixed by running the following commands inside vim:\n\n```\n:set binary \n:set noeol \n:wq\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "What is the best way to implement a Hash rotation method in C++?\r\n                \r\nI'm trying to understand hash tables and hash methods (such as division and multiplication), and I was wondering how to implement a rotation method when keys are serial. Say I had a function named ```\nint rotation(int value)```\n and I had a integer like ```\nint test = 123456```\n, and I wanted to get the result 612345. What would be the best way to implement this universally (lets say with a text file of 1000 integer groups)?\n    ", "Answer": "\r\n\n\n```\nint rotate(int value)\n{\n    bool isNegative = value < 0;\n    if(isNegative)\n        value = -value;\n    int v = value % 10;\n    value /= 10;\n    while(v < value)\n        v *= 10;\n    value += v;\n    return isNegative ? -value : value;\n}\n```\n\n\nThis will rotate your numbers according to their number of ciphers (123 gets 312, 1234 gets 4123).\n\nFor the file part: You would have to open it, read the values, convert them to int and call the function:\n\n```\n::std::ifstream in(\"filename\");\nif(in.fail())\n{\n    // error\n}\nelse\n{\n    int n;\n    for(;;)\n    {\n        in >> n;\n        if(in.eof() || in.fail())\n            break;\n\n        ::std::cout << rotate(n) << ::std::endl;\n    }\n    in.close();\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Should a utlity class be instantiated?\r\n                \r\nI am currently working on a new project for university and was curious as to the best way to handle a class I've created to perform utility operations such as hashing passwords.\n\nShould the utility class contain static methods so that I call them like \n\n```\nUtilities.hashPassword(password,salt); \n```\n\n\nOr should I create a new instance for each call\n\n```\nnew Utilities().hashPassword(password, salt);\n```\n\n\nRight now I have a new instance for each call to a function inside that class, but im concerned about the performance implications of this and am wondering if its even nessecary to do. \n\nMy original reason for instantiating them was because I wasn't sure how thread-safety worked and was concerned that multiple users calling the same static function would cause problems. After reading some material on java concurrency I'm now pretty sure that even if the method is static it would be thread-safe.\n\nShould I change them all to static methods? Would this improve performance? Right now my test server buckles under load.\n\nThanks\n    ", "Answer": "\r\nThread-safety does not care if a method is static or a true member method.\nThread-safety cares about concurrent modification to data. So, if your method is updating some generic data structure, you are NOT thread-safe just by making it static.\n\nArguments against \"static\": anything that is static is very hard to mock within unit tests. So be really careful about making stuff static just for convenience. \n\nRegarding the performance aspect: object creation is very cheap in java (not completely free, but cheap). In your case - you could keep it a member method - just avoid to throw away your utility object all the time.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Checking database table for inserted, updated, or delete rows through a c# console application?\r\n                \r\nHere is the situation: \nThe c# application that I have developed uses store procedures to update xml results in another web application. The process that I use is through XML Diff and Patch tools through Microsoft. \n\nHere is example code: \n\n```\n        public void MainTask(string[] proc, string[] otpt)\n        {\n            try\n            {\n                sw = File.AppendText(loggingFolder + loggingFile);\n            }\n            catch (Exception ex)\n            {\n                e.CreateDirectoriesError(ex.Message, errorFolder + errorFile);\n            }\n            sw.WriteLine(ConfigurationManager.AppSettings[\"MainStart\"]);\n            sw.WriteLine(ConfigurationManager.AppSettings[\"Main1\"]);\n            try\n            {\n                for (int i = 0; i < proc.Length; i++)\n                {\n\n                    SqlCommand cmd = new SqlCommand(proc[i], setupConnection(conn));\n                    sw.WriteLine(ConfigurationManager.AppSettings[\"Main2\"] + proc[i]);\n                    cmd.CommandType = CommandType.StoredProcedure;\n                    rdr = cmd.ExecuteXmlReader();\n                    sw.WriteLine(ConfigurationManager.AppSettings[\"Main3\"] + proc[i] + ConfigurationManager.AppSettings[\"Main4\"]);\n                    rdr.Read();\n                    xml.Load(rdr);\n                    if (xml.InnerXml == ConfigurationManager.AppSettings[\"_blankString\"])\n                    {\n                        rdr.Close();\n                        closeConnection();\n                        continue;\n                    }\n                    sw.WriteLine(ConfigurationManager.AppSettings[\"Main5\"] + proc[i] + ConfigurationManager.AppSettings[\"Main6\"] + otpt[i]);\n                    xml.Save(sourceFolder + otpt[i]);\n                    sw.WriteLine(ConfigurationManager.AppSettings[\"Main7\"] + otpt[i] + ConfigurationManager.AppSettings[\"Main8\"] + sourceFolder + otpt[i]);\n                    rdr.Close();\n                    closeConnection();\n\n\n                    sw.WriteLine(ConfigurationManager.AppSettings[\"Main9\"]);\n                    XmlWriterSettings setWriter = new XmlWriterSettings();\n                    setWriter.Indent = true;\n                    writ = XmlWriter.Create(differenceFolder + otpt[i], setWriter);\n                    PatchDifference(GenerateDiffernce(sourceFolder + otpt[i], destinationFolder + otpt[i], writ, otpt[i]), destinationFolder + otpt[i]);\n\n                }\n            }\n            catch (Exception ex)\n            {\n                sw.WriteLine(ConfigurationManager.AppSettings[\"MainError\"] + ex.Message);\n                e.MainError(ex.Message, errorFolder + errorFile);\n            }\n            sw.WriteLine(ConfigurationManager.AppSettings[\"MainDone\"]);\n            sw.Close();\n        }\n```\n\n\nMy problem:\nI need to check the table that I am calling in my cmd to check whether or not any tables have been updated in the last 24 hours. If() the tables have not been updated or new ones inserted continue to the next table. Else run the rest of the procedure. \n\nI have checked out many possibilities to try to accomplish this by using either Hash or check-sum values. However I'm concerned about the reality of reliability with these methods of implementation. Another way I could approach this situation is looking at dates, however some of the tables have UpdatedDate columns and others don't. So I am looking for something universal.\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Selecting unique elements from a List in C#\r\n                \r\nHow do I select the unique elements from the list ```\n{0, 1, 2, 2, 2, 3, 4, 4, 5}```\n so that I get ```\n{0, 1, 3, 5}```\n, effectively removing all instances of the repeated elements ```\n{2, 4}```\n?\n    ", "Answer": "\r\n```\nvar numbers = new[] { 0, 1, 2, 2, 2, 3, 4, 4, 5 };\n\nvar uniqueNumbers =\n    from n in numbers\n    group n by n into nGroup\n    where nGroup.Count() == 1\n    select nGroup.Key;\n\n// { 0, 1, 3, 5 }\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Cannot find module error for interfaces in Angular Universal build\r\n                \r\nMy Angular 7 portfolio-app with ngUniversal is setup as per the official cli documentation. I have interfaces set up for incoming data and some common functions.\n\n```\n\"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"npm run serve:ssr\",\n    \"build\": \"npm run build:ssr\",\n    \"test\": \"ng test\",\n    \"lint\": \"ng lint\",\n    \"e2e\": \"ng e2e\",\n    \"compile:server\": \"webpack --config webpack.server.config.js --progress --colors\",\n    \"serve:ssr\": \"node dist/server\",\n    \"build:ssr\": \"npm run build:client-and-server-bundles && npm run compile:server\",\n    \"build:client-and-server-bundles\": \"ng build --prod && ng run my-portfolio:server:production\"\n}\n```\n\n\nInterface description and folder structure:\n\n\n\nComponent using the interface:\n\n\n\n\n\nProblem\n\nThe universal build fails while running ng run my-portfolio:server:production following with error as shown in the screenshot.\n\nCause: Interfaces and common functions which are not registered in any modules. They're simply imported in the components.\n\n\n\nFull error:\n\n\n  PS C:\\Acclaris\\Practice\\AngularPractice\\my-portfolio> npm run build\n  \n  my-portfolio@0.0.0 build\n  C:\\Acclaris\\Practice\\AngularPractice\\my-portfolio npm run build:ssr\n  \n  my-portfolio@0.0.0 build:ssr\n  C:\\Acclaris\\Practice\\AngularPractice\\my-portfolio npm run\n  build:client-and-server-bundles && npm run compile:server\n  \n  my-portfolio@0.0.0 build:client-and-server-bundles\n  C:\\Acclaris\\Practice\\AngularPractice\\my-portfolio ng build --prod &&\n  ng run my-portfolio:server:production\n  \n  Date: 2019-05-10T10:14:15.321Z Hash: 49ca0a31b14b3f2ef33f Time:\n  28138ms chunk {0} runtime.274b2b523ee7c9b8154c.js (runtime) 2.19 kB\n  [entry] [rendered] chunk {1} es2015-polyfills.c5dd28b362270c767b34.js\n  (es2015-polyfills) 56.4 kB [initial] [rendered] chunk {2}\n  main.c6958b8ee410ac47c079.js (main) 708 kB [initial] [rendered] chunk\n  {3} polyfills.8bbb231b43165d65d357.js (polyfills) 41 kB [initial]\n  [rendered] chunk {4} styles.3e7ad83f8f97d37ca687.css (styles) 61.6 kB\n  [initial] [rendered] chunk {5} 5.da6e7079542863691897.js () 1.92 kB \n  [rendered] chunk {6} 6.89f434210c8e48ff257a.js () 718 bytes \n  [rendered] chunk {7} 7.04704f420e92f76cb831.js () 5.2 kB  [rendered]\n  \n  Date: 2019-05-10T10:14:28.776Z Hash: a1b5d85a17ec5299c6a4 Time: 8780ms\n  chunk {main} main.js, main.js.map (main) 4.17 kB [entry] [rendered]\n  \n  ERROR in\n  src/app/shared/components/portfolio-builder/portfolio-builder.component.ts(2,27):\n  error TS2307: Cannot find module 'src/app/shared/interfaces'.\n  src/app/core/portfolio/portfolio.component.ts(2,27): error TS2307:\n  Cannot find module 'src/app/shared/interfaces'.\n  src/app/shared/components/portfolio-builder/body/body.component.ts(2,22):\n  error TS2307: Cannot find module 'src/app/shared/interfaces'.\n  src/app/shared/components/sections/education/education.component.ts(2,27):\n  error TS2307: Cannot find module 'src/app/shared/interfaces'.\n  src/app/shared/components/sections/education/education.component.ts(3,33):\n  error TS2307: Cannot find module\n  'src/app/shared/functions/function-library'.\n  src/app/shared/components/utilities/edulet/edulet.component.ts(2,27):\n  error TS2307: Cannot find module 'src/app/shared/interfaces'.\n  src/app/shared/components/portfolio-builder/footer/footer.component.ts(2,26):\n  error TS2307: Cannot find module 'src/app/shared/interfaces'.\n  src/app/shared/components/portfolio-builder/header/header.component.ts(2,24):\n  error TS2307: Cannot find module 'src/app/shared/interfaces'.\n  src/app/shared/components/sections/profile/profile.component.ts(2,25):\n  error TS2307: Cannot find module 'src/app/shared/interfaces'.\n  src/app/shared/components/portfolio-builder/sidebar/sidebar.component.ts(2,32):\n  error TS2307: Cannot find module 'src/app/shared/interfaces'.\n  src/app/shared/components/sections/sidebar-section/sidebar-section.component.ts(2,32):\n  error TS2307: Cannot find module 'src/app/shared/interfaces'.\n  src/app/shared/components/sections/work/work.component.ts(2,22): error\n  TS2307: Cannot find module 'src/app/shared/interfaces'.\n  src/app/shared/components/sections/work/work.component.ts(3,33): error\n  TS2307: Cannot find module\n  'src/app/shared/functions/function-library'.\n  src/app/shared/components/utilities/worklet/worklet.component.ts(2,22):\n  error TS2307: Cannot find module 'src/app/shared/interfaces'.\n  \n  npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! my-portfolio@0.0.0\n  build:client-and-server-bundles: ```\nng build --prod && ng run\n  my-portfolio:server:production```\n npm ERR! Exit status 1 npm ERR! npm\n  ERR! Failed at the my-portfolio@0.0.0 build:client-and-server-bundles\n  script. npm ERR! This is probably not a problem with npm. There is\n  likely additional logging output above.\n  \n  npm ERR! A complete log of this run can be found in: npm ERR!\n  C:\\Users\\debmallya.bhattachar\\AppData\\Roaming\\npm-cache_logs\\2019-05-10T10_14_28_865Z-debug.log\n  npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! my-portfolio@0.0.0\n  build:ssr: ```\nnpm run build:client-and-server-bundles && npm run\n  compile:server```\n npm ERR! Exit status 1 npm ERR! npm ERR! Failed at the\n  my-portfolio@0.0.0 build:ssr script. npm ERR! This is probably not a\n  problem with npm. There is likely additional logging output above.\n  \n  npm ERR! A complete log of this run can be found in: npm ERR!\n  C:\\Users\\debmallya.bhattachar\\AppData\\Roaming\\npm-cache_logs\\2019-05-10T10_14_28_983Z-debug.log\n  npm ERR! code ELIFECYCLE npm ERR! errno 1 npm ERR! my-portfolio@0.0.0\n  build: ```\nnpm run build:ssr```\n npm ERR! Exit status 1 npm ERR! npm ERR!\n  Failed at the my-portfolio@0.0.0 build script. npm ERR! This is\n  probably not a problem with npm. There is likely additional logging\n  output above.\n  \n  npm ERR! A complete log of this run can be found in: npm ERR!\n  C:\\Users\\debmallya.bhattachar\\AppData\\Roaming\\npm-cache_logs\\2019-05-10T10_14_29_044Z-debug.log\n\n    ", "Answer": "\r\nChange import path of Portfolio interface from the one starting with 'src/app/*' to the direct path. \nFor example, for your PortfolioBuilderComponent import should look like this:\n\n```\nimport { Portfolio } from '../../interfaces/portfolio'\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Optional PreBuildEvent in MSBuild?\r\n                \r\nIs it possible to have an optional ```\n<PreBuildEvent>```\n in a ```\n*.csproj```\n file? I have the following:\n\n```\n<PropertyGroup>\n  <PreBuildEvent>git rev-parse HEAD &gt;../../git-hash.txt</PreBuildEvent>\n</PropertyGroup>\n```\n\n\nThis outputs the latest git hash to a file, which is embedded in the executable elsewhere.\n\nSince I'm a University student, I'm often writing code on the University machines (and not my linux machine at home) which have SVN and not git, causing the build process to fail. Is  it possible to make the above ```\n<PreBuildEvent />```\n optional so that if git isn't installed the build process doesn't fail?\n    ", "Answer": "\r\nJust skipping the build event would leave you with an empty git-hash.txt so that doesn't seem the best idea. Instead you could just try to run the git command, and if it fails write a dummy hash to the file. I don't know the command line syntax to do that (a ```\nPreBuildEvent```\n runs under cmd.exe) so here's an msbuild solution. Because of the ```\nBeforeTargets=\"Build\"```\n it will run before the build as well.\n\n```\n<Target Name=\"WriteGitHash\" BeforeTargets=\"Build\">\n  <Exec Command=\"git --work-tree=$(Repo) --git-dir=$(Repo)\\.git rev-parse HEAD 2> NUL\" ConsoleToMSBuild=\"true\" IgnoreExitCode=\"True\">\n    <Output TaskParameter=\"ConsoleOutput\" PropertyName=\"GitTag\" />\n  </Exec>\n  <PropertyGroup>\n    <GitTag Condition=\"'$(GitTag)' == ''\">unknown</GitTag>\n  </PropertyGroup>\n  <WriteLinesToFile File=\"$(Repo)\\git-hash.txt\" Lines=\"$(GitTag)\" Overwrite=\"True\"/>\n</Target>\n```\n\n\nSome notes:\n\n\nThe ```\n2> NUL```\n redirects standard error to the output so ```\nGitTag```\n will be empty in case of an error, in which case it's set to 'unknown'\nRelying on the current directory is nearly always a bad idea so specify the directory to run git in explicitly in a property\nSame for the output file\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "What is a suitable lexer generator that I can use to strip identifiers from many language source files?\r\n                \r\nI'm working on a group project for my University which is going to be used for plagiarism detection in Computer Science.\n\nMy group is primarily going off the hashing/fingerprinting techniques described in this journal article: Winnowing: Local Algorithms for Document Fingerprinting.  This is very similar to how the MOSS plagiarism detection system works.\n\nWe are basically taking k-gram hashes of fellow students source code and looking them up in a database for relevant matches (along with lots of optimization in how we determine which hashes to select as a document's fingerprints).\n\nThe first aspect of our project is the \"Front-End\" portion of it, which will hold some semantic knowledge about each file format our detection system can process. This will allow us to strip some details from the document that we no longer want for the purpose of plagiarism detection.  Basically we want to be able to rename all variables in various programming languages to a constant string or letter.\n\nWhat is a lightweight solution (lexer generator or something similar) that we can use to aid in renaming all variables in different languages source code files to constants? \n\nOur project is being written in Java. \n\nIdeally I'd simply like to be able to define a grammar for each language and then our front end will be able rename all identifiers in that languages source file to some constant.  We would then do this for each file format we wanted to support (java, c++, python, etc).\n    ", "Answer": "\r\nFor a lexer/parser generator, you should look at ANTLR. TXL, which is a textual transformation interpreter, is also worth a look. Ready-made grammars should be available for both.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Speed up CI process in Gitlab\r\n                \r\nI am very new with Gitlab and I don't know how to set up it properly. I want to know how can we speed up the process CI in Gitlab because currently my project took up to 20m to finish the process of checking, building, and deploying.\nI believe the reason is because each job make another run for ```\nnpm install```\n or ```\nyarn install```\n. I defined the cache as follow but it didn't speed up the process:\n```\ncache:\n key: ${CI_COMMIT_REF_SLUG}\n paths:\n   - node_modules/\n```\n\nthe first image I am using is ```\ndocket:git```\n, should I change it to another image so I can run ```\nnpm install```\n in ```\nbefore_script```\n? Or there is other way to speed up gitlab ci process?\nEDIT:\nAdd gitlab-ci.yml file, I removed some sensitive information, basically, it is the same with the one I used\n```\nimage: docker:git\n\nstages:\n- build\n- build-image\n- build-staging\n- build-image-staging\n- build-production\n- build-image-production\n- release\n- checkstyle #TO move up\n- test #To move up\n- deploy\n\nvariables:\n  CONTAINER_IMAGE: registry\n  HOST: \"\"\n  IP: \"\"\n  DOCKER_DRIVER: overlay2\n\nbefore_script:\n  - git checkout -B \"$CI_COMMIT_REF_NAME\" \"$CI_COMMIT_SHA\"\n  - echo \"CI_COMMIT_REF_NAME = \"$CI_COMMIT_REF_NAME\n  - BRANCH=$(git rev-parse --abbrev-ref HEAD) && echo \"BRANCH = \"$BRANCH\n  - ID=$(git rev-list --count $BRANCH) && echo \"ID = \"$ID\n  - TAG=$(git describe --abbrev=0 --tags || true) && echo \"TAG = \"$TAG\n  - REGISTRY=$CONTAINER_IMAGE\":\"$ID\"_\"$BRANCH\n  - DATE=`date '+%Y-%m-%d %H:%M:%S'`\n  - echo $'\\n\\n----------\\n'\"REGISTRY = \"$REGISTRY$'\\n'\"COMMIT   = \"$CI_COMMIT_SHA$'\\n'\"BRANCH   = \"$BRANCH$'\\n'\"DATE     = \"$DATE$'\\n----------\\n\\n'\n#  - docker login -u gitlab-ci-token -p $CI_BUILD_TOKEN registry.example.com\n\ncache:\n key: ${CI_COMMIT_REF_SLUG}\n paths:\n   - node_modules/\n\nanalysis-lint:\n  image: node:latest\n  stage: checkstyle\n  script:\n    - npm install\n    - ./node_modules/@angular/cli/bin/ng lint --type-check\n\n\nbuild-integration:\n  stage: build\n  image: node\n  script:\n    - yarn install\n    - ./node_modules/@angular/cli/bin/ng build ---prod --configuration=integration --aot --output-hashing all --source-map=false\n    - npm run webpack:server\n  artifacts:\n    paths:\n      - dist\n  only:\n    - develop\n\nbuild-testing:\n  stage: build\n  image: node\n  script:\n    - yarn install\n    - ./node_modules/@angular/cli/bin/ng build ---prod --configuration=testing --aot --output-hashing all --source-map=false\n    - npm run webpack:server\n  artifacts:\n    paths:\n      - dist\n  only:\n    - /^release.*$/\n\nbuild-staging:\n  stage: build-staging\n  image: node\n  script:\n    - yarn install\n    - ./node_modules/@angular/cli/bin/ng build ---prod --configuration=staging --aot --output-hashing all --source-map=false\n    - npm run webpack:server\n  artifacts:\n    paths:\n      - dist\n  only:\n    - /^hotfix.*$/\n    - master\n\nbuild-production:\n  stage: build-production\n  image: node\n  script:\n    - yarn install\n    - ./node_modules/@angular/cli/bin/ng build ---prod --configuration=production --aot --output-hashing all --source-map=false\n    - npm run webpack:server\n  artifacts:\n    paths:\n      - dist\n  only:\n    - master\n\nbuild-image-integration:\n  stage: build-image\n  script:\n    - docker build -t $REGISTRY-integration -f Dockerfile --build-arg ENVIRONMENT=integration .\n    - docker push $REGISTRY-integration\n  only:\n    - develop\n    - universal\n\nbuild-image-testing:\n  stage: build-image\n  script:\n    - docker build -t $REGISTRY-testing -f Dockerfile --build-arg ENVIRONMENT=testing .\n    - docker push $REGISTRY-testing\n  only:\n    - /^release.*$/\n\nbuild-image-staging:\n  stage: build-image-staging\n  script:\n    - docker build -t $REGISTRY-staging -f Dockerfile --build-arg ENVIRONMENT=staging .\n    - docker push $REGISTRY-staging\n  only:\n    - /^hotfix.*$/\n    - master\n\nbuild-image-production:\n  stage: build-image-production\n  script:\n    - docker build -t $REGISTRY-production -f Dockerfile --build-arg ENVIRONMENT=production .\n    - docker push $REGISTRY-production\n  only:\n    - master\n\nrelease-image-integration:\n  stage: release\n  script:\n    - docker pull $REGISTRY-integration\n    - docker tag $REGISTRY-integration $CONTAINER_IMAGE:$BRANCH\n    - docker push $CONTAINER_IMAGE:$BRANCH\n  only:\n    - develop\n    - universal\n\nrelease-image-testing:\n  stage: release\n  script:\n    - docker pull $REGISTRY-testing\n    - docker tag $REGISTRY-testing $CONTAINER_IMAGE:$BRANCH\n    - docker push $CONTAINER_IMAGE:$BRANCH\n    - docker tag $REGISTRY-testing $CONTAINER_IMAGE:release\n    - docker push $CONTAINER_IMAGE:release\n  only:\n    - /^release.*$/\n\nrelease-image-staging:\n  stage: release\n  script:\n    - docker pull $REGISTRY-staging\n    - docker tag $REGISTRY-staging $CONTAINER_IMAGE:$BRANCH-staging\n    - docker push $CONTAINER_IMAGE:$BRANCH-staging\n  only:\n    - /^hotfix.*$/\n    - master\n\nrelease-image-master:\n  stage: release\n  script:\n    - docker pull $REGISTRY-production\n    - docker tag $REGISTRY-production $CONTAINER_IMAGE:$BRANCH\n    - docker push $CONTAINER_IMAGE:$BRANCH\n  only:\n    - master\n\nrelease-image-latest:\n  stage: release\n  script:\n    - docker pull $REGISTRY-production\n    - docker tag $REGISTRY-production $CONTAINER_IMAGE:latest\n    - docker push $CONTAINER_IMAGE:latest\n    - docker tag $REGISTRY-production $CONTAINER_IMAGE\n    - docker push $CONTAINER_IMAGE\n  only:\n    - master\n\nrelease-image-production:\n  stage: release\n  script:\n    - if [ ! -z \"$TAG\" ]; then docker pull $REGISTRY-production;docker tag $REGISTRY-production $CONTAINER_IMAGE:$TAG;docker push $CONTAINER_IMAGE:$TAG;fi;\n  only:\n    - master\n    #- /^release.*$/\n    #- /^hotfix.*$/\n\ndevelopment:\n  stage: deploy\n  image: appropriate/curl\n  script:\n    - echo \"Deploy to development server\"\n    - curl\n  environment:\n    name: development\n    url:\n  before_script: []\n  when: manual\n  only:\n    - develop\n\nintegration-universal:\n  stage: deploy\n  image: appropriate/curl\n  script:\n    - echo \"Deploy to integration server\"\n    - curl\n  environment:\n    name: integration\n    url:\n  before_script: []\n  when: manual\n  only:\n    - universal\n\nintegration:\n  stage: deploy\n  image: appropriate/curl\n  script:\n    - echo \"Deploy to integration server\"\n    - curl\n  environment:\n    name: integration\n    url:\n  before_script: []\n  when: manual\n  only:\n    - develop\n\ntesting:\n  stage: deploy\n  image: appropriate/curl\n  script:\n    - echo \"Deploy to testing server\"\n    - curl\n  environment:\n    name: testing\n    url:\n  before_script: []\n  when: manual\n  only:\n    - /^release.*$/\n\nstaging:\n  stage: deploy\n  image: appropriate/curl\n  script:\n    - echo \"Deploy to staging server\"\n    - curl\n  environment:\n    name: staging\n    url:\n  before_script: []\n  when: manual\n  only:\n    - /^release.*$/\n    - /^hotfix.*$/\n    - master\n\nproduction:\n  stage: deploy\n  image: appropriate/curl\n  script:\n    - echo \"Deploy to production server\"\n    - curl\n  environment:\n    name: production\n    url:\n  before_script: []\n  when: manual\n  only:\n    - master\n```\n\n    ", "Answer": "\r\nWithout having seen the ```\nDockerfile```\n, it is still relatively safe to safe to say that the majority of time is spent building docker images.\n\nWhy would you need to have 4 different stages that execute ```\ndocker build```\n? You should get this down to exactly one. You can still pull the built image in other stages for testing and integration. I see that you are building the containers with different ```\n--build-arg```\ns. However, what is the point of building a special image for testing? You should test your production image.\n\nAnother thing would be looking at parallelization. GitLab CI executes jobs of the same stage in parallel. Some of your jobs seem to not depend on the previous one. Why not execute them in the same stage?\n\nFurther, I do not fully understand your ```\nbefore_script```\n. Why do you need to ```\ngit checkout```\n? Gitlab CI will automatically checkout your current commit's branch.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Collision Attacks, Message Digests and a Possible solution\r\n                \r\nI've been doing some preliminary research in the area of message digests. Specifically collision attacks of cryptographic hash functions such as MD5 and SHA-1, such as the Postscript example and X.509 certificate duplicate.\n\nFrom what I can tell in the case of the postscript attack, specific data was generated and embedded within the header of the postscript file (which is ignored during rendering) which brought about the internal state of the md5 to a state such that the modified wording of the document would lead to a final MD value equivalent to the original postscript file. \nThe X.509 took a similar approach where by data was injected within the comment/whitespace sections of the certificate.\n\nOk so here is my question, and I can't seem to find anyone asking this question:\n\n\nWhy isn't the length of ONLY the data being consumed added as a final block to the MD calculation?\nIn the case of X.509 - Why is the whitespace and comments being taken into account as part of the MD?\n\n\nWouldn't a simple processes such as one of the following be enough to resolve the proposed collision attacks:\n\n\nMD(M + |M|) = xyz\nMD(M + |M| + |M| * magicseed_0 +...+ |M| * magicseed_n) = xyz\n\n\nwhere :\n\n\nM : is the message\n|M| : size of the message\nMD : is the message digest function (eg: md5, sha, whirlpool etc)\nxyz : is the pairing of the acutal message digest value for the message M and |M|. <M,|M|>\nmagicseed_{i}: Is a set of random values generated with seed based on the internal-state prior to the size being added.\n\n\nThis technqiue should work, as to date all such collision attacks rely on adding more data to the original message.\n\nIn short, the level of difficulty involved in generating a collision message such that:\n\n\nIt not only generates the same MD \nBut is also comprehensible/parsible/compliant \nand is also the same size as the original message, \n\n\nis immensely difficult if not near impossible. Has this approach ever been discussed? Any links to papers etc would be nice.\n\nFurther Question: What is the lower bound for collisions of messages of common length for a hash function H chosen randomly from U, where U is the set of universal hash functions ?\n\nIs it 1/N (where N is 2^(|M|)) or is it greater? If it is greater, that implies there is more than 1 message of length N that will map to the same MD value for a given H.\n\nIf that is the case, how practical is it to find these other messages? bruteforce would be of O(2^N), is there a method of time complexity less than bruteforce?\n    ", "Answer": "\r\nCan't speak for the rest of the questions, but the first one is fairly simple - adding length data to the input of the md5, at any stage of the hashing process (1st block, Nth block, final block) just changes the output hash. You couldn't retrieve that length from the output hash string afterwards. It's also not inconceivable that a collision couldn't be produced from another string with the exact same length in the first place, so saying \"the original string was 17 bytes\" is meaningless, because the colliding string could also be 17 bytes.\n\ne.g.\n\n```\nmd5(\"abce(17bytes)fghi\") = md5(\"abdefghi<long sequence of text to produce collision>\")\n```\n\n\nis still possible.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Tracking a file over time\r\n                \r\nMy idea is to track a specific file on a file-system over time between two points in time, ```\nT1```\n and ```\nT2```\n. The emphasis here lies on looking at a file as a unique entity on a file-system. One that can change in data and attributes but still maintain its unique identity.\n\nThe ultimate goal is to determine whether or not the data of a file has (unwillingly) changed between ```\nT1```\n and ```\nT2```\n by capturing and recording the data-hash and creation/modification attributes of the file at ```\nT1```\n and comparing them with the equivalents at ```\nT2```\n. If all attributes are unchanged but the hash doesn't validate we can say that there is a problem. In all other cases we might be willing to say that a changed hash is the result of a modification and an unchanged hash and unchanged modification-attribute the result of no change on the file(data) at all.\n\nNow, there are several ways to refer to a file and corresponding drawbacks:\n\n\nThe path to the file: However, if the file is moved to a different location this method fails.\nA data-hash of the file-data: Would allow a file, or rather (a) pointer to the file-data on disk, to be found, even if the pointer has been moved to a different directory, but the data cannot change or this method fails as well.\n\n\nMy idea is to retrieve a fileId for that specific file at ```\nT1```\n to track the file at ```\nT2```\n, even if it has changed its location so it doesn't need to be looked at as a new file.\n\nI am aware of two methods pywin offers. ```\nwin32file.GetFileInformationByHandle()```\n and ```\nwin32file.GetFileInformationByHandleEx()```\n, but they obviously are restricted to specific file-systems, break cross-platform-compatibility and sway away from a universal approach to track the file.\n\nMy question is simple: Are there any other ideas/theories to track a file, ideally accross platforms/FSs?\n\nAny brainstormed food for thought is welcome!\n    ", "Answer": "\r\nIt's not really feasible in general, because the idea of file identity is an illusion (similar to the illusion of physical identity, but this isn't a philosophy forum).\n\n\nYou cannot track identity using file contents, because contents change.\nYou cannot track by any other properties attached to the file, because many file editors will save changes by deleting the old file and creating a new one.\n\n\nVersion control systems handle this in three ways:\n\n\n(CVS) Don't track move operations.\n(Subversion) Track move operations manually.\n(Git) Use a heuristic to label operations as \"move\" operations based on changes to the contents of a file (e.g., if a new file differs from an existing file by less than 50%, then it's labeled as a copy).\n\n\nThings like inode numbers are not stable and not to be trusted.  Here, you can see that editing a file with Vim will change the inode number, which we can examine with ```\nstat -f %i```\n:\n\n\n$ touch file.txt\n$ stat -f %i file.txt\n4828200\n$ vim file.txt\n...make changes to file.txt...\n$ stat -f %i file.txt \n4828218\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Having trouble with Perl script that converts XML to hash\r\n                \r\nI have a Perl script to convert the XML file below into a hash:\n\n```\n<university>\n   <name>svu</name>\n  <location>ravru</location>\n <branch>\n  <electronics>\n <student name=\"xxx\" number=\"12\">\n <semester number=\"1\"subjects=\"7\" rank=\"2\"/>\n </student>\n <student name=\"xxx\" number=\"15\">\n <semester number=\"1\" subjects=\"7\" rank=\"10\"/>\n <semester number=\"2\" subjects=\"4\" rank=\"1\"/>\n  </student>\n   <student name=\"xxx\" number=\"16\">\n   <semester number=\"1\"subjects=\"7\" rank=\"2\"/>\n  <semester number=\"2\"subjects=\"4\" rank=\"2\"/>\n   </student>\n</electronics>\n  </branch>\n   </university>.\n          . \n          .\n          .\n          .\n          .\n<data>\n  <student name=\"msr\" number=\"1\" branch=\"computers\" />\n   <student name=\"ksr\" number=\"2\" branch=\"electronics\" />\n  <student name=\"lsr\" number=\"3\" branch=\"EEE\" />\n  <student name=\"csr\" number=\"4\" branch=\"IT\" />\n   <student name=\"msr\" number=\"5\" branch=\"MEC\" />\n  <student name=\"ssr\" number=\"6\" branch=\"computers\" />\n  <student name=\"msr\" number=\"1\" branch=\"CIV\" />\n  .............................\n   ..............................\n    .....................\n </data>\n```\n\n\nHow can I create a hash table for the data elements, with the name and number as the key and branch is the value in that hash. I need this because some students have the same name and some students have same number.\n\nBy using this hash key I have to search in the university node for student if found and print the branch name of each student.\n\nI written some script in ```\nXML::Simple```\n but am not able to create a hash.\n\n```\n #!/usr/bin/perl\n use warnings;\n use strict;\n use Data::Dumper; \n use XML::Simple;\n\n my $xml = new XML::Simple;\n my $data = $xml->XMLin(\"data.xml\", forcearray => [ 'student' , 'semister' ],\n                                    KeyAttr    => { student  => \"+Name\"  } );\n\n print Dumper($data);\n```\n\n\nby using data dumper I am printing hole xml information. but I need to print only Data Node elements only please help me how to do this.\n    ", "Answer": "\r\nI would probably write my own ```\nXML::Parser```\n handler to combine attributes into key values (if that's something supported by ```\nXML::Simple```\n I couldn't find it in the docs).  This example should get you started:\n\n```\n#!/usr/bin/perl\nuse strict;\nuse warnings;\nuse XML::Parser;\nuse Data::Dumper;\n\nmy %hash;\n\nsub tag_start { my ($expat, $tagname) = (shift, shift);\n    # attributes are now in @_\n    my %a = grep { $_=$_=>shift } @_; # attribute hash for this tag\n    my $context = join('/',$expat->context()) || '';\n\n    if ($context eq 'xml/data') {\n        if ($tagname eq 'student') {\n            push @{($hash{\"$a{name}:$a{number}\"}||=[])}, $a{branch};\n        }\n    } elsif ($context eq ...) {\n        ...\n    }\n}\nmy $p = new XML::Parser(Handlers => { Start=>\\&tag_start });\n$p->parsefile('file.xml');\nprint Dumper \\%hash;\n```\n\n\nNote that to get this to work I had to clean up your XML a bit by enclosing it in an ```\n<xml>```\n tag and adding some missing spaces:\n\n```\n<xml>\n    <university>\n        <name>svu</name>\n        <location>ravru</location>\n        <branch>\n            <electronics>\n                <student name=\"xxx\" number=\"12\">\n                    <semester number=\"1\" subjects=\"7\" rank=\"2\"/>\n                </student>\n                <student name=\"xxx\" number=\"15\">\n                    <semester number=\"1\" subjects=\"7\" rank=\"10\"/>\n                    <semester number=\"2\" subjects=\"4\" rank=\"1\"/>\n                </student>\n                <student name=\"xxx\" number=\"16\">\n                    <semester number=\"1\" subjects=\"7\" rank=\"2\"/>\n                    <semester number=\"2\" subjects=\"4\" rank=\"2\"/>\n                </student>\n            </electronics>\n        </branch>\n    </university>\n    <data>\n        <student name=\"msr\" number=\"1\" branch=\"computers\" />\n        <student name=\"ksr\" number=\"2\" branch=\"electronics\" />\n        <student name=\"lsr\" number=\"3\" branch=\"EEE\" />\n        <student name=\"csr\" number=\"4\" branch=\"IT\" />\n        <student name=\"msr\" number=\"5\" branch=\"MEC\" />\n        <student name=\"ssr\" number=\"6\" branch=\"computers\" />\n        <student name=\"msr\" number=\"1\" branch=\"CIV\" />\n    </data>\n</xml>\n```\n\n\nResult:\n\n```\n$VAR1 = {\n          'ksr:2' => [\n                     'electronics'\n                   ],\n          'msr:1' => [\n                     'computers',\n                     'CIV'\n                   ],\n          'csr:4' => [\n                     'IT'\n                   ],\n          'ssr:6' => [\n                     'computers'\n                   ],\n          'msr:5' => [\n                     'MEC'\n                   ],\n          'lsr:3' => [\n                     'EEE'\n                   ]\n        };\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "What should I do to protect user information as non-company? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 9 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI am just a developer who is personally developing some websites (on a donation basis) on a cheap (but good) standard web hosting. I'm also taking Computer Science on university, so definitely I do care about security for example.\n\nSo I have a website where users can create accounts and they are stored in the database, and I want it to be as secure as possible without spending money on security. The server is just running PHP and a MySQL database on an Apache web server.\n\nThe practices I know I am able to do:\n\n\nUse complex database usernames and passwords.\nStore the password in combination with a hashing algorithm (preferably not MD5) and a salt.\nAlso not unimportant, use complex FTP usernames and passwords.\n\n\nPossibly flaws I know exist:\n\n\nThe database is 'accessible' via the internet, namely ```\nwww.host.tld/phpMyAdmin```\n, it still wants the username and password as input though. I do have access to ```\n.htaccess```\n files.\nIf someone hacks the database, they can get valuable information, namely email addresses.\n\n\nWhat else is there more that I can do, to do the best I can?\n    ", "Answer": "\r\nFor PHPMyAdmin\n\n\nChange the default admin user\nLock the access page to PHPMyAdmin using .htaccess\nDo not call the folder PHPMyAdmin\nUse virtual directory settings in Apache to prevent browsing of directories, or have dummy index.php in all folders\n\n\nMySQL Security\n\nFor MySQL security, create a new database user with the minimal security rights for your application to work - in cases when your PHP scripts are comprised, the damage can be contained.\n\nPHP\n\nCross site scripting and MySQL injection are still common issues too - there are various PHP libraries (free!) which helps to sanitize input. \n\nIf you do allow user's uploads, set the permissions for those folders to 755, and ensure PHP can read such folders - some web hosts insist folders to have permission 777 before PHP can write to them. Avoid such hosts at all costs.\n\nIf you really wish to go an extra mile, you can encrypt sensitive information as a deterrent, though the attacker probably has all the time in the world to crack those info. \n\nConsider using PHP frameworks that would have done the legwork for basic security, such as sanitizing of POST, GET and etc.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Recover password via MySQL (phpmyadmin)?\r\n                \r\nI have a script which uses this shop codecanyon.net/item/universal-digital-shop/2987770, however I have recently changed my password via it's admin panel front end(usually based on e.g. yoursite.com/settings.php) and did a silly move not check Notepad really saved my new password before closing it.\n\nNow I cannot login to my shop at all and need help folks.\n\nI check in phpmyamdin and it seems like its a hashed password (is it md5?), and I just don't know how these work.\n\nI can post chunks of code and I have root access to mysql and phpmyadmin (Im more comfortable via phpmyadmin though and not a shell).\n\nI don't want to populate the whole DB again as this will loose my customer records.\n\nAny ideas?\n\nThe code below includes a password and an login ID. But the password \"346g3643832823h2h39236323f263827\" and login ID \"admin\" are only used for example purposes and are not the actual live pieces of data in the database.\n\n```\n<?php\n/** DO NOT MODIFY OPTIONS BELOW. YOU CAN MODIFY THEM VIA ADMIN PANEL. */\ndefine('VERSION', '2.50');\ndefine('RECORDS_PER_PAGE', '50');\ndefine('DEMO_MODE', false);\ndefine('STATUS_DRAFT', 0);\ndefine('STATUS_ACTIVE', 1);\ndefine('STATUS_PENDING', 7);\ndefine('ABSPATH', dirname(dirname(__FILE__)));\n\n$options = array (\n\"version\" => VERSION,\n\"owner_email\" => \"alerts@\".str_replace(\"www.\", \"\", $_SERVER[\"SERVER_NAME\"]),\n\"from_name\" => \"Universal Digital Shop\",\n\"from_email\" => \"noreply@\".str_replace(\"www.\", \"\", $_SERVER[\"SERVER_NAME\"]),\n\"success_email_subject\" => \"Thank you for payment\",\n\"success_email_body\" => \"Dear {payer_name},\".PHP_EOL.PHP_EOL.\"Thank you for your purchasing \\\"{file_title}\\\". Please find download link below:\".PHP_EOL.\"{download_link}\".PHP_EOL.\"This link is valid {download_link_lifetime} days.\".PHP_EOL.PHP_EOL.\"Thanks,\".PHP_EOL.\"Universal Digital Shop\",\n\"failed_email_subject\" => \"Payment was not completed\",\n\"failed_email_body\" => \"Dear {payer_name},\".PHP_EOL.PHP_EOL.\"Thank you for your payment. Unfortunately, it was not completed.\".PHP_EOL.\"Payment status: {payment_status}.\".PHP_EOL.\"We will review your payment shortly.\".PHP_EOL.PHP_EOL.\"Thanks,\".PHP_EOL.\"Universal Digital Shop\",\n\"csv_separator\" => \";\",\n\"link_lifetime\" => \"2\",\n\"xsendfile\" => \"off\",\n\"enable_paypal\" => \"off\",\n\"paypal_id\" => \"\",\n\"paypal_sandbox\" => \"off\",\n\"enable_payza\" => \"off\",\n\"payza_id\" => \"\",\n\"payza_sandbox\" => \"off\",\n\"enable_interkassa\" =>\"off\",\n\"interkassa_shop_id\" => \"\",\n\"interkassa_currency\" => \"USD\",\n\"interkassa_secret_key\" => \"\",\n\"enable_authnet\" => \"off\",\n\"authnet_login\" => \"\",\n\"authnet_sandbox\" => \"off\",\n\"authnet_key\" => \"\",\n\"authnet_md5hash\" => \"\",\n\"enable_skrill\" => \"off\",\n\"skrill_id\" => \"\",\n\"skrill_secret_word\" => \"\",\n\"enable_egopay\" => \"off\",\n\"egopay_store_id\" => \"\",\n\"egopay_store_pass\" => \"\",\n\"enable_perfect\" => \"off\",\n\"perfect_account_id\" => \"\",\n\"perfect_payee_name\" => \"\",\n\"perfect_passphrase\" => \"\",\n\"enable_bitpay\" => \"off\",\n\"bitpay_key\" => \"\",\n\"bitpay_speed\" => \"medium\",\n\"enable_stripe\" => \"off\",\n\"stripe_secret\" => \"\",\n\"stripe_publishable\" => \"\",\n\"login\" => \"admin\",\n\"password\" => \"346g3643832823h2h39236323f263827\"\n);\n$paypal_currency_list = array(\"USD\", \"AUD\", \"BRL\", \"CAD\", \"CHF\", \"CZK\", \"DKK\", \"EUR\", \"GBP\",\n\"HKD\", \"HUF\", \"ILS\", \"JPY\", \"MXN\", \"MYR\", \"NOK\", \"NZD\", \"PHP\", \"PLN\", \"SEK\", \"SGD\", \"THB\", \"TRY\",   \n\n\"TWD\"););\n$interkassa_currency_list = array(\"USD\", \"EUR\", \"GBP\", \"RUR\", \"UAH\");\n$egopay_currency_list = array(\"USD\", \"EUR\");\n$perfect_currency_list = array(\"USD\", \"EUR\");\n$bitpay_currency_list = array(\"USD\", \"EUR\", \"GBP\", \"AUD\", \"CAD\", \"CHF\", \"CNY\", \"RUB\", \"DKK\", \"HKD\", \"PLN\", \"SGD\", \"THB\", \"BTC\");\n$stripe_currency_list = array(\"USD\", \"CAD\");\n?>\n```\n\n    ", "Answer": "\r\nDrop all tables in DB and repopulated them under same DB. Didn't need to worry too much about customer records anymore since Paypal held all my transaction records.\n\n;)\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Representing a truth-table\r\n                \r\nI have the following data which I would like to represent efficiently in C. Retrieval time is favoured over memory consumption, yet I would be happy not to drain all my memory.\n\nInput values are sequences l_1, ..., l_n of logical values ```\n0```\n, ```\n1```\n or ```\n?```\n (undef). Each sequence corresponds to a sequence of outputs l_1, ..., l_m of logical values (from the same universe).\n\nThe data is sparse. (The input-output mapping is only given for some sequences of inputs and implicit rules apply otherwise.)\n\nI considered:\n\n\nUsing hashing with good worst-case bounds. But that's a little unpleasant as I would have to hash the input sequence. For input sequences sufficiently long, this will deplete any (unsigned) integer quickly.\nThe use of trie, but then, if I was given a few dense tables, I would end up doing tons of memallocs and fragmenting the heap. And it probably wouldn't cache well.\n\n\nYet another unpleasant fact is that I have to remember the output sequence, which is variable in length. In the program, several instances of the data structure being discussed exist, each of which has different m and n values.\n\nYou may think of this problem as representing the truth-table for a propositional formula, except the output of the formula is multi-valued, the logic is ternary and the formula is unknown.\n\nAny ideas how to do this?\n\nThanks!\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "File truncate operation in Java\r\n                \r\nWhat is the best-practice way to truncate a file in Java? For example this dummy function, just as an example to clarify the intent:\n\n```\nvoid readAndTruncate(File f, List<String> lines)\n        throws FileNotFoundException {\n    for (Scanner s = new Scanner(f); s.hasNextLine(); lines.add(s.nextLine())) {}\n\n    // truncate f here! how?\n\n}\n```\n\n\nThe file can not be deleted since the file is acting as a place holder.\n    ", "Answer": "\r\nUse FileChannel.truncate:\n\n```\ntry (FileChannel outChan = new FileOutputStream(f, true).getChannel()) {\n  outChan.truncate(newSize);\n}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Laravel 9 is rejecting a valid password hashed with bcrypt\r\n                \r\nI've been spending a few days troubleshooting a failure of certain passwords to validate in Laravel 9. The password ```\ntestperson```\n resolves to the hash ```\n$2y$10$5xc/wAmNCKV.YhpWOfyNoetCj/r3Fs5TyAskgZuIF/LEItWfm7rPW```\n. A direct query on the corresponding database table confirms that this is the correct hash. Yet Laravel's authentication infrastructure rejects this password and denies authentication.\nThis is not universal. I have multiple passwords that are resolving correctly. For example, the password ```\neo```\n resolves to ```\n$2y$10$uNWYvMVmagIwQ2eXnVKLCOAK1QFQdcRtxbvlghf.Xpg0U1w.N./N2```\n, and Laravel authenticates that password. The same mechanism creates both of these user records, though they have different permissions (indicated by boolean values on the record).\nI tracked down the bug to the function ```\npassword_verify```\n, which was identified as returning false negatives in this Stack Overflow question and this Treehouse thread.\nSpecifically, here is the stack in Laravel that gets down to this failure point:\n\nThe ```\nlogin```\n route calls ```\n\\Illuminate\\Foundation\\Auth\\AuthenticatesUsers::login```\n via the controller class.\nThe ```\nlogin```\n method calls ```\n\\Illuminate\\Foundation\\Auth\\AuthenticatesUsers::attemptLogin```\n.\nThe ```\nattemptLogin```\n method calls the ```\nattempt```\n method of the controller's guard object.\n```\n\\Illuminate\\Auth\\SessionGuard::attempt```\n calls ```\n\\Illuminate\\Auth\\SessionGuard::hasValidCredentials```\n.\n```\n\\Illuminate\\Auth\\SessionGuard::hasValidCredentials```\n calls the ```\nvalidateCredentials```\n method on the guard's provider object.\n```\nIlluminate\\Auth\\EloquentUserProvider::validateCredentials```\n calls the ```\ncheck```\n method on its hasher object.\n```\nIlluminate\\Hashing\\HashManager::check```\n calls the ```\ncheck```\n method on its driver.\n```\nIlluminate\\Hashing\\BcryptHasher::check```\n calls ```\nIlluminate\\Hashing\\AbstractHasher::check```\n.\n```\nIlluminate\\Hashing\\AbstractHasher::check```\n calls ```\npassword_verify```\n.\n\nAfter unwinding this entire stack, I ran the following code in the ```\nlogin```\n method of the login controller:\n```\n$provider = $this->guard()->getProvider();\n$credentials =  $this->credentials($request);\n$user = $provider->retrieveByCredentials($credentials);\n$password_unhashed = $request['password'];\n$password_hashed = $user->getAuthPassword();\n$password_verify = password_verify($password_unhashed, $password_hashed);\nlogger('attemping login', compact('password_verify','password_unhashed','password_hashed'));\n```\n\nThat dumps this context:\n```\n{\n\"password_verify\": false,\n\"password_unhashed\": \"testperson\",\n\"password_hashed\": \"$2y$10$5xc/wAmNCKV.YhpWOfyNoetCj/r3Fs5TyAskgZuIF/LEItWfm7rPW\"\n}\n```\n\nAnd if I put that password into a ```\nSELECT users WHERE password=```\n query, I get the user that I'm expecting.\nWhat's going on here? And how do I get around this?\n    ", "Answer": "\r\nI think your assertion that the hash you provided is a hash of 'testperson' is in fact false. Since hashing is one-way, I can't tell you what the hash you showed is derived from. NOTE: This runs on PHP 7.4, but I don't think it will work on PHP 8 and beyond because of the deprecation of the salt option in password_hash().\n```\n<?php\n//$testhash = '$2y$10$5xc/wAmNCKV.YhpWOfyNoetCj/r3Fs5TyAskgZuIF/LEItWfm7rPW';\n$testhash = '$2y$10$uNWYvMVmagIwQ2eXnVKLCOAK1QFQdcRtxbvlghf.Xpg0U1w.N./N2';\n//$password = \"testperson\";\n$password = \"eo\";\n$options = array(\"cost\" => 10, \"salt\" => substr($testhash, 7, 22));\n$pwhash = password_hash($password, PASSWORD_BCRYPT, $options);\necho $pwhash.\"\\n\";\n$salt = substr($pwhash, 0, 29);\necho $salt.\"\\n\";\n$cryptpw = crypt($password, $salt);\necho $cryptpw.\"\\n\";\nif (password_verify($password, $cryptpw)) {\n  echo(\"Verified.\\n\");\n} else  {\n  echo(\"NOT Verified.\\n\");\n}\nif (password_needs_rehash($cryptpw, PASSWORD_BCRYPT, $options)) {\n  echo(\"Needs rehash.\\n\");\n} else {\n  echo(\"Doesn't need rehash.\\n\");\n}\n\n/*\ntestperson results...\n$2y$10$5xc/wAmNCKV.YhpWOfyNoeVNPMEcYrxepQeFAssFoAaIYs4WLmgZO\n$2y$10$5xc/wAmNCKV.YhpWOfyNoe\n$2y$10$5xc/wAmNCKV.YhpWOfyNoeVNPMEcYrxepQeFAssFoAaIYs4WLmgZO\nVerified.\nDoesn't need rehash.\n\neo results...\n$2y$10$uNWYvMVmagIwQ2eXnVKLCOAK1QFQdcRtxbvlghf.Xpg0U1w.N./N2\n$2y$10$uNWYvMVmagIwQ2eXnVKLCO\n$2y$10$uNWYvMVmagIwQ2eXnVKLCOAK1QFQdcRtxbvlghf.Xpg0U1w.N./N2\nVerified.\nDoesn't need rehash.\n*/\n?>\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "how to store additional data in a text file apart from it's content - C++\r\n                \r\nI am doing this small university project, where I have to create a console-based text editor with some features, and making files password protected is one of them. As I said, it's a university project for an introductory OOP course, so it doesn't need to be the most secure thing on planet. I am planning to use a simple Caesar cipher to encrypt my file.\nThe only problem is the password. I'll use the password as the encryption key and it will work, but the problem is handling the case where the password is wrong. If no checks are placed then it would just show gibberish, but I want to make it so that it displays a message in case of a wrong password.\nThe idea I have come up with is to somehow store the hash of the unencrypted file in that text file (but it shouldn't show that hash when I open the file up with notepad) and after decrypting with the provided password, I can just hash the contents and check if it matches with the hidden hash stored in that file. Is it possible?\nI am using Windows, by the way, and portability is not an issue.\n    ", "Answer": "\r\nIn general, you can't theoretically design a data format where nothing but plain text is a valid subset of it, but there can also be metadata (hash or something else). Just think about it: how do you store something other than text (i. e. metadata) in a file where every single byte is to be interpreted as text?\nThat said, there are some tricks to hide the metadata in plain sight. With Unicode, the palette of tricks is wider. For example, you can use spacelike characters to encode metadata or indicate metadata presence  in the way that the user won't notice. Consider Unicode BOM. It's the \"zero-length space\" character. Won't be seen in Notepad, serves as metadata. You could so something similar.\nThey already mentioned alternative data streams. While one of those could work to keep the metadata, an alternative data stream doesn't survive archival, e-mailing, uploading to Google Drive/OneDrive/Dropbox, copying with a program that is not aware of it, or copying to a filesystem that doesn't support it (e. g. a CD or a flash drive with FAT).\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "DataBase Handling Problems - Insertion trouble with PDO\r\n                \r\nI'm developing a website for my university and it include having users and everything around it but i have problem with this code :\n\n```\nerror_reporting(E_ALL);\n\nfunction hashSSHA($password) \n{\n    $salt = sha1(rand());\n    $salt = substr($salt, 0, 10);\n    $encrypted = base64_encode(sha1($password . $salt, true) . $salt);\n    $hash = array(\"salt\" => $salt, \"encrypted\" => $encrypted);\n    return $hash;\n}\n\n\n\n\n\nfunction storeUser($name, $email, $password) \n{\n\n\n    $uuid = uniqid('fss_u', true);\n    $hash = hashSSHA($password);\n    $encrypted_password =  password_hash($password, PASSWORD_DEFAULT); // $hash[\"encrypted\"]; // encrypted password\n    $salt = $hash[\"salt\"]; // salt\n    $regday = date(\"Y-m-j\");\n    $type_user= 'admin';\n    $db = new PDO('mysql:host=localhost:3308;dbname=fss-db', 'root', 'nabounanc1');\n\n    echo $encrypted_password ;\n    echo  '<br>';\n\n    $query=$db->prepare('INSERT INTO fss_users (unique_id,username,email,password,salt,create_time,type_user) VALUES (:uuid, :name, :email, :encrypted_password, :salt, :regday, :type_user) ');\n    $query->bindValue(':uuid', $uuid, PDO::PARAM_STR);\n    $query->bindValue(':name', $name, PDO::PARAM_STR);\n    $query->bindValue(':email', $email, PDO::PARAM_STR);\n    $query->bindValue(':encrypted_password', $encrypted_password, PDO::PARAM_STR);\n    $query->bindValue(':salt', $salt, PDO::PARAM_STR);\n    $query->bindValue(':regday', $regday, PDO::PARAM_STR);\n    $query->bindValue(':type_user', $type_user, PDO::PARAM_STR);\n\n\n    $query->execute();\n\n\n        // check for successful store\n\n    if ($query->execute()) \n    {\n        echo $query->rowCount();\n        echo  '<br>';\n       $result = $query->fetchAll();\n\n        print_r($result);\n\n        // get user details \n        $uid = $db->lastInsertId(); // last inserted id\n        $query=$db->prepare(\"SELECT * FROM fss_users WHERE unique_id = :uid\");\n        $query->bindValue(':uid', $uid, PDO::PARAM_STR);\n        $query->execute();\n\n\n\n\n        //Message\n        $message = \"Bienvenue sur FSS Votre inscription a ete accepte pseudo = $email mot de passe = $password !\";\n        //Titre\n        $titre = \"Bienvenue sur FSS !\";\n\n        mail($email, $titre, $message);  // Mail to the new user \n\n        // return user details\n        //return $query->fetchAll();\n        echo 'it\\'s good';\n        echo  '<br>';\n\n    } \n    else \n    {\n        echo 'error';\n        echo  '<br>';\n        //return $false;\n    }\n}\n$name = \"bachirdiop\";\n$email = \"testing@hotmail.com\";\n$password = \"bachirdiop3\";\nstoreUser($name, $email, $password);\n```\n\n\nBasically this function hash the password given by the user and stores it into a database and if everything is normal it should return the last recorded user but it doesn't. How can I fix this?\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Regex: Removing Space Between Quotes, And Stopping Before a Colon (With Yahoo Pipes)\r\n                \r\nI've been working on this for a while, but it's beyond my understanding of regex. \n\nI'm using Yahoo Pipes on an RSS, and I want to create hashtags from titles; so, I'd like to remove space from everything between quotes, but, if there's a colon within the quotes, I only want the space removed between the words before the colon.\n\nAnd, it would be great if I could also capture the unspaced words as a group, to be able to use: #$1 to output the hashtag in one step.\n\nSo, something like:\n\n```\n\"The New Apple: Worlds Within Worlds\" Before We Begin...\n```\n\n\nCould be substituted like #$1 - with this result:\n\n```\n\"#TheNewApple: Worlds Within Worlds\" Before We Begin...\n```\n\n\nAfter some work, I was able to come up with, this regex:\n\n```\n\\s(?=\\s)?|(‘|’|(Review)|:.*)\n```\n\n\n(\"Review\" was a word that often came before colons and wouldn't be stripped, if it were later in the title; that's what that's for, but I would like to not require that, to be more universal)\n\nBut, it has two problems:\n\n\nI have to use multiple steps. The result of that regex would be:\n\n```\n\"TheNewApple: Worlds Within Worlds\" Before We Begin...\n```\n\n\n\nAnd I could then add another regex step, to put the hash # in front\n\n\nBut, it only works if the quotes are first, and I don't know how to fix that...\n\n    ", "Answer": "\r\nYou can do this all in one step with regex, with a caveat. You run into problems with a repeated capturing group because only the last iteration is available in the replacement string. Searching for ```\n( (\\w+))+```\n and replacing with ```\n$2```\n will replace all the words with just the last match - not what we want.\n\nThe way around this is to repeat the pattern an arbitrary number of times that will suffice for your use. Each separate group can be referenced.\n\nSearch: ```\n\"(\\w+)(?: (\\w+))?(?: (\\w+))?(?: (\\w+))?(?: (\\w+))?(?: (\\w+))?```\n\n\nReplace: ```\n\"#$1$2$3$4$5$6```\n\n\nThis will replace up to 6-word titles, exactly as you need them. First, ```\n\"(\\w+)```\n matches any word following a quote. In the replacement string, it is put back as ```\n\"#$1```\n, adding the hashtag. The rest is a repeated list of ```\n(?: (\\w+))?```\n matches, each matching a possible space and word. Notice the space is part of a non-capturing group; only the word is part of the inner capture group. In the replacement string, I have ```\n$1$2$3$4$5$6```\n, which puts back the words, without the spaces. Notice that a colon will not match any part of this, so it will stop once it hits a colon.\n\nExamples:\n\n```\n\"The New Apple: Worlds Within Worlds\" Before We Begin...\n\"The New Apple\" Before We Begin...\n\"One: Two\"\nonly \"One\" word\nthis has \"Two Words\"\n\"The Great Big Apple Dumpling\"\n\"The Great Big Apple Dumpling Again: Part 2\"\n```\n\n\nResults:\n\n```\n\"#TheNewApple: Worlds Within Worlds\" Before We Begin...\n\"#TheNewApple\" Before We Begin...\n\"#One: Two\"\nonly \"#One\" word\nthis has \"#TwoWords\"\n\"#TheGreatBigAppleDumpling\"\n\"#TheGreatBigAppleDumplingAgain: Part 2\"\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Alternative to hashed SSN as a key in MySQL\r\n                \r\nI have a problem similar to the following:\n\nA person begins working as a consultant for \"Company A\". Their HR person sets up an account for them. A record is created for the person in a \"person\" table and a \"person-company\" one.\n\nThe person is also working for \"Company B\" (which Company A may or may not know about). When Company B enters their info they should NOT create a record in \"person\" but SHOULD create one in \"person-company\".\n\nThe person is required to do training for the state, so if they are logged into either company's site when they do the training, we want the total hours to remain with the person.\n\nI can set up a PK for the person table the joins them to each company, but I think I need something like a hash of the person's SSN with some extra \"xyz\" tacked on to be able to do a lookup. Company B would have the SSN for the person, which should be universal.\n\nQuestions:\n\n1) Is there some other method to join that one of you think would work better?\n\n2) If I do go with a hashed SSN approach, what's the best encryption to use for MySQL/PHP for one-way encryption?\n\nI read elsewhere that a public/private key solution may be best, but since the person is not setting up their own account initially I'm not sure how this would work.\n\nThanks\n    ", "Answer": "\r\nI think this article may be very relevant to what you are doing. If indeed you wish to \"anonymize\" the SSNs for security reasons and legal liability, then simply hashing them is not enough. \n\nJust hashing them would be a completely deterministic process, so to effectively \"mask\" individual SSNs, the process needs to be randomized. Otherwise you could simply brute force through all possible combinations of SSNs (which would be much less work required than trying to brute force the hash function) and look for a matching value.\n\nTo see why this holds take the most simplistic example that a SSN could just take on two values, 0 and 1. Regardless of the quality and strength of the hash function, in the end there will only be two possible outcomes and it's easy to see which is which.\n\nIt's the old game of why you shouldn't hash e.g. passwords directly without performing some preprocessing on them first. The underlying data just doesn't contain enough entropy and will therefore be an easy target for lookups in a precomputed table.\n\nThe minute your SSNs become private and confidential (they are not in every country, so forgive my stupid question in the comments :), the same best practices that are also used for password storage should also be applicable to your particular case, i.e. a slow adaptive hashing algorithm that compensates for the lack of initial entropy such as bcrypt, scrypt and PBKDF2 (which was already recommended by Marcus Adams).\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Finding the longest sequence of cards with dynamic programming\r\n                \r\nI'm attempting to write a program which is supposed to find the length of the longest sequence of cards (Crazy eights style) in a shuffled deck using dynamic programming.\nwe are not allowed to use the standard library (this includes vectors).\nSo far this is what I've written:\nThe program reads the deck from a file, stores it in an array.\nThen from within the ```\ncrazyUp()```\n function, I'm attempting to compare each element from within the array with the previous element, if they are equal, we compare with ```\ndict[]```\n which is supposed to act as a hash table for Memoization.\nI can't wrap my head around why the ```\nmaxLength```\n is always returning ```\n1```\n, any help would be greatly appreciated.\nThe file contains data for the cards like so:\n```\n9C\n9D\n5C\nAC\n4D\n3H\nKD\n4S\n```\n\nFor a total of 52 lines (52 cards).\nThe sequence is as follows:\nif a card's Rank or suit is similar to the one before it, add it to that sequence.\nThink of 8 as a universal card that can match with any other card, hence the name, crazy eight!\nExample:\n```\n9C\n9D\n5C\nAC\n8S\n```\n\nWould be a valid sequence.\nHowever:\n```\n5C\n9H\n3D\n```\n\nwould be completely skipped as none of them are \"like\" another.\n```\n#include <iostream>\n#include <fstream>\n#include<sstream>\n\nusing namespace std;\n\nint crazyUp(string[]);\nint index = 0;\n\nstring cards[55];\n\nint dict[55];\nint dictIndex = 0;\n\nint main()\n{\n    string file;\n    string word;\n    cout << \"Please input a file's name...\" << endl;\n    cin >> file;\n\n    ifstream fileName(file);\n\n    while (fileName >> word) {\n        cards[index++] = word;\n    }\n    \n    crazyUp(cards);\n    \n}\n\n\nint crazyUp(string cards[]) {\n    dict[0] = 0;\n    int maxLength = 0;\n    int length;\n\n    for (int i = 0; i < index; i++) {\n        length = 1;\n        for (int j = 0; j < i-1; j++) {\n            if (cards[i] == cards[j]) {\n                length = max(length, dict[j]+1);\n            }\n            \n\n        }\n        dict[i] = length;\n        maxLength = max(maxLength, length);\n        cout << maxLength;\n    }\n\n    return maxLength;\n}\n\n```\n\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Imported scss class names converted to hashes\r\n                \r\nI have a webpack configuration (based on React Universally). One thing I'm seeing is that class names on imported scss files are converted to hashes. \n\nI tried as an alternative to import styles as an object and reference the style classes by name, but they simply disappear. Didn't see this behavior on other projects and not sure what I'm doing wrong.\n\n```\nimport './styles/scss/app.scss';\n\nfunction App() {\n    return (\n      <Provider store={ store }>\n          <IntlProvider>\n              <MuiThemeProvider>\n                  <Switch>\n                      <Route exact path=\"/\" component={Home} />\n                      <Route component={Error404} />\n                  </Switch>\n              </MuiThemeProvider>\n          </IntlProvider>\n      </Provider>\n    )\n}\n\n\nimport styles from './portfolio.scss';\n\n[...]\nreturn (\n<div className={cx(styles.phoneAni, {active: imageOpen})}>\n    <img src=\"img/iphone_s01_1.png\"/>\n    <img src=\"img/iphone_s01_0.png\"/>\n    <img src=\"img/iphone_s02_2.png\"/>\n</div>\n);\n```\n\n\nwebpack\n\n```\nexport default function webpackConfigFactory(buildOptions) {\n  const { target, optimize = false } = buildOptions;\n\n  const isOptimize = optimize;\n  const isDev = !isOptimize;\n  const isClient = target === 'client';\n  const isServer = target === 'server';\n  const isNode = !isClient;\n\n  // Preconfigure some ifElse helper instnaces. See the util docs for more\n  // information on how this util works.\n  const ifDev = ifElse(isDev);\n  const ifOptimize = ifElse(isOptimize);\n  const ifNode = ifElse(isNode);\n  const ifClient = ifElse(isClient);\n  const ifDevClient = ifElse(isDev && isClient);\n  const ifOptimizeClient = ifElse(isOptimize && isClient);\n\n  console.log(`==> Creating ${isOptimize ? 'an optimised' : 'a development'} bundle configuration for the \"${target}\"`);\n\n  const bundleConfig = isServer || isClient\n    // This is either our \"server\" or \"client\" bundle.\n    ? config(['bundles', target])\n    // Otherwise it must be an additional node bundle.\n    : config(['additionalNodeBundles', target]);\n\n  if (!bundleConfig) {\n    throw new Error('No bundle configuration exists for target:', target);\n  }\n\n  const localIdentName = ifDev('[name]__[local]___[hash:base64:5]', '[hash:base64:10]');\n\n  let webpackConfig = {\n    // Define our entry chunks for our bundle.\n    entry: {\n      // We name our entry files \"index\" as it makes it easier for us to\n      // import bundle output files (e.g. `import server from './build/server';`)\n      index: removeNil([\n        // Required to support hot reloading of our client.\n        ifDevClient(() => `webpack-hot-middleware/client?reload=true&path=http://${config('host')}:${config('clientDevServerPort')}/__webpack_hmr`),\n        // We are using polyfill.io instead of the very heavy babel-polyfill.\n        // Therefore we need to add the regenerator-runtime as polyfill.io\n        // doesn't support this.\n        ifClient('regenerator-runtime/runtime'),\n        // The source entry file for the bundle.\n        path.resolve(appRootDir.get(), bundleConfig.srcEntryFile),\n      ]),\n    },\n\n    // Bundle output configuration.\n    output: {\n      // The dir in which our bundle should be output.\n      path: path.resolve(appRootDir.get(), bundleConfig.outputPath),\n      // The filename format for our bundle's entries.\n      filename: ifOptimizeClient(\n        // For our production client bundles we include a hash in the filename.\n        // That way we won't hit any browser caching issues when our bundle\n        // output changes.\n        // Note: as we are using the WebpackMd5Hash plugin, the hashes will\n        // only change when the file contents change. This means we can\n        // set very aggressive caching strategies on our bundle output.\n        '[name]-[chunkhash].js',\n        // For any other bundle (typically a server/node) bundle we want a\n        // determinable output name to allow for easier importing/execution\n        // of the bundle by our scripts.\n        '[name].js',\n      ),\n      // The name format for any additional chunks produced for the bundle.\n      chunkFilename: '[name]-[chunkhash].js',\n      // When targetting node we will output our bundle as a commonjs2 module.\n      libraryTarget: ifNode('commonjs2', 'var'),\n      // This is the web path under which our webpack bundled client should\n      // be considered as being served from.\n      publicPath: ifDev(\n        // As we run a seperate development server for our client and server\n        // bundles we need to use an absolute http path for the public path.\n        `http://${config('host')}:${config('clientDevServerPort')}${config('bundles.client.webPath')}`,\n        // Otherwise we expect our bundled client to be served from this path.\n        bundleConfig.webPath,\n      ),\n    },\n\n    target: isClient\n      // Only our client bundle will target the web as a runtime.\n      ? 'web'\n      // Any other bundle must be targetting node as a runtime.\n      : 'node',\n\n    // Ensure that webpack polyfills the following node features for use\n    // within any bundles that are targetting node as a runtime. This will be\n    // ignored otherwise.\n    node: {\n      __dirname: true,\n      __filename: true,\n    },\n\n    // Source map settings.\n    devtool: ifElse(\n        // Include source maps for ANY node bundle so that we can support\n        // nice stack traces for errors (the source maps get consumed by\n        // the `node-source-map-support` module to allow for this).\n        isNode\n        // Always include source maps for any development build.\n        || isDev\n        // Allow for the following flag to force source maps even for production\n        // builds.\n        || config('includeSourceMapsForOptimisedClientBundle'),\n      )(\n      // Produces an external source map (lives next to bundle output files).\n      'source-map',\n      // Produces no source map.\n      'hidden-source-map',\n    ),\n\n    // Performance budget feature.\n    // This enables checking of the output bundle size, which will result in\n    // warnings/errors if the bundle sizes are too large.\n    // We only want this enabled for our production client.  Please\n    // see the webpack docs on how you can configure this to your own needs:\n    // https://webpack.js.org/configuration/performance/\n    performance: ifOptimizeClient(\n      // Enable webpack's performance hints for production client builds.\n      { hints: 'warning' },\n      // Else we have to set a value of \"false\" if we don't want the feature.\n      false,\n    ),\n\n    resolve: {\n      // These extensions are tried when resolving a file.\n      extensions: config('bundleSrcTypes').map(ext => `.${ext}`),\n\n      // This is required for the modernizr-loader\n      // @see https://github.com/peerigon/modernizr-loader\n      alias: mergeDeep(\n        {\n          modernizr$: path.resolve(appRootDir.get(), './.modernizrrc'),\n        },\n        // For our optimised builds we will alias to the optimised versions\n        // of React and ReactDOM.\n        ifOptimize({\n          react$: path.resolve(\n            appRootDir.get(), './node_modules/react/dist/react.min.js',\n          ),\n          'react-dom$': path.resolve(\n            appRootDir.get(), './node_modules/react-dom/dist/react-dom.min.js',\n          ),\n          'react-dom/server$': path.resolve(\n            appRootDir.get(), './node_modules/react-dom/dist/react-dom-server.min.js',\n          ),\n        }),\n      ),\n    },\n\n    // We don't want our node_modules to be bundled with any bundle that is\n    // targetting the node environment, prefering them to be resolved via\n    // native node module system. Therefore we use the `webpack-node-externals`\n    // library to help us generate an externals configuration that will\n    // ignore all the node_modules.\n    externals: removeNil([\n      ifNode(\n        () => nodeExternals(\n          // Some of our node_modules may contain files that depend on our\n          // webpack loaders, e.g. CSS or SASS.\n          // For these cases please make sure that the file extensions are\n          // registered within the following configuration setting.\n          {\n            whitelist:\n              removeNil([\n                // We always want the source-map-support included in\n                // our node target bundles.\n                'source-map-support/register',\n                // We want react bundled with our node bundles for the optimised\n                // builds as we are going to resolve to the optmised versions\n                // of react via the webpack alias configuration.\n                ifOptimize('react'),\n                ifOptimize('react-dom'),\n                ifOptimize('react-dom/server'),\n              ])\n              // And any items that have been whitelisted in the config need\n              // to be included in the bundling process too.\n              .concat(config('nodeExternalsFileTypeWhitelist') || []),\n          },\n        ),\n      ),\n    ]),\n\n    plugins: removeNil([\n      // This grants us source map support, which combined with our webpack\n      // source maps will give us nice stack traces for our node executed\n      // bundles.\n      // We use the BannerPlugin to make sure all of our chunks will get the\n      // source maps support installed.\n      ifNode(() => new webpack.BannerPlugin({\n        banner: 'require(\"source-map-support\").install();',\n        raw: true,\n        entryOnly: false,\n      })),\n\n      // We use this so that our generated [chunkhash]'s are only different if\n      // the content for our respective chunks have changed.  This optimises\n      // our long term browser caching strategy for our client bundle, avoiding\n      // cases where browsers end up having to download all the client chunks\n      // even though 1 or 2 may have only changed.\n      ifClient(() => new WebpackMd5Hash()),\n\n      // These are special flags that you can use in your code in order to\n      // have advanced control over what is included/excluded in your bundles.\n      // For example you may only want certain parts of your code to be\n      // included/ran under certain conditions.\n      //\n      // The DefinePlugin is used by webpack to substitute any code\n      // that matches the property keys of the object you provide it below with\n      // the given value that has been assigned to each respective property.\n      //\n      // For example you may have the following in your code:\n      //   if (process.env.BUILD_FLAG_IS_CLIENT === true) {\n      //     console.log('Foo');\n      //   }\n      //\n      // If the BUILD_FLAG_IS_CLIENT was assigned a value of `false` the above\n      // code would be converted to the following by the webpack bundling\n      // process:\n      //   if (false === true) {\n      //     console.log('Foo');\n      //   }\n      //\n      // When your bundle is built using the UglifyJsPlugin unreachable code\n      // blocks like in the example above will be removed from the bundle\n      // final output. This is helpful for extreme cases where you want to\n      // ensure that code is only included/executed on specific targets, or for\n      // doing debugging.\n      //\n      // NOTE: You may be used to having to do NODE_ENV = production here to\n      // get optimized React/ReactDOM builds. Almost every blog and example\n      // will tell you to do this.  I have decided against this model as it\n      // often confused me when I was passing custom NODE_ENV values\n      // such as \"staging\" / \"test\" to my scripts.  Therefore to avoid any\n      // confusion we instead use the webpack alias feature to target the\n      // pre-optimised dist versions of React/ReactDOM when required.\n      new webpack.DefinePlugin({\n        // Is this the \"client\" bundle?\n        'process.env.BUILD_FLAG_IS_CLIENT': JSON.stringify(isClient),\n        // Is this the \"server\" bundle?\n        'process.env.BUILD_FLAG_IS_SERVER': JSON.stringify(isServer),\n        // Is this a node bundle?\n        'process.env.BUILD_FLAG_IS_NODE': JSON.stringify(isNode),\n        // Is this a development build?\n        'process.env.BUILD_FLAG_IS_DEV': JSON.stringify(isDev),\n      }),\n\n      // Generates a JSON file containing a map of all the output files for\n      // our webpack bundle.  A necessisty for our server rendering process\n      // as we need to interogate these files in order to know what JS/CSS\n      // we need to inject into our HTML. We only need to know the assets for\n      // our client bundle.\n      ifClient(() =>\n        new AssetsPlugin({\n          filename: config('bundleAssetsFileName'),\n          path: path.resolve(appRootDir.get(), bundleConfig.outputPath),\n        }),\n      ),\n\n      // We don't want webpack errors to occur during development as it will\n      // kill our dev servers.\n      ifDev(() => new webpack.NoEmitOnErrorsPlugin()),\n\n      // We need this plugin to enable hot reloading of our client.\n      ifDevClient(() => new webpack.HotModuleReplacementPlugin()),\n\n      // For our production client we need to make sure we pass the required\n      // configuration to ensure that the output is minimized/optimized.\n      ifOptimizeClient(\n        () => new webpack.LoaderOptionsPlugin({\n          minimize: true,\n        }),\n      ),\n\n      // For our production client we need to make sure we pass the required\n      // configuration to ensure that the output is minimized/optimized.\n      ifOptimizeClient(\n        () => new webpack.optimize.UglifyJsPlugin({\n          sourceMap: config('includeSourceMapsForOptimisedClientBundle'),\n          compress: {\n            screw_ie8: true,\n            warnings: false,\n          },\n          mangle: {\n            screw_ie8: true,\n          },\n          output: {\n            comments: false,\n            screw_ie8: true,\n          },\n        }),\n      ),\n\n      // For the production build of the client we need to extract the CSS into\n      // CSS files.\n      ifOptimizeClient(\n        () => new ExtractTextPlugin({\n          filename: '[name]-[chunkhash].css',\n          allChunks: true,\n        }),\n      ),\n\n      // -----------------------------------------------------------------------\n      // START: HAPPY PACK PLUGINS\n      //\n      // @see https://github.com/amireh/happypack/\n      //\n      // HappyPack allows us to use threads to execute our loaders. This means\n      // that we can get parallel execution of our loaders, significantly\n      // improving build and recompile times.\n      //\n      // This may not be an issue for you whilst your project is small, but\n      // the compile times can be signficant when the project scales. A lengthy\n      // compile time can significantly impare your development experience.\n      // Therefore we employ HappyPack to do threaded execution of our\n      // \"heavy-weight\" loaders.\n\n      // HappyPack 'javascript' instance.\n      happyPackPlugin({\n        name: 'happypack-javascript',\n        // We will use babel to do all our JS processing.\n        loaders: [{\n          path: 'babel-loader',\n          // We will create a babel config and pass it through the plugin\n          // defined in the project configuration, allowing additional\n          // items to be added.\n          query: config('plugins.babelConfig')(\n            // Our \"standard\" babel config.\n            {\n              // We need to ensure that we do this otherwise the babelrc will\n              // get interpretted and for the current configuration this will mean\n              // that it will kill our webpack treeshaking feature as the modules\n              // transpilation has not been disabled within in.\n              babelrc: false,\n\n              presets: [\n                // For our client bundles we transpile all the latest ratified\n                // ES201X code into ES5, safe for browsers.  We exclude module\n                // transilation as webpack takes care of this for us, doing\n                // tree shaking in the process.\n                ifClient(['latest', { es2015: { modules: false } }]),\n                // For a node bundle we use the awesome babel-preset-env which\n                // acts like babel-preset-latest in that it supports the latest\n                // ratified ES201X syntax, however, it will only transpile what\n                // is necessary for a target environment.  We have configured it\n                // to target our current node version.  This is cool because\n                // recent node versions have extensive support for ES201X syntax.\n                // Also, we have disabled modules transpilation as webpack will\n                // take care of that for us ensuring tree shaking takes place.\n                // NOTE: Make sure you use the same node version for development\n                // and production.\n                ifNode(['env', { targets: { node: true }, modules: false }]),\n                // Stage 3 javascript syntax.\n                // \"Candidate: complete spec and initial browser implementations.\"\n                // Add anything lower than stage 3 at your own risk. :)\n                'stage-0',\n                // JSX\n                'react',\n              ].filter(x => x != null),\n\n              plugins: [\n                'transform-class-properties',\n                'syntax-decorators',\n                'transform-decorators-legacy',\n                // This decorates our components with  __self prop to JSX elements,\n                // which React will use to generate some runtime warnings.\n                ifDev('transform-react-jsx-self'),\n                // Adding this will give us the path to our components in the\n                // react dev tools.\n                ifDev('transform-react-jsx-source'),\n                // Replaces the React.createElement function with one that is\n                // more optimized for production.\n                // NOTE: Symbol needs to be polyfilled. Ensure this feature\n                // is enabled in the polyfill.io configuration.\n                ifOptimize('transform-react-inline-elements'),\n                // Hoists element creation to the top level for subtrees that\n                // are fully static, which reduces call to React.createElement\n                // and the resulting allocations. More importantly, it tells\n                // React that the subtree hasn’t changed so React can completely\n                // skip it when reconciling.\n                ifOptimize('transform-react-constant-elements'),\n              ].filter(x => x != null),\n              env: {\n                  production: {\n                      presets: [\n                        ifOptimize('react-optimize'),\n                      ].filter(x => x != null)\n                  }\n              },\n            },\n            buildOptions,\n          ),\n        }],\n      }),\n\n      // HappyPack 'css' instance for development client.\n      ifDevClient(\n        () => happyPackPlugin({\n          name: 'happypack-devclient-css',\n          loaders: [\n            'style-loader',\n            {\n              loader: 'css-loader',\n              // Include sourcemaps for dev experience++.\n              options: {\n                sourceMap: true,\n                modules: true,\n                importLoaders: 1,\n                localIdentName,\n              },\n            },\n            {\n              loader: 'sass-loader',\n              options: {\n                outputStyle: 'expanded',\n                sourceMap: true,\n              },\n            },\n          ],\n        }),\n      ),\n\n      // END: HAPPY PACK PLUGINS\n      // -----------------------------------------------------------------------\n    ]),\n    module: {\n      rules: removeNil([\n        // JAVASCRIPT\n        {\n          test: /\\.jsx?$/,\n          // We will defer all our js processing to the happypack plugin\n          // named \"happypack-javascript\".\n          // See the respective plugin within the plugins section for full\n          // details on what loader is being implemented.\n          loader: 'happypack/loader?id=happypack-javascript',\n          include: removeNil([\n            ...bundleConfig.srcPaths.map(srcPath =>\n              path.resolve(appRootDir.get(), srcPath),\n            ),\n            ifOptimizeClient(path.resolve(appRootDir.get(), 'src/html')),\n          ]),\n        },\n\n        // CSS\n        // This is bound to our server/client bundles as we only expect to be\n        // serving the client bundle as a Single Page Application through the\n        // server.\n        ifElse(isClient || isServer)(\n          mergeDeep(\n            {\n              test: /\\.(css|scss)$/,\n            },\n            // For development clients we will defer all our css processing to the\n            // happypack plugin named \"happypack-devclient-css\".\n            // See the respective plugin within the plugins section for full\n            // details on what loader is being implemented.\n            ifDevClient({\n              loaders: [\n                  'happypack/loader?id=happypack-devclient-css',\n                  {\n                      loader: 'css-loader',\n                      // Include sourcemaps for dev experience++.\n                      options: {\n                        sourceMap: true,\n                        modules: true,\n                        importLoaders: 1,\n                        localIdentName,\n                      },\n                  },\n                  {\n                      loader: 'sass-loader',\n                      options: {\n                        outputStyle: 'expanded',\n                        sourceMap: true,\n                      },\n                  },\n              ],\n            }),\n            // For a production client build we use the ExtractTextPlugin which\n            // will extract our CSS into CSS files. We don't use happypack here\n            // as there are some edge cases where it fails when used within\n            // an ExtractTextPlugin instance.\n            // Note: The ExtractTextPlugin needs to be registered within the\n            // plugins section too.\n            ifOptimizeClient(() => ({\n              loader: ExtractTextPlugin.extract({\n                fallback: 'style-loader',\n                use: [\n                    {\n                        loader: 'css-loader',\n                        // Include sourcemaps for dev experience++.\n                        options: {\n                          sourceMap: true,\n                          modules: true,\n                          importLoaders: 1,\n                          localIdentName,\n                        },\n                    },\n                    {\n                        loader: 'sass-loader',\n                        options: {\n                          sourceMap: true,\n                        },\n                    },\n                ],\n              }),\n            })),\n            // When targetting the server we use the \"/locals\" version of the\n            // css loader, as we don't need any css files for the server.\n            ifNode({\n              loaders: [\n                  `css-loader/locals?modules=1&importLoaders=1&localIdentName=${localIdentName}`,\n                  'sass-loader',\n              ],\n          }),\n          ),\n        ),\n\n        // ASSETS (Images/Fonts/etc)\n        // This is bound to our server/client bundles as we only expect to be\n        // serving the client bundle as a Single Page Application through the\n        // server.\n        ifElse(isClient || isServer)(() => ({\n          test: new RegExp(`\\\\.(${config('bundleAssetTypes').join('|')})$`, 'i'),\n          loader: 'file-loader',\n          query: {\n            // What is the web path that the client bundle will be served from?\n            // The same value has to be used for both the client and the\n            // server bundles in order to ensure that SSR paths match the\n            // paths used on the client.\n            publicPath: isDev\n              // When running in dev mode the client bundle runs on a\n              // seperate port so we need to put an absolute path here.\n              ? `http://${config('host')}:${config('clientDevServerPort')}${config('bundles.client.webPath')}`\n              // Otherwise we just use the configured web path for the client.\n              : config('bundles.client.webPath'),\n            // We only emit files when building a web bundle, for the server\n            // bundle we only care about the file loader being able to create\n            // the correct asset URLs.\n            emitFile: isClient,\n          },\n        })),\n\n        // MODERNIZR\n        // This allows you to do feature detection.\n        // @see https://modernizr.com/docs\n        // @see https://github.com/peerigon/modernizr-loader\n        ifClient({\n          test: /\\.modernizrrc.js$/,\n          loader: 'modernizr-loader',\n        }),\n        ifClient({\n          test: /\\.modernizrrc(\\.json)?$/,\n          loader: 'modernizr-loader!json-loader',\n        }),\n      ]),\n    },\n  };\n\n  if (isOptimize && isClient) {\n    webpackConfig = withServiceWorker(webpackConfig, bundleConfig);\n  }\n\n  // Apply the configuration middleware.\n  return config('plugins.webpackConfig')(webpackConfig, buildOptions);\n}\n```\n\n    ", "Answer": "\r\nThe class names are converted to hashes because you're using css modules. In order to disable that you must set ```\nmodules: false,```\n in your css-loader options object. Or you may still use css modules, but you need to translate the classes in your html to use generated ones. \n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Heroku and PostgreSQL command-line using -c/--command to process query with characters that need to be escaped\r\n                \r\nI am familiar with the syntax of MySQL as far as escaping characters is concerned.  However, as the Ruby on Rails application that I am currently building uses PostgreSQL I am struggling to get the command line to behave properly.  I am trying to run a basic query through a shell script when rebasing the database to a more recent backup of the live database.\n\nFor example the command that I am trying to run looks like the following:\n\n```\npsql db_name u_name -c \"UPDATE users SET encrypted_password = 'crazy_long_hash_here';\"\n```\n\n\nI understand the security implications of the query being run, but this is for a development workstation where we need to be able to log in as varying classes of users to test different functionality.\n\nThe problem that I get when running this command is that the application the crashes with a Ruby Exception:\n\n```\nBCrypt::Errors::InvalidHash\n```\n\n\nThis message clearly states that the value of the hash is no longer correct.  However, if I run the same command in the standard command line style as follows:\n\n```\npsql db_name u_name\n\ndb_name=# UPDATE users SET encrypted_password = 'crazy_long_hash_here';\nUPDATE (row count)\n```\n\n\nthe command resets the password to the desired universal password and I am able to log in as any user to properly test any functionality.\n\nWhat part of the escaping procedure am I missing? The following characters outside of the standard [0-9][a-zA-Z] character set are used in the hash:\n\n```\n$ .\n```\n\n\nAny help would be appreciated, as google seems strangely silent on the matter.  The only other question I have found is not helpful: Escaping single quotes in shell for postgresql\n\n\nUPDATE\n\nI have also tried this with the here-document syntax to no avail:\n\n```\npsql db_name u_name << EOF\n  UPDATE users SET encrypted_password = 'crazy_long_hash_here';\nEOF\n```\n\n    ", "Answer": "\r\nSo as the case would have it, A little more fiddling resulted in the answer.  When running the command though the standard ```\npsql```\n interface, I needed to escape the ```\n$```\n to ```\n\\$```\n.\n\nHowever, this continued to fail when running:\n\n```\nheroku pg:psql -c \"UPDATE users SET encrypted_password = 'hash_with_\\$_in_string';\"\n```\n\n\nThe answer to this was to double escape  the characters, meaning:\n\n```\nheroku pg:psql -c \"UPDATE users SET encrypted_password = 'hash_with_\\\\\\$_in_string';\"\n```\n\n\nOnce both the ```\n\\```\n and the ```\n$```\n are each escaped, the heroku toolbelt properly interpreted the hash and all is well.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Iterate to every element of the multidimension array in ruby\r\n                \r\nI have a array i getting through an xml .i want to iterate every element to the array which is hash and get the every hash element value using key.\nI want to someting like this>>\n\narray>>\n\n```\neducation_split = [{\"University\"=>\"Institute Of Engineering And Emerging Technologies\", \"Degree\"=>\"MBA\", \"Year\"=>\"2007\"}, {\"University\"=>\"H.N.B. Garhwal University\", \"Degree\"=>\"MSC\", \"Year\"=>\"2005\"}, {\"University\"=>\"H.P. University\", \"Degree\"=>\"Med\", \"Year\"=>\"2003\"}, {\"University\"=>nil, \"Degree\"=>\"12th\", \"Year\"=>\"1999\"}, {\"University\"=>nil, \"Degree\"=>\"10th\", \"Year\"=>nil}] \n```\n\n\nnow i want to iterate to every element of the array and get the value of university ,degree,year in iteration. something like that..\n\n```\n education_split.each do |edu|\n     //here are some other things also like creating object\n      edu[\"University\"] \n      edu [\"Degree\"] \n      edu[\"Year\"]\n   end    \n```\n\n\nThis is also working but in some cases it is though error >> TypeError (no implicit conversion of String into Integer)\n\nhere all fields are string and values i am getting are also string.\n    ", "Answer": "\r\nJust need to check a hash :\n\n```\neducation_split.each do |edu|\n     //here are some other things also like creating object\n  if edu.is_a? Hash\n      edu[\"University\"] \n      edu [\"Degree\"] \n      edu[\"Year\"]\n  end\nend  \n```\n\n\nReading the error, I am sure your collection ```\neducation_split```\n contains also arrays with hashes. Now to prevent the error and as you interested only to hash that part of the code, just do a check if ```\nedu```\n in any particular iteration, is a hash or not. if hash, do your operation or skip it.\n\nTypeError (no implicit conversion of String into Integer) only comes, when you would try to get array elements using strings, instead of integers. Like ```\na = [1, 2]```\n, and now do ```\na['x']```\n, and see you would get the exact error you are now getting.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "memcached like software with disk persistence\r\n                \r\nI have an application that runs on Ubuntu Linux 12.04 which needs to store and retrieve a large number of large serialized objects. Currently the store is implemented by simply saving the serialized streams as files, where the filenames equal the md5 hash of the serialized object. However I would like to speed things up replacing the file-store by one that does in-memory caching of objects that are recently read/written, and preferably does the hashing for me.\n\nThe design of my application should not get any more complicated. Hence preferably would be a storing back-end that manages a key-value database and caching in an abstracted and efficient way. I am a bit lost with all of the key/value stores that are out there, and much of the topics/information seems to be outdated. I was initially looking at something like memcached+membase, but maybe there are better solutions out there. I looked into redis, mongodb, couchdb, but it is not quite clear to me if they fit my needs. \n\nMy most important requirements:\n\n\nTransparent saving to a persistent store in a way that the most recently written/read objects are quickly available by automatically caching them in memory.\nStore should survive a reboot. Hence in memory objects should be saved on disk asap. \nCurrently I am calculating the md5 manually. It would actually be nicer if the back-end does this for me. Hence the ability to get the hash-key when an object is stored, and be able to retrieve the object later using the hashkey.\nBig plus is that if there are packages available for Ubuntu 12.04, either in universe or through launchpad or whatever.\nOther than this, the software should preferably be light not be more complicated than necessary (I don't need distributed map-reduce jobs, etc)\n\n\nThanks for any advice!\n    ", "Answer": "\r\nI would normally suggest Redis because it will be fast and in-memory with asynch persistant store.  Plus you'll find you can use their different data types for other purposes so not as single-purpose as memcached.  As far as auto-hashing, I don't think it does that as you define your own keys when you store objects (as in most of them).\n\nOne downside to Redis is if you're storing a TON of binary objects, you'll be limited to available memory in RAM (unless sharding) so could reach performance limitations.  In that case you may store objects on file system, hash them, and store keys in Redis and match that to filename stored on file server and you'd be fine.\n\n--\n\nAn alternate option would be to check out ElasticSearch which is like Mongo in that it stores objects native as JSON, but it includes the Lucene search engine on top with RESTful API interface.  It \"warms up\" data in memory for fast response, but is also a persistent store and the nicest part is it auto-shards and auto-clusters using multicast to find other nodes.\n\n--\n\nHope that helps and if so, share the love!  ;-)\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is there a cleaner way to check a hashed password than this?\r\n                \r\nso far how i check for hashed passwords is the following is there a way to make this cleaner and possibly quicker for if the database grew lager than a test one at home/university?\n\n```\n<?php\n    $pass = array();\n    $index =0;\n    $result = mysqli_query($db, \"SELECT * FROM admins\");\n    while ($row = mysqli_fetch_array($result)) {\n        $pass[$index] = $row['password'];\n        $newhash = $pass[$index];\n        if (password_verify($password, $newhash)) {\n            echo \"Password Matched\";\n        } else {\n        }\n    }\n?>\n```\n\n\nEdit: Its been pointed out to add a userid but my orginal question was the way I'm verifying the data is this the best way or is there an optimized way to do so?\n    ", "Answer": "\r\nAs chris says you really should only select the row containing the asserted identity. While it is possible that two different admins could have the same password, as long as you created the stored password with password_hash() it is improbable that your current code would miss-identify a user (because each account should get a randomly assigned salt for the hash) ; ccKep's comment is probably wrong.\n\nSo that implies a query like:\n\n```\nSELECT password FROM admins WHERE username=?\n```\n\n\nThis eliminates the need to iterate through multiple rows in the resultset in your php code.\n\nIf you also add an index on username, then it will go even faster as the dbms has less work to do. A unique index simplifies the problem of checking for duplicate accounts.\n\nSince the value of the hashed password is dependent on the salt - which is effectively unique per row, there is no way to optimize the query performance around the hash or the cleartext of the password - you need to use the asserted identity; Graham's suggestion of searching based on the hashed password is wrong.\n\nIn practice, if the site has any significant asset value you should also be limiting the rate of logons to protect against brute force attacks, hence a better query would look something like:\n\n```\nSELECT password\nFROM admins\nWHERE username=?\nAND 5 < (SELECT COUNT(*)\n  FROM login_failures\n  WHERE client_ip = INET_ATON( ? )\n  AND attempt_timestamp > NOW() - INTERVAL 30 MINUTE)\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Bloom filters for determining which sets in a family are subsets of a given set\r\n                \r\nI am trying to use a Bloom filter to determine which sets from a family of sets ```\nA1```\n, ```\nA2```\n,...,```\nAm```\n are subsets of another fixed set ```\nQ```\n. I am hoping that someone can verify the correctness of the stated approach or offer any improvements.\n\nLet ```\nQ```\n be a given set of integers, containing anywhere from 1-10000 elements from the universe set ```\nU = {1,2,...,10000}```\n.\n\nAlso, let there be a family of sets ```\nA1```\n, ```\nA2```\n,...,```\nAm```\n each containing anywhere from 1-3 elements from the same universe set ```\nU```\n. The size ```\nm```\n is on the order of 5000.\n\noutline of algorithm:\n\nLet there be a collection of ```\nk```\n hash functions. For each element of ```\nQ```\n apply the hash functions and add it to a bitset of size ```\nn```\n, denoted ```\nQ_b```\n.\n\nAlso, for each of the ```\nAi```\n, ```\ni = 1,...,m```\n sets, apply the hash functions to each element of ```\nAi```\n, generating the bitset (also of size ```\nn```\n), denoted ```\nAi_b```\n.\n\nTo check if ```\nAi```\n is a subset of ```\nQ```\n, perform a logical AND on the two bitsets, ```\nQ_b & Ai_b```\n, and check if it is equal to the bitset ```\nAi_b```\n. That is, if ```\nQ_b & Ai_b == Ai_b```\n is false, then we know that ```\nAi```\n is not a subset of ```\nQ```\n; if it is true, then we do not know for sure (possibility of a false positive) and we need to check the given ```\nAi```\n using a deterministic approach.\n\nThe hope is that the filter tells us the majority of the ```\nAi```\n's that are not in ```\nQ```\n and we can check the ones that return true more carefully.\n\nIs this a good approach for my problem?\n\n(Side questions: How big should ```\nn```\n be?  What are some good hash functions to use?)\n    ", "Answer": "\r\nIf the range of values is rather small (as in your example), you can use a simple deterministic solution with linear time complexity. \n\n\nLet's create an array ```\nwas```\n (with indices from 1 to 10000, that is, one cell for each element of the universal set), initially filled with ```\nfalse```\n values.\nFor each element ```\nq```\n of ```\nQ```\n, we set ```\nwas[q] = true```\n.\nNow we iterate over all sets of the family. For each set ```\nA_i```\n, we iterate over all elements ```\nx```\n of the set and check if ```\nwas[x]```\n is true. If it's not for at least one ```\nx```\n, then ```\nA_i```\n is not a subset of ```\nQ```\n. Otherwise, it is.\n\n\nThis solution is clearly correct as it checks if one set is a subset of the other by definition. It's also rather simple and deterministic. The only potential downside it has is that it requires an auxiliary array of 10000 elements, but it looks admissible for most practical purposes (a bloom filter would require some extra space too, anyway). \n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Can't run Angular app, ng serve throwing errors\r\n                \r\nThe problem\nI am currently working on a project and managed to clone the repository into my computer. I used npm install to download the packages. The moment I use ng serve the errors show up. The application Fails to compile, but still runs in localhost. I am not sure how to approach this error and how to overcome it. Project is currently running on Angular 8. Provided below is the package.json file and the error in question.\npackage.json\n```\n{\n  \"name\": \"universal-demo\",\n  \"version\": \"7.0.0\",\n  \"license\": \"MIT\",\n  \"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"ng serve\",\n    \"ssr\": \"npm run build:universal && npm run generate:prerender && npm run server\",\n    \"ssr:debug\": \"ng build --aot=true --output-hashing=all --named-chunks=false --build-optimizer=true  && ng run universal-demo:server:dev && webpack && node server.js\",\n    \"ssr:cw\": \"ng build --aot=true --output-hashing=all --named-chunks=false --build-optimizer=true --watch\",\n    \"ssr:sw\": \"ng run universal-demo:server:dev --watch\",\n    \"ssr:webpack\": \"webpack --watch\",\n    \"ssr:server\": \"nodemon server.js\",\n    \"ssr:watch\": \"run-p ssr:universal:build:*\",\n    \"ssr:universal:build:browser\": \"ng run universal-demo:build:dev --watch\",\n    \"ssr:universal:build:server\": \"node ./node_modules/npm-delay 15000 && ng run universal-demo:server:dev --watch\",\n    \"ssr:universal:build:webpack\": \"node ./node_modules/npm-delay 30000 && webpack --config webpack.config.js --watch\",\n    \"ssr:universal:build:nodemon\": \"node ./node_modules/npm-delay 35000 && nodemon --inspect server.js\",\n    \"format:check\": \"prettier --write --config ./.prettierrc --list-different \\\"src/{app,environments,styles}/**/*{.ts,.json,.scss}\\\"\",\n    \"build\": \"ng build\",\n    \"build:server\": \"ng run universal-demo:server:production\",\n    \"build:prod\": \"ng build --prod\",\n    \"build:universal\": \"ng run universal-demo:build:production && ng run universal-demo:server:production && webpack --progress --colors\",\n    \"server\": \"node server.js\",\n    \"lint\": \"ng lint\",\n    \"e2e\": \"ng e2e\",\n    \"build:prerender\": \"npm run build:universal && npm run generate:prerender\",\n    \"generate:prerender\": \"node prerender.js\",\n    \"bundle-report\": \"ng build --stats-json && webpack-bundle-analyzer dist/stats.json\",\n    \"bundle-report:prod\": \"ng build --stats-json --prod && webpack-bundle-analyzer dist/stats.json\",\n    \"serve:prerender\": \"cd dist && http-server\"\n  },\n  \"private\": true,\n  \"dependencies\": {\n    \"@angular/animations\": \"^8.2.3\",\n    \"@angular/cdk\": \"^11.0.3\",\n    \"@angular/common\": \"8.2.3\",\n    \"@angular/compiler\": \"8.2.3\",\n    \"@angular/core\": \"8.2.3\",\n    \"@angular/forms\": \"8.2.3\",\n    \"@angular/material\": \"^11.0.3\",\n    \"@angular/platform-browser\": \"8.2.3\",\n    \"@angular/platform-browser-dynamic\": \"8.2.3\",\n    \"@angular/pwa\": \"0.803.0\",\n    \"@angular/router\": \"8.2.3\",\n    \"@angular/service-worker\": \"8.2.3\",\n    \"@gorniv/ngx-universal\": \"1.1.5\",\n    \"@nguniversal/common\": \"next\",\n    \"@nguniversal/express-engine\": \"next\",\n    \"@nguniversal/module-map-ngfactory-loader\": \"next\",\n    \"@ngx-meta/core\": \"7.0.0\",\n    \"@ngx-translate/core\": \"11.0.1\",\n    \"@ngx-translate/http-loader\": \"4.0.0\",\n    \"angular-bootstrap-md\": \"^8.1.1\",\n    \"bootstrap\": \"^4.3.1\",\n    \"chart.js\": \"^2.5.0\",\n    \"cookie-parser\": \"1.4.4\",\n    \"core-js\": \"3.2.1\",\n    \"font-awesome\": \"^4.7.0\",\n    \"hammerjs\": \"^2.0.8\",\n    \"intersection-observer\": \"^0.7.0\",\n    \"ng-lazyload-image\": \"^6.1.0\",\n    \"reflect-metadata\": \"0.1.13\",\n    \"rxjs\": \"6.5.2\",\n    \"rxjs-tslint\": \"^0.1.8\",\n    \"zone.js\": \"0.10.2\"\n  },\n  \"devDependencies\": {\n    \"@angular-devkit/build-angular\": \"^0.803.0\",\n    \"@angular/cli\": \"8.3.0\",\n    \"@angular/compiler-cli\": \"8.2.3\",\n    \"@angular/language-service\": \"8.2.3\",\n    \"@angular/platform-server\": \"8.2.3\",\n    \"@types/jasmine\": \"3.4.0\",\n    \"@types/jasminewd2\": \"2.0.6\",\n    \"@types/node\": \"12.0.10\",\n    \"chai\": \"4.2.0\",\n    \"codelyzer\": \"5.1.0\",\n    \"cross-env\": \"5.2.0\",\n    \"jasmine-core\": \"3.4.0\",\n    \"jasmine-spec-reporter\": \"4.2.1\",\n    \"karma\": \"4.2.0\",\n    \"karma-chrome-launcher\": \"2.2.0\",\n    \"karma-cli\": \"2.0.0\",\n    \"karma-coverage-istanbul-reporter\": \"2.1.0\",\n    \"karma-jasmine\": \"2.0.1\",\n    \"karma-jasmine-html-reporter\": \"1.4.2\",\n    \"mocha\": \"6.2.0\",\n    \"nodemon\": \"1.19.1\",\n    \"npm-delay\": \"1.0.4\",\n    \"npm-run-all\": \"4.1.5\",\n    \"prettier\": \"1.18.2\",\n    \"prettier-tslint\": \"0.4.2\",\n    \"protractor\": \"5.4.2\",\n    \"source-map-support\": \"0.5.13\",\n    \"ssri\": \"6.0.1\",\n    \"ts-mocha\": \"6.0.0\",\n    \"ts-node\": \"8.3.0\",\n    \"tslint\": \"5.19.0\",\n    \"typescript\": \"3.5.3\",\n    \"webpack-bundle-analyzer\": \"3.4.1\",\n    \"webpack-cli\": \"3.3.7\",\n    \"webpack-node-externals\": \"1.7.2\"\n  }\n}\n\n```\n\nError\n```\nERROR in ../node_modules/@nguniversal/common/common.d.ts:13:21 - error TS2694: Namespace '\"/../../../../../node_modules/@angular/core/core\"' has no exported member 'ɵɵFactoryDeclaration'.\n\n13     static ɵfac: i0.ɵɵFactoryDeclaration<StateTransferInitializerModule, never>;\n                       ~~~~~~~~~~~~~~~~~~~~\n../node_modules/@nguniversal/common/common.d.ts:14:21 - error TS2694: Namespace '\"/../../../../../node_modules/@angular/core/core\"' has no exported member 'ɵɵNgModuleDeclaration'.\n\n14     static ɵmod: i0.ɵɵNgModuleDeclaration<StateTransferInitializerModule, never, never, never>;\n                       ~~~~~~~~~~~~~~~~~~~~~\n../node_modules/@nguniversal/common/common.d.ts:15:21 - error TS2694: Namespace '\"/../../../../../node_modules/@angular/core/core\"' has no exported member 'ɵɵInjectorDeclaration'.\n\n15     static ɵinj: i0.ɵɵInjectorDeclaration<StateTransferInitializerModule>;\n                       ~~~~~~~~~~~~~~~~~~~~~\n../node_modules/@nguniversal/common/common.d.ts:23:21 - error TS2694: Namespace '\"/../../../../../node_modules/@angular/core/core\"' has no exported member 'ɵɵFactoryDeclaration'.\n\n23     static ɵfac: i0.ɵɵFactoryDeclaration<TransferHttpCacheModule, never>;\n                       ~~~~~~~~~~~~~~~~~~~~\n../node_modules/@nguniversal/common/common.d.ts:24:21 - error TS2694: Namespace '\"/../../../../../node_modules/@angular/core/core\"' has no exported member 'ɵɵNgModuleDeclaration'.\n\n24     static ɵmod: i0.ɵɵNgModuleDeclaration<TransferHttpCacheModule, never, [typeof i1.BrowserTransferStateModule], never>;\n                       ~~~~~~~~~~~~~~~~~~~~~\n../node_modules/@nguniversal/common/common.d.ts:25:21 - error TS2694: Namespace '\"/../../../../../node_modules/@angular/core/core\"' has no exported member 'ɵɵInjectorDeclaration'.\n\n25     static ɵinj: i0.ɵɵInjectorDeclaration<TransferHttpCacheModule>;\n                       ~~~~~~~~~~~~~~~~~~~~~\n../node_modules/@nguniversal/common/common.d.ts:35:21 - error TS2694: Namespace '\"/../../../../../node_modules/@angular/core/core\"' has no exported member 'ɵɵFactoryDeclaration'.\n\n35     static ɵfac: i0.ɵɵFactoryDeclaration<ɵTransferHttpCacheInterceptor, never>;\n                       ~~~~~~~~~~~~~~~~~~~~\n../node_modules/@nguniversal/common/common.d.ts:36:22 - error TS2694: Namespace '\"/../../../../../node_modules/@angular/core/core\"' has no exported member 'ɵɵInjectableDeclaration'.\n\n36     static ɵprov: i0.ɵɵInjectableDeclaration<ɵTransferHttpCacheInterceptor>;\n```\n\nWhat I've attempted so far\n\ndeleted node_modules and npm install\ndeleted both node_modules and package-lock.json and then used npm install\n\nalso used npm install --legacy-peer-deps with same outcome\n\n\nchanged node versions using NVM [v9, v12, v14, v16]\nuninstalled and reinstalled angular cli\n\nUpdate\nSomething I've left out is that there is another project that is basically the same, just used for a different purpose. It has the same package.json and has the same file structure. This other project in particular does work and compile properly to begin local development.\nWith that being said, I ended up just copying and pasting the node_modules folder from the other project and pasting it in this problematic one and it works! I haven't had any issues since.\n    ", "Answer": "\r\nThe errors you've shown are coming from the ```\nngUniversal/common```\n dependency, as you can see by the error messages. The dependency is set to ```\nnext```\n in your ```\npackage.json```\n. Try some specific version numbers until you get one that works. Do the same for any other packages with the same message. Versions can be found here: https://www.npmjs.com/package/@nguniversal/common\nThe first number is the major version, when that changes it indicates a breaking change.\n```\n\"@nguniversal/common\": \"next\",\n```\n\n```\n\"@nguniversal/common\": \"13.0.2\",\n```\n\n```\n\"@nguniversal/common\": \"12.1.3\",\n```\n\nYou should probably do this for any dependencies with the ```\nnext```\n value, since they are not necessarily stable releases.\nYou can also try the ```\nlatest```\n tag, which will get the latest stable release, (currently 13.0.2).\n```\n\"@nguniversal/common\": \"latest\",\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Perl: Accessing methods and json hashref data via the same object in class\r\n                \r\nI have an object class, that extends JSON with file read and write functionality.\nFrom the class usage perspective, I want to both use the methods of my class, and access the json hash with the same object.\n\nFor the example, I'll replace the file handling with from_json and to_json of a static string, but the principle would be the same. \n\nMy \"user\" code\n\n```\nuse Extended::JSON;\nmy $config = Extended::JSON->open('{\"Data\":{\"Property\":\"Data in json\"}}');\nprint \"Data from json: \" . $config->{Data}->{Property} . \"\\n\";\n$config->{Data}->{Property} = 'New Data in json';\n$config->write();\n```\n\n\nMy class\n\n```\nuse JSON;\npackage Extended::JSON;\n\nsub open\n{\n  my $class = shift;\n  my $data = shift;\n  my self = { _jsonhash => from_json( $data ) };\n  };\n  bless $self, $class;\n  return $self;\n}\nsub write\n{\n  my $self = shift;\n  print to_json( $self->{_jsonhash} );\n}\n```\n\n\nAs you see, the \"user\" code should be able to use the class attributes to directly access and change the returing hash from from_json, but the methods should act as for the class.\nI'm missing, if it is possible to somewhere universally \"wrap\" the attribute access to a class method to change the json hash, not the object attributes.\n\nThanks you!\nChristian\n    ", "Answer": "\r\nIf the class has no attributes (object variables), you can use the variable returned by ```\nfrom_json```\n as the object.\n\n```\npackage Extended::JSON;\n\nuse strict;\nuse warnings;\nuse feature qw( say );\n\nuse JSON qw( from_json to_json );\n\nsub open {\n   my ($class, $json) = @_;\n   return bless(from_json($json), $class);\n}\n\nsub write {\n   my ($self) = @_;\n   say to_json($self);\n}\n\n1;\n```\n\n\nYou can still use that approach if the class has attributes by hiding them when it comes time to regenerate the JSON.\n\n```\npackage Extended::JSON;\n\nuse strict;\nuse warnings;\nuse feature qw( say );\n\nuse JSON qw( from_json to_json );\n\nsub open {\n   my ($class, $json) = @_;\n   my $self = bless(from_json($json), $class);\n   # $self->{_foo} = ...;\n   return $self;\n}\n\nsub write {\n   my ($self) = @_;\n   delete local @$self{ grep /^_/, keys(%$self) };\n   say to_json($self);\n}\n\n1;\n```\n\n\nThe above requires the top-level of the JSON to be an \"object\" (hash), and it prevents certain keys from being used in that object. To avoid those limitations, we can use overload.\n\n```\npackage Extended::JSON;\n\nuse strict;\nuse warnings;\nuse feature qw( say );\n\nuse JSON qw( from_json to_json );\n\nuse overload '%{}' => \\&data;\n\nsub open {\n   my ($class, $json) = @_;\n   my $self = bless(\\{}, $class);\n   $$self->{data} = from_json($json);\n   # $$self->{foo} = ...;\n   return $self;\n}\n\nsub write {\n   my ($self) = @_;\n   say to_json($$self->{data});\n}\n\nsub data {\n   my ($self) = @_;\n   return $$self->{data};\n}\n\n1;\n```\n\n\nThe above creates a scalar-based object (as opposed to a hash-based object). Treating the reference as a hash reference gets the data obtained from the JSON, while treating the reference as a scalar reference gets the \"real\" object.\n\n```\n$ perl -e'\n   use Extended::JSON qw( );\n   my $o = Extended::JSON->open(q{{\"a\":123}});\n   CORE::say $o->{a};\n   $o->write;\n'\n123\n{\"a\":123}\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "I am getting a StackOverflow error in a hashfunction code but I cannot determine , can someone help me fix it/\r\n                \r\nI am creating a hash function by myself for my university assignment. My hash function works something like this ... It will take a string as input and add the ASCII values of every character into a integer variable named sum. This is done in the function named hash_func. Then in the function named MYHashfunc I have used recursion to decrease the value of sum such that it can be a value lesser than the size of the array in which I will store data in using my hash function. Since I am using seperate chaining method to resolve collisions , I used a LinkedList array.\nBut I am getting a stack overflow error when the function hash_func is called inside MYhashfunc. The code is given below:-\n\n```\npackage hashfunction;\n\nimport java.util.LinkedList;\nimport java.util.Scanner;\n\npublic class Hashfunction {\n\npublic static int MyhashFUNC(String str,int A){\n    int X=0;\n    int sum = hash_func(str);\n    if(sum<A)\n        return sum;\n    else{\n        X = X+sum%10;\n        sum /= 10;\n        return(MyhashFUNC(str, A));\n    }\n}\n\npublic static int hash_func(String str) {\n    int sum = 0;\n    int len = str.length();\n    for (int i = 0; i < len; i++) {\n        if (str.charAt(i) >= '0' && str.charAt(i) <= '9') {\n            sum += (int) str.charAt(i);\n        } else if (str.charAt(i) >= 'a' && str.charAt(i) <= 'z' || \n         str.charAt(i) >= 'A' && str.charAt(i) <= 'Z') {\n            sum += (int) str.charAt(i);\n        }\n    }\n   return sum;\n}\n\npublic static void main(String[] args) {\n    Scanner sc = new Scanner(System.in);\n    int N;\n    int z;\n    N = sc.nextInt();\n    String[] str_array = new String[N];\n    LinkedList<String>[] l_list = new LinkedList[N];\n    for (int i = 0; i < N; i++) {\n        l_list[i] = new LinkedList<String>();\n    }\n    for (int i = 0; i < N; i++) {\n        str_array[i] = sc.next();\n    }\n    for (int i = 0; i < N; i++) {\n        z = MyhashFUNC(str_array[i],N);\n        if(l_list[z].peek()!=\"-1\"){\n                l_list[z].set(z, str_array[i]);\n        }\n        else{\n            l_list[z].add(str_array[i]);\n        }\n    }\n\n    for (int i = 0; i < N; i++) {\n        int size = l_list[i].size();\n          for (int j = 0; j < size; j++) {\n              System.out.println(l_list[i].get(j));\n        }\n    }\n}\n}\n```\n\n    ", "Answer": "\r\nIn the method\n\n```\npublic static int MyhashFUNC(String str,int A){\n    int X=0;\n    int sum = hash_func(str);\n    if(sum<A)\n        return sum;\n    else{\n        X = X+sum%10;\n        sum /= 10;\n        return(MyhashFUNC(str, A));  // Call again MyhashFUNC with same parameters\n    }\n}\n```\n\n\nif ```\nsum >= a```\n you enter the ```\nelse```\n block and you call again the same method with the same parameters. This will generate the ```\nStackOverFlow```\n.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Why a variable is undefined in this circumstance?\r\n                \r\nI have a menu, which browses through a list of slide on a specific page. It all went well with the following code (HTML):\n\n```\n<ul class=\"dropdown\">\n      <li class=\"orange\"><a href=\"#\" data-jumpslide=\"2\"><div class=\"list-item one-liner\"><div class=\"left\"><span id=\"functionality-icon\"></span></div> <div class=\"right\">When to do it</div><div class=\"clear\"></div></div></a></li>\n      <li class=\"orange\"><a href=\"#\" data-jumpslide=\"3\" ><div class=\"list-item one-liner\"><div class=\"left\"><span id=\"functionality-icon\"></span></div> <div class=\"right\">Key faces</div><div class=\"clear\"></div></div></a></li>\n      <li class=\"orange\"><a href=\"#\" data-jumpslide=\"5\" ><div class=\"list-item one-liner\"><div class=\"left\"><span id=\"functionality-icon\"></span></div> <div class=\"right\">Functionalities</div><div class=\"clear\"></div></div></a></li>\n</ul>\n```\n\n\nand the jQuery is:\n\n```\n$('a[data-jumpslide]').each(function(idx, ele){\n    $(ele).on('click', function(){\n        var slideToJump = $(this).data('jumpslide');\n        window.slider.go(false, slideToJump+1);\n    });\n});\n```\n\n\nAs you can see, whenever the link is clicked, it just moved the slide on that specific page.\n\nBut now I had to create a universal menu, which could move the slider on any page. So if I am the page \"Page 1\" and I click a link for a 7th slide at \"Page 2\", the code should open the Page 2 and move to the slide 7.\n\nThis is how I changed the HTML:\n\n```\n<ul class=\"dropdown\">\n      <li class=\"orange\"><a href=\"page2.php\" data-jumpslide=\"2\" data-goto=\"page2\"><div class=\"list-item one-liner\"><div class=\"left\"><span id=\"functionality-icon\"></span></div> <div class=\"right\">When to do it</div><div class=\"clear\"></div></div></a></li>\n      <li class=\"orange\"><a href=\"page2.php\" data-jumpslide=\"2\" data-goto=\"page2\" ><div class=\"list-item one-liner\"><div class=\"left\"><span id=\"functionality-icon\"></span></div> <div class=\"right\">Key faces</div><div class=\"clear\"></div></div></a></li>\n      <li class=\"orange\"><a href=\"page2.php\" data-jumpslide=\"2\" data-goto=\"page2\" ><div class=\"list-item one-liner\"><div class=\"left\"><span id=\"functionality-icon\"></span></div> <div class=\"right\">Functionalities</div><div class=\"clear\"></div></div></a></li>\n</ul>\n```\n\n\nand this is how I changed the jQuery:\n\n```\n$('a[data-jumpslide]').each(function(idx, ele){\n    $(ele).on('click', function(e){\n        e.preventDefault();\n        var href = $(this).attr('href');\n        var slideToJump = $(this).data('jumpslide');\n        var gotoPage = $(this).data('data-goto');\n\n        slideToJump++;\n        if (gotoPage != '') {\n              window.slider.go(false, slideToJump);\n        }   \n        else {\n            var url = href+'#'+slideToJump;\n            window.location = url;\n        }\n    });\n});\n\nvar hash = window.location.hash;\n$(document).ready(function(idx, ele){\n  if (hash != '') {        \n        window.setTimeout(function () {\n          window.slider.go(false, hash);\n        }, 2000);\n    }\n    else {\n        console.log(\"No hash found  \" + hash);\n    }\n});\n```\n\n\nSo what happening here is that when a user clicks on any link that takes it to another page (say Page 2), it redirects and after it reaches there, it checks whether the pages has hash or not. If it does, it moves the slide to that number.\n\nNow all is good here except the slide doesn't move to the number! Even though the code is same, even though ```\nwindow.slider```\n is getting the slider in it, yet it still doesn't move.\n\nWhen I created a console.log on the Slide Change Callback function, it gives me \"undefined\". (code below)\n\n```\n    slideChangedCallBack: function(idx, slide) {\n\n        if (console) console.log('slideChangedCallBack: '+slide.data('pagename'));\n        if (s && s.t){\n            s.pageName = slide.data('pagename');\n            s.t();\n        }\n\n        ...\n    }\n```\n\n    ", "Answer": "\r\nSo, for anyone looking for exact change, I did this :-\n\n```\nvar hash = window.location.hash.substring(1);\n```\n\n\ninstead of this:-\n\n```\nvar hash = window.location.hash;\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Hash Table in C (find the frequency of every word)\r\n                \r\nI want to create a hash table for an exercise I have to send in my University.\nThe program will open a number of files, break each file's content to ```\n<<words>>```\n (tokens) and it will save each ```\n<<word>>```\n in a hash table with the frequency of each ```\n<<word>>```\n. \n\nIn case the word is already in the hash table , the program will increase the word's frequency.\n\nAt the end the program will print the words and it's frequencies accordingly.\nAlso the frequencies should be printed from the highest word frequency to the lowest.\nThe comparison of the ```\n<<words>>```\n will ignore upper and lower case letters. \n\nFor example if a file contains : ```\none two three four Two Three Four THREE FOUR FoUr```\n\nIt should print:\n\n\n  four 4\n  three 3\n  two 2\n  one 1\n\n\nThe professor gave us a template that we should complete but I'm really confused on what to do with the ```\ninsert_ht()```\n and ```\nclear_ht()```\n functions as well as the compare one. \n\nHere is the code :\n\n```\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n#include <ctype.h>\n\n#define HTABLE_SIZ 1001\n#define MAX_LINE_SIZ 1024\n\n/* Hash Table */\ntypedef struct node* link;\nstruct node { char *token; int freq; link next; };\n\nlink htable[HTABLE_SIZ] = { NULL }; /* Table of lists (#buckets) */\nint size = 0; /* Size (number of elements) of hash table */\n\nunsigned int hash (char *tok );\nvoid insert_ht (char *data);\nvoid clear_ht ( );\nvoid print_ht ( );\n\nvoid Process(FILE *fp);\n\n\nint main(int argc, char *argv[])\n{\n    int i;\n    FILE *fp;\n    for (i=1; i < argc; i++)\n    {\n        fp = fopen(argv[i],\"r\");\n        if (NULL == fp)\n        {\n            fprintf(stderr,\"Problem opening file: %s\\n\",argv[i]);\n            continue;\n        }\n    Process(fp);\n    fclose(fp);\n    }\n    print_ht();\n    clear_ht();\n    return 0;\n}\n\n\nvoid Process(FILE *fp)\n{\n    const char *seperators = \" ?!'\\\";,.:+-*&%(){}[]<>\\\\\\t\\n\";\n\n    char line[MAX_LINE_SIZ];\n    char *s;\n    while((fgets(line,MAX_LINE_SIZ, fp)) != NULL)\n    {\n        for (s=strtok(line,seperators); s; s=strtok(NULL,seperators))\n            insert_ht(s);\n        }\n    }\n\n/* Hash Function */\nunsigned int hash(char *tok)\n{\n    unsigned int hv = 0;\n    while (*tok)\n        hv = (hv << 4) | toupper(*tok++);\n    return hv % HTABLE_SIZ;\n}\n\n\nvoid insert_ht(char *token)\n{\n……………………………………………\n}\nvoid clear_ht()\n{\n……………………………………………\n}\nint compare(const void *elem1, const void *elem2)\n{\n……………………………………………\n}\nvoid print_ht()\n{\n    int i, j=0;\n    link l, *vector = (link*) malloc(sizeof(link)*size);\n    for (i=0; i < HTABLE_SIZ; i++)\n        for (l=htable[i]; l; l=l->next)\n            vector[j++] = l;\n        qsort(vector,size,sizeof(link),compare);\n        for (i=0; i < size; i++)\n            printf(\"%-50s\\t%7d\\n\",vector[i]->token,vector[i]->freq);\n        free(vector);\n}\n```\n\n    ", "Answer": "\r\nI'll answer you in a new post because it's hard to be exhaustive in comments.\n\n1. Malloc\n\n\n  Why would I need to use malloc then ? Shouldn't i write directly to the htable? (on the insert_ht() funtion) \n\n\nYou need to use malloc because you declare a char pointer in struct (```\nchar *token```\n). The thing is that you never initialize the pointer to anything, and as far you don't know the size of the token, you need to malloc every token. But, as you use ```\nstrdup(token)```\n, you don't need to malloc token because strdup does. So don't forget to free every token in order to avoid memory leaks.\n\n2. Segfault\n\nI can't test you code, but it seems like the following line causes the segmentation fault :\n\n```\nlist = htable[hashval]->token \n```\n\n\nIndeed, you try to access token while ```\nhtable[hashval]```\n is NULL, and to assign a char * to a link type (list).\n\nYou need to loop with this : \n\n```\nfor(list = htable[hashval]; list != NULL; list = list->next) { ... }\n```\n\n\n3. Notes\n\n\n```\nif (x=1)```\n should be ```\nif(x==1)```\n.\nDon't malloc new_list if you don't need to.\nBecause new_list if used when htable[hashval] is NULL, ```\nnew_list->next = htable[hashval];```\n will set new_list->next to NULL.\nYou should use the -Wall option in gcc (for warnings) and you may use valgrind to understand your segmentation faults. In this case, use gcc with debug mode (-g).\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Preventing \"Pass the Hash\" Attacks in a Web API?\r\n                \r\nI may not be going about this in the best way possible, but as a school project, I'm in a group where we are developing a system to handle checking in and checking out of dorm rooms and handling any charges that may arise because of damages to items in the room. We are digitizing the project and putting in on the web and writing a native iPhone app (part of the assignment, I'd rather just make it a universal web app, but oh well). For the iPhone app, we're putting together a web API but I'm having some doubts about our attempts to keep the API secure.\n\nOur process right now is a call to the API to log in with a username and a password hash, if it was a successful login, an Authentication Token is generated and returned to the iPhone in XML along with various other data. Each subsequent request requires this AuthToken. A user can either sign out, or if there's inactivity for 20 minutes, the AuthToken is destroyed server side. But this leaves the API open for the \"Pass The Hash\" attack where anybody listening in on the request can get the password hash out of the query string. Anybody with Wireshark and a simple filter can wait for somebody to sign in when everybody would be moving into the dorms and be able to manipulate just about anything.\n\nEvery single request is susceptible to the Pass The Hash attack. When logging in, the username and password can be repeated later to obtain a different AuthToken. Not only that, any already generated AuthTokens could be used and the session extended without the real user knowing.\n\nI have thought of the idea of tying the AuthToken to an IP address and rejecting requests that use a valid AuthToken from an alternate IP address, is this reliable or will the iPhone be jumping IP addresses when on the cell network instead of Wifi? I want to give any malicious users a hard time, but obviously not legitimate users.\n\nThe project is still in the early stages so now would be the time to make drastic changes to the API like this. Any tips on securing a web API would be awesome.\n    ", "Answer": "\r\nYour best bet would be to send everything over SSL. That will prevent anyone listening to the wire and sniffing either the password hash or the authentication token.\n\nYou should also consider sending a nonce to the client that gets hashed along with the password to prevent replay attacks.\n\nIt's also pretty easy to change the authentication token on each request. This prevents both replay and session-fixation. Just make sure the tokens are good random numbers.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to wrap web app to windows 10 desktop app?\r\n                \r\nI have a web application (accessed by clients using standard browser) which is available on custom individual url for each client and I would like to \"generate\" a \"custom desktop application\" (probably zip file with some exe file and some xml config file - where exe runs webview with url from xml) which can be installed on clients computer (Windows 10 support only is ok). This application when launched will open frameless window (no menu, no url bar etc) of a custom size with webview opening the defined https url (url will contain secret login hash for user / or session must be kept even after computer restart etc ..). This app should run only online (no offline mode needed).\nThe goal is to take one simple specific proces (entering simple data) from a complex web application and make this simple process easilly accessible for users (just click icon and enter data, submit to server, close, no login ever needed).\nI have no experience developing for Windows 10, but I expect there must be some easy \"universal app\" solution as probably more developers are solving the same problem. What are the most (time) effective, but standard and safe possibilities?\n    ", "Answer": "\r\nI found a nice tutorial: https://www.todesktop.com/guides/nativefier. This tool is based on Node.js, which works fine on macOS, Windows, or Linux.\nFirst, install Node.js on your computer. Then, run\n```\nnpm i nativefier -g\n```\n\nto install nativefier. You can wrap your web app into native app simply by running\n```\nnativefier \"your-url\" --name \"Application Name\"\n```\n\nYou can read the post for further information, like code signing your application, generating native installers, etc.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Unable to retrieve values from Axios Response data\r\n                \r\n```\n<script>\nexport default {\n  props: {\n    id: Number,\n  },\n\n  data () {\n    return {\n      password: null,\n      loading: true,\n      errored: false\n    }\n  },\n  created () {\n    axios\n      .get('http://localhost:3000/users?filter[where][id]=' + this.id)\n      .then(response => {\n        this.password = response.data.password\n      })\n      .catch(error => {\n        console.log(error)\n        this.errored = true\n      })\n      .finally(() => this.loading = false)\n  }\n}\n</script>\n```\n\nThe above is the code in question, i'm simply trying to retrieve a current user's password as plain text.\n(It's for a University Project, and I need to display the user's password hash as text to prove it's hashed).\nThe above code will return this array in dev tools:\n```\n[ \n  { \n    \"id\": 1, \n    \"user\": \"SomeUsername\", \n    \"password\": \"SomePasswordHash\" \n  } \n]\n}\n```\n\nHowever, If I run that code as whole, it will not return anything when using:\n```\n{{ this.password }}\n```\n\nI have no idea why this is.\nEdit:\nIf I change password to info, and then this.info = response.data, then access that array via this.info[0].password it works, however when I refresh I'll get this error: Cannot read properties of null (reading '0').\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "MD5 value mismatch between SQL server and PostgreSQL\r\n                \r\nIn order to write some code to do consistency check of data stored in both SQL server and PostgreSQL, I plan to calculate the MD5 on table data for both the databases, and verify if they are equal.\nThis works fine as long as data is plain text ( ANSI ) as below:\n\n```\nsql-server> SELECT master.dbo.fn_varbintohexstr(HashBytes('MD5', 'a'));\n0x0cc175b9c0f1b6a831c399e269772661\n\n\npostgres=# select MD5('a');\n0cc175b9c0f1b6a831c399e269772661\n```\n\n\nNow, If I try to use some Hangul(Korean) characters, MD5 match fails:\n\n```\nsql-server> SELECT master.dbo.fn_varbintohexstr(HashBytes('MD5', '무'));\n0x7827b52f65d9f7777d37071cbbbf7f2d\n\n\npostgres=# select MD5('무');\ncb3e9be1a3a28b355eabae1fa1e291b3\n```\n\n\nAs per my understanding, reason of mismatch is that unicode characters are stored as  UCS-2 encoding (fixed 16 bits encoding) in SQL server and UTF-8 encoding in PostgreSQL. And as MD5 works on character bits, the character bits sequence would be different in both SQL server and PostgreSQL.\n\nAS I have been dealing mostly with Hangul character-set, the workaround I used in PostgreSQL is to convert the encoding from UTF-8 to UHC ( Universal Hangul Character-set) before calculating hash as below:\n\n```\npostgres=# select MD5(CONVERT('무'::bytea,'UTF8','UHC'));\n7827b52f65d9f7777d37071cbbbf7f2d\n```\n\n\nAs you can see, the above hash value is same as that for SQL server.\n\nAll is fine as long as I am dealing with Hangul characters. But some tables contains mix of Hangul and Chinese characters, and the conversion fails in that case:\n\n```\npostgres=# select MD5(CONVERT('무么'::bytea,'UTF8','UHC'));\nERROR:  character 0xe4b988 of encoding \"UTF8\" has no equivalent in \"UHC\"\npostgres=# \n```\n\n\nThe error makes sense as there are no equivalent of Chinese characters in UHC character-set.\n\nHow can I make it work? Basically, I need to find way to convert UCS-2 to UTF-8 in SQL server, or to convert UTF-8 to UCS-2 in PostgreSQL before calculating MD5. I want to perform all these operations within database engine, and not load data in external application to calculate MD5, as some tables have huge data set.\n\nSQL server version 2005\nPostgreSQL version 9.1\n    ", "Answer": "\r\nUnfortunately, PostgreSQL does not support UTF-16 / UCS-2 either.\n\nBut, you can write a function, to convert utf8 ```\ntext```\n to ucs2 binary data (```\nbytea```\n):\n\n```\ncreate or replace function text_to_ucs2be(input_in_utf8 text)\n  returns bytea\n  immutable\n  strict\n  language sql\nas $$\n  select decode(string_agg(case\n           when code_point < 65536\n           then lpad(to_hex(code_point), 4, '0')\n         end, ''), 'hex')\n  from   regexp_split_to_table(input_in_utf8, '') chr,\n         ascii(chr) code_point\n$$;\n\ncreate or replace function text_to_ucs2le(input_in_utf8 text)\n  returns bytea\n  immutable\n  strict\n  language sql\nas $$\n  select decode(string_agg(case\n           when code_point < 65536\n           then lpad(to_hex(code_point & 255), 2, '0')\n             || lpad(to_hex(code_point >> 8), 2, '0')\n         end, ''), 'hex')\n  from   regexp_split_to_table(input_in_utf8, '') chr,\n         ascii(chr) code_point\n$$;\n```\n\n\nNote: these functions above will strip out any non-BMP code-points (therefore the name ucs2 in them).\n\nThe following statements should give you the same results:\n\n```\n-- on PostgreSQL\nselect md5(text_to_ucs2le('무'));\n\n-- on SQL server\nselect master.dbo.fn_varbintohexstr(HashBytes('MD5', N'무'));\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Unable to retrieve values from Axios Response data\r\n                \r\n```\n<script>\nexport default {\n  props: {\n    id: Number,\n  },\n\n  data () {\n    return {\n      password: null,\n      loading: true,\n      errored: false\n    }\n  },\n  created () {\n    axios\n      .get('http://localhost:3000/users?filter[where][id]=' + this.id)\n      .then(response => {\n        this.password = response.data.password\n      })\n      .catch(error => {\n        console.log(error)\n        this.errored = true\n      })\n      .finally(() => this.loading = false)\n  }\n}\n</script>\n```\n\nThe above is the code in question, i'm simply trying to retrieve a current user's password as plain text.\n(It's for a University Project, and I need to display the user's password hash as text to prove it's hashed).\nThe above code will return this array in dev tools:\n```\n[ \n  { \n    \"id\": 1, \n    \"user\": \"SomeUsername\", \n    \"password\": \"SomePasswordHash\" \n  } \n]\n}\n```\n\nHowever, If I run that code as whole, it will not return anything when using:\n```\n{{ this.password }}\n```\n\nI have no idea why this is.\nEdit:\nIf I change password to info, and then this.info = response.data, then access that array via this.info[0].password it works, however when I refresh I'll get this error: Cannot read properties of null (reading '0').\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is a data structure implementation with O(1) search possible without using arrays?\r\n                \r\nI am currently taking a university course in data structures, and this topic has been bothering me for a while now (this is not a homework assignment, just a purely theoretical question).\n\nLet's assume you want to implement a dictionary. The dictionary should, of course, have a search function, accepting a key and returning a value.\n\nRight now, I can only imagine 2 very general methods of implementing such a thing:\n\n\nUsing some kind of search tree, which would (always?) give an O(log n) worst case running time for finding the value by the key, or,\nHashing the key, which essentially returns a natural number which corresponds to an index in an array of values, giving an O(1) worst case running time.\n\n\nIs O(1) worst case running time possible for a search function, without the use of arrays?\n\nIs random access available only through the use of arrays?\nIs it possible through the use of a pointer-based data structure (such as linked lists, search trees, etc.)?\n\nIs it possible when making some specific assumptions, for example, the keys being in some order?\n\nIn other words, can you think of an implementation (if one is possible) for the search function and the dictionary that will receive any key in the dictionary and return its value in O(1) time, without using arrays for random access?\n    ", "Answer": "\r\nHere's another answer I made on that general subject.\nEssentially, algorithms reach their results by processing a certain number of bits of information. The length of time they take depends on how quickly they can do that.\n\nA decision point having only 2 branches cannot process more than 1 bit of information. However, a decision point having n branches can process up to log(n) bits (base 2).\n\nThe only mechanism I'm aware of, in computers, that can process more than 1 bit of information, in a single operation, is indexing, whether it is indexing an array or executing a jump table (which is indexing an array).\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "SQLite.net Extensions loads the same entity multiple times rather than returning the same reference\r\n                \r\nI'm using the PCL version of SQLite.Net Extensions in a Windows 10 universal app.  This is my first time using it.  It generally seems to be working, but it appears to be loading an entity multiple times rather than reusing a reference to the same object.\n\nAccording to the SQLite.Net extensions documentation:\n\n\n  SQLite-Net Extensions will ensure that any object is loaded only once from the database and will resolve circular dependencies and inverse relationships while maintaining integral reference. This means that any returned object of the same class with the same identifier will be a reference to exactly the same object.\n\n\nThat does not appear to be happening for me.  Here is my code:\n\n```\npublic class Group {\n    [PrimaryKey, AutoIncrement]\n    public Guid Id { get; set; }\n\n    public string GroupName { get; set; }\n\n    public override string ToString() {\n        return string.Format(\"Group [ID: {0}, HashCode: {1}] GroupName={2}\", Id.ToString().Last(4), GetHashCode(), GroupName);\n    }\n\n    [ManyToMany(typeof(GroupMember), CascadeOperations = CascadeOperation.CascadeRead)]\n    public List<Member> Members { get; set; }\n\n    public void DebugIt() {\n        Debug.WriteLine(this);\n        foreach (var member in Members) Debug.WriteLine(\"    \" + member);\n    }\n}\n\npublic class Member {\n    [PrimaryKey, AutoIncrement]\n    public Guid Id { get; set; }\n\n    public string Name { get; set; }\n\n    public override string ToString() {\n        return string.Format(\"Member [ID: {0}, HashCode: {1}] Name={2}\", Id.ToString().Last(4), GetHashCode(), Name);\n    }\n\n    [ManyToMany(typeof (GroupMember), CascadeOperations = CascadeOperation.CascadeRead)]\n    public List<Group> Groups { get; set; }\n\n    public void DebugIt() {\n        Debug.WriteLine(this);\n        foreach (var group in Groups) Debug.WriteLine(\"    \" + group);\n    }\n}\n\npublic class GroupMember {\n    [PrimaryKey, AutoIncrement]\n    public Guid Id { get; set; }\n    [ForeignKey(typeof(Group))]\n    public Guid GroupID { get; set; }\n    [ForeignKey(typeof(Member))]\n    public Guid MemberId { get; set; }\n}\n\npublic class DatabaseGroups {\n    private const string FileName = \"db.sqlite\";\n    private SQLiteConnection _db;\n\n    public async Task<bool> LoadAsync() {\n        var exists = await FileHelper.DoesFileExistAsync(FileName);\n        _db = new SQLiteConnection(new SQLitePlatformWinRT(), DatabaseFullPath,\n            exists ? SQLiteOpenFlags.ReadWrite : SQLiteOpenFlags.Create | SQLiteOpenFlags.ReadWrite);\n        if (!exists) InitializeWithDefaults();\n        return await FileHelper.DoesFileExistAsync(FileName);\n    }\n\n    private void InitializeWithDefaults() {\n        _db.CreateTable<Group>();\n        _db.CreateTable<Member>();\n        _db.CreateTable<GroupMember>();\n\n        var group1 = new Group {GroupName = \"Group 1\"};\n        var group2 = new Group {GroupName = \"Group 2\"};\n        var member1 = new Member {Name = \"Bob\"};\n        var member2 = new Member {Name = \"Jane\"};\n\n        _db.Insert(group1);\n        _db.Insert(group2);\n        _db.Insert(member1);\n        _db.Insert(member2);\n\n        group1.Members = new List<Member> {member1, member2};\n        _db.UpdateWithChildren(group1);\n\n        group2.Members = new List<Member> {member1, member2};\n        _db.UpdateWithChildren(group2);\n    }\n\n    private static StorageFolder DatabaseFolder {\n        get { return ApplicationData.Current.LocalFolder; }\n    }\n\n    private static string DatabaseFullPath {\n        get { return Path.Combine(DatabaseFolder.Path, FileName); }\n    }\n\n    public void DebugIt() {\n        foreach (var groupId in _db.Table<Group>().Select(g => g.Id)) {\n            var group = _db.GetWithChildren<Group>(groupId);\n            group.DebugIt();\n        }\n        foreach (var memberId in _db.Table<Member>().Select(m => m.Id)) {\n            var member = _db.GetWithChildren<Member>(memberId);\n            member.DebugIt();\n        }\n    }\n}\n\nprotected override async void OnLaunched(LaunchActivatedEventArgs e) {\n    _db = new DatabaseGroups();\n    await _db.LoadAsync();\n    _db.DebugIt();\n```\n\n\nWhen it runs, I create some initial data.  I then load those objects using GetWithChildren and debug it.  Here are the results:\n\n```\nGroup[ID: 4858, HashCode: 51192825] GroupName = Group 1\n    Member[ID: dbfa, HashCode: 64971671] Name = Jane\n    Member[ID: b047, HashCode: 30776584] Name = Bob\nGroup[ID: 30f0, HashCode: 53439890] GroupName = Group 2\n    Member[ID: dbfa, HashCode: 36062904] Name = Jane\n    Member[ID: b047, HashCode: 9089598] Name = Bob\nMember[ID: b047, HashCode: 20305449] Name = Bob\n    Group[ID: 30f0, HashCode: 9648315] GroupName = Group 2\n    Group[ID: 4858, HashCode: 29803642] GroupName = Group 1\nMember[ID: dbfa, HashCode: 36899882] Name = Jane\n    Group[ID: 30f0, HashCode: 23318221] GroupName = Group 2\n    Group[ID: 4858, HashCode: 60865449] GroupName = Group 1\n```\n\n\nAs you can see, the objects appear to be loading correctly, but the object references for Group 1 (for example) are different (see the hash code).  \n\nAm I perhaps misinterpreting the way SQLite.Net Extensions handles object references?  Perhaps it handles reusing object references within a single call to GetWithChildren, but not across multiple calls on the same SQLiteConnection?\n\nIf that's the case, how are you supposed to load a more complex object graph with these sorts of relationships?\n    ", "Answer": "\r\nYou are correct, SQLite-Net Extensions caches the objects for recursive calls to avoid reference loops and handle inverse relationships, but it doesn't cache the objects between calls.\n\nSQLite-Net Extensions is just a thin layer over SQLite.Net, if integral reference is important for you, you can go back to manual queries for more complexes operations.\n\nIf you have any suggestion or pull request, they are always welcome ;)\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Adldap 2 can connect and find a user but cannot authenticate\r\n                \r\nQuestion: why can I not authenticate a known registered user through Adldap despite being able to access information about the user using Adldap on laravel 5.2?\n\nI am attempting to use Adldap on laravel 5.2 to authenticate users at a university. I have successfully managed to connect to the ldap server with the admin credentials and can even retrieve information about the user. \n`\n    \n\n```\nnamespace App\\Http\\Controllers;\n\nuse Auth;\nuse Input;\nuse Adldap;\n\nclass AuthController extends Controller\n{\n    public function authenticate()\n    {\n\n        $username = Input::get('username');\n\n        $password = Input::get('password');\n\n        $authentic = Adldap::authenticate($username,$password);\n\n        $userData = Adldap::users()->find($username);\n\n        var_dump( $authentic );\n\n        dd( $userData );\n\n    }\n}\n```\n\n\n`\n\nwhen I try to log in, dumping $authentic gives me false despite having the correct password (I dumped it as well to check). However, with the same username if I dump $userData i get a massive array of (correct) user information. Using my username, if I open the $userData object up I can see what email groups i'm in, my campus mailing address, my work title etc.\nDumped variables. I am very new to using ldap and am not quite sure how everything works. Also, its probably worth noting that despite me being the guy doing the setup and such I do not have much access to the servers. Everything is on an as needed basis.\n\nOne though was that the ldap server took care of any password hashing on that end. However, since i'm getting connected but the authentication fails could it be that I need to hash the password on my end? Please explain any solutions in detail. As an Ag engineer none of this is exactly my field but sometimes branching out is a necessity.\n\nThere are no errors. I'm on wamp and in logs/php_error (I assume this is the equivalent local version of /var/log/debug). Additionally, apache_error shows no problems.\n    ", "Answer": "\r\nYou better check the messages created in /var/log/debug while trying to log in. Please add these messages to this post.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How to set password to user in database directly?\r\n                \r\nI have a website for university timetable connected to microsoft sql server, there I have django table auth_user. Users can't register by themselves, university provides them with username and password. So in the table auth_user I have to fill data manually, but how can I fill the field which is responsible for password since it has to be hashed? I found only way to set password is to log in as admin, and change passwords in admin site, but that is not quite correct in terms of working with database as if I had to fill more than 100 students, it would be tiresome to do so. Maybe there is another approach to fill passwords directly in the database?\n    ", "Answer": "\r\nYou can set the password for a user in Django by using the ```\nset_password```\n method\n```\nfrom django.contrib.auth.models import User\nu = User.objects.get(username='john')\nu.set_password('new password')\nu.save()\n```\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "GOST digital signature verification with OpenSSL in Ruby\r\n                \r\nI have my certificate, source document, detached signature in Base64 format. Signature created by UEC (Universal Electronic Card - Russian smart card project) on Windows in CryptoARM program (I think GOST hash function is used).\n\nI'm using Ubuntu 13.10 and have installed OpenSSL 1.0.1e (GOST support is included by default AFAIK).\n\nI've installed UEC CA sertificate in Ubuntu CA store:\n\n```\nsudo cp ~/uec/uec.cer /usr/local/share/ca-certificates/uec.crt\nsudo update-ca-certificates\n```\n\n\nAnd console verification succeeds (sure if file wasn't changed):\n\n```\n$ openssl smime -verify -engine gost -inform DER -in ~/uec/to_be.txt.sig -content ~/uec/to_be.txt \nengine \"gost\" set.\nVerification successful\nOriginal file contents goes here\n```\n\n\nSo, I'm trying to do validation in Ruby (2.0.0 and 2.1.0), as noted in this question: Digital signature verification with OpenSSL\n\n```\nrequire 'openssl'\nOpenSSL::Engine.load\nengine = OpenSSL::Engine.by_id('gost')\n\ncert_store = OpenSSL::X509::Store.new\ncert_store.set_default_paths\nmy_cert = File.read('/home/envek/uec/envek-b64.cer')\ndata = File.read('/home/envek/uec/to_be.txt')\nsignature = OpenSSL::PKCS7.new(File.read('/home/envek/uec/to_be.txt.der.sig'))\n\nsignature.verify([my_cert], cert_store, data, OpenSSL::PKCS7::DETACHED || OpenSSL::PKCS7::NOVERIFY)\n# => false\nsignature\n# => #<OpenSSL::PKCS7:0x00000002168918 @data=\"\\xEF\\xBB\\xBF\\xD0\\xAD\\xD1\\x82\\xD0\\xBE \\xD1\\x84\\xD0\\xB0\\xD0\\xB9\\xD0\\xBB, \\xD0\\xBA\\xD0\\xBE\\xD1\\x82\\xD0\\xBE\\xD1\\x80\\xD1\\x8B\\xD0\\xB9 \\xD1\\x8F \\xD0\\xBF\\xD0\\xBE\\xD0\\xB4\\xD0\\xBF\\xD0\\xB8\\xD1\\x88\\xD1\\x83\", @error_string=\"unsupported algorithm\">\n```\n\n\nSo, I really don't know why it's just returns false, does my engine loading impacts anything or not. How to say ```\nPKCS7#verify```\n to use correct algorithm, provided by GOST engine?\n\nAny ideas?\n\nFiles:\n\n\nMy certificate: https://www.dropbox.com/s/16pvqwyd58h93u0/envek-b64.cer\nUEC CA certificate: https://www.dropbox.com/s/vylglojbrqk5uww/uec_b64.cer \nOriginal file: https://www.dropbox.com/s/ciw1tr63eullhrx/to_be.txt \nSignature: https://www.dropbox.com/s/4kx77qdaoesi6gf/to_be.txt.sig\n\n\nP.S. To make OpenSSL work properly next steps are required (found here):\n\nThis string should be added at most top of ```\n/etc/ssl/openssl.cnf```\n\n\n```\nopenssl_conf = openssl_def\n```\n\n\nThese strings should be added at most bottom of ```\n/etc/ssl/openssl.cnf```\n\n\n```\n[openssl_def]\nengines = engine_section\n[engine_section]\ngost = gost_section\n[gost_section]\ndefault_algorithms = ALL\nengine_id = gost\n```\n\n\nAfter that next command should show next output:\n\n```\n$ openssl ciphers | tr \":\" \"\\n\" | grep GOST\nGOST2001-GOST89-GOST89\nGOST94-GOST89-GOST89\n```\n\n    ", "Answer": "\r\nWeird thing, I've just tried to call ```\nOpenSSL::Engine#set_default```\n method with value of ```\n0xFFFF```\n on it. Just:\n\n```\nengine.set_default(0xFFFF)\n```\n\n\nAnd it works!!!\n\nDocumentation is absolutely unclear about it. What it does, what flags it receives as values? Anyone, explain me, please.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "graph - What are the disadvantages if I replace each linked list in adjacency-list with hash table?\r\n                \r\nIn CLRS excise 22.1-8 (I am self learning, not in any universities)\n\n\n  Suppose that instead of a linked list, each array entry Adj[u] is a\n  hash table containing the vertices v for which (u,v) ∈ E. If all\n  edge lookups are equally likely, what is the expected time to\n  determine whether an edge is in the graph? What disadvantages does\n  this scheme have? Suggest an alternate data structure for each edge\n  list that solves these problems. Does your alternative have\n  disadvantages compared to the hash table?\n\n\nSo, if I replace each linked list with hash table, there are following questions:\n\n\nwhat is the expected time to determine whether an edge is in the graph?\nWhat are the disadvantages?\nSuggest an alternate data structure for each edge list that solves these problems\nDoes your alternative have disadvantages compared to the hash table?\n\n\nI have the following partial answers:\n\n\nI think the expected time is O(1), because I just go Hashtable t = Adj[u], then return t.get(v);\nI think the disadvantage is that Hashtable will take more spaces then linked list.\n\n\nFor the other two questions, I can't get a clue. \n\nAnyone can give me a clue?\n    ", "Answer": "\r\nThe answer to question 3 could be a binary search tree.  \n\nIn an adjacency matrix, each vertex is followed by an array of V elements. This O(V)-space cost leads to fast (O(1)-time) searching of edges.\n\nIn an adjacency list, each vertex is followed by a list, which contains only the n adjacent vertices.  This space-efficient way leads to slow searching (O(n)).\n\nA hash table is a compromise between the array and the list.  It uses less space than V, but requires the handle of collisions in searching.\n\nA binary search tree is another compromise -- the space cost is minimum as that of lists, and the average time cost in searching is O(lg n).\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Is there a way to force usage of named parameters instead of your standard function_name(var, var)?\r\n                \r\nI really like named parameters, as they greatly help with the readability of my code.\n\nRuby uses pseudo-named parameters with hashes, and I've implemented a few methods using that technique, but adding these three lines to every method with parameters would get cumbersome:\n\n```\ndef something_does_something_with(parameters = {})\n  default_params = {:some => option, :another => something}\n  parameters = default_params.merge(parameters)\n  ...\nend\n```\n\n\nor the method header could be like this:\n\n```\ndef something_does_something_with(parameters = {:some => option, :another => something})\n```\n\n\nbut then I think if I supply any parameters at all, it overrides the entire default hash.\n\nWhen I worked with Objective-C, named-variables were my favorite thing in the programming universe.\n\nIs there a way to modify the default way Ruby looks at method headers such that named-parameterss are required, or at least easier?\n    ", "Answer": "\r\nYou definitely can not use second example because it will work only in case when you're passing a full set of parameters.\n\nIn reference to your first example you could make it short like this:\n\n```\ndef something_does_something_with(parameters = {})\n  parameters = {:some=>option,:another=>something}.merge(parameters)\n  ...\nend\n```\n\n\nAnd finally, the named parameters is planning to implement in the next version of Ruby - Ruby 2.0\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "How do I 'hash' my passwords in a user table in MySQL (PhpMyAdmin)?\r\n                \r\nI am a beginner to all this, I am currently doing it as a module in my university assignment and currently not at university therefore I cannot get help there.\nI am trying to encrypt the passwords I have stored in a user table, I have no idea what to do, I'm aware a lot of them can be decrypted and that is not something I want. The photo is of my current users table, the passwords are randomly generated - not encrypted. Can anyone please help me on how to hash my passwords and still make it possible to login properly, I am a complete beginner to all this and just need some guidance on solving this problem,\nthanks.\n```\nCREATE TABLE databaseusers ( \n    databaseruser_id int(5) NOT NULL, \n    database_user varchar(25) NOT NULL, \n    database_password varchar(40) NOT NULL \n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n```\n\nUsers table\n\n    ", "Answer": "\r\nYou cannot safely store user passwords with SQL alone, you need a dedicated password-hash-function of your programming environment. Instead of encryption one should use hashing, the password is stored in a unretrieveable form then, but this is enough to do a verification of a login.\nIf your language is PHP, you can solve it with the password_hash() function, the field for the password hash should then be ```\nvarchar(255)```\n:\n```\n// Hash a new password for storing in the database.\n// The function automatically generates a cryptographically safe salt.\n$hashToStoreInDb = password_hash($_POST['password'], PASSWORD_DEFAULT);\n\n// Check if the hash of the entered login password, matches the stored hash.\n// The salt and the cost factor will be extracted from $existingHashFromDb.\n$isPasswordCorrect = password_verify($_POST['password'], $existingHashFromDb);\n```\n\nExample code for PHP you can find in this answer and if you are interested in more information about safely storing passwords you may have a look at my tutorial.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Gem for Finding Associated University of Email Addresses\r\n                \r\nI don't think this exists, but I thought I should ask before reverting to finding some CSV table with this data. I receive a college email from the user and I want to find the name of the University that is associated with the suffix of the email. For example, ```\njason@uga.edu```\n should be associated to ```\nUniversity of Georgia```\n. Is there some Rails gem that could help me solve my dilemma or am I going to have to create some database or hash with this data?\n    ", "Answer": "\r\nYou could probably hack something together with: https://github.com/weppos/whois.\n\n```\nrequire 'whois'\n\nr = Whois.whois(\"uga.edu\")\nr.registrant_contact.organization # => \"University of Georgia\" \n```\n\n\nIf you have considerable amount of traffic, think about a caching mechanism that'll store the WHOIS information, so you don't have to ask the WHOIS server for each and every request.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Clustering text using 3 different approches (MinHash, HAC, K-means)\r\n                \r\nI have a set of university courses (around 30000). Each course have the following attributes, here is an example:\n\n\nTitle: Machine Learning\nInstitution name: Department of Information Technology\nDescription: After the course the student should be able to: set up and solve typical machine learning problems, by implementation or\nwith simulation tools, determine different learning methods\napplicability for different types of learning problems, ie know the\nmethods strengths and weaknesses, set a good representation of the\ndata, recognize the typical effects of bad choices and determine how\nto improve the results and describe how and why the machine learning\nand natural computation methods.\\\n\n\nI want to cluster these courses to help users with suggestions on similar courses that they liked or searched for. An example would be if a user is searching for \"Machine learning\" the system could suggest \"Artificial intelligence\" since the content of those courses are a little bit similar. I am thinking of three different approaches to do this.\n\n\nTF-IDF + spherical K-means\nlevenstein similarity + hierarchical agglomerative clustering\nJaccard similarity + minhash + locality sensitive hashing (LSH)\n\n\nMy plan is to try this methods in Matlab, come up with the best one and eventually (if I have time enough) implement it. I am planning to do this as my master thesis, all information on this would be highly appreciated. Does it make sense to compare this three methods on the given dataset?\n\nThank you.\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Filter large json file with Ruby\r\n                \r\nAs a total beginner of programming, I am trying to filter a JSON file for my master's thesis at university. The file contains approximately 500 hashes of which 115 are the ones I am interested in. \n\nWhat I want to do:\n\n(1) Filter the file and select the hashes I am interested in\n\n(2) For each selected hash, return only some specific keys\n\nThe format of the array with the hashes (\"loans\") included:\n\n```\n{\"header\": {\n   \"total\":546188,\n   \"page\":868,\n   \"date\":\"2013-04-11T10:21:24Z\",\n   \"page_size\":500},\n \"loans\": [{\n   \"id\":427853,\n   \"name\":\"Peter Pan\",\n   ...,\n   \"status\":\"expired\",\n   \"paid_amount\":525,\n   ...,\n   \"activity\":\"Construction Supplies\",\n   \"sector\":\"Construction\",\" },\n    ... ]\n }\n```\n\n\nBeing specific, I would like to have the following:\n\n(1) Filter out the \"loans\" hashes with \"status\":\"expired\"\n\n(2) Return for each such \"expired\" loan certain keys only: \"id\", \"name\", \"activity\", ...\n\n(3) Eventually, export all that into one file that I can analyse in Excel or with some stats software (SPSS or Stata)\n\nWhat I have come up with myself so far is this:\n\n```\nrequire 'rubygems'\nrequire 'json'\n\ntoberead = File.read('loans_868.json')\nanother = JSON.parse(toberead)\n\nread = another.select {|hash| hash['status'] == 'expired'}\n\nputs hash\n```\n\n\nThis is obviously totally incomplete. And I feel totally lost.\nRight now, I don't know where and how to continue. Despite having googled and read through tons of articles on how to filter JSON...\n\nIs there anyone who can help me with this?\n    ", "Answer": "\r\nThe JSON will be parsed as a hash, 'header' is one key, 'loans' is another key.\n\nso after your JSON.parse line, you can do\n\n```\nloans = another['loans']\n```\n\n\nnow loans is an array of hashes, each hash representing one of your loans.\nyou can then do\n\n```\nexpired_loans = loans.select {|loan| loan['status'] == 'expired'}\nputs expired_loans\n```\n\n\nto get at your desired output.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "apt-get update fail with Hash Sum mismatch in docker\r\n                \r\n```\nFROM ubuntu:14.04\n\nRUN apt-get update\nRUN apt-get install -y build-essential git\nRUN apt-get install -y python python-dev python3 python3-dev python3-pip\nRUN apt-get install -y nginx uwsgi uwsgi-plugin-python3 supervisor\n\nRUN pip3 install uwsgi\n\n#RUN echo \"\\ndaemon off;\" >> /etc/nginx/nginx.conf\nRUN chown -R www-data:www-data /var/lib/nginx\n\n# UWSGI\nENV UWSGIVERSION 2.0.11.2\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n            build-essential \\\n            libjansson-dev \\\n            libpcre3-dev \\\n            libssl-dev \\\n            libxml2-dev \\\n            wget \\\n            zlib1g-dev\n\nRUN cd /usr/src && \\\n    wget --quiet -O - http://projects.unbit.it/downloads/uwsgi-${UWSGIVERSION}.tar.gz | \\\n    tar zxvf -\n\nRUN cd /usr/src/uwsgi-${UWSGIVERSION} && make\nRUN cp /usr/src/uwsgi-${UWSGIVERSION}/uwsgi /usr/local/bin/uwsgi\nRUN PYTHON=/usr/local/python3.4/bin/python3.4\nRUN cd /usr/src/uwsgi-${UWSGIVERSION} && ./uwsgi --build-plugin \"plugins/python python34\"\nRUN mkdir -p /usr/local/lib/uwsgi/plugins\nRUN cp /usr/src/uwsgi-${UWSGIVERSION}/*.so /usr/local/lib/uwsgi/plugins\n\n# Java\nENV VERSION 7\nENV UPDATE 80\nENV BUILD 15\n\nENV JAVA_HOME /usr/lib/jvm/java-${VERSION}-oracle\nENV JRE_HOME ${JAVA_HOME}/jre\n\nRUN apt-get update && apt-get install ca-certificates curl -y && \\\n    curl --silent --location --retry 3 --cacert /etc/ssl/certs/GeoTrust_Global_CA.pem \\\n    --header \"Cookie: oraclelicense=accept-securebackup-cookie;\" \\\n    http://download.oracle.com/otn-pub/java/jdk/\"${VERSION}\"u\"${UPDATE}\"-b\"${BUILD}\"/server-jre-\"${VERSION}\"u\"${UPDATE}\"-linux-x64.tar.gz \\\n    | tar xz -C /tmp && \\\n    mkdir -p /usr/lib/jvm && mv /tmp/jdk1.${VERSION}.0_${UPDATE} \"${JAVA_HOME}\" && \\\n    apt-get autoclean && apt-get --purge -y autoremove && \\\n    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n\nRUN update-alternatives --install \"/usr/bin/java\" \"java\" \"${JRE_HOME}/bin/java\" 1 && \\\n    update-alternatives --install \"/usr/bin/javac\" \"javac\" \"${JAVA_HOME}/bin/javac\" 1 && \\\n    update-alternatives --set java \"${JRE_HOME}/bin/java\" && \\\n    update-alternatives --set javac \"${JAVA_HOME}/bin/javac\"\n\n# Node\nRUN \\\n  apt-get update && \\\n  apt-get install -yqq curl \\\n  wget git python build-essential g++ libkrb5-dev libfreetype6 libfontconfig \\\n  libjpeg8 libpng12-0 libicu-dev libcurl3 libcurl3-gnutls libcurl4-openssl-dev \\\n  libcurl3 libcurl3-gnutls libcurl4-openssl-dev && \\\n  curl --silent --location https://deb.nodesource.com/setup_0.10 | bash - && \\\n  apt-get install -yqq nodejs && \\\n  wget -O - 'https://s3.amazonaws.com/travis-phantomjs/phantomjs-2.0.0-ubuntu-14.04.tar.bz2' | tar xjf - -C ~/ && \\\n  mv ~/phantomjs /usr/local/bin/ && \\\n  npm install -g npm@2.7.5 && \\\n  apt-get autoremove -yqq && \\\n  apt-get clean\nRUN npm install -g bower\n\n# Redis\nRUN apt-get update -qq && apt-get install -y python-software-properties sudo\nRUN apt-get install -y redis-server\nRUN chown -R redis:redis /var/lib/redis\n\n# psycopg2\nRUN apt-get install -y python-psycopg2\nRUN apt-get install -y libpq-dev\n\n# lxml\nRUN apt-get install -y python3-lxml\nRUN apt-get install -y libxml2-dev libxslt-dev\n\n# Pillow\nRUN apt-get install -y libjpeg8 libjpeg62-dev libfreetype6 libfreetype6-dev\n\n# Project\nENV PROJECT_DIR /home/ubuntu/workspace/ward\nADD . ${PROJECT_DIR}\nRUN cd ${PROJECT_DIR} && bower --allow-root install\nRUN cd ${PROJECT_DIR} && pip3 install -r requirements.txt\nRUN mkdir ${PROJECT_DIR}/www/logs\nRUN cd ${PROJECT_DIR}/www && python3 manage.py migrate --noinput\nRUN cd ${PROJECT_DIR}/www && python3 manage.py collectstatic --noinput\n\nRUN ln -s ${PROJECT_DIR}/conf/nginx-app.conf /etc/nginx/sites-enabled/\nRUN ln -s ${PROJECT_DIR}/conf/uwsgi.ini /etc/uwsgi/apps-enabled/\nRUN ln -s ${PROJECT_DIR}/conf/celeryd.conf /etc/default/celeryd\nRUN ln -s ${PROJECT_DIR}/conf/celerybeat.conf /etc/default/celerybeat\nRUN ln -s ${PROJECT_DIR}/code/supervisor-app.conf /etc/supervisor/conf.d/\n\nRUN cp ${PROJECT_DIR}/conf/celeryd /etc/init.d/\nRUN chmod +x /etc/init.d/celeryd\nRUN update-rc.d celeryd defaults\nRUN update-rc.d celeryd enable\nRUN chown root:root /etc/init.d/celeryd\nRUN chmod 755 /etc/init.d/celeryd\n\nRUN cp ${PROJECT_DIR}/conf/celerybeat /etc/init.d/\nRUN chmod +x /etc/init.d/celerybeat\nRUN update-rc.d celeryd defaults\nRUN update-rc.d celeryd enable\nRUN chown root:root /etc/init.d/celerybeat\nRUN chmod 755 /etc/init.d/celerybeat\n\nservice nginx start\nservice uwsgi start\nservice celeryd start\nservice celerybeat start\n\nVOLUME [\"/data\", \\\n        \"/etc/nginx/site-enabled\", \"/var/log/nginx\", \\\n        \"/etc/uwsgi/apps-enabled\", \"/var/log/uwsgi\", \\\n        \"/var/log/celery\", \\\n        \"/var/lib/redis\", \"/etc/redis\"]\n\nEXPOSE 80\nEXPOSE 443\n# EXPOSE 6379\n\n#CMD [\"supervisord\", \"-n\"]\n```\n\n\nI make this dockerfile.\nIt could build yesterday, but it doesn't work today.\n\nThis is a error message.\n    Get:20 http://archive.ubuntu.com trusty/universe amd64 Packages [7589 kB]\n    Fetched 21.4 MB in 1min 3s (336 kB/s)\n    W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/trusty-updates/main/binary-amd64/Packages  Hash Sum mismatch\n\n```\nE: Some index files failed to download. They have been ignored, or old ones used instead.\n```\n\n\nI don't know why it doesn't work.\n    ", "Answer": "\r\nYour docker file cannot be used by us to reproduce to the problem since you have some serious project and project dependency resolution in place\n\nLooking at your error lets assume it happens at \n\n```\nFROM ubuntu:14.04\nRUN apt-get update\n```\n\n\nDo you see this multiple times you tried to build. \n\n\nHash mismatches can occur if the signed package hash does not match the hash value of the package obtained\nYou hit the split second where the package is updated in the repo but the signed hash is still being updated\n\n\nMost likely its 2. Since I ran with 14.04 right now and don't see this. \n\n```\nGet:19 http://archive.ubuntu.com/ubuntu/ trusty/main librtmp0 amd64 2.4+20121230.gitdf6c518-1 [57.5 kB]\nGet:20 http://archive.ubuntu.com/ubuntu/ trusty-updates/main libcurl3-gnutls amd64 7.35.0-1ubuntu2.6 [165 kB]\nGet:21 http://archive.ubuntu.com/ubuntu/ trusty/main libedit2 amd64 3.1-20130712-2 [86.7 kB]\n```\n\n\nIf you can reproduce the error every time you build it. \n\n\nPlease don't post the entire docker file (like i said, it does not help me or anyone else). \nDo some work comment the entire thing and uncomment one instruction at a time.\nWhen you hit the error message edit your post with the block which is not commented.\n\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Evenly distributing byte array into buckets\r\n                \r\nI'm trying to implement a minimal perfect hash function, and one of the primary ways to to this is to be able to take the known universe of keys and split them into decently even buckets of a small size (the smaller the better within reason). The problem is I cant figure out how to distribute these byte arrays. The entire set of keys is available at the outset of the algorithm. Does anyone have any suggestions of what I could do?\n\nThanks!\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Filter large json file with Ruby\r\n                \r\nAs a total beginner of programming, I am trying to filter a JSON file for my master's thesis at university. The file contains approximately 500 hashes of which 115 are the ones I am interested in. \n\nWhat I want to do:\n\n(1) Filter the file and select the hashes I am interested in\n\n(2) For each selected hash, return only some specific keys\n\nThe format of the array with the hashes (\"loans\") included:\n\n```\n{\"header\": {\n   \"total\":546188,\n   \"page\":868,\n   \"date\":\"2013-04-11T10:21:24Z\",\n   \"page_size\":500},\n \"loans\": [{\n   \"id\":427853,\n   \"name\":\"Peter Pan\",\n   ...,\n   \"status\":\"expired\",\n   \"paid_amount\":525,\n   ...,\n   \"activity\":\"Construction Supplies\",\n   \"sector\":\"Construction\",\" },\n    ... ]\n }\n```\n\n\nBeing specific, I would like to have the following:\n\n(1) Filter out the \"loans\" hashes with \"status\":\"expired\"\n\n(2) Return for each such \"expired\" loan certain keys only: \"id\", \"name\", \"activity\", ...\n\n(3) Eventually, export all that into one file that I can analyse in Excel or with some stats software (SPSS or Stata)\n\nWhat I have come up with myself so far is this:\n\n```\nrequire 'rubygems'\nrequire 'json'\n\ntoberead = File.read('loans_868.json')\nanother = JSON.parse(toberead)\n\nread = another.select {|hash| hash['status'] == 'expired'}\n\nputs hash\n```\n\n\nThis is obviously totally incomplete. And I feel totally lost.\nRight now, I don't know where and how to continue. Despite having googled and read through tons of articles on how to filter JSON...\n\nIs there anyone who can help me with this?\n    ", "Answer": "\r\nThe JSON will be parsed as a hash, 'header' is one key, 'loans' is another key.\n\nso after your JSON.parse line, you can do\n\n```\nloans = another['loans']\n```\n\n\nnow loans is an array of hashes, each hash representing one of your loans.\nyou can then do\n\n```\nexpired_loans = loans.select {|loan| loan['status'] == 'expired'}\nputs expired_loans\n```\n\n\nto get at your desired output.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "The effect of otherwise inconsequential calls to readlines() on the speed of open(filepath) in Python 3.7\r\n                \r\nI'm working on a university assignment that consists in building a hash table.\n\nOne of the tasks requires that we measure the time taken to associate a list of keys with some value, and measure the effects of different hash bases and different table capacities on the execution time. Incidentally, collisions are resolved by linear probing.\n\nWe're asked to read the keys from a file and associate them with the value of 1, as in:\n\n```\nwith open(filename, 'r', encoding='utf-8-sig') as data:\n    for line in data.readlines():\n        line = line.strip('\\n')\n        hashtable[line] = 1\n```\n\n\nHowever, given certain combinations of suboptimal hash bases and table capacities, such as 1 and 250,727 respectively, or 3 and 250,727, the excerpt above is intolerably slow. It seems to run indefinitely. For instance, it didn't complete after running for several hours!\n\nCuriously, if I add some expression like ```\nlen(data.readlines())```\n or ```\ntype(data.readlines())```\n, accessing the ```\nfile```\n object before entering the loop, then the program completes in less than one second with the same parameters.\n\n```\nwith open(filename, 'r', encoding='utf-8-sig') as data:\n    len(data.readlines())\n    for line in data.readlines():\n        line = line.strip('\\n')\n        hashtable[line] = 1\n```\n\n\nCan anyone clarify this for me?\n\nThank you!\n    ", "Answer": "\r\nThe length is calculated where based on where ```\nthe file starts(The memory location at which the file starts) - where the file ends(The memory location at which the file ends)```\n.\nIt is an O(1) operation that why it takes less time.\nOn the other hand, insertion in a hashtable is an O(1) operation but depending on the number of entries the total time taken would be O(n). O(1) for each number so total O(n) for n entries. But if you are implementing a linear probing method for collision handling.\nMaybe an element sarts at 1st position and gets hashed at the end of the hashtable. So, here the worst-case complexity would be O(n^2). If you could upload the file on cloud and share the link i would be able to share some more analytics.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Generating a Structure for Aggregation\r\n                \r\nSo here's a question. What I want to do is generate a data structure given a set of input values.\n\nSince this is a multiple language submission, let's consider the input list to be an array of key/value pairs. And therefore an array of Hash, Map, Dictionary or whatever term that floats your boat. I'll keep all the notation here as JSON, hoping that's universal enough to translate / decode.\n\nSo for input, let's say we have this:\n\n```\n[ { \"4\": 10 }, { \"7\": 9 }, { \"90\": 7 }, { \"1\": 8 } ] \n```\n\n\nMaybe a little redundant, but lets stick with that.\n\nSo from that input, I want to get to this structure. I'm giving a whole structure, but the important part is what gets returned for the value under \"weight\":\n\n```\n[\n   { \"$project\": {\n       \"user_id\": 1,\n       \"content\": 1,\n       \"date\": 1,\n       \"weight\": { \"$cond\": [\n           { \"$eq\": [\"$user_id\": 4] },\n           10,\n           { \"$cond\": [ \n               { \"$eq\": [\"$user_id\": 7] },\n               9,\n               { \"$cond\": [\n                   { \"$eq\": [\"$user_id\": 90] },\n                   7,\n                   { \"$cond\": [\n                       { \"$eq\": [\"$user_id\": 1] },\n                       8, \n                       0\n                   ]}\n               ]}\n           ]}\n       ]}\n   }}\n]\n```\n\n\nSo the solution I'm looking for populates the structure content for \"weight\" as shown in the structure by using the input as shown.\n\nYes the values that look like numbers in the structure must be numbers and not strings, so whatever the language implementation, the JSON encoded version must look exactly the same.\n\nAlternately, give me a better approach to get to the same result of assigning the weight values based on the matching ```\nuser_id```\n.\n\nDoes anyone have an approach to this?\n\nWould be happy with any language implementation as I think it is fair to just see how the structure can be created. \n\nI'll try to add myself, but kudos goes to the good implementations.\n\nHappy coding.\n    ", "Answer": "\r\nWhen I had a moment to think about this, I ran back home to perl and worked this out:\n\n```\nuse Modern::Perl;\n\nuse Moose::Autobox;\nuse JSON;\n\nmy $encoder = JSON->new->pretty;\n\nmy $input = [ { 4 => 10 }, { 7 => 9 }, { 90 => 7 }, { 1 => 8 } ];\n\nmy $stack = [];\n\nforeach my $item ( reverse @{$input} ) {\n\n  while ( my ( $key, $value ) = each %{$item} ) {\n    my $rec = {\n      '$cond' => [\n        { '$eq' => [ '$user_id', int($key) ] },\n        $value\n      ]\n    };\n\n    if ( $stack->length == 0 ) {\n      $rec->{'$cond'}->push( 0 );\n    } else {\n      my $last = $stack->pop;\n      $rec->{'$cond'}->push( $last );\n    }\n\n    $stack->push( $rec );\n  }\n\n}\n\nsay $encoder->encode( $stack->[0] );\n```\n\n\nSo the process was blindingly simple.\n\n\nGo through each item in the array and get the key and value for the entry\nCreate a new \"document\" that has in array argument to the \"$cond\" key just two of required three entries. These are the values assigned to test the \"$user_id\" and the returned \"weight\" value.\nTest the length of the outside variable for stack, and if it was empty (first time through) then push the value of ```\n0```\n as seen in the last nested element to the end of the \"$cond\" key in the document.\nIf there was something already there (length > 0) then take that value and push it as the third value in the \"$cond\" key for the document.\nPut that document back as the value of stack and repeat for the next item\n\n\nSo there are a few things in the listing such as reversing the order of the input, which isn't required but produces a natural order in the nested output. Also, my choice for that outside \"stack\" was an array because the test operators seemed simple. But it really is just a singular value that keeps getting re-used, augmented and replaced.\n\nAlso the JSON printing is just there to show the output. All that is really wanted is the resulting value of stack to be merged into the structure.\n\nThen I converted the logic to ruby, as was the language used by the OP from where I got the inspiration for how to generate this nested structure:\n\n```\nrequire 'json'\n\ninput = [ { 4 => 10 }, { 7 => 9 }, { 90 => 7 }, { 1 => 8 } ]\n\nstack = []\n\ninput.reverse_each {|item|\n\n  item.each {|key,value|\n    rec = {\n      '$cond' => [\n        { '$eq' => [ '$user_id', key ] },\n        value\n      ]\n    }\n\n    if ( stack.length == 0 )\n      rec['$cond'].push( 0 )\n    else\n      last = stack.pop\n      rec['$cond'].push( last )\n    end\n\n    stack.push( rec )\n  }\n\n}\n\nputs JSON.pretty_generate(stack[0])\n```\n\n\nAnd then eventually into the final form to generate the pipeline that the OP wanted:\n\n```\nrequire 'json'\n\nuserWeights = [ { 4 => 10 }, { 7 => 9 }, { 90 => 7}, { 1 => 8 } ]\n\nstack = []\n\nuserWeights.reverse_each {|item|\n\n  item.each {|key,value|\n    rec = {\n      '$cond' => [\n        { '$eq' => [ '$user_id', key ] },\n        value\n      ]\n    }\n\n    if ( stack.length == 0 )\n      rec['$cond'].push( 0 )\n    else\n      last = stack.pop\n      rec['$cond'].push( last )\n    end\n\n    stack.push( rec )\n  }\n\n}\n\npipeline = [\n    { '$project' => {\n        'user_id' => 1,\n        'content' => 1,\n        'date' => 1,\n        'weight' => stack[0]\n    }},\n    { '$sort' => { 'weight' => -1, 'date' => -1 } }\n]\n\nputs JSON.pretty_generate( pipeline )\n```\n\n\nSo that was a way to generate a structure to be passed into aggregate in order to apply \"weights\" that are specific to a ```\nuser_id```\n and sort the results in the collection.\n    ", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
{"Question": "Safe Exam Browse Request Hash\r\n                \r\nI'm currently doing a audit of some Safe Exam Browser features as part of my master thesis, and I started looking into the browser request hash that is used to ensure that people use the safe exam browser in the LMS our university has opted for. As an attempt I just created my own SEB config file (on my mac), and I managed to compute the correct browser request hash from the key I got from said config file. If I re-open the same file in the SEB config editor it still presents me with the same hash (so I can get it again later).\n\nHowever, the SEB config file I got from the LMS system we use, when I open that and copy the config key, I am unable to generate the correct browser request hashes (using the same code). Does this has to do with OSX vs Windows somehow? Or is there some other mechanism that I don't know about?\n\n\n\nOriginally asked at the Safe Exam Browser forums.\n    ", "Answer": "", "Knowledge_point": "Universal Hashing", "Tag": "算法分析"}
