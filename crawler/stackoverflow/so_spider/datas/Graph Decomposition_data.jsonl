{"Question": "graph decomposition using min cut function\r\n                \r\nSo I have a graph with vertices that are connected by edges. The min cut  is a function that calculates the minimum number of edges which allows the graph to be separated into two connected subgraphs. So the problem I have is that the min cut function does not give me a balanced cut, i.e. the graph is divided into two: a subgraph and a vertex, and the worst case of my problem is when the vertex appears in the decomposition of the graph.\nAny solution?\nfor the code in python:\n```\nimport networkx as nx\n\nA=nx.Graph()\nA.add_edges_from([(1,2),(1,3),(1,4),(1,5),(2,3),(2,4),(2,5),(3,4),(3,5),(4 ,5)])\n\nA.remove_edges_from(nx.minimum_edge_cut(A))\nnx.draw_networkx(A)\n```\n\nBefore decomposition:\n\nAfter decomposition:\n\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Graph branch decomposition\r\n                \r\n\nHello, \n\nI would like to know about an algorithm to produce a graph decomposition into branches with rank in the following way:\n\n```\nRank | path (or tree branch)\n0      1-2\n1      2-3-4-5-6\n1      2-7\n2      7-8\n2      7-9\n```\n\n\nThe node 1 would be the Root node and the nodes 6, 8 and 9 would be the end nodes.\nthe rank of a branch should be given by the number of bifurcation nodes up to the root node. Let's assume that the graph has no loops (But I'd like to have no such constraint)\n\nI am electrical engineer, and perhaps this is a very standard problem, but so far I have only found the BFS algorithm to get the paths, and all the cut sets stuff. I also don't know if this applies.\n\nI hope that my question is clear enough.\n\nPS: should this question be in stack overflow?\n    ", "Answer": "\r\nFrom your example, I'm making some assumptions:\n\n\nYou want to bifurcate whenever a node's degree is > 2\nYour input graph is acyclic\n\n\nWith an augmented BFS this is possible from the root ```\nr```\n. The following will generate ```\ncomp_groups```\n, which will be a list of components (each of which is a list of its member vertices). The rank of each component will be under the same index in the list ```\nrank```\n.\n\n```\ncomp[1..n] = -1           // init all vertices to belong to no components\ncomp[r] = 0               // r is part of component 0\ncomp_groups = [[r]]       // a list of lists, with the start of component 0\nrank[0] = 0               // component 0 (contains root) has rank 0 \nnext_comp_id = 1\n\nqueue = {r}               // queues for BFS\nnext_queue = {}\n\nwhile !queue.empty()\n  for v in queue\n     for u in neighbors(v)\n        if comp[u] == -1                       // test if u is unvisited\n          if degree(v) > 2\n            comp[u] = next_comp_id             // start a new component\n            next_comp_id += 1\n            rank[comp[u]] = rank[comp[v]] + 1  // new comp's rank is +1\n            comp_groups[comp[u]] += [v]        // add v to the new component\n          else\n            comp[u] = comp[v]                  // use same component\n          comp_group[comp[u]] += [u]           // add u to the component\n          next_queue += {u}                    // add u to next frontier\n  queue = next_queue                     // move on to next frontier\n  next_queue = {}\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Directed graph decomposition\r\n                \r\nI want to decompose a directed acyclic graph into minimum number of components such that in each component the following property holds true-\nFor all pair of vertices (u,v) in a components, there is a path from u to v or from v to u.\n\nIs there any algorithm for this?\n\nI know that when the or is replaced by and in the condition, it is same as finding the number of strongly connected components(which is possible using DFS).\n\n*EDIT: * What happens if the Directed graph contains cycles (i.e. it is not acyclic)?\n    ", "Answer": "\r\nMy idea is to order the graph topologically O(n) using DFS, and then think about for what vertices can this property be false. It can be false for those who are joining from 2 different branches, or who are spliting into 2 different branches. \n\nI would go from any starting vertex(lowest in topological ordering) and follow it's path going into random branches, till you cannot go further and delete this path from graph(first component).This would be repeated till the graph is empty and you have all such components.\n\nIt seems like a greedy algorithm, but consider you find a very short path in the first run(by having a random bad luck) or you find a longest path(good luck). Then you would still have to find that small branch component in another step of algorithm.\n\nComplexity would be O(n*number of components).\n\nWhen there is and condition, you should be considering any oriented graph, as DAG cannot have strongly connected component.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "A perfect matching decomposition Graph in bipartite graph\r\n                \r\nI'm looking for an algorithm that shows perfect matching decomposition Graph in the bipartite graph. I know the theory of this issue but I can't design this algorithm in Python.\n\nDef : A perfect matching decomposition is a decomposition such that each subgraph Hi in the decomposition is a perfect matching (A perfect matching M in graph G is a matching such that every vertex of G is incident with one of the edges of M )\n\nas I'm a beginner in python could you please guide me about the below code ( how can I test it that its a perfect matching decomposition graph or not? ) \nvertices : 20 and degree should be between 4&8\n\n```\nimport networkx as nx\nfrom networkx import bipartite\n\ndef plotGraph(graph):\nimport matplotlib.pyplot as plt\nfig=plt.figure()\nax=fig.add_subplot(111)\n\npos=[(ii[1],ii[0]) for ii in graph.nodes()]\npos_dict=dict(zip(graph.nodes(),pos))\nnx.draw(graph,pos=pos_dict,ax=ax,with_labels=True)\nplt.show(block=False)\nreturn\n\ndef enumMaximumMatching(g):\nall_matches=[]\nreturn all_matches\n\nif __name__=='__main__':\ng=nx.Graph()\nedges=[\n    [(1,0), (0,0)],\n    [(1,0), (0,8)],\n    [(1,0), (0,7)],\n    [(1,0), (0,5)],\n    [(1,0), (0,6)],\n    [(1,1), (0,7)],\n    [(1,1), (0,0)],\n    [(1,1), (0,9)],\n    [(1,1), (0,4)],\n    [(1,1), (0,8)],\n    [(1,1), (0,6)],\n    [(1,2), (0,2)],\n    [(1,2), (0,3)],\n    [(1,2), (0,8)],\n    [(1,2), (0,0)],\n    [(1,3), (0,9)],\n    [(1,3), (0,7)],\n    [(1,3), (0,6)],\n    [(1,3), (0,5)],\n    [(1,3), (0,4)],\n    [(1,3), (0,3)],\n    [(1,4), (0,3)],\n    [(1,4), (0,0)],\n    [(1,4), (0,2)],\n    [(1,4), (0,8)],\n    [(1,5), (0,2)],\n    [(1,5), (0,6)],\n    [(1,5), (0,7)],\n    [(1,5), (0,8)],\n    [(1,5), (0,0)],\n    [(1,6), (0,1)],\n    [(1,6), (0,7)],\n    [(1,6), (0,5)],\n    [(1,6), (0,6)],\n    [(1,7), (0,4)],\n    [(1,7), (0,3)],\n    [(1,7), (0,2)],\n    [(1,7), (0,1)],\n    [(1,8), (0,8)],\n    [(1,8), (0,9)],\n    [(1,8), (0,7)],\n    [(1,8), (0,6)],\n    [(1,9), (0,4)],\n    [(1,9), (0,3)],\n    [(1,9), (0,2)],\n    [(1,9), (0,1)],\n    [(1,9), (0,5)],\n    [(1,6), (0,2)],\n    [(1,6), (0,9)],\n\n]\n\nfor ii in edges:\n    g.add_node(ii[0],bipartite=0)\n    g.add_node(ii[1],bipartite=1)\n\ng.add_edges_from(edges)\nplotGraph(g)\n\nall_matches=enumMaximumMatching(g)\n\nfor mm in all_matches:\n    g_match=nx.Graph()\n    for ii in mm:\n        g_match.add_edge(ii[0],ii[1])\n    plotGraph(g_match)\n```\n\n\nThank you\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "tree-decomposition of a graph\r\n                \r\nI need a start point to implement an algorithm in c to generate a tre-decomposition of a graph in input. What i'm looking for it's an algorithm to do this thing.  i will like to have a pseudocode of the algorithm, i don't care about the programming language and I do not care about complexity\n\nOn the web there is a lot of theory but nothing in practice. I've tried to understand how to do an algorithm that can be implemented in c. But it's to hard\n\ni've tried to use the following information:\n\n\nAlgorithm for generating a tree decomposition\nhttps://math.mit.edu/~apost/courses/18.204-2016/18.204_Gerrod_Voigt_final_paper.pdf\n\n\nand a lot of other info-material. But nothing of this link was useful.\n\ncan anyone help me?\n    ", "Answer": "\r\nSo, here is the algorithm to find a node in the tree.\n\n\nSelect arbitrary node v\nStart a DFS from v, and setup subtree sizes\nRe-position to node v (or start at any arbitrary v that belongs to the tree)\nCheck mathematical condition of centroid for v\nIf condition passed, return current node as centroid\nElse move to adjacent node with ‘greatest’ subtree size, and back to step 4\n\n\nAnd the algorithm for tree decomposition\n\n\nMake the centroid as the root of a new tree (which we will call as the ‘centroid tree’)\nRecursively decompose the trees in the resulting forest\nMake the centroids of these trees as children of the centroid which last split them.\n\n\nAnd here is an example code.\n\nhttps://www.geeksforgeeks.org/centroid-decomposition-of-tree/amp/\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Any implementations of graph st-ordering or ear-decomposition?\r\n                \r\nI'm in the search for an implementation of an ear-decomposition algorithm (http://www.ics.uci.edu/~eppstein/junkyard/euler/ear.html). I examined networkx and didn't find one. Although the algorithm layout is vaguely in my mind, I'd like to see some reference implementation, too.\nI'm aware of Ulrik Brandes publication on a linear time Eager st-ordering algorithm, which results in an ear decomposition as a side product, if I understand correctly (it even includes pseudocode, which I'm trying to base my implementation on).\nSide problem: First step could be an st-ordering of a graph. Are there any implementations for st-ordering algorithms you know?\nThanks for your input. I'd really like to contribute e.g. to networkx by implementing the ear-decomposition algorithm in python.\n    ", "Answer": "\r\nI dug this up, but the credit goes to it's author Minjae Park\n\n```\n# 20106911 Minjae Park\n# Finding an Ear decomposition of 2(-edge)-connected graph\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\ncolorList = [\"orange\", \"blue\", \"red\", \"green\", \"magenta\", \"purple\", \"yellow\", \"black\"]\nglobal count\ncount=0\n\n'''\nInput Graph\n'''\n# Complete Graph\n#G=nx.complete_graph(6)\n\n'''\n# Non 2-connected (but 2-edge-connected) Graph\nG=nx.Graph()\nG.add_edge(0,1)\nG.add_edge(1,2)\nG.add_edge(2,0)\nG.add_edge(2,3)\nG.add_edge(3,4)\nG.add_edge(4,5)\nG.add_edge(5,3)\nG.add_edge(4,2)\n'''\n\n# Petersen Graph\nG=nx.petersen_graph()\n\n'''\nTesting 2-edge-connectivity\n'''\nfor e in G.edges():\n    H=nx.Graph(G)\n    G.remove_edge(*e)\n    if not nx.is_connected(G):\n        raise SystemExit(\"G is not 2-edge-connected. This algorithm is not valid.\")\n    G=H\n\n'''\nTesting 2-connectivity\n'''\nfor v in G.nodes():\n    H=nx.Graph(G)\n    G.remove_node(v)\n    if not nx.is_connected(G):\n        print \"G is not 2-connected. The result is not an open ear decomposition.\"\n    G=H\n\n'''\nAlgorithm for Finding an Ear Decomposition\n'''\ndef makeSpanningTree(G,root):\n    T=nx.Graph()\n    T.add_node(root)\n    T.node[root]['dfsnum']=len(T.nodes())\n    makeSpanningTreeDFS(G,T,root)\n    return T\n\ndef makeSpanningTreeDFS(G,T,current):\n    if not 'child' in T.node[current]:\n        T.node[current]['child']=[]\n    for neighbor in G.neighbors(current):\n        if not neighbor in T.nodes():\n            T.add_node(neighbor)\n            T.add_edge(current,neighbor)\n            T.node[neighbor]['dfsnum']=len(T.nodes())\n            T.node[neighbor]['parent']=current\n            T.node[current]['child'].append(neighbor)\n            makeSpanningTreeDFS(G,T,neighbor)\n\ndef assignNonTreeEdgeLabel(G,T,current):\n    global count\n    subrootdfsnum=T.nodes(data=True)[current][1]['dfsnum']\n    for node,nodeattr in T.nodes(data=True):\n        if nodeattr['dfsnum']>subrootdfsnum:\n            if ((current,node) in G.edges() or (node,current) in G.edges()) and not ((current,node) in T.edges() or (node,current) in T.edges()):\n                G[current][node]['label']=count\n                count+=1\n    for neighbor in T.nodes(data=True)[current][1]['child']:\n        assignNonTreeEdgeLabel(G,T,neighbor)\n\ndef assignTreeEdgeLabel(G,T,current):\n    if not T.nodes(data=True)[current][1]['child']:\n        label=[]\n        for neighbor in G.neighbors(current):\n            if 'label' in G[current][neighbor]:\n                label.append(G[current][neighbor]['label'])\n        if 'parent' in T.node[current]:\n            parent=T.node[current]['parent']\n            G[current][parent]['label']=min(label)\n    else:\n        for neighbor in T.nodes(data=True)[current][1]['child']:\n            if not 'label' in T.node[neighbor]:\n                assignTreeEdgeLabel(G,T,neighbor)\n        if 'parent' in T.node[current]:\n            parent=T.node[current]['parent']\n            label=[]\n            for neighbor in G.neighbors(current):\n                if 'label' in G[current][neighbor]:\n                    label.append(G[current][neighbor]['label'])\n            G[current][parent]['label']=min(label)\n\n\nT=makeSpanningTree(G,0)\nassignNonTreeEdgeLabel(G,T,0)\nassignTreeEdgeLabel(G,T,0)\n\n'''\nOutput\n'''\npos=nx.circular_layout(G)\near_list=[[] for i in range(count+1)]\nfor (x,y) in G.edges():\n    ear=G[x][y]['label']\n    ear_list[ear].append((x,y))\nnx.draw_networkx_nodes(G,pos)\nnx.draw_networkx_labels(G,pos)\nfor i in range(len(ear_list)):\n    nx.draw_networkx_edges(G,pos,edgelist=ear_list[i],edge_color=colorList[i%len(colorList)],alpha=0.5,width=3)\nnx.draw_networkx_edge_labels(G,pos,alpha=0.5)\n\nplt.show()\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Finding a tree decomposition of a graph in Python\r\n                \r\nI need an algorithm that can find a tree decomposition given a graph in Python, the graphs will be small so it doesn't need to be very efficient. I have looked around but cannot find much on this at all. Anyone know of a package that can do this or some sudo code I could use to make my own?\n    ", "Answer": "\r\nHave you looked at networkx? See: https://networkx.github.io/documentation/latest/reference/algorithms/approximation.html\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Obtaining a tree decomposition from an elimination ordering and chordal graph\r\n                \r\nI need a nice tree decomposition of a graph given an elimination ordering and a chordalization of the graph.\n\nMy idea is to obtain all cliques in the graph (which I can do) and build a binary tree starting from a root and make children (i.e., cliques) depending on how many veritices the cliques have in common. I want to do this until all cliques are used and hence, I have a tree. The problem is that the cliques could have more than 2 vertices so I can not recursively run for each vertex as then, the tree might not be binary.\n\nhttp://en.wikipedia.org/wiki/Tree_decomposition\nhttp://en.wikipedia.org/wiki/Chordal_graph\n\nI am doing an implementation in python and currently I have the chordal graph, a list of all cliques and an elimination ordering. Ideas, and/or code are more than welcome! \n    ", "Answer": "\r\nTo construct a non-nice (in general) tree decomposition of a chordal graph: find a perfect elimination ordering, enumerate the maximal cliques (the candidates are a vertex and the neighbors that appear after it in the ordering), use each clique as a decomposition node and connect it to the next clique in the ordering that it intersects. I didn't describe that quite right; see my subsequent answer.\n\nA nice tree decomposition is defined as follows (definition from Daniel Marx). Nice tree decompositions are rooted. Each node is of one of four types.\n\n```\nLeaf (no children): a set {v}\nIntroduce (exactly one child): a set S union {v} with child S (v not in S)\nForget (exactly one child): a set S with child S union {v} (v not in S)\nJoin (exactly two children): a set S with children S and S\n```\n\n\nRoot the non-nice tree decomposition arbitrarily and start a recursive conversion procedure at the root. If the current node has no children, then construct the obvious chain consisting of a leaf node with introduce ancestors. Otherwise, observe that, if some vertex belongs to at least two children, then it belongs to the current node. Recursively convert the children and chain forget ancestors until their sets are subsets of the current node's. The easiest way to proceed in theory is to introduce the missing elements to each child, then join en masse. Since the running time of the next step, however, often depends exponentially on the set size, it might be wise to try some heuristics to join children before their subsets are complete.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How to Change Color of Line graphs in Statsmodel Decomposition Plots\r\n                \r\nThe default color of the seasonal decomposition graphs is a light blue.\n1. How can you change the colors so that each of the lines is a different color?\n2. If each plot can not have a separate color, how would I change all of the colors to say red?\n\nI've tried adding arguments to decomposition.plot(color = 'red')\nand searching the documentation for clues.\n\n```\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n# I want 7 days of 24 hours with 60 minutes each\nperiods = 7 * 24 * 60\ntidx = pd.date_range('2016-07-01', periods=periods, freq='D')\nnp.random.seed([3,1415])\n\n# This will pick a number of normally distributed random numbers\n# where the number is specified by periods\ndata = np.random.randn(periods)\n\nts = pd.Series(data=data, index=tidx, name='TimeSeries')\n\ndecomposition = sm.tsa.seasonal_decompose(ts, model ='additive')\nfig = decomposition.plot()\nplt.show()\n```\n\n\nA decomposition plot in which each graph is a different color.\n\n    ", "Answer": "\r\nThe ```\ndecomposition```\n object in the code you posted uses pandas in the plotting method. I don't see a way of passing colors directly to the ```\nplot```\n method, and it doesn't take **kwargs. \n\nA work around would be to call the pandas plotting code directly on the object:\n\n```\nfig, axes = plt.subplots(4, 1, sharex=True)\n\ndecomposition.observed.plot(ax=axes[0], legend=False, color='r')\naxes[0].set_ylabel('Observed')\ndecomposition.trend.plot(ax=axes[1], legend=False, color='g')\naxes[1].set_ylabel('Trend')\ndecomposition.seasonal.plot(ax=axes[2], legend=False)\naxes[2].set_ylabel('Seasonal')\ndecomposition.resid.plot(ax=axes[3], legend=False, color='k')\naxes[3].set_ylabel('Residual')\n```\n\n\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Time series decomposition and graphing from custom metrics in Azure Logs\r\n                \r\nWhile learning Azure Log processing I started recording simple queue counts as metrics via AppInsight. Currently I process them in a fairly simple way and show them in a same graph.\nThe simple query is like\n```\ncustomMetrics\n| where cloud_RoleName == \"abc\" and name endswith \"_ApproximateMessagesCount\"\n| summarize avg(valueSum) by name, bin(timestamp, 30s)\n| render timechart; \n```\n\nThen I thought that I could try forecastig on these counts and draw the forecast together with the counts. So I went to read the documentation and found an example on time series decomposition as described at https://learn.microsoft.com/en-us/azure/data-explorer/anomaly-detection#time-series-decomposition-model. Perfect!\nSo I modified the query a bit to\n```\nlet min_t = startofday(now());\nlet max_t = endofday(now());\nlet dt = 1h;\nlet horizon=24h;\ncustomMetrics\n| where cloud_RoleName == \"abc\" and name endswith \"_ApproximateMessagesCount\"\n| make-series queueCount=avg(valueSum) on timestamp from min_t to max_t step dt by name\n| extend forecast = series_decompose_forecast(queueCount, toint(horizon/dt))\n| render timechart with(title=\"abc\", ysplit=panels)\n```\n\nAlas! A few problems I don't understand really:\n\nHow to show a graph of actual counts together with a forecast of counts.\nHow could one split a graph on each \"swim line\" with each swim line having the the actual counts over time as well as the forecast? In the example the ```\nysplit=panels```\n seem to achieve this, but maybe it doesn't work like that with multiple graphs?\n\nI may have also misunderstood on how ```\nmake-series```\n and ```\nextend```\n function. I mean that I see a graph from which I can determine the counts and then the graphs go to zero quickly, maybe because the forecast is so. So, effectively the actual ```\nqueueCount```\n is shown on ```\ny axis```\n (while all the graphs are on the same swimlane).\nThe test code to report queue counts is just simple\n```\nstring queueName = $\"{queueName}_ApproximateMessagesCount\";\nint queueLength = <someValue>;\nTelemetryClient.TrackMetric(new MetricTelemetry\n{\n    Name = queueName,\n    Sum = queueLength,\n    Count = 1,\n    Timestamp = DateTime.UtcNow\n});\n```\n\n    ", "Answer": "\r\n\nIf you have both the actual counts and the forecast in the table then 'render timechart shows both. Note that you need to specify to max_t+horizon in make-series.\n\n```\nlet min_t = datetime(2017-01-05);\nlet max_t = datetime(2017-02-03 22:00);\nlet dt = 2h;\nlet horizon=7d;\ndemo_make_series2\n| make-series num=avg(num) on TimeStamp from min_t to max_t+horizon step dt by sid \n| where sid == 'TS1'   //  select a single time series just to get cleaner visualization\n| extend forecast = series_decompose_forecast(num, toint(horizon/dt))\n| render timechart\n```\n\n\n\nysplit=panels works only in KE (Kusto Explorer, the desktop app)\nDon't undrstand your last question re. make-series and extend. Please clarify.\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Is there an R function for combining graph for decomposition of time series graph?\r\n                \r\nI used ```\npar()```\n to combine the graph, but it is not working\n```\nd1=decompose(abuja)\nd2=decompose(ilorin)\nd3=decompose(jos)\nd4=decompose(lafia)\nd5=decompose(lokoja)\nd6=decompose(makurdi)\nd7=decompose(minna)\n\npar(mfrow=c(7,1))\nplot(d1)\nplot(d2)\nplot(d3)\nplot(d4)\nplot(d5)\nplot(d6)\nplot(d7)\n```\n\nAfter doing this, it did not work.\n    ", "Answer": "\r\nWelcome to SO! ```\npar()```\n won't work because ```\nplot.decomposed.ts()```\n (which you implicitly call when calling ```\nplot()```\n) isn't designed to work that way. The most straightforward alternative is to use ```\nautoplot()```\n from the ```\nforecast```\n package to generate decomposition plots and combine them using patchwork.\nHere is an example.\n```\nm1 = decompose(co2)\nm2 = decompose(AirPassengers)\nm3 = decompose(UKgas)\np1 = autoplot(m1)\np2 = autoplot(m2)\np3 = autoplot(m3)\np1 / p2 / p3\n```\n\nThe last line, ```\np1 / p2 / p3```\n, tells R to stack them vertically. If you want to stack them horizontally, use ```\np1 + p2 + p3```\n. If you're being feisty, you can also try ```\n(p1 + p2)/p3```\n to stack the first two horizontally and the last one beneath it.\n\n\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Algorithm for generating a tree decomposition\r\n                \r\nI want to construct a tree decomposition: http://en.wikipedia.org/wiki/Tree_decomposition and I have the chordal graph and a perfect elimination ordering. I am following advice given in a previous thread,\nnamely:\n\n\n  To construct a non-nice (in general) tree decomposition of a chordal graph: find a perfect elimination ordering, enumerate the maximal\n  cliques (the candidates are a vertex and the neighbors that appear\n  after it in the ordering), use each clique as a decomposition node and\n  connect it to the next clique in the ordering that it intersects.\n\n\nThis does not work however and I can not figure out why. Consider the following example:\n\nPerfect elimination ordering: \n\n```\n['4', '3', '5', '7', '6', '2', '0', '1']\n```\n\n\nChordal graph:\n\n\n\nTree decomposition:\n\n\n\nI am using python and my current algorithm is the following:\n\n```\nT=nx.Graph()\n    nodelist=[]\n    for i in eo:\n        vertex=str(i)\n        bag=set()\n        bag.add(vertex)\n        for j in chordal_graph.neighbors(str(i)):\n            bag.add(str(j))\n        T.add_node(frozenset(bag))\n        nodelist.append(frozenset(bag))\n        chordal_graph.remove_node(str(i))\n    for node1 in range(len(nodelist)):\n        found=False\n        for node2 in range(node1+1,len(nodelist)):\n            if found==False and len(nodelist[node1].intersection(nodelist[node2]))>0:\n                T.add_edge(nodelist[node1],nodelist[node2])\n                found=True\n    nx.draw(T)\n    p.show()     \n```\n\n\nwhere ```\neo```\n is a list of the perfect ordering and 'chordal_graph' is a graph object for ```\nnetworkx```\n.\n    ", "Answer": "\r\nSo that was my (bad, as it turns out) advice. Your tree decomposition has some cliques that aren't maximal, i.e., {2, 0, 1}, {0, 1}, and {1}. The list of candidate cliques is\n\n```\n{4, 5, 6} (maximal)\n{3, 2} (maximal)\n{5, 6, 2, 0} (maximal)\n{7, 2, 1} (maximal)\n{6, 2, 0, 1} (maximal)\n{2, 0, 1} (not maximal: subset of {6, 2, 0, 1})\n{0, 1} (not maximal: subset of {6, 2, 0, 1})\n{1} (not maximal: subset of {6, 2, 0, 1})\n```\n\n\nThen the decomposition should look like\n\n```\n                {3, 2}\n                  |\n{4, 5, 6}----{5, 6, 2, 0}\n                  |\n             {7, 2, 1}\n                  |\n             {6, 2, 0, 1},\n```\n\n\nwhich is wrong as well, since the 0 vertices aren't connected. Sorry about that.\n\nWhat I should have done is to set aside the maximality condition for the moment and to connect each clique K to the next candidate seeded with a vertex belonging to K. This ensures that every vertex in K that exists in at least one other subsequent clique also appears in the target of the connection. Then the decomposition looks like this\n\n```\n{4, 5, 6}----{5, 6, 2, 0}\n                  |\n             {6, 2, 0, 1}\n                  |\n   {3, 2}----{2, 0, 1}----{7, 2, 1}\n                  |\n                {0, 1}\n                  |\n                {1}\n```\n\n\nand you can splice out the non-maximal cliques by checking, for each clique in reverse order, whether it's a superset of its parent, and if so, reparenting its parent's children to it.\n\n```\n{4, 5, 6}----{5, 6, 2, 0}\n                  |\n   {3, 2}----{6, 2, 0, 1}----{7, 2, 1}\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "What is the difference between the tree-cut decomposition and tree decomposition?\r\n                \r\nif i want to find the cut-width and tree-width, and compare them. Therefore, in the following example how the tree-cut decomposition and tree decomposition would look like?\ngraph\nas far as i understood from literature, they both correspond to the same decomposition.\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Algorithms for Tree Decomposition\r\n                \r\nI want to understand the optimum algorithm for a tree decomposition of any graph. Is there any good sites where I can look up because I cannot find proper materials to understand the logic behind tree decomposition.\n    ", "Answer": "\r\nThe PACE (Parameterized Algorithms and Computational Experiments Challenge) challenge is a competition for implementing fast algorithms (with a worst-case exponential running time). In 2016 and 2017, one of the challenges was to compute tree decompositions. See here for reports and (inside the reports) the links to the implementations of submitted solutions.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Neo4j native Java: Creating a new database in memory rather than hard disk\r\n                \r\nAs a part of graph decomposition, I split a graph to multiple ones. \nIn order to use them in the next procedures, I want to store splitted graphs in a data structure.\nHow can I store these multiple graphs by a data structure in a memory rather than using disk path for that?\nIf I use newEmbeddedDatabase for that, I should provide a path which is not my requirement.\n    ", "Answer": "\r\nYou have two main options to do this.   \n\nOption 1\n\nYou can use an ```\nImpermanentGraphDatabase```\n as seen in this link where unit testing with neo4j is discussed.   Note that this actually does still write temporary files, you just don't need to know/care where they are stored.\n\nOption 2\n\nThe second option is to use linux to set up something like ramfs.  The idea here is that you're actually creating something which looks like a disk, but it's actually all in memory and so transient/not saved to a hard disk.  This option lets you treat your in-memory graph databases just like any other (just another directory on disk).\n\nNeo4j doesn't have an option to do things purely in memory with no persistence to disk, so either of these options lets you do what you want, without having to really think about the disk persistence bits.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Create a nice tree decomposition in python\r\n                \r\nI know that networkx has a function for creating a tree decomposition:\n```\nimport networkx as nx\nfrom networkx.algorithms.approximation import treewidth_min_degree\n\nG = createGraph() # an arbitrary function returning a networkx graph\nwidth,decomposition = treewidth_min_degree(G)\n```\n\nIs there a function transforming this tree into a nice tree decomposition.\nEdit:\nA \"nice\" tree decomposition is a rooted binary tree with four different types of nodes:\n\nLeaf nodes have no children and bag size\nIntroduce nodes have one child. The child has the same vertices as the parent with one deleted.\nForget nodes have one child. The child has the same vertices as the parent with one added.\nJoin nodes have two children,\nEnd edit.\n\nI know there is a linear algorithm doing this, but I wonder if this is already implemented somewhere.\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How to solve Cholesky decomposition error in Tensorflow caused by low precision datatype tf.float32?\r\n                \r\nIt seems that tensorflow only supports tf.float32 for training neural network, and this causes a Cholesky decomposition issues in my algorithm.\n\nThe following code is a part of my computational graph, where X_latent is a tensor to be passed in, and it returns a positive definte matrix.\n\n```\ndef get_mat_LX(self, X_latent):\n    dim_P = int(X_latent.shape[1])\n    col = tf.reduce_sum(X_latent*X_latent, 1)\n    col = tf.reshape(col, [-1, 1])\n    prod = tf.matmul(X_latent, tf.transpose(X_latent))\n    log_mat_LX = - (0.5/self.rho+0.5/self.sigma)*col + 1/self.sigma*prod - (0.5/self.rho+0.5/self.sigma)*tf.transpose(col) + 2*math.log(self.alpha**0.5)-2*dim_P*math.log((math.pi*self.rho)**0.5)\n    mat_LX = tf.exp(log_mat_LX)\n\n    return mat_LX\n```\n\n\nLater on, I need to do cholesky decomposition for this positive definite matrix: \n\n```\ntensorflow_matrix = self.get_mat_LX(latent_var)\nchol = tf.cholesky(tensorflow_matrix)\n```\n\n\nHowever, because of datatype issue, some small value in the matrix (e.g. 3.825485980697876e-41) will be truncated to 0.0000000e+00 by tensorflow, and thus the \nreturned matrix is no longer positive definte and causes a cholesky decomposition issue:\n\n```\nCholesky decomposition was not successful. The input might not be valid. \n```\n\n\nFor example, for \n\n```\nlatent_var:\narray([[ 1.4992752 ,  0.4027754 , -0.9438937 , ..., -1.7518392 ,\n    -0.59829473, -0.9430773 ],\n   [ 1.4987504 ,  1.3747126 ,  0.2517067 , ..., -0.11252454,\n    -1.0589341 ,  1.9861003 ],\n   [ 3.739284  ,  0.03519648, -0.7698121 , ...,  1.0399017 ,\n     1.4340334 ,  0.5115541 ],\n   ...,\n   [ 1.8933645 , -0.8301757 , -1.9493791 , ...,  2.5583518 ,\n     0.65306365,  1.7139516 ],\n   [ 1.3454692 , -1.4221896 , -0.04142132, ...,  1.6484771 ,\n    -2.1860263 ,  2.0473964 ],\n   [ 1.3071399 , -1.4299753 ,  0.09692653, ..., -3.2884247 ,\n     0.42087832,  1.6871212 ]], dtype=float32)\n\ntensorflow_matrix = self.get_mat_LX(latent_var)\n```\n\n\nIf we compute with numpy,\n\n```\ndef get_numpy_mat_LX(self, latent_var):\n    col = np.matrix(np.sum(latent_var*latent_var,1).reshape((-1,1)))\n    prod = np.matrix(latent_var)*np.matrix(latent_var).transpose()\n    log_mat_LX = - (0.5/self.rho+0.5/self.sigma)*col + 1/self.sigma*prod - (0.5/self.rho+0.5/self.sigma)*col.T + 2*math.log(self.alpha**0.5)-2*latent_var.shape[1]*math.log((math.pi*self.rho)**0.5)\n    mat_LX = np.exp(log_mat_LX)\n\n    return mat_LX\n\nactual_matrix = self.get_numpy_mat_LX(latent_var)\n```\n\n\nWe will get:\n\n```\nactual_matrix:\n  [[1.99842083e-26 6.19800080e-35 3.82548598e-41 ... 1.48187120e-38\n  1.40404417e-37 3.77790018e-32]\n  [6.19800080e-35 4.52321870e-21 2.29967037e-41 ... 2.37359479e-34\n  7.37417026e-35 6.34270132e-35]\n [3.82548598e-41 2.29967037e-41 9.54639589e-30 ... 1.57905177e-41\n  1.39829079e-40 1.38537289e-44]\n ...\n [1.48187120e-38 2.37359479e-34 1.57905177e-41 ... 1.92394478e-27\n  2.51794412e-36 3.13480019e-42]\n [1.40404417e-37 7.37417026e-35 1.39829079e-40 ... 2.51794412e-36\n  2.30329527e-25 3.14763134e-36]\n [3.77790018e-32 6.34270132e-35 1.38537289e-44 ... 3.13480019e-42\n  3.14763134e-36 2.03249691e-24]]\n\ntensorflow_matrix:\n[[1.9983998e-26 6.1980058e-35 0.0000000e+00 ... 1.4818499e-38\n 1.4040427e-37 3.7778746e-32]\n [6.1980058e-35 4.5232227e-21 0.0000000e+00 ... 2.3735830e-34\n  7.3741981e-35 6.3427609e-35]\n [0.0000000e+00 0.0000000e+00 9.5463307e-30 ... 0.0000000e+00\n  0.0000000e+00 0.0000000e+00]\n ...\n [1.4818499e-38 2.3735830e-34 0.0000000e+00 ... 1.9239023e-27\n  2.5179270e-36 0.0000000e+00]\n [1.4040427e-37 7.3741981e-35 0.0000000e+00 ... 2.5179270e-36\n  2.3032796e-25 3.1476519e-36]\n [3.7778746e-32 6.3427609e-35 0.0000000e+00 ... 0.0000000e+00\n  3.1476519e-36 2.0325064e-24]]\n```\n\n\nSome values in the original matrix is truncated to 0.0000000e+00. For example \n\n```\ntensorflow_matrix[0,2]:\n0.0\nactual_matrix[0,2]:\n3.825485980697876e-41\n```\n\n\nI somehow fixed the issue by tf.cast the matrix to tf.float32 and tf.float64 before and after neural network part of the computational graph.\nBut I wondered if there is a way to solve this problem without changing it to tf.float64 before and after.\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "STL decomposition Python - graph is plotted, values are N/A\r\n                \r\nI have a time series with 1hr time interval, which I'm trying to decompose - with seasonality of a week.\n\n```\nTime                    Total_request\n2018-04-09 22:00:00     1019656 \n2018-04-09 23:00:00     961867  \n2018-04-10 00:00:00     881291  \n2018-04-10 01:00:00     892974  \n\nimport pandas as pd\nimport statsmodels as sm\n\nd.reset_index(inplace=True)\nd['env_time'] = pd.to_datetime(d['env_time'])\nd = d.set_index('env_time')\ns=sm.tsa.seasonal_decompose(d.total_request, freq = 24*7)\n```\n\n\nThis gives me a resulting graphs of Seasonal, Trend, Residue - https://i.stack.imgur.com/DVAuR.jpg\n\nBut on trying to extract the residual values using s.resid I get this - \n\n```\nenv_time\n2018-04-09 20:00:00   NaN\n2018-04-09 21:00:00   NaN\n2018-04-09 22:00:00   NaN\n```\n\n\nI get values when I modify it to a lower frequency. What's strange is why I can't derive the values, when it's being plotted. I have found similar questions being asked, none of the answers were relevant to this case. \n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Unity Shader Graph, UV Map decomposition to use as opacity gradient\r\n                \r\nI'm still trying to learn how Unity Shader graph works but I hit a wall.\nI'm getting the UV map, which I know ranges from 0 to 1 in the red channel and 0 to 1 in the green channel.\nI'm extracting the red channel to blend two textures together but red channels looks weird.\n\nAs you can see, if I apply a contrast node, the line separating black and white is WAY to the left. I expected it to be right in the middle. I thought this could be a contrast gotcha but as you can see in the blend node, it shows more of the second texture than the first one.\nIn the preview node, you can see it has way more white values than black values\nWhat I wanted was to blend the two textures equaly from each side and control the blend (opacity contrast) with a number, with \"0\" being totally blended and \"1\" being not blended at all (left side = tex1 and right side = tex2)\n\n    ", "Answer": "\r\nI manage to get it working. The red channel wasn't the culprit, but rather the contrast node.\nI solved the problem using a REMAP node.\nwhen contrast is 0\nRemap\nIn MinMax = 0..1\nOut MinMax = 0..1\nwhen contrast is 1\nRemap\nIn MinMax = 0.4999..0.5\nOut MinMax = 0..1\nSo it generates a black and white texture with the division line right in the middle\n\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "\"NA\" results in Decomposition of Additive Time Series in R\r\n                \r\nI'm trying to understand my \"decomposition of additive time series\" graph.  Here's my code:\n\n```\ndbs_discs <- ts(RC$Disconnects, frequency =12, start=c(2013,1))\ndiscs_dbs <- decompose(dbs_discs)\nplot(discs_dbs)\ndiscs_dbs\n```\n\n\nand my results:\n\n```\n$trend\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug      Sep      Oct      Nov      Dec\n2013       NA       NA       NA       NA       NA       NA 301.8891 302.4746 302.6317 303.1842 304.2663 304.2212\n2014 304.6779 306.3847 309.0182 310.5303 309.9420 309.1160 307.1276 304.2277 302.4454 301.2108 300.1494 299.7908\n2015 299.5936 299.2328 298.4888 297.8479 297.3363 296.2674       NA       NA       NA       NA       NA       NA  \n```\n\n\nAs a result, my trend graph shows nothing plotted until mid 2013.  Is there a reason why it's showing NA?  What does it mean?  Why would there be no values?\n\nThanks!\n    ", "Answer": "\r\nIt seems the ```\ndecompose```\n function uses a 12-month 2-way moving average to determine the trend component of the series. (See ```\n?filter```\n and the code underneath ```\ndecompose```\n). That is, the trend value in July 2013 will be the moving average for the 6 months before and 6 months after (inclusive).\n\nIf you want to perform trend-cycle decomposition but don't want to trim off your end-points, perhaps it's worth looking at the ```\nmFilter```\n package, which implements several filters. Note that in basically all trend-cycle decompositions there are end-point issues (ie. mistaking trend and cycle), so buyer beware. \n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Domain Decomposition with PETSc\r\n                \r\nDoes anyone have any experience on Domain Decomposition using PETSc library? \nI have used PETSc for creating my vectors and matrix within my c++ code. I also used KSP to solve the linear system. \nI used follwoing examples tp learn about DD in PETSc but they were not helpful for me:\n\nhttp://www.mcs.anl.gov/petsc/petsc-3.5/src/snes/examples/tutorials/ex10d/ex10.c.html\nhttp://www.mcs.anl.gov/petsc/petsc-3.2/src/snes/examples/tutorials/ex12.c.html\nhttp://www.mcs.anl.gov/petsc/petsc-current/src/snes/examples/tutorials/ex62.c.html​\n\nI would be grateful if you give me some hints on how to do the graph partitioning and domain decomposition with PETSc.\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How to decide either is it stationary data or not ? what if p value is less than 0.05 but decomposition shows trend and seasonality in graph\r\n                \r\nDecomposition result\n\nDicky-fuller test\n\nWhich should I consider and go forward with result?\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Dulmage-Mendelsohn matrix decomposition in Python\r\n                \r\nMatlab has a function called ```\ndmperm```\n that computes the so-called \nDulmage–Mendelsohn decomposition of a ```\nn x n```\n matrix.\n\nFrom wikipedia, the Dulmage–Mendelsohn is a partition of the vertices of a bipartite graph into subsets, with the property that two adjacent vertices belong to the same subset if and only if they are paired with each other in a perfect matching of the graph. \n\nLooking both on scipy and numpy, I could not find this function, nor some similar version. Is it possible to implement it using basic linear algebra operations?\nAny idea if this is implemented in some Python package?\n    ", "Answer": "\r\n\n  \"Any idea if this is implemented in some Python package?\"\n\n\nWell as MATLAB have a Python API, this is definitely a yes. The package is called ```\nmatlab.engine```\n, and you can see here for installation. Note that you will probably have to install it with sudo rights.\n\nFor example usage let ```\nA```\n be some matrix, then you can find the ```\ndmperm```\n with\n\n```\nimport matlab.engine\neng = matlab.engine.start_matlab()\n#Define A\nB = eng.dmperm(eng.double(A)) #Apply MATLABs dmperm\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "python-igraph: cannot decompose directed graph (neither STRONG nor WEAK)\r\n                \r\nUsing python-igraph 0.7.1 (Python 3.4.8), I want to decompose a directed graph into all unconnected subgraphs. However, ```\ng.decompose(mode=WEAK)```\n fails:\n\n```\nigraph._igraph.InternalError: Error at components.c:469: only 'IGRAPH_WEAK' is implemented, Invalid value\n```\n\n\nAs the error message and google (e.g. click) tells me, ```\nSTRONG```\n decomposition is not implemented and thus expected to fail, however ```\nWEAK```\n decomposition SHOULD work. Well, it doesn't for me!\n\nHere's a minimal example:\n\n```\nfrom igraph import Graph, WEAK\ng = Graph(directed=True)\ng.add_vertices(4)\ng.add_edges([(0, 1), (2, 3)])\ng.decompose(mode=WEAK) # <= FAILS!\n```\n\n\nI've tried ```\nmode=\"weak\"```\n, ```\nmode=\"WEAK\"```\n, ```\nmode=WEAK```\n, and ```\nmode=1```\n, and always get the same error.\n\nIs there maybe a workaround, i.e. a set of some other commands that lead to the same result? (Note that I'm not really familiar with graph theory, so I might miss something obvious in this regard.)\n    ", "Answer": "\r\n\n  Is there maybe a workaround, i.e. a set of some other commands that lead to the same result?\n\n\nIndeed, I've found a workaround that seems to work:\n\n```\nfrom igraph import graph, WEAK\ng = Graph(directed=True)\ng.add_vertices(6)\ng.vs[\"ts\"] = [0, 1, 2, 3, 1, 2] # timesteps\ng.add_edges([(0, 1), (1, 2), (2, 3), (4, 5)])\n\n# sgs = g.decompose(mode=WEAK) # fails!\n#< workaround\ng.to_undirected() # we won't need 'g' anymore afterwards\nsgs = g.decompose(mode=WEAK)\nfor sg in sgs:\n    sg.to_directed(mutual=True)\n    es_del = sg.es.select(lambda e: e.graph.vs[e.source][\"ts\"] > e.graph.vs[e.target][\"ts\"])\n    sg.delete_edges(es_del)\n#>\n```\n\n\nI convert the original directed graph to an unidirected graph, decompose it into subgraphs, convert all subgraphs back to directed graphs with edges in both directions, and remove all edges that point in the wrong direction. Of course if the original graph should be retained, we should make a copy of it first (or convert it back to a directed graph after decomposition).\n\nWarning: Be aware that edge attributes seem to be lost when converting the directed graph to undirected! As far as I can see, they need to be restored manually (at least that's what I'm doing).\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Making sense of the decomposition plot in R\r\n                \r\nI'm trying to understand what does the decomposition function and plot actually indicate. I'm using the following as an example:\n\n```\nlibrary(TTR)\nt <- ts(co2, frequency=12, start=1, deltat=1/12)\ntd <- decompose(t)\nplot(td)\n```\n\n\nI'm not quite sure what the seasonal component and random component actually tells us. \n\nSpecifically, why does the seasonal graph fluctuates between -3 and + 2, for all time. Shouldn't it have similar fluctuations as the observed series? And what does actually the range between -3 and +2 mean? \n\nFurthermore, what does the random component actually say? Are those the residual errors, and seasonal + random = observed???\n\nAny help will be greatly appreciated since i am quite confused at this point. Decomposed graph looks like:\n\n    ", "Answer": "\r\nYou can answer this yourself really, by accessing the components using ```\n$```\n\n\n```\nlibrary(TTR)\nt <- ts(co2, frequency=12, start=1, deltat=1/12)\ntd <- decompose(t)\n```\n\n\nHere's the seasonal component on its own:\n\n```\nplot(td$seasonal)\n```\n\n\n\n\nNow add the trend:\n\n```\nplot(td$seasonal + td$trend)\n```\n\n\n\n\nand finally the random component:\n\n```\nplot(td$seasonal + td$trend + td$random)\n```\n\n\n\n\nand compare this to the original data:\n\n```\nplot(td$x)\n```\n\n\n\n\nSo the time series is decomposed into a seasonal component, which by definition repeats without variation, a trend line around which the seasonal component varies, and a \"random\" component which is akin to residuals.\n\nCreated on 2020-03-03 by the reprex package (v0.3.0)\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Add Data Line to VAR HD Graph Using ggplot\r\n                \r\nI've made a graph using the code on Historical Variance Error Decomposition plot in R and got a good looking graph. And I would like to add the original data line to this graph. First one is the one I got and second one is the one I'm trying to make. I've tried melting it into the dataframe but then it does not give me a line but gives me bars. Any help is appreciated!\nMine\nI want my to look like this\n```\n  ex <- HD[,,1] \n  ex1 <- as.data.frame(ex) # transforming the HD matrix as data frame #\n  ex2 <- ex1[3:216,1:3] # taking our the first 2 rows as they are N/As #\n  colnames(ex2) <- c(\"Inflation\", \"Interest\", \"Unemployment\") # renaming columns #\n  ex2$Period <- 1:nrow(ex2) # creating an id column #\n  col_id <- grep(\"Period\", names(ex2)) # setting the new variable as id #\n  ex3 <- ex2[, c(col_id, (1:ncol(ex2))[-col_id])] # moving id variable to the first column #\n  molten.ex <- melt(ex3, id = \"Period\") # melting the data frame #\n  \n  molten.ex$variable <- factor(molten.ex$variable, levels = c(\"Inflation\",\"Interest\",\"Unemployment\"))\n  \n  ggplot(molten.ex, aes(x = Period, y = value, fill = variable)) + \n    geom_bar(stat = \"identity\", width = 0.6) + \n    guides(fill = guide_legend(reverse = TRUE)) +\n    # Making the R plot look more like excel for comparison... \n    scale_y_continuous(limits = c(-6,8), breaks = seq(-6,8, by = 2)) +\n    scale_fill_manual(name = NULL, \n                      values = c(Inflation = \"#FFc000\",  # yellow\n                                 Interest = \"#9932cc\",  # grey\n                                 Unemployment = \"#5E99CE\")) + # orange\n    theme(rect = element_blank(),\n          panel.grid.major.y = element_line(colour = \"#DADADA\"),\n          legend.position  = \"bottom\",\n          axis.ticks = element_blank(),\n          axis.title = element_blank(),\n          legend.key.size = unit(3, \"mm\"))\n```\n\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Decompose even graph in minimum and maximum cycles\r\n                \r\nIs there an algorithm that can decompose a simple undirected graph (with all vertices having even degrees) into maximum and minimum amount of cycles?\nI know that it can be decomposed into cycles because it has even degrees in every vertex, but is there a way to find what is the minimum and maximum amount of possible cycles in a decomposition?\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "What is the simplest way to plot a decomposition tree in Mathematica?\r\n                \r\nI would like to plot a \"decomposition tree\" in Mathematica.\n\nI have a function ```\nf```\n that takes an object and returns all the components of that object as a list.  For the purpose of this question, let's just decompose Mathematica expressions as follows (my actual ```\nf```\n relies on an external database to decompose different kinds of objects, so I can't easily post it):\n\n```\nf[e_?AtomQ] := {}\nf[e_] := List @@ e\n```\n\n\nI would like to create a tree plot that shows how an object is decomposed as we recursively keep applying ```\nf```\n.  For the particular example ```\nf```\n above, we should get something very similar to the output of ```\nTreeForm```\n, except that a full expression should be displayed (rather than just a head) at each node.  The children of a node are going to be its components as returned by ```\nf```\n.\n\nNote that elements can repeat in a decomposition tree like this, but not elements are repeated in the output of ```\nTreePlot```\n as it works with graphs.  One idea would be to generate a unique \"internal name\" for each node, construct a graph, and use TreePlot, setting it to display the actual form of the nodes rather than their \"internal name\"\n    ", "Answer": "\r\nHow about this?\n\n```\ntf[x_] := f[x] /. {{} :> x, r_ :> x @@ tf /@ r}\n```\n\n\n\n\nIf any of the terms are not inert, this \"simple\" (?) approach will not work.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "ValueError: x must have 2 complete cycles requires 24 observations. x only has 15 observation(s)\r\n                \r\nI am working on some time decomposition graphing as I am looking to forecast sales data for a project. After loading in the data and dropping the columns, I have a dataframe with two columns, one for the date sold, and the other for the amount. When I tried to create a time decomposition graph, here's what I had:\n```\n    import statsmodels.api as sm\n    from pylab import rcParams\n\n    rcParams['figure.figsize'] = 18, 8\n    decomposition = sm.tsa.seasonal_decompose(y, model='additive')\n    fig = decomposition.plot()\n    plt.show()\n```\n\nand the error I got:\n```\n    ValueError: x must have 2 complete cycles requires 24 observations. x only has 15 \n    observation(s)\n```\n\nI believe it is because the data only spans 15 months (x = 15). Can anyone clarify or help me tweak the code in order to be able to plot the time decomposition? Thanks!\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Getting different results while using different(but correct) algorithms to find LU Decomposition in Python. Both Algorithms are working fine in Java\r\n                \r\n```\nMatrix:\n2  -1  -1\n-1   3  -1\n-1  -1  3\n```\n\nI know there are in-built modules to find the LU decomposition, but this is an algorithms assignment so I'm refraining to use them.\nI'm using two algorithms:\n\nDoolittle's Algorithm (This one is giving the correct answer)\nAlgorithm provided in \"Introduction to Algorithms book\" (This one is not. But working fine in Java).\n\nDoolittle's Algorithm Implementation\n```\ndef luDecomposition(mat, n):\n    lower = [[0.0 for _ in range(n)]\n             for _ in range(n)]\n    upper = [[0.0 for _ in range(n)]\n             for _ in range(n)]\n\n    # Decomposing matrix into Upper\n    # and Lower triangular matrix\n    for i in range(n):\n\n        # Upper Triangular\n        for k in range(i, n):\n\n            # Summation of L(i, j) * U(j, k)\n            total = 0.0\n            for j in range(i):\n                total += (lower[i][j] * upper[j][k])\n\n                # Evaluating U(i, k)\n            upper[i][k] = mat[i][k] - total\n\n            # Lower Triangular\n        for k in range(i, n):\n            if i == k:\n                lower[i][i] = 1.0  # Diagonal as 1\n            else:\n\n                # Summation of L(k, j) * U(j, i)\n                total = 0.0\n                for j in range(i):\n                    total += (lower[k][j] * upper[j][i])\n\n                    # Evaluating L(k, i)\n                lower[k][i] = (mat[k][i] - total) / upper[i][i]\n\n    return lower, upper\n```\n\nOutput:\n```\nLower Triangular Matrix:      Upper Triangular Matrix:\n[[ 1.   0.   0. ]                [[ 2.  -1.  -1. ]\n [-0.5  1.   0. ]                 [ 0.   2.5 -1.5]\n [-0.5 -0.6  1. ]]                [ 0.   0.   1.6]]\n```\n\n\nImplementation according to the book.\n```\ndef luDecomposition(mat, n):\n\n    # Initializing the matrices\n    lower = np.array([[0.0]*n for _ in range(n)])\n    for i in range(n):\n        lower[i][i] = 1.0\n\n    upper = np.array([[0.0]*n for _ in range(n)])\n\n    # Decomposing matrix into Upper\n    # and Lower triangular matrix\n    for k in range(n):\n        upper[k][k] = mat[k][k]\n\n        for a in range(k + 1, n):\n            lower[a][k] = mat[a][k] / upper[k][k]\n            upper[k][a] = mat[k][a]\n\n        for x in range(k + 1, n):\n            for y in range(k + 1, n):\n                mat[x][y] -= lower[x][k] * upper[k][y]\n\n    return lower, upper\n```\n\n\n\nOutput:\n```\nLower Triangular Matrix:       Upper Triangular Matrix:\n[[ 1.   0.   0. ]              [[ 2. -1. -1.]\n [-0.5  1.   0. ]               [ 0.  2. -1.]\n [-0.5 -0.5  1. ]]              [ 0.  0.  1.]]\n```\n\nI've checked multiple times and the codes are implemented correctly.\nOnly if you are interested:\nThis is part of an assignment to find the number of spanning trees of a graph by using Kirchoff's Theorem. In step 4 of the theorem, the cofactor is calculated using LU decomposition to reduce calculations. The complete code can be found here\n    ", "Answer": "\r\nI got this solution from reddit. Original answer and follow-up questions here\n\nI was passing the matrix as a numpy array that is why I got the wrong result.\n```\nluDecomposition(np.array([[2,-1,-1],[-1,3,-1],[-1,-1,3]]), 3)\n```\n\ngives the wrong result but\n```\nluDecomposition([[2,-1,-1],[-1,3,-1],[-1,-1,3]], 3)\n```\n\ngives the correct one. This is because numpy takes np.array([[2,-1,-1],[-1,3,-1],[-1,-1,3]]) to be a matrix of integers and the -= operator will not change that datatype:\nTo deal with this, you could do\n```\nmat = np.asarray(mat, dtype=float)  # or complex or whatever\n```\n\nhowever altering a matrix that was passed as an argument like this is generally bad practice (unexpected side effects are bad, in general), so it's probably best to make a copy with something like:\n```\nmat = np.array(mat, dtype=float)\nmat = np.asarray(mat, dtype=float).copy()\n```\n\nI'm not actually sure which is better, but I would hazard a guess that the second is faster if mat is already an array of an appropriate type (but it makes a copy twice otherwise).\n\nThe reason Doolittle's algorithm gave the correct answer because it wasn't altering the matrix and the other algorithm did.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "problem to determine the chromatic polynomial of a graph\r\n                \r\nfor a homework graph theory, I'm asked to determine the chromatic polynomial of the following graph\n\n\n\nFor the Descomposition Theorem of Chromatic Polynomials. if G=(V,E), is a connected graph and e belong E \n\n```\nP (G, λ) = P (Ge, λ) -P(Ge', λ)\n```\n\n\nwhere Ge denotes de subgraph obtained by deleting de edge e from G  (Ge= G-e) and Ge' is the subgraph obtained by identifying the vertices {a,b} = e \n\nWhen calculating chromatic Polynomials, i shall place brackets about a graph to indicate its chromatic polynomial. removes an edge any of the original graph to calculate the chromatic polynomial by the method of decomposition.\n\n\n\n```\n P (G, λ) = P (Ge, λ)-P (Ge', λ) = λ (λ-1)^4 - [λ(λ-1)*(λ^2 - 3λ + 3)]\n```\n\n\nBut the response from the answer key and the teacher is:\n\n```\nP (G, λ) = λ (λ-1)(λ-2)(λ^2-2λ-2)\n```\n\n\nI have operated on the polynomial but I can not reach the solution that I ask .. what am I doing wrong?\n    ", "Answer": "\r\nmath.stackexchange.com told me as a way to solve my problem. Here's the solution:\n\nhttps://math.stackexchange.com/questions/33946/problem-to-determine-the-chromatic-polynomial-of-a-graph\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Can I fix overwriting text in a plotted model?\r\n                \r\nI'm making a plot of the Variance decomposition in my VAR model\nMy Plot looks a bit odd. The text is overwriting in the graph. How do i fix this?\n```\nP1FEVD <- fevd(P1modelvar,n.ahead=10)\nplot(P1FEVD, main=\"VD 1986-1999\")\n```\n\n\n    ", "Answer": "\r\nUnder the hood, this all relies on ```\nbarplot```\n. Hence, you can try a couple of things:\nOverlapping text: Use ```\noma```\n\nThe overlapping text is actually because the two plots are too close together. You can fix this by providing a larger margin (i.e. a larger white space around the plot). Here, we can fix the problem by adding a wide margin below each plot:\n```\nplot(P1FEVD, oma = c(3, 0, 0, 0))\n```\n\nLegend fix #1: Use ```\nbeside```\n\nThis will place the legend next to the plot. However, it will also extend your x-axis.\n```\nplot(P1FEVD, \n     main = \"VD 1986-1999\", \n     beside = TRUE)\n```\n\nLegend fix #2: Use ```\nargs.legend```\n\nWe can provide further arguments towards the legend with ```\nargs.legend```\n. Importantly, we can define the ```\nx```\n and ```\ny```\n position in your graph. This may take a few tries to get the ```\nx```\n position right, but:\n```\nplot(P1FEVD, \n     main = \"VD 1986-1999\",\n     args.legend = list(x = nrow(P1FEVD[[1]]) + 3.5, \n                        y = 1))\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "All non-redundant paths in a directed graph (DiGraph) from NetworkX . Python\r\n                \r\nI'm looking for a way to navigate directed graphs to find the building blocks of the graph. ```\nNetworkX```\n has ```\nall_simple_paths```\n that finds all paths between ```\nsource```\n and ```\ntarget```\n nodes.  What I would like to do is similar, but to find not exactly the same.  An unbiased decomposition of a network into minimal units \n\nThis directed network ```\nH```\n below, is built on the following paths:\n\n```\n#Non-redundant paths\n['A', 'B', 'C', 'E', 'F']\n['A', 'B', 'D', 'E', 'F']\n['G','C','E','F']\n```\n\n\nWhat is the most efficient way in Python to find these paths? Would navigating through ```\nnx.adjacency_matrix(H).toarray()```\n be the way to do it? \n\n\n\n```\n#!/usr/bin/python\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nH = nx.DiGraph()\n\nnodes = list(\"ABCDEFG\")\nedges = [(\"A\",\"B\"),(\"B\",\"C\"),(\"B\",\"D\"),(\"C\",\"E\"),(\"D\",\"E\"),(\"E\",\"F\"),(\"G\",\"C\")]\nH.add_nodes_from(nodes)\nH.add_edges_from(edges)\ncoords = {\"A\":(-2,0),\"B\":(-1,0),\"C\":(0,1),\"D\":(0,-1),\"E\":(1,0),\"F\":(2,0),\"G\":(0,2)}\nnx.draw(H,pos=coords,with_labels=True)\n\npaths = nx.all_simple_paths(H, source=\"A\", target=\"F\") \n\nfor path in paths: print(path)    \n#['A', 'B', 'C', 'E', 'F']\n#['A', 'B', 'D', 'E', 'F']\n```\n\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "read daily data with weekly seasonal in R\r\n                \r\nI have a daily data with a weekly seasonal component ranging from 2017-03-01 to 2017-05-29. I want to do a seasonal decomposition in R. My code was as follows.\n\n```\nser = ts(series[,2], frequency=7, start=c(2017,1,1 ))\nplot(decompose(ser))\n```\n\n\nI got a graph as follows.\n\n\nBut the X axis is wrong in the graph. How can I correct it..?\n    ", "Answer": "\r\nit isn't correct because you have not correctly expressed the arguments ```\nfrequency```\n.\n\nReading the help of the function ```\nts()```\n you can see that:\n\nfrequency the number of observations per unit of time.\n\nSo you can try use this code:\n\n```\nser = ts(series[,2], frequency=365, start=c(2017,1))\nplot(decompose(ser))\n```\n\n\nBecause being daily data, every year you have 365 observations.\nVerify that it is the correct solution\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "x-axis range in Seasonal Decomposition of Time Series (STL)\r\n                \r\nI am performing a Seasonal and Trend Decomposition by Loess on a yearly time series, where I extract a long term trend and tri-annual quasi-periodic series from the raw data. The problem is that it doesn't seem to exist an option of ```\nts```\n with periodicity on the order of years. Nevertheless, I was able to overcome this limitation by running the analyses with a random frequency for the TS and the final results were satisfactory:\n\n```\nloessyearly <- ts(yrobs, start = 1993, frequency = 2) #create a TS with random frequency of 2\nstlyearly <- stl(loessyearly, s.window = 3, t.window = 9) #s.window = 3 to smooth under a quasi-periodicity of three years\n```\n\n\nObviously, when plotting the graphs the X-axis follows the frequency I choose at random (i.e. 2 years):\n\n\n\nHowever, I would like the x-axis to depict one year per each observation (i.e. frequency = 1).  Is that possible?\n\nI know an option would be trying this decomposition with ```\nstlplus()```\n, but I would like to try it via ```\nstl()```\n first, since I found it even more difficult o deal with the graphical options of the ```\nstlplus()```\n function.\nThank you! \n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Graph in logarithmic scale for ordinates for the Jacobi method\r\n                \r\nI have to program the Jacobi, Gauss Seidel and SOR methods to resolve Ax=b. We use a decomposition A=M-N.\n\nFor the Jacobi method, for example, we use M=diag(A) and N=M-A.\n\nI programmed a function\n\n```\njacobi(A,b,Imax,err,x0)\n```\n\n\nwith the matrix A, the column vector b, a maximum number of iterations Imax, a tolerance err, for the Jacobi method. I used a \"stop test\"  where  is  the \"residual\" at the step k.\n\nHere is my code :\n\n```\nimport numpy as np\nimport scipy.linalg as la\n\ndef jacobi(A,b,Imax,eps,x0):\n    M=np.diag(np.diag(A))\n    N=M-A\n    r=np.dot(A,x0)-b\n    x=x0\n    i=0\n    err=1+eps\n    res=[]\n    while ((i<Imax) and ((la.norm(r))>=eps)):\n        x=np.dot(np.dot((la.inv(M)),N),x)+np.dot((la.inv(M)),b)\n        r=np.dot(A,x)-b\n        err = la.norm(r,2)\n        res.append(err)\n        i=i+1\n    return (x,i,res)\n```\n\n\nand the test :\n\n```\nA=np.array([[2,0,0],[4,5.4,0],[7,8,9]])\nx0=np.array([[1],[1],[1]])   \nb=np.array([[20],[8],[7]])\nprint(jacobi(A,b,1000,10**(-3),x0))\n```\n\n\nNow, I have to represent on a graph (in logarithmic scale for ordinates) the values  in function of n.\n\nI just would like to know how to represent a graph in logarithmic scal ? I'm beginner in Python and I don't know how to represent a graph... I tried to be clear, sorry for my bad English...\n    ", "Answer": "\r\nhere is a log plot for some dummy values\n\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nj = np.power(10,range(10))    # <--- here use your computed values instead\n\nplt.plot(j)\nplt.yscale('log')\nplt.show()\n```\n\n\n\n\nIf you want log-log set the ```\nxscale```\n as well.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "What does my code finding biconnected components of a graph not operate well?\r\n                \r\nI would like to determine the biconnected components of a graph. In my code I need to use the vector of edges laying on a given node (myEdges). Could you help me why I get edges with biCompID = -1 after the decomposition? My code is the following:\n\n```\n#include \"stdafx.h\"\n#include \"edge.h\"\n#include \"node.h\"\n\nnode::node(int nb)\n   : nb(nb), biLow(-1), biDisc(-1), biParent(NULL)\n   {}\n\nedge::edge(int nb, node* firstNode, node* secondNode)\n         : nb(nb), firstNode(firstNode), secondNode(secondNode), biCompID(-1)\n      {}\nnode* edge::getOtherNode(node* nd)\n{\n   if(firstNode == nd)\n   {\n      return secondNode;\n   }\n   else\n   {\n      return firstNode;\n   }\n}\n\nvoid BCCD(vector<node*> nodes, vector<edge*> edges);\n\nint main()\n{\n   vector<edge*> edges;\n   vector<node*> nodes;\n   vector<pair<int,int>> list;\n\n   ifstream data;\n   data.open(\"bicon.txt\",ios::in);   \n//\n// Construct the graph from the input\n//\n   int a, b;\n   while(!data.eof())\n   {\n      data >> a >> b;\n      list.push_back(make_pair(a,b));\n   }\n   int nNodes = 0;\n   for (int i = 0; i < (int)list.size(); i++)\n   {\n       if (nNodes < list[i].first) nNodes = list[i].first;\n       if (nNodes < list[i].second) nNodes = list[i].second;\n   }\n   for (int i = 0; i < nNodes; i++) nodes.push_back(new node(i));\n   for (int i = 0; i < (int)list.size(); i++) \n       edges.push_back(new edge(i, nodes[list[i].first - 1], nodes[list[i].second - 1]));\n//\n// Save the edges of each node \n//\n   edge* actEdge;\n   for (int i = 0; i < (int)edges.size(); i++)\n   {\n      actEdge = edges[i];\n      actEdge->firstNode->myEdges.push_back(actEdge);\n      if(actEdge->firstNode != actEdge->secondNode) \n               actEdge->secondNode->myEdges.push_back(actEdge);\n   }\n//\n// Decompose the graph into biconnected components\n//\n   BCCD(nodes, edges);\n   system(\"pause\");\n   return 0;\n}\nvoid biDFS(node* actNode, vector<edge*> &stackEdges, vector<edge*> &resultEdges, int &biCompID)\n{\n   static int count = 0;\n   actNode->biDisc = actNode->biLow = ++count;\n   int nChildren = 0;\n   node* otherNode;\n   edge *neighborEdge, *stackEdge, *resultEdge;\n   bool cond1, cond2, cond3, cond4;\n   for (int i = 0; i < (int)actNode->myEdges.size(); i++)\n   {\n      neighborEdge = actNode->myEdges[i];\n      otherNode = neighborEdge->getOtherNode(actNode);\n      if (otherNode->biDisc == -1)\n      {\n         nChildren++;\n         otherNode->biParent = actNode;\n         stackEdges.push_back(neighborEdge);\n         biDFS(otherNode, stackEdges, resultEdges, biCompID);\n         actNode->biLow = min(actNode->biLow, otherNode->biLow);\n         if (actNode->biParent == NULL && nChildren > 1 || actNode->biParent != NULL && otherNode->biLow >= actNode->biDisc)\n         {\n            biCompID++;\n            for(;;)\n            {\n               stackEdge = stackEdges.back();\n               stackEdge->biCompID = biCompID;\n               stackEdges.pop_back();\n               if (stackEdge == neighborEdge) break;\n               resultEdges.push_back(stackEdge);\n            }\n         }\n      }\n      else if (otherNode != actNode->biParent)\n      {\n         actNode->biLow = min(actNode->biLow, otherNode->biDisc);\n         cond1 = cond2 = cond3 = cond4 = false;\n         for (int j = 0; j < (int)stackEdges.size() && !cond1 && !cond2; j++)\n         {\n             stackEdge = stackEdges[j];\n             if (stackEdge == neighborEdge) cond1 = true;\n             if (stackEdge->firstNode == otherNode && stackEdge->secondNode == actNode) cond2 = true;\n         }\n         for (int j = 0; j < (int)resultEdges.size() && !cond3 && !cond4; j++)\n         {\n             resultEdge = resultEdges[j];\n             if (resultEdge == neighborEdge) cond3 = true;\n             if (resultEdge->firstNode == otherNode && resultEdge->secondNode == actNode) cond4 = true;\n         }\n         if (!cond1 && !cond2 && !cond3 && !cond4) \n         {\n             stackEdges.push_back(neighborEdge);\n         }\n      }\n   }\n}\nvoid BCCD(vector<node*> nodes, vector<edge*> edges)\n{\n   int biCompID = -1;\n   vector<edge*> stackEdges;\n   vector<edge*> resultEdges;\n   node* actNode;\n   edge* stackEdge;\n   unsigned stackSize;\n   for (int i = 0; i < (int)nodes.size(); i++)\n   {\n      actNode = nodes[i];\n      if (actNode->biDisc  == -1) biDFS(actNode, stackEdges, resultEdges, biCompID);\n      stackSize = stackEdges.size();\n      if (stackSize != 0)\n      {\n         biCompID++;\n         while (stackSize > 0)\n         {\n            stackEdge = stackEdges.back();\n            stackEdge->biCompID = biCompID;\n            stackEdges.pop_back();\n            resultEdges.push_back(stackEdge);\n            stackSize--;\n         }\n      }\n   }\n   for (int i = 0; i < (int)edges.size(); i++) cout << edges[i]->biCompID << endl;\n}\n```\n\n\nThe header files are:\n\nstdafx.h\n\n```\n#pragma once\n#include <stdio.h>\n#include <cstring>\n#include <iostream>\n#include <vector>\n#include <fstream>\n#include <cmath>\n#include <iomanip>\n#include <sstream>\n#include <algorithm>\nusing namespace std;\n```\n\n\nnode.h\n\n```\nclass edge;\n\nclass node\n{\npublic:\n      node(int nb);\n      int nb;\n      vector<edge*> myEdges;\n      int biLow, biDisc;\n      node* biParent;\n};\n```\n\n\nedge.h\n\nclass node;\n\n```\nclass edge\n{\npublic:\n      edge(int nb, node* firstNode, node* secondNode);\n      int nb;\n      node *firstNode, *secondNode;\n      int biCompID;\n      node* getOtherNode(node* nd);\n};\n```\n\n\nA \"good\" input (```\nbicon.txt```\n) is:\n\n```\n1 2\n2 3\n3 4\n4 5\n2 4\n3 5\n2 6\n6 7\n1 7\n6 8\n6 9\n8 9\n9 10\n11 12\n```\n\n\nMy wrong input can be found on https://drive.google.com/file/d/0BywmWaXkwDsNbHltVGd3TFBtNWM/view?usp=sharing.\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How to turn a linear string into a trie?\r\n                \r\nI am using the make me a hanzi open-source chinese character dataset. As part of this dataset there are strings which provide the decomposition of chinese characters into their individual units (called radicals). I want to turn the strings describing the decomposition of characters into tries (so that I can use ```\nnetworkx```\n to render the decompositions).\n\nFor example for this database entry:\n\n```\n{\"character\":\"⺳\",\"definition\":\"net, network\",\"pinyin\":[],\"decomposition\":\"⿱冖八\",\"radical\":\"⺳\",\"matches\":[[0],[0],[1],[1]]}```\n\n\nThe decomposition for this character would be. \n\n```\n- Node(1, char='⿱')\n  - Node(2, char='冖') # an edge connects '⿱' to '冖'\n  - Node(3, char='八') # an edge connects '⿱' to '八' \n```\n\n\nSo far, I have come up with a script to turn the string decompositions into dictionaries (but not graphs).\n\n```\ndecomposition_types = {\n    'top-bottom': '⿱',\n    'left-right': '⿰',\n    'diagonal-corners': '⿻',\n    'over-under': '⿺',\n    'under-over': '⿹',\n    'over-under-reversed': '⿸',\n    'top-bottom-middle': '⿳',\n    'left-right-middle': '⿲',\n    'inside-outside': '⿴',\n    'outside-inside': '⿵',\n    'outside-inside2': '⿷',\n    'inside-outside2': '⿶'\n    # 'unknown': '？'\n}\n\ndecomposition_types_reversed = dict(((value, key) for key, value in decomposition_types.items()))\n\nfile = []\n\nif not os.path.isfile('data/dictionary.json'):\n\n    with open('data/dictionary.txt') as d:\n        for line in d:\n            file.append(json.loads(line))\n\n    for i, item in enumerate(file):\n        item['id'] = i + 1\n\n    json.dump(file, open('data/dictionary.json', 'w+'))\n\nelse:\n    file = json.load(open('data/dictionary.json'))\n\n\ndef is_parsed(blocks):\n    for block in blocks:\n        if not block['is_unit']:\n            return False\n    return True\n\n\ndef search(character, dictionary=file):\n    for hanzi in dictionary:\n        if hanzi['character'] == character:\n            return hanzi\n    return False\n\n\ndef parse(decomp):\n    if len(decomp) == 1:\n        return {\"spacing\": '？'}\n    blocks = []\n    n_loops = 0\n    for item in decomp:\n        blocks.append({\"char\": item, \"is_spacing\": item in decomposition_types_reversed, \"is_unit\": False})\n    while not is_parsed(blocks):\n        for i, item in enumerate(blocks):\n            if \"is_spacing\" in item:\n                if item['is_spacing']:\n                    next_items = decomposition_types_reversed[item['char']].count('-') + 1\n                    can_match = True\n                    for x in blocks[i + 1:i + 1 + next_items]:\n                        try:\n                            if x['char'] in decomposition_types_reversed:\n                                can_match = False\n                        except KeyError:\n                            pass\n                    if can_match:\n                        blocks[i] = {\"spacing\": item['char'],\n                                     \"chars\": [l['char'] if 'char' in l else l for l in\n                                               blocks[i + 1:i + 1 + next_items]],\n                                     \"is_unit\": True}\n                        del blocks[i + 1:i + 1 + next_items]\n        n_loops += 1\n        if n_loops > 10:\n            print(decomp)\n            sys.exit()\n    return blocks\n\n```\n\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How to fit intercept into particular value in ggplot linear model graph?\r\n                \r\nI would like to generate a decomposition vs time graph using ggplot in R as following code :\n\n```\nggplot(data=deco, aes(x = week, y = mass, group=interaction(treatment,habitat),\n                      colour=habitat, linetype=treatment)) +   \n geom_point() +  \n theme_classic() + \n geom_smooth(method='lm',formula=y~x)\n```\n\n\nBecause the initial litter wight was 5 gram, so I expected that all the lines would start from 5 gram in the 0 week. However, I realized that all the lines have different starting points. I tried to find any solution to swift them all but all methods did not work well to me. Would be great if any of you could solve this.\n\n \n    ", "Answer": "\r\nAs Gregor indicated, this isn't trivial as group sizes are unknown and ```\ngeom_smooth```\n is a bit limited. There are two options, either fit your models outside ```\nggplot```\n. Or suppress the intercept, shift the y, and relabel your axis.\n\n```\nggplot(mtcars, aes(wt, drat, col = factor(am))) + \n  geom_point() + \n  geom_smooth(method = 'lm') +\n  xlim(0, 6)\n\n\nIntercept <- 5\nggplot(mtcars, aes(wt, drat - Intercept, col = factor(am))) + \n  geom_point() + \n  geom_smooth(method = 'lm', formula = y ~ x + 0) +\n  scale_y_continuous(labels = function(x) x + Intercept) +\n  xlim(0, 6)\n```\n\n\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How can I cluster a graph in Python?\r\n                \r\nLet G be a graph. So G is a set of nodes and set of links. I need to find a fast way to partition the graph. The graph I am now working has only 120*160 nodes, but I might soon be working on an equivalent problem, in another context (not medicine, but website development), with millions of nodes.\nSo, what I did was to store all the links into a graph matrix:\n```\nM=numpy.mat(numpy.zeros((len(data.keys()),len(data.keys()))))\n```\n\nNow M holds a 1 in position s,t, if node s is connected to node t. I make sure M is symmetrical M[s,t]=M[t,s] and each node links to itself M[s,s]=1.\nIf I remember well if I multiply M with M, the results is a matrix that represents the graph that connects vertexes that are reached on through two steps.\nSo I keep on multplying M with itself, until the number of zeros in the matrix do not decrease any longer. Now I have the list of the connected components.\nAnd now I need to cluster this matrix.\nUp to now I am pretty satisfied with the algorithm. I think it is easy, elegant, and reasonably fast. I am having trouble with this part.\nEssentially I need to split this graph into its connected components.\nI can go through all the nodes, and see what are they connected to.\nBut what about sorting the matrix reordering the lines. But I don't know if it is possible to do it.\nWhat follows is the code so far:\n```\ndef findzeros(M):\n    nZeros=0\n    for t in M.flat:\n        if not t:\n            nZeros+=1\n    return nZeros\n\nM=numpy.mat(numpy.zeros((len(data.keys()),len(data.keys()))))    \nfor s in data.keys():\n    MatrixCells[s,s]=1\n    for t in data.keys():\n        if t<s:\n            if (scipy.corrcoef(data[t],data[s])[0,1])>threashold:\n                M[s,t]=1\n                M[t,s]=1\n\nnZeros=findzeros(M)\nM2=M*M\nnZeros2=findzeros(M2)\n\nwhile (nZeros-nZeros2):\n    nZeros=nZeros2\n    M=M2\n    M2=M*M\n    nZeros2=findzeros(M2)\n```\n\n\nEdit:\nIt has been suggested that I use SVD decomposition. Here is a simple example of the problem on a 5x5 graph. We shall use this since with the 19200x19200 square matrix is not that easy to see the clusters.\n```\nimport numpy\nimport scipy\n\nM=numpy.mat(numpy.zeros((5,5)))\n\nM[1,3]=1\nM[3,1]=1\nM[1,1]=1\nM[2,2]=1\nM[3,3]=1\nM[4,4]=1\nM[0,0]=1\n\nprint M\n\nu,s,vh = numpy.linalg.linalg.svd(M)\nprint u\nprint s\nprint vh\n```\n\nEssentially there are 4 clusters here: (0),(1,3),(2),(4)\nBut I still don't see how the svn can help in this context.\n    ", "Answer": "\r\nWhy not use a real graph library, like Python-Graph? It has a function to determine connected components (though no example is provided). I'd imagine a dedicated library is going to be faster than whatever ad-hoc graph code you've cooked up.\n\nEDIT: NetworkX seems like it might be a better choice than python-graph; its documentation (here for the connected components function) certainly is.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Access private struct within class c++\r\n                \r\nTrying to implement an adjacency matrix graph and practice OOP. I've been stuck on implementing the inserNode(string ) method.\n\nMy troubles are with accessing the private data fields. What am I completely missing?\n\nSome of the errors:\n\nGraph.cpp:30:26: error: unknown type name 'node'\n             graph[id] = new node;\n                             ^\n\nGraph.cpp:35:10: error: use of undeclared identifier 'numnodes'\n             numnodes++;\n\nGraph.cpp:34:19: error: expected ';' at end of declaration\n         graph[id]->nodename = name;\n\nGraph.cpp:34:15: error: decomposition declaration '[id]' requires an initializer\n\ngraph.h\n\n```\n#include <iostream>\nusing namespace std;\n\nclass Graph {\n\n    public:\n        Graph();\n        int insertNode(string name);\n\n    private:\n        static const int vertices = 20;\n        int nodeCount;\n\n        struct node {\n            int nodeid; // node position in graph[]\n            string nodename; // username\n        };\n\n        // pointers to the graph nodes \n        node *graph[vertices];\n\n        // adjacency matrix for graph. True if edge is going from node i to j.\n        bool edges[vertices][vertices];\n};\n\n#endif\n```\n\n\ngraph.cpp\n\n```\n#include \"Graph.h\"\n\nGraph::Graph() {\n    for (int i = 0; i < vertices; i++) {\n        graph[i] = 0;\n        for (int j = 0; j < vertices; j++ )\n            edges[i][j] = 0;\n    }\n}\n\n      /* create node and insert pointer in first  available graph position. Returns id value, -1 if unsuccessful. */\nint insertNode(string name) {\n    int id = 0;\n    while (id < vertices) {\n        if (graph[id] == NULL) {\n            graph[id] = new node;\n                if (!graph[id]) \n                    return -1;\n                 graph[id]->nodeid = id;\n                 graph[id]->nodename = name;\n                 numnodes++;\n                 return id;\n        }\n        id++;\n   }\n   return -1;\n}\n```\n\n    ", "Answer": "\r\nThe ```\ninsertNode```\n you've defined is not the same way you declared in ```\nGraph```\n. You've just made a free function called ```\ninsertNode```\n, which isn't a member of ```\nGraph```\n and therefore can't access ```\nGraph```\n. You need to define it like so:\n\n```\nint Graph::insertNode(string name)\n{\n}\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "understanding of cumsum dfunction\r\n                \r\nlet us suppose that we have  SVD decomposition of some matrices\n\n```\n[U E V]=svd(X);\n```\n\n\nand i want to  sketch graph of  cumulative sum of singular value ,so i have done  like this\n\n```\nsigmas=diag(E);\n %figure; plot(log10(sigmas)); title('Singular Values (Log10 Scale)');\nfigure; plot(cumsum(sigmas) / sum(sigmas)); title('Cumulative Percent of Total Sigmas');\n```\n\n\nand get following  graph\n\n\n\ni wound to understand this chart,as we see till  approximately 4,line is not linear,but after the 4 it became linear,does it means that first four  singular value have most impact on the  chart?and others  effect  is just  a bit small?thanks in advance\n    ", "Answer": "\r\nWithout knowing more about the data, I cannot explain why the singular values appear the way they do here. However, generally in mathematics, larger singular values imply more \"importance\" to that data.\n\nI'm not sure why we are looking at the normalized cumulative sum; however, from these results we can infer that there are distinct \"groups\" of singular values with identical (or nearly identical) values, and that the earlier groups have larger singular values.\n\nAgain, without seeing the data, this seems to imply that there are artificial \"groupings\" within the eigenvectors of the matrix. And, because smaller smaller values give less weight to those eigenvectors, your first singular values represent the more \"important\" eigenvalues.\n\nIt is up to you for your data and application to decide if the effect of later singular values is \"small\" or not.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Fix layout of plot(fevd()) function\r\n                \r\nI am trying to plot FEVD (forecast error variance decomposition) for my VAR analysis. As you can see on the image, the legend screws up the graph and information. as this is an automatically created legend, I don’t know how to reposition it. I do not know much yet about plotting in R.\n\nThe only code i use to get this is :\n\n```\nlibrary(vars)\nvar <- VAR(varTable2 , p=4 , type = \"both\")\nplot(fevd(var, n.ahead = 10 ))\n```\n\n\nThanks in advance\n\n\n    ", "Answer": "\r\nLegends do not resize well in R. You have to set your plotting window first and then chart your data. \nHere's how to do it in Windows. ```\nwin.graph```\n opens a blank plotting window of the specified width. In Unix/Linux, you should look at ```\nX11()```\n and in Mac, at ```\nquartz()```\n. You might also consider shorter variable names. \n\n```\nlibrary(vars)\ndata(Canada)\ncolnames(Canada) <-c(\"Long column name1\",\"Long column name2\",\"Long column name3\",\"Long column name4\")\nvar <- VAR(Canada , p=4 , type = \"both\")\nwin.graph(width=13,height=8)\nplot(fevd(var, n.ahead = 10 ))\n```\n\n\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Matrix-Algebra Design Decomposition\r\n                \r\nI am looking at refactoring some very complex code which is a subsystem of a project I have at work. Part of my examination of this code is that it is incredibly complex, and contains a lot of inputs, intermediate values and outputs depending on some core business logic.\n\nI want to redesign this code to be easier to maintain as well as executing a hell of a lot faster, so to start off with I have been trying to look at each of the parameters and their dependencies on each other. This has lead to quite a large and tangled graph and I would like a mechanism for simplifying this graph.\n\nA while back I came across a technique in a book about SOA design called \"Matrix Design Decomposition\" which uses a matrix of outputs and the dependencies they have on the inputs, applies some form of matrix algebra and can generate Business Process diagrams for those dependencies.\n\nI know there is a web tool available at http://www.designdecomposition.com/ however it is limited in the number of input/output dependencies you can have. I have tried looking around for the algorithmic source for this tool (so I could attempt to implement it myself without the size limitation), however I have had no luck. \n\nDoes anybody know a similar technique that I could use? Currently I am even considering taking the dependency matrix and applying some Genetic Algorithms to see if evolution can come up with a simpler workflow...\n\nCheers,\n\nAidos\n\nEDIT: \n\nI will explain the motivation:\n\nThe original code was written for a system which computed all of the values (about 60) every time the user performed an operation (adding, removing or modifying certain properties of a item). This code was written over ten years ago and is definitely showing signs of age - others have added more complex calculations into the system and now we are getting completely unreasonable performance (up to 2 minutes before control is returned to the user). It has been decided to detach the calculations from the user actions and provide a button to \"recalculate\" the values. \n\nMy problem arises because there are so many calculations that are going on and they are based on the assumption that all of the required data will be available for their computation - now when I try to re-implement the calculations I keep encountering problems because I haven't got the result for a different calculation that this calculation relies on. \n\nThis is where I want to use the matrix-decomposition approach. The MD approach allows me to specify all of the inputs and outputs and gives me the \"simplest\" workflow that I can use for generating all of the outputs. \n\nI can then use this \"workflow\" to know the precedence of the calculations I need to perform to get the same result without generating any exceptions. It also shows me which parts of the calculation system I can parallelise and where the fork and join points will be (I won't worry about that part just yet). At the moment all I have is an insanely large matrix with lots of dependencies showing in it, with no idea where to start.\n\nI will elaborate from my comment a little more:\n\nI don't want to use the solution from the EA process in the actual program. I want to take the dependency matrix and decompose it into modules that I will then code manually - this is purely a design aid - I am just interested in what the inputs/outputs for these modules will be. Basically a representation of the complex interdependencies between these calculations, as well as some idea of precedence.\n\nSay I have A requires B and C. D requires A and E. F requires B, A and E, I want to effectively partition the problem space from a complex set of dependencies into a \"workflow\" that I can examine to get a better understanding. Once I have this understanding I can come up with a better design / implementation that is still human readable, so for the example I know I need to calculate A, then C, then D, then F.\n\n--\n\nI know this seems kind of strange, if you take a look at the website I linked to before the matrix based decomposition there should give you some understanding of what I am thinking of...\n    ", "Answer": "\r\nkquinn, If it's the piece of code I think he's referring to (I used to work there), it's already a black box solution that no human can understand as is.  He's not looking to make it more complicated, less in fact.  What he's trying to achieve is a whole heap of interlinked calculations.\n\nWhat currently happens, is that whenever anything changes, it's an avalanche of events which cause a whole bunch of calculations to fire off, which in turn causes a whole bunch more events which continues on until finally it reaches a state of equilibrium.\n\nWhat I assume he wants to do is find the dependencies for those outlying calculations and work in from there so they can be rewritten and find a way for the calculations from happening for the sake of it, rather than because they need to.\n\nI can't offer much advice in regards to simplifying the graph, as unfortunately it's not something I have much experience in.  That said, I would start looking for those outlying calculations which have no dependencies, and just traverse the graph from there.  Start building up a new framework that includes the core business logic of each calculation in the simplest possible way, and refactor the crap out of it along the way.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Understanding an Einsum usage for graph convolution\r\n                \r\nI am reading the code for the spatial-temporal graph convolution operation here:\nhttps://github.com/yysijie/st-gcn/blob/master/net/utils/tgcn.py and I'm having some difficulty understanding what is happening with an einsum operation. In particular\nFor ```\nx```\n a tensor of shape ```\n(N, kernel_size, kc//kernel_size, t, v)```\n, where\nkernel_size is typically 3, and lets say ```\nkc=64*kernel_size```\n, ```\nt```\n is the number of frames, say 64, and ```\nv```\n the number of vertices, say 25. ```\nN```\n is the batch size.\nNow for a tensor ```\nA```\n of shape (3, 25, 25) where each dimension is a filtering op on the graph vertices, an einsum is computed as:\n```\nx = torch.einsum('nkctv,kvw->nctw', (x, A))\n```\n\nI'm not sure how to interpret this expression. What I think it's saying is that for each batch element, for each channel ```\nc_i```\n out of 64, sum each of the three matrices obtained by matrix multiplication of the (64, 25) feature map at that channel with the value of ```\nA[i]```\n. Do I have this correct? The expression is a bit of a mouthful, and notation wise there seems to be a bit of a strange usage of ```\nkc```\n as one variable name, but then decomposition of ```\nk```\n as kernel size and ```\nc```\n as the number of channels (192//3 = 64) in the expression for the einsum. Any insights appreciated.\n    ", "Answer": "\r\nHelps when you look closely at the notation:\n\n```\nnkctv```\n for left side\n```\nkvw```\n on the right side\n```\nnctw```\n being the result\n\nMissing from the result are:\n\n```\nk```\n\n```\nv```\n\n\nThese elements are summed together into a single value and squeezed, leaving us the resulting shape.\nSomething along the lines of (expanded shapes (added 1s) are broadcasted and sum per element):\n\nleft: ```\n(n, k, c, t, v, 1)```\n\nright: ```\n(1, k, 1, 1,  v, w)```\n\n\nNow it goes (l, r for left and right):\n\n```\ntorch.mul(l, r)```\n\n```\ntorch.sum(l, r, dim=(1, 4))```\n\nsqueeze any singular dimensions\n\nIt is pretty hard to get, hence Einstein's summation helps in terms of thinking about resulting shapes “mixed” with each other, at least for me.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Error while trying to decompose a time series: non-numeric argument\r\n                \r\nI am new to R and am working on an assignment where I import some JSON data to (1) create a time series graph and (2) decompose the time-series. It's the decompose function where I'm struggling. Here is what works...\n```\n# Import JSON & convert to data.frame\naor <- fromJSON.....\naor <- as.data.frame......\n\n# Combine the year and month into a date format\naor$date <- as.yearmon(paste(aor$year, aor$month), \"%Y %m\")\n\n# Ensure data is float not chr\naor$mwh <- as.numeric(aor$mwh)\n\n# Prep the data.frame for time-series analysis by converting to xts\naor <- xts(x = aor, order.by = aor$date)\n\n# Successfully output a time-series graph.\ndygraph(aor)\n```\n\nHere is a sample of aor up to this point...\n```\n> aor\n         mwh         date      \nJan 2001 \"  1.42000\" \"Jan 2001\"\nFeb 2001 \"  1.28400\" \"Feb 2001\"\nMar 2001 \"  1.25800\" \"Mar 2001\"\nApr 2001 \"  1.53600\" \"Apr 2001\"\nMay 2001 \"  1.47100\" \"May 2001\"\nJun 2001 \"  1.91800\" \"Jun 2001\"\nJul 2001 \"  2.37800\" \"Jul 2001\"\n\n> dput(head(aor, 10))\n \nstructure(c(\"  1.42000\", \"  1.28400\", \"  1.25800\", \"  1.53600\", \n\"  1.47100\", \"  1.91800\", \"  2.37800\", \"  2.47000\", \"  1.65100\", \n\"  1.58100\", \"Jan 2001\", \"Feb 2001\", \"Mar 2001\", \"Apr 2001\", \n\"May 2001\", \"Jun 2001\", \"Jul 2001\", \"Aug 2001\", \"Sep 2001\", \"Oct 2001\"\n), .Dim = c(10L, 2L), .Dimnames = list(NULL, c(\"mwh\", \"date\")))\n```\n\nThe code I thought would produce the decomposition graphic...\n```\nts <- as.ts(aor)\n\n> ts\n             mwh     date\nJan  1   1.42000 Jan 2001\nFeb  1   1.28400 Feb 2001\nMar  1   1.25800 Mar 2001\nApr  1   1.53600 Apr 2001\nMay  1   1.47100 May 2001\nJun  1   1.91800 Jun 2001\nJul  1   2.37800 Jul 2001\n\n\nd <- decompose(ts)\nplot(d)\n```\n\nI get this error when trying to decompose ts...\n\nError in `-.default`(x, trend) : non-numeric argument to binary operator\n\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Is there a friendly name for this data structure?\r\n                \r\nWhile working on a feature selector for a machine learning algorithm in Python, I generated a data structure with the following code: \n\n```\n# Perform set partitioning on the results\ngroups = []\nfor t in results:\n    (jthName,kthName) = t\n    jthGroup = -1\n    kthGroup = -1\n\n    # Just a simple list of hashes with online merging\n    for idx,group in enumerate(groups):\n        if jthName in group:\n            jthGroup = idx\n        if kthName in group:\n            kthGroup = idx\n    if jthGroup == kthGroup:\n        if jthGroup == -1: # Implicit: \"and kthGroup == -1\"\n            groups.append(set((jthName,kthName)))\n    elif jthGroup != kthGroup:\n        if kthGroup == -1:\n            # Merge kthName into jthGroup\n            groups[jthGroup].add(kthName)\n        elif jthGroup == -1:\n            # Merge jthName into kthGroup (redundant if naturally-ordered)\n            groups[kthGroup].add(jthName)\n        else:\n            # Merge jthGroup and kthGroup, since we have a connecting pair\n            merged = set()\n            merged.update(groups[jthGroup])\n            merged.update(groups[kthGroup])\n            groups.remove(groups[jthGroup])\n            groups.remove(groups[kthGroup])\n            groups.append(merged)\n```\n\n\nWhere my input, ```\nresults```\n, is a list of tuple{2} and ```\ngroups```\n is a list of set. Note that my code isn't necessarily efficient here; it's provided only for illustrative purposes.\n\nMy data structure, ```\ngroups```\n, has the following properties:\n\n\nFor each ```\n(jthName,kthName)```\n:\n\n\nIf neither element of ```\n(jthName,kthName)```\n is found in any contained set, create ```\nset((jthName,kthName))```\n within our list of sets.\nIf exactly one of ```\n(jthName,kthName)```\n is found in one contained set, coalesce the unfound element into that set.\nIf each element of ```\n(jthName,kthName)```\n is found in a different set, coalesce the two referenced sets into a single set.\n\nLoop invariant: ```\njthName```\n and ```\nkthName```\n cannot be contained in more than one set.\n\n\n\nMy justification for this data structure is to create a flat decomposition of an unknown set of connected nodegraphs, where each unique element name is a node and each unique pair is an edge. My rationale is that my graphs are incomplete, and I require this view to select only the known members of each graph to feed into an algorithm that will regressively determine graph connectivity and directionality of the edges (that is, the complete set of DAGs expressed by the data). But, I digress.\n\nIs there a friendly name for the data structure represented by variable ```\ngroups```\n? If so or if not, is there a more time- or space-efficient means of performing this decomposition?\n    ", "Answer": "\r\nI think what you're looking for is something called a Disjoint-set data structure.\n\nIt's often used when doing Kruskal's because it allows you to do n lookups in amortized nlog*n (actually less than that) time if you implement the disjoint-set data structure with path compression.\n\nIt's pretty reasonable to implement and I think the wiki page pseudo code lends itself nicely to python. If you need more assistance, this SO question might help.\n\nIf you used the disjoint-set data structure, your code would look like this:\n\n```\nfor t in results:\n   (jName, kName) = t\n\n   union(jName, kName)\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Visualization of k-cores using networkx\r\n                \r\nI would like to plot the k-cores of a graph in a nice visualization which can illustrate the different k-core values (for instance, https://www.researchgate.net/figure/Illustration-of-the-k-core-decomposition-Here-k-max-3_fig1_326621799).\nA sample of dataset is the following:\n```\nNode Target Label\nA      F      1\nA      B      1\nA      N      1\nB      A      0\nB      F      0 \nC      F      1\nA      V      1\nD      S      0\nD      E      0\nF      A      1\nF      B      1\nF      G      1\nG      E      0\nE      W      0\n```\n\nTo identify the nodes with at least degree k I am using the following piece of code:\n```\nG = nx.from_pandas_edgelist(df, 'Node', 'Target')\n\nkcore=nx.k_core(G)\n\nplt.subplot(121)\nplt.title('Full')\nnx.draw(G,with_labels=True)\n\n\nplt.subplot(122)\nplt.title('Main')\nnx.draw(kcore,with_labels=True)\n\nplt.show()\n```\n\nI'm trying to figure it out, for example using a loop across k values or assigning different colors depending on the value of k...\nAny help or tips would be welcome.\n    ", "Answer": "\r\nAre you looking for something like this:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom collections import defaultdict\n\n# create dataframe\n# df = ...\n\n# build graph\nG = nx.from_pandas_edgelist(df, 'Node', 'Target')\n\n# build a dictionary of k-level with the list of nodes\nkcores = defaultdict(list)\nfor n, k in nx.core_number(G).items():\n    kcores[k].append(n)\n\n# compute position of each node with shell layout\npos = nx.layout.shell_layout(G, list(kcores.values()))\ncolors = {1: 'red', 2: 'green'}  # need to be improved, for demo\n\n# draw nodes, edges and labels\nfor kcore, nodes in kcores.items():\n    nx.draw_networkx_nodes(G, pos, nodelist=nodes, node_color=colors[kcore])\nnx.draw_networkx_edges(G, pos, width=0.2)\nnx.draw_networkx_labels(G, pos)\nplt.show()\n```\n\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "MPI Domain Decomposition and Block Tridiagonal Matrix\r\n                \r\nI need some advice regarding how to build a block tri-diagonal matrix having decomposed a 2D domain with MPI.\nLet me explain, I need to solve a heat diffusion like equation in 2D, and to do so I decomposed the domain in blocks with MPI using the virtual Cartesian topology.\nThe decomposed domain is reported\nhere, where the numbers in the cells represent the cell coordinate in global indices.\nWhen I discretize my equation with finite differences for example, I obtain a block tridiagonal matrix like this, where again the numbers in the matrix are the cell coordinate in the same global indices as before.\nAs solver I use MUMPS, which requires to input the spars matrix with three vectors for each process containing:\n\nGlobal Row indices\nGlobal Column indices\nCorresponding element entry (the problem coefficient)\n\nIn serial it is trivial on how to do this, and in MPI with 1D domain (which leads to a tridiagonal matrix) it is easy as well.\nThe problem is that I am not able to find a clear sequence of indices to be assigned with do loops on each process, since the position in the computational domain (first image) after the discretization leads to non contiguous elements in the matrix (second image).\nI know that graph reordering is performed by libraries such as Parmetis, but from what I understood it does not preserve the original domain decomposition pattern since it reorders at matrix level.\nI think this should be trivial to do since it is a very common problem, but I cannot see a clear way to do it.\n    ", "Answer": "\r\nYour partitioned domain is based on the sequential ordering. Translating that to local ordering is kinda tricky. It would be easier if you first numbered all points on proc 0, then all points on proc 1, et cetera. That gives you a matrix with a slightly different structure, but the same solution and largely the same properties.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "R time series forecasting trend line is wrong because of the indexing\r\n                \r\nI'm trying to figure out the answer to a question about forecasting.\nI'm using the tsibble package and following a textbook written by Hyndman and Athanasopoulos. It's a great textbook but also above my weight. https://otexts.com/fpp3/\nHere is the first 100 rows of my data as a tsibble:\n```\nstructure(list(X = 1:100, Time = structure(c(1546351200, 1546354800, \n1546358400, 1546362000, 1546365600, 1546369200, 1546372800, 1546376400, \n1546380000, 1546383600, 1546387200, 1546390800, 1546437600, 1546441200, \n1546444800, 1546448400, 1546452000, 1546455600, 1546459200, 1546462800, \n1546466400, 1546470000, 1546473600, 1546477200, 1546524000, 1546527600, \n1546531200, 1546534800, 1546538400, 1546542000, 1546545600, 1546549200, \n1546552800, 1546556400, 1546560000, 1546563600, 1546610400, 1546614000, \n1546617600, 1546621200, 1546624800, 1546628400, 1546632000, 1546635600, \n1546639200, 1546642800, 1546646400, 1546650000, 1546696800, 1546700400, \n1546704000, 1546707600, 1546711200, 1546714800, 1546718400, 1546722000, \n1546725600, 1546729200, 1546732800, 1546736400, 1546783200, 1546786800, \n1546790400, 1546794000, 1546797600, 1546801200, 1546804800, 1546808400, \n1546812000, 1546815600, 1546819200, 1546822800, 1546869600, 1546873200, \n1546876800, 1546880400, 1546884000, 1546887600, 1546891200, 1546894800, \n1546898400, 1546902000, 1546905600, 1546909200, 1546956000, 1546959600, \n1546963200, 1546966800, 1546970400, 1546974000, 1546977600, 1546981200, \n1546984800, 1546988400, 1546992000, 1546995600, 1547042400, 1547046000, \n1547049600, 1547053200), tzone = \"\", class = c(\"POSIXct\", \"POSIXt\"\n)), Orders = c(390.9300738, 424.5024938, 459.9507418, 493.879574, \n521.1915476, 543.6420076, 564.1188556, 583.1138192, 599.9870792, \n608.2502946, 589.0774506, 552.9864864, 460.0146478, 513.6096, \n565.54751, 614.3622836, 649.610842, 673.080916, 694.1457822, \n714.687121, 730.0065136, 727.6420116, 704.9715348, 669.8276592, \n596.9598262, 627.6943506, 663.224885, 689.6623702, 705.7821348, \n705.2804398, 702.4425002, 686.257045, 673.0440842, 631.4105592, \n590.2226836, 557.170647, 505.5489378, 514.1362306, 518.0591858, \n519.7312244, 515.538957, 517.0255898, 516.4563428, 519.1586616, \n518.8452174, 494.1823666, 468.0562396, 444.6603772, 465.6368096, \n475.6484144, 481.8889642, 489.3110196, 492.9861102, 495.1878822, \n496.2013992, 500.4736856, 502.9525222, 490.2884824, 465.9459928, \n446.4332428, 468.827488, 475.2297188, 480.3550016, 486.8966308, \n488.641556, 492.2285006, 493.485411, 501.271116, 501.7387056, \n485.8849556, 462.3912654, 444.3381798, 423.7514376, 442.9296904, \n456.1299334, 459.6065968, 466.132121, 468.2358706, 476.4634124, \n481.580409, 484.5936224, 477.3972256, 458.546062, 439.0321916, \n397.7730592, 418.1761574, 426.6949568, 435.0296632, 438.8624322, \n437.5872586, 441.9915442, 445.0284556, 443.4291354, 445.9624284, \n430.1143198, 420.7732792, 485.8293664, 494.2056144, 502.3287016, \n509.0143842)), class = c(\"tbl_ts\", \"tbl_df\", \"tbl\", \"data.frame\"\n), row.names = c(NA, -100L), key = structure(list(.rows = structure(list(\n    1:100), ptype = integer(0), class = c(\"vctrs_list_of\", \"vctrs_vctr\", \n\"list\"))), class = c(\"tbl_df\", \"tbl\", \"data.frame\"), row.names = c(NA, \n-1L)), index = structure(\"Time\", ordered = TRUE), index2 = \"Time\", interval = structure(list(\n    year = 0, quarter = 0, month = 0, week = 0, day = 0, hour = 1, \n    minute = 0, second = 0, millisecond = 0, microsecond = 0, \n    nanosecond = 0, unit = 0), .regular = TRUE, class = c(\"interval\", \n\"vctrs_rcrd\", \"vctrs_vctr\")))\n```\n\nI create a graph.\n\nMy next step is to understand the trend, seasonality and cycle in the data so I create a decomposition.\n```\ndcmp <- sales %>%\n  model(stl = STL(Orders))\n```\n\nThe code above compiles but the code below doesn't work.\n```\ndcmp <- components(dcmp)\n\nError in `transmute()`:\n! Problem while computing `cmp = map(.fit, components)`.\nCaused by error in `UseMethod()`:\n! no applicable method for 'components' applied to an object of class \"null_mdl\"\nBacktrace:\n  1. generics::components(dcmp)\n  8. fabletools:::map(.fit, components)\n  9. base::lapply(.x, .f, ...)\n 11. fabletools:::components.mdl_ts(X[[i]], ...)\n 12. generics::components(object$fit, ...)\n```\n\nI google the error message and discover eventually that I'm supposed to fill in the missing gaps.\nSo I fill in the missing gaps and change the NA values that appear to 0.\n```\nsales <- sales %>%\n  fill_gaps()\n\nsales$Orders[is.na(sales$Orders)] <- 0\n```\n\nI'm then able to do my decomposition and graph it.\n```\ndcmp <- sales %>%\n  model(stl = STL(Orders))\n\ndcmp <- components(dcmp)\n\ndcmp %>%\n  as_tsibble() %>%\n  autoplot(Orders, colour=\"gray\") +\n  geom_line(aes(y=trend), colour = \"#D55E00\") +\n  labs(\n    y = \"Orders\",\n    title = \"Orders\") + \n  labs(caption = \"fake data\")\n```\n\n\nBut now I'm totally stuck because this isn't what I wanted. I want to find the trend in the sales but this orange trend line really undershoots it because it's averaging with all the zeros.\nHow does someone who is skilled in time series forecasting take data that has hours from 9am-8pm and zero values in the interval, and turn that into a time series dataframe where you can work with these time series decomposition components. I don't just want to approximate what STL is doing with a moving average because I want to the STL picture.\nHere is a screenshot from the textbook that is closer to what I was expecting as the output. You see the line is closer to the real data and not pulled down with zeros.\n\nI'm hoping for answers using this set of packages for time series forecasting in R because there are things in this textbook I want to use also.\n    ", "Answer": "\r\na ```\ntsibble```\n can't handle missing data very well for forecasting if the missing data is structural, like no weekends for stock data or data based on business hours. It works with prophet, but not for ```\nSNAIVE```\n or other functions.\nWhat you can do is create an index (or use a variable as an index). In the example below I use your variable X as the index. Normally I would create one via ```\nmutate```\n. Keep any groupings you need in mind when creating an index.\nForecasting is done based on this index. So ```\nforecast(model, h = 12)```\n will forecast 12 index values into the future. You then need to translate that back into your Time column.\nSee code below to get you started.\n```\nlibrary(fpp3)\n\nsales <- df1\n\n# If not there use mutate to create an index, now I just use X as the index\nfc <- sales %>% \n  # mutate(idx = row_number()) %>% \n  tsibble(index = X)\n\n\ndcmp <- fc %>%\n  model(stl = STL(Orders))\n\ndcmp %>%\n  components() %>% \n  as_tsibble() %>%\n  autoplot(Orders, colour=\"gray\") +\n  geom_line(aes(y=trend), colour = \"#D55E00\") +\n  labs(\n    y = \"Orders\",\n    title = \"Orders\") + \n  labs(caption = \"fake data\")\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Decompose undirected graph to minimum paths and cycles\r\n                \r\nI would like to decompose undirected graph into a minimum number of paths and cycles that are edge-disjoint. \n\nMy idea is to first take longest paths, but it is not polynomial.\n\nDo you know any polynomial algorithm?\n    ", "Answer": "\r\ncould be fun is to use max flow / minimum cut - cut the graph in half using the least amount of cuts - do this a few times recursively until you get tractable sized subsets to run your longest path algorithm on.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Problem with pattern matching in ocaml\r\n                \r\nI wrote the function used to decompose a Boolean function, the problem is that the compilation I get this : \"Warning 5: this function application is partial, maybe some arguments are missing.\"\nHow can I solve this problem? I've set wrong the patter matching or  I can not run this operation with pattern matching\n\nThe code is the following:\n\n```\n         let rec decomposition state_init state prec formula =        \n            match formula with        \n            And form -> (fun () -> \n                    let f1 = List.hd form in\n                    let f2 = And(List.tl form )in                      \n\n                    let new_state = Forms (state_init,f1) in\n\n                    decomposition state_init new_state state f1;            \n\n                    decomposition state_init new_state state f2;\n\n                    Hashtbl.add graph new_state ((\"\",false,state :: []) , []) ;\n\n                    let x = Hashtbl.find graph state in\n                    let succ = state :: snd x in\n                    let (desc,last,ptrs) = fst x in\n\n                    Hashtbl.replace graph state ( (\"And-node\",last,ptrs) , succ))   \n```\n\n    ", "Answer": "\r\n```\ndecomposition state_init new_state state f1```\n has type ```\nunit -> unit```\n (because you're returning ```\nfun () -> ...```\n). So if you just call it like that, it won't do anything.\n\nYou either have to call it as ```\ndecomposition state_init new_state state f1 ()```\n, or remove the ```\nfun () ->```\n bit, so the unit argument isn't necessary.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Subband decomposition using Daubechies filter\r\n                \r\nI have the following two 8-tap filters:\n\n```\nh0 ['-0.010597', '0.032883', '0.030841', '-0.187035', '-0.027984', '0.630881', '0.714847', '0.230378']\nh1 ['-0.230378', '0.714847', '-0.630881', '-0.027984', '0.187035', '0.030841', '-0.032883', '-0.010597']\n```\n\n\nHere they are on a graph:\n\n\n\nI'm using it to obtain the approximation (lower subband of an image).  This is ```\na(m,n)```\n in the following diagram:  \n\n\n\nI got the coefficients and diagram from the book Digital Image Processing, 3rd Edition, so I trust that they are correct.  The star symbol denotes one dimensional convolution  (either over rows or over columns).  The down arrow denotes downsampling in one dimension (either over rows, or columns).\n\nMy problem is that the filter coefficients for ```\nh0```\n and ```\nh1```\n sum to greater than 1 (approximately 1.4 or sqrt(2) to be exact).  Naturally, if I convolve any image with the filter, the image will get brighter.  Indeed, here's what I get (expected result on right):\n\n \n\nCan somebody suggest what the problem is here?  Why should it work if the convolution filter coefficients sum to greater than 1?\n\nI have the source code, but it's quite long so I'm hoping to avoid posting it here.  If it's absolutely necessary, I'll put it up later.  \n\nEDIT\n\nWhat I'm doing is:\n\n\nDecompose into subbands\nFilter one of the subbands\nRecompose subbands into original image\n\n\nNote that the point isn't just to have a displayable subband-decomposed image -- I have to be able to perfectly reconstruct the original image from the subbands as well.  So if I scale the filtered image in order to compensate for my decomposition filter making the image brighter, this is what I will have to do:\n\n\nDecompose into subbands\nApply intensity scaling to approximation subband\nFilter one of the subbands\nApply inverse intensity scaling to approximation subband\nRecompose subbands into original image \n\n\nStep 2 performs the scaling.  This is what @Benjamin is suggesting.  The problem is that then step 4 becomes necessary, or the original image will not be properly reconstructed.  This longer method will work.  However, the textbook explicitly says that no scaling is performed on the approximation subband.  Of course, it's possible that the textbook is wrong.  However, what's more possible is I'm misunderstanding something about the way this all works -- this is why I'm asking this question.\n\nEDIT (2010/7/8)\n\nI wrote to the author of the book for a confirmation.  He said that you do have to perform the scaling, despite of what is being said in the book.\n    ", "Answer": "\r\nIf you know the sum of the kernel, why not correct the brightness after convolution by dividing by the correct factor? \n\n(For example you can take a 3x3 average with the kernel ```\n[1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9]```\n, but also with ```\n[2, 2, 2, 2, 2, 2, 2, 2, 2]```\n then divide the resulting values by ```\n18```\n...)\n\nEDIT: As stated in my comment, h1 sums to 0. It appears that h0 and h1 are reversed / inverse of each other, so some of the h0 coefficients may have the wrong sign.\n\nEDIT2: Please read comments. I know this post does not answer the question, I am leaving it here for the comments.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Plotting communities with python igraph\r\n                \r\nI have a graph ```\ng```\n in python-igraph. I can get a ```\nVertexCluster```\n community structure with the following:\n\n```\ncommunity = g.community_multilevel()\n```\n\n\n```\ncommunity.membership```\n gives me a list of the group membership of all the vertices in the graph.\n\nMy question is really simple but I haven't found a python-specific answer on SO. How can I plot the graph with visualization of its community structure? Preferably to PDF, so something like\n\n```\nlayout = g.layout(\"kk\")\nplot(g, \"graph.pdf\", layout=layout) # Community detection?\n```\n\n\nThanks a lot.\n    ", "Answer": "\r\nA nice way to plot the communities could be the following using ```\nmark_groups```\n:\n\n\n\nExample:\n\n```\nfrom igraph import *\nimport random\nrandom.seed(1)\n\n\ng = Graph.Erdos_Renyi(30,0.3)\ncomms = g.community_multilevel()\n\n\nplot(comms, mark_groups = True)\n```\n\n\n\n\nThis results in the following:\n\n\n\nHope this helps.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Which algorithm is best to use for finding LCA in tree?\r\n                \r\nI am interested in finding the distance of two nodes in a tree, in a least complexity possible. The finding process is in some queries and updates on the tree (adding and deleting a node).\nThis problem can use LCA as the tool for optimization. However, by these posts I found, there are some algorithm available.\nhttps://cp-algorithms.com/graph/lca.html\nhttps://cp-algorithms.com/graph/lca_binary_lifting.html\nHere is the summary,\n\nLCA + Sqrt Decomposition\n\nPreprocessing time: O(N)\nQuery time: O(√N)\nAllow to update tree: Yes\n\nLCA + Segment Tree\n\nPreprocessing time: O(N)\nQuery time: O(log N)\nAllow to update tree: Yes\n\nLCA + Sparse Table\n\nPreprocessing time: O(N log N)\nQuery time: O(1)\nAllow to update tree: No\n\nLCA + Binary Lifting\n\nPreprocessing time: O(N log N)\nQuery time: O(log N)\nAllow to update tree: Yes\nMy question is, what is the best algorithm best to use?\nOr which algorithm is best to use under what conditions?\nAre there any other advantages or drawbacks that is not mentioned above for each algorithm?\n    ", "Answer": "\r\nFrom personal experience as competitive programmer:\n\nLCA + Sqrt Decomposition - never used this one, seems quite slow query time\nLCA + Segment Tree - this one is quite good the only downside being a bit tedious to code - I rarely use this, usually on some problems where segment tree is already required (i.e. Heavy Light Decomposition)\nLCA + Sparse Table - that O(1) query time is a must for some problems even though they might be rare\nLCA + Binary Lifting - the one I use 90% of the time - easy to code and quite fast\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Is there a simple way to compute a \"smooth\" function with the following characteristics in C/C++?\r\n                \r\nIn order to specify some things first: The user should be able to create a graph by specifying 3 to 5 points on a 2D field. \nThe first and the last points are always at the bounds of that field (their position may only be changed in y direction - not x). The derivation of the graph at these positions should be 0.\nThe position of the 3rd and following points may be specified freely. \nA graph should be interpolated, which goes through all the points. However, this graph should be as smooth and flat as possible. (please apologize for not being mathematically correct)\n\nThe important thing: I need to sample values of that graph afterwards and apply them to a discrete signal. Second thing: Within the range of the x-Axis the values of the function should not exceed the boundaries on the y-Axis.. In my pics that would be 0 and 1 on the y-Axis.\nI created some pics to illustrate what I am talking about using 3 points.\n\nSome thoughts I had:\n\n\nUse (cubic?) splines: their characteristics could be applied to form such curves without too many problems. However, as far as I know, they don't relate to a global x-Axis. They are specified in relation to the next point, through a parameter usually called (s). Therefore it will be difficult to sample the values of the graph related to the x-Axis. Please correct me when I am wrong.\ncreate a matrix, which contains the points and the derivations at those points and solve that matrix using LU decomposition or something equivalent.\n\n\nSo far, I don't have in depth understanding of these techniques, so I might miss some great technique or algorithm I haven't known about yet.\n\nThere is one more thing, that would be great to be able to do: Being able to adjust the steepness of the curve via the change of one or a few parameters. I illustrated this by using a red and a black graph in some of my pictures.\nAny ideas or hints how to solve that efficiently?\n\n\n\n\n    ", "Answer": "\r\nDo you understand how splines are arrived at? \n\nSummary of doing splines\n\nYou break the range into pieces based on the control points (splitting at the control points or putting the breaks between them), and plop some parameterized function into each sub-range, then constrain the functions by the control points, artificially introduced end-point constraints, and inter-segment constrains.\n\nIf you've counted your degrees of freedom and constraints right, you get a solvable system of equations which tells you the right parameters in terms of the control points and away you go.\n\nThe result is a set of parameters for a piecewise function. Generally a piecewise continuous and differentiable function, because what would be the point otherwise.\n\nHow you can use that in this case\n\nSo consider making each interior point the center of a segment which will be occupied by a peak-like function (Gaussian on a linear background, maybe) and use the end points as constraints.\n\nFor ```\nn```\n total points you'd have ```\nD*(n-2)```\n parameters if each segment has ```\nD```\n parameters. You have four end-point constraints ```\nf(start)=y_0```\n, ```\nf(end)=y_n```\n, ```\nf'(start) = f'(end) = 0)```\n, and some set of match constraints between the segments:\n\n```\nf_n(between n and n+1) = f_n+1(between n and n+1)\nf'_n(between n and n+1) = f'_n+1(between n and n+1)\n...\n```\n\n\nplus each segment is constrained by it's relationship to the control point (usually either ```\nf(point n) = y_n```\n or ```\nf'(point n) = 0```\n or both (but you get to decide).\n\nHow many matching constraints you can have depends on the number of degrees of freedom (total number of constraints must equal total number of DoF, right?). You have have to introduce some extra endpoint constraints in the form ```\nf''(start) = 0```\n ... to get it right.\n\nAt that point you're just looking at a lot of tedious algebra to earn how to translate this into a big system of linear equation which you can solve with a matrix inversion. \n\nMost numeric methods books will cover this stuff.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Searching through an acyclic graph with minimum time complexity\r\n                \r\nGiven a city network in the form of an acyclic graph, there are N cities and each city has a unique ID, which is an integer that ranges from 1 to N/ There exists N-1 unique paths between each pair of cities. The population of each city is provided.\nGiven Q queries, the number of cities that have a population smaller than or equal to W on the path from node U to node V should be returned. Here, U and V are the given cities' ID.\nFor example, consider N = 3, with a population of [1, 2, 3] each node. Node connections are: [[1, 2], [2, 3]]. Given a single query (Q = 1) and a set of U, V, W is given as: [[1, 3, 2]].\nAcross the path from node 1 to 3, since there are two cities having a population smaller than or equal to 2, the answer will be 2.\nMy approach uses Depth-First-Search.\n```\nint *city_population(int N, int *population, int **road, int Q, int **cities)\n{\n    int *city_count = (int *)calloc(Q, sizeof(int));\n    int *path = (int *)calloc(N, sizeof(int));\n    int *visited = (int *)calloc(N, sizeof(int));\n    int u, v, w, pathIdx, pathFound, count;\n\n    for (int i = 0; i < Q; i++)\n    {\n        pathIdx = 0;\n        pathFound = 0;\n        count = 0;\n\n        // u is the starting node\n        u = cities[i][0];\n        // v is the stopping node\n        v = cities[i][1];\n        // w is the max population\n        w = cities[i][2];\n\n        DFS(N, u - 1, v, population, visited, road, path, &pathIdx, &pathFound);\n\n        if (pathFound)\n        {\n            for (int j = 0; j < pathIdx; j++)\n            {\n                if (population[path[j]] <= w)\n                    count++;\n            }\n        }\n\n        city_count[i] = count;\n\n        reset_array(visited, N);\n        reset_array(path, N);\n    }\n\n    free(path);\n    free(visited);\n\n    return city_count;\n}\n\nvoid DFS(int N, int startIdx, int stopIdx, int *population, int *visited, int **road, int *path, int *pathIdx, int *pathFound)\n{\n    int connection;\n\n    visited[startIdx] = 1;\n    path[*pathIdx] = startIdx;\n    (*pathIdx)++;\n\n    if (startIdx + 1 == stopIdx)\n    {\n        *pathFound = 1;\n        return;\n    }\n\n    for (int i = 0; i < N - 1 && !*pathFound; i++)\n    {\n        connection = road[i][1];\n        if (road[i][0] == startIdx + 1 && !visited[connection - 1])\n            DFS(N, connection - 1, stopIdx, population, visited, road, path, pathIdx, pathFound);\n\n        else\n        {\n            connection = road[i][0];\n            if (road[i][1] == startIdx + 1 && !visited[connection - 1])\n                DFS(N, connection - 1, stopIdx, population, visited, road, path, pathIdx, pathFound);\n        }\n    }\n\n    if (!*pathFound)\n        (*pathIdx)--;\n\n    visited[startIdx] = 0;\n}\n\nvoid reset_array(int *arr, int size)\n{\n    for (int i = 0; i < size; i++)\n        arr[i] = 0;\n}\n```\n\nAlthough this approach works for a small N value, apparently it doesn't when the value of N gets exponentially large, over the extent of 6 digits. If that's the case, which implementation should be used for constructing a search algorithm that has minimum time complexity?\nSome recommend implementing Breadth-First-Search, the others recommend changing the 2D data structure - road - to something less resource-consuming. Some suggest using Heavy-Light Decomposition, though I'm unaware of which approach to be the most suitable.\n    ", "Answer": "\r\n\nAlthough this approach works for a small N value, apparently it doesn't when the value of N gets exponentially large, over the extent of 6 digits. If that's the case, which implementation should be used for constructing a search algorithm that has minimum time complexity?\n\nAn acyclic graph with N nodes has up to N-1 edges.  For each node your DFS traverses, it scans the entire edge list, for O(N2) asymptotic complexity overall.  This is the primary place where there is room for an algorithmic improvement.\nOne alternative would involve\n\nincluding both directions of each edge in the edge list, and\npre-sorting the edge list by start node\n\nThen, instead of scanning the whole list when processing each node, you could use a binary search to find the edges outgoing from that node.  That would reduce the asymptotic complexity to O(N logN).\nThis does double the storage requirement for the edge list, but that's not going to be an issue unless you are already pushing up pretty close to your limit.\nAnother alternative would involve\n\nrepresenting the edges by giving each node a list of all the nodes connected to it (each one of size proportional to the number of connected nodes)\n\nThen, the DFS won't need to search at all to find the outgoing edges from any node.  No more than N nodes will be traversed, so this reduces the asymptotic complexity to O(N).\n\nNote also that even with one of those improvements, your DFS is susceptible to issues with unfortunately shaped graphs.  For example, consider a connected acyclic graph in which no node has more than two edges.  Such a graph contains one path that traverses every single node without backtracking.  If you have to worry about tens or hundreds of thousands of nodes, then your DFS is likely to run out of stack space trying to recurse so deeply.  Either one of the suggested improvements would allow you to introduce limited protection against this by iterating instead of recursing when the current node has only one (unvisited) successor to visit.  It would still be possible, however, to construct a graph for which the DFS could go to linear depth.\nAdditional possible improvements include\n\nDefine a structure type containing the graph details -- number of nodes, weight (population) of each node, and whatever representation of the edges you choose.  Pass around a pointer to that, instead of passing each item individually.\n\nSince your graph is acyclic, you don't need to keep track of all visited nodes.  Each call to ```\nDFS()```\n only needs to know the immediate predecessor node.  As long as it does not return to that node directly, nothing deeper in the recursion will return to it (or any previous node) either.\n\nYou don't need to record a path.  The DFS can instead keep track of the required count of small populations as it goes, obviating any need to traverse the path again to count populations.\n\n```\nDFS()```\n does not presently return anything.  You could modify it to use its return value instead of an ```\nout```\n parameter to indicate whether it reached the target node.\n\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Potential issue with move for Compound Graph [cytoscape.js]?\r\n                \r\nI've been experimenting Compound Graph editor based on Cytoscape.js.\nI created a function for adding a compound node for a set of selected nodes with the same parent.\nRoughly, it is first tested if the selected nodes have the same parent. If the case, a new node is create, as a child of the parent, and all the selected nodes are moved to this new node.\nIt seems to work well.\nE.g. I created a set of nodes, and grouped them with two levels of decomposition the following way.\nA hierarchy of compound nodes created with the created functions\nHowever, when trying to apply the function on a set of nodes at layer 2 as illustrated in the next figure:\nInitiating grouping for a set of nodes layer 2\nThe result is quite surprising, as illustrated in the next figure. The hierarchy of compound nodes is fully changed, while applying exactly the same algorithm than previously.\nresult of using the function\nSo I'm wondering if somebody already faced this, and if any idea concerning the reason of this behavior.\nIn order to debug, I tried to identify the place where the move function is defined in the code (but I didn't succeed yet) in order to put a breakpoint on the call to the move function and to identify which part of the code could cause the unexpected change of parents.\nSo if someone know where I can find this piece of code, it will also be helpful for understanding where the issue is coming from.\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Trying to make sense of Latent Semantic Indexing(LSI)\r\n                \r\nI am in the process of learning Singular Value Decomposition and for what purposes I can use this concept and the book that I am reading mentioned that SVD is used in Latent Semantic Indexing. I read few articles about LSI and it seems like LSI is mainly used in search engines and in similar applications. I wanted to use LSI for a small data analysis project that I am working on and I am not sure if it makes sense for my application. Here is what I am working with.\n\nI have a list of about 20000 games and 2 of the attributes of this list are the game genre and the platform the game was released on. I wanted to use LSI to gain some information about the platform and the genre attributes.\n\nSo at first I created a Co-occurrence matrix where rows represent the 24 different genres and columns represent 22 different platforms. Then I did SVD decomposition of the co-occurrence matrix and extracted first two columns of U and V and made a 2d plot for U and V.The plot looks like this.\n\n\n\nSo my question is, can this be considered a meaningful usage of Latent Semantic Indexing and also how can I interpret from this graph ? For example, we see that Genre Action and platform PC are far away from all other variables, does this tell us anything about this genre and platform in particular ?\n\nThank you.\n    ", "Answer": "\r\nSVD is usually used to reduce dimensionality and uncover hidden macro characteristics or pattern for a more fine grained behaviour. Almost all recommendation system problem could be approached with this method.\n\nI am not sure about the parameter you used to used to realise your SVD (if it has really converged) but some possible interpretations of your graph :\n\n\nclearly 2 type of gaming platform (which happens to be true console vs PC)\nThere are games like \"SPORT\" which are mainly played on console while others like \"shooter, RPG, Simulation, Adventure\" are more plateforme agnostic (found equivalently on both).\nI am not very sure about \"strategy\" and \"action\" game\n\n\ngood luck\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Python, fourier series of discrete data\r\n                \r\nI am trying to Find the Fourier series representation for n number of harmonics of a discrete time data set. The data is not originally periodic, so I performed a periodic extension on the data set and the result can be seen in the waveform below.\n\n\n\nI have tried to replicate the solution in this question : Calculate the Fourier series with the trigonometry approach\nHowever The results I received did not produce a proper output as can be seen in the pictures below. It seems that the computation simply outputs an offset version of the original signal. \n\n\n\nThe data set I am working with is a numpy array with 4060 elements. \nHow can I properly compute and graph the Fourier series decomposition of a discrete data set?\n\nHere is the code i used, it is almost identical to that in the example referred to in the link, except changes have been made to accommodate my own signal data. \n\n```\n # dat is a list with the original non periodic data\n # persig is essentially dat repeated over several periods\n\n # Define \"x\" range.\n l = len(persig)\n x = np.linspace(0,1,l)\n print len(x)\n\n # Define \"T\", i.e functions' period.\n T = len(dat)\n print T\n L = T / 2\n\n # \"f(x)\" function definition.\n def f(x): \n\n     persig = np.asarray(persig)\n     return persig\n\n# \"a\" coefficient calculation.\n def a(n, L, accuracy = 1000):\n     a, b = -L, L\n     dx = (b - a) / accuracy\n     integration = 0\n     for x in np.linspace(a, b, accuracy):\n         integration += f(x) * np.cos((n * np.pi * x) / L)\n     integration *= dx\n     return (1 / L) * integration\n\n# \"b\" coefficient calculation.\n def b(n, L, accuracy = 1000):\n     a, b = -L, L\n     dx = (b - a) / accuracy\n     integration = 0\n     for x in np.linspace(a, b, accuracy):\n         integration += f(x) * np.sin((n * np.pi * x) / L)\n     integration *= dx\n     return (1 / L) * integration\n\n# Fourier series.   \n def Sf(x, L, n = 5):\n     a0 = a(0, L)\n     sum = np.zeros(np.size(x))\n     for i in np.arange(1, n + 1):\n         sum += ((a(i, L) * np.cos((i * np.pi * x) / L)) + (b(i, L) * np.sin((i * np.pi * x) / L)))\n     return (a0 / 2) + sum   \n\n\n plt.plot(x, f(x))\n\n\n plt.plot(x, Sf(x, L))\n\n\n plt.show()\n```\n\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "ImportError: cannot import name 'treewidth'\r\n                \r\nI have been trying to import the necessary modules from the NetworkX library but it keeps throwing the ImportError or the AttributeError.\n\nI am using the NetworkX library for my final year project and I would like to use this function to compute the treewidth decomposition for a given graph, values of which I need fore my other functions. \n\nI have tried the following things:\n\n\nInitially I tried importing the way suggested in the documentation\n\n\n```\nimport networkx as nx \nfrom networkx.algorithms import approximation\n\nG = nx.barbell_graph(5, 1)\n\ndecom = approximation.treewidth_min_fill_in(G)\n```\n\n\nBut this just gives me the following error:\n\n```\nAttributeError: module 'networkx.algorithms.approximation' has no attribute 'treewidth_min_fill_in'\n```\n\n\n\nI also tried importing it directly by using:\n\n\n```\nimport networkx as nx\n\nG = nx.barbell_graph(5, 1)\n\ndecom = nx.algorithms.approximation.treewidth.treewidth_min_fill_in(G) \n```\n\n\nBut this just gives me the following error:\n\n```\nAttributeError: module 'networkx.algorithms.approximation' has no attribute 'treewidth'\n```\n\n\n\nThen I found a possible solution here and it works only in case of cliques but not when trying to import 'treewidth'.\n\n\n```\nimport networkx as nx \nfrom networkx.algorithms.approximation import treewidth\n\nG = nx.barbell_graph(5, 1)\n\ndecom = treewidth.treewidth_min_fill_in(G)\n```\n\n\nBut this just gives me the following error:\n\n```\nImportError: cannot import name 'treewidth'\n```\n\n\nI haven't had any success finding a solution. Any help is appreciated!\n\nNOTE: This is my first question on SO, hence please give any additional feedback on the format of this question.\n    ", "Answer": "\r\nWhich version of the networkx package do you use and how did you install it?\n\nI have tried to do ```\npip install networkx```\n and then executed your code with Python 3.7:\n\n```\nimport networkx as nx \nfrom networkx.algorithms import approximation\n\nG = nx.barbell_graph(5, 1)\n\ndecom = approximation.treewidth_min_fill_in(G)\nprint(decom)\n```\n\n\nand the output is:\n\n```\n(4, <networkx.classes.graph.Graph object at 0x1070d4ef0>)\n```\n\n\nYou can use try to see version of your module using:\n\n```\nimport networkx\nprint(networkx.__version__)\n```\n\n\nAnd what methods are used:\n\n```\nimport networkx\nfrom networkx.algorithms import approximation\n\nprint(dir(approximation))\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Pseudoinverse of large sparse matrix in R\r\n                \r\nI am trying to calculate the pseudoinverse of a large sparse matrix in R using the singular value decomposition. The matrix L is roughly 240,000 x 240,000, and I have it stored as type dgCMatrix. $L$ represents the Laplacian of a large diameter graph, and I happen to know that the pseudoinverse $L^+$ should also be sparse. From empirical observations of a smaller subset of this graph, L is ~.07% sparse, while $L^+$ which is ~2.5% sparse. \n\nI have tried using pinv, ginv, and other standard pseudoinverse functions, but they error out due to memory constraints. I then tried to opt for the sparse matrix svd provided by the package irlba, which I was then going to use to compute the pseudoinverse using the standard formula after converting all outputs to sparse matrices. My code is here:\n\n```\nlim = 40\ndigits = 4\nSVD =irlba(L,lim)\ntU = round(SVD$u,digits)\nnonZeroU = which(abs(U)>0,arr.ind = T)\nsparseU = sparseMatrix(i=nonZeroU[,2],j=nonZeroU[,1],x = U[nonZeroU])\nV = round(SVD$v,digits)\nnonZeroV = which(abs(V)>0,arr.ind = T)\nsparseV = sparseMatrix(i=nonZeroV[,1],j=nonZeroV[,2],x = U[nonZeroV])\nD = as(Diagonal(x=1/SVD$d),\"sparseMatrx\")\npL =D%*%sparseU\npL = sparseV%*%pL\n```\n\n\nI am able to get to the last line without an issue, but then I get an error due to memory constraints that says\n\n```\nError in sparseV %*% pL : \n  Cholmod error 'problem too large' at file ../Core/cholmod_dense.c, line 105\n```\n\n\nOf course I could piece together the pseudoinverse entry by entry using a for loop and vector multiplications, but I would like to be able to calculate it using a simple function that takes advantage of the sparsity of the resultant pseudoinverse matrix. Is there any way to use the SVD of L to efficiently and approximately compute the pseudoinverse of $L+$, other than calculating each row individually?\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Given a tree, find the kth node in the path from node 'a' to node 'b' in Log(n)?\r\n                \r\nGiven a tree, I need to find the 'k'th node in the path from 'a' to 'b'. That means that I need to find the node at the 'kth' position in the path from node 'a' to node 'b'. I was thinking on the lines of Lowest-common-ancestor and/or heavy-light decomposition but I'm not sure if that's the way to do it. Any guidance in the right direction is appreciated.\n\nDetails:\n\n\nThe tree is NOT a binary tree. It's an undirected graph having n-1 edges, n vertices and no cycles. Just your regular tree\nThe tree has vertices numbered from 1 to n. There are n-1 undirected edges connecting n-1 pairs of these vertices\n'a' and 'b' are any 2 vertices numbered from 1 to n in the tree. We are to find the 'k'th node in the path from 'a' to 'b'. It is guaranteed that the value of 'k' is <= number of nodes in the path from 'a' to 'b'\n\n\nA BFS applied on every query (kth node from 'a' to 'b') is not an option as the number of queries are large. \n    ", "Answer": "\r\nDo the lowest-common-ancestor, and keep for every node it's depth (distance to the root).\n\nNext figure out if the k'th node is on the a to lca or lca to b part. The difference in depth is the number of nodes between them, so if ```\ndepth[a] - depth[lca] > k```\n then the node is on lca-b part.\n\nIf the node is on the a to lca part, just find the k'th ancestor of a. Should be log(N) using the LCA data you precalculated.\n\nIf the node is on the b to lca part, then you can compute k' which is the distance of the kth node from b (```\nk' = 1 + depth[a] + depth[b] - 2*depth[lca] - k)```\n) and then get k' ancestor of b (again easy with the the LCA data).\n\nOverall all the lookups are logN and the other steps are constant so you do O(LogN) per query.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How do minimum multicut algorithms avoid trivial solutions?\r\n                \r\nI've been reading some papers on multicut algorithms for segmenting graph structures. I'm specifically interested in this work which proposes an algorithm to solve an extension of the multicut problem:\n\nhttps://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Keuper_Efficient_Decomposition_of_ICCV_2015_paper.pdf\n\nRegarding the edge costs, it says:\n\n\n  ...for any pair\n  of nodes, a real-valued cost (reward) to all decompositions\n  for which these nodes are in distinct components\n\n\nFair enough. It further says that the solution to the multicut problem is a simple binary vector of length equal to the number of edges in the graph, in which a '1' indicates that the corresponding edge separates two vertices belonging to distinct graph components:\n\n\n  for every edge vw ∈ E ∪ F , y(v,w) = 1 if and only if v and w\n  are in distinct components of G.\n\n\nBut then the optimization problem is written as:\n\n\n\nThis doesn't seem to make sense. If the edge weights depict rewards for that edge connecting nodes in distinct components, shouldn't this be a maximization problem? And in either case, if all edge weights are positive, wouldn't that lead to a trivial solution where ```\ny```\n is an all-zeros vector? The above expression is followed by some constraints in the paper, but I couldn't figure out how any of those prevent this outcome.\n\nFurthermore, when it later tries to generate an initial solution using Greedy Additive Edge Contraction, it says:\n\n\n  Alg. 1 starts from the decomposition into\n  single nodes. In every iteration, a pair of neighboring components is joined for which the join decreases the objective\n  value maximally. If no join strictly decreases the objective\n  value, the algorithm terminates.\n\n\nAgain, if edge weights are rewards for keeping nodes separate, wouldn't joining any two nodes reduce the reward? And even if I assume for a second that edge weights are penalties for keeping nodes separate, wouldn't this method simply lump all the nodes into a single component?\n\nThe only way I see where this would work is if the edge weights are a balanced combination of positive and negative values, but I'm pretty sure I'm missing something because this constraint isn't mentioned anywhere in literature.\n    ", "Answer": "\r\nBetter late than never, here's the answer:\n\nThe weights c_e for cutting the edge e are not restricted to be positive as defined in Definition 1. In fact, Equation (7) specifies that they are log-ratios of two complementary probabilities. That means if the estimated probability for edge ```\ne```\n being cut is greater than 0.5, then c_e will be negative. If it's smaller, then c_e will be positive.\n\nWhile the trivial \"all edges cut\" solution is still feasible, it is quite unlikely that it is also optimal in any \"non-toy\" instance where you will have edges that are more likely to be cut while others are more likely to remain.  \n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "3D volume processing using dask\r\n                \r\nI’m exploring 3D interactive volume convolution with some simple stencils using dask right now.\nLet me explain what I mean:\n\n\nAssume that you have a 3D data which you would like to process through Sobel Transform (for example to get L1 or L2 gradient).\nThen you divide your input 3D image into subvolumes (with some overlapping boundaries – for 3x3x3 stencil Sobel it will demand +2 samples overlap/padding)\nNow let’s assume that you create a delayed computation of the Sobel 3D transform on entire 3D volume – but not executing it yet.\n\n\nAnd now the most important part:\n\n\nI want to write a function which will extract some particular 2D section from the virtually transformed data.\nAnd then finally let dask everything to compute:\n\n\nBut what I need dask to do is not to compute the entire transform for me and then provide a section.\n\n\nI need it to execute only those tasks which are needed to compute that particular 2D transformed image slice.\n\n\n\n\nDo you think – it’s possible?\n\nIn order to explain it with image – please consider this to be a 3D domain decomposition (this is from DWT – but good for illustration from here):\n\nillistration of domain decomposition\n\nAnd assume that there is a function which computes 3D transform of the entire volume using dask.\nBut what I would like to get – for example – is 2D image of the transformed 3D data which consists from LLL1,LLH1,HLH1,HLL1 planes (essentially a single slice).\n\nThe important part is not to compute the whole subcubes – but let dask somehow automatically track the dependencies in the compute graph and evaluate only those.\n\nPlease don’t worry about compute v.s. copy time.\nAssume that it has perfect ratio.\n\nLet me know if more clarification is needed!\nThanks for your help!\n    ", "Answer": "\r\nI'm hearing a few questions.  I'll answer each individually\n\n\nCan Dask track which tasks are required for a subset of outputs and only compute those?\n\n\nYes.  Lazy Dask operations produce a dependency graph.  In the case of dask.arrays this graph is per-chunk.  If your output only depends on a subset of the graph then Dask will remove tasks that are not necessary.  The in-depth docs for this are here and the cull optimization in particular.\n\nAs an example consider this 100,000 by 100,000 array\n\n```\n>>> x = da.random.random((100000, 100000), chunks=(1000, 1000))\n```\n\n\nAnd lets say that I add a couple of 1d slices from it\n\n```\n>>> y = x[5000, :] + x[:, 5000].T\n```\n\n\nThe resulting optimized graph is only large enough to compute the output\n\n```\n>>> graph = y._optimize(y.dask, y._keys())  # you don't need to do this\n>>> len(graph)                              # it happens automatically\n301\n```\n\n\nAnd we can compute the result quite quickly:\n\n```\nIn [8]: %time y.compute()\nCPU times: user 3.18 s, sys: 120 ms, total: 3.3 s\nWall time: 936 ms\nOut[8]: \narray([ 1.59069994,  0.84731881,  1.86923216, ...,  0.45040813,\n        0.86290539,  0.91143427])\n```\n\n\nNow, this wasn't perfect.  It did have to produce all of the 1000x1000 chunks that our two slices touched.  But you can control the granularity there.\n\nShort answer: Dask will automatically inspect the graph and only run those tasks that are necessary to evaluate the output.  You don't need to do anything special to do this.\n\n\nIs it a good idea to do overlapping array computations with dask.array?\n\n\nMaybe.  The relevant doc page is here on Overlapping Blocks with Ghost Cells.  Dask.array has convenience functions to make this easy to write down.  However it will create in-memory copies.  Many people in your position find memcopy too slow.  Dask generally doesn't support in-place computation so we can't be as efficient as proper MPI code.  I'll leave the performance question here to you though.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Cumulative Explained Variance for PCA in Python\r\n                \r\nI have a simple R script for running FactoMineR's PCA on a tiny dataframe in order to find the cumulative percentage of variance explained for each variable:\n\n```\nlibrary(FactoMineR)\na <- c(1, 2, 3, 4, 5)\nb <- c(4, 2, 9, 23, 3)\nc <- c(9, 8, 7, 6, 6)\nd <- c(45, 36, 74, 35, 29)\n\ndf <- data.frame(a, b, c, d)\n\ndf_pca <- PCA(df, ncp = 4, graph=F)\nprint(df_pca$eig$`cumulative percentage of variance`)\n```\n\n\nWhich returns: \n\n```\n> print(df_pca$eig$`cumulative percentage of variance`)\n[1]  58.55305  84.44577  99.86661 100.00000\n```\n\n\nI'm trying to do the same in Python using scikit-learn's decomposition package as follows: \n\n```\nimport pandas as pd\nfrom sklearn import decomposition, linear_model\n\na = [1, 2, 3, 4, 5]\nb = [4, 2, 9, 23, 3]\nc = [9, 8, 7, 6, 6]\nd = [45, 36, 74, 35, 29]\n\ndf = pd.DataFrame({'a': a,\n                  'b': b,\n                  'c': c, \n                  'd': d})\n\npca = decomposition.PCA(n_components = 4)\npca.fit(df)\ntransformed_pca = pca.transform(df)\n\n# sum cumulative variance from each var\ncum_explained_var = []\nfor i in range(0, len(pca.explained_variance_ratio_)):\n    if i == 0:\n        cum_explained_var.append(pca.explained_variance_ratio_[i])\n    else:\n        cum_explained_var.append(pca.explained_variance_ratio_[i] + \n                                 cum_explained_var[i-1])\nprint(cum_explained_var)\n```\n\n\nBut this results in:\n\n```\n[0.79987089715487936, 0.99224337624509307, 0.99997254568237226, 1.0]\n```\n\n\nAs you can see, both correctly add up to 100%, but it seems the contributions of each variable differ between the R and Python versions. Does anyone know where these differences are coming from or how to correctly replicate the R results in Python?\n\nEDIT: Thanks to Vlo, I now know that the differences stem from the FactoMineR PCA function scaling the data by default. By using the sklearn preprocessing package (pca_data = preprocessing.scale(df)) to scale my data before running PCA, my results match the \n    ", "Answer": "\r\nThanks to Vlo, I learned that the differences between the FactoMineR PCA function and the sklearn PCA function is that the FactoMineR one scales the data by default. By simply adding a scaling function to my python code, I was able to reproduce the results.\n\n```\nimport pandas as pd\nfrom sklearn import decomposition, preprocessing\n\na = [1, 2, 3, 4, 5]\nb = [4, 2, 9, 23, 3]\nc = [9, 8, 7, 6, 6]\nd = [45, 36, 74, 35, 29]\ne = [35, 84, 3, 54, 68]\n\n\ndf = pd.DataFrame({'a': a,\n                  'b': b,\n                  'c': c, \n                  'd': d})\n\n\npca_data = preprocessing.scale(df)\n\npca = decomposition.PCA(n_components = 4)\npca.fit(pca_data)\ntransformed_pca = pca.transform(pca_data)\n\ncum_explained_var = []\nfor i in range(0, len(pca.explained_variance_ratio_)):\n    if i == 0:\n        cum_explained_var.append(pca.explained_variance_ratio_[i])\n    else:\n        cum_explained_var.append(pca.explained_variance_ratio_[i] + \n                                 cum_explained_var[i-1])\n\nprint(cum_explained_var)\n```\n\n\nOutput:\n\n```\n[0.58553054049052267, 0.8444577483783724, 0.9986661265687754, 0.99999999999999978]\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "What is the most standard file format and notation for persisting expressive directed graphs?\r\n                \r\nI am interested in persisting individual directed graphs.  This question is not asking for a full-scale graph database solution, but for a document format that I can use to save and individual arbitrary directed graph.  I don't know what notation and file format would be the smartest choice.\n\nMy primary concerns are:\n\n\nExpressiveness/Flexibility - I want the ability to express graphs of different types.  While the standard use case would be a simple directed graph, it should be possible to express trees, cyclical graphs, multi-graphs.  As a bare minimum, I would expect support for labeling and weighting of edges and nodes.  Notations for describing higraphs and edge composition/hyper-edges would also be highly desirable, although I am aware that such solutions may not exist.  \nType System-Independence - I am interested in representing the structural qualities of graphs.  Some solutions include an extensible type system for typed edges and nodes (e.g. RDF/OWL).  I would only be interested in such a representation, if there were a clearly defined canonical decomposition of typed elements into primitives (nodes/edges/attributes).  What I am trying to avoid here is the ability for multiple representations of equivalent graphs, where the equivalence is not discernible.\nCanonical Representation - There should be a mechanism that allows the graph to be represented canonically (in such a way that lexical equivalence of canonical-representations could be used to determine equivalence).\nPresentation Independent - I would prefer a notation that is not dependent upon the presentation of the graph.  This would include spatial orientation, colors, font, etc.  I am only interested in representing the data.  One of the features I don't like about DOT language, DGML or SVG (at least for this particular purpose) is the focus on visual representation.\nStandardized / Open / Compatible - The less implementation work that I have to do, the better.  If the format is standardized and reliable tools already exist for working with the format, then it is more preferable.  Accompanying this requirement is another, that the format should be highly-compatible.  The proprietary nature of Microsoft's DGML is a reason for my aversion, despite the Visual Studio tooling and the fact that I work primarily with .NET (now).  The fact that W3C publishes RDF standards is a motivation for considering a limited subset of RDF as a representational tool.  I also appreciate GXL and GraphML, because they have well documented xml schemas, thereby facilitating the ability to integrate their data with any xml-compatible software package.\nSimplicity / Readability - I appreciate human-readable syntax and ease of interpretation.  I also appreciate representation that simplifies parsing.  For this reason, I like GML, but I am concerned it is not mainstream enough to be a realistic choice.  I would also consider JSON or YAML for readability, if they were not so limited in their respective abilities to represent complex (non-DAG) structures.\nEfficiency / Concise Representation - It's worth considering that whatever format I end up choosing will inevitably have to be persisted and transferred over some network.  Therefore, file size is a relevant consideration.  \n\n\nOverview\n\nI recognize that I will most likely be unable to find a solution that satisfies every criteria on my wishlist.  I am simply asking for the file format that is closest to what I want and that doesn't limit extensibility for unsupported use cases.\n    ", "Answer": "\r\nWhat about Trivial Graph Format:\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Degree Matrix in Spectral Clustering\r\n                \r\nI am currently learning spectral clustering.\n\nWe decomposite the Laplacian Matrix which calculated by L = D - W.\n\nW is the adjacent matrix.\n\nHowever, I have found a lot codes online like\nspectral clustering\n\nthey directly calculate D by diag(sum(W)). \n\nI know that D should be degree matrix which means each value on the diagonal are the degree for each point.\n\nBut if W is a weighted graph , diag(sum(W)) is not equal to the actual \"Degree matrix\"... \n\nWhy they  still do this.\n    ", "Answer": "\r\nWhen you work with weighted graphs you can compute the degree matrix from the weighted adjacency matrix, some time it is good to have weights because they hide geometric information. Moreover, if you have the weighted adj matrix computing degree matrix using the binary form of your weighted adj matrix is easy. In addition, I think your question has more theoretical (e.g. Mathoverflow) than programming foundation (e.g stackoverflow) ;). In any case, you should consult this link for more intuitive explanation of L and its geometric relation. \n\nGood luck :)  \n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Recreate sets from combinations of these sets\r\n                \r\nI came across a specific problem and looking for some algorithm for it. The problem to solve is as described below.\n\nLet's say we have combinations like below \n\n1 - 3 - 5\n\n1 - 4 - 5\n\n1 - 8 - 5\n\n2 - 4 - 5\n\n3 - 4 - 5\n\n2 - 4 - 7\n\nThese combinations were generated from given sets, in this particular case let's say from\n\n{1},{3,4,8},{5}\n\n{2,3},{4},{5}\n\n{2},{4},{7}\n\nWhat I want to do is recreate sets from these combinations. I know for these combinations you have more than one solution, e.g. \n\n1st solution\n\n{1}, {3, 4, 8}, {5}\n\n{2, 3}, {4}, {5}\n\n{2}, {4}, {7}\n\n2nd solution \n\n{1}, {3, 8}, {5}\n\n{1, 2, 3}, {4}, {5}\n\n{2}, {4}, {7}\n\n3rd solution\n\n{1}, {3, 4, 8}, {5}\n\n{3}, {4}, {5}\n\n{2}, {4}, {5, 7}\n\nBut the final (optimal) solution would be the one with as little sets as possible or the random one in case they are all equivalent in terms of sets count. \n\nDo algorithms for such a problem exist? I appreciate if anybody who has been dealing with this kind of problem can give me some hints. \n\nEDIT: looks like what I'm looking for is a decomposition of n-ary product (Cartesian product for N)\n\nEDIT: after more research on the topic I found out that the problem is known in a 'graph theory' as the 'minimum clique cover' problem\n\nregards,\nbaz\n    ", "Answer": "\r\nThis is not really an answer, more an extended comment.  Your 'compressed representations' do not, in fact save any space at all.\n\nIn the uncompressed representation you store:\n\n\nthe rule that says that each group is made up of 3 symbols; and\n18 (in your example) symbols.\n\n\nThis can be stored in 1R+18S (where R is the space required to store a rule, S the space required to store a symbol)\n\nIn your supposed-to-be compressed representations you have to store:\n\n\nthe rule that says that each group is made up of 3 sets of symbols;\nthe symbols in each set; and\nanother symbol which delimits each set from the next.\n\n\nIn your first 'compressed' representation I count 1R+12S+8D (where D is the space required for storing one delimiter).  If S==D then this is 1R+20S -- more than your uncompressed representation.\n\nIn your other two 'compressed' representations I count the same: 1R+12S+8D, and 1R+12S+8D.\n\nI haven't figured out whether this non-compression is an essential feature of your proposal, or an accidental feature of the example you have chosen.\n\nDo you mean, when you write that \n\n\n  the count of elements in combination\n  is actually N\n\n\nthat some combinations will have 3 elements, others 4, others 2 or 5 etc ?\n\nI suggest that you clarify your question.\n\nEDIT: @bazeusz: now it seems that you are looking for the cartesian product of the sets.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Does D3 Support a Compositional (bottom-up) Approach?\r\n                \r\nNormally when I model things, say in a 3D scene graph, I describe low-level components first and then compose them into more and more complicated artifacts. For example, create a tire, then some lugnuts and then a hub and compose them together to define a wheel. Then take the wheel, body and engine and create a car. Finally, place the cars in the world.\n\nD3, however, is usually expressed in a decomposition (top-down modeling). So it might have you create a table, then define its headers, then define its rows, then define the cells in each row.\n\nThere are advantages and disadvantages to both approaches, though for 3D modeling I've seen numerous novices get themselves tangled up trying to describe a complicated scene using top-down approaches. Much easier to think about low-level components in their own space, then then how to place and add them to their parent structures.\n\nD3 is heavily biased against this, but I decided to try a compositional modeling approach anyway.\n\nIn short, it doesn't seem to be supported. To make concrete what I mean, I tried to create a table header component () first, and then afterwards append the table headers to a table element. I got this error message:\n\n```\nUncaught InvalidCharacterError: Failed to execute 'createElement' on 'Document': The tag name provided ('[object HTMLDivElement]') is not a valid name.```\n\n\nI broke it down to a very simple example, and it still breaks.\n\n\r\n\r\n```\nvar component = document.createElement(\"div\");\r\nvar d3component = d3.select(component).text(\"XYZZY\");\r\nvar root = d3.select('#target');\r\n// root.append(d3component);        // FAILURE\r\nroot.append('div').text(\"FROTZ\");```\n\r\n```\n<!DOCTYPE html> <html lang=\"en\">\r\n  <head>\r\n    <script src=\"http://d3js.org/d3.v3.min.js\"></script>\r\n  </head>\r\n  <body>\r\n    <div id=\"target\"></div>\r\n  </body>\r\n</html>```\n\r\n\r\n\r\n\n\nIf you leave the ```\nroot.append(d3component)```\n line commented out, it runs successfully. However, if you uncomment it, D3 will fail with the above error message.\n\nAm I missing something here, or is this approach truly unsupported by D3?\n    ", "Answer": "\r\nHaving given this problem a second thought, I am not happy with my original answer, which I am going to keep at the end of this post, because it is correct, but it somehow begs the question of \"Does D3 Support a Compositional (bottom-up) Approach?\". The answer should be quite yes, but not really.\n\n\nYes, because D3 allows for mimicking a bottom-up approach by doing it the way you tried it. That failed for technical shortcomings, which can be cured as I explained in my original answer below, but this feels somewhat awkward and abusive. The main reason for this is, that D3 provides no means for explicitly creating elements. In most cases, DOM elements are created implicitly when doing an ```\n.append()```\n or an ```\n.insert()```\n. The only exception to this is providing a function to these methods which returns a DOM element created using methods outside of D3. It is important to understand, that this is not considered to be a deficiency but is by design.\nNot really, because at its core D3 is all about binding data to the hierarchy of a DOM tree. The concept of selections allows for keeping your data and the corresponding part of the DOM in sync. This is done by obtaining a selection, binding data to it and use the enter, update, exit scheme to add or remove elements from the DOM. Furthermore, when adding elements to a selection or when doing a subselection in many cases data bound to a parent element will be inherited by its children. This calls for a strictly hierarchical top-down approach.\n\nWhether or not there is data involved, doing it the D3 way will start off with a selection be it initially empty or not (it may very well be ```\nd3.select(document)```\n to start root-most) and add to or remove from it in a top-down fashion. \n\n\nTo cut a long story short, it is possible to mock a bottom-up approach using D3, though it wasn't designed to be used this way. If you decide to give it a try, keep in mind that it won't be neither the most elegant nor the most intuitive way, and is likely to introduce some overhead. To compose your layout in a bottom-up way it might be worth evaluating other libraries which might be better suited for this.\n\n\n\nOriginal Answer\n\nYou cannot use ```\n.append()```\n to add one of D3's selections to the DOM. You can, however, provide a function to ```\n.append()```\n which returns a DOM element to be appended. This could be done by replacing the erroneous line with\n\n```\nroot.append(function() { return component; });\n```\n\n\nOr, if you don't have a direct reference to the element, you may retrieve it from a selection\n\n```\nroot.append(function() { return d3component.node(); });\n```\n\n\nYour example might then look like this\n\n\r\n\r\n```\nvar component = document.createElement(\"div\");\r\nvar d3component = d3.select(component).text(\"XYZZY\");\r\nvar root = d3.select('#target');\r\nroot.append(function() { return component; });\r\n// ...or...\r\n//root.append(function() { return d3component.node(); });```\n\r\n```\n<!DOCTYPE html> <html lang=\"en\">\r\n  <head>\r\n    <script src=\"http://d3js.org/d3.v3.min.js\"></script>\r\n  </head>\r\n  <body>\r\n    <div id=\"target\"></div>\r\n  </body>\r\n</html>```\n\r\n\r\n\r\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Reuse of computed Constant Values in Tensorflow\r\n                \r\nI have encountered the following behavior and I'm not quite sure if this is a bug or if I'm missing out on something.\n\nMinimal Example\n\n```\n# Create constant Matrix A\nA1 = tf.convert_to_tensor(A, dtype=tf.float64)\n\n# Compute the cholesky decomposition\nsqrtA1 = tf.linalg.cholesky(tfA1)\n\nxi = tf.placeholder(tf.float64, shape=[10000])\n\n# Matrix multiplication with the chompsky matrix\nRessqrtA1 = tf.tensordot(tfsqrtA1, tfxi, [[1],[0]])\n\n# Regular Matrix multiplication\nResA1 = tf.tensordot(tfA1, tfxi, [[1],[0]])\n```\n\n\nUpon Computing something like this: \n\n```\nwith tf.Session() as sess:\n    _ = sess.run(ResA1, feed_dict={xi: np.random.randn(10000)})\n    _ = sess.run(ResA1, feed_dict={xi: np.random.randn(10000)})\n```\n\n\nOne could assume that the first ```\nsess.run()```\n will take more time since the Graph has to be build and optimized. And that is what happens, here are some timings just like expected:\n\n```\nnormal matmul\nWall time: 36.1 s -- first run \n\n41.1 ms ± 2.15 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) \n--second run with timit\n```\n\n\nLong Calculation times despite a constant Operation\n\nNow one might think that the same holds when multiplying the ```\nsqrtA1```\n, but unfortunately its not.\nComputing the following:\n\n```\nwith tf.Session() as sess:\n    _ = sess.run(RessqrtA1, feed_dict={xi: np.random.randn(10000)})\n    _ = sess.run(RessqrtA1, feed_dict={xi: np.random.randn(10000)})\n```\n\n\nHere are some timings: \n\n```\ncholesky\nWall time: 2min 38s\n24.7 s ± 750 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n\n\nI ran the TensorFlow Profiler and obtained this information: \n\n```\nProfile:\nnode name | requested bytes | total execution time | accelerator execution time | cpu execution time\nCholesky                     800.00MB (100.00%, 50.00%),     24.30sec (100.00%, 99.81%),             0us (0.00%, 0.00%),     24.30sec (100.00%, 99.81%)\nMatMul                          80.00KB (50.00%, 0.00%),         46.45ms (0.19%, 0.19%),             0us (0.00%, 0.00%),         46.45ms (0.19%, 0.19%)\nReshape                               0B (0.00%, 0.00%),            15us (0.00%, 0.00%),             0us (0.00%, 0.00%),            15us (0.00%, 0.00%)\nConst                         800.00MB (50.00%, 50.00%),            10us (0.00%, 0.00%),             0us (0.00%, 0.00%),            10us (0.00%, 0.00%)\n```\n\n\nDifference between TF 1.7 and TF 1.6\n\nNow for the interesting Part: This is only when you use Version 1.7.0 or higher\nI tried this with Version 1.6.0 and the Matmuls with the cholesky matrix took not longer than a normal matmul: \n\n```\nWall time: 1min 18s -- first run\n49.7 ms ± 1.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n-- timeit on the second sess.run()\n```\n\n\nWhen looking into the Computation in Version 1.6 with the Profiler you get: \n\n```\nProfile:node name | requested bytes | total execution time | accelerator execution time | cpu execution time\nCholesky                    800.00MB (100.00%, 100.00%),       891us (100.00%, 100.00%),             0us (0.00%, 0.00%),       891us (100.00%, 100.00%)\n======================End of Report==========================\n```\n\n\nLastely, using this code, while replacing ```\nRessqrtA1 = tf.tensordot(chol, tfxi, [[1],[0]])```\n,where ```\nchol```\n is another Placeholder, gives us the desired outcome again.\n\n```\n%time temp = sess.run(tfsqrtA1)\n%timeit _ = sess.run(tfRessqrtA1, feed_dict={tfxi: np.random.randn(10000), chol: temp})\n\ncholesky\nWall time: 1min 40s\n42.6 ms ± 3.86 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n\n\nMy Thoughts:\n\nSomehow since Version 1.7 for every session run the cholesky calculation happens despite being from constant origin and thus it would suffice to calculate it once and reuse the outcome.  \n\nI think that such a computation happens in version 1.6 during the construction of the Graph as I measured that the calculation of the Cholesky Decomposition takes about 26sec in V1.6 (used a Placeholder so that it needs to be calculated during runtime), yet during the run it only took 900us as shown above.  \n\nIn v1.7 they somehow changed this behavior. \n\nThe Question\n\nDo you think this behavior change is normal or desired? If so, what would be a better approach when you have a similar situation? \n\nI'm thankful for every information, maybe someone else noticed some related problems using the linear algebra class. \n\nWhat i have tried so far:\n\n\ntinkering around with the float size (i.e. ```\ntf.float32```\n instead of ```\ntf.float64```\n)\nused different Versions in seperate environments.\nused the profiler to investigate the times of different Ops\nran on a different pc \ntiming in Python (e.g. ```\ntimeit```\n or ```\npycallgraph```\n) \n\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Correct data structure for point location algorithm\r\n                \r\nI am dealing with a programming challenge, In that I need to find the region which the given point belongs to. There is some methods here for that problem.\n\nPoint Location\n\nSo I decide to use slab decomposition, It is fast enough, easier to implement, and space is not much of a problem for me. However, I am having trouble about where to start. Here is an example of slab decomposition from a pdf file made by UC Santa Barbara.\n\n\n\nI store the geometrical shape in a node dictionary like an undirected graph (Using coordinates).\n\n```\ndefaultdict(<type 'list'>, {(4.0, 5.0): [(1.0, 2.0), (2.0, 3.0), (3.0, 5.0)],\n (1.0, 2.0): [(2.0, 3.0), (2.0, 4.0), (3.0, 5.0), (4.0, 5.0)],\n (2.0, 3.0): [(1.0, 2.0), (2.0, 4.0), (4.0, 5.0)],\n (2.0, 4.0): [(1.0, 2.0), (2.0, 3.0), (3.0, 5.0)],\n (3.0, 5.0): [(1.0, 2.0), (2.0, 4.0), (4.0, 5.0)]})\n```\n\n\nLike so. (Still don't know if a edges list would be better ?)\n\nNow I know how to solve the problem, however having difficulty deciding which way to implement because the input will be a very complex geometrical shape and finding the point will be expensive in cpu resources.\n\nI decided to store each point (sorted using x coordinates) in a sorted list(using bisect). To acquire slabs. However, I could not find a way to find how the slabs intersect with the region edges, or how to partition the slab as shown in the picture. Actually I did find a way but it did not feel feasible to me. I could check the edges which starts from the left of the slab and end at the right of it. Which means the edge is crossing the slab. This is fine, however to accomplish that I would have to check almost the half of the vertices each time a new node is given and the regions increase. So This method sounded like a failure to me. There is also the problem to knowing which region does the region in the slab belongs to. Given that we are doing all this to avoid checking all the regions one-by-one to improve speed.\n\nIf you could give me some ideas on the subject, I don't require any piece of code. I just need advice from experienced users here. I am stuck because I can't be sure how to implement it and I don't want to start it the wrong way and rewrite it whole. (I can assure you this is no homework.)\n\nNote1: I could not be sure about the data structure, should I make a structure for regions?, or for points ? or edges ? Or do I need a structure for a creating a search tree ? What would I have to store in this structure ?\n\nNote2: I know how to find which slabs the point lies. I also know how to find the point with binary search inside the slabs. I am lacking more conseptual knowledge, hence experience. Like how to represent regions in the first place.\n\nThanks in advance.\n    ", "Answer": "\r\nI would advise to use your vertices only to reference too. Use an edge list to represent the graph and assign a left and right incident region to each edge. Now use these edges in your slab decomposition. As soon as you find your slab and your slab part you know also its region as both edges near your point reference to it. \n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Efficient algorithm for computing a compositional square root\r\n                \r\nSuppose that we have a finite alphabet, for example ```\nX = [1,2,3,..., n]```\n and a map ```\nf```\n with keys in ```\nX```\n and values also in ```\nX```\n (a mathematical function from ```\nX```\n to ```\nX```\n).\n\nDefinition: A functional square root, or compositional square root, or half-iterate, is another map ```\ng```\n with keys in ```\nX```\n and values in ```\nX```\n such that ```\ng[g[x]]==f[x]```\n for all ```\nx```\n in ```\nX```\n.\n\nExample 1: If ```\nf = {'A':'A', 'B':'A', 'C':'A'}```\n then ```\ng = {'A':'A', 'B':'A', 'C':'B'}```\n can be a functional square root of ```\nf```\n. The functional square root is not unique. The map ```\nf```\n is also its own functional square root.\n\nExample 2: The map ```\nf = ['A':'B', 'B':'A']```\n doesn't have a functional square root. In fact, if we define ```\ng['A'] = 'A'```\n, then ```\n'A'```\n would be a fixed point of ```\ng```\n and therefore it would have to be a fixed point of ```\nf```\n. If we define ```\ng['A'] = 'B'```\n, then we would need to define ```\ng['B'] = 'B'```\n and ```\n'B'```\n would have to be a fixed point of ```\nf```\n. So, sometimes the functional square root doesn't exist.\n\nI have computed this by hand sometimes, but now I was thinking that I don't really have an efficient strategy to compute it.\n\nQuestion: [Design an efficient algorithm to] Compute a functional square root given an input map.\n\nI think it is fair to work out first the case that the input is guaranteed to have a functional square root.\n\nExistence of an algorithm: There is always the brute force solution of generating all maps ```\ng```\n from ```\nX```\n to ```\nX```\n and composing each with itself to check if they are equal to ```\nf```\n.\n\n\n\nSome observations I have seen so far that allow to reduce the problem for some inputs:\n\n\nEach fixed point, a key ```\nx```\n such that ```\nf[x] == x```\n, allows one to define ```\ng[x] = x```\n.\nEach key ```\ny```\n that is mapped to a fixed point, ```\nf[y] == x```\n and ```\nf[x] == x```\n, allows one to define ```\ng[y] = x```\n.\nNo other keys, besides the ones satisfying the condition in point (2), should be mapped by ```\ng```\n to a fixed point of ```\nf```\n.\nA case that there is an efficient algorithm: If ```\nf```\n is injective (no two keys are sent to the same value) then ```\nf```\n is a permutation of ```\nX```\n in that case a square root can be computed from its cycle decomposition of the permutation. Finding the cycle decomposition can be done in one pass and a square root from that also. \n\n\nThe fact that for bijections the square root can be computed (it looks) linearly makes me hopeful that there may be some efficient way to compute the general case. If I got my definitions right, the problem is NP since a given ```\ng```\n one can verify that ```\ng```\n is a compositional square root of ```\nf```\n in linear time. To me it doesn't smell like NP-complete, but I have very little experience with proving those reductions from one problem to another.\n\n\n\nThe problem can also be seen in the following way. The input map ```\nf```\n can be seen as a directed graph of a finite automaton in which each state has out-degree 1. Computing the compositional square root would be to find an automaton with the same set of states that would behave like the original one when stepping two steps at a time.\n    ", "Answer": "\r\nThe problem can be broken down into two problems.\n\nIf you take the intersection of the ranges of ```\nf```\n, ```\nf^2```\n, ```\nf^3```\n, ... you'll come up with a finite set ```\nY```\n that ```\nf```\n operates as a permutation on.  As you noted in observation 4, we can find all of the square roots of a permutation.  There may be many such, but we know what choices we could have made on the way to finding them.  We either pair up cycles of the same length with some rotation, or we reorder a cycle of odd length.  If we can't do that, there is no square root, and all of the ways of making a square root can be enumerated by the ways to make those two choices.\n\nThe second problem is the transient values, that form chains ending in ```\nY```\n.  The chains are easy to find, and we know where they end.  If ```\ng```\n is to be a square root of ```\nf```\n, then a chain of length ```\n2k```\n or ```\n2k-1```\n for ```\ng```\n has to be a pair of chains of ```\nf```\n of length ```\nk```\n and either ```\nk```\n or ```\nk-1```\n such that the end of the first is ```\ng(x)```\n where ```\nx```\n is the end of the second.  That is a very strong condition, because either those chains both end in the same cycle of odd length with the possibly shorter one ending almost halfway back in ```\np```\n, or else those chains both end in different cycles of the same length, and came from a cycle of even length.\n\nI would therefore suggest tackling the problem as follows.\n\nFirst turn X into a graph where you have a directed arrow from ```\nx```\n to ```\nf(x)```\n.  Find all cycles and chains.  The union of all cycles is ```\nY```\n, and first verify that ```\nf```\n has a square root in ```\nY```\n.  After that, sanity check that the chains could be matched up.  If not, there is no square root.\n\nIf you pass a couple of sanity checks, try to actually match up the chains recursively into a working square root for the part of the graph that contains them.  If this succeeds, then you can easily match up the rest of the cycles and you have a square root.  If it fails, then there is no square root.\n\nIn most cases, you'll find there is no square root fairly fast.  The possibly slow approach is the matching of the chains recursively.  Hopefully there aren't too many of those decisions, and they are fairly easy to make, but an exponential backtracking is not, a priori, impossible to find.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Consistent size for GraphPlots\r\n                \r\nUpdate 10/27: I've put detailed steps for achieving consistent scale in an answer. Basically for each Graphics object you need to fix all padding/margins to 0 and manually specify plotRange and imageSize that such that 1) plotRange includes all graphics 2) imageSize=scale*plotRange\n\nStill now sure how to do 1) in full generality, a solution that works for Graphics consisting of points and thick lines (AbsoluteThickness) is given\n\n\n\nI'm using \"Inset\" in VertexRenderingFunction and \"VertexCoordinates\" to guarantee consistent appearance among subgraphs of a graph. Those subgraphs are drawn as vertices of another graph, using \"Inset\". There are two problems, one is that resulting boxes are not cropped around the graph (ie, graph with one vertex still gets placed in a big box), and another is that there's strange variation among sizes (you can see one box is vertical). Can anyone see a way around these problems?\n\nThis is related to an earlier question of how to keep vertex sizes looking the same, and while Michael Pilat's suggestion of using Inset works to keep vertices rendering at the same scale, overall scale may be different. For instance on the left branch, the graph consisting of vertices 2,3 is stretched relative to the \"2,3\" subgraph in the top graph, even though I'm using absolute vertex positioning for both\n\n\n(source: yaroslavvb.com)  \n\n```\n(*utilities*)intersect[a_, b_] := Select[a, MemberQ[b, #] &];\ninduced[s_] := Select[edges, #~intersect~s == # &];\nNeeds[\"GraphUtilities`\"];\nsubgraphs[\n   verts_] := (gr = \n    Rule @@@ Select[edges, (Intersection[#, verts] == #) &];\n   Sort /@ WeakComponents[gr~Join~(# -> # & /@ verts)]);\n\n(*graph*)\ngname = {\"Grid\", {3, 3}};\nedges = GraphData[gname, \"EdgeIndices\"];\nnodes = Union[Flatten[edges]];\nAppendTo[edges, #] & /@ ({#, #} & /@ nodes);\nvcoords = Thread[nodes -> GraphData[gname, \"VertexCoordinates\"]];\n\n(*decompose*)\nedgesOuter = {};\npr[_, _, {}] := None;\npr[root_, elim_, \n   remain_] := (If[root != {}, AppendTo[edgesOuter, root -> remain]];\n   pr[remain, intersect[Rest[elim], #], #] & /@ \n    subgraphs[Complement[remain, {First[elim]}]];);\npr[{}, {4, 5, 6, 1, 8, 2, 3, 7, 9}, nodes];\n\n(*visualize*)\n\nvrfInner = \n  Inset[Graphics[{White, EdgeForm[Black], Disk[{0, 0}, .05], Black, \n      Text[#2, {0, 0}]}, ImageSize -> 15], #] &;\nvrfOuter = \n  Inset[GraphPlot[Rule @@@ induced[#2], \n     VertexRenderingFunction -> vrfInner, \n     VertexCoordinateRules -> vcoords, SelfLoopStyle -> None, \n     Frame -> True, ImageSize -> 100], #] &;\nTreePlot[edgesOuter, Automatic, nodes, \n EdgeRenderingFunction -> ({Red, Arrow[#1, 0.2]} &), \n VertexRenderingFunction -> vrfOuter, ImageSize -> 500]\n```\n\n\nHere's another example, same problem as before, but the difference in relative scales is more visible. The goal is to have parts in the second picture match precisely the parts in the first picture.\n\n\n(source: yaroslavvb.com)  \n\n```\n(* Visualize tree decomposition of a 3x3 grid *)\n\ninducedGraph[set_] := Select[edges, # \\[Subset] set &];\nSubset[a_, b_] := (a \\[Intersection] b == a);\ngraphName = {\"Grid\", {3, 3}};\nedges = GraphData[graphName, \"EdgeIndices\"];\nvars = Range[GraphData[graphName, \"VertexCount\"]];\nvcoords = Thread[vars -> GraphData[graphName, \"VertexCoordinates\"]];\n\nplotHighlight[verts_, color_] := Module[{vpos, coords},\n   vpos = \n    Position[Range[GraphData[graphName, \"VertexCount\"]], \n     Alternatives @@ verts];\n   coords = Extract[GraphData[graphName, \"VertexCoordinates\"], vpos];\n   If[coords != {}, AppendTo[coords, First[coords] + .002]];\n   Graphics[{color, CapForm[\"Round\"], JoinForm[\"Round\"], \n     Thickness[.2], Opacity[.3], Line[coords]}]];\n\njedges = {{{1, 2, 4}, {2, 4, 5, 6}}, {{2, 3, 6}, {2, 4, 5, 6}}, {{4, \n     5, 6}, {2, 4, 5, 6}}, {{4, 5, 6}, {4, 5, 6, 8}}, {{4, 7, 8}, {4, \n     5, 6, 8}}, {{6, 8, 9}, {4, 5, 6, 8}}};\njnodes = Union[Flatten[jedges, 1]];\n\nSeedRandom[1]; colors = \n RandomChoice[ColorData[\"WebSafe\", \"ColorList\"], Length[jnodes]];\nbags = MapIndexed[plotHighlight[#, bc[#] = colors[[First[#2]]]] &, \n   jnodes];\nShow[bags~\n  Join~{GraphPlot[Rule @@@ edges, VertexCoordinateRules -> vcoords, \n    VertexLabeling -> True]}, ImageSize -> Small]\n\nbagCentroid[bag_] := Mean[bag /. vcoords];\nfindExtremeBag[vec_] := (\n   vertList = First /@ vcoords;\n   coordList = Last /@ vcoords;\n   extremePos = \n    First[Ordering[jnodes, 1, \n      bagCentroid[#1].vec > bagCentroid[#2].vec &]];\n   jnodes[[extremePos]]\n   );\n\nextremeDirs = {{1, 1}, {1, -1}, {-1, 1}, {-1, -1}};\nextremeBags = findExtremeBag /@ extremeDirs;\nextremePoses = bagCentroid /@ extremeBags;\nvrfOuter = \n  Inset[Show[plotHighlight[#2, bc[#2]], \n     GraphPlot[Rule @@@ inducedGraph[#2], \n      VertexCoordinateRules -> vcoords, SelfLoopStyle -> None, \n      VertexLabeling -> True], ImageSize -> 100], #] &;\n\nGraphPlot[Rule @@@ jedges, VertexRenderingFunction -> vrfOuter, \n EdgeRenderingFunction -> ({Red, Arrowheads[0], Arrow[#1, 0]} &), \n ImageSize -> 500, \n VertexCoordinateRules -> Thread[Thread[extremeBags -> extremePoses]]]\n```\n\n\nAny other suggestions for aesthetically pleasing visualization of graph operations are welcome.\n    ", "Answer": "\r\nHere are the steps needed to achieve precise control over relative scales of graphics objects.\n\nTo achieve consistent scale one needs to explicitly specify input coordinate range (regular coordinates) and output coordinate range (absolute coordinates). Regular coordinate range depends on ```\nPlotRange```\n, ```\nPlotRangePadding```\n (and possibly others options?). Absolute coordinate range depends on ```\nImageSize```\n,```\nImagePadding```\n (and possibly other options?). For ```\nGraphPlot```\n, it is sufficient to specify ```\nPlotRange```\n and ```\nImageSize```\n. \n\nTo create Graphics object that renders at a pre-determined scale, you need to figure out ```\nPlotRange```\n needed to fully include the object, corresponding ```\nImageSize```\n and return ```\nGraphics```\n object with these settings specified. To figure out the necessary ```\nPlotRange```\n when thick lines are involved it is easier to deal with ```\nAbsoluteThickness```\n, call it ```\nabs```\n. To fully include those lines you could take the smallest ```\nPlotRange```\n that includes endpoints, then offset minimum x and maximum y boundaries by abs/2, and offset maximum x and minimum y boundaries by (abs/2+1). Note that these are output coordinates.\n\nWhen combining several ```\nscale-calibrated```\n Graphics objects you need to recalculate ```\nPlotRange/ImageSize```\n and set them explicitly for the combined Graphics object.\n\nTo Inset ```\nscale-calibrated```\n objects into ```\nGraphPlot```\n you need to make sure that coordinates used for automatic ```\nGraphPlot```\n positioning are in the same range. For that, you could pick several corner nodes, fix their positions manually, and let automatic positioning do the rest.\n\nPrimitives ```\nLine```\n/```\nJoinedCurve```\n/```\nFilledCurve```\n render joins/caps differently depending on whether the line is (almost) collinear, so one needs to manually detect collinearity.\n\nUsing this approach, rendered images should have width equal to\n\n```\n(inputPlotRange*scale + 1) + lineThickness*scale + 1```\n\n\nFirst extra ```\n1```\n is to avoid the \"fencepost error\" and second extra 1 is the extra pixel needed to add on the right to make sure thick lines are not cut-off\n\nI've verified this formula by doing ```\nRasterize```\n on combined ```\nShow```\n and rasterizing a 3D plot with objects mapped using ```\nTexture```\n and viewed with ```\nOrthographic```\n projection and it matches the predicted result. Doing 'Copy/Paste' on objects ```\nInset```\n into ```\nGraphPlot```\n, and then Rasterizing, I get an image that's one pixel thinner than predicted.\n\n\n(source: yaroslavvb.com)  \n\n```\n(**** Note, this uses JoinedCurve and Texture which are Mathematica 8 primitives.\n      In Mathematica 7, JoinedCurve is not needed and can be removed *)\n\n(** Global variables **)\nscale = 50;\nlineThickness = 1/2; (* line thickness in regular coordinates *)\n\n(** Global utilities **)\n\n(* test if 3 points are collinear, needed to work around difference \\\nin how colinear Line endpoints are rendered *)\n\ncollinear[points_] := \n Length[points] == 3 && (Det[Transpose[points]~Append~{1, 1, 1}] == 0)\n\n(* tales list of point coordinates, returns plotRange bounding box, \\\nuses global \"scale\" and \"lineThickness\" to get bounding box *)\n\ngetPlotRange[lst_] := (\n   {xs, ys} = Transpose[lst];\n   (* two extra 1/\n   scale offsets needed for exact match *)\n   {{Min[xs] - \n      lineThickness/2, \n     Max[xs] + lineThickness/2 + 1/scale}, {Min[ys] - \n      lineThickness/2 - 1/scale, Max[ys] + lineThickness/2}}\n   );\n\n(* Gets image size for given plot range *)\n\ngetImageSize[{{xmin_, xmax_}, {ymin_, ymax_}}] := (\n   imsize = scale*{xmax - xmin, ymax - ymin} + {1, 1}\n   );\n\n(* converts plot range to vertices of rectangle *)\n\npr2verts[{{xmin_, xmax_}, {ymin_, ymax_}}] := {{xmin, ymin}, {xmax, \n    ymin}, {xmax, ymax}, {xmin, ymax}};\n\n(* lifts two dimensional coordinates into 3d *)\n\nlift[h_, coords_] := Append[#, h] & /@ coords\n(* convert Raster object to array specification of texture *)\n\nraster2texture[raster_] := Reverse[raster[[1, 1]]/255]\n\nSubset[a_, b_] := (a \\[Intersection] b == a);\ninducedGraph[set_] := Select[edges, # \\[Subset] set &];\nvalues[dict_] := Map[#[[-1]] &, DownValues[dict]];\n\n\n(** Graph Specific Stuff *)\ngraphName = {\"Grid\", {3, 3}};\nverts = Range[GraphData[graphName, \"VertexCount\"]];\nedges = GraphData[graphName, \"EdgeIndices\"];\nvcoords = Thread[verts -> GraphData[graphName, \"VertexCoordinates\"]];\njedges = {{{1, 2, 4}, {2, 4, 5, 6}}, {{2, 3, 6}, {2, 4, 5, 6}}, {{4, \n     5, 6}, {2, 4, 5, 6}}, {{4, 5, 6}, {4, 5, 6, 8}}, {{4, 7, 8}, {4, \n     5, 6, 8}}, {{6, 8, 9}, {4, 5, 6, 8}}};\njnodes = Union[Flatten[jedges, 1]];\n\n\n(* Generate diagram with explicit PlotRange,ImageSize and \\\nAbsoluteThickness *)\nplotHL[verts_, color_] := (\n   coords = verts /. vcoords;\n   obj = JoinedCurve[Line[coords], \n     CurveClosed -> Not[collinear[coords]]];\n\n   (* Figure out PlotRange and ImageSize needed to respect scale *)\n\n    pr = getPlotRange[verts /. vcoords];\n   {{xmin, xmax}, {ymin, ymax}} = pr;\n   imsize = scale*{xmax - xmin, ymax - ymin};\n   lineForm = {Opacity[.3], color, JoinForm[\"Round\"], \n     CapForm[\"Round\"], AbsoluteThickness[scale*lineThickness]};\n   g = Graphics[{Directive[lineForm], obj}];\n   gg = GraphPlot[Rule @@@ inducedGraph[verts], \n     VertexCoordinateRules -> vcoords];\n   Show[g, gg, PlotRange -> pr, ImageSize -> imsize]\n   );\n\n(* Initialize all graph plot images *)\nSeedRandom[1]; colors = \n RandomChoice[ColorData[\"WebSafe\", \"ColorList\"], Length[jnodes]];\nClear[bags];\nMapThread[(bags[#1] = plotHL[#1, #2]) &, {jnodes, colors}];\n\n(** Ploting parent graph of subgraphs **)\n\n(* figure out coordinates of subgraphs close to edges of bounding \\\nbox, use them to anchor parent GraphPlot *)\n\nbagCentroid[bag_] := Mean[bag /. vcoords];\nfindExtremeBag[vec_] := (vertList = First /@ vcoords;\n   coordList = Last /@ vcoords;\n   extremePos = \n    First[Ordering[jnodes, 1, \n      bagCentroid[#1].vec > bagCentroid[#2].vec &]];\n   jnodes[[extremePos]]);\n\nextremeDirs = {{1, 1}, {1, -1}, {-1, 1}, {-1, -1}};\nextremeBags = findExtremeBag /@ extremeDirs;\nextremePoses = bagCentroid /@ extremeBags;\n\n(* figure out new plot range needed to contain all objects *)\n\nfullPR = getPlotRange[verts /. vcoords];\nfullIS = getImageSize[fullPR];\n\n(*** Show bags together merged ***)\nimage1 = \n Show[values[bags], PlotRange -> fullPR, ImageSize -> fullIS]\n\n(*** Show bags as vertices of another GraphPlot ***)\nGraphPlot[\n Rule @@@ jedges,\n EdgeRenderingFunction -> ({Gray, Thick, Arrowheads[.05], \n     Arrow[#1, 0.22]} &),\n VertexCoordinateRules -> \n  Thread[Thread[extremeBags -> extremePoses]],\n VertexRenderingFunction -> (Inset[bags[#2], #] &),\n PlotRange -> fullPR,\n ImageSize -> 3*fullIS\n ]\n\n(*** Show bags as 3d slides ***)\nmakeSlide[graphics_, pr_, h_] := (\n  Graphics3D[{\n    Texture[raster2texture[Rasterize[graphics, Background -> None]]],\n    EdgeForm[None],\n    Polygon[lift[h, pr2verts[pr]], \n     VertexTextureCoordinates -> pr2verts[{{0, 1}, {0, 1}}]]\n    }]\n  )\nyoffset = 1/2;\nslides = MapIndexed[\n   makeSlide[bags[#], getPlotRange[# /. vcoords], \n     yoffset*First[#2]] &, jnodes];\nShow[slides, ImageSize -> 3*fullIS]\n\n(*** Show 3d slides in orthographic projection ***)\nimage2 = \n Show[slides, ViewPoint -> {0, 0, Infinity}, ImageSize -> fullIS, \n  Boxed -> False]\n\n(*** Check that 3d and 2d images rasterize to identical resolution ***)\nDimensions[Rasterize[image1][[1, 1]]] == \n Dimensions[Rasterize[image2][[1, 1]]]\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How to update angular-plotly.js plot when data changes in angular 7\r\n                \r\nI am developing app in Angular 8 and displaying 'scatter' plot using angular-plotly.js. For data in x,y I am getting data from rest api. And I am able to plot graph using the data from rest api. On frontend I have a two dropdown from where user can select option and send those options value to backend and in response I get new data back to frontend. Till this point everything works fine, but the issue I am facing is that my plot is not getting updated with new data. Frontend still showing the initial plot not the new plot with different data value.\n\nMy Html code:\n\nHere is my two dropdown from where user select options and based on this options from backend through restapi I receive data points to plot my graph. And when user changes any option from dropdown and submit new choice to backend I again receive new data point and I need to remove my old plot and display new plot based on new dataset.\n\n```\n<form (ngSubmit)=\"segmentSelection()\" #ff=\"ngForm\">\n            <div id=\"userSelection\" ngModelGroup=\"userData\" #userData=\"ngModelGroup\">\n          <mat-form-field>\n            <mat-label>Choose Segment Key</mat-label>\n            <mat-select id=\"selectme\" ngModel name=\"segmentKey\">\n              <mat-option *ngFor=\"let segment of segKeys\" [value]=\"segment\">\n                {{segment}}\n              </mat-option>\n            </mat-select>\n          </mat-form-field>\n\n          <mat-form-field style=\"margin-left: 30px;\">\n            <mat-label>Choose Target Variable</mat-label>\n            <mat-select id=\"myselect\" ngModel name=\"target_variable\">\n              <mat-option *ngFor=\"let msg of mymessage\" [value]=\"msg\">\n                {{msg}}\n              </mat-option>\n            </mat-select>\n          </mat-form-field>\n          <button type=\"submit\" class=\"btn btn-success btn-color\" (click)=\"plotDiv()\" style=\"margin-left: 20px;\">Submit</button>\n        </div>\n        </form>\n\n<plotly-plot [data]=\"graph.data\" [layout]=\"graph.layout\"></plotly-plot>\n```\n\n\nMy component.ts code:\n\n```\nmyhistoricPlot = []\nmyhistoricDate = []\n\nsegmentSelection(){\n  this.plotselection.segmentKey = this.segmentData.value.userData.segmentKey;\n  this.plotselection.target_variable = this.segmentData.value.userData.target_variable;\n  console.log(this.segmentData);\n  fetch(\"http://localhost:5000/data-decomposition\", {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\"\n    },body: JSON.stringify({\n      segmentKey: this.segmentData.value.userData.segmentKey,\n      target_variable: this.segmentData.value.userData.target_variable\n    })\n  }).then(res => res.json()\n    // console.log(res);\n    // console.log('hello saheb')\n    // console.log(res.json())\n\n  ).then(myjson => {\n\n    myjson['Data'].forEach(element =>{\n      this.myhistoricPlot.push(element)\n    })\n    myjson['Date'].forEach(element =>{\n      this.myhistoricDate.push(element)\n    })\n    // console.log(this.myhistoricPlot)})\n    // this.myhistoricDate = myjson['Date'];\n    // this.myhistoricPlot = myjson['Data'];\n    console.log(this.myhistoricDate);\n    console.log(this.myhistoricPlot);\n    console.log(myjson)\n    this.myhistoricPlot = [];\n    this.myhistoricDate = [];\n    })\n}\n\npublic graph = {\n    data: [\n        { x: this.myhistoricDate, y: this.myhistoricPlot, type: 'scatter', mode: 'lines+points', marker: {color: 'black'} },\n\n    ],\n    layout: {width: 1200, height: 600, title: 'Historical Actuals Plot'}\n}; \n```\n\n\n****Note: myhistoricDate, myhistoricPlot are the two list with some numerical values, in which I am receiving data from restapi.\n\nThank you....your help on this issue would be highly appreciated\n    ", "Answer": "\r\nObject and arrays are bound by reference, so Angular won't detect a change to  ```\ngraph.data```\n until its reference is changed. I usually use a helper method to ensure that happens.\n```\nsegmentSelection() {\n  ...\n  .then(myjson => this.setData(myjson.Date, myjson.Data));\n}\n\nsetData(x, y) {\n  this.data[0].x = x;\n  this.data[1].y = y;\n  this.data = { ...this.data };\n}\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "SharePoint 2010 with PerformancePoint, Excel Services and Reporting Services References\r\n                \r\nI've recently been tasked with becoming the SharePoint 2010 expert.  However, to also include PerformancePoint Services (PPS), Excel Services and integration of Reporting Services(SSRS).  Key points regarding PPS is Decomposition Tree and Show Details.  The use of SSRS would be beyond the traditional Report Viewer but to, from what I understand, to return graphs which then are used in PPS.\n\nThe next item I needs to learn is Branding; specifically from two angles.  First, Branding a single site (site collection), for example an intranet.  The next would be an internet site where Branding is based upon the user login or by url.  An example of this might be each customer has it's own domain/sub-domain and each has it's own Branding.  However, all of which is hosted by the same SharePoint instance.\n\nMy question is, since I have to do this quickly, what would you recommend as must have references?  This could include How-To's, Tutorials, Blogs, Books, Video's, etc.  By the way, I love video's.  I'm a visual and hands on type person.\n\nNever the less, thanks for your thoughts!\nJeff\n    ", "Answer": "\r\nFor those who are also interested in learning SharePoint 2010 and PerformancePoint Services 2010, here are some helpful links:\n\nOnline\n\n\nSharePoint Training Index: http://office.microsoft.com/en-us/sharepoint-server-help/CL010257455.aspx\nSharePoint Training: http://office.microsoft.com/en-us/sharepoint-server-help/CH010372432.aspx\nSharePoint Lists: http://sharepoint.microsoft.com/Blogs/GetThePoint/Lists/Posts/Post.aspx?ID=383\nSharePoint Pro IT Training: http://technet.microsoft.com/en-us/sharepoint/ee410529\nSharePoint Advanced IT Training: http://technet.microsoft.com/en-us/sharepoint/ff678054.aspx\nSharePoint Business Intelligence Training: http://technet.microsoft.com/en-us/sharepoint/hh239866\nSharePoint Developer Training: http://msdn.microsoft.com/en-us/sharepoint/aa905692\nSharePoint Advanced Developer Training: http://technet.microsoft.com/en-us/sharepoint/ff420385.aspx\n\n\nBooks:\n\n\nMicrosoft SharePoint 2010, Unleashed\nSharePoint 2010 Administrator's Companion\nPro SharePoint 2010 Business Intelligence Solutions\nSharePoint 2010 Business Intelligence, 24-Hour Trainer\nSharePoint 2010 Development with Visual Studio 2010\nProfessional SharePoint 2010 Branding and User Interface Design\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Create an array of pointers to maps in stack memory, if possible\r\n                \r\nSo, as the title explains, I need to declare an array of pointers to maps.\n\nAn example:\n\n```\nmap<int,int>* mp[10] ; \nmp[0] = new map<int,int>() ;\nmp[0][0] = 21 ;\ncout<<mp[0][0]<<endl ;\n```\n\n\nGives the error:\n\n```\nmain.cpp: In function ‘int main()’:\nmain.cpp:15:16: error: no match for ‘operator=’ (operand types are ‘std::map’ and ‘int’)\n     mp[0][0] = 21 ;\n```\n\n\nOkay. So this means that ```\nmp[0][0]```\n is a map, and not the 0th map in ```\nmp```\n, for which we're putting the value of 0 as 21. So this means that ```\nmp[0]```\n is an array of maps. This means that the statement ```\nmap<int,int>* mp[10]```\n actually creates an array of arrays. \n\n\n  So, what is ```\nmp[0] = new map<int,int>()```\n doing? Is it not creating an object of the type ```\nmap<int,int>```\n in heap, and putting it in ```\nmp[0]```\n?\n\n\nThis is confusing me, and questioning my basics. How should I create an array of pointers? \n\nWhen I do:\n\n```\nmap<int,int>* mp = new map<int,int>() ;\nmp[0][21] = 99 ;\ncout<<mp[0][21]<<endl ;\n```\n\n\nIt works fine. How do I make a pointer to an object of ```\nmap```\n? And how do I create an array of these pointers to objects?\n\nAny help is appreciated.\n\n\n\nWhy this savagery? \n\nUse case: There's a graph o f say size ```\nn```\n. Now I need to merge maps for all the children of a node. Now there's a ```\nbigChild```\n which is the child, which has the subtree of maximum size. That is ```\nsubtree(bigChild)>=subtree(other children)```\n. \n\nThere's a HLD (heavy light decomposition) concept of merging smaller children's data into the data of the big child, and bam, you get the answer for the current node (the parent of these all children). \n\nThe data structure that saves the answers is a map. So, I have to create an array of maps of size ```\nn```\n. Now for the ```\nmp[node]```\n, I need to add the information of the smaller children, and merge them into the ```\nmp[bigChild]```\n. That is, we don't want to add the data of```\nmp[bigChild]```\n to some other map. Instead, we want to add all data into it, so that time complexity is O(n lg n).\n\nThat means we assign ```\nmp[bigChild]```\n to ```\nmp[bigChild]```\n. If I'd declared ```\nmap<int,int> mp[10]```\n. Then ```\nmp[node] = mp[bigChild]```\n would have copied all the members of ```\nmp[bigChild]```\n, which we do not want. So, instead, ```\nmp[node]```\n should point to ```\nmp[bigChild]```\n. And then we add the data of the smaller children to the this map.\n\nI hope this helps.\n    ", "Answer": "\r\n\n  So this means that mp[0][0] is a map and not the 0th map in mp\n\n\n```\nmp```\n does not contain maps. It contains pointers to maps.\n\n\n  So this means that mp[0] is an array of maps.\n\n\nIt does not mean that. ```\nmp[0]```\n is the pointer at index 0 of the array.\n\n\n  This means that the statement ```\nmap<int,int>* mp[10]```\n actually creates an array of arrays. \n\n\nThat declares an array of pointers.\n\n\n  So, what is mp[0] = new map() doing? Is it not creating an object of the type map in heap, and putting it in mp[0]?\n\n\nIt creates a dynamic object of type ```\nmap<int,int>```\n indeed. But that object is not \"put\" into ```\nmp[0]```\n. The object is in the free store. What is stored in ```\nmp[0]```\n is a pointer to the object.\n\n\n  How should I create an array of pointers? \n\n\nYou've already created an array of pointers successfully. What you've failed to do is indirect through the pointer within the array to access the map  - From another perspective, you've accidentally indirected through the pointer by using the subscript operator when you presumably attempted to use the subscript operator of the map:\n\n```\nmp[0][0] = 21 ;\n   ^  ^    ^\n   |  |    Assignment of value 21 into map; ill-formed\n   |  index 0 of pointer; type: map\n   index 0 of array; type: pointer\n```\n\n\nFollowing would be correct:\n\n```\nmp[0][0][0] = 21 ;\n   ^  ^  ^    ^\n   |  |  |    Assignment of integer; OK\n   |  |  The value mapped to key 0; type: int\n   |  index 0 of pointer; type: map\n   index 0 of array; type: pointer\n```\n\n\nHowever, it is somewhat confusing to access a singular object through a pointer using the subscript operation. Following is more conventional:\n\n```\n (*mp[0])[0] = 21 ;\n```\n\n\nTo understand the equivalence, remember that ```\na[b]```\n is same as ```\n*(a + b)```\n for pointers. Therefore ```\na[0]```\n is same as ```\n*a```\n.\n\n\n\nP.S. There is hardly ever a need to allocate a ```\nstd::map```\n dynamically. It would be simpler and more efficient to create an array of maps instead of an array of pointers to maps.\n\nP.P.S You leak the dynamic map. Avoid bare owning pointers (i.e. don't allocate dynamically unless you need to, and when you need to do so, use smart pointer).\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How Can I Define Only the Gradient for a Tensorflow Subgraph?\r\n                \r\nFirst: I am only a few days in with Tensorflow, so please bear with me.\n\nI started out from the cifar10 tutorial code and I am now using a combination of convolutions and eigenvalue decompositions that break the symbolic differentiation. I.e. the graph gets built, then upon calling ```\ntrain()```\n the script halts with \"No gradient defined for operation [...] (op type: SelfAdjointEig)\". No surprise there.\n\nThe inputs to the subgraph in question are still only the input feature maps and the filters being used, and I have the formulas for the gradients at hand and they should be straight-forward to implement given the inputs to the subgraph and the gradient with respect to its output.\n\nFrom what I can see in the docs, I can register a gradient method for custom Ops with ```\nRegisterGradient```\n or override them with the experimental ```\ngradient_override_map```\n.\nBoth of those should give me access to exactly the things I need. For example, searching on Github I find a lot of examples that access the op's inputs as ```\nop.input[0]```\n or such.\n\nThe problem I have is that I want to essentially \"shortcut\" a whole subgraph, not a single op, so I have no single op to decorate.\nSince this is happening in one of the convolutional layers of the cifar example I tried using the scope object for that layer. \nConceptually, what enters and exits that scope's graph is exactly what I want so if I could somehow override the whole scope's gradients that would \"already\" do it.\n\nI saw ```\ntf.Graph.create_op```\n which (I think) I could use to register a new type of operation and I could then override that Operation type's gradient computation with aforementioned methods. But I don't see a way of defining that op's forward pass without writing it in C++...\n\nMaybe I am approaching this the wrong way entirely?\nSince all of my forward or backward operations can be implemented with the python interface I obviously want to avoid implementing anything in C++.\n    ", "Answer": "\r\nHere's a trick from Sergey Ioffe:\n\nSuppose you want group of ops that behave as f(x) in forward mode, but as g(x) in the backward mode. You implement it as\n\n```\nt = g(x)\ny = t + tf.stop_gradient(f(x) - t)\n```\n\n\nSo in your case your g(x) could be an identity op, with a custom gradient using ```\ngradient_override_map```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Dimension Reduction Using varimax rotation and method used is Principal . to find component score coefficient matrix?\r\n                \r\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn import decomposition\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom factor_analyzer import FactorAnalyzer\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_excel('excel path')\ndf.drop(['DISTRICT'],axis=1,inplace=True)\nnumCOl = df\nnumCorr = numCOl.corr()\nprint(numCorr.round(3))\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n\nchi_square_value,p_value=calculate_bartlett_sphericity(df)\nchi_square_value, p_value\n\n#performing kaiser mayer test (KO test)\nfrom factor_analyzer.factor_analyzer import calculate_kmo\nkmo_all,kmo_model = calculate_kmo(df)\nkmo_model\n\nfa = FactorAnalyzer()\nfa.fit(df)\n\nev,v = fa.get_eigenvalues()\nprint(ev)\n\n# ploting the graph\nplt.scatter(range(1,df.shape[1]+1),ev)\nplt.plot(range(1,df.shape[1]+1),ev)\nplt.title('Screen Plot')\nplt.xlabel('Number of Factors')\nplt.ylabel('Eigenvalue')\nplt.grid()\nplt.show()\n\nfa = FactorAnalyzer(n_factors=4,method='principal',rotation='varimax')\nfa.fit(df)\ndata_1 = pd.DataFrame(fa.loadings_,index=df.columns)\n\ndata_1.round(3)\n```\n\nhere all my code which gives me rotated component matrix but i'm looking for component score coefficient matrix??? Dimension Reduction Using varimax rotation and method used is Principal . to find component score coefficient matrix???\n    ", "Answer": "\r\nI got the solution of this --\n\nAfter above steps find cumalities\n\n```\nPrint(cumalities.round(3))\n```\n\n\nComponent score Coefficent Matrix\n\n```\npcScore = np.linalg.inv(numCorr).dot(data_1)\nNew_Data = pd.DataFrame(pcScore,index=df.columns)\nPrint(New_Data.round(3))\n```\n\nenter image description here\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How should I start with learning math required for AI [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is off-topic. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\r\n                \r\n                    \r\n                        Closed 10 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI have studied mathematics, but that was long time ago. I have been a programmer for 8 years but when I started to study concepts in AI and data mining I find it very difficult to understand the theory.\n\nNow I have wasted 2-3 years and I have got nothing.  I need to first understand the math concepts required to learn AI and data mining. \n\nI don't know where to start.  Which books and tutorials do you recommend I should start with from the AI point of view.\n\nHow should I go about obtaining the fundamental requirements to use AI and Data Mining concepts.\n\nEDIT:\nI got this list from internet\n\nMatrix algebra: most machine learning models are represented as matrices and vectors. Concepts like eigenvectors and singular value decomposition appear all over the place.\n\nBayesian statistics: probability, Bayes' rule, common distributions (e.g., beta, Dirichlet, Gaussian), etc.\n\nMultivariable calculus: most learning techniques use gradients and Hessians at their core to fit parameters. (If you want to get fancier, study numerical optimization.)\n\nInformation theory: entropy, KL divergence, etc. Just the basics here.\n\nIn limited cases, higher-level math can be useful. E.g., to understand manifold learning, you'll want to know some basic notions from geometry and topology. Occasionally abstract algebra is used (e.g., see \"expectation semirings\" for learning on hyper-graphs). I would learn these as-needed, but if you have a chance to learn them early it can't hurt.\n\nCan anyone recommend some books on those\n    ", "Answer": "\r\nMy resource for studying math : http://www.khanacademy.org/\n\nYou will be able to find A LOT on all math fields.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Looking for Objective Metrics for Software Quality [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nThere are various types of quality that can be measured in software products, e.g. fitness for purpose (e.g. end use), maintainability, efficiency. Some of these are somewhat subjective or domain specific (e.g. good GUI design principles may be different across cultures or dependent on usage context, think military versus consumer usage).\n\nWhat I'm interested in is a deeper form of quality related to the network (or graph) of types and their inter-relatedness, that is, what types does each type refer to, are there clearly identifiable clusters of interconnectivity relating to a properly tiered architecture, or conversely is there a big 'ball' of type references ('monolithic' code). Also the size of each type and/or method (e.g. measured in quantity of Java byte code or .Net IL) should give some indication of where large complex algorithms have been implemented as monolithic blocks of code instead of being decomposed into more manageable/maintainable chunks.\n\nAn analyis based on such ideas may be able to calculate metrics that are at least a proxy for quality. The exact threshold/decision points between high and low quality would I suspect be subjective, e.g. since by maintainability we mean maintainability by human programmers and thus the functional decomposition must be compatible with how human minds work. As such I wonder if there can ever be a mathematically pure definition of software quality that transcends all possible software in all possible scenarios.\n\nI also wonder if this a dangerous idea, that if objective proxies for quality become popular that business pressures will cause developers to pursue these metrics at the expense of the overall quality (those aspects of quality not measured by the proxies).\n\nADDENDUM: Another way of thinking about quality is from the point of view of entropy. Entropy is the tendency of systems to revert from ordered to disordered states. Anyone that has ever worked on a real world, medium to large scale software project will appreciate the degree to which quality of the code base tends to degrade over time. Business pressures generally result in changes that focus on new functionality (except where quality itself is the principle selling point, e.g. in avionics software), and the eroding of quality through regression issues and 'shoe-horning' functionaility where it does not fit well from a quality and maintenance perspective. So, can we measure the entropy of software? And if so, how?\n    ", "Answer": "\r\nNDepend, at least for .NET code, will provide the best metrics for software quality that we have to date.  They have 82 different code metrics. Is this what you are looking for?  If you are a .NET programmer, you may find this blog post about NDepend analysis of a very popular/large open source project to be interesting.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Generating a distance-grid around given points\r\n                \r\nI'm dealing with a (a) set of points P in Euclidean space (let's assume the plane to keep things simple), (b) a bounding rectangle [a,b] x [c,d] containing all the points in P, along with (c) a vector (m,n) of positive integers encoding subdivisions along each dimension. The basic question is\n\n\n  How does one efficiently build an m x n grid of distances to P?\n\n\nIn particular, I'd like to generate a cubical approximation to the distance function from P in the obvious m x n grid decomposition of the rectangle [a,b] x [c,d]: chop the X-direction into m pieces and the Y direction into n pieces, and for each little rectangle R which results from the partition, compute a positive integer distance d(R) to P defined as follows. \n\nConsider a weighted graph G whose vertex set V corresponds to little rectangles in our grid with an edge of weight $1$ between neighbors (even diagonal neighbors). Call a vertex \"red\" if the corresponding rectangle contains a point of P. What I want to compute is the function d on V taking positive integer values which associates to each vertex the shortest distance in G to a red vertex. So, if a little rectangle actually contains a point from P, then its associated vertex gets assigned \"0\". If it does not contain a point of P but is adjacent (even diagonally) to a rectangle which does, then it gets \"1\" and so on...\n\nThe naive approach of considering the distance of each point from each little rectangle and keeping track of the minimum incurs a cost of |P|mn which appears prohibitive for large grids. So here's the second approach that I considered: set each d(R) to some large MAX to each R in the grid which does NOT contain a point from P. Then, set d(R) = 0 for each R that does contain a point of P and throw all its neighbors with strictly larger d-values into a Queue. Then, iterate this until the queue is empty: \n\n\npop a grid rectangle R from the queue,\nif  d(R) > 1 + d(R') where R' is the neighbor whose processing enqueued R, then set d(R) = 1 + d(R') and enqueue all neighbors of R' with d-values exceeding 1+d(R).\n\n\nI'd appreciate any ideas on how one solves this problem more efficiently than this \"second approach\".\n    ", "Answer": "\r\nYou are talking about the topological distance in a binary image. The rectangles can be seen as pixels and those containing points of P define distance 0. Distance 1 is for the pixels immediately adjacent (by an edge or a corner, 8-connexity), distance 2 for the pixels immediately adjacent to the latter and so on.\n\nComputing the distance map with a queue is a good idea. Initialize the queue by pushing all 8-neighbors of the level0 pixels. Then pop them, assigning level1, and push the unassigned 8-neighbors... (like what you describe).\n\nThis is linear process (every pixel is handled a constant number of times), so that it is optimal, as the level of every pixel needs to be computed. You can't do much better.\n\nI guess that you can also fill the distance map by a two-passes scanline process.\n\n1) process top-left to down-right, considering the 4 top and left 8-neighbors of every pixel in turn. (Take the smallest level among the neighbors and add 1).\n\n2) then process bottom-right to top-left, considering the 4 bottom and right 8-neighbors.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Build clustering upon matrix lines with Kmeans\r\n                \r\nBIG EDIT\n\nThe original code was:\n\nThe the plotting of a graph that corresponds to the reading of a text file with n lines. Each line contains 4 columns,the first three columns are coordinates of (x,y,z) points, and the fourth column is a binary variable not necessary for this plotting. At each 20 lines read, a skeleton is read, this skeleton being a group of 20 (x,y,z) points or joints, each joint made by the first three columns of each line.\n\nExample of a text file content: A text file contains 860 lines, and 860/20 = 43, being 20 the number of joints to create a skeleton of (x,y,z) joints. Then, the text file is made of 43 skeletons, that generates a movement. Therefore, the text file represents a movement. I've called it \"example\" because the numbers vary. \n\nAfter building the code to read the skeleton's movements, I've made a big 2D array that contains all the movements together, and the result was a 22797x400 array, where each line is a skeleton. Therefore, there are 22797 skeletons, with 400 columns for each. I've called this last 2D array of final_array.\n\nI've applied the Singular Value Decomposition (SVD) to final_array, where I've used the V matrix from SVD (that results in S, V and D matrices) to make a multiplication between final_array and a reduced version of V (which is originally 400x400), resulting in a 22797x3 2D array, since the reduced version of V was 400x3. This was necessary for some reasons that don't need to be mentioned here, but it was for dimension reduction to plot the skeletons in upcoming parts of the process. \n\nHence, I have a 22797x3 2D array, where each line represents a skeleton, built from operations explained above, and I need to apply clustering to this matrix, where each line will be clustered to a group, using Kmeans from Scikit-learn in Python. It must be a cluster with 100 clustering groups.\n\nWhat I need to have as  result is the kmeans_labels result, with a list of 22797 elements, informing was group of the 100 clustering groups each line (skeleton) was grouped at.\n\nSo far I've tried:\n\n```\nkmeans = KMeans(n_clusters=100, random_state=0).fit(matrix)\n```\n\n\nBut the result was the following error message:\n\nNumber of distinct clusters (68) found smaller than n_clusters (100). Possibly due to duplicate points in X.\n  return_n_iter=True)\n\nIt doesn't matter how many times I change the groups number, the error message returns with a smaller value.\nAny hep?\n    ", "Answer": "\r\nThis error means that your data matrix is mostly composed of repeated vectors.\nSo from your 22797 data points, there are only 68 different vectors and the rest are just repetitions of these 68 values.\nTry printing the matrix. I believe you either you are not reading the data as you should, or you are not measuring them the right way \n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Graphing Gaussian Process in R using rstan\r\n                \r\nI am trying to understand where I am going wrong with ```\nrstan```\n. I have figured out a workaround, but it seems like there should be a better option for graphing draws from the posterior than what I have come up with. \n\nI am trying to learn how to use ```\nrstan```\n for modeling a Gaussian process related to another question I have open on CV (shameless plug but if you have ideas that can help out there I am all ears). \n\nI figured as a first step I would try going through the stan documentation examples of a Gaussian process. So I built a model simply designed to draw random squared exponential covariance functions. \n\n```\nlibrary(rstan)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(ggplot2)\noptions(mc.cores=parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nx<-seq(0, 30, by=.01)\n\nmodel<-'\ndata{\n    int<lower=1> N;\n    real x[N];\n  }\n\ntransformed data {\n  matrix[N, N] L;\n  matrix[N, N] K;\n  vector[N] mu = rep_vector(0, N);\n  for (i in 1:(N - 1)) {\n    K[i, i] = 1 + 0.1;\n    for (j in (i + 1):N) {\n      K[i, j] = exp(-0.5 * square(x[i] - x[j]));\n      K[j, i] = K[i, j];\n    }\n  }\n  K[N, N] = 1 + 0.1;\n  L = cholesky_decompose(K);\n}\n\nparameters {\n  vector[N] eta;\n}\n\nmodel {\n  eta ~ normal(0, 1);\n}\ngenerated quantities {\n  vector[N] y;\n  y = mu + L*eta;\n}\n'\n```\n\n\nI followed the documentation's suggestion of including a Cholesky decomposition on transformed data. \n\nUsing ```\nstan```\n I fit the model as follows: \n\n```\ndat<-list(N=length(x),\n          x=x)\n\nfit <- stan(model_code = model,\n            data = dat, \n            iter = 1000, \n            chains = 1, \n            pars = c('y', 'eta'),\n            control = list(adapt_delta=.99, \n                           max_treedepth=10)\n            )\n```\n\n\nI can visualize the posterior distributions of each of my draws using the following code: \n\n```\nposterior<-as.matrix(fit)\nmcmc_areas(posterior, \n           pars=c('y[1]', 'y[2]'),\n           prob = .90\n           )\n```\n\n\nWhich produces: \n\n\n\nI really want to look at the results of each process (not all 500 but some random draws thereof). \n\nI tried multiple alternative strategies and eventually landed on the following: \n\n```\npost.y<-extract(fit, pars='y')\n\ndraws<-sample(1:500, size = 10)\n\nDF<-data.frame(Time=x, y=colMeans(post.y$y), Draw=rep('Mu', length(x)))\nfor(i in 1:length(draws)){\n  DF.temp<-data.frame(Time=x, y=post.y$y[i,], Draw=rep(paste0('posterior', i), length(x)))\n  DF<-rbind(DF, DF.temp)\n}\n\ng1<-ggplot(aes(x=Time, y=y), data=DF)\ng2<-g1+geom_line(aes(x=Time, y=y, group=Draw, color=Draw), data=DF[DF$Draw!='Mu',], alpha=.25, show.legend = F)\ng3<-g2+geom_line(aes(x=Time, y=y), data=DF[DF$Draw=='Mu',], lwd=1.5)\ng3\n```\n\n\nAnd this code produces:\n\n\nThis seems like a lot of extra hoops to jump through. I tried alternative approaches using other functions in the ```\nrstan```\n family (e.g., ```\nppc_dens_overlay```\n), but they all resulted in errors or did not return what I wanted. \n\nSo my question here is really about alternative, simpler options I can use to visualize the overall average of my draws for each value of $y_i$ as well as the overall mean of all draws for each value (which should be 0 in this case but may not in other cases when data changes over time in a structure way). \n\nI am relatively new to ```\nrstan```\n (have used ```\nrbugs```\n and ```\nrjags```\n) so I may be simply unaware of some simple set of functions that can make this process easier. \n\nThanks in advance for any help. \n    ", "Answer": "\r\nYou could reproduce your second figure with a bit less code using ```\nmatplot```\n, which conveniently works with matrix data.\n\n```\npost.y <- rstan::extract(fit, 'y')$y\npost.y.sub <- post.y[sample(1:nrow(post.y), 10),]\n\nmatplot(x, t(post.y.sub), type = 'l', lty = 1, col = adjustcolor(palette(), 0.25))\nlines(colMeans(post.y) ~ x, lwd = 2)\n```\n\n\nIf you prefer ggplot2, the hard part is getting the posterior samples into a data frame. I find the ```\ndplyr```\n and ```\ntidyr```\n libraries helpful here. It looks like a lot of code, but it's flexible when your models get more complicated.\n\n```\nlibrary(dplyr)\nlibrary(tidyr)\n\ndf.rep <- post.y %>% \n  t() %>%\n  as.data.frame() %>% \n  mutate(x = x) %>% \n  gather(rep, post.y, -x)\n\ndf.mean <- df.rep %>% \n  group_by(x) %>% \n  summarize(mu = mean(post.y))\n\ndf.rep.sub <- df.rep %>% \n  filter(rep %in% sample(unique(rep), 10))\n\nggplot() +\n  geom_line(data = df.rep.sub, aes(x, post.y, col = rep), alpha = 0.25, show.legend = F) +\n  geom_line(data = df.mean, aes(x, mu), lwd = 1.5)\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Why are my curved edges not updating correctly?\r\n                \r\nI'm trying to customize a layout of JGraph. I want to create curved edges, but I have a problem. Every time I create a group of vertex on JGraph, and I am firing an event to update this graph, the edge is missing points compared to the previous state. Here is a example:\n\n\n\nCan someone help me?\n\nHere is my code:\n\n```\n    class CurveGraphView extends mxGraphView {\n\n        public CurveGraphView(mxGraph graph) {\n            super(graph);\n        }\n\n        /* Only override this if you want the label to automatically position itself on the control point */\n        @Override\n        public mxPoint getPoint(mxCellState state, mxGeometry geometry) {\n            double x = state.getCenterX();\n            double y = state.getCenterY();\n\n            if (state.getAbsolutePointCount() == 3) {\n                mxPoint mid = state.getAbsolutePoint(1);\n                x = mid.getX();\n                y = mid.getY();\n            }\n            return new mxPoint(x, y);\n        }\n    //    /* Makes sure that the full path of the curve is included in the bounding box */ \n\n        @Override\n        public mxRectangle updateBoundingBox(mxCellState state) {\n\n            List<mxPoint> points = state.getAbsolutePoints();\n            mxRectangle bounds = super.updateBoundingBox(state);\n\n            Object style = state.getStyle().get(\"edgeStyle\");\n            if (CurvedEdgeStyle.KEY.equals(style) && points != null && points.size() == 3) {\n                Rectangle pathBounds = CurvedShape.createPath(state.getAbsolutePoints()).getBounds();\n                Rectangle union = bounds.getRectangle().union(pathBounds);\n                bounds = new mxRectangle(union);\n                state.setBoundingBox(bounds);\n            }\n            return bounds;\n        }\n\n}\n\nclass CurvedEdgeStyle implements mxEdgeStyle.mxEdgeStyleFunction {\n\npublic static final String KEY = \"curvedEdgeStyle\";\n\n@Override\npublic void apply(mxCellState state, mxCellState source, mxCellState target, List<mxPoint> points, List<mxPoint> result) {\n    mxPoint pt = (points != null && points.size() > 0) ? points.get(0) : null;\n    if (source != null && target != null) {\n        double x = 0;\n        double y = 0;\n        if (pt != null) {                \n            result.add(pt);\n        } else {\n            x = (target.getCenterX() + source.getCenterX()) / 2;\n            y = (target.getCenterY() + source.getCenterY()) / 2;\n            mxPoint point = new mxPoint(x, y);\n            result.add(point);\n        }\n    }\n}\n\n}\n\n\n\nclass CurvedShape extends mxConnectorShape {\n\n    public static final String KEY = \"curvedEdge\";\n    private GeneralPath path;\n\n    @Override\n    public void paintShape(mxGraphics2DCanvas canvas, mxCellState state) {\n\n        List<mxPoint> abs = state.getAbsolutePoints();\n        int n = state.getAbsolutePointCount();\n        mxCell aux = (mxCell)state.getCell();\n        if (n < 3) {\n            super.paintShape(canvas, state);\n        } else if (configureGraphics(canvas, state, false)) {\n            Graphics2D g = canvas.getGraphics();\n            path = createPath(abs);\n            g.draw(path);\n            paintMarker(canvas, state, false);\n            paintMarker(canvas, state, true);\n        }\n    }\n\n    /* Code borrowed from here: http://www.codeproject.com/Articles/31859/Draw-a-Smooth-Curve-through-a-Set-of-2D-Points-wit */\n    public static GeneralPath createPath(List<mxPoint> abs) {\n        mxPoint[] knots = abs.toArray(new mxPoint[abs.size()]);\n\n        int n = knots.length - 1;\n        mxPoint[] firstControlPoints = new mxPoint[n];\n        mxPoint[] secondControlPoints = new mxPoint[n];    // Calculate first Bezier control points    // Right hand side vector    \n        double[] rhs = new double[n];    // Set right hand side X values    \n        for (int i = 1; i < n - 1; ++i) {\n            rhs[i] = 4 * knots[i].getX() + 2 * knots[i + 1].getX();\n        }\n        rhs[0] = knots[0].getX() + 2 * knots[1].getX();\n        rhs[n - 1] = (8 * knots[n - 1].getX() + knots[n].getX()) / 2.0;    // Get first control points X-values    \n        double[] x = getFirstControlPoints(rhs);    // Set right hand side Y values    \n        for (int i = 1; i < n - 1; ++i) {\n            rhs[i] = 4 * knots[i].getY() + 2 * knots[i + 1].getY();\n        }\n        rhs[0] = knots[0].getY() + 2 * knots[1].getY();\n        rhs[n - 1] = (8 * knots[n - 1].getY() + knots[n].getY()) / 2.0;    // Get first control points Y-values    \n        double[] y = getFirstControlPoints(rhs);    // Fill output arrays.    \n        for (int i = 0; i < n; ++i) {        // First control point        \n            firstControlPoints[i] = new mxPoint(x[i], y[i]);        // Second control point        \n            if (i < n - 1) {\n                secondControlPoints[i] = new mxPoint(2 * knots[i + 1].getX() - x[i + 1], 2 * knots[i + 1].getY() - y[i + 1]);\n            } else {\n                secondControlPoints[i] = new mxPoint((knots[n].getX() + x[n - 1]) / 2, (knots[n].getY() + y[n - 1]) / 2);\n            }\n        }\n        GeneralPath path = new GeneralPath();\n        path.moveTo(knots[0].getX(), knots[0].getY());\n        for (int i = 1; i < n + 1; i++) {\n            path.curveTo(firstControlPoints[i - 1].getX(), firstControlPoints[i - 1].getY(), secondControlPoints[i - 1].getX(), secondControlPoints[i - 1].getY(), knots[i].getX(), knots[i].getY());\n        }\n        return path;\n    }/// <summary>/// Solves a tridiagonal system for one of coordinates (x or y)/// of first Bezier control points./// </summary>/// <param name=\"rhs\">Right hand side vector.</param>/// <returns>Solution vector.</returns>\n\n    private static double[] getFirstControlPoints(double[] rhs) {\n        int n = rhs.length;\n        double[] x = new double[n]; // Solution vector.    \n        double[] tmp = new double[n]; // Temp workspace.    \n        double b = 2.0;\n        x[0] = rhs[0] / b;\n        for (int i = 1; i < n; i++) // Decomposition and forward substitution.    \n        {\n            tmp[i] = 1 / b;\n            b = (i < n - 1 ? 4.0 : 3.5) - tmp[i];\n            x[i] = (rhs[i] - x[i - 1]) / b;\n        }\n        for (int i = 1; i < n; i++) {\n            x[n - i - 1] -= tmp[n - i] * x[n - i]; // Backsubstitution.    \n        }\n        return x;\n    }\n\n    @Override\n    protected mxLine getMarkerVector(List<mxPoint> points, boolean source, double markerSize) {\n        if (path == null || points.size() < 3) {\n            return super.getMarkerVector(points, source, markerSize);\n        }\n        double coords[] = new double[6];\n        double x0 = 0;\n        double y0 = 0;\n        double x1 = 0;\n        double y1 = 0;\n        PathIterator p = path.getPathIterator(null, 2.0);\n        if (source) {\n            p.currentSegment(coords);\n            x1 = coords[0];\n            y1 = coords[1];\n            p.next();\n            p.currentSegment(coords);\n            x0 = coords[0];\n            y0 = coords[1];\n        } else {\n            while (!p.isDone()) {\n                p.currentSegment(coords);\n                x0 = x1;\n                y0 = y1;\n                x1 = coords[0];\n                y1 = coords[1];\n                p.next();\n            }\n        }\n        return new mxLine(x0, y0, new mxPoint(x1, y1));\n    }\n\n}\n```\n\n\nextends mxGraph and Override the constructor adding:\n\n```\nmxGraphics2DCanvas.putShape(CurvedShape.KEY, new CurvedShape());\nmxStyleRegistry.putValue(CurvedEdgeStyle.KEY, new CurvedEdgeStyle()); \ngetStylesheet().getDefaultEdgeStyle().put(mxConstants.STYLE_SHAPE, CurvedShape.KEY);\ngetStylesheet().getDefaultEdgeStyle().put(mxConstants.STYLE_EDGE, CurvedEdgeStyle.KEY);\n```\n\n\nand Override createGraphView() returning CurveGraphView.\n\nHave told me it has to do something with absolute points.\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Including multiple seasonal terms in Python statsmodels.tsa ARIMA\r\n                \r\nI am trying to model a time series in python using python 2.7.11 and the excellent statsmodels.tsa package. My data consists of hourly measurements of traffic intensity over several weeks. Thus, the data has multiple seasonal components, days form a 24 hour period; weeks form a 168 hour period. \n\nAt this point, the modeling options in statsmodels.tsa are not set up to handle multiple seasonality, as they only allow for the specification of one seasonal factor. However, I came across the work of Rob Hyneman on multiple seasonality in R. He advocates modeling seasonal components of a time series using Fourier series, including a Fourier series in the model for the frequencies corresponding to each of seasonal periods.\n\nI've used Welch's method to obtain the power spectral density of the signal in my observed time series, extracted the peaks in the signal which correspond to the frequencies at which I expect my seasonal effects, and used the frequency and amplitude to generate a sine wave pattern corresponding to the seasonal trends I expect in my data. As an aside, I think this allows me to bypass Hyneman's step of selecting the value of k based on the AIC, because I am using the signal inherent in the observed data.\n\nTo ensure that the sine waves match the occurrence of the seasonal pattern in the data, I match the peak of both sine wave patterns to the peaks in the observed data by visually selecting a peak within one of the 24-hour periods, and matching the hour of its occurrence to the highest value of the variable representing the sine wave. Prior to this, I have checked that the daily peaks occur at the same hour consistently.  \n\nSo far, so good it seems - plots of the sine waves constructed with the obtained frequencies and amplitudes roughly correspond to the observed data. I then fit an ARIMA(2,0,0) model, including both of the decomposition-based variables as exogenous variables. At this point, I want to test the predictive utility of the model. However, this is where things get complicated.\n\nWhen I am using ARIMA from the statsmodels package, the estimates I get from fitting the model form a pattern which replicates the sine waves, but with a range of values matching my observation. There is still a lot of variance in the observations which is not explained by the seasonal trends, leading me to believe that somewhere in the model fitting procedure something is not going the way it is supposed to.\n\nUnfortunately, I am not sufficiently well-versed in the art of time series modeling to know if my unexpected results are due to the nature of exogenous variables I am including, statsmodels functionality that I should be using, but am omitting, or wrongful assumptions about the concept of seasonal trends. \n\nSome concrete questions I have are:\n\n\nis it possible to include multiple seasonal trends (i.e. Fourier- or decomposition-based) in an ARIMA model using statsmodels in python?\ncould reconstruction of the seasonal trend using sine waves cause difficulties when the sine waves are included as exogenous variables in the model as specified above and in the code below?\nwhy does the model specified int he code below not yield predictions which match the observed data more closely?\n\n\nAny help is much appreciated!\n\nBest wishes, and thanks in advance,\n\nEvert\n\np.s.: Sorry if my code sample and data file are overly long - as I am not sure what causes the unexpected results I thought I'd post the whole thing. Also, apologies for not following PEP8 at times - I'm still learning :)\n\n\n\nCode sample:\n\n```\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy.signal import welch\nimport operator\n\n\n# Function which plots rolling mean of data set in order to estimate stationarity\n# 'timeseries' = Data to be used for ARIMA modeling\n#\n\n\ndef plotmean(timeseries, show=0, path=''):\n    rolmean = pd.rolling_mean(timeseries, window=12)\n    rolstd = pd.rolling_std(timeseries, window=12)\n    fig = plt.figure(figsize=(12, 8))\n    orig = plt.plot(timeseries, color='blue', label='Observed scores')\n    mean = plt.plot(rolmean, color='red', label='Rolling mean')\n    std = plt.plot(rolstd, color='black', label='Rolling SD')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    if show != 0:\n        plt.show()\n    if path != '':\n        plt.savefig(path, format='png', bbox_inches='tight')\n    plt.clf()\n\n\n#\n# Function to decompose a function over time f(t) into a spectrum of signal amplitude and frequency\n# 'dta' = The dataset used\n# 'show' = Whether or not to show plot\n# 'path' = Where to store plot, if desirable\n#\n# Output:\n# frequency range and spectral density range\n#\n\n\ndef runwelch(dta, show, path):\n    nps = (len(dta) / 2) + 8\n    nov = nps / 2\n    fft = nps\n    fs_temp = .0002778\n    # Set to 1/3600 because of hourly sampling\n    f, Pxx_den = welch(dta, fs=fs_temp, nperseg=nps, noverlap=nov, nfft=fft, scaling=\"spectrum\")\n    plt.plot(f, Pxx_den)\n    plt.ylim([0.5e-7, 10])\n    plt.xlabel('frequency [Hz]')\n    plt.ylabel('PSD [V**2/Hz]')\n    if show != 0:\n        plt.show()\n    if path != '':\n        plt.savefig(path, format='png', bbox_inches='tight')\n    plt.clf()\n    return f, Pxx_den\n\n\n#\n# Function which gets amplitude and frequency of n most important periodical cycles, and provides plot\n# to visually inspect if they correspond to expected seasonal components.\n# 'freq' = output of Welch decomposition\n# 'density' = output of Welch decomposition\n# 'n' = desired number of peaks to extract\n# 'show' = whether to show plots of corresponding sine functions\n\n\ndef getsines(n_obs, freq, density, n, show):\n    ftemp = freq\n    dtemp = density\n    fstore = []\n    dstore = []\n    astore = []\n    fs_temp = .0002778\n    # Set to 1/3600 because of hourly sampling\n    samplespace = n_obs * 3600\n    for a in range(0, n, 1):\n        max_index, max_value = max(enumerate(dtemp), key=operator.itemgetter(1))\n        dstore.append(max_value)\n        fstore.append(ftemp[max_index])\n        astore.append(np.sqrt(max_value))\n        dtemp[max_index] = 0\n    if show == 1:\n        for b in range(0, len(fstore), 1):\n            sound_sine = sine(fstore[b], samplespace, fs_temp, astore[b], 1)\n            plt.plot(sound_sine)\n            plt.show()\n            plt.clf()\n    return fstore, astore\n\n\ndef sine(freq, time_interval, rate, amp):\n    w = 2. * np.pi * freq\n    t = np.linspace(0, time_interval, time_interval * rate)\n    y = amp * np.sin(w * t)\n    return y\n\n\n#\n# Function which adapts the calculated sine waves for the returned sines for k = 1 through k = kmax\n# 'dta' = Data set\n\n\ndef buildFterms(dta, fstore, astore):\n    n = len(fstore)\n    n_obs = len(dta)\n    fs_temp = .0002778\n    # Set to 1/3600 because of hourly sampling\n    samplespace = n_obs * 3600 + (24 * 3600)\n    # Add one excess day for later fitting of sine waves to peaks\n    store = []\n    for i in range(0, n, 1):\n        tmp = sine(fstore[i], samplespace, 0.0002778, astore[i])\n        store.append(tmp)\n    k_168_store = store[0]\n    k_24_store = store[1]\n    k_24 = np.transpose(k_24_store)\n    k_168 = np.transpose(k_168_store)\n    k_24 = pd.Series(k_24)\n    k_168 = pd.Series(k_168)\n    dta_ind, dta_val = max(enumerate(dta.iloc[120:143]), key=operator.itemgetter(1))\n    # Visually inspect mean plot, select interval which has clear and representative peak, use to determine index.\n    k_24_ind, k_24_val = max(enumerate(k_24.iloc[0:23]), key=operator.itemgetter(1))\n    # peak in sound level at index 1 is matched by peak in sine wave at index 7. Thus, sound level[0] corresponds to\\\n    # sine waves[6]\n    # print dta_ind, dta_val, k_24_ind, k_24_val\n    k_24_sel = k_24[6:1014]\n    k_168_sel = k_168[6:1014]\n    exog = pd.concat([k_24_sel, k_168_sel], axis=1)\n    return exog\n\n\n#\n# Function which takes data, makes a plot of the ACF and PACF, and saves the plot, if needed\n# 'x' = Time series data, time indexed, over which to plot the ACF and PACF.\n# 'show' = Whether or not to show the resulting plot (0 = don't show [default], 1 = show)\n# 'path' = A full file path specification indicating whether or not the file should be saved (default = 0, don't save)\n# Use output plot to visually interpret necessary parameters p, d, q, and seasonal component for SARIMAX procedure\n#\n\n\ndef plotpacf(x, show=0, path=''):\n    dflength = len(x)\n    nlags = dflength * .80\n    fig = plt.figure(figsize=(12, 8))\n    ax1 = fig.add_subplot(211)\n    fig = sm.graphics.tsa.plot_acf(x.squeeze(), lags=nlags, ax=ax1)\n    ax2 = fig.add_subplot(212)\n    fig = sm.graphics.tsa.plot_pacf(x, lags=nlags, ax=ax2)\n    if show != 0:\n        plt.show()\n    if path != '':\n        plt.savefig(path, format='png', bbox_inches='tight')\n    plt.clf()\n\n\n#\n# Function to calculate the Dickey-Fuller test of stationarity\n# 'dta' = Time series data, time indexed, over which to test for stationarity using the Dickey-Fuller test.\n#\n\ndef dftest(dta):\n    print 'Results of Dickey-Fuller Test:'\n    dftest = sm.tsa.stattools.adfuller(dta, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n    for key, value in dftest[4].items():\n        dfoutput['Critical Value (%s)' % key] = value\n    if dfoutput[0] < dfoutput[4]:\n        dfoutput['Stationary'] = 'True'\n    else:\n        dfoutput['Stationary'] = 'False'\n    print dfoutput\n\n\n#\n# Function to difference the time series, in order to determine optimal value of d for ACF and PACF\n# 'dta' = Data, time series indexed, to be differenced\n# 'd' = Order of differencing to be applied\n# 'show' = Whether or not to show the resulting plot (0 = don't show [default], 1 = show)\n# 'path' = A full file path specification indicating whether or not the file should be saved (default = 0, don't save)\n#\n\n\ndef diffit(dta, d, show, path=''):\n    templist = []\n    for i in range(0, (len(dta) - d), 1):\n        tempval = dta[i] - dta[i + d]\n        templist.append(tempval)\n    y = templist[d:len(templist)]\n    y = pd.Series(y)\n    plotpacf(y, show, path)\n    return y\n\n\n#\n# Function to fit the ARIMA model based on parameters obtained from the ACF / PACF plot\n# 'dta' = Time series data, time indexed, over which to fit a SARIMAX model.\n# 'exog' = Exogenous variables used in ARIMA model\n# 'p' = Number of AutoRegressive lags, initially based on the cutoff point of the ACF\n# 'd' = Order of differencing based on visual examination of ACF and PACF plots\n# 'q' = Number of Moving Average lags, initially based on the utoff point of the PACF\n# 'show' = Whether or not to show the resulting plot (0 = don't show [default], 1 = show)\n# 'path' = A full file path specification indicating whether or not the file should be saved (default = 0, don't save)\n#\n\n\ndef runARIMA(dta, exogvar, p, d, q, show=0, path=''):\n    mod = sm.tsa.ARIMA(dta, (p, d, q), exogvar)\n    results = mod.fit()\n    resids = results.resid.values\n    summarised = results.summary()\n    print summarised\n    plotpacf(resids, show, path)\n    return results\n\n\n#\n# Function to use fitted ARIMA for prediction of observed data, compare predicted to observed\n# 'dta' = Data used in ARIMA prediction\n# 'exog' = Exogenous variables fitted in the model\n# 'arima' = Result from correctly fitted ARIMA model, likely on the residuals of a decomposed time series\n# 'datrng' = Range of dates used for original time series definition, used for specifying predictions\n# 'show' = Whether or not to show the resulting plot (0 = don't show [default], 1 = show)\n# 'path' = A full file path specification indicating whether or not the file should be saved (default = 0, don't save)\n#\n\n\ndef ARIMAcompare(dta, exogvar, arima, datrng, show=0, path=''):\n    dflength = len(datrng) - 1\n    observation = dta\n    prediction = arima.predict(start=3, end=dflength, exog=exogvar, dynamic=True)\n    df = pd.concat([prediction, observation], axis=1)\n    df.columns = ['predicted', 'observed']\n    plt.plot(prediction)\n    plt.plot(observation)\n    if show != 0:\n        plt.show()\n    if path != '':\n        plt.savefig(path, format='png', bbox_inches='tight')\n    plt.clf()\n    return df\n\n\n#\n# Function use fitted ARIMA model for predictions\n# 'pred_hours' = number of hours we want to predict scores for\n# 'firsttime' = last timestamp in observations\n# 'df' = data frame containing data on which the ARIMA model was previously fitted\n# 'results' = output of the modeling procedure\n# 'freq' = Frequency of seasonal cycle that was used in decomposition\n# 'decomp' = Output of the time series decomposition step\n# 'mark' = Amount of hours included in the graph prior to prediction. Set at as close to 2 weeks as possible.\n# 'show' = Whether or not to show the resulting plot (0 = don't show [default], 1 = show)\n# 'path' = A full file path specification indicating whether or not the file should be saved (default = 0, don't save)\n#\n# Output: A dataframe with observed and predicted values. Note that predictions > 5 time units are considered unreliable\n# by modeling standards.\n#\n\n\ndef pred(pred_hours, k, df, arima, show=0, path=''):\n    n_obs = len(df.index)\n    lastdt = df.index[n_obs - 1]\n    lastdt = lastdt.to_datetime()\n    datrng = pd.date_range(lastdt, periods=(pred_hours + 1), freq='H')\n    future = pd.DataFrame(index=datrng, columns=df.columns)\n    df = pd.concat([df, future])\n    lendf = len(df.index)\n    df['predicted'] = arima.predict(start=n_obs, end=lendf, exog=k, dynamic=True)\n    print df\n    marked = 2 * pred_hours\n    df[['predicted', 'observed']].ix[-marked:].plot(figsize=(12, 8))\n    if show != 0:\n        plt.show()\n    if path != '':\n        plt.savefig(path, format='png', bbox_inches='tight')\n    plt.clf()\n    return df[['predicted', 'observed']].ix[-marked:]\n\n\ndirnow = os.getcwd()\nfpath = dirnow + '/sounds_full2.csv'\nfhand = open(fpath)\ndta = pd.read_csv(fhand, sep=',')\ndta_sel = dta.iloc[1248:2256, 2]\n#\n#\n#\n# Extract start and end date of measurements from sound data, adding one hour because\n# the last hour of the last day is not counted\n#\nsound_start = dta.iloc[1248, 0]\n# The above .iloc value needs to be changed depending on the length of the sound data set being read in.\n#\n# Establish start date\nsound_start = re.sub('-', '/', sound_start)\nsound_start = re.sub('_', ' ', sound_start)\nsound_start = sound_start + ':00'\nsound_start = pd.to_datetime(sound_start, format='%d/%m/%Y %H:%M:%S')\n#\n# Establish end date\nindexer = len(dta.index) - 1\nsound_end = dta.iloc[indexer, 0]\nsound_end = re.sub('-', '/', sound_end)\nsound_end = re.sub('_', ' ', sound_end)\nsound_end = sound_end + ':00'\nsound_end = pd.to_datetime(sound_end, format='%d/%m/%Y %H:%M:%S')\nsound_diff = sound_end - sound_start\n#\n# Derive number of periods and create data set\nnum_observed = (sound_diff.days * 24) + ((sound_diff.seconds + 3600) / 3600)\nusedates3 = pd.date_range(sound_start, periods=num_observed, freq='H')\nusedates3 = pd.Series(usedates3)\nusedates3.index = dta_sel.index\ntimedfreq = pd.concat([usedates3, dta_sel], axis=1)\ntimedfreq.index = timedfreq.iloc[:, 0]\nfreqset = pd.Series(timedfreq.iloc[:, 1])\nfilepath = dirnow + '/Sound_RollingMean.png'\nplotmean(freqset, 0, filepath)\n# Plotted mean shows recurring (seasonal) trends at periods of 24 hours and 168 hours.\n# This means a seasonal model is needed that accounts for both of these influences\n# To do so, Fourier series representing the 24- and 168 hour seasonal trends can be added to the ARIMA-model\n#\n#\n#\n#\n# Check for stationarity of data\n#\ndftest(freqset)\n# Time series can be considered stationary\n#\n#\n#\n# Establish frequencies and amplitudes with which to fit ARIMA model\n#\n# Decompose signal into frequency and amplitude\n#\nfilepath = dirnow + \"/Welch.png\"\nf, Pxx_den = runwelch(freqset, 0, filepath)\n#\n# Obtain sine wave parameters, optionally view test plots to check periodicity\nfreqs, amplitudes = getsines(len(freqset), f, Pxx_den, 2, 0)\n#\n# Use parameters to build Fourier series for observed data with varying values for k\nexog_sel = buildFterms(freqset, freqs, amplitudes)\nexog_sel.index = freqset.index\n#\n# fit ARIMA model, plot ACF and PACF for fitted model, check for effects orders of differencing on residuals\n#\nfilepath = dirnow + '/Sound_resid_ACFPACF.png'\nSound_ARIMA = runARIMA(freqset, exog_sel, 1, 0, 0, show=0, path=filepath)\nsound_residuals = Sound_ARIMA.resid\n#\n# Plot various acf / pacf plots of differencing given model residuals\nfilepath = dirnow + '/Sound_resid_ACFPACF_d1.png'\ntempdta_d1 = diffit(sound_residuals, 1, 0, filepath)\nfilepath = dirnow + '/Sound_resid_ACFPACF_d2.png'\ntempdta_d2 = diffit(sound_residuals, 2, 0, filepath)\n# Of the two differenced models, one order of differencing seems to yield the best results\n# Visual inspection of plots and model output suggests model with p = 2, d = 0 or p = 1, d = 1 to be optimal.\n#\n#\n#\n# Find optimal form of model\nfilepath = dirnow + '/Sound_resid_ACFPACF_200.png'\nSound_ARIMA_200 = runARIMA(freqset, exog_sel, 2, 0, 0, show=0, path=filepath)\nfilepath = dirnow + '/Sound_resid_ACFPACF_110.png'\nSound_ARIMA_110 = runARIMA(freqset, exog_sel, 1, 1, 0, show=0, path=filepath)\n# Based on model output and ACF / PACF plot comparison for 'Sound_resid_ACFPACF_110.png' and \\\n# 'Sound_resid_ACFPACF_200.png', the model parameters for p = 2, d = 0, q = 0 are closer to optimal.\n#\n# Use selected model to predict observed values\nfilepath = dirnow + '/Sound_PredictObserved.png'\nsound_comparison = ARIMAcompare(freqset, exog_sel, Sound_ARIMA_200, usedates3, 0, filepath)\n#\n# Predict values and store for Sound dataset\nfilepath = dirnow + '/Sound_PredictFuture.png'\nsound_storepred = pred(168, exog_sel.iloc[0:170, :], sound_comparison, Sound_ARIMA_200, 0, filepath)\n```\n\n\n\n\nData file\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "CVXPY - Not able to get elements of a vector expression and use them as array index\r\n                \r\nProblem Statement: I am trying to model and optimize a placement problem of placing the nodes of an undirected graph in a grid of cells such that the weighted Euclidean length is minimized subject to the constraint each grid cell can contain only a certain number of nodes based on its weighted-capacity. I am trying to use CVXPY to frame the model as a convex problem of weighted edge length minimization and optimize. The grid is organized as a 2X3 matrix.\nWhen I try to convert the node locations to a grid number with an expression and try to use the elements of the expression vector as an array index ( so that I can sum up all node weights in that grid cell), CVXPY does not seem to work.\n\nHere is some code I tried. Sample graph of 10 nodes with weighted edge matrix. Line 97 (gn = gridNum[i]) and 99 seem to have an issue( highlighted by PROBLEM comment. The last print line output is expected to show all the nodes are in only grid cell 0.\n\n\n\n```\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n@author: rajkumar\n\"\"\"\n\nimport numpy as np\nimport cvxpy as cvxp\nimport matplotlib.pyplot as plt\n\n\n# Problem data creation\n# Given a set of nodes and weighted edges,\n# place them on a grid of cells which have a link capacity,\n# such that the total euclidean distance is minimized\n# subject to the constraint that number of nodes in a grid cell\n# dont exceed a certain capacity\n\n# Will deal with maximizing grid cell link capacity objective function later\n# once I get the grid cell number issue fixed\n# E.g graph - 10 nodes with connectivity matrix\n# 6 grid cells arranged as 2x3\nnum_nodes = cvxp.Parameter(nonneg=True,value=10)\nnum_grid_cells = cvxp.Parameter(nonneg=True,value=6)\n# Max X and Y grid coordinate values\nmax_X = cvxp.Parameter(nonneg=True, value=(num_grid_cells.value/2))\nmax_Y = cvxp.Parameter(nonneg=True,value=(num_grid_cells.value/3))\n\n#Weight of each node\nnodeWts = np.array([10,20,15,12,19,11,14,9,12,8])\n#Capacity of each grid cell\ngridCellCapacities = np.array([30,30,30,30,30,30])\n# created a Variable intialized with above\ngridCapacity = cvxp.Parameter(shape=num_grid_cells.value, value=gridCellCapacities, nonneg=True)\n\n# Node adjacency matrix\ncellConnectivity = np.matrix([[0.,5.,2.,3.,4.,5.,0.,0.,0.,0.],\n                              [5.,0.,0.,2.,3.,4.,5.,6.,7.,8.],\n                              [2.,0.,0.,2.,3.,4.,5.,6.,7.,8.],\n                              [3.,2.,2.,0.,2.,3.,4.,5.,6.,7.],\n                              [4.,3.,3.,2.,0.,2.,3.,4.,5.,6.],\n                              [5.,4.,4.,3.,2.,0.,2.,3.,4.,5.],\n                              [0.,5.,5.,4.,3.,2.,0.,2.,3.,4.],\n                              [0.,6.,6.,5.,4.,3.,2.,0.,2.,3.],\n                              [0.,7.,7.,6.,5.,4.,3.,2.,0.,8.],\n                              [0.,8.,8.,7.,6.,5.,4.,3.,8.,0.]])\n\n\n\ncellWeightedDeg = np.matrix([[19,0,0,0,0,0,0,0,0,0],\n                     [0,40,0,0,0,0,0,0,0,0],\n                     [0,0,37,0,0,0,0,0,0,0],\n                     [0,0,0,34,0,0,0,0,0,0],\n                     [0,0,0,0,32,0,0,0,0,0],\n                     [0,0,0,0,0,32,0,0,0,0],\n                     [0,0,0,0,0,0,28,0,0,0],\n                     [0,0,0,0,0,0,0,31,0,0],\n                     [0,0,0,0,0,0,0,0,42,0],\n                     [0,0,0,0,0,0,0,0,0,49]])\n# Positive semi definite laplacian\ncellLP = cellWeightedDeg - cellConnectivity\n\nlocX = cvxp.Variable(num_nodes.value,pos=True)\nlocY = cvxp.Variable(num_nodes.value,pos=True)\n\n# Variable to store sum of node weights in a grid cell\ngridWt = cvxp.Variable(num_grid_cells.value, pos=True)\n\n#Number of nodes in each grid cell - num_nodes x num_grid_cells matrix\nnodesInGridCells = cvxp.Variable(shape=(num_nodes.value,num_grid_cells.value))\n\n#### Some workaround if not positive semi definite\ncellLP = 0.5*(cellLP+cellLP.T)           # make Q symmetric\nw, v = np.linalg.eig(cellLP)   # eigen decomposition\nprint(\"Eigenvalues\\n\",w)\nw1 = min(w)               # first eigen value\nprint(\"Smallest eigenvalue\",w1)\ntol = 1.0e-10          # tolerance \nf = 0                  # factor for diagonal perturbation\nif w1<tol:                 \n  f = -w1 + tol\ncellLP += f*np.eye(10)\n\nprint('cellLP - sh', cellLP)\n\n# Get grid number from X,Y locations of nodes\n# Grid arrangement\n# 3,4,5\n# 0,1,2\n\ngridW = np.array([0,0,0,0,0,0])\ngridNum = cvxp.ceil(locX) + cvxp.abs(cvxp.square(cvxp.ceil(locY))) - 2\n\n# The following loop is to get the gridNum from the above expression vector\n# so that it can be used to index into gridW - weight array\n# Elementwise extraction from gridNum expression is not working\n# PROBLEM IN THIS LOOP\nfor i in range(num_nodes.value):\n    gn = gridNum[i]\n    print('ISGV', gn.is_vector())\n    gridW[gn.value] += nodeWts[i]\nprint(\"GN_IS_VEC: \", gridNum.is_vector())\n\nconstraints = [locX >= 0.1, locY >= 0.1, locX <= max_X.value, locY <= max_Y.value]\n\nobjectiveX = (1/2)*cvxp.quad_form(locX,cellLP)\nobjectiveY = (1/2)*cvxp.quad_form(locY,cellLP)\n\n# PSD based objective\n#objectiveX = (1/2)*cvxp.quad_form(locX, cvxp.Parameter(shape=cellLP.shape, value=cellLP, PSD=True))\n#objectiveY = (1/2)*cvxp.quad_form(locY, cvxp.Parameter(shape=cellLP.shape, value=cellLP, PSD=True))\n\nprob = cvxp.Problem(cvxp.Minimize(objectiveX+objectiveY), constraints)\nprob.solve();\nprint('MINWL',prob.value)\nprint('X: ', locX.value)\nprint('Y: ', locY.value)\nprint('GN: ', gridNum.value)\n# Wrong values printed - Only grid number 0 should have all the nodes\nprint('GW: ', gridW)\n```\n\n\n    OUTPUT FROM CVXPY\n\n```\nEigenvalues\n [-7.10542736e-15  5.82050153e+01  1.91148884e+01  5.18253316e+01\n  3.09023706e+01  4.05651162e+01  3.85097439e+01  3.38040401e+01\n  3.48251251e+01  3.62483688e+01]\n\nSmallest eigenvalue -7.105427357601002e-15\n\ncellLP - sh [[19. -5. -2. -3. -4. -5.  0.  0.  0.  0.]\n [-5. 40.  0. -2. -3. -4. -5. -6. -7. -8.]\n [-2.  0. 37. -2. -3. -4. -5. -6. -7. -8.]\n [-3. -2. -2. 34. -2. -3. -4. -5. -6. -7.]\n [-4. -3. -3. -2. 32. -2. -3. -4. -5. -6.]\n [-5. -4. -4. -3. -2. 32. -2. -3. -4. -5.]\n [ 0. -5. -5. -4. -3. -2. 28. -2. -3. -4.]\n [ 0. -6. -6. -5. -4. -3. -2. 31. -2. -3.]\n [ 0. -7. -7. -6. -5. -4. -3. -2. 42. -8.]\n [ 0. -8. -8. -7. -6. -5. -4. -3. -8. 49.]]\n\nISGV True\nISGV True\nISGV True\nISGV True\nISGV True\nISGV True\nISGV True\nISGV True\nISGV True\nISGV True\nGN_IS_VEC:  True\nMINWL 2.872635462836115e-11\nX:  [0.16948144 0.16948144 0.16948144 0.16948144 0.16948144 0.16948144\n 0.16948144 0.16948144 0.16948144 0.16948144]\nY:  [0.16948144 0.16948144 0.16948144 0.16948144 0.16948144 0.16948144\n 0.16948144 0.16948144 0.16948144 0.16948144]\nGN:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\nGW: [130 130 130 130 130 130]   <---- Problem here\n```\n\n    ", "Answer": "\r\nOk. I found a way to make CVXPY compare values and then generate an index.\n\n```\nfor i in range(num_nodes.value):\n    idx = 0\n    for j in range(num_grid_cells.value):\n        if(gridNum[i].value.__eq__(j)):\n            idx = j\n            break\n    gridW[idx] += nodeWts[i]\n```\n\n\nThough this gives the correct grid cell weights - gridW, the optimizer is not able to Minimize ( minimized distance is inf ) when the weight constraint is added like:\n\n```\nconstraints = [locX >= 0.1, locY >= 0.1, locX <= max_X.value, locY <= max_Y.value, gridW <= gridCapacity]\n```\n\n\n\n\nOUTPUT from CVXPY:\n\n```\nEigenvalues\n [-7.10542736e-15  5.82050153e+01  1.91148884e+01  5.18253316e+01\n  3.09023706e+01  4.05651162e+01  3.85097439e+01  3.38040401e+01\n  3.48251251e+01  3.62483688e+01]\nSmallest eigenvalue -7.105427357601002e-15\ncellLP - sh [[19. -5. -2. -3. -4. -5.  0.  0.  0.  0.]\n [-5. 40.  0. -2. -3. -4. -5. -6. -7. -8.]\n [-2.  0. 37. -2. -3. -4. -5. -6. -7. -8.]\n [-3. -2. -2. 34. -2. -3. -4. -5. -6. -7.]\n [-4. -3. -3. -2. 32. -2. -3. -4. -5. -6.]\n [-5. -4. -4. -3. -2. 32. -2. -3. -4. -5.]\n [ 0. -5. -5. -4. -3. -2. 28. -2. -3. -4.]\n [ 0. -6. -6. -5. -4. -3. -2. 31. -2. -3.]\n [ 0. -7. -7. -6. -5. -4. -3. -2. 42. -8.]\n [ 0. -8. -8. -7. -6. -5. -4. -3. -8. 49.]]\nMINWL inf\nX:  None\nY:  None\nGN:  None\nGW: [130   0   0   0   0   0]\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Pinecone upserting wrong values\r\n                \r\nI have the variable ```\nmeta```\n equal to the following list\n```\n[{'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n',\n  'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n  'id_name': '0704.0001'},\n {'abstract': '  We describe a new algorithm, the $(k,\\\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n',\n  'title': 'Sparsity-certifying Graph Decompositions',\n  'id_name': '0704.0002'}...,\n```\n\n```\nfor i in tqdm(range(0, len(arxiv_dataset['abstract']), batch_size)):\n    # set end position of batch\n    i_end = min(i+batch_size, len(arxiv_dataset['abstract']))\n    # get batch of lines and IDs\n    abstracts_batch = arxiv_dataset['abstract'][i: i+batch_size]\n    titles_batch = arxiv_dataset['title'][i: i+batch_size]\n    id_list = arxiv_dataset['id'][i: i+batch_size]\n    print(id_list)\n\n    ids_batch = [str(n) for n in range(i, i_end)]\n    # create embeddings\n    res = openai.Embedding.create(input=abstracts_batch, engine=MODEL)\n    embeds = [record['embedding'] for record in res['data']]\n    # prep metadata and upsert batch\n    meta = [{'abstract': abstract, 'title': title, 'id_name': id_item} for abstract, title, id_item in zip(abstracts_batch, titles_batch, id_list)]\n    \n    to_upsert = zip(ids_batch, embeds, meta)\n    # upsert to Pinecone\n    index.upsert(vectors=list(to_upsert))\n```\n\nI'm using tqdm to batch data and then upsert to pinecone. However, when upserting, pinecone butchers the id_name and sets it to some datetime format. Thus, when I query back, I'm not getting the id_names of each of the results. I'm instead getting a value like  ```\n'id_name': datetime.date(704, 4, 26)```\n\nHow do I fix it so that data remains same while upserting and I get that value back when querying?\n    ", "Answer": "", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Failed testing on Ubuntu 12.04\r\n                \r\nAfter installing the scikit-learn from source code of version 0.14.1 by 'sodu python setup.py install', I tested the package by 'nosetests sklearn --exe', and received the following information:\n\n```\n==================================================================================\n/home/elkan/Downloads/MS2PIP/scikit-learn/sklearn/feature_selection/selector_mixin.py:7: DeprecationWarning: sklearn.feature_selection.selector_mixin.SelectorMixin has been renamed sklearn.feature_selection.from_model._LearntSelectorMixin, and this alias will be removed in version 0.16\n  DeprecationWarning)\n/home/elkan/Downloads/MS2PIP/scikit-learn/sklearn/pls.py:7: DeprecationWarning: This module has been moved to cross_decomposition and will be removed in 0.16\n  \"removed in 0.16\", DeprecationWarning)\n.......S................../home/elkan/Downloads/MS2PIP/scikit-learn/sklearn/cluster/hierarchical.py:746: DeprecationWarning: The Ward class is deprecated since 0.14 and will be removed in 0.17. Use the AgglomerativeClustering instead.\n  \"instead.\", DeprecationWarning)\n.........../usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py:1423: UserWarning: \n    Atlas (http://math-atlas.sourceforge.net/) libraries not found.\n    Directories to search for the libraries can be specified in the\n    numpy/distutils/site.cfg file (section [atlas]) or by setting\n    the ATLAS environment variable.\n  warnings.warn(AtlasNotFoundError.__doc__)\n.............................................../home/elkan/Downloads/MS2PIP/scikit-learn/sklearn/manifold/spectral_embedding_.py:226: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n  warnings.warn(\"Graph is not fully connected, spectral embedding\"\n..................................SS..............S.................................................../home/elkan/Downloads/MS2PIP/scikit-learn/sklearn/utils/extmath.py:83: NonBLASDotWarning: Data must be of same type. Supported types are 32 and 64 bit float. Falling back to np.dot.\n  'Falling back to np.dot.', NonBLASDotWarning)\n....................../home/elkan/Downloads/MS2PIP/scikit-learn/sklearn/decomposition/fastica_.py:271: UserWarning: Ignoring n_components with whiten=False.\n  warnings.warn('Ignoring n_components with whiten=False.')\n..................../home/elkan/Downloads/MS2PIP/scikit-learn/sklearn/utils/extmath.py:83: NonBLASDotWarning: Data must be of same type. Supported types are 32 and 64 bit float. Falling back to np.dot.\n  'Falling back to np.dot.', NonBLASDotWarning)\n....................................S................................../home/elkan/Downloads/MS2PIP/scikit-learn/sklearn/externals/joblib/test/test_func_inspect.py:134: UserWarning: Cannot inspect object <functools.partial object at 0xbdebf04>, ignore list will not work.\n  nose.tools.assert_equal(filter_args(ff, ['y'], (1, )),\n\nFAIL: Check that gini is equivalent to mse for binary output variable\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/nose/case.py\", line 197, in runTest\n    self.test(*self.arg)\n  File \"/home/elkan/Downloads/MS2PIP/scikit-learn/sklearn/tree/tests/test_tree.py\", line 301, in test_importances_gini_equal_mse\n    assert_almost_equal(clf.feature_importances_, reg.feature_importances_)\n  File \"/usr/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 452, in assert_almost_equal\n    return assert_array_almost_equal(actual, desired, decimal, err_msg)\n  File \"/usr/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 800, in assert_array_almost_equal\n    header=('Arrays are not almost equal to %d decimals' % decimal))\n  File \"/usr/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 636, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nArrays are not almost equal to 7 decimals\n\n(mismatch 70.0%)\n x: array([ 0.2925143 ,  0.27676187,  0.18835709,  0.04181255,  0.03699054,\n        0.01668818,  0.03661717,  0.03439216,  0.04422749,  0.03163866])\n y: array([ 0.29599052,  0.27676187,  0.19146823,  0.03837769,  0.03699054,\n        0.01811955,  0.0362238 ,  0.03439216,  0.04137032,  0.03030531])\n>>  raise AssertionError('\\nArrays are not almost equal to 7 decimals\\n\\n(mismatch 70.0%)\\n x: array([ 0.2925143 ,  0.27676187,  0.18835709,  0.04181255,  0.03699054,\\n        0.01668818,  0.03661717,  0.03439216,  0.04422749,  0.03163866])\\n y: array([ 0.29599052,  0.27676187,  0.19146823,  0.03837769,  0.03699054,\\n        0.01811955,  0.0362238 ,  0.03439216,  0.04137032,  0.03030531])')\n\n\n----------------------------------------------------------------------\nRan 3950 tests in 150.890s\n\nFAILED (SKIP=19, failures=1)\n\n\n==================================================================================\n```\n\n\nThe python version is 2.7.3, OS is 32 bit.\nSo, what the problem might be?\n\nThanks.\n    ", "Answer": "\r\nIt's a numerical precision discrepancy on 32 bit platforms. You can safely ignore it as the failing test is checking the values of the attribute ```\nclf.feature_importances_```\n of a random forest which usually do not need to be precise to be useful (interpretation of the most important features contributing to the RF model).\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How to find a key using Mongodb (pymongo)\r\n                \r\nI am having a problem to find a particular value of a key with pymongo. below I printed two document of a collection\n```\npprint(list(papers.find().limit(2)))\n\n[{'_id': ObjectId('5fa9a4db76fdd8d66273c643'),\n'abstract': '  A fully differential calculation in perturbative quantum '\n          'chromodynamics is\\n'\n          'presented for the production of massive photon pairs at hadron '\n          'colliders. All\\n'\n          'next-to-leading order perturbative contributions from '\n          'quark-antiquark,\\n'\n          'gluon-(anti)quark, and gluon-gluon subprocesses are included, '\n          'as well as\\n'\n          'all-orders resummation of initial-state gluon radiation valid '\n          'at\\n'\n          'next-to-next-to-leading logarithmic accuracy. The region of '\n          'phase space is\\n'\n          'specified in which the calculation is most reliable. Good '\n          'agreement is\\n'\n          'demonstrated with data from the Fermilab Tevatron, and '\n          'predictions are made for\\n'\n          'more detailed tests with CDF and DO data. Predictions are shown '\n          'for\\n'\n          'distributions of diphoton pairs produced at the energy of the '\n          'Large Hadron\\n'\n          'Collider (LHC). Distributions of the diphoton pairs from the '\n          'decay of a Higgs\\n'\n          'boson are contrasted with those produced from QCD processes at '\n          'the LHC, showing\\n'\n          'that enhanced sensitivity to the signal can be obtained with '\n          'judicious\\n'\n          'selection of events.\\n',\n'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n'authors_parsed': [['Balázs', 'C.', ''],\n                 ['Berger', 'E. L.', ''],\n                 ['Nadolsky', 'P. M.', ''],\n                 ['Yuan', 'C. -P.', '']],\n'categories': 'hep-ph',\n'comments': '37 pages, 15 figures; published version',\n'doi': '10.1103/PhysRevD.76.013009',\n'id': '0704.0001',\n'journal-ref': 'Phys.Rev.D76:013009,2007',\n'license': None,\n'report-no': 'ANL-HEP-PR-07-12',\n'submitter': 'Pavel Nadolsky',\n'title': 'Calculation of prompt diphoton production cross sections at '\n       'Tevatron and\\n'\n       '  LHC energies',\n'update_date': '2008-11-26',\n'versions': [{'created': 'Mon, 2 Apr 2007 19:18:42 GMT', 'version': 'v1'},\n           {'created': 'Tue, 24 Jul 2007 20:10:27 GMT', 'version': 'v2'}]},\n{'_id': ObjectId('5fa9a4db76fdd8d66273c644'),\n'abstract': '  We describe a new algorithm, the $(k,\\\\ell)$-pebble game with '\n          'colors, and use\\n'\n          'it obtain a characterization of the family of '\n          '$(k,\\\\ell)$-sparse graphs and\\n'\n          'algorithmic solutions to a family of problems concerning tree '\n          'decompositions of\\n'\n          'graphs. Special instances of sparse graphs appear in rigidity '\n          'theory and have\\n'\n          'received increased attention in recent years. In particular, '\n          'our colored\\n'\n          'pebbles generalize and strengthen the previous results of Lee '\n          'and Streinu and\\n'\n          'give a new proof of the Tutte-Nash-Williams characterization of '\n          'arboricity. We\\n'\n          'also present a new decomposition that certifies sparsity based '\n          'on the\\n'\n          '$(k,\\\\ell)$-pebble game with colors. Our work also exposes '\n          'connections between\\n'\n          'pebble game algorithms and previous sparse graph algorithms by '\n          'Gabow, Gabow and\\n'\n          'Westermann and Hendrickson.\\n',\n'authors': 'Ileana Streinu and Louis Theran',\n'authors_parsed': [['Streinu', 'Ileana', ''], ['Theran', 'Louis', '']],\n'categories': 'math.CO cs.CG',\n'comments': 'To appear in Graphs and Combinatorics',\n'doi': None,\n'id': '0704.0002',\n'journal-ref': None,\n'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/',\n'report-no': None,\n'submitter': 'Louis Theran',\n'title': 'Sparsity-certifying Graph Decompositions',\n'update_date': '2008-12-13',\n'versions': [{'created': 'Sat, 31 Mar 2007 02:26:18 GMT', 'version': 'v1'},\n           {'created': 'Sat, 13 Dec 2008 17:26:00 GMT', 'version': 'v2'}]}]\n```\n\nI am trying to get the nombre of articls that are not published by \"Damien Chablat\".\nthe articles here are the key title and also we have to use the author key.\nI just entred the following command but it's not working:\n```\npprint(len(list(papers.find({},{\"authors\":{$ne: \"Damien Chablat\"}, '_id':0}))))\n```\n\nthe error is :\n```\npprint(len(list(papers.find({},{\"authors\":{$ne: \"Damien Chablat\"}, '_id':0}))))\n                                           ^\nSyntaxError: invalid syntax\n```\n\nplease help me.thanks in advance\n    ", "Answer": "\r\nThis first request is for searching a string using regex to find a value(\"Damien Chablat\")\n```\n pprint(len(list(papers.find({\"authors\": {\"$regex\": \"Damien Chablat\"}}, \n {\"authors\": 1,\"authors_parsed\": 1,\"_id\":0}))))\n```\n\nthis gave 32 a the number of article published by damien\nBut we need to find out the articls that are not published by Damien so as follow:\n```\n papers.count_documents({\"authors\": {\"$not\": {\"$regex\": \"Damien \n Chablat\"}}})\n```\n\nthis one gave 9968\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Plot the timeframe of each unique sound loop in a song, with rows sorted by sound similarity using python Librosa\r\n                \r\nBackground\nHere's a video of a song clip from an electronic song. At the beginning of the video, the song plays at full speed. When you slow down the song you can hear all the unique sounds that the song uses. Some of these sounds repeat.\n\nMp3, Wav and MIDI of the audio in the video\n\nProblem Description\nWhat I am trying to do is create a visual like below, where a horizontal track/row is created for each unique sound, with a colored block on that track that corresponds to each timeframe in the song that sound is playing. The tracks/rows should be sorted by how similar the sounds are to each, with more similar sounds being closer together. If sounds are so identical a human couldn't tell them apart, they should be considered the same sound.\n\nI'll accept an imperfect solution if it can generally do what I'm asking\n\n\n\nWatch the video linked above for a video description of what I am saying. It includes a visual grid that I created manually which almost matches the grid I am trying to produce.\n\nIf for example, each of the 5 waves below represents the soundwave that a sound makes, each of these sounds would be considered similar, and would be placed close to each other vertically on the grid.\n\n\nAttempts\nI've been looking at an example for Laplacian segmentation in librosa. The graph labelled structure components looks like it might be what I need. From reading the paper, it looks like they are trying to break the song into segments like chorus, verse, bridge... but I am essentially trying to break the song into 1 or 2 beat fragments.\n\nHere is the code for the Laplacian Segmentation (there's a Jupyter Notebook as well if you would prefer that).\n\r\n\r\n```\n# -*- coding: utf-8 -*-\n\"\"\"\n======================\nLaplacian segmentation\n======================\n\nThis notebook implements the laplacian segmentation method of\n`McFee and Ellis, 2014 <http://bmcfee.github.io/papers/ismir2014_spectral.pdf>`_,\nwith a couple of minor stability improvements.\n\nThroughout the example, we will refer to equations in the paper by number, so it will be\nhelpful to read along.\n\"\"\"\n\n# Code source: Brian McFee\n# License: ISC\n\n\n###################################\n# Imports\n#   - numpy for basic functionality\n#   - scipy for graph Laplacian\n#   - matplotlib for visualization\n#   - sklearn.cluster for K-Means\n#\nimport numpy as np\nimport scipy\nimport matplotlib.pyplot as plt\n\nimport sklearn.cluster\n\nimport librosa\nimport librosa.display\nimport matplotlib.patches as patches\n\n#############################\n# First, we'll load in a song\ndef laplacianSegmentation(fileName):\n    y, sr = librosa.load(librosa.ex('fishin'))\n\n\n    ##############################################\n    # Next, we'll compute and plot a log-power CQT\n    BINS_PER_OCTAVE = 12 * 3\n    N_OCTAVES = 7\n    C = librosa.amplitude_to_db(np.abs(librosa.cqt(y=y, sr=sr,\n                                            bins_per_octave=BINS_PER_OCTAVE,\n                                            n_bins=N_OCTAVES * BINS_PER_OCTAVE)),\n                                ref=np.max)\n\n    fig, ax = plt.subplots()\n    librosa.display.specshow(C, y_axis='cqt_hz', sr=sr,\n                            bins_per_octave=BINS_PER_OCTAVE,\n                            x_axis='time', ax=ax)\n\n\n    ##########################################################\n    # To reduce dimensionality, we'll beat-synchronous the CQT\n    tempo, beats = librosa.beat.beat_track(y=y, sr=sr, trim=False)\n    Csync = librosa.util.sync(C, beats, aggregate=np.median)\n\n    # For plotting purposes, we'll need the timing of the beats\n    # we fix_frames to include non-beat frames 0 and C.shape[1] (final frame)\n    beat_times = librosa.frames_to_time(librosa.util.fix_frames(beats,\n                                                                x_min=0,\n                                                                x_max=C.shape[1]),\n                                        sr=sr)\n\n    fig, ax = plt.subplots()\n    librosa.display.specshow(Csync, bins_per_octave=12*3,\n                            y_axis='cqt_hz', x_axis='time',\n                            x_coords=beat_times, ax=ax)\n\n\n    #####################################################################\n    # Let's build a weighted recurrence matrix using beat-synchronous CQT\n    # (Equation 1)\n    # width=3 prevents links within the same bar\n    # mode='affinity' here implements S_rep (after Eq. 8)\n    R = librosa.segment.recurrence_matrix(Csync, width=3, mode='affinity',\n                                        sym=True)\n\n    # Enhance diagonals with a median filter (Equation 2)\n    df = librosa.segment.timelag_filter(scipy.ndimage.median_filter)\n    Rf = df(R, size=(1, 7))\n\n\n    ###################################################################\n    # Now let's build the sequence matrix (S_loc) using mfcc-similarity\n    #\n    #   :math:`R_\\text{path}[i, i\\pm 1] = \\exp(-\\|C_i - C_{i\\pm 1}\\|^2 / \\sigma^2)`\n    #\n    # Here, we take :math:`\\sigma` to be the median distance between successive beats.\n    #\n    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n    Msync = librosa.util.sync(mfcc, beats)\n\n    path_distance = np.sum(np.diff(Msync, axis=1)**2, axis=0)\n    sigma = np.median(path_distance)\n    path_sim = np.exp(-path_distance / sigma)\n\n    R_path = np.diag(path_sim, k=1) + np.diag(path_sim, k=-1)\n\n\n    ##########################################################\n    # And compute the balanced combination (Equations 6, 7, 9)\n\n    deg_path = np.sum(R_path, axis=1)\n    deg_rec = np.sum(Rf, axis=1)\n\n    mu = deg_path.dot(deg_path + deg_rec) / np.sum((deg_path + deg_rec)**2)\n\n    A = mu * Rf + (1 - mu) * R_path\n\n\n    ###########################################################\n    # Plot the resulting graphs (Figure 1, left and center)\n    fig, ax = plt.subplots(ncols=3, sharex=True, sharey=True, figsize=(10, 4))\n    librosa.display.specshow(Rf, cmap='inferno_r', y_axis='time', x_axis='s',\n                            y_coords=beat_times, x_coords=beat_times, ax=ax[0])\n    ax[0].set(title='Recurrence similarity')\n    ax[0].label_outer()\n    librosa.display.specshow(R_path, cmap='inferno_r', y_axis='time', x_axis='s',\n                            y_coords=beat_times, x_coords=beat_times, ax=ax[1])\n    ax[1].set(title='Path similarity')\n    ax[1].label_outer()\n    librosa.display.specshow(A, cmap='inferno_r', y_axis='time', x_axis='s',\n                            y_coords=beat_times, x_coords=beat_times, ax=ax[2])\n    ax[2].set(title='Combined graph')\n    ax[2].label_outer()\n\n\n    #####################################################\n    # Now let's compute the normalized Laplacian (Eq. 10)\n    L = scipy.sparse.csgraph.laplacian(A, normed=True)\n\n\n    # and its spectral decomposition\n    evals, evecs = scipy.linalg.eigh(L)\n\n\n    # We can clean this up further with a median filter.\n    # This can help smooth over small discontinuities\n    evecs = scipy.ndimage.median_filter(evecs, size=(9, 1))\n\n\n    # cumulative normalization is needed for symmetric normalize laplacian eigenvectors\n    Cnorm = np.cumsum(evecs**2, axis=1)**0.5\n\n    # If we want k clusters, use the first k normalized eigenvectors.\n    # Fun exercise: see how the segmentation changes as you vary k\n\n    k = 5\n\n    X = evecs[:, :k] / Cnorm[:, k-1:k]\n\n\n    # Plot the resulting representation (Figure 1, center and right)\n\n    fig, ax = plt.subplots(ncols=2, sharey=True, figsize=(10, 5))\n    librosa.display.specshow(Rf, cmap='inferno_r', y_axis='time', x_axis='time',\n                            y_coords=beat_times, x_coords=beat_times, ax=ax[1])\n    ax[1].set(title='Recurrence similarity')\n    ax[1].label_outer()\n\n    librosa.display.specshow(X,\n                            y_axis='time',\n                            y_coords=beat_times, ax=ax[0])\n    ax[0].set(title='Structure components')\n\n\n    #############################################################\n    # Let's use these k components to cluster beats into segments\n    # (Algorithm 1)\n    KM = sklearn.cluster.KMeans(n_clusters=k)\n\n    seg_ids = KM.fit_predict(X)\n\n\n    # and plot the results\n    fig, ax = plt.subplots(ncols=3, sharey=True, figsize=(10, 4))\n    colors = plt.get_cmap('Paired', k)\n\n    librosa.display.specshow(Rf, cmap='inferno_r', y_axis='time',\n                            y_coords=beat_times, ax=ax[1])\n    ax[1].set(title='Recurrence matrix')\n    ax[1].label_outer()\n\n    librosa.display.specshow(X,\n                            y_axis='time',\n                            y_coords=beat_times, ax=ax[0])\n    ax[0].set(title='Structure components')\n\n    img = librosa.display.specshow(np.atleast_2d(seg_ids).T, cmap=colors,\n                            y_axis='time', y_coords=beat_times, ax=ax[2])\n    ax[2].set(title='Estimated segments')\n    ax[2].label_outer()\n    fig.colorbar(img, ax=[ax[2]], ticks=range(k))\n\n\n    ###############################################################\n    # Locate segment boundaries from the label sequence\n    bound_beats = 1 + np.flatnonzero(seg_ids[:-1] != seg_ids[1:])\n\n    # Count beat 0 as a boundary\n    bound_beats = librosa.util.fix_frames(bound_beats, x_min=0)\n\n    # Compute the segment label for each boundary\n    bound_segs = list(seg_ids[bound_beats])\n\n    # Convert beat indices to frames\n    bound_frames = beats[bound_beats]\n\n    # Make sure we cover to the end of the track\n    bound_frames = librosa.util.fix_frames(bound_frames,\n                                        x_min=None,\n                                        x_max=C.shape[1]-1)\n\n    ###################################################\n    # And plot the final segmentation over original CQT\n\n\n    # sphinx_gallery_thumbnail_number = 5\n\n    bound_times = librosa.frames_to_time(bound_frames)\n    freqs = librosa.cqt_frequencies(n_bins=C.shape[0],\n                                    fmin=librosa.note_to_hz('C1'),\n                                    bins_per_octave=BINS_PER_OCTAVE)\n\n    fig, ax = plt.subplots()\n    librosa.display.specshow(C, y_axis='cqt_hz', sr=sr,\n                            bins_per_octave=BINS_PER_OCTAVE,\n                            x_axis='time', ax=ax)\n\n    for interval, label in zip(zip(bound_times, bound_times[1:]), bound_segs):\n        ax.add_patch(patches.Rectangle((interval[0], freqs[0]),\n                                    interval[1] - interval[0],\n                                    freqs[-1],\n                                    facecolor=colors(label),\n                                    alpha=0.50))```\n\r\n\r\n\r\n\n\nOne major thing that I believe would have to change would be the number of clusters, they have 5 in the example, but I don't know what I would want it to be because I don't know how many sounds there are. I set it to 400 producing the following result, which didn't really feel like something I could work with. Ideally I would want all the blocks to be solid colors: not colors in between the max red and blue values.\n\n(I turned it sideways to look more like my examples above and more like the output I'm trying to produce)\n\nAdditional Info\nThere may also be a drum track in the background and sometimes multiple sounds are played at the same time. If these multiple sound groups get interpreted as one unique sound that's ok, but I'd obviously prefer if they could be distinguished as separate sounds.\nIf it makes it easier you can remove a drum loop using\n```\ny, sr = librosa.load(librosa.ex('exampleSong.mp3'))\ny_harmonic, y_percussive = librosa.effects.hpss(y)\n```\n\n\nUpdate\nI was able to separate the sounds by transients. Currently this kind of works, but it separates into too many sounds, from I could tell, it seemed like it was mostly just separating some sounds into two though. I can also create a midi file from the software I'm using, and using that to determine the transient times, but I would like to solve this problem without the midi file if I could. The midi file was pretty accurate, and split the sound file into 33 sections, whereas that transient code split the sound file into 40 sections. Here's a visualization of the midi\n\nSo that parts that still need to be solved would be\n\nBetter transient separation\nSorting the sounds\n\n    ", "Answer": "\r\nBelow is a script that uses Non-negative Matrix Factorization (NMF) on mel-spectrograms to decompose the input audio. I took the first seconds with complete audio of your uploaded audio WAV, and ran the code to get the following output.\nBoth the code and the audio clip can be found in the Github repository.\n\nThis approach seems to do pretty reasonably on short audio clips when the BPM is known (seems to be around 130 with given example) and the input audio is roughly aligned to the beat. No guarantee it will work as well on a whole song, or other songs.\nThere are many ways it could be improved:\n\nUsing a more compact and perceptual vector than mel-spectrogram as NMF. Possibly a transformation learned from music. Either an embedding an autoencoder.\nDe-duplicate NMF components into \"primary\" components.\nAdding constraints to the NMF, such as temporal. Lots of research papers out there\nAutomatically detecting BPM and doing alignment\nBetter perceptual sorting. Might want to have groups, such as chords, single tones, percussive\n\n```\nimport os.path\nimport sys\n\nimport librosa\nimport pandas\nimport numpy\nimport sklearn.decomposition\nimport skimage.color\n\nfrom matplotlib import pyplot as plt\nimport librosa.display\nimport seaborn\n\n\n\ndef decompose_audio(y, sr, bpm, per_beat=8,\n                    n_components=16, n_mels=128, fmin=100, fmax=6000):\n    \"\"\"\n    Decompose audio using NMF spectrogram decomposition,\n    using a fixed number of frames per beat (@per_beat) for a given @bpm\n    NOTE: assumes audio to be aligned to the beat\n    \"\"\"\n    \n    interval = (60/bpm)/per_beat\n    T = sklearn.decomposition.NMF(n_components)\n    S = numpy.abs(librosa.feature.melspectrogram(y, hop_length=int(sr*interval), n_mels=128, fmin=100, fmax=6000))\n    \n    comps, acts = librosa.decompose.decompose(S, transformer=T, sort=False)\n    \n    # compute feature to sort components by\n    ind = numpy.apply_along_axis(numpy.argmax, 0, comps)\n    #ind = librosa.feature.spectral_rolloff(S=comps)[0]\n    #ind = librosa.feature.spectral_centroid(S=comps)[0]\n\n    # apply sorting\n    order_idx = numpy.argsort(ind)\n    ordered_comps = comps[:,order_idx]\n    ordered_acts = acts[order_idx,:]\n    \n    # plot components\n    librosa.display.specshow(librosa.amplitude_to_db(ordered_comps,\n                                                  ref=numpy.max),y_axis='mel', sr=sr)\n    \n    return S, ordered_comps, ordered_acts\n\n\n\ndef plot_colorized_activations(acts, ax, hop_length=None, sr=None, value_mod=1.0):\n\n    hsv = numpy.stack([\n        numpy.ones(shape=acts.shape),\n        numpy.ones(shape=acts.shape),\n        acts,\n    ], axis=-1)\n\n    # Set hue based on a palette\n    colors = seaborn.color_palette(\"husl\", hsv.shape[0])\n    for row_no in range(hsv.shape[0]):\n        c = colors[row_no]\n        c = skimage.color.rgb2hsv(numpy.stack([c]))[0]\n        hsv[row_no, :, 0] = c[0]\n        hsv[row_no, :, 1] = c[1]\n        hsv[row_no, :, 2] *= value_mod\n\n    colored = skimage.color.hsv2rgb(hsv)\n    \n    # use same kind of order as librosa.specshow\n    flipped = colored[::-1, :, :]\n\n    ax.imshow(flipped)\n    ax.set(aspect='auto')\n    \n    ax.tick_params(axis='x',\n        which='both',\n        bottom=False,\n        top=False,\n        labelbottom=False)\n    \n    ax.tick_params(axis='both',\n        which='both',\n        bottom=False,\n        left=False,\n        top=False,\n        labelbottom=False)\n    \n\ndef plot_activations(S, acts):\n    fig, ax = plt.subplots(nrows=4, ncols=1, figsize=(25, 15), sharex=False)\n    \n    # spectrogram\n    db = librosa.amplitude_to_db(S, ref=numpy.max)\n    librosa.display.specshow(db, ax=ax[0], y_axis='mel')\n\n    # original activations\n    librosa.display.specshow(acts, x_axis='time', ax=ax[1])\n\n    # colorize\n    plot_colorized_activations(acts, ax=ax[2], value_mod=3.0)\n\n    # thresholded\n    q = numpy.quantile(acts, 0.90, axis=0, keepdims=True) + 1e-9\n    norm = acts / q\n    threshold = numpy.quantile(norm, 0.93)\n    plot_colorized_activations((norm > threshold).astype(float), ax=ax[3], value_mod=1.0)\n    return fig\n\ndef main():\n    audio_file = 'silence-end.wav'\n    audio_bpm = 130\n    sr = 22050\n    audio, sr = librosa.load(audio_file, sr=sr)\n    S, comps, acts = decompose_audio(y=audio, sr=sr, bpm=audio_bpm)\n    fig = plot_activations(S, acts)\n    fig.savefig('plot.png', transparent=False)\n\nmain()\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How do I change the \"str\" ​labels in a function to \"int\" and return a plt.legend() that concatenates the two labels in an \"int, str\" format?\r\n                \r\nI have a function that allows me to display the circle of correlations of my pca.\nThe problem with this function is that the labels of my variables (column names) prevent me from reading my results correctly.\nTo overcome this problem, I have to insert a line of code before my function to associate numbers with the labels of the variables (of the df used to make my pca):\n```\nn_labels = [value for value in range(1, (len(df.columns) + 1))]\n```\n\nI tried unsuccessfully to insert this line in my function:\n```\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\nimport numpy as np\nimport pandas as pd\n\n\ndef display_circles(pcs,\n                    n_comp,\n                    pca,\n                    axis_ranks,\n                    labels=None,\n                    label_rotation=0,\n                    lims=None):\n    for d1, d2 in axis_ranks:  # We display the first 3 factorial planes, so the first 6 components\n        if d2 < n_comp:\n\n            # figure initialization\n            fig, ax = plt.subplots(figsize=(10, 8))\n\n            # determination of graph limits\n            if lims is not None:\n                xmin, xmax, ymin, ymax = lims\n            elif pcs.shape[1] < 30:\n                xmin, xmax, ymin, ymax = -1, 1, -1, 1\n            else:\n                xmin, xmax, ymin, ymax = min(pcs[d1, :]), max(pcs[d1, :]), min(\n                    pcs[d2, :]), max(pcs[d2, :])\n\n            # arrow display\n            # if there are more than 30 arrows, the triangle is not displayed at their end\n            if pcs.shape[1] < 30:\n                plt.quiver(np.zeros(pcs.shape[1]),\n                           np.zeros(pcs.shape[1]),\n                           pcs[d1, :],\n                           pcs[d2, :],\n                           angles='xy',\n                           scale_units='xy',\n                           scale=1,\n                           color=\"grey\")\n            else:\n                lines = [[[0, 0], [x, y]] for x, y in pcs[[d1, d2]].T]\n                ax.add_collection(\n                    LineCollection(lines, axes=ax, alpha=.1, color='black'))\n\n            # display of variable names\n            if labels is not None:\n                for i, (x, y) in enumerate(pcs[[d1, d2]].T):\n                    if x >= xmin and x <= xmax and y >= ymin and y <= ymax:\n                        plt.text(x,\n                                 y,\n                                 labels[i],\n                                 fontsize='22',\n                                 ha='center',\n                                 va='bottom',\n                                 rotation=label_rotation,\n                                 color=\"red\",\n                                 alpha=0.7)\n\n            # circle display\n            circle = plt.Circle((0, 0), 1, facecolor='none', edgecolor='b')\n            plt.gca().add_artist(circle)\n\n            # setting graph limits\n            plt.xlim(xmin, xmax)\n            plt.ylim(ymin, ymax)\n\n            # display of horizontal and vertical lines\n            plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n            plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n\n            # names of the axes, with the percentage of inertia explained\n            plt.xlabel('F{} ({}%)'.format(\n                d1 + 1, round(100 * pca.explained_variance_ratio_[d1], 1)),\n                       fontsize=14)\n            plt.ylabel('F{} ({}%)'.format(\n                d2 + 1, round(100 * pca.explained_variance_ratio_[d2], 1)),\n                       fontsize=14)\n\n            plt.title(\"Circle of correlations (F{} and F{})\".format(\n                d1 + 1, d2 + 1),\n                      size=24)\n            plt.show()\n```\n\nThis is how I call my function:\n```\nimport pandas as pd\nfrom sklearn import decomposition, preprocessing\n\n\n# Here a dataset for the example:\ncolumn_1 = [1, 2, 3, 4, 5, 6, 7 ,8]\ncolumn_2 = [4, 2, 9, 23, 3, 52, 41, 4]\ncolumn_3 = [9, 8, 7, 6, 6, 9, 24, 11]\ncolumn_4 = [45, 36, 74, 35, 29, 45, 29, 39]\ncolumn_5 = [35, 84, 3, 54, 68, 78, 65, 97]\ncolumn_6 = [24, 96, 7, 54, 67, 69, 88, 95]\ncolumn_7 = [5, 39, 72, 42, 22, 41, 24, 41]\ncolumn_8 = [30, 98, 8, 67, 68, 41, 27, 87]\n\n\ndf = pd.DataFrame({'column_1': column_1,\n                  'column_2': column_2,\n                  'column_3': column_3, \n                  'column_4': column_4,\n                  'column_5': column_5,\n                  'column_6': column_6,\n                  'column_7': column_7, \n                  'column_8': column_8})\n\n\npca_data = preprocessing.scale(df)\n\npca = decomposition.PCA(n_components = 8)\npca.fit(pca_data)\n\n# We set the number of components\nn_comp = 2\n# Recovery of the components of the pca object\npcs = pca.components_\n# We label a number to each column name\nn_labels = [value for value in range(1, (len(df.columns) + 1))]\ndisplay_circles(pcs, n_comp, pca, [(0, 1), (0, 2)], labels=n_labels)\n\nfor element in zip(n_labels, df.columns):\n    print(element)\n```\n\nHere is my obtained result:\n\nEdit 1: that i would like (UPD: with the answer of @Stef -Thanks you very much and congratulations for this solution-)\nit's almost perfect but the problem is when I use this function:\n```\nn_comp = 3\npcs = pca.components_\n# I always have to write this line to get a label number\nn_labels=[value for value in range(1,(len(list_candidates.columns)+1))]\ndisplay_circles(pcs, n_comp, pca, [(0, 1), (0, 2)], labels=n_labels)\n```\n\non my real dataframe, this throws me two problems:\n\nI still have to include the line\n\n```\nn_labels=[value for value in range(1,(len(list_candidates.columns)+1))]\n```\n\nto obtain a label number instead of the name of my variables.\n\nI get the error message \"NameError: name 'df' is not defined\" when running\n\n```\ndisplay_circles(pcs, n_comp, pca, [(0, 1), (0, 2)], labels=n_labels)\n```\n\nSo I'm looking to define my ```\ndisplay_circles()```\n function so that when I set the ```\nlabels=\"name_of_the_df\"```\n argument it returns me the same result as\n```\nn_labels=[value for value in range(1,(len(\"name_of_the_df\".columns)+1))]\n```\n\nplus a ```\nplt.legend()```\n like the one made by @Steph (thanks)\nTo get this (desired) result:\n\nI also have to modify \"name_of_the_df\" in the function definition:\n```\n   #legend\n   plt.legend(n_labels,\n              candidate_list.columns,\n              handler_map={int: IntHandler()},\n              bbox_to_anchor=(1, 1))\n```\n\n    ", "Answer": "\r\nYou can define your own legend handler for integers:\n```\nfrom matplotlib.text import Text\n\nclass IntHandler:\n    def legend_artist(self, legend, orig_handle, fontsize, handlebox):\n        x0, y0 = handlebox.xdescent, handlebox.ydescent\n        text = Text(x0, y0, str(orig_handle), color='red')\n        handlebox.add_artist(text)\n        return text\n```\n\nand then call\n```\nplt.legend(n_labels, df.columns, handler_map={int: IntHandler()}, bbox_to_anchor=(1,1))\n```\n\nbefore ```\nplt.show()```\n in ```\ndisplay_circles```\n:\n\n  \nFull example as per comment below and edited question:\n```\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\nfrom matplotlib.text import Text\nimport numpy as np\nimport pandas as pd\nfrom sklearn import decomposition, preprocessing\n\nclass IntHandler:\n    def legend_artist(self, legend, orig_handle, fontsize, handlebox):\n        x0, y0 = handlebox.xdescent, handlebox.ydescent\n        text = Text(x0, y0, str(orig_handle), color='red')\n        handlebox.add_artist(text)\n        return text\n\ndef display_circles(pcs,\n                    n_comp,\n                    pca,\n                    axis_ranks,\n                    labels=None,\n                    label_rotation=0,\n                    lims=None):\n    if labels == None:\n        labels = range(1, len(pca.feature_names_in_) + 1)\n    \n    for d1, d2 in axis_ranks:  # We display the first 3 factorial planes, so the first 6 components\n        if d2 < n_comp:\n\n            # figure initialization\n            fig, ax = plt.subplots(figsize=(10, 8))\n\n            # determination of graph limits\n            if lims is not None:\n                xmin, xmax, ymin, ymax = lims\n            elif pcs.shape[1] < 30:\n                xmin, xmax, ymin, ymax = -1, 1, -1, 1\n            else:\n                xmin, xmax, ymin, ymax = min(pcs[d1, :]), max(pcs[d1, :]), min(\n                    pcs[d2, :]), max(pcs[d2, :])\n\n            # arrow display\n            # if there are more than 30 arrows, the triangle is not displayed at their end\n            if pcs.shape[1] < 30:\n                plt.quiver(np.zeros(pcs.shape[1]),\n                           np.zeros(pcs.shape[1]),\n                           pcs[d1, :],\n                           pcs[d2, :],\n                           angles='xy',\n                           scale_units='xy',\n                           scale=1,\n                           color=\"grey\")\n            else:\n                lines = [[[0, 0], [x, y]] for x, y in pcs[[d1, d2]].T]\n                ax.add_collection(\n                    LineCollection(lines, axes=ax, alpha=.1, color='black'))\n\n            # display of variable names\n            if labels is not None:\n                for i, (x, y) in enumerate(pcs[[d1, d2]].T):\n                    if x >= xmin and x <= xmax and y >= ymin and y <= ymax:\n                        plt.text(x,\n                                 y,\n                                 labels[i],\n                                 fontsize='22',\n                                 ha='center',\n                                 va='bottom',\n                                 rotation=label_rotation,\n                                 color=\"red\",\n                                 alpha=0.7,\n                        )\n\n            # circle display\n            circle = plt.Circle((0, 0), 1, facecolor='none', edgecolor='b')\n            plt.gca().add_artist(circle)\n\n            # setting graph limits\n            plt.xlim(xmin, xmax)\n            plt.ylim(ymin, ymax)\n\n            # display of horizontal and vertical lines\n            plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n            plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n\n            # names of the axes, with the percentage of inertia explained\n            plt.xlabel('F{} ({}%)'.format(\n                d1 + 1, round(100 * pca.explained_variance_ratio_[d1], 1)),\n                       fontsize=14)\n            plt.ylabel('F{} ({}%)'.format(\n                d2 + 1, round(100 * pca.explained_variance_ratio_[d2], 1)),\n                       fontsize=14)\n\n            plt.title(\"Circle of correlations (F{} and F{})\".format(\n                d1 + 1, d2 + 1),\n                      size=24)\n\n            plt.legend(labels,\n                       pca.feature_names_in_,\n                       handler_map={int: IntHandler()},\n                       bbox_to_anchor=(1,1))\n            \n            plt.show()\n\n\n\n# Here a dataset for the example:\ncolumn_1 = [1, 2, 3, 4, 5, 6, 7 ,8]\ncolumn_2 = [4, 2, 9, 23, 3, 52, 41, 4]\ncolumn_3 = [9, 8, 7, 6, 6, 9, 24, 11]\ncolumn_4 = [45, 36, 74, 35, 29, 45, 29, 39]\ncolumn_5 = [35, 84, 3, 54, 68, 78, 65, 97]\ncolumn_6 = [24, 96, 7, 54, 67, 69, 88, 95]\ncolumn_7 = [5, 39, 72, 42, 22, 41, 24, 41]\ncolumn_8 = [30, 98, 8, 67, 68, 41, 27, 87]\n\n\ndf = pd.DataFrame({'column_1': column_1,\n                  'column_2': column_2,\n                  'column_3': column_3, \n                  'column_4': column_4,\n                  'column_5': column_5,\n                  'column_6': column_6,\n                  'column_7': column_7, \n                  'column_8': column_8})\n\n\npca_data = preprocessing.scale(df)\n\npca = decomposition.PCA(n_components = 8)\npca.fit(pd.DataFrame(pca_data, columns=df.columns))\n\n# We set the number of components\nn_comp = 2\n# Recovery of the components of the pca object\npcs = pca.components_\n\ndisplay_circles(pcs, n_comp, pca, [(0, 1), (0, 2)])\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How to bind html in Angular 8\r\n                \r\nI am struggling to display html in Angular 8. I am developing an app in angular 8 and my back-end code is in Flask. My front-end is receiving a JSON data which is a html template. Now I need to display that template in Angular.\n\nThe JSON data I am receiving is given below:\n\n`\n\n```\n<html>\n <head>\n   <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n   <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css\">\n   <style>body{ margin:0 100; background:whitesmoke; }</style>\n </head>\n <body>\n   <h1></h1>\n<div>\n<div id=\"bff3697b-6ab4-4b03-9932-87484de30381\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>\n<script type=\"text/javascript\">\n\n     window.PLOTLYENV=window.PLOTLYENV || {};\n\n if (document.getElementById(\"bff3697b-6ab4-4b03-9932-87484de30381\")) {\n     Plotly.newPlot(\n         'bff3697b-6ab4-4b03-9932-87484de30381',\n         [{\"histnorm\": \"probability\", \"opacity\": 0.8, \"type\": \"histogram\", \"x\": [7665.7, 16227.56, 5445.08, 4294.01, 3047.38, 5792.88, 7783.29, 10969.92, 3976.19, 13985.95, 6483.98, 5076.62, 3846.69, 7423.5, 6705.06, 4447.09, 5558.81, 5744.26, 5157.67, 5464.3, 4803.11, 6736.29, 4400.25, 4484.76, 5003.46, 6313.67, 4096.05, 7641.69, 7888.45, 13288.3, 16026.95, 5317.73, 6627.48, 10342.09, 9298.19, 5579.38, 5891.89, 5705.15, 5392.63, 5067.95, 5657.95, 5476.77, 3743.82, 7279.83, 3888.18, 4015.99, 5205.88, 6687.31, 5946.34, 3610.91, 3973.64, 3195.0, 11049.83, 5227.45, 6193.73, 6591.53, 7057.34, 5737.5, 5258.5, 5069.92, 7953.26, 9283.54, 20324.1, 6967.87, 7435.68, 8926.81, 15106.95, 9990.29, 8352.12, 9858.95, 12826.69, 10861.36, 11405.47, 8848.72, 6619.18, 12410.85, 16577.89, 16159.03, 5420.87, 24308.08, 16805.05, 11619.75, 14463.16, 9663.49, 13049.21, 12432.9, 18482.13, 19264.55, 8231.32, 7914.63, 5614.09, 7130.98, 17311.07, 17664.37, 16790.96, 7568.41, 13016.7, 13253.78, 11118.24, 9756.02, 7754.46, 16462.75, 9420.9, 5456.42, 5342.85, 13557.58, 6291.29, 7484.99, 7330.86, 16943.34, 23108.17, 11866.71, 24322.77, 17990.83, 10609.29, 11754.42, 16209.23, 20445.39, 16411.1, 13269.05, 15041.91, 9685.15, 32292.39, 11588.38, 15466.97, 21857.76, 19501.92, 13998.62, 23485.92, 30581.47, 18229.58, 14396.43, 17588.07, 15501.08, 8244.17, 12275.61, 14533.65, 17338.42, 13910.32, 24438.67, 22122.61, 17292.22, 18912.3, 13955.66, 13300.51, 11405.18, 14351.98, 15731.26, 33969.35, 10393.69, 11834.89, 11072.75, 18563.32, 12115.78, 11548.46, 15311.1, 17217.91, 16151.7, 18554.97, 14446.26, 11165.57, 15348.44, 19014.68, 18717.08, 11900.41, 17082.0, 15714.11, 14843.93, 23042.99]}, {\"line\": {\"color\": \"black\"}, \"opacity\": 0.8, \"type\": \"scatter\", \"x\": [3047.38, 3359.7231313131315, 3672.066262626263, 3984.409393939394, 4296.752525252525, 4609.095656565656, 4921.4387878787875, 5233.781919191919, 5546.12505050505, 5858.468181818182, 6170.811313131313, 6483.154444444444, 6795.497575757576, 7107.840707070707, 7420.183838383838, 7732.526969696969, 8044.8701010101, 8357.213232323233, 8669.556363636362, 8981.899494949495, 9294.242626262625, 9606.585757575756, 9918.928888888888, 10231.27202020202, 10543.61515151515, 10855.958282828282, 11168.301414141413, 11480.644545454543, 11792.987676767676, 12105.330808080806, 12417.673939393939, 12730.017070707068, 13042.360202020202, 13354.703333333331, 13667.046464646464, 13979.389595959594, 14291.732727272727, 14604.075858585857, 14916.41898989899, 15228.76212121212, 15541.105252525253, 15853.448383838382, 16165.791515151512, 16478.134646464645, 16790.477777777774, 17102.820909090908, 17415.164040404037, 17727.50717171717, 18039.8503030303, 18352.193434343433, 18664.536565656563, 18976.879696969696, 19289.222828282826, 19601.56595959596, 19913.90909090909, 20226.25222222222, 20538.59535353535, 20850.938484848484, 21163.281616161614, 21475.624747474747, 21787.967878787877, 22100.31101010101, 22412.65414141414, 22724.997272727273, 23037.340404040402, 23349.68353535353, 23662.026666666665, 23974.369797979794, 24286.712929292928, 24599.056060606057, 24911.39919191919, 25223.74232323232, 25536.085454545453, 25848.428585858583, 26160.771717171716, 26473.114848484845, 26785.45797979798, 27097.80111111111, 27410.14424242424, 27722.48737373737, 28034.830505050504, 28347.173636363634, 28659.516767676763, 28971.859898989896, 29284.203030303026, 29596.54616161616, 29908.88929292929, 30221.232424242422, 30533.57555555555, 30845.918686868685, 31158.261818181814, 31470.604949494948, 31782.948080808077, 32095.29121212121, 32407.63434343434, 32719.977474747473, 33032.3206060606, 33344.66373737373, 33657.00686868686, 33969.35], \"y\": [0.249283457622531, 0.2674333873470217, 0.2861467938154564, 0.3053607661797153, 0.3250039824583579, 0.34499692347945327, 0.3652522125488463, 0.3856750833735535, 0.40616397648807745, 0.42661126200403043, 0.44690408398108245, 0.46692531915597935, 0.4865546402264487, 0.5056696714311, 0.5241472218586721, 0.5418645798229751, 0.5587008498135356, 0.5745383120314507, 0.5892637833938382, 0.6027699581788769, 0.6149567062172908, 0.6257323067348848, 0.6350145966220898, 0.6427320130458113, 0.6488245119089158, 0.6532443456739198, 0.6559566864585457, 0.6569400830298694, 0.6561867433094863, 0.6537026371855928, 0.6495074177344868, 0.6436341623055888, 0.636128938241205, 0.6270502012064288, 0.6164680371206057, 0.6044632614398879, 0.5911263919781937, 0.5765565135180727, 0.5608600541108191, 0.5441494941655111, 0.5265420301605734, 0.5081582150726898, 0.489120597412479, 0.4695523801024612, 0.4495761193595623, 0.42931248229090196, 0.4088790801254196, 0.388389391939393, 0.36795179145053275, 0.3476686870155601, 0.32763578243376934, 0.30794146359709057, 0.288666313496316, 0.26988275565011915, 0.2516548237196403, 0.23403805295152275, 0.21707948719385872, 0.20081779358192822, 0.18528347561510858, 0.17049917425564357, 0.15648004587882092, 0.14323420538945944, 0.1307632225814578, 0.1190626598392077, 0.10812263954041262, 0.09792842999345658, 0.08846103939997974, 0.07969780814361545, 0.07161299063678907, 0.06417831897683102, 0.057363541738973275, 0.05113693233729595, 0.04546576248778323, 0.040316737385615496, 0.035656390240083526, 0.031451434776970426, 0.027669075205349968, 0.024277273942556778, 0.021244978090104335, 0.01854230625037641, 0.01614069776781578, 0.014013026870604623, 0.01213368448330337, 0.010478630683371588, 0.009025420892231873, 0.007753208933006081, 0.006642730061455654, 0.0056762669936145475, 0.004837601822833305, 0.004111956549991866, 0.0034859247526226496, 0.0029473967001460246, 0.002485479991154406, 0.0020904175516451773, 0.0017535045963510698, 0.0014670059239780112, 0.0012240746954412796, 0.001018673635405657, 0.0008454994040622246, 0.0006999107098184589]}],\n         {\"showlegend\": false, \"template\": {\"data\": {\"scatter\": [{\"type\": \"scatter\"}]}}, \"title\": {\"text\": \"Total Volume: Histogram and Normalized-Density\"}},\n         {\"responsive\": true}\n     )\n };\n\n</script>\n</div>\n </body>\n</html>\n```\n\n\n` \nThe issue I am facing is that I am unable to display the plot which is in above html template in Angular. The same template if I save it in a file with .html extension and then try to open that file in browser the get to see the plot in browser but same html template do not bind in angular 8, or rather not displaying it.\n\nMy app.component.html:\n\n```\n<div [innerHtml] = \"someData\"></div>\n```\n\n\nMy app.component.ts:\n\n```\nimport { Component, OnInit, ViewChild} from '@angular/core';\n\n@Component({\n  selector: 'app-data-decomposition',\n  templateUrl: './data-decomposition.component.html',\n  styleUrls: ['./data-decomposition.component.css']\n\n})\nexport class DataDecompositionComponent implements OnInit {\n someData = `<html>\n <head>\n   <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n   <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css\">\n   <style>body{ margin:0 100; background:whitesmoke; }</style>\n </head>\n <body>\n   <h1></h1>\n   <div>\n\n\n<div id=\"bff3697b-6ab4-4b03-9932-87484de30381\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>\n<script type=\"text/javascript\">\n\n     window.PLOTLYENV=window.PLOTLYENV || {};\n\n if (document.getElementById(\"bff3697b-6ab4-4b03-9932-87484de30381\")) {\n     Plotly.newPlot(\n         'bff3697b-6ab4-4b03-9932-87484de30381',\n         [{\"histnorm\": \"probability\", \"opacity\": 0.8, \"type\": \"histogram\", \"x\": [7665.7, 16227.56, 5445.08, 4294.01, 3047.38, 5792.88, 7783.29, 10969.92, 3976.19, 13985.95, 6483.98, 5076.62, 3846.69, 7423.5, 6705.06, 4447.09, 5558.81, 5744.26, 5157.67, 5464.3, 4803.11, 6736.29, 4400.25, 4484.76, 5003.46, 6313.67, 4096.05, 7641.69, 7888.45, 13288.3, 16026.95, 5317.73, 6627.48, 10342.09, 9298.19, 5579.38, 5891.89, 5705.15, 5392.63, 5067.95, 5657.95, 5476.77, 3743.82, 7279.83, 3888.18, 4015.99, 5205.88, 6687.31, 5946.34, 3610.91, 3973.64, 3195.0, 11049.83, 5227.45, 6193.73, 6591.53, 7057.34, 5737.5, 5258.5, 5069.92, 7953.26, 9283.54, 20324.1, 6967.87, 7435.68, 8926.81, 15106.95, 9990.29, 8352.12, 9858.95, 12826.69, 10861.36, 11405.47, 8848.72, 6619.18, 12410.85, 16577.89, 16159.03, 5420.87, 24308.08, 16805.05, 11619.75, 14463.16, 9663.49, 13049.21, 12432.9, 18482.13, 19264.55, 8231.32, 7914.63, 5614.09, 7130.98, 17311.07, 17664.37, 16790.96, 7568.41, 13016.7, 13253.78, 11118.24, 9756.02, 7754.46, 16462.75, 9420.9, 5456.42, 5342.85, 13557.58, 6291.29, 7484.99, 7330.86, 16943.34, 23108.17, 11866.71, 24322.77, 17990.83, 10609.29, 11754.42, 16209.23, 20445.39, 16411.1, 13269.05, 15041.91, 9685.15, 32292.39, 11588.38, 15466.97, 21857.76, 19501.92, 13998.62, 23485.92, 30581.47, 18229.58, 14396.43, 17588.07, 15501.08, 8244.17, 12275.61, 14533.65, 17338.42, 13910.32, 24438.67, 22122.61, 17292.22, 18912.3, 13955.66, 13300.51, 11405.18, 14351.98, 15731.26, 33969.35, 10393.69, 11834.89, 11072.75, 18563.32, 12115.78, 11548.46, 15311.1, 17217.91, 16151.7, 18554.97, 14446.26, 11165.57, 15348.44, 19014.68, 18717.08, 11900.41, 17082.0, 15714.11, 14843.93, 23042.99]}, {\"line\": {\"color\": \"black\"}, \"opacity\": 0.8, \"type\": \"scatter\", \"x\": [3047.38, 3359.7231313131315, 3672.066262626263, 3984.409393939394, 4296.752525252525, 4609.095656565656, 4921.4387878787875, 5233.781919191919, 5546.12505050505, 5858.468181818182, 6170.811313131313, 6483.154444444444, 6795.497575757576, 7107.840707070707, 7420.183838383838, 7732.526969696969, 8044.8701010101, 8357.213232323233, 8669.556363636362, 8981.899494949495, 9294.242626262625, 9606.585757575756, 9918.928888888888, 10231.27202020202, 10543.61515151515, 10855.958282828282, 11168.301414141413, 11480.644545454543, 11792.987676767676, 12105.330808080806, 12417.673939393939, 12730.017070707068, 13042.360202020202, 13354.703333333331, 13667.046464646464, 13979.389595959594, 14291.732727272727, 14604.075858585857, 14916.41898989899, 15228.76212121212, 15541.105252525253, 15853.448383838382, 16165.791515151512, 16478.134646464645, 16790.477777777774, 17102.820909090908, 17415.164040404037, 17727.50717171717, 18039.8503030303, 18352.193434343433, 18664.536565656563, 18976.879696969696, 19289.222828282826, 19601.56595959596, 19913.90909090909, 20226.25222222222, 20538.59535353535, 20850.938484848484, 21163.281616161614, 21475.624747474747, 21787.967878787877, 22100.31101010101, 22412.65414141414, 22724.997272727273, 23037.340404040402, 23349.68353535353, 23662.026666666665, 23974.369797979794, 24286.712929292928, 24599.056060606057, 24911.39919191919, 25223.74232323232, 25536.085454545453, 25848.428585858583, 26160.771717171716, 26473.114848484845, 26785.45797979798, 27097.80111111111, 27410.14424242424, 27722.48737373737, 28034.830505050504, 28347.173636363634, 28659.516767676763, 28971.859898989896, 29284.203030303026, 29596.54616161616, 29908.88929292929, 30221.232424242422, 30533.57555555555, 30845.918686868685, 31158.261818181814, 31470.604949494948, 31782.948080808077, 32095.29121212121, 32407.63434343434, 32719.977474747473, 33032.3206060606, 33344.66373737373, 33657.00686868686, 33969.35], \"y\": [0.249283457622531, 0.2674333873470217, 0.2861467938154564, 0.3053607661797153, 0.3250039824583579, 0.34499692347945327, 0.3652522125488463, 0.3856750833735535, 0.40616397648807745, 0.42661126200403043, 0.44690408398108245, 0.46692531915597935, 0.4865546402264487, 0.5056696714311, 0.5241472218586721, 0.5418645798229751, 0.5587008498135356, 0.5745383120314507, 0.5892637833938382, 0.6027699581788769, 0.6149567062172908, 0.6257323067348848, 0.6350145966220898, 0.6427320130458113, 0.6488245119089158, 0.6532443456739198, 0.6559566864585457, 0.6569400830298694, 0.6561867433094863, 0.6537026371855928, 0.6495074177344868, 0.6436341623055888, 0.636128938241205, 0.6270502012064288, 0.6164680371206057, 0.6044632614398879, 0.5911263919781937, 0.5765565135180727, 0.5608600541108191, 0.5441494941655111, 0.5265420301605734, 0.5081582150726898, 0.489120597412479, 0.4695523801024612, 0.4495761193595623, 0.42931248229090196, 0.4088790801254196, 0.388389391939393, 0.36795179145053275, 0.3476686870155601, 0.32763578243376934, 0.30794146359709057, 0.288666313496316, 0.26988275565011915, 0.2516548237196403, 0.23403805295152275, 0.21707948719385872, 0.20081779358192822, 0.18528347561510858, 0.17049917425564357, 0.15648004587882092, 0.14323420538945944, 0.1307632225814578, 0.1190626598392077, 0.10812263954041262, 0.09792842999345658, 0.08846103939997974, 0.07969780814361545, 0.07161299063678907, 0.06417831897683102, 0.057363541738973275, 0.05113693233729595, 0.04546576248778323, 0.040316737385615496, 0.035656390240083526, 0.031451434776970426, 0.027669075205349968, 0.024277273942556778, 0.021244978090104335, 0.01854230625037641, 0.01614069776781578, 0.014013026870604623, 0.01213368448330337, 0.010478630683371588, 0.009025420892231873, 0.007753208933006081, 0.006642730061455654, 0.0056762669936145475, 0.004837601822833305, 0.004111956549991866, 0.0034859247526226496, 0.0029473967001460246, 0.002485479991154406, 0.0020904175516451773, 0.0017535045963510698, 0.0014670059239780112, 0.0012240746954412796, 0.001018673635405657, 0.0008454994040622246, 0.0006999107098184589]}],\n         {\"showlegend\": false, \"template\": {\"data\": {\"scatter\": [{\"type\": \"scatter\"}]}}, \"title\": {\"text\": \"Total Volume: Histogram and Normalized-Density\"}},\n         {\"responsive\": true}\n     )\n };\n\n</script>\n</div>\n </body>\n</html>`\n\nngOnInit() { }\n\n}\n```\n\n\nIn the above JSON data if I add for example ```\n<h1> or <p>```\n tag its showing ```\nh1```\n and ```\np```\n tag content on Angular 8 but not displaying plotly plot. I think there is some issue displaying the plotly plot. Do I need to install any additional package to do this, as I already install plotly.js for my angular project. \n    ", "Answer": "\r\nIn Angular, you can use ```\ninnerHTML```\n in your template, like this:\n\n```\n<div [innerHTML]=\"theHtmlString\"></div>\n```\n\n\nSomewhere in your Typescript component, you can assign the HTML value to ```\ntheHtmlString```\n:\n\n```\n@Component({\n...\n})\nexport class MyComponent {\n  theHtmlString: string;\n\n  constructor() {}\n\n  async ngOnInit() {\n    this.theHtmlString = await getHtmlPlotFromAPI();\n  }  \n}\n```\n\n\nNote that it is typically better practice to return raw JSON data from your backend and bind it to your Angular template on the client side.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "How to run a .py module?\r\n                \r\nI've got zero experience with Python. I have looked around some tutorial materials, but it seems difficult to understand a advanced code. So I came here for a more specific answer. \nFor me the mission is to redo the code in my computer. \n\nHere is the scenario:\n\nI'm a graduate student studying tensor factorization in relation learning. A paper[1] providing a code to run this algorithm, as follows:\n\n```\nimport logging, time\nfrom numpy import dot, zeros, kron, array, eye, argmax\nfrom numpy.linalg import qr, pinv, norm, inv \nfrom scipy.linalg import eigh\nfrom numpy.random import rand\n\n__version__ = \"0.1\" \n__all__ = ['rescal', 'rescal_with_random_restarts']\n\n__DEF_MAXITER = 500\n__DEF_INIT = 'nvecs'\n__DEF_PROJ = True\n__DEF_CONV = 1e-5\n__DEF_LMBDA = 0\n\n_log = logging.getLogger('RESCAL') \n\ndef rescal_with_random_restarts(X, rank, restarts=10, **kwargs):\n    \"\"\"\n    Restarts RESCAL multiple time from random starting point and \n    returns factorization with best fit.\n    \"\"\"\n    models = []\n    fits = []\n    for i in range(restarts):\n        res = rescal(X, rank, init='random', **kwargs)\n        models.append(res)\n        fits.append(res[2])\n    return models[argmax(fits)]\n\ndef rescal(X, rank, **kwargs):\n    \"\"\"\n    RESCAL \n\n    Factors a three-way tensor X such that each frontal slice \n    X_k = A * R_k * A.T. The frontal slices of a tensor are \n    N x N matrices that correspond to the adjecency matrices \n    of the relational graph for a particular relation.\n\n    For a full description of the algorithm see: \n      Maximilian Nickel, Volker Tresp, Hans-Peter-Kriegel, \n      \"A Three-Way Model for Collective Learning on Multi-Relational Data\",\n      ICML 2011, Bellevue, WA, USA\n\n    Parameters\n    ----------\n    X : list\n        List of frontal slices X_k of the tensor X. The shape of each X_k is ('N', 'N')\n    rank : int \n        Rank of the factorization\n    lmbda : float, optional \n        Regularization parameter for A and R_k factor matrices. 0 by default \n    init : string, optional\n        Initialization method of the factor matrices. 'nvecs' (default) \n        initializes A based on the eigenvectors of X. 'random' initializes \n        the factor matrices randomly.\n    proj : boolean, optional \n        Whether or not to use the QR decomposition when computing R_k.\n        True by default \n    maxIter : int, optional \n        Maximium number of iterations of the ALS algorithm. 500 by default. \n    conv : float, optional \n        Stop when residual of factorization is less than conv. 1e-5 by default\n\n    Returns \n    -------\n    A : ndarray \n        array of shape ('N', 'rank') corresponding to the factor matrix A\n    R : list\n        list of 'M' arrays of shape ('rank', 'rank') corresponding to the factor matrices R_k \n    f : float \n        function value of the factorization \n    iter : int \n        number of iterations until convergence \n    exectimes : ndarray \n        execution times to compute the updates in each iteration\n    \"\"\"\n\n    # init options\n    ainit = kwargs.pop('init', __DEF_INIT)\n    proj = kwargs.pop('proj', __DEF_PROJ)\n    maxIter = kwargs.pop('maxIter', __DEF_MAXITER)\n    conv = kwargs.pop('conv', __DEF_CONV)\n    lmbda = kwargs.pop('lmbda', __DEF_LMBDA)\n    if not len(kwargs) == 0:\n        raise ValueError( 'Unknown keywords (%s)' % (kwargs.keys()) )\n\n    sz = X[0].shape\n    dtype = X[0].dtype \n    n = sz[0]\n    k = len(X) \n\n    _log.debug('[Config] rank: %d | maxIter: %d | conv: %7.1e | lmbda: %7.1e' % (rank, \n        maxIter, conv, lmbda))\n    _log.debug('[Config] dtype: %s' % dtype)\n\n    # precompute norms of X \n    normX = [norm(M)**2 for M in X]\n    Xflat = [M.flatten() for M in X]\n    sumNormX = sum(normX)\n\n    # initialize A\n    if ainit == 'random':\n        A = array(rand(n, rank), dtype=dtype)\n    elif ainit == 'nvecs':\n        S = zeros((n, n), dtype=dtype)\n        T = zeros((n, n), dtype=dtype)\n        for i in range(k):\n            T = X[i]\n            S = S + T + T.T\n        evals, A = eigh(S,eigvals=(n-rank,n-1))\n    else :\n        raise 'Unknown init option (\"%s\")' % ainit\n\n    # initialize R\n    if proj:\n        Q, A2 = qr(A)\n        X2 = __projectSlices(X, Q)\n        R = __updateR(X2, A2, lmbda)\n    else :\n        R = __updateR(X, A, lmbda)\n\n    # compute factorization\n    fit = fitchange = fitold = f = 0\n    exectimes = []\n    ARAt = zeros((n,n), dtype=dtype)\n    for iter in xrange(maxIter):\n        tic = time.clock()\n        fitold = fit\n        A = __updateA(X, A, R, lmbda)\n        if proj:\n            Q, A2 = qr(A)\n            X2 = __projectSlices(X, Q)\n            R = __updateR(X2, A2, lmbda)\n        else :\n            R = __updateR(X, A, lmbda)\n\n        # compute fit value\n        f = lmbda*(norm(A)**2)\n        for i in range(k):\n            ARAt = dot(A, dot(R[i], A.T))\n            f += normX[i] + norm(ARAt)**2 - 2*dot(Xflat[i], ARAt.flatten()) + lmbda*(R[i].flatten()**2).sum()\n        f *= 0.5\n\n        fit = 1 - f / sumNormX\n        fitchange = abs(fitold - fit)\n\n        toc = time.clock()\n        exectimes.append( toc - tic )\n        _log.debug('[%3d] fit: %.5f | delta: %7.1e | secs: %.5f' % (iter, \n            fit, fitchange, exectimes[-1]))\n        if iter > 1 and fitchange < conv:\n            break\n    return A, R, f, iter+1, array(exectimes)\n\ndef __updateA(X, A, R, lmbda):\n    n, rank = A.shape\n    F = zeros((n, rank), dtype=X[0].dtype)\n    E = zeros((rank, rank), dtype=X[0].dtype)\n\n    AtA = dot(A.T,A)\n    for i in range(len(X)):\n        F += dot(X[i], dot(A, R[i].T)) + dot(X[i].T, dot(A, R[i]))\n        E += dot(R[i], dot(AtA, R[i].T)) + dot(R[i].T, dot(AtA, R[i]))\n    A = dot(F, inv(lmbda * eye(rank) + E))\n    return A\n\ndef __updateR(X, A, lmbda):\n    r = A.shape[1]\n    R = []\n    At = A.T    \n    if lmbda == 0:\n        ainv = dot(pinv(dot(At, A)), At)\n        for i in range(len(X)):\n            R.append( dot(ainv, dot(X[i], ainv.T)) )\n    else :\n        AtA = dot(At, A)\n        tmp = inv(kron(AtA, AtA) + lmbda * eye(r**2))\n        for i in range(len(X)):\n            AtXA = dot(At, dot(X[i], A)) \n            R.append( dot(AtXA.flatten(), tmp).reshape(r, r) )\n    return R\n\ndef __projectSlices(X, Q):\n    q = Q.shape[1]\n    X2 = []\n    for i in range(len(X)):\n        X2.append( dot(Q.T, dot(X[i], Q)) )\n    return X2\n```\n\n\nIt's boring to paste such a long code but there is no other way to figure out my problems. I'm sorry about this.\n\nI import this module and pass them arguments according to the author's website:\n\n```\nimport pickle, sys\nfrom rescal import rescal\n\nrank = sys.argv[1]\nX = pickle.load('us-presidents.pickle')\nA, R, f, iter, exectimes = rescal(X, rank, lmbda=1.0)\n```\n\n\nThe dataset us-presidents.rdf can be found here.\n\nMy questions are:\n\n\nAccording to the code note, the tensor X is a list. I don't quite understand this, how do I relate a list to a tensor in Python? Can I understand tensor = list in Python?\nShould I convert RDF format to a triple(subject, predicate, object) format first? I'm not sure of the data structure of X. How do I assignment values to X by hand?\nThen, how to run it?\n\n\nI paste the author's code without his authorization, is it an act of infringement? if so, I am so sorry and I will delete it soon. \n\nThe problems may be a little bored, but these are important to me. Any help would be greatly appreciated.\n\n[1] Maximilian Nickel, Volker Tresp, Hans-Peter Kriegel,\nA Three-Way Model for Collective Learning on Multi-Relational Data,\nin Proceedings of the 28th International Conference on Machine Learning, 2011 , Bellevue, WA, USA\n    ", "Answer": "\r\nTo answer Q2: you need to transform the RDF and save it before you can load it from the file 'us-presidents.pickle'. The author of that code probably did that once because the Python native pickle format loads faster. As the pickle format includes the datatype of the data, it is possible that ```\nX```\n is some numpy class instance and you would need either an example pickle file as used by this code, or some code doing the pickle.dump to figure out how to convert from RDF to this particular pickle file as ```\nrescal```\n expects it.\n\nSo this might answer Q1: the tensor consists of a list of elements. From the code you can see  that the ```\nX```\n parameter to rescal has a length (```\nk = len(X)```\n ) and can be indexed (```\nT = X[i]```\n). So it elements are used as a list (even if it might be some other datatype, that just behaves as such.\n\nAs an aside: If you are not familiar with Python and are just interested in the result of the computation, you might get more help contacting the author of the software.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "ARIMA Model - MissingDataError: exog contains inf or nans\r\n                \r\nI am trying to forecast few values using ARIMA Model. I get the following error. I have tried to remove the stationarity and other necessary conditions for the forecasting. Can someone point me out why this error is generated and how to fix this? Im new to Python. Thanks in advance. \n\nError completer error tree as follows.\n\n```\nMissingDataError                          Traceback (most recent call last)\n<ipython-input-7-35993c1e078a> in <module>\n 37 from statsmodels.tsa.stattools import adfuller\n 38 print(\"Results of Dickey-Fuller Test:\")\n ---> 39 dftest = adfuller(indexedDataset[\"like\"], autolag='AIC')\n 40 \n 41 dfoutput = pd.Series(dftest[0:4],index=['Test Statistics','p-value', \n'#Lags Used','#Number of observations used'])\n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\stattools.py in adfuller(x, maxlag, regression, autolag, store, regresults)\n239         if not regresults:\n240             icbest, bestlag = _autolag(OLS, xdshort, fullRHS, startlag,\n--> 241                                        maxlag, autolag)\n242         else:\n243             icbest, bestlag, alres = _autolag(OLS, xdshort, fullRHS, \nstartlag,\n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site- \npackages\\statsmodels\\tsa\\stattools.py in _autolag(mod, endog, exog, \nstartlag, maxlag, method, modargs, fitargs, regresults)\n 84     method = method.lower()\n 85     for lag in range(startlag, startlag + maxlag + 1):\n ---> 86         mod_instance = mod(endog, exog[:, :lag], *modargs)\n 87         results[lag] = mod_instance.fit()\n 88 \n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py in __init__(self, endog, exog, missing, hasconst, **kwargs)\n815                  **kwargs):\n816         super(OLS, self).__init__(endog, exog, missing=missing,\n--> 817                                   hasconst=hasconst, **kwargs)\n818         if \"weights\" in self._init_keys:\n819             self._init_keys.remove(\"weights\")\n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py in __init__(self, endog, exog, weights, missing, hasconst, **kwargs)\n661             weights = weights.squeeze()\n662         super(WLS, self).__init__(endog, exog, missing=missing,\n--> 663                                   weights=weights, hasconst=hasconst, **kwargs)\n664         nobs = self.exog.shape[0]\n665         weights = self.weights\n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py in __init__(self, endog, exog, **kwargs)\n177     \"\"\"\n178     def __init__(self, endog, exog, **kwargs):\n--> 179         super(RegressionModel, self).__init__(endog, exog, **kwargs)\n180         self._data_attr.extend(['pinv_wexog', 'wendog', 'wexog', \n'weights'])\n181 \n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py in __init__(self, endog, exog, **kwargs)\n210 \n211     def __init__(self, endog, exog=None, **kwargs):\n--> 212         super(LikelihoodModel, self).__init__(endog, exog, **kwargs)\n213         self.initialize()\n214 \n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py in __init__(self, endog, exog, **kwargs)\n 62         hasconst = kwargs.pop('hasconst', None)\n 63         self.data = self._handle_data(endog, exog, missing, hasconst,\n ---> 64                                       **kwargs)\n 65         self.k_constant = self.data.k_constant\n 66         self.exog = self.data.exog\n\n ~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py in _handle_data(self, endog, exog, missing, hasconst, **kwargs)\n 85 \n 86     def _handle_data(self, endog, exog, missing, hasconst, **kwargs):\n ---> 87         data = handle_data(endog, exog, missing, hasconst, **kwargs)\n 88         # kwargs arrays could have changed, easier to just attach here\n 89         for key in kwargs:\n\n ~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\base\\data.py in handle_data(endog, exog, missing, hasconst, **kwargs)\n631     klass = handle_data_class_factory(endog, exog)\n632     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n--> 633                  **kwargs)\n\n ~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\base\\data.py in __init__(self, endog, exog, missing, hasconst, **kwargs)\n 77 \n 78         # this has side-effects, attaches k_constant and const_idx\n ---> 79         self._handle_constant(hasconst)\n 80         self._check_integrity()\n 81         self._cache = resettable_cache()\n\n ~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\base\\data.py in _handle_constant(self, hasconst)\n131             ptp_ = self.exog.ptp(axis=0)\n132             if not np.isfinite(ptp_).all():\n--> 133                 raise MissingDataError('exog contains inf or nans')\n134             const_idx = np.where(ptp_ == 0)[0].squeeze()\n135             self.k_constant = const_idx.size\n\nMissingDataError: exog contains inf or nans\n```\n\n\n\n\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 10, 6\n\ndataset = pd.read_csv(\"data.csv\")\n#Parse strings to datetime type\ndataset['Date'] = pd.to_datetime(dataset['Date'], \ninfer_datetime_format=True)\nindexedDataset = dataset.set_index(['Date'])\n\nfrom datetime import datetime\nindexedDataset.tail(5)\n\n#plot graph\nplt.xlabel(\"Date\")\nplt.ylabel(\"Number of Likes\")\nplt.plot(indexedDataset)\n\n#Determining the rolling statistics\nrolmean = indexedDataset.rolling(window=12).mean()\n\nrolstd = indexedDataset.rolling(window=12).std()\nprint(rolmean, rolstd)\n\n#plot tolling statistics\norig = plt.plot(indexedDataset, color=\"blue\", label=\"original\")\nmean = plt.plot(rolmean, color=\"red\", label=\"Rolling Mean\")\nstd = plt.plot(rolstd, color=\"black\", label= \"Rolling std\")\nplt.legend(loc=\"best\")\nplt.title=(\"Rolling Mean and Standard Deviation\")\n\n#Perform Dickey-Fuller test\nfrom statsmodels.tsa.stattools import adfuller\nprint(\"Results of Dickey-Fuller Test:\")\ndftest = adfuller(indexedDataset[\"like\"], autolag='AIC')\n\ndfoutput = pd.Series(dftest[0:4],index=['Test Statistics','p-value', '#Lags \nUsed','#Number of observations used'])\nfor key, value in dftest[4].items():\ndfoutput['Critical Value (%s)' %key] = value\n\nprint(dfoutput)\n\n#Estimating trend\nindexedDataset_logScale = np.log(indexedDataset)\nplt.plot(indexedDataset_logScale)\n\nmovingAverage = indexedDataset_logScale.rolling(window=12).mean()\nmovingSTD = indexedDataset_logScale.rolling(window=12).std()\nplt.plot(indexedDataset_logScale)\nplt.plot(movingAverage, color=\"red\")\n\ndatasetLogScaleMinusMovingAverage = indexedDataset_logScale - movingAverage\ndatasetLogScaleMinusMovingAverage.head(12)\n\n#remove Nan Values\ndatasetLogScaleMinusMovingAverage.dropna(inplace=True)\ndatasetLogScaleMinusMovingAverage.head(10)\n\nfrom statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n\n#determing rolling statistics\nmovingAverage = timeseries.rolling(window=12).mean()\nmovingSTD = timeseries.rolling(window=12).std()\n\n#plot rolling statistics\norig = plt.plot(timeseries, color='blue',label='Original')\nmean = plt.plot(movingAverage, color='red', label='Rolling Mean')\nstd = plt.plot(movingSTD, color='black', label= 'Rolling std')\nplt.legend(loc='best')\nplt.title=(\"Rolling Mean and Standard Deviation\") \nplt.show(block=False)\n\n#Perform Dickey-Fuller test:\nprint('Results of Dickey-Fuller Test:')\ndftest = adfuller(indexedDataset[\"like\"], autolag='AIC')\ndfoutput = pd.Series(dftest[0:4],index=['Test Statistics','p-value', '#Lags \nUsed','#Number of observations used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\nprint(dfoutput)\n\n\ntest_stationarity(datasetLogScaleMinusMovingAverage)\n\nexponentialDecayWeightAverage = \nindexedDataset_logScale.ewm(halflife=12,min_periods=0,adjust=True).mean()\nplt.plot(indexedDataset_logScale)\nplt.plot(exponentialDecayWeightAverage, color='red')\n\ndatasetLogScaleMinusMovingAverageExponentialDecayAverage = \nindexedDataset_logScale - exponentialDecayWeightAverage\ntest_stationarity(datasetLogScaleMinusMovingAverageExponentialDecayAverage)\n\ndatasetLogDiffShifting = indexedDataset_logScale - \nindexedDataset_logScale.shift()\nplt.plot(datasetLogDiffShifting)\n\ndatasetLogDiffShifting.dropna(inplace=True)\ntest_stationarity(datasetLogDiffShifting)\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(indexedDataset_logScale)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(indexedDataset_logScale, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label=\"Seasonality\")\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()\n\ndecomposedLogData = residual\ndecomposedLogData.dropna(inplace=True)\ntest_stationarity(decomposedLogData)\n\ndecomposedLogData = residual\ndecomposedLogData.dropna(inplace=True)\ntest_stationarity(decomposedLogData)\n\n#ACF and PACF plates\nfrom statsmodels.tsa.stattools import acf, pacf\n\nlag_acf = acf(datasetLogDiffShifting, nlags=20)\nlag_pacf = pacf(datasetLogDiffShifting, nlags=20, method=\"ols\")\n\n#plot ACF\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)),linestyle='-- \n',color='gray')\n    plt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)),linestyle='-- \n',color='gray')\n# plt.title(\"Autocorrelation Function\")\n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)),linestyle='--',color='gray')\n    plt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)),linestyle='--',color='gray')\n# plt.title(\"Partial Autocorrelation Function\")\nplt.tight_layout()\n\nfrom statsmodels.tsa.arima_model import ARIMA\n\n#AR MODEL\nmodel = ARIMA(indexedDataset_logScale, order=(2, 1, 2))\nresult_AR = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(result_AR.fittedvalues, color='red')\nprint('RSS: %.4f'% sum((result_AR.fittedvalues- \ndatasetLogDiffShifting[\"like\"])**2))\nprint('Plotting AR model')\n\n#MA MODEL\nmodel = ARIMA(indexedDataset_logScale, order=(2,1,2))\nresults_MA = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(results_MA.fittedvalues, color='red')\nprint('RSS: %.4f'% sum((results_MA.fittedvalues- \ndatasetLogDiffShifting[\"like\"])**2))\nprint('Plotting AR model')\n\n model = ARIMA(indexedDataset_logScale, order=(2,1,2))\n results_ARIMA = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(results_ARIMA.fittedvalues, color=\"red\")\nprint('RSS: %.4f'% sum((results_MA.fittedvalues- \ndatasetLogDiffShifting[\"like\"])**2))\n\npredictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\nprint(predictions_ARIMA_diff.head())\n\n#Convert to cumulative sum\npredictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\nprint(predictions_ARIMA_diff_cumsum.head())\n\npredictions_ARIMA_log = pd.Series(indexedDataset_logScale[\"like\"].iloc[0], \nindex=indexedDataset_logScale.index)\npredictions_ARIMA_log = \npredictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\npredictions_ARIMA_log.head()\n\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(indexedDataset)\nplt.plot(predictions_ARIMA)\n\nindexedDataset_logScale\n\nresults_ARIMA.plot_predict(1,264)\n# x=results_ARIMA.forecast(steps=120)\n```\n\n    ", "Answer": "\r\nThere are some missing values in your dataset, you need to preprocess your data before passing it to the seasonal_decompose method.\n\n```\nindexedDataset = dataset.set_index(['Date'])\nindexedDataset = indexedDataset.fillna(method='ffill')\n```\n\n\nYou can also check other methods to fill your missing values from here\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "pip search finds tensorflow, but pip install does not\r\n                \r\nI am trying to build a Django app that would use Keras models to make recommendations. Right now I'm trying to use one custom container that would hold both Django and Keras. Here's the Dockerfile I've written.\n\n```\n# myproject/docker/app/Dockerfile\n\nFROM python:3.7-alpine  # I've tried 3.5, 3.6 and 3.7\n\nRUN apk add --no-cache postgresql-libs && \\\n  apk add --no-cache --virtual .build-deps \\\n    gfortran \\\n    build-base \\\n    freetype-dev \\\n    libpng-dev \\\n    openblas-dev \\\n    postgresql-dev \\\n    python3-dev \\\n    wget\n\nWORKDIR /app\nCOPY ./misc/requirements.txt /app/\nRUN pip search tensorflow\nRUN pip install tensorflow\nRUN pip install -r /app/requirements.txt\n\nCOPY . /app\n\nEXPOSE 8000\nENTRYPOINT [\"exec /start.sh\"]\n```\n\n\nProblem is, when I try to build app image, pip can't install tensorflow, even though ```\npip search tensorflow```\n lists tensorflow (1.12) in results.\n\n```\n$ docker-compose -f \"docker/docker-compose.yml\" --project-directory /path/to/myproject build\npsql uses an image, skipping\nredis uses an image, skipping\nBuilding app\nStep 1/12 : FROM python:3.7-alpine\n3.7-alpine: Pulling from library/python\ncd784148e348: Already exists\na5ca736b15eb: Already exists\nf320f547ff02: Pull complete\n2edd8ff8cb8f: Pull complete\n9381128744b2: Pull complete\nDigest: sha256:f708ad35a86f079e860ecdd05e1da7844fd877b58238e7a9a588b2ca3b1534d8\nStatus: Downloaded newer image for python:3.7-alpine\n ---> 1a8edcb29ce4\nStep 2/12 : ENV PYTHONBUFFERED 1\n ---> Running in 5178b24df888\nRemoving intermediate container 5178b24df888\n ---> 0f928fbf30f1\nStep 3/12 : RUN apk add --no-cache postgresql-libs &&   apk add --no-cache --virtual .build-deps     gfortran     build-base     freetype-dev     libpng-dev     openblas-dev     postgresql-dev     python3-dev     wget\n ---> Running in 2a8f4653e3f9\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz\n(1/5) Installing db (5.3.28-r0)\n(2/5) Installing libsasl (2.1.26-r14)\n(3/5) Installing libldap (2.4.46-r0)\n(4/5) Installing libpq (10.5-r0)\n(5/5) Installing postgresql-libs (10.5-r0)\nOK: 19 MiB in 39 packages\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz\n(1/36) Installing binutils (2.30-r5)\n(2/36) Installing gmp (6.1.2-r1)\n(3/36) Installing isl (0.18-r0)\n(4/36) Installing libgomp (6.4.0-r9)\n(5/36) Installing libatomic (6.4.0-r9)\n(6/36) Installing pkgconf (1.5.3-r0)\n(7/36) Installing libgcc (6.4.0-r9)\n(8/36) Installing mpfr3 (3.1.5-r1)\n(9/36) Installing mpc1 (1.0.3-r1)\n(10/36) Installing libstdc++ (6.4.0-r9)\n(11/36) Installing gcc (6.4.0-r9)\n(12/36) Installing libquadmath (6.4.0-r9)\n(13/36) Installing libgfortran (6.4.0-r9)\n(14/36) Installing gfortran (6.4.0-r9)\n(15/36) Installing libmagic (5.32-r0)\n(16/36) Installing file (5.32-r0)\n(17/36) Installing musl-dev (1.1.19-r10)\n(18/36) Installing libc-dev (0.7.1-r0)\n(19/36) Installing g++ (6.4.0-r9)\n(20/36) Installing make (4.2.1-r2)\n(21/36) Installing fortify-headers (0.9-r0)\n(22/36) Installing build-base (0.5-r1)\n(23/36) Installing libpng (1.6.34-r1)\n(24/36) Installing freetype (2.9.1-r1)\n(25/36) Installing zlib-dev (1.2.11-r1)\n(26/36) Installing libpng-dev (1.6.34-r1)\n(27/36) Installing freetype-dev (2.9.1-r1)\n(28/36) Installing openblas-ilp64 (0.3.0-r0)\n(29/36) Installing openblas (0.3.0-r0)\n(30/36) Installing openblas-dev (0.3.0-r0)\n(31/36) Installing libressl-dev (2.7.4-r0)\n(32/36) Installing postgresql-dev (10.5-r0)\n(33/36) Installing python3 (3.6.6-r0)\n(34/36) Installing python3-dev (3.6.6-r0)\n(35/36) Installing wget (1.19.5-r0)\n(36/36) Installing .build-deps (0)\nExecuting busybox-1.28.4-r2.trigger\nOK: 488 MiB in 75 packages\nRemoving intermediate container 2a8f4653e3f9\n ---> 0a6733c0891e\nStep 4/12 : WORKDIR /app\n ---> Running in e99a4dadbd78\nRemoving intermediate container e99a4dadbd78\n ---> 11d698c20e86\nStep 5/12 : COPY ./misc/requirements.txt /app/\n ---> aa6b85587b84\nStep 6/12 : RUN pip search tensorflow\n ---> Running in a4434a87e740\ntensorflow (1.12.0)                                      - TensorFlow is an open source machine learning framework for everyone.\ntensorflow-qndex (0.0.22)                                - tensorflow-qnd x tensorflow-extenteten\ntensorflow-estimator (1.10.12)                           - TensorFlow Estimator.\nmesh-tensorflow (0.0.5)                                  - Mesh TensorFlow\ntensorflow-io (0.1.0)                                    - TensorFlow IO\ntensorflow-plot (0.2.0)                                  - TensorFlow Plot\ntensorflow-lattice (0.9.8)                               - TensorFlow Lattice provides lattice models in TensorFlow\ntensorflow-datasets (0.0.2)                              - tensorflow/datasets is a library of datasets ready to use with TensorFlow.\ntensorflow-extenteten (0.0.22)                           - TensorFlow extention library\ncxflow-tensorflow (0.5.0)                                - TensorFlow extension for cxflow.\nemloop-tensorflow (0.1.0)                                - TensorFlow extension for emloop.\ntensorflow-k8s (0.0.2)                                   - Tensorflow serving extension\ntensorflow-transform (0.11.0)                            - A library for data preprocessing with TensorFlow\ndask-tensorflow (0.0.2)                                  - Interactions between Dask and Tensorflow\ntensorflow-tracer (1.1.0)                                - Runtime Tracing Library for TensorFlow\nsagemaker-tensorflow (1.12.0.1.0.0.post1)                - Amazon Sagemaker specific TensorFlow extensions.\ntensorflow-qnd (0.1.11)                                  - Quick and Dirty TensorFlow command framework\ntensorflow-probability (0.5.0)                           - Probabilistic modeling and statistical inference in TensorFlow\ntensorflow-utils (0.1.0)                                 - Classes and methods to make using TensorFlow easier\ntensorflow-model (0.1.1)                                 - Command-line tool to inspect TensorFlow models\ntensorflow-lattice-gpu (0.9.8)                           - TensorFlow Lattice provides lattice models in TensorFlow\ntensorflow-template (0.2)                                - A tensorflow template for quick starting a deep learning project.\ntensorflow-rocm (1.12.0)                                 - TensorFlow is an open source machine learning framework for everyone.\nintel-tensorflow (1.12.0)                                - TensorFlow is an open source machine learning framework for everyone.\ntensorflow-font2char2word2sent2doc (0.0.12)              - TensorFlow implementation of Hierarchical Attention Networks for Document Classification\ntensorflow-gpu (1.12.0)                                  - TensorFlow is an open source machine learning framework for everyone.\ntensorflow-aarch64 (1.2)                                 - Tensorflow r1.2 for aarch64[arm64,pine64] CPU only.\ntensorflow-fedora28 (1.9.0rc0)                           - TensorFlow is an open source machine learning framework for everyone.\ntensorflow-model-analysis (0.11.0)                       - A library for analyzing TensorFlow models\ntensorflow-transform-canary (0.9.0)                      - A library for data preprocessing with TensorFlow\nrav-tensorflow-transform (0.7.0.910)                     - A library for data preprocessing with TensorFlow\ntensorflow-serving-api (1.12.0)                          - TensorFlow Serving Python API.\ntensorflow-serving-client (0.0.10)                       - Python client for tensorflow serving\ntensorflow-hub (0.2.0)                                   - TensorFlow Hub is a library to foster the publication, discovery, and consumption of reusable parts of machine learning models.\ntensorflow-estimator-2.0-preview (1.13.0.dev2019010100)  - TensorFlow Estimator.\nngraph-tensorflow-bridge (0.8.0)                         - Intel nGraph compiler and runtime for TensorFlow\ntensorflow-probability-gpu (0.4.0)                       - Probabilistic modeling and statistical inference in TensorFlow\nsimple-tensorflow-serving (0.6.6)                        - The simpler and easy-to-use serving service for TensorFlow models\ntensorflow-auto-detect (1.11.0)                          - Automatically install CPU or GPU tensorflow determined by looking for a CUDA installation.\ntensorflow-serving-api-python3 (1.8.0)                   - *UNOFFICIAL* TensorFlow Serving API libraries for Python3\ntensorflow-exercise-hx (1.0.1)                           - tensorflow&#32451;&#20064;&#65306;&#40482;&#23614;&#33457;&#31181;&#31867;&#39044;&#27979;&#65292;&#21152;&#24030;&#25151;&#20215;&#39044;&#27979;\ntensorflow-metadata (0.9.0)                              - Library and standards for schema and statistics.\ntensorflow-tensorboard (1.5.1)                           - TensorBoard lets you watch Tensors Flow\nresnet-tensorflow (0.0.1)                                - Deep Residual Neural Network\nmlops-tensorflow (0.1.0)                                 - \ntensorflow-gpu-macosx (1.8.1)                            - Unoffcial NVIDIA CUDA GPU support version of Google Tensorflow for MAC OSX 10.13. For more info, please check out my github page. I highly recommend you directly download and install it from my github's release. If you insist on compiling it, you'd do it on a shell to debug.\nsyntaxnet-with-tensorflow (0.2)                          - SyntaxNet: Neural Models of Syntax\ntensorflow-data-validation (0.11.0)                      - A library for exploring and validating machine learning data.\nogres (0.0.2)                                            - Thin tensorflow wrapper. Requires tensorflow\ntfmesos (0.0.10)                                         - Tensorflow on Mesos\ntf-estimator-nightly (1.12.0.dev20181217)                - TensorFlow Estimator.\nTFBOYS (0.0.1)                                           - TensorFlow BOYS\nTFTree (0.1.0)                                           - Tree to tensorflow\ntfdebugger (0.1.1)                                       - TensorFlow Debugger\ntfextras (0.0.8)                                         - Tensorflow extras\ntfu (0.0.1.dev0)                                         - tensorflow utils\ntnt (0.12.0.7)                                           - tnt is not tensorflow\neasytf (13.9)                                            - Tensorflow CS\ntftf (0.0.29)                                            - TensorFlow TransFormer\ntf-datasets (0.0.1)                                      - tensorflow/datasets\ntfds-nightly (0.0.2.dev201901020014)                     - tensorflow/datasets is a library of datasets ready to use with TensorFlow.\ntf-common (1.0.0)                                        - A common liberary of tensorflow\nParticleFlow (0.0.1)                                     - Particle simulations with tensorflow\ntf_decompose (0.1)                                       - Tensor decomposition with TensorFlow\ntensorbase (0.3)                                         - Minimalistic TensorFlow Framework\nminiflow (0.2.9)                                         - Minimal implementation of TensorFlow\nbob.learn.tensorflow (1.0.3)                             - Bob bindings for tensorflow\nquantile-transformer-tf (1.2)                            - An implementation of QuantileTransformer in tensorflow\ntf-env (0.1.0)                                           - RL environments for TensorFlow.\ntfseqestimator (2.2.0)                                   - Sequence estimators for Tensorflow\nwavenet (0.1.2)                                          - An implementation of WaveNet for TensorFlow.\ntf_kaldi_io (0.3.0)                                      - kaldi-io for Tensorflow\nptfutils (0.0.29)                                        - Useful modules for tensorflow\niceflow (0.0.1a2)                                        - tensorflow meta-framework\ntf2onnx (0.3.2)                                          - Tensorflow to ONNX converter\ntensorforce (0.4.3)                                      - Reinforcement learning for TensorFlow\nsimnets (0.0.1)                                          - SimNets implementation in tensorflow\nsaliency (0.0.2)                                         - Saliency methods for TensorFlow\ntensor-lib (1.8.19)                                      - Simplified tensorflow library\ntensorsets (0.1.0)                                       - Standard datasets for TensorFlow.\ntfstage (0.1.7)                                          - TensorFlow project scaffolding\ntop-hat (0.0.2)                                          - Recommendation system in TensorFlow\nTensorMol (0.1)                                          - TensorFlow+Molecules = TensorMol\ntfshop (0.0.1)                                           - common tensorflow paradigms\nkfac (0.1.0)                                             - K-FAC for TensorFlow\ntransferflow (0.1.8)                                     - Transfer learning for Tensorflow\ntrain (0.0.3)                                            - Training utilities for TensorFlow.\nvibranium (0.1.0)                                        - Opinionated Tensorflow projects\ntf-data (0.0.4)                                          - Easy datasets for tensorflow\ntensorfunk (0.0.0)                                       - tensorflow model converter to create tensorflow-independent prediction functions.\ntf1 (1.1.0)                                              - F1-score metric for TensorFlow\ntensorboard-easy (0.2.3)                                 - A tensorflow-independent tensorboard logger\ngpflow (1.3.0)                                           - Gaussian process methods in tensorflow\nlayer (0.1.14)                                           - tensorflow custom comfort wrapper\ntflab (0.1.3)                                            - A laboratory for experimenting with Tensorflow abstraction\ntensorpack (0.9.0.1)                                     - Neural Network Toolbox on TensorFlow\ntensorflowservingclient (0.5.1.post2)                    - Prebuilt tensorflow serving client\nEasyFlow (0.1.dev3)                                      - Modular Distributed TensorFlow Framework\ntfgraph (0.2)                                            - Python's Tensorflow Graph Library\nserving-utils (0.6.0)                                    - Some utilities for tensorflow serving\nRemoving intermediate container a4434a87e740\n ---> a14248285cb2\nStep 7/12 : RUN pip install tensorflow\n ---> Running in 2c14fe29c431\nCollecting tensorflow\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\nNo matching distribution found for tensorflow\nERROR: Service 'app' failed to build: The command '/bin/sh -c pip install tensorflow' returned a non-zero code: 1\n```\n\n\nDo I have to resort to building tensorflow from source?\n\nEDIT\nWriting this question made me realize I could use two separate containers: one for my Django app and a prebuilt tensorflow with gpu. I would still like to learn how to resolve issues like this, but any pointers to docs how to make two separate docker containers talk, would be appreciated.\n    ", "Answer": "\r\nIt looks like tensorflow only publishes wheels (and only up to 3.6), and Alpine linux is not ```\nmanylinux1```\n-compatible due to its use of ```\nmusl```\n instead of ```\nglibc```\n. Because of this, ```\npip```\n cannot find a suitable installation candidate and fails. Your best options are probably to build from source or change your base image.\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
{"Question": "Getting slow response form google OR tools\r\n                \r\nI am using the OR tool Python library to resolve the cutting stock problem. But it has been taking too long for the last few days to respond. It is taking time more than 1 minute to respond.\nI am using Python code as an API and passing the below data.\nAPI Data :\n```\n{\n    \"child_rolls\": [\n        {\n            \"roll\": 3,\n            \"size\": 1490\n        },\n        {\n            \"roll\": 8,\n            \"size\": 1490\n        },\n        {\n            \"roll\": 23,\n            \"size\": 1500\n        },\n        {\n            \"roll\": 9,\n            \"size\": 1500\n        },\n        {\n            \"roll\": 8,\n            \"size\": 1480\n        },\n        {\n            \"roll\": 30,\n            \"size\": 1480\n        }\n    ],\n    \"parent_rolls\": [\n        {\n            \"roll\": null,\n            \"size\": 4470\n        }\n    ]\n}\n```\n\nWhen I passed the above data in API it is taking more than 1 minute to return the output of the above data. This means when I pass large data to my GCP Python script gives me a slow response.  So, I need your help to improve performance for all types of data. Could you please guide me and help to resolve this issue?\nPython code:\n```\nimport pdb\nimport math\nfrom ortools.linear_solver import pywraplp\nfrom math import ceil\nfrom random import randint\nimport json\n# from read_lengths import get_data\n# import typer\nfrom typing import Optional\nfrom flask import Flask\nfrom flask_cors import CORS\nfrom flask import request\n\n\n\ndef newSolver(name, integer=False):\n    return pywraplp.Solver(name,\n                           pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING\n                           if integer else\n                           pywraplp.Solver.GLOP_LINEAR_PROGRAMMING)\n\n\n'''\nreturn a printable value\n'''\n\n\ndef SolVal(x):\n    if type(x) is not list:\n        return 0 if x is None \\\n            else x if isinstance(x, (int, float)) \\\n            else x.SolutionValue() if x.Integer() is False \\\n                    else int(x.SolutionValue())\n    elif type(x) is list:\n        return [SolVal(e) for e in x]\n\n\ndef ObjVal(x):\n    return x.Objective().Value()\n\n\ndef gen_data(num_orders):\n    R = []  # small rolls\n    # S=0 # seed?\n    for i in range(num_orders):\n        R.append([randint(1, 12), randint(5, 40)])\n    return R\n\n\ndef solve_model(demands, parent_width=100,parent_main_width=0):\n    '''\n        demands = [\n            [1, 3], # [quantity, width]\n            [3, 5],\n            ...\n        ]\n\n        parent_width = integer\n    '''\n    num_orders = len(demands)\n    solver = newSolver('Cutting Stock', True)\n    k, b = bounds(demands, parent_width)\n\n    # array of boolean declared as int, if y[i] is 1,\n    # then y[i] Big roll is used, else it was not used\n    y = [solver.IntVar(0, 1, f'y_{i}') for i in range(k[1])]\n\n    # x[i][j] = 3 means that small-roll width specified by i-th order\n    # must be cut from j-th order, 3 tmies\n    x = [[solver.IntVar(0, b[i], f'x_{i}_{j}') for j in range(k[1])]\n         for i in range(num_orders)]\n\n    # unused_widths = [solver.NumVar(0, parent_width, f'w_{j}')\n    #                  for j in range(k[1])]\n\n    unused_widths = []\n    for j in range(k[1]):\n        print(\"J\",j)\n        var_name = f'w_{j}'\n        var = solver.NumVar(0, parent_width, var_name)\n\n        unused_widths.append(var)\n      \n        print(f\"Created variable {var_name} with domain [0, {parent_width}] and value {var}.\")\n\n    #print(\"SolVal\",SolVal(unused_widths))\n  \n    # will contain the number of big rolls used\n    nb = solver.IntVar(k[0], k[1], 'nb')\n    print('1st of nb when intilize',nb)\n\n    # consntraint: demand fullfilment\n    for i in range(num_orders):\n        # small rolls from i-th order must be at least as many in quantity\n        # as specified by the i-th order\n        solver.Add(sum(x[i][j] for j in range(k[1])) >= demands[i][0])\n\n    # constraint: max size limit\n    for j in range(k[1]):\n        # total width of small rolls cut from j-th big roll,\n        # must not exceed big rolls width\n        solver.Add(\n            sum(demands[i][1]*x[i][j] for i in range(num_orders))\n            <= parent_width*y[j]\n        )\n\n        # width of j-th big roll - total width of all orders cut from j-th roll\n        # must be equal to unused_widths[j]\n        # So, we are saying that assign unused_widths[j] the remaining width of j'th big roll\n        solver.Add(parent_main_width*y[j] - sum(demands[i][1]*x[i][j]\n                   for i in range(num_orders)) == unused_widths[j])\n\n        '''\n    Book Author's note from page 201:\n    [the following constraint]  breaks the symmetry of multiple solutions that are equivalent \n    for our purposes: any permutation of the rolls. These permutations, and there are K! of \n    them, cause most solvers to spend an exorbitant time solving. With this constraint, we \n    tell the solver to prefer those permutations with more cuts in roll j than in roll j + 1. \n    The reader is encouraged to solve a medium-sized problem with and without this \n    symmetry-breaking constraint. I have seen problems take 48 hours to solve without the \n    constraint and 48 minutes with. Of course, for problems that are solved in seconds, the \n    constraint will not help; it may even hinder. But who cares if a cutting stock instance \n    solves in two or in three seconds? We care much more about the difference between two \n    minutes and three hours, which is what this constraint is meant to address\n    '''\n        if j < k[1]-1:  # k1 = total big rolls\n            # total small rolls of i-th order cut from j-th big roll must be >=\n            # totall small rolls of i-th order cut from j+1-th big roll\n            solver.Add(sum(x[i][j] for i in range(num_orders))\n                       >= sum(x[i][j+1] for i in range(num_orders)))\n\n    # find & assign to nb, the number of big rolls used\n    solver.Add(nb == solver.Sum(y[j] for j in range(k[1])))\n    print('2nd nb while executing',nb)\n\n    ''' \n    minimize total big rolls used\n    let's say we have y = [1, 0, 1]\n    here, total big rolls used are 2. 0-th and 2nd. 1st one is not used. So we want our model to use the \n    earlier rolls first. i.e. y = [1, 1, 0]. \n    The trick to do this is to define the cost of using each next roll to be higher. So the model would be\n    forced to used the initial rolls, when available, instead of the next rolls.\n\n    So instead of Minimize ( Sum of y ) or Minimize( Sum([1,1,0]) )\n    we Minimize( Sum([1*1, 1*2, 1*3]) )\n  '''\n\n    '''\n  Book Author's note from page 201:\n\n  There are alternative objective functions. For example, we could have minimized the sum of the waste. This makes sense, especially if the demand constraint is formulated as an inequality. Then minimizing the sum of waste Chapter 7  advanCed teChniques\n  will spend more CPU cycles trying to find more efficient patterns that over-satisfy demand. This is especially good if the demand widths recur regularly and storing cut rolls in inventory to satisfy future demand is possible. Note that the running time will grow quickly with such an objective function\n  '''\n\n    Cost = solver.Sum((j+1)*y[j] for j in range(k[1]))\n    # print(\"Cost == \",Cost)\n\n    solver.Minimize(Cost)\n    # solver.Add(Cost)\n    #print(\"COST DATA = \",solver.Minimize(Cost))\n    status = solver.Solve()\n    numRollsUsed = SolVal(nb)\n   #nb, x, w, demands\n    return status, \\\n        numRollsUsed, \\\n        rolls(numRollsUsed, SolVal(x), SolVal(unused_widths), demands), \\\n        SolVal(unused_widths), \\\n        solver.WallTime()\n\n\ndef bounds(demands, parent_width=100):\n    '''\n    b = [sum of widths of individual small rolls of each order]\n    T = local var. stores sum of widths of adjecent small-rolls. When the width reaches 100%, T is set to 0 again.\n    k = [k0, k1], k0 = minimum big-rolls requierd, k1: number of big rolls that can be consumed / cut from\n    TT = local var. stores sum of widths of of all small-rolls. At the end, will be used to estimate lower bound of big-rolls\n    '''\n    num_orders = len(demands)\n    b = []\n    T = 0\n    k = [0, 1]\n    TT = 0\n\n    for i in range(num_orders):\n        # q = quantity, w = width; of i-th order\n        quantity, width = demands[i][0], demands[i][1]\n        # TODO Verify: why min of quantity, parent_width/width?\n        # assumes widths to be entered as percentage\n        # int(round(parent_width/demands[i][1])) will always be >= 1, because widths of small rolls can't exceed parent_width (which is width of big roll)\n        b.append( min(demands[i][0], int(round(parent_width / demands[i][1]))) )\n        #b.append(min(quantity, int(round(parent_width / width))))\n\n        # if total width of this i-th order + previous order's leftover (T) is less than parent_width\n        # it's fine. Cut it.\n        if T + quantity*width <= parent_width:\n            T, TT = T + quantity*width, TT + quantity*width\n        # else, the width exceeds, so we have to cut only as much as we can cut from parent_width width of the big roll\n        else:\n            while quantity:\n                if T + width <= parent_width:\n                    T, TT, quantity = T + width, TT + width, quantity-1\n                else:\n                    k[1], T = k[1]+1, 0  # use next roll (k[1] += 1)\n    k[0] = int(round(TT/parent_width+0.5))\n\n    print('k', k)\n    print('b', b)\n\n    return k, b\n\n\n'''\n  nb: array of number of rolls to cut, of each order\n  \n  w: \n  demands: [\n    [quantity, width],\n    [quantity, width],\n    [quantity, width],\n  ]\n'''\n\n\ndef rolls(nb, x, w, demands):\n    consumed_big_rolls = []\n    num_orders = len(x)\n    # go over first row (1st order)\n    # this row contains the list of all the big rolls available, and if this 1st (0-th) order\n    # is cut from any big roll, that big roll's index would contain a number > 0\n    for j in range(len(x[0])):\n        # w[j]: width of j-th big roll\n        # int(x[i][j]) * [demands[i][1]] width of all i-th order's small rolls that are to be cut from j-th big roll\n        h=0\n        print (\"int(x[i][j]\",int(x[h][j])) \n        h=h+1\n        RR = [abs(w[j])] + [int(x[i][j])*[demands[i][1]] for i in range(num_orders)\n                            if x[i][j] > 0]  # if i-th order has some cuts from j-th order, x[i][j] would be > 0\n        print(\"RR \",RR)\n        consumed_big_rolls.append(RR)\n\n    return consumed_big_rolls\n\n\n'''\nthis model starts with some patterns and then optimizes those patterns\n'''\n\n\ndef solve_large_model(demands, parent_width=100):\n    num_orders = len(demands)\n    iter = 0\n    patterns = get_initial_patterns(demands)\n    # print('method#solve_large_model, patterns', patterns)\n\n    # list quantities of orders\n    quantities = [demands[i][0] for i in range(num_orders)]\n    print('quantities', quantities)\n\n    while iter < 20:\n        status, y, l = solve_master(\n            patterns, quantities, parent_width=parent_width)\n        iter += 1\n\n        # list widths of orders\n        widths = [demands[i][1] for i in range(num_orders)]\n        new_pattern, objectiveValue = get_new_pattern(\n            l, widths, parent_width=parent_width)\n\n        # print('method#solve_large_model, new_pattern', new_pattern)\n        # print('method#solve_large_model, objectiveValue', objectiveValue)\n\n        for i in range(num_orders):\n            # add i-th cut of new pattern to i-thp pattern\n            patterns[i].append(new_pattern[i])\n\n    status, y, l = solve_master(\n        patterns, quantities, parent_width=parent_width, integer=True)\n\n    return status, \\\n        patterns, \\\n        y, \\\n        rolls_patterns(patterns, y, demands, parent_width=parent_width)\n\n\n'''\nDantzig-Wolfe decomposition splits the problem into a Master Problem MP and a sub-problem SP.\n\nThe Master Problem: provided a set of patterns, find the best combination satisfying the demand\n\nC: patterns\nb: demand\n'''\n\n\ndef solve_master(patterns, quantities, parent_width=100, integer=False):\n    title = 'Cutting stock master problem'\n    num_patterns = len(patterns)\n    n = len(patterns[0])\n    # print('**num_patterns x n: ', num_patterns, 'x', n)\n    # print('**patterns recived:')\n    # for p in patterns:\n    #   print(p)\n\n    constraints = []\n\n    solver = newSolver(title, integer)\n\n    # y is not boolean, it's an integer now (as compared to y in approach used by solve_model)\n    y = [solver.IntVar(0, 1000, '') for j in range(n)]  # right bound?\n    # minimize total big rolls (y) used\n    Cost = sum(y[j] for j in range(n))\n    solver.Minimize(Cost)\n\n    # for every pattern\n    for i in range(num_patterns):\n        # add constraint that this pattern (demand) must be met\n        # there are m such constraints, for each pattern\n        constraints.append(solver.Add(\n            sum(patterns[i][j]*y[j] for j in range(n)) >= quantities[i]))\n\n    status = solver.Solve()\n    y = [int(ceil(e.SolutionValue())) for e in y]\n\n    l = [0 if integer else constraints[i].DualValue()\n         for i in range(num_patterns)]\n    # sl =  [0 if integer else constraints[i].name() for i in range(num_patterns)]\n    # print('sl: ', sl)\n\n    # l =  [0 if integer else u[i].Ub() for i in range(m)]\n    toreturn = status, y, l\n    # l_to_print = [round(dd, 2) for dd in toreturn[2]]\n    # print('l: ', len(l_to_print), '->', l_to_print)\n    # print('l: ', toreturn[2])\n    return toreturn\n\n\ndef get_new_pattern(l, w, parent_width=100):\n    solver = newSolver('Cutting stock sub-problem', True)\n    n = len(l)\n    new_pattern = [solver.IntVar(0, parent_width, '') for i in range(n)]\n\n    # maximizes the sum of the values times the number of occurrence of that roll in a pattern\n    Cost = sum(l[i] * new_pattern[i] for i in range(n))\n    solver.Maximize(Cost)\n\n    # ensuring that the pattern stays within the total width of the large roll\n    solver.Add(sum(w[i] * new_pattern[i] for i in range(n)) <= parent_width)\n\n    status = solver.Solve()\n    return SolVal(new_pattern), ObjVal(solver)\n\n\n'''\nthe initial patterns must be such that they will allow a feasible solution, \none that satisfies all demands. \nConsidering the already complex model, let’s keep it simple. \nOur initial patterns have exactly one roll per pattern, as obviously feasible as inefficient.\n'''\n\n\ndef get_initial_patterns(demands):\n    num_orders = len(demands)\n    return [[0 if j != i else 1 for j in range(num_orders)]\n            for i in range(num_orders)]\n\n\ndef rolls_patterns(patterns, y, demands, parent_width=100):\n    R, m, n = [], len(patterns), len(y)\n\n    for j in range(n):\n        for _ in range(y[j]):\n            RR = []\n            for i in range(m):\n                if patterns[i][j] > 0:\n                    RR.extend([demands[i][1]] * int(patterns[i][j]))\n            used_width = sum(RR)\n            R.append([parent_width - used_width, RR])\n\n    return R\n\n\n'''\nchecks if all small roll widths (demands) smaller than parent roll's width\n'''\n\n\ndef checkWidths(demands, parent_width):\n    for quantity, width in demands:\n        if width > parent_width:\n            print(\n                f'Small roll width {width} is greater than parent rolls width {parent_width}. Exiting')\n            return False\n    return True\n\n\n'''\n    params\n        child_rolls: \n            list of lists, each containing quantity & width of rod / roll to be cut\n            e.g.: [ [quantity, width], [quantity, width], ...]\n        parent_rolls: \n            list of lists, each containing quantity & width of rod / roll to cut from\n            e.g.: [ [quantity, width], [quantity, width], ...]\n'''\n\n\ndef StockCutter1D(child_rolls, parent_rolls, output_json=True, large_model=True):\n\n    # at the moment, only parent one width of parent rolls is supported\n    # quantity of parent rolls is calculated by algorithm, so user supplied quantity doesn't matter?\n    # TODO: or we can check and tell the user the user when parent roll quantity is insufficient\n    # pdb.set_trace()\n    parent_width_main = parent_rolls[0][1]\n    qua_arr = []\n    for item in range(len(child_rolls)):\n        qua_arr.append(child_rolls[item][1])\n    #     # print(item)\n    # # print(qua_arr)\n    # # \n    if min(qua_arr) <=20:\n        parent_width_second = min(qua_arr)*7\n    else: \n        parent_width_second = parent_width_main\n\n\n    if parent_width_second > parent_width_main:\n        parent_width = parent_width_main\n    else:\n        parent_width = parent_width_second\n    \n    print(\"parent_width_main==\",parent_width_main)\n    print(\"parent_width_second==\",parent_width_second)\n    print(\"parent_width==\",parent_width)\n    if not checkWidths(demands=child_rolls, parent_width=parent_width):\n        return []\n\n    print('child_rolls', child_rolls)\n    print('parent_rolls', parent_rolls)\n\n    if not large_model:\n        print('Running Small Model...')\n        status, numRollsUsed, consumed_big_rolls, unused_roll_widths, wall_time = \\\n            solve_model(demands=child_rolls, parent_width=parent_width,parent_main_width=parent_width_main)\n\n        # convert the format of output of solve_model to be exactly same as solve_large_model\n        print('consumed_big_rolls before adjustment: ', consumed_big_rolls)\n        new_consumed_big_rolls = []\n        for big_roll in consumed_big_rolls:\n            if len(big_roll) < 2:\n                # sometimes the solve_model return a solution that contanis an extra [0.0] entry for big roll\n                consumed_big_rolls.remove(big_roll)\n                continue\n            unused_width = big_roll[0]\n            subrolls = []\n            for subitem in big_roll[1:]:\n                if isinstance(subitem, list):\n                    # if it's a list, concatenate with the other lists, to make a single list for this big_roll\n                    subrolls = subrolls + subitem\n                else:\n                    # if it's an integer, add it to the list\n                    subrolls.append(subitem)\n            new_consumed_big_rolls.append([unused_width, subrolls])\n        print('consumed_big_rolls after adjustment: ', new_consumed_big_rolls)\n        consumed_big_rolls = new_consumed_big_rolls\n\n    else:\n        print('Running Large Model...')\n        status, A, y, consumed_big_rolls = solve_large_model(\n            demands=child_rolls, parent_width=parent_width)\n\n    numRollsUsed = len(consumed_big_rolls)\n    # print('A:', A, '\\n')\n    # print('y:', y, '\\n')\n\n    STATUS_NAME = ['OPTIMAL',\n                   'FEASIBLE',\n                   'INFEASIBLE',\n                   'UNBOUNDED',\n                   'ABNORMAL',\n                   'NOT_SOLVED'\n                   ]\n\n    output = {\n        \"statusName\": STATUS_NAME[status],\n        \"numSolutions\": '1',\n        \"numUniqueSolutions\": '1',\n        \"numRollsUsed\": numRollsUsed,\n        \"solutions\": consumed_big_rolls  # unique solutions\n    }\n\n    # print('Wall Time:', wall_time)\n    print('numRollsUsed', numRollsUsed)\n    print('Status:', output['statusName'])\n    print('Solutions found :', output['numSolutions'])\n    print('Unique solutions: ', output['numUniqueSolutions'])\n\n    if output_json:\n        return output\n    else:\n        return consumed_big_rolls\n\n\n'''\nDraws the big rolls on the graph. Each horizontal colored line represents one big roll.\nIn each big roll (multi-colored horizontal line), each color represents small roll to be cut from it.\nIf the big roll ends with a black color, that part of the big roll is unused width.\n\nTODO: Assign each child roll a unique color\n'''\n\nif __name__ == '__main__':\n\n    app = Flask(__name__)\n    # Enable the CORS\n    CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n\n    @app.route(\"/\")\n    def index():\n        return \"This is index page, please go ahead...! \"\n\n    @app.route(\"/track\", methods=['POST'])\n    def track():\n        try:\n            print('request.data==', request.get_json())\n            req_data = request.get_json()\n            child_rolls = []\n            parent_rolls = []\n            child_rolls_obj = req_data.get('child_rolls', [])\n            for item in child_rolls_obj:\n                data = list(item.values())\n                # data.reverse()\n                child_rolls.append(data)\n            parent_rolls_obj = req_data.get('parent_rolls', [])\n            parent_rolls.append(list(parent_rolls_obj[0].values()))\n            # pdb.set_trace()\n            # number_of_cut = req_data.get('number_of_cut', [])\n            if(not(len(child_rolls) and len(parent_rolls))):\n                return {\"success\": False, \"message\": \"Request data is not enough!\"}\n            consumed_big_rolls = StockCutter1D(\n                child_rolls, parent_rolls, output_json=True, large_model=False)\n            # return {\"success\": True, \"data\": {\"solutions\": consumed_big_rolls}}\n            return consumed_big_rolls\n        except Exception as error:\n            return {\"success\": False, \"message\": \"Something went wrong!\", \"error\": error}\n            # raise error\n\n    app.config[\"FLASK_APP\"] = 'main.py'\n    app.config[\"FLASK_ENV\"] = 'development'\n\n    app.run(host=\"0.0.0.0\", debug=True, port=5000, use_reloader=False)\n```\n\nExpected output be like this:\n```\n{\n    \"numRollsUsed\": 29,\n    \"numSolutions\": \"1\",\n    \"numUniqueSolutions\": \"1\",\n    \"solutions\": [\n        [\n            0.0,\n            [\n                1490,\n                1500,\n                1480\n            ]\n        ],\n        [\n            0.0,\n            [\n                1490,\n                1500,\n                1480\n            ]\n        ],\n        [\n            29.999999999999957,\n            [\n                1480,\n                1480,\n                1480\n            ]\n        ],\n        [\n            10.000000000000151,\n            [\n                1500,\n                1480,\n                1480\n            ]\n        ],\n        [\n            10.000000000000151,\n            [\n                1500,\n                1480,\n                1480\n            ]\n        ],\n        [\n            10.000000000000151,\n            [\n                1500,\n                1480,\n                1480\n            ]\n        ],\n        [\n            10.000000000000151,\n            [\n                1500,\n                1480,\n                1480\n            ]\n        ],\n        [\n            10.000000000000151,\n            [\n                1500,\n                1480,\n                1480\n            ]\n        ],\n        [\n            0.0,\n            [\n                1490,\n                1500,\n                1480\n            ]\n        ],\n        [\n            0.0,\n            [\n                1490,\n                1500,\n                1480\n            ]\n        ],\n        [\n            10.000000000000151,\n            [\n                1500,\n                1480,\n                1480\n            ]\n        ],\n        [\n            10.000000000000151,\n            [\n                1500,\n                1480,\n                1480\n            ]\n        ],\n        [\n            10.000000000000151,\n            [\n                1500,\n                1480,\n                1480\n            ]\n        ],\n        [\n            0.0,\n            [\n                1490,\n                1500,\n                1480\n            ]\n        ],\n        [\n            10.000000000000151,\n            [\n                1500,\n                1480,\n                1480\n            ]\n        ],\n        [\n            0.0,\n            [\n                1490,\n                1500,\n                1480\n            ]\n        ],\n        [\n            0.0,\n            [\n                1490,\n                1500,\n                1480\n            ]\n        ],\n        [\n            29.999999999999957,\n            [\n                1480,\n                1480,\n                1480\n            ]\n        ],\n        [\n            10.000000000000151,\n            [\n                1500,\n                1480,\n                1480\n            ]\n        ],\n        [\n            10.000000000000151,\n            [\n                1500,\n                1480,\n                1480\n            ]\n        ],\n        [\n            0.0,\n            [\n                1490,\n                1500,\n                1480\n            ]\n        ],\n        [\n            10.000000000000151,\n            [\n                1500,\n                1480,\n                1480\n            ]\n        ],\n        [\n            0.0,\n            [\n                1490,\n                1490,\n                1490\n            ]\n        ],\n        [\n            1469.9999999999998,\n            [\n                1500,\n                1500\n            ]\n        ],\n        [\n            1469.9999999999998,\n            [\n                1500,\n                1500\n            ]\n        ],\n        [\n            1469.9999999999998,\n            [\n                1500,\n                1500\n            ]\n        ],\n        [\n            1469.9999999999998,\n            [\n                1500,\n                1500\n            ]\n        ],\n        [\n            1469.9999999999998,\n            [\n                1500,\n                1500\n            ]\n        ],\n        [\n            1469.9999999999998,\n            [\n                1500,\n                1500\n            ]\n        ]\n    ],\n    \"statusName\": \"OPTIMAL\"\n}\n```\n\n    ", "Answer": "\r\nResuls are strange. The problem is trivial.\nI ran your data with the arc flow simple model.\nI used this as data:\n```\nDESIRED_LENGTHS = [1490] * 11 + [1500] * 32 + [1480] * 38\nPOSSIBLE_CAPACITIES = [4470]\n```\n\nI get the optimal value is 0.012s\n```\nCpSolverResponse summary:\nstatus: OPTIMAL\nobjective: 9000\nbest_bound: 9000\nintegers: 24\nbooleans: 0\nconflicts: 0\nbranches: 0\npropagations: 0\ninteger_propagations: 1\nrestarts: 0\nlp_iterations: 0\nwalltime: 0.0117221\nusertime: 0.0117223\ndeterministic_time: 0.000500589\ngap_integral: 0.000746385\nsolution_fingerprint: 0xce6c9c6971fb9e21\n```\n\n    ", "Knowledge_point": "Graph Decomposition", "Tag": "算法分析"}
