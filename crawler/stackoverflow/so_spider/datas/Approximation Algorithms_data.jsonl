{"Question": "0-1 integer linear programming approximation algorithms\r\n                \r\nHi I am looking for an approximation algorithm for 0-1 integer linear programming. Currently the approximation algorithms I find need to relax the interval to be [0,1]. However, my problem can only treat 0 or 1 as the solution. \n\nDoes anyone have ideas? Thank you in advance. \n    ", "Answer": "\r\nThe classic procedure to obtain an integral solution would be branch-and-bound. If this is not what you are looking fore, provide more details.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Traveling salesman library using approximation algorithm\r\n                \r\nI'm currently doing a project that requires some fast TSP solving (about 50-100 nodes in 2 seconds). There are a lots of approximation algorithms out there, but I don't have time nor will to analyze them and code them myself.\n\nAre there any free libraries that can solve TSP problem (approximation will do too)? Something like ```\nsortedNodes = solveTspPrettyPlease(nodes, 2sec)```\n would be just great.\n\nThanks in advance.\n    ", "Answer": "\r\n\nhttp://code.google.com/p/java-traveling-salesman/ - genetic algorithm solution\nhttp://www.adaptivebox.net/CILib/code/tspcodes_link.html - links to many solutions, not all of which (if any?) are libraries\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Difference Between Heuristic and Approximation Algorithm in Bin Packing\r\n                \r\nI'm reading up on the one dimensional bin packing problem and the different solutions that can be used to solve it.\n\nBin Packing Problem Definition: Given a list of objects and their weights, and a collection of bins of fixed size, find the smallest number of bins so that all of the objects are assigned to a bin.\n\nSolutions I'm studying: Next Fit, First Fit, Best Fit, Worst Fit, First Fit Decreasing, Best Fit Decreasing\n\nI notice that some articles I read call these \"approximation algorithms\", and others call these \"heuristics\". I know that there is a difference between approximation algorithms and heuristics:\n\nHeuristic: With some hard problems, it's difficult to get an acceptable solution in a decent run time, so we can get an \"okay\" solution by applying some educated guesses, or arbitrarily choosing. \n\nApproximation Algorithm: This gives an approximate solution, with some \"guarantee\" on it's performance (maybe a ratio, or something like that)\n\nSo, my question is are these solutions that I'm studying heuristic or approximation algorithms? I'm more inclined to believe that they are heuristic because we're choosing the next item to be placed in a bin by some \"guess\". We're not guaranteed of some optimal solution. So why do some people call them approximation algorithms?\n\nIf these aren't heuristic algorithms, then what are examples of heuristic algorithms to solve the bin packing problem?\n    ", "Answer": "\r\nAn algorithm can be both a heuristic and an approximation algorithm -- the two terms don't conflict. If some \"good but not always optimal\" strategy (the heuristic) can be proven to be \"not too bad\" (the approximation guarantee), then it qualifies as both.\n\nAll of the algorithms you listed are heuristic because they prescribe a \"usually good\" strategy, which is the heuristic. For any of the algorithms where there is an approximation guarantee (the \"error\" must be bounded somehow), then you can also say it's an approximation algorithm.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation Algorithm for Vertex Cover\r\n                \r\nIf ```\nP```\n is not equal to ```\nNP```\n can it be shown that there is no approximation algorithm which comes within ```\nk```\n of the optimal vertex cover, where ```\nk```\n is a fixed constant?\n    ", "Answer": "\r\nIf the question is to be understood in terms of an additive error, such an algorithm does not exist. Aiming at a contradiction, suppose ```\nA```\n is such an algorithm; this means that there is a nonnegative integer ```\nk```\n such that for any graph ```\nG```\n,\n\n```\nA(G) <= tau(G) + k\n```\n\n\nholds, where ```\nA(G)```\n is the cardinality of the vertex cover of ```\nG```\n generated by ```\nA```\n and ```\ntau(G)```\n denotes the cardinality of a minimum vertex cover. Let ```\nk```\n be chosen minimal with respect to the existence of the mentioned algorithm. In particular, we have ```\nk => 1```\n, since otherwise the vertex cover problem could be solved in polynomial time, which is impossible unless ```\nP=NP```\n holds.\n\nLet ```\nG```\n be an arbitraty graph; create a graph ```\nG'```\n by taking ```\nk+1```\n isomorphic copies of ```\nG```\n; then\n\n```\ntau(G') = (k + 1) tau(G)\n```\n\n\nholds. Furthermore we obtain the following.\n\n```\nA(G') <= tau(G) + k\n       = (k + 1) tau(G) + k\n```\n\n\nLet ```\nG*```\n be the isomorphic copy of ```\nG```\n in ```\nG'```\n with the smallest vertex cover generated by ```\nA```\n; let ```\nA(G*)```\n denote the size of this vertex cover. Aiming at a contradiction, we assume that\n\n```\nA(G*) >= tau(G*) + k\n```\n\n\nholds. This means that\n\n```\nA(G') >= (k + 1) A(G*)\n      >= (k + 1) (tau(G*) + k)\n       = (k + 1) (tau(G) + k)\n       = (k + 1) tau(G) + k + k^2\n       > (k + 1) tau(G) + k\n```\n\n\nholds, since ```\nk > 0```\n holds. This is a contradiction to the approximation quality of ```\nA```\n. This means that\n\n```\nA(G*) < tau(G*) + k\n```\n\n\nholds. As ```\ntau(G*) = tau(G)```\n holds, this means that we have used ```\nA```\n to generate a vertex cover of ```\nG```\n with cardinality strictly smaller than\n\n```\ntau(G) + k\n```\n\n\nwhich is a contradiction, since ```\nk```\n was chosen minimally and all construction steps can be carried out within a polynomially bounded running time, resulting in a runtime bound which is polynomially bounded as well.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation Algorithm- using expectation\r\n                \r\nNote: You don't have to understand Approximation Algorithms to answer this\n\nHello. \n I need to prove an algorithm approximation by using expectation.\n The algorithm takes ```\nx_i \\in {0,1,2}```\n such that ```\ni\\in 1,...n+2```\n and there are constants ```\nc_i \\in 0,1,2```\n such that ```\ni\\in 1,...,n```\n and would like to find an assignment to the variables such that ```\nx_i +x_(i+1)+x_(i+2) !=  0 \\mod(3)```\n  for all ```\ni```\n such that the number of indexes such that ```\nx_i +x_(i+2) = c_i \\mod(3)```\n is maximized. \n it does the following:\nchoose ```\nx_1 , x_2 \\in 0,1,2```\n independently and uniformly at random and then\n for each ```\ni\\in 3,...,n+2```\n given the values of ```\nx_(i-2),x_(i-1)```\n assign to ```\nx_i```\n one of two values in ```\n{b\\in 0,1,2 | x_(i-1)+x_(i-2)+b != 0 \\mod(3)}```\n \nuniformly at random. \nthe assignment to each ```\nx_i```\n is independent for all ```\nx_j```\n such the ```\nj<i-2```\n. \n\n\nI need to prove this algorithm gives a ```\n1/3```\n approximation to the problem described, using expectation(meaning proving that for some X random variable that we assign to this question, ```\nE[X]=1/3```\n)\n I am struggling with defining such X and calculating this, I keep getting 2\\3 instead of 1\\3.\n can anyone help with the calculation?\n  \n    ", "Answer": "\r\nYou can prove that each x_i is uniformly distributed over {0, 1, 2} and pairwise independent by induction. The base case (n=2) is immediate, and the induction step follows from the fact you're given x_i being independent from x_j (with j < i-2), and a simple enumeration of cases.\n\nThe result follows immediately, since P(x_i + x_{i+2} = c_i) is 1/3, and by linearity of expectation, E[X] = n/3.\n\nClarifying the last statement: Let V_i be a random variable such that V_i is 1, if x_i + x_{i+2} = c_i. Otherwise, V_i is 0. Then X = sum(V_i i=1..n), and E[X] = sum(E[V_i] i=1..n) by linearity of expectation. However, E[V_i] = P(x_i + x_{i+2} = c_i) = 1/3.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation algorithms for maximum-weight independent set when vertex's weight could be negative\r\n                \r\nThere are many approximation algorithms for maximum-weight independent set. But most of them assume non-negative weights. Are there any algorithms that work for possible negative weights? \n    ", "Answer": "\r\nIgnore the negative-weight vertices. Consider any independent set that includes a negative-weight vertex. If you remove that vertex, the resulting set is still an independent set, but you've increased its total weight.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "What are the known(polynomial) approximation algorithms to get MIP(Mixed-Integer Programming) solutions from LP relaxation?\r\n                \r\nI want to control these approximation algorithms in time and solution quality with my own approximation algorithm for a problem of mine and the MIP solution itself.\nSo what are the known approximation algorithms that will be created from the LP relaxation of the problem?\nNote: If you wanna explain it I will appreciate it but if you don't names of those algorithms are more than enough.\nThank you all in advance.\n    ", "Answer": "\r\nMIP solvers are very good in finding good solutions quickly. So, Stop on time limit, essentially makes it a polynomial approximation algorithm. It gives you a constant time complexity, and in many cases a good solution. An alternative is to stop on the gap (no good complexity bound). The idea is that MIP solvers show most improvements at the beginning of the search. These stopping conditions exploit that.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Druid Default Distinct Approximation Algorithm\r\n                \r\nIs there a way to modify the default HLL approximation algorithm with ThetaSketch in Druid? So that while querying for count distinct, druid by default uses ThetaSketch instead of HLL.\n    ", "Answer": "\r\nI believe you need to be explicit at query time, e.g. using ```\nAPPROX_COUNT_DISTINCT_DS_THETA```\n versus ```\nAPPROX_COUNT_DISTINCT_DS_HLL```\n\nhttps://druid.apache.org/docs/latest/querying/sql.html#aggregation-functions\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "approximation algorithm for optimization version of partitioning?\r\n                \r\nin optimization version of partitioning problem we want to partition set ```\nX```\n to disjoint subset ```\nA```\n and ```\nB```\n such that ```\nmax(sum(A),sum(B))```\n be minimal.A approximation algorithm has been suggested in wikipedia, but I can't understand why this approximation is 4/3.\n    ", "Answer": "\r\n```\nOPT >= sum(all)/2\n```\n\n\nconsider that the greedy algorithm give you some answer like S1 and S2 where:\n\n\n  max(sum(S1), sum(S2)) > 4/3 OPT\n\n\n(without losing generality assume sum(S1) > sum(S2)) so we have :\n\n\n  sum(S1) > 4/3 OPT >= 2 sum(all)/3 \n\n\nso :\n\n\n  sum(S1) > 4/3 OPT >= 2 sum(all)/3 so sum(S1) > 2 sum(all)/3 \n\n\nso :\n\n\n  sum(S1) > 2 sum(S2) \n\n\nSo in one of the steps of algorithm when sum(S1) was smaller than sum(S2) you must add an element like ```\nA```\n to S1 and after that you didnt add any element to S1\n\nSo ```\nA```\n must be bigger than final state of sum(S2) (because  A +  sum(S1) > 2 sum(S2))\n\nso ```\nA```\n is bigger than current state sum(S1) (because current state of sum(S1) < current state of sum(S2) < final state of sum(S1) < A )\n\nAnd your list is sorted in descending order, so ```\nA```\n must be the first element in the sorted list(biggest element). so your s1 must just contain ```\nA```\n and you have. \n\nalso you know that the sum(OPT) >= A because either it contains A or the other part contains A but sum(OPT) is bigger that sum of other parts element so \n\n```\nsum(OPT) > A\n```\n\n\nbut you have : \n\n\n  A = sum(S1) > 4/3 sum(OPT) > sum(OPT) > A\n\n\nand it is a contradiction :)\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Vogel's approximation algorithm in javascript\r\n                \r\nI have very little experience with Java and am trying to use my little knowledge to interpret some java source code i found online here for Vogel's approximation algorithm into javascript with no luck.\n\nPS: There is also source code for python in the link.\nworker.js\n\n```\nvar scope = self;\nvar thread = {\n    sum: function ( items) {\n        total = 0;\n        for(var i = 0; i < items.length; i++) {\n            total = total+items[i];\n        }\n\n        return total;\n    },\n\n    min: function (costs) {\n        return Math.min.apply(Math, costs);\n    },\n\n    min2: function (costs, min) {\n        j = []\n        for(var i = 0; i<costs.length; i++) {\n            if(costs[i] > min) {\n                j.push(costs[i]);\n            }\n        }\n\n        return Math.min.apply(Math, j);\n    },\n\n    vam: function (demand, supply, costs) {\n        var nRows = supply.length;\n        var ncols = demand.length;\n\n        var supplyLeft = this.sum(supply);\n        var totalCost = 0;\n\n        //stuck here\n    },\n}\n\nself.onmessage = function(e) {\n    thread.vam(e.data.demand, e.data.supply, e.data.costs);\n}\n```\n\n\nThis is section of java source i dont understand\n\n```\nstatic int[] nextCell() throws Exception {\n        Future<int[]> f1 = es.submit(() -> maxPenalty(nRows, nCols, true));\n        Future<int[]> f2 = es.submit(() -> maxPenalty(nCols, nRows, false));\n\n        int[] res1 = f1.get();\n        int[] res2 = f2.get();\n\n        if (res1[3] == res2[3])\n            return res1[2] < res2[2] ? res1 : res2;\n\n        return (res1[3] > res2[3]) ? res2 : res1;\n    }\n\n    static int[] diff(int j, int len, boolean isRow) {\n        int min1 = Integer.MAX_VALUE, min2 = Integer.MAX_VALUE;\n        int minP = -1;\n        for (int i = 0; i < len; i++) {\n            if (isRow ? colDone[i] : rowDone[i])\n                continue;\n            int c = isRow ? costs[j][i] : costs[i][j];\n            if (c < min1) {\n                min2 = min1;\n                min1 = c;\n                minP = i;\n            } else if (c < min2)\n                min2 = c;\n        }\n        return new int[]{min2 - min1, min1, minP};\n    }\n\n    static int[] maxPenalty(int len1, int len2, boolean isRow) {\n        int md = Integer.MIN_VALUE;\n        int pc = -1, pm = -1, mc = -1;\n        for (int i = 0; i < len1; i++) {\n            if (isRow ? rowDone[i] : colDone[i])\n                continue;\n            int[] res = diff(i, len2, isRow);\n            if (res[0] > md) {\n                md = res[0];  // max diff\n                pm = i;       // pos of max diff\n                mc = res[1];  // min cost\n                pc = res[2];  // pos of min cost\n            }\n        }\n        return isRow ? new int[]{pm, pc, mc, md} : new int[]{pc, pm, mc, md};\n    }\n```\n\n\nAny help will be appreciated\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Rectangle approximation algorithm\r\n                \r\nI have an enumeration of just under 32 absolute rectangle sizes and I need to given dimensions and find the best approximation among my enumeration.\n\nIs there any better (ie more readable and maintainable) way than the spaghetti code I am formulating out of lots of nested ```\nif```\n's and ```\nelse```\n's?\n\nAt the moment I have just:\n\n```\nenum imgOptsScale {\n    //Some relative scales\n    w005h005 = 0x8,\n    w010h010 = 0x9,\n    w020h020 = 0xA,\n    w040h040 = 0xB,\n    w070h070 = 0xC,\n    w100h100 = 0xD,\n    w150h150 = 0xE,\n    w200h200 = 0xF,\n    w320h320 = 0x10,\n    w450h450 = 0x11,\n    w200h010 = 0x12,\n    w200h020 = 0x13,\n    w200h070 = 0x14,\n    w010h200 = 0x15,\n    w020h200 = 0x16,\n    w070h200 = 0x17\n};\nimgOptsScale getClosestSizeTo(int width, int height);\n```\n\n\nand I thought I'd ask for help before I got too much further into coding up. I should emphasise a bias away from too elaborate libraries though I am more interested in algorithms than containers this is supposed to run on a resource constrained system.\n    ", "Answer": "\r\nI think I'd approach this with a few arrays of structs, one for horizontal measures and one for vertical measures.\n\nRead through the arrays to find the next larger size, and return the corresponding key. Build the final box measure from the two keys. (Since 32 only allows 5 bits, this is probably not very ideal -- you'd probably want 2.5 bits for the horizontal and 2.5 bits for the vertical, but my simple approach here requires 6 bits -- 3 for horizontal and 3 for vertical. You can remove half the elements from one of the lists (and maybe adjust the ```\n<< 3```\n as well) if you're fine with one of the dimensions having fewer degrees of freedom. If you want both dimensions to be better represented, this will probably require enough re-working that this approach might not be suitable.)\n\nUntested pseudo-code:\n\n```\nstruct dimen {\n    int x;\n    int key;\n}\n\nstruct dimen horizontal[] = { { .x = 10, .key = 0 },\n                              { .x = 20, .key = 1 },\n                              { .x = 50, .key = 2 },\n                              { .x = 90, .key = 3 },\n                              { .x = 120, .key = 4 },\n                              { .x = 200, .key = 5 },\n                              { .x = 300, .key = 6 },\n                              { .x = 10000, .key = 7 }};\n\nstruct dimen vertical[] = { { .x = 10, .key = 0 },\n                           { .x = 20, .key = 1 },\n                           { .x = 50, .key = 2 },\n                           { .x = 90, .key = 3 },\n                           { .x = 120, .key = 4 },\n                           { .x = 200, .key = 5 },\n                           { .x = 300, .key = 6 },\n                           { .x = 10000, .key = 7 }};\n\n/* returns 0-63 as written */\nint getClosestSizeTo(int width, int height) {\n    int horizontal_key = find_just_larger(horizontal, width);\n    int vertical_key = find_just_larger(vertical, height);\n    return (horizontal_kee << 3) & vertical_key;\n}\n\nint find_just_larger(struct dimen* d, size) {\n    int ret = d.key;\n    while(d.x < size) {\n        d++;\n        ret = d.key;\n    }\n    return ret;\n}\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation algorithm in broadcasting\r\n                \r\nWhat does an approximation solution to a braodcasting algorithm mean...\nI have been working on an algorithm which says that it has a solution of 12 approximation.\nWhat does it actually mean..\nCan anybody please help me...\n    ", "Answer": "\r\nIn simple(ish) words it means that the algorithm is guaranteed to produce a result which is within a factor of 12 of the optimum solution.\n\nFor your example, if the aim is to reduce the broadcast latency then a 12 factor approximation would have latency at worst 12 times worse than the latency of the optimum solution.\n\nThe reason why, what may appear a poor ratio, might actually be useful in reality is that it may not be possible in any practical way to actually calculate the optimum, or it may simply take too long for a particular application - hence the best approximation may well be very useful.\n\nIt is also worth noting that the 'factor 12' is the worst case - i.e. it is guaranteed it will not be worse that 12 times the optimum. In practice it may be that results are often much better than this worst case.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Kolmogorov Complexity Approximation Algorithm\r\n                \r\nI'm looking for a algorithm that can compute an approximation of the Kolmogorov complexity of given input string.  So if K is the Kolmogorov complexity of a string S, and t represents time, then the function would behave something like this.. limit(t->inf)[K_approx(t,S)] = K.\n    ", "Answer": "\r\nIn theory, a program could converge on the Kolmogorov complexity of its input string as the running time approaches infinity.  It could work by running every possible program in parallel that is the length of the input string or shorter.  When a program of a given length is found, that length is identified as the minimum length known for now, is printed, and no more programs >= that length are tried.  This algorithm will (most likely) run forever, printing shorter and shorter lengths, converging on the exact Kolmogorov complexity given infinite time.\n\nOf course, running an exponential number of programs is highly intractible.  A more efficient algorithm is to post a code golf on StackOverflow.  A few drawbacks:\n\n\nIt can take a few days before good results are found.\nIt uses vast amounts of our most valuable computing resources, costing thousands of dollars in productivity loss.\nResults are produced with less frequency over time as resources are diverted to other computations.\nThe algorithm terminates prematurely for many inputs, meaning it does not work in general.\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "RGB Similar Color Approximation Algorithm\r\n                \r\nGiven that in RGB we can represent 256^3 combinations = 16,777,216 colors, and since the human eye can only distinguish roughly 10,000,000, there is obviously a surplus of 6,777,216 RGB combinations that chromatically are indistinguishable from counterpart colors.\n\nCompression algorithms work on this basis when approximating out spacial difference in color ranges across a frame I believe. With that in mind, how can one reliably compute whether a given color is within a range of 'similarity' to another? \n\nOf course, 'similarity' will be some kind of arbitrary/tunable parameter that can be tweaked, but this is an approximation anyway. So any pointers, pseudocode, intuitive code samples, resources out there to help me model such a function?\n\nMany thanks for your help\n    ", "Answer": "\r\nThere are many ways of computing distances between colors, the simplest ones being defined on color components in any color space. These are common \"distances\" or metrics between RGB colors (r1,g1,b1) and (r2,g2,b2):\n\n\nL1: abs(r1-r2) + abs(g1-g2) + abs(b1-b2)\nL2: sqrt((r1-r2)² + (g1-g2)²  + (b1-b2)²)\nL∞: max(abs(r1-r2), abs(g1-g2), abs(b1-b2))\n\n\nThese however don't take into account the fact that human vision is less sensitive to color than to brightness. For optimal results you should convert from RGB to a color space that encodes brightness and color separately. Then use one of the above metrics in the new color space, possibly giving more weight to the brightness component and less to the color components.\n\nAreas of color that are indistinguishable form each other are called MacAdam ellipses. The ellipses become nearly circular in the CIELUV and CIELAB color spaces, which is great for computation, but unfortunately going from RGB into these color spaces is not so simple.\n\nJPEG converts colors into YCbCr, where Y is brightness and the two C's encode color, and then halves the resolution of the C components. You could do the same and then use a weighed version of one of the above metrics, for example:\n\n```\ndiff = sqrt(1.4*sqr(y1-y2) + .8*sqr(cb1-cb2) + .8*sqr(cr1-cr2)) \n```\n\n\nThe article on color difference in wikipedia has more examples for different color spaces.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Longest path approximation algorithm from a given node\r\n                \r\nI'm looking for an approximation algorithm for the following problem - I have an unweighted, undirected graph, with cycles, and want to find the longest path starting from a given node.\nI do value speed over performance (so a O(n^5) algorithm would probably be an overkill).\n\nThis is not homework (I swear!) or work related, but I will appreciate any tip you might have.\n    ", "Answer": "\r\n\nI'm looking for an approximation algorithm for the following problem ...\n\nScientists are looking for it as well.  They have also proved that polynomial constant-factor approximation doesn't exist if P ≠ NP.  And the abstract of this article claims that it contains an approximation algorithm for your problem.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Understanding Curve Global Approximation algorithm\r\n                \r\nProblem description\nI am trying to understand and implement the Curve Global Approximation, as proposed here:\nhttps://pages.mtu.edu/~shene/COURSES/cs3621/NOTES/INT-APP/CURVE-APP-global.html\nTo implement the algorithm it is necessary to calculate base function coefficients, as described here:\nhttps://pages.mtu.edu/~shene/COURSES/cs3621/NOTES/spline/B-spline/bspline-curve-coef.html\nI have trouble wrapping my head around some of the details.\n\nFirst there is some trouble with variable nomenclature. Specifically I am tripped up by the fact there is  as function parameter as well as input and . Currently I assume, that first I decide how many knot vectors I want to find for my approximation. Let us say I want 10. So then my parameters are:\n\n\nI assume this is what is input parameter  in the coefficient calculation algorithm?\n\nThe reason this  tripped me up is because of the sentence:\n\n\nLet u be in knot span \n\nIf input parameter  was one of the elements of the knot vector , then there was no need for an interval. So I assume  is actually one of these elements (  ?), defined earlier:\n\nIs that assumption correct?\n\nMost important question. I am trying to get my N to work with the first of the two links, i.e. the implementation of the Global Curve Approximation. As I look at the matrix dimensions (where P, Q, N dimensions are mentioned), it seems that N is supposed to have n rows and h-1 columns. That means, N has rows equal to the amount of data points and columns equal to the curve degree minus one. However when I look at the implementation details of N in the second link, an N row is initialized with n elements. I refer to this:\n\n\nInitialize N[0..n] to 0; // initialization\n\nBut I also need to calculate N for all parameters  which correspond to my parameters  which in turn correspond to the datapoints. So the resulting matrix is of ddimension ```\n( n x n )```\n. This does not correspond to the previously mentioned ```\n( n x ( h - 1 ) )```\n.\nTo go further, in the link describing the approximation algorithm, N is used to calculate Q. However directly after that I am asked to calculate N which I supposedly already had, how else would I have calculated Q? Is this even the same N? Do I have to calculate a new N for the desired amount of control points?\nConclusion\nIf somebody has any helpful insight on this - please do share. I aim to implement this using C++ with Eigen for its usefulness w.r.t. to solving ```\nM * P = Q```\n and matrix calculations. Currently I am at a loss though. Everything seems more or less clear, except for N and especially its dimensions and whether it needs to be calculated multiple times or not.\nAdditional media\n\n\n\nIn the last image it is supposed to say, \"[...] used before in the calculation of Q\"\n    ", "Answer": "\r\nThe 2nd link is telling you how to compute the basis function of B-spline curve at parameter u where the B-spline curve is defined by its degree, knot vector [u0,...um] and control points. So, for your first question, if you want to have 10 knots in your knot vector, then the typical knot vector will look like: \n\n[0, 0, 0, 0, 0.3, 0.7, 1, 1, 1, 1]\n\nThis will be a B-spline curve of degree 3 with 6 control points. \n\nFor your 2nd question, The input parameter u is generally not one of the knots [u0, u1,...um]. Input parameter u is simply the parameter we would like to evaluate the B-spline curve at. The value of u actually varies from 0 to 1 (assuming the knot vector ranges is also from 0 to 1). \n\nFor your 3rd questions, N (in the first link) represents a matrix where each element of this matrix is a Ni,p(tj). So, basically the N[] array computed from 2nd link is actually a row vector of the matrix N in the first link. \n\nI hope my answers have cleared out some of your confusions. \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation algorithm for constrained maximum spanning sub-tree for directed-graph\r\n                \r\nGiven a directed graph, where each vector has a non-negative cost and each vertex has a non-negative profit, how do you find the spanning sub-tree of the graph with maximum profit? I want to constrain the cost is smaller to a given budget. I'm looking for an approximation algorithm for the problem with polynomial time complexity, and the theoretical approximation factor.\n    ", "Answer": "\r\n\n  a spanning sub-tree\n\n\nI'm going to assume that you meant an arborescence. Fix a root u (or just try all vertices for an extra factor of |V|). For every arc a, let xa be a 0-1 indicator variable for membership in the arborescence. For every vertex v, let yv be the same. The obvious integer program is\n\n\n  maximize ∑v profit(v) yv\n  ∑a cost(a) xa ≤ budget\n  ∀v≠u. ∀S&subseteq;V, u&in;S, v∉S. yv ≤ ∑a&in;S×(V&setminus;S) xa\n  ∀a. xa &in; {0, 1}\n  ∀v. yv &in; {0, 1}\n\n\nUnfortunately, the integrality gap is infinite. After having the same problem with several other formulations, I am beginning to suspect rather strongly that there is no approximation algorithm with a “reasonable” approximation ratio. This problem sort of combines the buy-at-bulk mechanic with the budgeted maximum coverage problem, making it possible for the marginal cost of additional coverage to become extremely cheap, which seems as though it should lead to the kind of thresholding behavior that precludes approximation. One way out may be to settle for a bicriteria approximation (i.e., give the approximation algorithm a larger budget).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "When does MST-based approximation algorithm not return optimal solution to TSP?\r\n                \r\nGiven a complete, weighted graph G,such that its edge weights satisfy the triangle inequality but the MST-based approximation algorithm for TSP does not find an optimal solution.\n    ", "Answer": "\r\nYour question is not a question. Did you mean give a complete... ? \n\nThe MST approximation rarely gives the optimal solution. It should be pretty easy to come up with an example.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation Algorithm between two NP compete problems\r\n                \r\nSuppose that a O(n2)-time alpha-approximate algorithm exists for one of the two problems in each of the following pairs:\n\n\nVertex Cover and Independent Set\nIndependent Set and Clique\nMax-Flow and Min-Cut\n\n\nDoes this guarantee that a O(n2)-time alpha-approximate algorithm exists for the other problem in the pair? I know that Clique reduces to Independent Set which in turn reduces to Vertex Cover.\n    ", "Answer": "\r\nNot necessarily, for two reasons.\n\nFirst of all, NP reductions are generally not linear in complexity. Some of them are, but usually a problem of complexity ```\nn```\n will reduce to some other NP problem of size ```\nn^3```\n or something. Even if we found a linear-time 3SAT algorithm, we wouldn't have found linear-time algorithms for all NP-hard problems -- just polynomial algorithms. So if by \"similar\" you mean \"also ```\nn^2```\n\", not in general.\n\nSecondly, approximations don't generally transfer. Because of the non-linear growth in complexity (that's a simplification of why, but it'll do), approximation guarantees generally don't survive the reduction process. As a result, while all NP-complete problems are in a sense comrades in exact solution hardness, they are far from it in approximation hardness.\n\nIn certain specific cases, approximations do transfer (and one of your examples -- left as an exercise for the reader -- most definitely transfers). But it's in no way guaranteed.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fast & accurate atan/arctan approximation algorithm\r\n                \r\nAny fast & accurate atan/arctan approximation function/algorithm out there? Input: x = (0, 1] (minimum x requirement). Output: in radians\n\n```\ndouble FastArcTan(double x)\n{\n    return M_PI_4*x - x*(fabs(x) - 1)*(0.2447 + 0.0663*fabs(x));\n}\n```\n\n\nI've found this function above online but it gave a max error of 1.6 radians which is too big.\n    ", "Answer": "\r\nOP reported a max error of 1.6 radians (92 degrees) which is inconsistent with testing OP's code below, max error input ```\nx```\n: 0 to 1 range of about 0.0015 radians.  I suspect a mis-code or testing outside the range of 0...1.  Did OP means 1.6 milli-radians?\n\n\n\nPerhaps a more accurate ```\na*x^5 + b*x^3 + c*x```\n will still be fast enough for OP.  It is about 4x more accurate on my machine on average and 2x better worst case.  It uses an optimal 3-term polynomial as suggested by @Foon and @Matthew Pope\n\n```\n#include <math.h>\n#include <stdio.h>\n\n#ifndef M_PI_4\n#define M_PI_4 (3.1415926535897932384626433832795/4.0)\n#endif\n\ndouble FastArcTan(double x) {\n  return M_PI_4*x - x*(fabs(x) - 1)*(0.2447 + 0.0663*fabs(x));\n}\n\n#define A 0.0776509570923569\n#define B -0.287434475393028\n#define C (M_PI_4 - A - B)\n#define FMT \"% 16.8f\"\n\ndouble Fast2ArcTan(double x) {\n  double xx = x * x;\n  return ((A*xx + B)*xx + C)*x;\n}\n\nint main() {\n  double mxe1 = 0, mxe2 = 0;\n  double err1 = 0, err2 = 0;\n  int n = 100;\n  for (int i=-n;i<=n; i++) {\n    double x = 1.0*i/n;\n    double y = atan(x);\n    double y_fast1 = FastArcTan(x);\n    double y_fast2 = Fast2ArcTan(x);\n    printf(\"%3d x:% .3f y:\" FMT \"y1:\" FMT \"y2:\" FMT \"\\n\", i, x, y, y_fast1, y_fast2);\n    if (fabs(y_fast1 - y) > mxe1 ) mxe1  = fabs(y_fast1 - y);\n    if (fabs(y_fast2 - y) > mxe2 ) mxe2  = fabs(y_fast2 - y);\n    err1 += (y_fast1 - y)*(y_fast1 - y);\n    err2 += (y_fast2 - y)*(y_fast2 - y);\n  }\n  printf(\"max error1: \" FMT \"sum sq1:\" FMT \"\\n\", mxe1, err1);\n  printf(\"max error2: \" FMT \"sum sq2:\" FMT \"\\n\", mxe2, err2);\n}\n```\n\n\nOutput\n\n```\n ...\n 96 x: 0.960 y:      0.76499283y1:      0.76582280y2:      0.76438526\n 97 x: 0.970 y:      0.77017091y1:      0.77082844y2:      0.76967407\n 98 x: 0.980 y:      0.77529750y1:      0.77575981y2:      0.77493733\n 99 x: 0.990 y:      0.78037308y1:      0.78061652y2:      0.78017777\n100 x: 1.000 y:      0.78539816y1:      0.78539816y2:      0.78539816\nmax error1:       0.00150847sum sq1:      0.00023062\nmax error2:       0.00084283sum sq2:      0.00004826\n```\n\n\n\n\nUnclear why OP's code uses ```\nfabs()```\n given \"Input: x = (0, 1]\".\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "graph algorithm, approximation algorithm\r\n                \r\nAfter removing the leaves of the dfs tree of a random graph , suppose the number of edges left is |S|, can we prove that the matching for that graph will be |S|/2?\n    ", "Answer": "\r\nHere's a proof.\n\nTheorem: Let ```\nT```\n be any tree with ```\ni```\n leaves.  There is a ```\n(|T|-i)/2```\n matching in ```\nT```\n.\n\nProof: by induction.  If ```\nT```\n is a tree with ```\ni```\n leaves, let ```\nT'```\n be the tree that results when removing all the leaves from ```\nT```\n.  ```\nT'```\n has ```\nj <= i```\n leaves.  Similarly, let ```\nT''```\n be the tree that results when removing all the leaves from ```\nT'```\n.  ```\nT''```\n has ```\nk <= j```\n leaves.\n\nApply the theorem by induction to ```\nT''```\n, so there exists a matching of size ```\n(|T''|-k)/2 = (|T|-i-j-k)/2```\n in ```\nT''```\n.  The set of edges ```\nT-T'```\n contains at least ```\nj```\n edges that are not incident to any edge in ```\nT''```\n or to each other (pick one incident to each leaf in ```\nT'```\n), so add those edges to make a matching in ```\nT```\n of size ```\n(|T|-i+j-k)/2```\n.  Since ```\nj >= k```\n, this is at least ```\n(|T|-i)/2```\n edges. QED.\n\nI've glossed over the floor/ceiling issues with the /2, but I suspect the proof would still work if you included them.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "3 approximation algorithm(randomised or deterministic)\r\n                \r\nLet 3 colorful sets problem. Given M sets of size three over {1...n} elements. We are given in other words sets S1, S2, ... , Sm where, for every i, Si = {x, y, z} for some x, y, z ∈ {1, ... , n}. What I want to find is to pick a set of elements E ⊆ {1, ... , n} so that to maximize the number of sets that contain exactly one element in E, namely, to maximize the |{i |Si ∩ E| = 1}| . A solution could use a 3 approximation polynomial time algo.\nI am thinking a randomized algo that guarantee the approximation ration or a deterministic one. I have some ideas but I am not sure how to actually implement it. Any help would be appreciated.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation algorithm. How to find shortest cycle that gains >= x points\r\n                \r\nThere's a map with points:\n\n\n\nThe green number next to each point is that point's ID and the red number is the bonus for that point. I have to find fastest cycle that starts and ends at the point #1 and that gains at least x (15 in this case) bonus points. I can use cities several times; however, I will gain bonus points only once.\nI have to do this using Approximation algorithm, but I don't really know where to start.\n\nThe output would look like this:\n\n```\n(1,3,5,2,1) (11.813 length)\n```\n\n\n\n    ", "Answer": "\r\nIsn't that a NP problem?  If it is, you cannot find the fastest one without testing all possibilities, which would take rather long.\n\nThe problem is similar to the Traveling Sales Man, IMHO. The best known solution for this problem so far is the Ant Colony Solution. This solution does not guarantee to always find the best solution possible, but it will find at least a pretty good solution within an acceptable amount of time.\n\nI think it may be possible to modify the Ant Colony Solution to also address your problem, by taking the bonus points into account somehow. Probably not the answer you hopped to hear, but the best one I have to offer at the moment.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Debugging Polygon Approximation Algorithm Code C++ [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                This question is unlikely to help any future visitors; it is only relevant to a small geographic area, a specific moment in time,  or an extraordinarily narrow situation that is not generally applicable to the worldwide audience of the internet. For help making  this question more broadly applicable, visit the help center.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI was wondering if it would be possible for you to help me debug some code of mine. I've written the following code for a class project where we are implementing a polygon approximation algorithm. But I can't seem to get the code to do what I'd like for it to do. Here's a link to a wiki article of the algorithm: http://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm\n\nThe problem I'm having is that the second array, closedStack either isn't having the values inside updated properly, or it's not being displayed properly. But the first array, the one that is read in from the file displays properly. I also kept getting the closedStack error of it being full, so I changed the if statements to not use the fileSize variable so that may also have an issue with it. If you need me to explain any of the logic or variables, etc, just ask and I'll explain. \n\n```\n#include \"math.h\"\n#include <iostream>\n#include <fstream>\n#include \"glut.h\"\n\nusing namespace std;\n\nstruct point{\n    int x, y;\n};\n\nvoid display(void);\nvoid fileRead();\nvoid oPush(point);\nvoid cPush(point);\npoint oPop();\nint deviation();\n\npoint pixel[2000];\nint fileSize = 0;\nint errorAllowed= 5;\nint errorDeviation=0;\nint oTop = 0;\nint cTop = 0;\npoint first;\npoint last;\npoint openStack[5000];\npoint closedStack[5000];\nint V1 = 0;\nint V2 = 0;\n\nvoid main(int argc, char **argv){\n\n    fileRead();\n\n    glutInit(&argc, argv);\n    glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB);\n    glutInitWindowSize(500,500);\n    glutInitWindowPosition(75,75);\n    glutCreateWindow(\"Ramer's Iterative Algorithm\");\n    glutDisplayFunc(display);\n    gluOrtho2D(0,500,0,500);\n\n    fileSize = fileSize/2;\n\n    int tmp1 = pixel[0].x+pixel[0].y;\n    int tmp2 = 0;\n\n\n    for(int i = 0; i<2000; i++){\n        tmp2 = pixel[i].x+pixel[i].y;\n\n        if(tmp2 < tmp1){\n            tmp1 = tmp2;\n            V1 = i;\n        }\n    }\n\n    for(int i = 0; i<2000; i++){\n        tmp2 = pixel[i].x+pixel[i].y;\n\n        if(tmp2 > tmp1){\n            tmp1 = tmp2;\n            V2 = i;\n        }\n    }\n\n    oPush(pixel[V1]);\n    oPush(pixel[V2]);\n    oPush(pixel[V1]);\n\n    do{\n        first = oPop();\n        last = oPop();\n\n        int Mid = deviation();\n\n        if(errorDeviation>errorAllowed){\n            oPush(last);\n            oPush(pixel[Mid]);\n            oPush(first);\n        }\n\n        else if(errorDeviation<=errorAllowed){\n            oPush(last);\n            cPush(first);\n        }\n\n    }while(oTop>=2);\n\n    glutMainLoop();\n\n    cin >> V1;\n\n}\n\nvoid display(void){\n    glClearColor(0,0,0,0);\n    glClear(GL_COLOR_BUFFER_BIT);\n    glColor3f(1,1,1);\n\n    glBegin(GL_POINTS);\n    for(int i=0; i<2000; i++)\n        glVertex2i(pixel[i].x,pixel[i].y);\n    glEnd();\n\n    glColor3f(1,0,0);\n    glBegin(GL_LINE_STRIP);\n    for(int i=0;i<=cTop; i++)\n        glVertex2i(closedStack[i].x,closedStack[i].y);\n    glEnd();\n\n    glFlush();\n}\n\nvoid fileRead(){\n    char fileName[20];\n\n    cout << \"Enter the name of the file you would like to parse data from: \";\n    cin >> fileName;\n\n    ifstream boundary;\n    boundary.open(fileName);\n    if(boundary.fail()){\n        cout << \"Could not open '\" << fileName << \"' for reading.\\n\";\n        exit(0);\n    }\n\n    for(int i=0; !boundary.eof(); i++){\n        boundary >> pixel[i].x;\n        boundary >> pixel[i].y;\n        fileSize ++;\n    }\n\n}\n\nvoid oPush(point p){\n    if(oTop>fileSize){\n        cout << \"Full Stack--Open\\n\";\n        exit(0);\n    }\n\n    else{\n        oTop++;\n        openStack[oTop]= p;\n    }\n}\n\nvoid cPush(point p){\n    if(cTop>10000){\n        cout << \"Full Stack--Closed\\n\";\n        exit(0);\n    }\n\n    else{\n        cTop++;\n        closedStack[cTop]= p;\n    }\n}\n\npoint oPop(){\n    point temp;\n\n    if(oTop<=0){\n        cout << \"Stack Empty\\n\";\n        exit(0);\n    }\n\n    else{\n        temp = pixel[oTop];\n        oTop--;\n    }\n    return temp;\n}\n\nint deviation(){\n    float y = last.y-first.y;\n    float x = last.x-first.x;\n    float theta = atan(y/x);\n    int most = 0;\n\n    for(int i = V1+1; i < V2; i++){\n        float ped = (-(pixel[i].x-first.x)*sin(theta))+((pixel[i].y-last.y)*cos(theta));\n        float errDev= abs(ped);\n\n        if(errDev>most)\n            most = i;\n        errorDeviation = (int)errDev;\n    }\n\n    return most;\n}\n```\n\n\nTo elaborate on V1 and V2, they are supposed to be the bottom left and top right most points of the array. The loop that checks I've simplified to:\n\n```\nfor(int i = 0; i<fileSize; i++){\ntmp2 = (pixel[i].x)+(pixel[i].y);\n\nif(tmp2 <= tmp1){\n    tmp1 = tmp2;\n    V1 = i;\n}\n\nif(tmp2 >= tmp1){\n    tmp1 = tmp2;\n    V2 = i;\n}\n```\n\n\nThe loop adds the point's x+y together, and then checks it against the previous point to update it. The point with the lowest x+y should end up being V1 and the highest should be V2. But I don't think the loop is functioning properly. By zooming in on what get's drawn, the second portion of the display loop only has three points. It looks like the first and second point of the initial array as well as 0,0 get plotted. Not sure why that's happening.\n    ", "Answer": "\r\nAs it stands, your question is really too broad. You need to make some attempt to narrow it down yourself. Here's a good checklist: http://msmvps.com/blogs/jon_skeet/archive/2012/11/24/stack-overflow-question-checklist.aspx\n\nYou will not fix your problem by trying to write a complete working program in one shot. Visual output is invaluable, but it cannot replace textual output of the actual list of points. Your question and comments suggest that you're trying to debug this just by looking at the final visual output. You need to break this down into smaller pieces and look at the data in between each step so that you can understand where your program is diverging from your expectation.\n\nAs you say \"the second array, closedStack either isn't having the values inside updated properly, or it's not being displayed properly\". The first step to solving this problem is to find out which of these is the case! Does the array have the wrong values, or is it being displayed incorrectly? You can find that out by printing out the intermediate values and comparing them to what you expect when you solve the problem by hand. If the input is too large, create a smaller set of input points that you can work with by hand. Once you have narrowed it down you will either find that you can solve the problem yourself or that you have a much more focused question to ask that will elicit better answers.\n\n\n\nFrom a rough inspection, I'd guess:\n\n\nIt seems likely you are misunderstanding the input. You haven't described what it is. If it's a sequence of points that form a piece-wise linear curve then it's not clear why it would be helpful to pick out the elements that are southwest-most and northeast-most (assuming east is +ve x and north +ve y). If it's a cloud of points then you'll need to do more than just pick out the corners to get the rest in a meaningful order.\nIt seems your function ```\ndeviation()```\n makes the assumption that index first is less than index last. But there appears to be nothing to guarantee this. For a start, perhaps V1 has a greater index than V2. Even if it doesn't, you populate the initial stack with [V1, V2, V1], guaranteeing that ```\ndeviation()```\n will see inputs where ```\nfirst```\n > ```\nlast```\n. While I'm certainly not going to guarantee that there's nothing wrong with ```\ndeviation()```\n, my guess is that there is a problem with what you're feeding into it.\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximate string matching algorithms\r\n                \r\nHere at work, we often need to find a string from the list of strings that is the closest match to some other input string. Currently, we are using Needleman-Wunsch algorithm. The algorithm often returns a lot of false-positives (if we set the minimum-score too low), sometimes it doesn't find a match when it should (when the minimum-score is too high) and, most of the times, we need to check the results by hand. We thought we should try other alternatives.\n\nDo you have any experiences with the algorithms?\nDo you know how the algorithms compare to one another?\n\nI'd really appreciate some advice.\n\nPS: We're coding in C#, but you shouldn't care about it - I'm asking about the algorithms in general.\n\n\n\nOh, I'm sorry I forgot to mention that.\n\nNo, we're not using it to match duplicate data. We have a list of strings that we are looking for - we call it search-list. And then we need to process texts from various sources (like RSS feeds, web-sites, forums, etc.) - we extract parts of those texts (there are entire sets of rules for that, but that's irrelevant) and we need to match those against the search-list. If the string matches one of the strings in search-list - we need to do some further processing of the thing (which is also irrelevant).\n\nWe can not perform the normal comparison, because the strings extracted from the outside sources, most of the times, include some extra words etc.\n\nAnyway, it's not for duplicate detection.\n    ", "Answer": "\r\nOK, Needleman-Wunsch(NW) is a classic end-to-end (\"global\") aligner from the bioinformatics literature.  It was long ago available as \"align\" and \"align0\" in the FASTA package.  The difference was that the \"0\" version wasn't as biased about avoiding end-gapping, which often allowed favoring high-quality internal matches easier.  Smith-Waterman, I suspect you're aware, is a local aligner and is the original basis of BLAST.  FASTA had it's own local aligner as well that was slightly different.  All of these are essentially heuristic methods for estimating Levenshtein distance relevant to a scoring metric for individual character pairs (in bioinformatics, often given by Dayhoff/\"PAM\", Henikoff&Henikoff, or other matrices and usually replaced with something simpler and more reasonably reflective of replacements in linguistic word morphology when applied to natural language).\n\nLet's not be precious about labels:  Levenshtein distance, as referenced in practice at least, is basically edit distance and you have to estimate it because it's not feasible to compute it generally, and it's expensive to compute exactly even in interesting special cases:  the water gets deep quick there, and thus we have heuristic methods of long and good repute.\n\nNow as to your own problem:  several years ago, I had to check the accuracy of short DNA reads against reference sequence known to be correct and I came up with something I called \"anchored alignments\".\n\nThe idea is to take your reference string set and \"digest\" it by finding all locations where a given N-character substring occurs.  Choose N so that the table you build is not too big but also so that substrings of length N are not too common.  For small alphabets like DNA bases, it's possible to come up with a perfect hash on strings of N characters and make a table and chain the matches in a linked list from each bin.  The list entries must identify the sequence and start position of the substring that maps to the bin in whose list they occur.  These are \"anchors\" in the list of strings to be searched at which an NW alignment is likely to be useful.\n\nWhen processing a query string, you take the N characters starting at some offset K in the query string, hash them, look up their bin, and if the list for that bin is nonempty then you go through all the list records and perform alignments between the query string and the search string referenced in the record.  When doing these alignments, you line up the query string and the search string at the anchor and extract a substring of the search string that is the same length as the query string and which contains that anchor at the same offset, K.\n\nIf you choose a long enough anchor length N, and a reasonable set of values of offset K (they can be spread across the query string or be restricted to low offsets) you should get a subset of possible alignments and often will get clearer winners.  Typically you will want to use the less end-biased align0-like NW aligner.\n\nThis method tries to boost NW a bit by restricting it's input and this has a performance gain because you do less alignments and they are more often between similar sequences.  Another good thing to do with your NW aligner is to allow it to give up after some amount or length of gapping occurs to cut costs, especially if you know you're not going to see or be interested in middling-quality matches.\n\nFinally, this method was used on a system with small alphabets, with K restricted to the first 100 or so positions in the query string and with search strings much larger than the queries (the DNA reads were around 1000 bases and the search strings were on the order of 10000, so I was looking for approximate substring matches justified by an estimate of edit distance specifically).  Adapting this methodology to natural language will require some careful thought:  you lose on alphabet size but you gain if your query strings and search strings are of similar length.\n\nEither way, allowing more than one anchor from different ends of the query string to be used simultaneously might be helpful in further filtering data fed to NW.  If you do this, be prepared to possibly send overlapping strings each containing one of the two anchors to the aligner and then reconcile the alignments... or possibly further modify NW to emphasize keeping your anchors mostly intact during an alignment using penalty modification during the algorithm's execution.\n\nHope this is helpful or at least interesting.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "approximate algorithm for top-k query in hive?\r\n                \r\neveryone,in hive ,we use \n\n```\nselect word,count(*) as cnt from table group by word order by cnt limit N\n```\n\n\nfor top-N query.\nAs we kown the speed is not fast,i learn about some approximate algorithm for top-k query ,such as countsketch algorithm or another algorithm.\nCould we add approximate algorithm to hive for speed up top-k query?\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Logarithm function-approximation algorithm\r\n                \r\nI created a function to calculate the parameters of a logarithm-function. \n\nMy aim is to predict the future results of data points that follow a logarithm function. But what is the most important is that my algorithm fits the last results better than the whole data points as it is the prediction that matters. I currently use Mean Squared Error to optimize my parameters but I do not know how to weight it such as it takes my most recent data points as more important than the first ones.\n\n\nHere is my equation:\n\n\ny = C * log( a * x + b )\n\n\nHere is my code:\n\n```\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\ndef approximate_log_function(x, y):\n\n    C = np.arange(0.01, 1, step = 0.01)\n    a = np.arange(0.01, 1, step = 0.01)\n    b = np.arange(0.01, 1, step = 0.01)\n\n    min_mse = 9999999999\n    parameters = [0, 0, 0]\n\n    for i in np.array(np.meshgrid(C, a, b)).T.reshape(-1, 3):\n\n        y_estimation = i[0] * np.log(i[1] * np.array(x) + i[2])  \n        mse = mean_squared_error(y, y_estimation)\n\n        if mse < min_mse:\n            min_mse = mse\n            parameters = [i[0], i[1], i[2]]\n\nreturn (min_mse, parameters)\n```\n\n\n\nYou can see in the image below the orange curve is the data I have and the blue line is my fitted line. We see that the line stretch a bit away from the line on the end and I would like to avoid that to improve the prediction from my function.\n\nlogarithm function graph\n\nMy question is twofold:\n\n\nIs this actually the best way to do it or is it best to use another function (such as the increasing form of an Exponential Decay)? (y = C ( 1 - e-kt ), k > 0)\nHow can I change my code so that the last values are more important to be fitted than the first ones. \n\n    ", "Answer": "\r\nUsually, in non-linear least-squares, the inverse of the y values is taken as weight, that essentially eliminates outliers, you can expand on that idea by adding a function to calculate the weight based on the x position. \n\n```\ndef xWeightA(x):\n    container=[]\n    for k in range(len(x)):\n        if k<int(0.9*len(x)):\n           container.append(1)\n        else:\n            container.append(1.2)\n   return container\n\ndef approximate_log_function(x, y):\n\n    C = np.arange(0.01, 1, step = 0.01)\n    a = np.arange(0.01, 1, step = 0.01)\n    b = np.arange(0.01, 1, step = 0.01)\n\n    min_mse = 9999999999\n    parameters = [0, 0, 0]\n    LocalWeight=xWeightA(x)\n\n    for i in np.array(np.meshgrid(C, a, b)).T.reshape(-1, 3):\n\n        y_estimation = LocalWeight*i[0] * np.log(i[1] * np.array(x) + i[2])  \n        mse = mean_squared_error(y, y_estimation)\n\n        if mse < min_mse:\n            min_mse = mse\n            parameters = [i[0], i[1], i[2]]\n\n    return (min_mse, parameters)\n```\n\n\nAlso, it looks like you're evaluating through the complete objective function, that makes the code to take to much time to find the minimum (at least on my machine). You can use curve_fit or polyfit as suggested, but if the goal is to generate the optimizer try adding an early break or a random search through the grid. Hope it helps \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Hitting Set algorithm approximation\r\n                \r\nI want to solve the problem below from a textbook I am reading, but I am not sure how to go about it. In fact I am not sure if it is correct at all since I think we would need the maximum frequency of an element in the sets and not the maximum size of a set, a value that I has no use I can think of.\n\nWe have a set A = {a 1 .....a n } and\na collection of subsets of A, say B 1 , B 2 , ..., B m . Each element a i ∈ A has a weight w i > 0. The\nproblem is to find a subset H ⊆ A such that the total weight of the elements in H is minimized, and at\nthe same time, H intersects all the subsets of the collection, i.e., H ∩ B i not ∅ for every i = 1, ..., m.\nLet b = max i |B i | be the maximum size of the subsets B 1 , B 2 , ..., B m . Give a polynomial-time\nb-approximation algorithm for this problem.\n    ", "Answer": "\r\nOne possible answer is to solve the LP relaxation and take all elements whose indicator is greater than or equal to 1/b. Proof that this is a correct b-approximation left as an exercise.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How does C compute sin() and other math functions?\r\n                \r\nI've been poring through .NET disassemblies and the GCC source code, but can't seem to find anywhere the actual implementation of ```\nsin()```\n and other math functions... they always seem to be referencing something else.\n\nCan anyone help me find them?  I feel like it's unlikely that ALL hardware that C will run on supports trig functions in hardware, so there must be a software algorithm somewhere, right? \n\n\n\nI'm aware of several ways that functions can be calculated, and have written my own routines to compute functions using taylor series for fun.  I'm curious about how real, production languages do it, since all of my implementations are always several orders of magnitude slower, even though I think my algorithms are pretty clever (obviously they're not).\n    ", "Answer": "\r\nIn GNU libm, the implementation of ```\nsin```\n is system-dependent. Therefore you can find the implementation, for each platform, somewhere in the appropriate subdirectory of sysdeps.\n\nOne directory includes an implementation in C, contributed by IBM. Since October 2011, this is the code that actually runs when you call ```\nsin()```\n on a typical x86-64 Linux system. It is apparently faster than the ```\nfsin```\n assembly instruction. Source code: sysdeps/ieee754/dbl-64/s_sin.c, look for ```\n__sin (double x)```\n.\n\nThis code is very complex. No one software algorithm is as fast as possible and also accurate over the whole range of x values, so the library implements several different algorithms, and its first job is to look at x and decide which algorithm to use.\n\n\nWhen x is very very close to 0, ```\nsin(x) == x```\n is the right answer.\nA bit further out, ```\nsin(x)```\n uses the familiar Taylor series. However, this is only accurate near 0, so...\nWhen the angle is more than about 7°, a different algorithm is used, computing Taylor-series approximations for both sin(x) and cos(x), then using values from a precomputed table to refine the approximation.\nWhen |x| > 2, none of the above algorithms would work, so the code starts by computing some value closer to 0 that can be fed to ```\nsin```\n or ```\ncos```\n instead.\nThere's yet another branch to deal with x being a NaN or infinity.\n\n\nThis code uses some numerical hacks I've never seen before, though for all I know they might be well-known among floating-point experts. Sometimes a few lines of code would take several paragraphs to explain. For example, these two lines\n\n```\ndouble t = (x * hpinv + toint);\ndouble xn = t - toint;\n```\n\n\nare used (sometimes) in reducing x to a value close to 0 that differs from x by a multiple of π/2, specifically ```\nxn```\n × π/2. The way this is done without division or branching is rather clever. But there's no comment at all!\n\n\n\nOlder 32-bit versions of GCC/glibc used the ```\nfsin```\n instruction, which is surprisingly inaccurate for some inputs. There's a fascinating blog post illustrating this with just 2 lines of code.\n\nfdlibm's implementation of ```\nsin```\n in pure C is much simpler than glibc's and is nicely commented. Source code: fdlibm/s_sin.c and fdlibm/k_sin.c\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Understanding Polynomial TIme Approximation Scheme\r\n                \r\nIs an approximation algorithm the same as a Polynomial Time Approximation Algorithm (PTAS)? E.g. It can be shown that A(I) <= 2 * OPT(I) for vertex cover. Does it mean that Vertex Cover has a 2-polynomial time approximation algorithm or a PTAS?\n\nThanks! \n\nNote: The text in Italics is the edit I made after I posted my question.\n    ", "Answer": "\r\nNo, this isn't necessarily the case. A PTAS is an algorithm where given any ε > 0, you can approximate the answer to a factor of (1 + ε) in polynomial time. In other words, you can get arbitrarily good approximations.\n\nSome problems are known (for example, MAX-3SAT) that have approximation algorithms for specific factors (for example, 5/8), but where it's known that unless P = NP there is a hard limit to how well the problem can be approximated in polynomial time. For example, the PCP theorem says that MAX-3SAT doesn't have a polynomial-time 7/8 approximation unless P = NP. It's therefore possible that MAX-3SAT has a PTAS, but only if P = NP.\n\nHope this helps!\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Show that 2 times optimal approximation algorithm for traveling salesman (TSP) does not compute an optimal solution\r\n                \r\nI have a review for the final exam and this question has been particularly confusing for me. I need an example on 3 vertices that the 2 times optimal approximation algorithm for traveling salesman problem (TSP) does not compute a 2 times optimal solution, if the triangular inequality does not hold for the cost. \nI tried an example with a triangle with costs of sides 1, 1, and 10. However, to get the Hamiltonian cycle, all three sides have to be traversed anyways. Then the optimal solution would be no different than the approximation solution with this algorithm. Am I looking at this all wrong? I would appreciate any help on this.\n    ", "Answer": "\r\nIf you have vertices A, B, C, with edge costs wAB, wAC and wBC, and suppose the triangle inequality doesn't hold. Say wBC > wAB + wAC.\n\nThen, suppose we're starting at A, the approximation algorithm will find a minimum spanning tree with root A. This is:\n\n```\n  A\n / \\\nB   C\n```\n\n\nThe solution from the approximation algorithm is the pre-order enumeration of this tree (returning to A), which is A->B->C->A. This has total weight wAB + wBC + wCA. However, the path A->B->A->C->A has weight wAB + wAC + (wAB + wAC) < wAB + wAC + wBC = wAB + wBC + wCA. The < step here uses the original assumption about the triangle inequality not holding.\n\nBy picking wBC large enough, we can make the approximation arbitrarily bad (and for example, worse than 2 times optimal). For example, with your weights of 1, 1, 10, the optimal path has total cost 4, but the approximation algorithm gives 12.\n\nThe error in your thinking was that, on seeing that the approximation algorithm generates a hamiltonian cycle, inferring that any solution to TSP must be a hamiltonian cycle.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Finding approximation ratio of an algorithm\r\n                \r\nI am trying to find the solution of the traveling salesman problem using various techniques. I have tested my code for some sample data. The optimal solution is known. Now I want to find the approximation ratio of my algorithm. how will I do that? Suppose, my code gives the cost of C(x) and the optimal cost is C(y). Then will the value of C(x)/C(y) be the approximation ratio or there is other way to find this out?\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "which sorting algorithms give near / approximate sort sooner?\r\n                \r\nWhich sorting algorithms produce intermediate orderings which are good approximations?\n\nBy \"good approximation\" I mean according to metrics such as Kendall's tau and Spearman's footrule for determining how \"far\" an ordered list is from another (in this case, the exact sort)\n\nThe particular application I have in mind is where humans are doing subjective pairwise comparison and may not be able to do all n log n comparisons required by, say, heapsort or best-case quicksort.\n\nWhich algorithms are better than others at getting the list to a near / approximate sort sooner?\n    ", "Answer": "\r\nYou may want to check out the shell sort algorithm.\n\nAFAIK it is the only algorithm you can use with a subjective comparison (meaning that you won't have any hint about median values or so) that will get nearer to the correct sort at every pass.\n\nHere is some more information http://en.wikipedia.org/wiki/Shell_sort\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "WSClock algorithm as approximation\r\n                \r\nI have a question regarding the WSClock algorithm used for page replacement in operating systems.\n\nAs far as I understand, the WSClock combines the features of both Working Set (a page is in the WS if it was referenced to in last T time intervals, so the last reference time is stored for the each page with R&M bits) and the Clock algorithm (page frame table is stored as a circular list, when page fault occurs the list is traversed searching for the pages which are not present in the working set).\n\nOn each page reference the HW sets its R bit to 1. On some time intervals the OS resets R bits for all pages to 0.\n\nWhen a page fault occurs the algorithm traverses over the list and does the following for each page:\n\n```\n1) If R == 1  : set R = 0, set t = process_t, go further\n2) Else if R == 0 and (t - process_t) <= T, go further\n3) Else if R == 0 and (t - process_t) > T \n        1) If M = 1 - set page for writing to disk, go further\n        2) If M = 0 - this is our victim, evict\n\nIf there are no pages for immediate evict are found - traverse until a disk write for any marked page is finished, then evict.\nIf there are no pages scheduled for a disk write - take the one which was referenced to the most time ago.\n```\n\n\nThe algorithm looks just fine for me except for one thing:\n\nThe last time a page was reference to is changed only on one occasion: if a page fault has occured after this page was referenced to (R = 1) but the OS has not reset the R bit to 0 yet (R = 0). So as I understand this - the last used time is only approximated by that approach which leads to a better performance.\n\nSo for really-really frequently used pages which are no doubt present in the WS of the process everything is good: their reference frequency is higher than the frequency of the OS resetting event.\n\nBut there should be some unlucky pages which are referenced to pretty frequently but unfortunately for them no page faults happen after these references. But they are also so unlucky that when page faults occurs - they look like they have not been referenced to although this is not true. \n\nSo from what I see it looks like the OS takes snapshots of the working set only during page faults events. These snapshots represent working set of pages being referred to in time intervals between R bit resetting and page faults. \n\nWhy is that a good approximation of the real work set?\n\nIs the quality of the approximation (and thus the effectiveness of the algorithm) determined by the value of the time interval when R bits are reset? \n\nSo this value should not be too low to be able to catch most of these unlucky pages I've mentioned above but also not be too big to be able not to catch pages which are really already not in the Working Set?                                                                                                                                                                                                     \n    ", "Answer": "\r\nActually here is the explanation:\nThis algorithm uses an approximation of the Working Set.\n\nThis approximation stores some part of the real actual working set of a process. Also it may not to store some part of this actual working set.\n\nThe operating system developer task is to find algorithm's parameters (working set presence time interval, resetting bit time interval) so that this approximation would be good enough for the algorithm to work fast enough and also have acceptable number of page faults compared to the algorithm which works with the real actual working set all the time.\n\nAlso it is worth mentioning that the parameters can be adjusted dynamically by the OS during computer work for the algorithm to stay a close enough approximation for the actual Working Set.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation for set cover\r\n                \r\nI started learning about approximation algorithms,\nI'm reading a book about that and I don't understand the analysis for the set cover algorithm.\n\nCan someone please explain lemma 2.3 ? \nit's short but I don't understand it...\n\nhttp://view.samurajdata.se/psview.php?id=0482e9ff&page=13\n    ", "Answer": "\r\nThe algorithm is assigning a \"price\" ```\nprice(e)```\n to each element of the universe ```\nU```\n where that price is the cost of the set ```\nS```\n used to cover ```\ne```\n divided by the number of elements newly covered by the set ```\nS```\n (any elements already covered must have been assigned a lower price by a previous set due to the definition of the algorithm).\n\nThere exists an optimal solution which chooses a set of sets with total cost ```\nOPT```\n. As that solution covers all elements, it certainly covers whatever elements have not yet been covered. Covering the rest of the elements (the set ```\nCBar```\n in the notation of the proof) at cost ```\nOPT```\n would mean covering each element at cost-effectiveness ```\nOPT/|CBar|```\n by the definition of cost-effectiveness (aka price). As the optimal solution contains a set which covers all remaining elements, suppose we pick a set ```\nS```\n from the optimal solution which covers at least one remaining element (```\ne_k```\n in lemma 2.3). Note that we are choosing the set with the best cost-effectiveness, so its cost-effectiveness must be at least as good as the average cost-effectiveness of the sets in the optimal solution of ```\nOPT/|CBar|```\n.\n\nThe last part is that due to the definitions, ```\n|CBar|=n-(k-1)=n-k+1```\n as ```\nk-1```\n elements have already been covered and we are looking at covering element ```\nk```\n. Therefore, the cost-effectiveness of ```\nS```\n and therefore ```\nprice(e_k)```\n is bounded by ```\nOPT/(n-k+1)```\n.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to QuickCheck an approximative algorithm?\r\n                \r\nI am testing an approximative algorithm (FastDTW) against the optimal solution by calculating the relative error and comparing that to the errors given in the paper [1]. \n\nProblem is that the errors can get much larger than the ones given in the paper and without setting the tolerance to \"accept all\" there is no way that all tests pass.\n\nIs there a way to tell ```\nQuickCheck```\n that I expect only ```\nn```\n of the tests to pass? I see that there is the function ```\ncover```\n. But just wrapping the test in that does not seem to work as expected.\n\nAlternatively I could run the test several times manually and pass if at least ```\nn```\n tests pass, but I hope that this can be achieved through ```\nQuickCheck```\n.\n\nEdit in response to Carsten\n\nI wrapped like this:\n\n```\nactualTest :: [Double] -> [Double] -> Bool\nactualTest = ... -- runs dtw and fastDtw, compares errors against goal\n\ncoverTest :: Property\ncoverTest = cover True percentage label actualTest\n```\n\n\nBut I am not sure about the first parameter ```\nclass```\n, or the label one. Thinking about it more ... I guess ```\ncover```\n is used to ensure that at least a certain percentage of tests conform to a certain condition.\n\n[1] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.432.4253&rep=rep1&type=pdf#page=64\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "error bound in function approximation algorithm\r\n                \r\nSuppose we have the set of floating point number with \"m\" bit mantissa and \"e\" bits for exponent. Suppose more over we want to approximate a function \"f\".\n\nFrom the theory we know that usually a \"range reduced function\" is used and then from such function we derive the global function value.\n\nFor example let x = (sx,ex,mx) (sign exp and mantissa) then...\nlog2(x) = ex + log2(1.mx) so basically the range reduced function is \"log2(1.mx)\".\n\nI have implemented at present reciprocal, square root, log2 and exp2, recently i've started to work with the trigonometric functions. But i was wandering if given a global error bound (ulp error especially) it is possible to derive an error bound for the range reduced function, is there some study about this kind of problem? Speaking of the log2(x) (as example) i would lke to be able to say...\n\n\"ok i want log2(x) with k ulp error, to achieve this given our floating point system we need to approximate log2(1.mx) with p ulp error\"\n\nRemember that as i said we know we are working with floating point number, but the format is generic, so it could be the classic F32, but even for example e=10, m = 8 end so on.\n\nI can't actually find any reference that shows such kind of study. Reference i have (i.e. muller book) doesn't treat the topic in this way so i was looking for some kind of paper or similar. Do you know any reference?\n\nI'm also trying to derive such bound by myself but it is not easy...\n    ", "Answer": "\r\nThere is a description of current practice, along with a proposed improvement and an error analysis, at https://hal.inria.fr/ensl-00086904/document. The description of current practice appears consistent with the overview at https://docs.oracle.com/cd/E37069_01/html/E39019/z4000ac119729.html, which is consistent with my memory of the most talked about problem being the mod pi range reduction of trigonometric functions.\n\nI think IEEE floating point was a big step forwards just because it standardized things at a time when there were a variety of computer architectures, so lowering the risks of porting code between them, but the accuracy requirements implied by this may have been overkill: for many problems the constraint on the accuracy of the output is the accuracy of the input data, not the accuracy of the calculation of intermediate values.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Why in max-cut ( weigthted graph )we can't do an approximation lower than 1/2?\r\n                \r\nI have been trying to solve the Max-Cut problem using  approximation algorithms, but I still don't get how the best approximation I can get is 1/2. I could use any help or paper of proof of this fact.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "An approximate algorithm for finding Steiner Forest\r\n                \r\nConsider a weighted graph G=(V,E,w). We are given a family of subsets of vertices V_i.\n\nA Steiner Forest is a forest that for each subset of vertices V_i connects all of the vertices in this subset with a tree.\n\nExample: only one subset V_1 = V. In this case a Steiner forest is a spanning tree of the whole graph.\n\nExample: a graph P4 (path with 4 vertices) and two subsets: V_1 = {v1, v4} and V_2 = {v2, v3}. The Steiner tree for this example is the whole graph.\n\nEnough theory. Finding such a forest with minimal weight is difficult (NP-complete). Do you know any quicker approximate algorithm to find such a forest with non-optimal weight?\n    ", "Answer": "\r\nChapter 20 of Approximation Algorithms by Vijay Vazirani describes a schema for generating a Steiner Forest. The analysis uses LP-duality, which he uses to determine the factor of the algorithm:\n\n(This is a factor-2 algorithm but in practice it probably fares quite well)\n\nApproximation Algorithms\n\nAlso: see the note in 22.5 that describes three papers for further reading, including a survey of the topic.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How does this division approximation algorithm work?\r\n                \r\nI'm working on a game with a software renderer to get the most accurate PS1 look. As I was doing research on how the PS1 graphics/rendering system worked, reason for the wobbly vertices etc, I came across some documentation regarding the way they did their divide. Here is the link to it: http://problemkaputt.de/psx-spx.htm#gteoverview (see the \" GTE Division Inaccuracy\" section)\n\nThe relevant code:\n\n```\n  if (H < SZ3*2) then                            ;check if overflow\n    z = count_leading_zeroes(SZ3)                ;z=0..0Fh (for 16bit SZ3)\n    n = (H SHL z)                                ;n=0..7FFF8000h\n    d = (SZ3 SHL z)                              ;d=8000h..FFFFh\n    u = unr_table[(d-7FC0h) SHR 7] + 101h        ;u=200h..101h\n    d = ((2000080h - (d * u)) SHR 8)             ;d=10000h..0FF01h\n    d = ((0000080h + (d * u)) SHR 8)             ;d=20000h..10000h\n    n = min(1FFFFh, (((n*d) + 8000h) SHR 16))    ;n=0..1FFFFh\n  else n = 1FFFFh, FLAG.Bit17=1, FLAG.Bit31=1    ;n=1FFFFh plus overflow flag\n```\n\n\nI'm having a hard time understanding how this works, what is this 'unr' table? why are we shifting things?\nIf someone could give a more detailed explanation as to how this thing is actually achieving the divide, it would be appreciated.\n    ", "Answer": "\r\nThis algorithm is a fixed-point division of two unsigned 16-bit fractional values in [0,1). It first computes an initial 9-bit approximation to the reciprocal of the divisor via a table lookup, refines this with a single Newton-Raphson iteration for the reciprocal, xi+1 := xi * (2 - d * xi), resulting in a reciprocal accurate to about 16 bits, finally multiplies this by the dividend, yielding a 17-bit quotient in [0,2).\n\nFor the table lookup, the divisor is first normalized into [0.5, 1) by applying a scale factor 2z, obviously the dividend then needs to be adjusted by the same scale factor. Since the reciprocals of operands in [0.5, 1), are going to be [1,2], the integer bit of the reciprocal is known to be 1, so one can use 8-bit table entries to produce a 1.8 fixed-point reciprocal by adding ```\n0x100```\n (= 1). The reason ```\n0x101```\n is used here is not clear; it may be due to a requirement that this step always provides an overestimate of the true reciprocal.\n\nThe next two steps are verbatim translations of the Newton-Raphson iteration for the reciprocal taking into account fixed-point scale factors. So ```\n0x2000000```\n represents 2.0. The code uses ```\n0x2000080```\n since it incorporates a rounding constant ```\n0x80```\n (=128) for the following division by 256, used for rescaling the result. The next step likewise adds ```\n0x00000080```\n as a rounding constant for a rescaling division by 256. Without the scaling, this would be a pure multiplication.\n\nThe final multiplication ```\nn*d```\n multiplies the reciprocal in ```\nd```\n with the dividend in ```\nn```\n to yield the quotient in 33 bits. Again, a rounding constant of 0x8000 is applied before dividing by 65536 to rescale into the proper range, giving a quotient in 1.16 fixed-point format.\n\nContinuous rescaling is typical of fixed-point computation where one tries to keep intermediate results as large as possible to maximize the accuracy of the final result. What is a bit unusual is that rounding is applied in all of the intermediate arithmetic, rather than just in the final step. Maybe it was necessary to achieve a specified level of accuracy. \n\nThe function is not all that accurate, though, likely caused by inaccuracy in the initial approximation. Across all non-exceptional cases, 2,424,807,756 match a correctly rounded 1.16 fixed-point result, 780,692,403 have an error of 1 ulp, 15,606,093 have a 2-ulp error, and 86,452 have a 3-ulp error. In a quick experiment, the maximum relative error in the initial approximation ```\nu```\n was 3.89e-3. An improved table lookup reduces the maximum relative error in ```\nu```\n to 2.85e-3, reducing but not eliminating 3-ulp errors in the final result.\n\nIf you want to look at a specific example, consider ```\nh```\n=0.3 (```\n0x4ccd```\n) divided by ```\nSZ3```\n=0.2 (```\n0x3333```\n). Then ```\nz```\n=2, thus ```\nd```\n=0.2*4 = 0.8 (```\n0xcccc```\n). This leads to ```\nu```\n = 1.25 (```\n0x140```\n). Since the estimate is quite accurate, we expect (2 - d * u) to be near 1, and in fact, ```\nd```\n = 1.000015 (```\n0x10001```\n). The refined reciprocal comes out to ```\nd```\n=1.250015 (```\n0x14001```\n), and quotient is therefore ```\nn```\n=1.500031 (```\n0x18002```\n). \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Generality of Approximation for NP Hard Problems\r\n                \r\nI've been struggling with this question for more than a week. What is the most general approximation result that can be achieved for an NP-hard problem?\n\n\nApproximation Algorithm\nApproximation Scheme\nFully Polynomial-time Approximation Scheme\nPolynomial-time Approximation Scheme\n\n\nI can't tell if I'm the only one who doesn't understand that 'most general' means.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Why does an iterative approximation algorithm sometime have fewer iterations despite larger input values?\r\n                \r\n\nWhy does my iterative approximation loop get executed fewer times when executed for 24690 than 12345 which is half the size?\n\nI am using a bisection algorithm or bisecting search.  Please help me.\n    ", "Answer": "\r\nIt really depends on how your bisection loop is terminated.  The divide-and-average method (also called Newton's method) can be iterated a fixed number of times (obviously not in your case) or it can go until the successive differences are within some tolerance.  In the latter case, the number of iterations doesn't depend on size, it depends on the remainders of the divisions (i.e. how close the initial guesses are).\nHope this helps :-)\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Trying to establish a simple closest approximation algorithm\r\n                \r\nI'm trying to create an algorithm in python that takes an input array (like the one mentioned below) and is able to store indices of numbers closest to every 300ms interval.\nExample input:\n\n[0, 4.2, 12.3, 24.7, 33, 39.2, 47.7, 56.6, 68.3, 80.6, 90.8, 103,\n109.8, 119.5, 129.8, 147.1, 155.3, 167, 173.8, 183.3, 190.4, 198.2, 207, 216.4, 224.3, 234.3, 244.7, 255.8, 262, 274, 286.2, 291.5,\n300.8, 306.6, 312.1, 324.3, 330.9, 336.5, 344.1, 350.6, 359, 369.3, 378, 387.5, 393.4, 402.8, 409.5, 416, 423.1, 433, 443, 451.1, 456.8, 469.4, 475.4, 486.8, 497.1, 508.6, 520.3, 532.5, 539.8, 548.4, 559.5, 571.6, 584.1, 593.3, 601.3, 610.4, 620.9, 628.6, 639.6, 646.1, 651.5, 661.7, 676.1, 685.4, 696.7, 705.6, 717, 726.1, 731.8, 739, 745.3, 752.1, 759.9, 768.1, 779.9, 789, 806.5, 818.2, 830.4,\n842.6, 848.9, 861.3, 867.3, 875.3, 881.8, 889.1, 897.8, 903.7, 912.5, 919.9, 928.7, 938.3, 947.9, 958.1, 968.5, 973.9, 983.8, 993.9, 1004.2, 1015.1, 1021.3, 1028.4, 1037.8, 1047.8, 1053.4, 1061.5, 1069.9, 1081.9, 1094.1, 1100.5, 1109.5, 1121.7, 1127.6, 1133.4, 1142.1, 1147.6, 1155.4, 1163.4, 1174, 1183.9, 1190.6, 1198, 1206.7, 1213.4, 1223, 1230.3, 1241.3, 1252.5, 1264.3, 1276.3, 1284.3, 1296.5, 1308.9, 1316.7, 1323.5, 1332.9, 1339.1, 1347.7, 1358.5, 1369.1, 1377.1, 1385.5, 1397.1, 1407.9, 1420, 1431.5, 1438.5, 1451, 1456.8, 1464.6, 1474.4, 1482.8, 1487.7, 1495.7, 1506, 1517.8, 1526, 1536.1, 1542.9, 1548.9, 1555.4, 1566.2, 1577, 1589.1, 1600.1,\n1609.1, 1617.2, 1628.5, 1640.3, 1649.9, 1660.3, 1670.5, 1681.1, 1692.1, 1703.3, 1709.3, 1721.1, 1730.9, 1740.8, 1753.8, 1761, 1769.6, 1777.3, 1783.9, 1794.3, 1806.5]\n\nMy approach was to use an absolute value function to find the closest value and then up the interval each time. However, this will interfere with values like 0 (since abs(0-300) = 300 even though this answer is not correct. I have no idea what to do.\nHere's what I kinda started with:\n```\ndef check_num(prev, cur, iteration):\n    MULTIPLE = 300\n    num = MULTIPLE * iteration\n    \n    # returns true if current number is closer\n    if abs(prev-num) > abs(cur-num):\n        return True\n    # returns false if current number is further\n    elif abs(prev-num) < abs(cur-num):\n        return False\n    # if they are equal returns true still [edge case]\n    else: return True\n\ndef arr_index_finder(arr):\n    # define arr of indecies that work\n    indexArr = []\n    # initialize basic values\n    prev = arr[0]\n    curr = prev\n    iteration = 1\n    index = 0\n    for el in arr:\n        prev = curr\n        curr = el\n        index += 1\n        if check_num(prev, curr, 1):\n            # code for the alg\n```\n\n    ", "Answer": "\r\nYou're on the right track, one approach is the calculate the ```\nabs```\n, and keep track of the corresponding index in a tuple, then run ```\nmin```\n over the tuples.\n```\nl = [0, 4.2, 12.3, 24.7, 33, 39.2, 47.7, 56.6, ...] # ...'s for brevity\nfor m in [300,600,900,1200,1500,1800]:\n    print(min((abs(m-i),idx) for idx, i in enumerate(l)))\n```\n\nThis will give you the minimum value and the index, if you just want the index then replace the last line with;\n```\n     print(min((abs(m-i),idx) for idx, i in enumerate(l))[1])\n```\n\nThis will be slow for very large lists, but there's scope to improve the time complexity if needed by restricting your search.\nAnd finally, if you just want a list of the indices then the following list comprehension will calculate it;\n```\n[min((abs(m-i),idx) for idx, i in enumerate(l))[1] for m in [300,600,900,1200,1500,1800]]\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation Algorithm for non-intersecting paths in a grid\r\n                \r\nI recently came across this question and thought I could share it here, since I wasn't able to get it.\n\nWe are given a 5*5 grid numbered from 1-25, and a set of 5 pairs of points,that are start and end points of a path on the grid.\n\nNow we need to find 5 corresponding paths for the 5 pairs of points, such that no two paths should overlap. Also note that only vertical and horizontal moves are allowed. Also the combined 5 path should cover the entire grid.\n\nFor example we are given the pair of points as: \n\n```\nP={1,22},{4,17},{5,18},{9,13},{20,23}\n```\n\n\nThen the corresponding paths will be\n\n\n```\n1-6-11-16-21-22```\n\n```\n4-3-2-7-12-17```\n\n```\n5-10-15-14-19-18```\n\n```\n9-8-13```\n\n```\n20-25-24-23```\n\n\n\nWhat I have thought of so far:\nMaybe i can compute all paths from source to destination for all pairs of points and then check if there's no common point in the paths. However this seems to be of higher time complexity.\n\nCan anyone propose a better algorithm? I would be glad if one could explain through a pseudo code.Thanks \n    ", "Answer": "\r\nThis problem is essentially the Hamiltonian path/cycle problem problem (since you can connect the end of one path to the start of another, and consider all the five paths as a part of one big cycle). There are no known efficient algorithms for this, as the problem is NP-complete, so you do essentially need to try all possible paths with backtracking (there are fancier algorithms, but they're not much faster).\n\nYour title asks for an approximation algorithm, but this is not an optimization problem - it's not the case that some solutions are better than others; all correct solutions are equally good, and if it isn't correct, then it's completely wrong - so there is no possibility for approximation.\n\n\n\nEdit: The below is a solution to the original problem posted by the OP, which did not include the \"all cells must be covered\" constraint. I'm leaving it up for those that might face the original problem.\n\nThis can be solved with a maximum flow algorithm, such as Edmonds-Karp.\n\nThe trick is to model the grid as a graph where there are two nodes per grid cell; one \"outgoing\" node and one \"incoming\" node. For each adjacent pair of cells, there are edges from the \"outgoing\" node in either cell to the \"incoming\" node in the other cell. Within each cell, there is also an edge from the \"incoming\" to the \"outgoing\" node. Each edge has the capacity 1. Create one global source node that has an edge to all the start nodes, and one global sink node to which all end nodes have an edge.\n\nThen, run the flow algorithm; the resulting flow shows the non-intersecting paths.\n\nThis works because all flow coming in to a cell must pass through the \"internal\" edge from the \"incoming\" to the \"ougoing\" node, and as such, the flow through each cell is limited to 1 - therefore, no paths will intersect. Also, Edmonds-Karp (and all Floyd-Warshall based flow algorithms) will produce integer flows as long as all capacities are integers.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "MATLAB: Newton Algorithm - two-sided approximation of Jacobian matrix\r\n                \r\nI have a question concerning the Newton Algorithm in MATLAB.\n\nThe algorithm should be capable of performing a two-sided approximation \ntechnique for the Jacobian Matrix in case that there is no analytical Jacobian provided.\n\n```\nfunction [x,fx,ef] = newton(f,x,cc)\n\n% convergence criteria\ntole = cc(1,1); told = cc(2,1); maxiter = cc(3,1);\n\n% newton algorithm\nef = 0;\nfor j = 1:maxiter\n   [fx,dfx] = f(x);\n   xp = x − dfx\\fx;\n   D = (norm(x−xp)<=tole*(1+norm(xp)) && norm(fx)<=told);\n   if D == 1;\n      ef = 1; break;\n   else\n      x = xp;\n   end\nend\n```\n\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximate string matching algorithms state-of-the-art [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 7 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI seek a state of the art algorithms to approximate string matching.\nDo you offer me references(article, thesis,...)?\nthank you\n    ", "Answer": "\r\nYou might have got your answer already but I want to convey my points on approximate string matching so that, others might benefit. I am speaking from my experience having worked to solve cloud service problems to handle really large scale requirements.\n\nIf we just want to talk about the Approximate String matching algorithms, then there are many. \nFew of them are:\nJaro-Winkler, Edit distance(Levenshtein), Jaccard similarity, Soundex/Phonetics based algorithms etc.\nA simple googling would give us all the details.\n\nIrony is, they work while you try to match two given input strings. Alright theoretically and to demonstrate the way fuzzy or approximate string matching works.\n\nHowever, grossly understated point is, how do we use the same in production settings. Not everybody that I know of who were scouting for an approximate string matching algorithm knew how they could solve the same in the production environment.\n\nAssuming that we have a list of millions of names and if we want to search a given input name against all the entries in the list using one of the standard algorithms above would mean disaster.\n\nA typical, edit distance algorithm has a time complexity of O(N^2) where N is the number of characters in a string. To scan the list of size M, the complexity would be O(M * N^2). This would mean very high hardware requirements and it just doesn't work in your favor regardless of how much h/w you want to stack up. \n\nThis is where we have to start thinking about other approaches.\nOne of the common approaches to solve such a problem in the production environment is to use a standard search engine like - \nApache Lucene.\n\nhttps://lucene.apache.org/\n\nLucene indexing engine indexes the reference data(Called as documents) and input query can be fired against the engine. The results are returned which are ranked based on how close they are to the input.\nThis is close to how google search engine works. Googles crawles and index the whole web but you should have a miniature system mimicking what Google does.\n\nThis works for most of the cases including complicated name matching where the first, middle and the last names are interchanged.\n\nYou can select your results based on the scores emitted by Lucene.\n\nWhile you mature in your role, you will start thinking about using hosted solutions like Amazon Cloudsearch which wraps the Solr and ElastiSearch for you. Of-course it uses Lucene underneath and keeps you independent of potential size of the index due to larger reference data which is used for indexing.\n\nhttp://aws.amazon.com/cloudsearch/\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximate String Matching Algorithms for names\r\n                \r\nI'm looking for fuzzy string algorithms for the following example: given a database of existing names, match inputs to either the best-matched name if the match accuracy is higher than the input threshold (say 90%), or NA otherwise\n\n```\ndatabase = [James Bond, Michael Smith]\n```\n\n\ninput\n\n```\nJames L Bond->James Bond\nJBondL->James Bond\nBond,James->James Bond\nBandJamesk->James Bond\nJenny,Bond->N/A\n```\n\n\nCurrently, most algorithms like Levenstein and phonetic based ones like Soundex can't match inverted names like BondJames. So far cosine and Jacquard yield the best results, but I'm looking for more, so that I can choose the best or possibly combine algorithms.\n    ", "Answer": "\r\nGiven your examples, I would consider:\n\n\nSeparating n1 - the name in the input and n2 - a name in the database into words (by delimiters and capital letters): n1 -> {u1,u2,...}, n2 -> {v1,v2,...}\nFinding the permutation of the order of words in n2 that minimizes s = sum(L(u, v)) where L is the Levenshtein distance.\nSelecting the database entry that minimizes s.\n\n\nWhen the number of words in L1 and the number of words in L2 don't match - you should 'penalize' s.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation algorithm for TSP variant, fixed start and end anywhere but starting point + multiple visits at each vertex ALLOWED\r\n                \r\nNOTE: Due to the fact that the trip does not end at the same place it started and also the fact that every point can be visited more than once as long as I still visit all of them, this is not really a TSP variant, but I put it due to lack of a better definition of the problem.\n\nSo..\n\nSuppose I am going on a hiking trip with n points of interest. These points are all connected by hiking trails. I have a map showing all trails with their distances, giving me a directed graph.\n\nMy problem is how to approximate a tour that starts at a point A and visits all n points of interest, while ending the tour anywhere but the point where I started and I want the tour to be as short as possible.\n\nDue to the nature of hiking, I figured this would sadly not be a symmetric problem (or can I convert my asymmetric graph to a symmetric one?), since going from high to low altitude is obviously easier than the other way around.\n\nAlso I believe it has to be an algorithm that works for non-metric graphs, where the triangle inequality is not satisfied, since going from a to b to c might be faster than taking a really long and weird road that goes from a to c directly. I did consider if triangle inequality still holds, since there are no restrictions regarding how many times I visit each point, as long as I visit all of them, meaning I would always choose the shortest of two distinct paths from a to c and thus never takr the long and weird road.\n\nI believe my problem is easier than TSP, so those algorithms do not fit this problem. I thought about using a minimum spanning tree, but I have a hard time convincing myself that they can be applied to a non-metric asymmetric directed graph.\n\nWhat I really want are some pointers as to how I can come up with an approximation algorithm that will find a near optimal tour through all n points\n    ", "Answer": "\r\nTo reduce your problem to asymmetric TSP, introduce a new node u and make arcs of length L from u to A and from all nodes but A to u, where L is very large (large enough that no optimal solution revisits u). Delete u from the tour to obtain a path from A to some other node via all others. Unfortunately this reduction preserves the objective only additively, which make the approximation guarantees worse by a constant factor.\n\nThe target of the reduction Evgeny pointed out is non-metric symmetric TSP, so that reduction is not useful to you, because the approximations known all require metric instances. Assuming that the collection of trails forms a planar graph (or is close to it), there is a constant-factor approximation due to Gharan and Saberi, which may unfortunately be rather difficult to implement, and may not give reasonable results in practice. Frieze, Galbiati, and Maffioli give a simple log-factor approximation for general graphs.\n\nIf there are a reasonable number of trails, branch and bound might be able to give you an optimal solution. Both G&S and branch and bound require solving the Held-Karp linear program for ATSP, which may be useful in itself for evaluating other approaches. For many symmetric TSP instances that arise in practice, it gives a lower bound on the cost of an optimal solution within 10% of the true value.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "What approximate TSP algorithm does Google OR-Tools use?\r\n                \r\nI came across Google OR-Tools which computes the TSP with reasonable approximations as discussed in this link. I am curious to know what specfic algorithm this tool uses for TSP. Does it have any specific optimizations (to the code) that make it perform well? (There are several approximate algorithms for the TSP, I am just curious to know if it uses a mix of multiple algorithms or which specific algorithm it uses). \n    ", "Answer": "\r\nSee comment here:\n\nhttps://github.com/google/or-tools/issues/920#issuecomment-435880431\n\nit links to:\n\nhttps://www.researchgate.net/publication/226021015_A_Constraint_Programming_Toolkit_for_Local_Search\n\nwhich is a good starting point to understand the technology used.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Finding path between all points using approximation algorithm\r\n                \r\nThere is 1<=n<=1000 cities. I have to find path that connects all the cities (every city can be visited only once) which starts and ends in city number 1. In this path the maximum length between 2 cities must be as short as possible.\n\nEg: \n\n\n\nInput:\n\n```\ncoordinates of cities\n```\n\n\nOutput:\n\n```\n5 1 3 //longest connection is 5 and it is between cities 1 and 3\n1 3 6 4 5 2 1 //path\n```\n\n    ", "Answer": "\r\nHere is an approximation algorithm that should give better results on average than a naive greedy algorithm:\n\n\nConsider the graph to be complete - there is an edge between every pair of vertices for a total of ```\nn(n-1)/2```\n edges.\nSort the edges in descending order of their weights/distances.\nIterate from the highest distance edge to the lowest distance edge, and remove it if after removing that edge, both its end-points still have degree atleast ceil(n/2) (Dirac's theorem for ensuring a Hamiltonian cycle exists). You could use a stronger result like Ore's theorem to be able to trim even more edges, but the computation complexity will increase.\nIn the remaining graph, use a greedy algorithm to find a Hamiltonian cycle. The greedy algorithm basically starts from 1, and keeps selecting the edge with the least distance to a node that does not already form part of the cycle so far. So in your example, it will first pick 1 -> 2, then 2->4, then 4->5 and so on. The last selected vertex will then have a path back to 1.\n\n\nYou could directly use the greedy algorithm given in step 4 on the input graph, but the pre-processing steps 1-3 should in general greatly improve your results on most graphs.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Why the outcome of approximation algorithm more precise then the minimum step?\r\n                \r\nIn this approximate research algorithm, I set epsilon to 0.01 and step to 0.0001. \n\nThe run outcome is:\n\n```\nans = 0.9949999999999067. \n```\n\n\nSince ```\nans```\n adds 0.0001 each step, the outcome should be precise to fourth digit after the point. \n\nHow come it has so many digits?\n\nCode is as follows:\n\n```\nx = 1\nepsilon = 0.01\nstep = epsilon**2\nnumGuess = 0\nans = 0.0\nwhile abs(ans**2 - x) >= epsilon and ans <= x:\n    ans += step\n    numGuess += 1\nprint('numGuess =', numGuess)\nif abs(ans**2 - x) >= epsilon:\n    print('Failed on square root of',x)\nelse:\n    print(ans, 'is close to square root of',x)\n```\n\n    ", "Answer": "\r\nYour software does not use decimal for floating-point arithmetic. It uses binary-based floating-point. The string “.01” in your source code is converted to binary-based floating-point, resulting in a value that is close but different.\n\nTherefore the results of the computations you ask about are near simple decimal values but are different. Printing the values with many decimal digits reveals those differences.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fuzzy search algorithm (approximate string matching algorithm)\r\n                \r\nI wish to create a fuzzy search algorithm. \nHowever, upon hours of research I am really struggling.\n\nI want to create an algorithm that performs a fuzzy search on a list of names of schools.\n\nThis is what I have looked at so far:\n\nMost of my research keep pointing to \"string metrics\" on Google and Stackoverflow such as:\n\n\nLevenshtein distance\nDamerau-Levenshtein distance\nNeedleman–Wunsch algorithm\n\n\nHowever this just gives a score of how similar 2 strings are. The only way I can think of implementing it as a search algorithm is to perform a linear search and executing the string metric algorithm for each string and returning the strings with scores above a certain threshold. (Originally I had my strings stored in a trie tree, but this obviously won't help me here!)\n\nAlthough this is not such a bad idea for small lists, it would be problematic for lists with lets say a 100,000 names, and the user performed many queries.\n\nAnother algorithm I looked at is the Spell-checker method, where you just do a search for all potential misspellings. However this also is highly inefficient as it requires more than 75,000 words for a word of length 7 and error count of just 2.  \n\nWhat I need?\n\nCan someone please suggest me a good efficient fuzzy search algorithm. with:\n\n\nName of the algorithm\nHow it works or a link to how it works\nPro's and cons and when it's best used (optional)\n\n\nI understand that all algorithms will have their pros and cons and there is no best algorithm.\n    ", "Answer": "\r\nConsidering that you're trying to do a fuzzy search on a list of school names, I don't think you want to go for traditional string similarity like Levenshtein distance. My assumption is that you're taking a user's input (either keyboard input or spoken over the phone), and you want to quickly find the matching school.\n\nDistance metrics tell you how similar two strings are based on substitutions, deletions, and insertions. But those algorithms don't really tell you anything about how similar the strings are as words in a human language.\n\nConsider, for example, the words \"smith,\" \"smythe,\" and \"smote\". I can go from \"smythe\" to \"smith\" in two steps:\n\n```\nsmythe -> smithe -> smith\n```\n\n\nAnd from \"smote\" to \"smith\" in two steps:\n\n```\nsmote -> smite -> smith\n```\n\n\nSo the two have the same distance as strings, but as words, they're significantly different. If somebody told you (spoken language) that he was looking for \"Symthe College,\" you'd almost certainly say, \"Oh, I think you mean Smith.\" But if somebody said \"Smote College,\" you wouldn't have any idea what he was talking about.\n\nWhat you need is a phonetic algorithm like Soundex or Metaphone. Basically, those algorithms break a word down into phonemes and create a representation of how the word is pronounced in spoken language. You can then compare the result against a known list of words to find a match.\n\nSuch a system would be much faster than using a distance metric. Consider that with a distance metric, you need to compare the user's input with every word in your list to obtain the distance. That is computationally expensive and the results, as I demonstrated with \"smith\" and \"smote\" can be laughably bad.\n\nUsing a phonetic algorithm, you create the phoneme representation of each of your known words and place it in a dictionary (a hash map or possibly a trie). That's a one-time startup cost. Then, whenever the user inputs a search term, you create the phoneme representation of his input and look it up in your dictionary. That is a lot faster and produces much better results.\n\nConsider also that when people misspell proper names, they almost always get the first letter right, and more often than not pronouncing the misspelling sounds like the actual word they were trying to spell. If that's the case, then the phonetic algorithms are definitely the way to go.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm/approximation for combined independent set/hamming distance\r\n                \r\nInput: Graph G\nOutput: several independent sets, so that the membership of a node to all independent sets is unique. A node therefore has no connections to any node in its own set. Here is an example path.\nSince clarification was called for here another rephrasal:\nDivide a given graph into sets so that\n\ni can tell a node from all others by its membership in sets e.g. if node i is present only in set A no other node should be present in set A only\nif node j is present in set A and B then no other node should be present in set A and B only. if the membership of nodes is coded by a bit pattern, then these bit patterns have hamming distance at least one\n\nif two nodes are adjacent in the graph, they should not be present in the same set, hence be an independent set\n\n\nExample:\nB has no adjacent nodes\nD=>A, A=>D\nSolution:\n\nA B /\n/ B D\n\nA has bit pattern 10 and no adjacent node in its set. B has bit pattern 11 and no adjacent node, D has 01\ntherefore all nodes have hamming distance at least 1 an no adjacent nodes => correct\nWrong, because D and A are connected:\n\nA D B\n/ D B\n\nA has bit pattern 10 and D in its set, they are adjacent. B has bit pattern 11 and no adjacent node, D has 11 as has B, so there are two errors in this solution and therefore it is not accepted.\nOf course this should be extended to more Sets as the number of Nodes in the Graph increases, since you need at least ```\nlog(n)```\n sets.\nI already wrote a transformation into MAX-SAT, to use a sat-solver for this. but the number of clauses is just to big. A more direct approach would be nice. So far I have an approximation, but I would like an exact solution or at least a better approximation.\nI have tried an approach where I used a particle swarm to optimize from an arbitrary solution towards a better one. However the running time is pretty awful and the results are far from great. I am looking for a  dynamic algorithm or something, however i cannot fathom how to divide and conquer this problem.\n    ", "Answer": "\r\nNot a complete answer, and I don't know how useful it will be to you. But here goes:\n\nThe hamming distance strikes me as a red herring. Your problem statement says it must be at least 1 but it could be 1000. It suffices to say the bit encoding for each node's set memberships is unique.\n\nYour problem statement doesn't spell it out, but your solution above suggests every node must be a member of at least 1 set. ie. a bit encoding of all 0's is not allowed for any node's set memberships.\n\nIgnoring connected nodes for a moment, disjoint nodes are easy: Simply number them sequentially with an unused bit encoding. Save those for last.\n\nYour example above uses directed edges, but again, that strikes me as a red herring. If A cannot be in the same set as D because A=>D, D cannot be in the same set as A regardless whether D=>A.\n\nYou mention needing at least log(N) sets. You will also have at most N sets. A fully connected graph (with (N^2-N)/2 undirected edges) will require N sets each containing a single node.\n\nIn fact, if your graph contains a fully connected simplex of M dimensions (M in 1..N-1) with M+1 vertices and (M^2+M)/2 undirected edges, you will require at least M+1 sets.\n\nIn your example above, you have one such simplex (M=1) with 2 vertices {A,D} and 1 (undirected) edge {(A,D)}.\n\nIt would seem that your problem boils down to finding the largest fully connected simplexes in your graph. Stated differently, you have a routing problem: How many dimensions do you need to route your edges so none cross? It doesn't sound like a very scalable problem.\n\nThe first large simplex found is easy. Every vertex node gets a new set with its own bit.\n\nThe disjoint nodes are easy. Once the connected nodes are dealt with, simply number the disjoint nodes sequentially skipping any previously used bit patterns. From your example above, since A and D take 01 and 10, the next available bit pattern for B is 11.\n\nThe tricky part then becomes how to fold any remaining simplexes as much as possible into the existing range before creating any new sets with new bits. When folding, one must use 2 or more bits (sets) for each node, and the bits (sets) must not intersect with the bits (sets) for any adjacent node.\n\nConsider what happens to your example above when one adds another node, C, to the example:\n\nIf C connects directly to both A and D, then the initial problem becomes finding the 2-simplex with 3 vertices {A,C,D} and 3 edges {(A,c),(A,D),(C,D)}. Once A, C and D take the bit patterns 001, 010 and 100, the lowest available bit pattern for disjoint B is 011.\n\nIf, on the other hand, C connects directly A or D but not both, the graph has two 1-simplexes. Supposing we find the 1-simplex with vertices {A,D} first giving them the bit patterns 01 and 10, the problem then becomes how to fold C into that range. The only bit pattern with at least 2 bits is 11, but that intersects with whichever node C connects to so we have to create a new set and put C in it. At this point, the solution is similar to the one above.\n\nIf C is disjoint, either B or C will get the bit pattern 11 and the remaining one will need a new set and get the bit pattern 100.\n\nSuppose C connects to B but not to A or D. Again, the graph has two 1-simplexes but this time disjoint. Suppose {A,D} is found first as above giving A and D the bit patterns 10 and 01. We can fold B or C into the existing range. The only available bit pattern in the range is 11 and either B or C could get that pattern as neither is adjacent to A or D. Once 11 is used, no bit patterns with 2 or more bits set remain, and we will have to create a new set for the remaining node giving it the bit pattern 100.\n\nSuppose C connects to all 3 A, B and D. In this case, the graph has a 2-simplex with 3 vertexes {A,C,D} and a 1-simplex with 2 vertexes {B, C}. Proceeding as above, after processing the largest simplex, A, C and D will have bit patterns 001, 010, 100. For folding B into this range, the available bit patterns with 2 or more bits set are: 011, 101, 110 and 111. All of these except 101 intersect with C so B would get the bit pattern 101.\n\nThe question then becomes: How efficiently can you find the largest fully-connected simplexes?\n\nIf finding the largest fully connected simplex is too expensive, one could put an approximate upper bound on potential fully connected simplexes by finding maximal minimums in terms of connections:\n\n\nSweep through the edges updating the\nvertices with a count of the\nconnecting edges.\nFor each connected node, create an array of Cn counts initially zero\nwhere Cn is the count of edges\nconnected to the node n.\nSweep through the edges again, for the connected nodes n1 and n2,\nincrement the count in n1\ncorresponding to Cn2 and vice versa.\nIf Cn2 > Cn1, update the last count\nin the n1 array and vice versa.\nSweep through the connected nodes again, calculating an upper bound on\nthe largest simplex each node could\nbe a part of. You could build a pigeon-hole array with a list of vertices\nfor each upper bound as you sweep through the nodes.\nWork through the pigeon-hole array from largest to smallest extracting and\nfolding nodes into unique sets.\n\n\nIf your nodes are in a set N and your edges in a set E, the complexity will be:\nO(|N|+|E|+O(Step 5))\n\nIf the above approximation suffices, the question becomes: How efficiently can you fold nodes into existing ranges given the requirements?\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "translate approximate algorithm for subset sum into php code\r\n                \r\nCross-posted in MathExchange to elicit more responses. \n\n================================================================================\n\nI was writing my original question in StackOverflow, when I realized it was asked before.\n\nTurns out I have what is known as a subset sum problem, so I went to wikipedia and found this part.\n\n```\nThe algorithm for the approximate subset sum problem is as follows:\n initialize a list S to contain one element 0.\n for each i from 1 to N do\n   let T be a list consisting of xi + y, for all y in S\n   let U be the union of T and S\n   sort U\n   make S empty \n   let y be the smallest element of U \n   add y to S \n   for each element z of U in increasing order do\n      //trim the list by eliminating numbers close to one another\n      //and throw out elements greater than s\n     if y + cs/N < z ≤ s, set y = z and add z to S \n if S contains a number between (1 − c)s and s, output yes, otherwise no\n```\n\n\nBut I have problem understanding the pseudo code written in wikipedia.\n\nFor instance, I thought the objective is to find the closest set of numbers that can match S. \n\nBut here S is a list. What is this S list with element 0?\n\nAnd what on earth is  ```\nif y + cs/N < z ≤ s```\n? How do I even write this out in code?\n\nI was hoping someone can help me translate this into php code. \n\nAt least I am more familiar with that. It need not be a full translation. \n\nAs long as the answers make me understand this approximate algorithm enough for me to write it in php code myself, that will do.\n    ", "Answer": "\r\nTo answer the sub-questions you posted on math.stackexchange.com one by one:\n\nWhat is the difference between the big ```\nS```\n and the small ```\ns```\n? The big ```\nS```\n is a list variable which is initially the list ```\n[0]```\n and is modified over the course of the execution of the code. The little ```\ns```\n is a number that remains constant. Specifically, ```\ns```\n is the number from this question:\n\n\n  Given a set of integers and an integer s, does any non-empty subset sum to s?\n\n\nBut what is ```\nS```\n, anyway? Roughly speaking, ```\nS```\n represents the set of all \"useful\" sums that we can make using the elements we've seen so far (if I'm not mistaken).\n\nDoes \"a list with element 0\" mean a list containing a single number, which is zero? Yes, that's what that means.\n\nWhat does ```\ny + cs/N < z ≤ s```\n mean? It means that ```\ny + c*s/N < z```\n and ```\nz ≤ s```\n. So the ```\nif```\n will fail whenever ```\ny + c*s/N ≥ z```\n or ```\nz > s```\n (or both).\n\nAnd some questions you didn't ask, but which seem likely to come up:\n\nWhat is ```\nN```\n? ```\nN```\n is the number of elements in the set we are given.\n\nWhat is ```\nxi```\n? ```\nxi```\n is the i-th element of the set we are given.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fast approximate algorithm for RGB/LAB conversion?\r\n                \r\nI am working on a data visualization tool using OpenGL, and the LAB color space is the most comprehensible color space for visualization of the data I'm dealing with (3 axes of data are mapped to the 3 axes of the color space). Is there a fast (e.g. no non-integer exponentiation, suitable for execution in a shader) algorithm for approximate conversion of LAB values to and from RGB values?\n    ", "Answer": "\r\nIf doing the actual conversion calculation in a shader is too complex/expensive, you can always use a lookup table. Since both color spaces have 3 components, you can use a 3D RGB texture to represent the lookup table.\n\nUsing a 3D texture might sound like a lot of overhead. Since 8 bits/component is often used to represent colors in OpenGL, you would need a 256x256x256 3D texture. At 4 bytes/texel, that's a 64 MByte texture, which is not outrageous, but very substantial.\n\nHowever, depending on how smooth the values in the translation table are, you might be able to get away with a lower resolution. Keep in mind that texture sampling uses linear interpolation. If piecewise linear interpolation is good enough with a certain base-resolution of the lookup table, you can greatly reduce the size.\n\nIf you go this direction, and can't afford to use 64 MBytes for the LUT, you'll have to play with the size of the LUT, and make a possible size/performance vs. quality tradeoff.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "successive approximation in python\r\n                \r\nSuccessive approximation is a general method in which on each iteration of an algorithm, we find a closer estimate of the answer for which we are seeking. One class of successive approximation algorithms uses the idea of a fixed point. If f(x) is a mathematical function, then finding the x such that f(x) = x gives us the fixed point of f.\n\nOne way to find a fixed point is to start with some guess (e.g. guess = 1.0) and, if this is not good enough, use as a next guess the value of f(guess). We can keep repeating this process until we get a guess that is within epsilon of f(guess).\n\nhere's my code:\n\n```\ndef fixedPoint(f, epsilon):\n    \"\"\"\n    f: a function of one argument that returns a float\n    epsilon: a small float\n\n    returns the best guess when that guess is less than epsilon \n    away from f(guess) or after 100 trials, whichever comes first.\n    \"\"\"\n    guess = 1.0\n    for i in range(100):\n        if abs(f(guess) - guess) < epsilon:\n            return guess\n        else:\n            guess = f(guess)\n    return guess\n```\n\n\nfurther now...\n\n```\ndef sqrt(a):\n    def tryit(x):\n        return 0.5 * (a/x + x)\n    return fixedPoint(tryit, 0.0001)\n```\n\n\nI want to compute square root of a number \"a\", is the fixed point of the function f(x) = 0.5 * (a/x + x). AND DONE.\n\n(Above solutions are correct)\n    ", "Answer": "\r\nSuccessive approximation is a general method in which on each iteration of an algorithm, we find a closer estimate of the answer for which we are seeking. One class of successive approximation algorithms uses the idea of a fixed point. If f(x) is a mathematical function, then finding the x such that f(x) = x gives us the fixed point of f.\n\nOne way to find a fixed point is to start with some guess (e.g. guess = 1.0) and, if this is not good enough, use as a next guess the value of f(guess). We can keep repeating this process until we get a guess that is within epsilon of f(guess).\n\nexample in python:\n\n```\ndef fixedPoint(f, epsilon):\n\"\"\"\nf: a function of one argument that returns a float\nepsilon: a small float\n\nreturns the best guess when that guess is less than epsilon \naway from f(guess) or after 100 trials, whichever comes first.\n\"\"\"\n     guess = 1.0   \n     for i in range(100):\n           if -epsilon < f(guess) - guess < epsilon:\n                  return guess\n           else:\n                  guess = f(guess)\n\n      return guess\n```\n\n\nfurther let function f be for finding square root using babylon method:\n\n```\ndef sqrt(a):\n     def babylon(x):\n           def test(x):\n                return 0.5 * ((a / x) + x)\n           return test(x)\n\n     return fixedPoint(babylon, 0.0001)\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "C# dictionary with Key being string and value being a counter. Approximation algorithm and thread safety\r\n                \r\n```\nclass Program\n    {\n        static Dictionary<string, int> Dictionary = new Dictionary<string, int>();\n\n        static void Main(string[] args)\n        {\n            Stopwatch stopwatch = new Stopwatch();\n            stopwatch.Start();\n\n            Thread[] threads = new Thread[500];\n\n            for(int i = 0; i < threads.Length; i++)\n            {\n                threads[i] = new Thread(InsertAlphabet);\n                threads[i].Start();\n            }\n            for (int i = 0; i < threads.Length; i++)\n            {\n                threads[i].Join();\n            }\n\n            Console.WriteLine(Dictionary.Count);\n\n            Console.WriteLine(stopwatch.ElapsedMilliseconds);\n\n            foreach (KeyValuePair<string,int> kvp in Dictionary)\n            {\n                Console.WriteLine(kvp.Key + \" \" + kvp.Value);\n            }\n\n            stopwatch.Stop();\n            Console.ReadLine();\n\n        }\n\n        private static void InsertAlphabet()\n        {\n            string[] alphabetArray = { \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\" };\n            foreach (var alphabet in alphabetArray)\n            {\n                Add(alphabet);\n            }\n        }\n\n        public static void Add(string bar)\n        {\n            lock (Dictionary)\n            {\n                if (!Dictionary.ContainsKey(bar))\n                {\n                    Dictionary.Add(bar, 1);\n                }\n                else\n                {\n                    Dictionary[bar] += 1;\n                }\n            }\n        }\n    }\n```\n\n\nI have created this simple console application to make sure that the data inserted into the dictionary is accurate.\n\nThe time that I took to insert the alphabets as key and the count as the value was approximately 3 seconds for 500 threads trying to insert at the same time.\n\nIs there a way I can improve the performance of this by having some kind of approximation involved (data doesn't need to be 100% accurate. Allowed accuracy 95%).\n\nAlso are there suggestions on how can I improve the increment of count in the dictionary. \n    ", "Answer": "\r\nI believe you can accomplish this safely using the ConcurrentDictionary overload of AddOrUpdate that takes a delegate to generate the new value.\n\nThe delegate receives the current value, if there is one.  You can provide an implementation of the delegate that adds the incremental value to the existing value.  If there was not already a value present, the parameter supplied to AddOrUpdate will be directly assigned as the value for that key.\n\nSince with this solution ConcurrentDictionary internally locks on the key value being updated until your delegate returns and the internal value is updated, multi-threaded performance should be far better than your current locking on the entire dictionary structure.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to justify the approximation ratio of this deterministic algorithm.(Weighted MAX-k-Cut)\r\n                \r\nThe weighted MAX-k-CUT problem asks for the maximum-weighted cut in a weighted undirected graph.\nSuppose now each vertex is one-by-one greedily assigned to the group that can maximize the total weight of new cuts.\nHow to know the approximation rate of the deterministic algorithm?\n    ", "Answer": "\r\nIt's 1/2. This particular algorithm can be obtained by applying the method of conditional expectations to the randomized approximation that choose the assignment of each node independently and uniformly at random, which obtains 1/2 of the total edge weight in expectation and hence at least 1/2 of OPT.\nAn example where the ratio is 1/2 + ε can be obtained by taking K2,n with unit edge weights, adding an infinitestimally costed edge between the two vertices on one side and forcing the algorithm to consider them first.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fast algorithms to approximate distance between two strings\r\n                \r\nI am working on a project that requires to calculate minimum distance between two strings. The maximum length of each string can be 10,000(m) and we have around 50,000(n) strings. I need to find distance between each pair of strings. I also have a weight matrix that contains the the weight for each character pairs. Example, weight between (a,a) = (a,b) = 0.\n\n\n\nJust iterating over all pair of string takes O(n^2) time. I have seen algorithms that takes O(m) time for finding distance. Then, the overall time complexity becomes O(n^2*m). Are there any algorithms which can do better than this using some pre-processing? It's actually the same problem as auto correct. \n\nDo we have some algorithms that stores all the strings in a data structure and then we query the approximate distance between two strings from the data structure? Constructing the data structure can take O(n^2) and query processing should be done in less than O(m).\n\n\n  s1 = abcca, s2 = bdbbe\n\n\nIf we follow the above weighted matrix and calculate Euclidean distance between the two:\n\n\n  sqrt(0^2 + 9^2 + 9^2 + 9^2 + 342^2)\n\n\nContext: I need to cluster time series and I have converted the time series to SAX representation with around 10,000 points. In order to cluster, I need to define a distance matrix. So, i need to calculate distance between two strings in an efficient way.\n\nNote: All strings are of same length and the alphabet size is 5.\n\nhttps://web.stanford.edu/class/cs124/lec/med.pdf\n\nhttp://stevehanov.ca/blog/index.php?id=114\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How can I use the Approximate algorithm in this problem?\r\n                \r\n\nImagine that a needle in the speedometer, which shows the speed, has\nfallen off.\nIt was reattached, but not at the right angle. So, although the\nspeedometer showed the value of the current speed ```\nv```\n, its actual\nvalue was ```\nv+k```\n, where ```\nk```\n is an unknown constant (probably also\nnegative). So we started keeping honest records of the trips we made\nto find out the value of the mysterious constant k.\nInput:\nThe first line of the input contains two integers: ```\nn```\n (1 ≤ n ≤\n1000), which represents the number of parts of a single run, and ```\nt```\n\n(1 ≤ t ≤ 10^6), which represents the total run time.\nThis is followed by ```\nn```\n lines, where each describes one part of the\ntrip that we recorded. Each line contains two integers: ```\ns```\n (1 ≤ s ≤\n1000) and ```\nv```\n (|v| ≤ 1000), the distance and speed indicated by\nthe speedometer with the needle stuck on during that part of the\njourney. Keep in mind that even though the speedometer needle on the\nglove box may have negative readings, its actual speed was always\ngreater than 0 during every part of the trip. The time is given in\nhours, the distance in kilometres and the speed in kilometres per\nhour.\nOutput:\nThe problem is to find K. The mysterious constant k given in\nkilometers per hour.\nExample of Input:\n```\n3 5\n4 -1\n4 0\n10 3\n```\n\nOutput:\n```\n3.000000000\n```\n\nInput:\n```\n4 10\n5 3\n2 2\n3 6\n3 1\n```\n\nOutput:\n```\n-0.508653377\n```\n\n\nWell, I was told that this problem can be solved with Approximate algorithm.\nCan someone write a pseudocode solution or explain how exactly I can solve this problem with this algorithm?\n    ", "Answer": "\r\nAll of this was mentioned in comments, but it's not clear...\nIf you guess a value for k, then you can determine how long the whole trip would have taken if that guess was correct:\nTotal time T = distance1/(speed1+k) + distance2/(speed2+k)...\nIf this total time is more than the actual total time given in the problem, then your guess is too small (you went faster than you guessed).  If the guessed total time is less than the actual total time, then your guess is too big (you went slower than you guessed).\nWith the ability to make and test these guesses, you can play the higher/lower game to narrow down the range of possible k values until you get as close to the real value as you want.\nYou can't necessarily get to the exact value, which is why this could be called an approximate algorithm.  But the numbers we work with also have limited precision, so they probably can't even represent the exact value.  You can easily get one of the closest representable values, which is just as good as an \"exact\" calculation.\nThe algorithm for playing the higher/lower game is called \"binary search\".  It's a little tricky with doubles, so I'll write it out.  Given a function ```\nisTooHigh```\n that tests a guess for ```\nk```\n, you can do it like this:\n```\ndouble findk()\n{\n    double lo = -minOfAll(speeds);\n    double hi = max(1.0, lo);\n    while(!isTooHigh(hi)) {\n        hi *= 2.0;\n    }\n    while(lo < hi) {\n        double test = lo + (hi-lo)*0.5;\n        if (isTooHigh(test)) {\n            if (test >= hi) {\n               // we've reached the limit of precision\n               break;\n            }\n            hi = test;\n        } else {\n            if (test <= lo) {\n               // we've reached the limit of precision\n               break;\n            }\n            lo = test;\n        }\n    }\n    return lo;\n}\n```\n\nNote that, because of the way double-precision numbers are represented, this loop could iterate up to a 2000 times if k is 0 or very close to it. If you don't want that, then you can change the ```\n(lo < hi)```\n test to ```\n(lo < hi - REALLY_SMALL)```\n.  You will have to put up with a less accurate answer, but you can limit the maximum number of iterations.\nThere are also other approximate algorithms you could use that require fewer iterations, like Newton's method\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "What is an Approximation Factor?\r\n                \r\nHow is an approximation Factor different than time-complexity? I have heard, for example, of polynomial algorithms with exponential factors, what does that mean? Does that mean it is not technically  in polynomial time?\n    ", "Answer": "\r\nDon't have enough reputation points, hence posting as answer. \n\nPerhaps your use of factor in two different senses is the source of the confusion. Time is but one factor out of many possible complexity factors, such as storage, bandwidth, etc. Exponential factors in the case of polynomial algorithms refer to the factors of the terms in the mathematical equation. They do not necessarily imply time as a factor, but they do not exclude it either. It depends on what the algorithm is modeling. \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Runtime Approximation of a given algorithm\r\n                \r\nI tried to calculate the time-complexity of this algorithm:\n\n```\nfor i = 1, ..., n\n    for j = 1, ..., i\n    {\n        for k = 1, ..., j\n            print k\n        k = 2\n        while (k < i)\n            k = k^2\n    }\n```\n\n\nI tried to calculate the time but I didn't succeed.\n    ", "Answer": "\r\nPlease show a little more research effort next time.\n\nBut still I will try it looking only at the complexity times.\n\n```\nfor i = 1, ..., n     //n times\n    for j = 1, ..., i  //n times\n    {\n        for k = 1, ..., j  //n times\n            print k;  // constant\n        k = 2;  //constant\n        while (k < i) \n            k = k^2;  //log2n - 1\n   }\n```\n\n\nEvery loop is executed ```\nn```\n times. The variable assignments are always constant, so O(1). In the while loop k starts at 2 and after the first time k is 4. The second time through it becomes 16. Third time: 256... It is clearly logarithmic; So it will execute O(log2(n)) times. \n\n```\nn(n(n(O(1))+O(1) + log2n - 1)) = n(n(n + log2n) = n^2(n + log2n) = n^3 + n^2(log2n)```\n Since the base of the logarithm in time analysis is irrelevant, the 2 goes away. So ```\nn^3 + n^2(log n)```\n. The defining term is ```\nn^3```\n so that is in O(n^3).\n\nI hope that helps. \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fast ArcSin implementation or approximation in c#\r\n                \r\nI need to calculate ASin a lot of times in my project. In C# it takes to much time. \n\nI use Math.Asin() from System namespace\n\nThe question is there any way to implement Asin function faster in C#. \n\nAny approximation algorithm or another implementation which can work faster?\n    ", "Answer": "\r\nAfter some debate in the comments and a benchmark for testing, it is apparent that the solution I have given below isn't really going to improve on the performance of ```\nSystem.Math.Asin()```\n. In fact, both of the calls are nearly negligible and shouldn't be a huge impact for any application. If you are running into performance issues, your cause is likely one of the following:\n\n\nYour arcsin call isn't actually the bottleneck. You need to profile your application to determine the bottleneck for sure. Premature optimization is a common mistake and can lead to a lot of wasted time.\nYou're calling the function too many times. You may have a valid reason for calling a function many times, but the calls are likely able to be optimized as a higher level. Possible solutions would be to use parallel calls or to change your order of operations to reduce calls. If you're calling something like ```\ny = sin(x); z = asin(y)```\n, then you're making extra unnecessary calls. That is a trivial example, but many more complicated computations can have similar effects mathematically.\nYou're calling the function somewhere you shouldn't. For example, if you're trying to do computations in a GUI or rendering thread, you're going to run into performance issues and have a lack of responsiveness. This is a common design mistake and it should be noted that computations should not be done in a GUI thread.\nYour use case isn't feasible. If you're doing something like live data transformation and visualization, then there's an upper limit on how much data you can process in real-time. This is dependent on the hardware and there isn't much that can be done other than offloading the computations to somewhere with more processing power. A case like this is where cloud computing can come in handy.\n\n\nThose points being made, the solution below is still a valid path towards optimization in C#. Do note that optimization should not be done until profiling an application so you actually know that the bottleneck is where you think it is. Note that this isn't the only route to optimization. Paths such as parallel processing or choosing different algorithms are also valid.\n\n\n\nA bit of a different solution, but you can try to load the C standard library with something like the answer to this question. The C standard library on Windows is ```\nmsvcrt.dll```\n and should contain a function ```\nasin```\n:\n\n```\n[DllImport(\"msvcrt.dll\", EntryPoint=\"asin\",\nExactSpelling=false, CharSet=CharSet.Unicode,\nSetLastError=true)]\nstatic extern double asin(double radians);\n\n//calling the function\nstatic void Main()\n{\n    double x = asin(0);\n}\n```\n\n\nIf this still isn't fast enough, you could write a fast asin algorithm in C. There's this question, which is similar to yours. You could also create a faster square root function to be used in this solution as well.\n\nDepending on how accurate you need it, you could also do a Taylor series approximation if you're going to have an angle close to 0. You could also shift the angle to be close to zero, but that would need a bit more trickery.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "In python, speeding up a data stream word counting approximation algorithm\r\n                \r\nThis is for a homework assignment I am working. I have a working version of the code, but it currently takes ~1 hour to run on the files we've been given. I'll share an example of the files, as well as my code (and a high level description), and then could use thoughts on why my code is running as slow as it is. The first file below is the words file, for which I am approximating the number of times each word (represented as a number) has appeared:\n\n```\nthe_words.txt\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n13\n16\n17\n6\n18\n19\n20\n21\n22\n23\n24\n25\n6\n26\n27\n28\n29\n30\n9\n31\n32\n33\n34\n15\n35\n36\n37\n9\n38\n39\n11\n40\n13\n41\n42\n```\n\n\nThe second file includes the parameters for 5 hash functions used in my script:\n\n```\nthe_hashes.txt\n3   1561\n17  277\n38  394\n61  13\n78  246\n```\n\n\nand here's a version of my code. At a high level, I (1) do my imports and set variables, (2) create a hash function, (3) loop over the words in the_words.txt (which is an int, confusing I know), hash each word using the 5 hash functions, and increment in the C matrix by 1 the value in the appropriate index. My code:\n\n```\n# imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\n# variables used throughout the program\ndlt = math.exp(-5)\neps = math.exp(1) * math.pow(10, -4)\nmy_p = 123457\n\nthe_hashes = map(str.split, open('the_hashes.txt', 'r'))\nthe_hashes = [[int(float(j)) for j in i] for i in the_hashes]\nend = len(the_hashes)\n\nrows = math.ceil(math.log(1/dlt))\ncols = math.ceil(math.exp(1)/eps)\nC = np.zeros((rows,cols))\n\n\n# Returns hash(x) for hash function \n# given by parameters a, b, p and n_buckets\ndef hash_fun(a, b, p, n_buckets, x):\n    y = x % p\n    hash_val = (a*y + b) % p\n    output = hash_val % n_buckets\n    return(output)\n\n\n# read the file line by line, implementing the algorithm\ncounter = 0\nwith open(\"the_words.txt\", \"r\") as file:\n    for word in file:\n        counter = counter + 1\n        my_x = int(word)\n\n        # loop over the 5 different pairs of (a,b) values for the hashes\n        for i in range(0,end):\n            my_a = the_hashes[i][0]\n            my_b = the_hashes[i][1]\n\n            my_output = hash_fun(my_a, my_b, my_p, cols, my_x)\n            C[i,my_output] += 1\n\n        if(counter % 10000 == 0):\n            print counter\n```\n\n\nHowever, for a file with 200M words, this takes too long for me currently. Is there anything obvious that is causing my code to run slow? I know it can take a while to stream over 200M words, but I would like to cut this down from the hour it is currently taking. \n\nThanks!\n    ", "Answer": "\r\nIf you can't load the data in memory, there are some parts you can inline and factor out:\n\n```\nmy_range = range(0, end)  # python 2 only, see note below\nwith open(\"the_words.txt\", \"r\") as file:\n    for word in file:\n        counter = counter + 1\n        y = int(word) % p  # factor this out: save 160 million calculations\n        # loop over the 5 different pairs of (a,b) values for the hashes\n        for i in my_range:\n            my_a = the_hashes[i][0]\n            my_b = the_hashes[i][1]\n\n            # save a function call by inlining\n            # my_output = hash_fun(my_a, my_b, my_p, cols, my_x)\n\n            hash_val = (a*y + b) % p\n            my_output = hash_val % n_buckets\n            C[i,my_output] += 1\n\n        if(counter % 10000 == 0):\n            print counter\n```\n\n\nI would also look at the math in ```\nhash_val = ...```\n to see if you can factor out some calculations.\n\nFor ```\nrange(0, end)```\n depending on which version of python you're using, you might want to cache the call. See https://stackoverflow.com/a/40294986/1138710). (I suspect python 2 from your print statement).\n\nAlso, I suggest reading Python performance characteristics for some interesting ways to improve performance, or at least understand better what you're doing.\n\nThe above are just guesses. Check out How can you profile a script? for how to profile your code and know for sure where the bottleneck is.\n\nMy other guess, since you're using numpy, would be to rely on its matrix calculation functions, which I think are going to be better optimized. ```\n(a*y + b) % p```\n looks like nice vector math to me :)\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Convergence guarantee of Policy Gradient with function approximation\r\n                \r\nIs there any convergence proof of the Policy Gradient algorithm with \"general\" value/Q function approximation ?\nSeminal papers (Sutton1999 & Tsitsiklis1999) prove the theorem using a compatibility assumption (i.e. the Q-function approximation is linear w.r.t to policy's features). Also later improvement such as DPG (Silver14) also have similar assumptions.\nYet in practice this compatibility assumption is not satisfied, policy network and Q-function network have their own, independent, set of parameters.\nHence I wonder to which extend those methods are supported by theoretical guarantees.\nThanks,\n(Sutton1999) : Policy gradient methods for reinforcement learning with function approximation, Sutton et al, 1999\n(Silver2014) : Deterministic Policy Gradient Algorithms, Silver et al, 2014\n(Tsitsiklis1999) : Actor-Critic Algorithms, Tsitsiklis et al, 1999\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Can 1 approximation algorithm be used for multiple NP-Hard problems?\r\n                \r\nSince any NP Hard problem be reduced to any other NP Hard problem by mapping, my question is 1 step forward; \nfor example every step of that algo : could that also be mapped to the other NP hard?\n\nThanks in advance\n    ", "Answer": "\r\nFrom http://en.wikipedia.org/wiki/Approximation_algorithm we see that\n\nNP-hard problems vary greatly in their approximability; some, such as the bin packing problem, can be approximated within any factor greater than 1 (such a family of approximation algorithms is often called a polynomial time approximation scheme or PTAS). Others are impossible to approximate within any constant, or even polynomial factor unless P = NP, such as the maximum clique problem.\n(end quote)\n\nIt follows from this that a good approximation in one NP-complete problem is not necessarily a good approximation in another NP-complete problem. In that fortunate world we could use easily-approximated NP-complete problems to find good approximate algorithms for all other NP-complete problems, which is not the case here, as there are hard-to-approximate NP-complete problems.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximate algorithm for minimum raggedness word wrap\r\n                \r\nI'm looking for an efficient algorithm (ideally, C-like pseudocode) to give an approximate solution to the following partition problem. Given a sequence S = {a_i : i=1,...,n} and a bound B, determine a partition of S into some number m of contiguous subsequences as follows. For each k, let s_k be the sum of the elements of k-th subsequence. The partition must satisfy:\n\n\ns_k ≤ B for each k (assume that the values of B and the a_i are such that this is always possible)\nm is minimal (no smaller partition satisfies #1);\nsome measure of dispersion (for example, the variance or the maximum pair-wise difference among the s_k) is minimal among all partitions of size m.\n\n\nI know that this is closely related to the minimum raggedness word wrap algorithm. I am looking for something that can give a \"good enough\" solution for small values of n (less than 15) without pulling out heavy ammunition like dynamic programming, but also something a little faster than brute force.\n    ", "Answer": "\r\nLet S denote the sum of all the items and let n be the number of items. If you fit the items on m lines, you will have at least S/m weight on every line. Because S/m &leq; B, you get m &geq; S/B. I would start from ceiling(S/B) as the value of m and then increase m by one until a solution is found.\n\nWhen m is set and n is given, it's just a matter of recursively searching for the correct boundaries. You guess the boundaries between lines one by one (recursively), and backtrack when solution becomes infeasible. If you find a solution, you store it for reference because it might be the best dispersion-wise. Eventually you choose the best solution. If there are no solutions, then you increase m by one and redo.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Different output with c++ pi approximation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has an answer here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\n\n  Possible Duplicate:\n  Vastly different output C++ monte carlo approximation  \n\n\n\n\nOn my 64-bit ubuntu computer, the following code works as expected, and returns a close approximation for pi with both algorithms. However, on the lab machine, where I must demo the code, a 32-bit rhel 3 machine, the second algorithm always returns 4, and I cannot figure out why. Any insight would be appreciated.\n\n```\n/*\n * RandomNumber.h\n *\n *  \n *      \n */\n\n#ifndef RANDOMNUMBER_H_\n#define RANDOMNUMBER_H_\n\nclass RandomNumber {\npublic:\nRandomNumber() {\n    x = time(NULL);\n    m = pow(2, 31); //some constant value\n    M = 65915 * 7915; //multiply of some simple numbers p and q\n    method = 1;\n}\nRandomNumber(int seed) {\n    x = ((seed > 0) ? seed : time(NULL));\n    m = pow(2, 31); //some constant value\n    method = 1; //method number\n    M = 6543 * 7915; //multiply of some simple numbers p and q\n}\nvoid setSeed(long int seed) {\n    x = seed; //set start value\n}\n\nvoid chooseMethod(int method) {\n    this->method = ((method > 0 && method <= 2) ? method : 1); //choose one of two method\n}\n\nlong int linearCongruential() { //first generator, that uses linear congruential method\n    long int c = 0; // some constant\n    long int a = 69069; //some constant\n    x = (a * x + c) % m; //solution next value\n    return x;\n}\n\nlong int BBS() { //algorithm Blum - Blum - Shub\n    x = (long int) (pow(x, 2)) % M;\n    return x;\n}\ndouble nextPoint() { //return random number in range (-1;1)\n    double point;\n    if (method == 1) //use first method\n        point = linearCongruential() / double(m);\n    else\n        point = BBS() / double(M);\n    return point;\n}\nprivate:\nlong int x; //current value\nlong int m; // some range for first method\nlong int M; //some range for second method\nint method; //method number\n};\n\n#endif /* RANDOMNUMBER_H_ */\n```\n\n\nAnd the test class: \n\n```\n#include <iostream>\n#include <stdlib.h>\n#include <math.h>\n#include <iomanip>\n#include \"RandomNumber.h\"\nusing namespace std;\n\nint main() {\ncout.setf(ios::fixed);\ncout.precision(6);\nRandomNumber random;\nsrand((unsigned) time(NULL));\ncout << \"---------------------------------\" << endl;\ncout << \"   Monte Carlo Pi Approximation\" << endl;\ncout << \"---------------------------------\" << endl;\ncout << \" Enter number of points: \";\nlong int k1;\ncin >> k1;\ncout << \"Select generator number: \";\nint method;\ncin >> method;\nrandom.chooseMethod(method);\ncout << \"---------------------------------\" << endl;\nlong int k2 = 0;\ndouble sumX = 0;\ndouble sumY = 0;\nfor (long int i = 0; i < k1; i++) {\n    double x = pow(-1, int(random.nextPoint() * 10) % 2)\n            * random.nextPoint();\n    double y = pow(-1, int(random.nextPoint() * 10) % 2)\n            * random.nextPoint();\n    sumX += x;\n    sumY += y;\n    if ((pow(x, 2) + pow(y, 2)) <= 1)\n        k2++;\n\n}\ndouble pi = 4 * (double(k2) / k1);\ncout << \"M(X)  = \" << setw(10) << sumX / k1 << endl; //mathematical expectation of x\ncout << \"M(Y)  = \" << setw(10) << sumY / k1 << endl; //mathematical expectation of y\ncout << endl << \"Pi = \" << pi << endl << endl; //approximate Pi\n\nreturn 0;\n}\n```\n\n    ", "Answer": "\r\nThe problem is that ```\npow```\n returns a ```\ndouble```\n, which loses precision at the low end.  Converting to ```\nlong int```\n for the ```\n%```\n operator always returns the same result, and so your RNG outputs constant -60614748.\n\n```\nx = time(0)                 1354284781\npow(x, 2)                  1.83409e+18   0x1.973fdc9dc7787p+60\n(long int) pow(x, 2)       -2147483648    0x80000000\n(long int) pow(x, 2) % M     -60614748\n```\n\n\nThe fix is to change ```\nx = (long int) (pow(x, 2)) % M;```\n to ```\nx = x * x % M```\n, performing all arithmetic within ```\nlong int```\n.  Note that this is still strictly speaking incorrect, as signed overflow is undefined; more correct is to use ```\nunsigned long```\n.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Problem with implementing successive approximation algorithm in C++ with Visual Studio 2008\r\n                \r\nI'm trying to implement an algorithm we did in numerical methods class. I have an identical procedure written in Maple and it works fine. I can't understand why it isn't working in C++. \n\nAny help or tips would be appreciated; thanks in advance.\n\n```\n#include \"stdafx.h\"\n#include \"math.h\"\n#include <iostream>\n\nusing namespace std;\n\ndouble absval(double val)\n{\n   if (val >= 0) return val;\n   else return -val;\n}\ndouble G(double x)\n{\n   double g;\n\n   g = 1/((x*x)+12);\n   return g;\n}\nint main()\n{\ndouble c = 0.011834; /* constant */\ndouble eps = 0.00001; /* precision */\ndouble x0 = 0; /* initial aproximation */\ndouble x1,x2,c1;\nint i,no;\ni = 1;\n\ncout << \"enter max nr of iterations \";\ncin >> no; \n\nx1 = x0;\nx2 = G(x1);\nc1 = (1-c)*eps/c;\n\nwhile (absval(x1-x0) > c1)\n{\n    i = i+1;\n    x1 = x2;\n    x2 = G(x1);\n\n}\ncout << x2;\nif (i<no) cout << \" solution found in allowed nr of iterations \";\nelse cout << \"allowed nr of iterations surpassed \";\n\n}\n```\n\n\nWhen running the code it asks for the allowed number of iterations and after inserting that it closes.\n    ", "Answer": "\r\n```\ndouble x0 = 0; /* initial aproximation */\n.\n.\nx1 = x0;\n.\n.\nwhile (absval(x1-x0) > c1)\n```\n\n\nat this point, ```\nx1 == x0```\n and ```\nc1```\n is positive, so the loop body is never entered; this is probably not what you intended.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Clarkson's 2-approximation Weighted Vertex Cover Algorithm Runtime analysis\r\n                \r\nA well-known 2-approximation for a Minimum Weighted Vertex Cover Problem is the one proposed by Clarkson:\n\nClarkson, Kenneth L. \"A modification of the greedy algorithm for vertex cover.\" Information Processing Letters 16.1 (1983): 23-25.\n\nEasy-to-read pseudo code of the algorithm can be found here see section 32.1.2.\nThe algorithm, according to the paper, has a runtime complexity of ```\nO(|E|*log|V|)```\n where E is the set of edges and V the set of vertices. I'm not entirely sure how they get this result.\n\nLet d(v) be the degree of vertex v in a graph, and w(v) be some weight function.\nExcluding some of the technicalities from the algorithm, the algorithm looks like this:\n\n```\nwhile( |E| != 0){ //While there are still edges in the graph\n    Pick a vertex v \\in V for which w(v)/d(v) is minimized;\n    for( u : (u,v) \\in E){\n        update w(u);\n        ...\n    }\n    delete v and all edges incident to it from the graph.\n}\n```\n\n\nThe outer loop produces the term ```\n|E|```\n in the runtime complexity. That means that picking a vertex out of a list of vertices which minimizes some ratio can be done in ```\nlog n```\n time. As far as I can tell, finding a minimum value out of a list of values takes ```\nn-1```\n comparisons, not ```\nlog n```\n. Finally, the inner for loop runs for every neighbor of v, so yields a complexity of d(v) which is dominated by n-1. Hence I would conclude that the algorithm has a runtime complexity of ```\nO(|E|*|V|)```\n. \nWhat am I missing here?\n    ", "Answer": "\r\nKeep the vertices in a balanced binary search tree ordered by w(v)/d(v). Finding the min is O(log |V|). Each time we delete an edge uv, we have to update u's key (by removing it and reinserting it into the tree with the new key), which takes time O(log |V|). Each of these steps is done at most |E| times.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fast approximate algorithm for cardinality of sets intersection\r\n                \r\nI have a pool sets (with size of pool n), with all sets not fitting in the RAM. I can only fit only a small fraction, say 1-5% of all sets into RAM. \n\nThe problem is given query set Q I need to return top k sets with largest cardinality of intersect with Q. \n\n\nAssuming Q in from the same pool of sets. \nFor general Q.\n\n\nk is small, in hundreds, while n in hundreds of millions. Total number of district elements in all sets in hundreds of millions, too. \n\n\nThere are many probabilistic data structures, KMV, MinHash and it's\nvariants, which one should I use?  \nCan I modify HyperLogLog for my\ntask?  \nWhich of those structures can be assembled into some kind of index?\n\n\nI did some experiments representing sets as bloom filters. Because sets size varies greatly I have to use very large bloomfilters, which is inefficient (bloomfiltes take 5x space of original dataset). Adaptive bloomfiters from https://github.com/jaybaird/python-bloomfilter produce only 3-5x compression of the dataset, so this is still pretty much infeasible.   \n    ", "Answer": "\r\nK-Minimum Values data structure is extremely memory efficient. Unlike Bloom filters, it does not provide membership test, only set-theoretic operations and cardinality estimate.\n\nMight work for you, depending on cardinalities of your sets and your error tolerance.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to find (optimal) integer ratio of different precision?\r\n                \r\nIf I have a variable ```\nm```\n of type ```\nuint32```\n and ```\nr```\n of type ```\nuint16```\n as well as a constant ```\nfloat64```\n with i.e. value ```\nf=0.5820766091346741```\n. How do I find ```\nm,r```\n which satisfy ```\nf=r/m```\n?\n\nSimilar as Fraction.limit_denominator from python.\n\nThis github repo contains various best-rational approximation algorithms but only limits the denominator.\n    ", "Answer": "\r\nThe straightforward answer would be:\n\n```\n     ROUND(f * 10^8)\nf = ----------------\n         10^8\n```\n\n\nThen, you can implement a small loop that attempts to divide both numerator and denominator by prime numbers (starting from 2 and up). Something like (code not checked of course):\n\n```\nvar m = f * 10^8 ;\nvar r = 10^8     ;\nvar Prime_Numbers = [2,3,5,7,11,13,17,19,....] ;\n\nfor (var I = 0 ; I < Prime_Numbers.length ; I++) {\n\n    if ((Prime_Numbers[I] > m) ||\n        (Prime_Numbers[I] > r)    ) {\n\n       break;\n    }\n\n    if (((m % Prime_Numbers[I]) == 0) &&\n         (r % Prime_Numbers[I]) == 0)    ) {\n          m = m / Prime_Numbers[I] ;\n          r = r / Prime_Numbers[I] ;\n    }\n\nconsole.log(\"Best m is: \" + m) ;\nconsole.log(\"Best r is: \" + r) ;\n:\n:\n}\n```\n\n\nNow, the question would be how many primary numbers I should include in the list?\n\nHard to say, but intuitively not too many... I would say it would depend on how rigorous you are about OPTIMAL.\n\nHope this gives you some direction.\n\nCheers!!\n\nEDIT:\n\nThinking a little bit further, to always get the ABSOLUTE OPTIMAL values, you need to include all primary number up to half the max value you wish as precision. For instance, if tour precision needs to be 8 digits (99999999), you need to include all primary numbers up to (99999999/2).\n\nEDIT 2:\n\nAdded an exit condition in the loop.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Bezier curve approximation for large amount of points\r\n                \r\nI have about hundred points, that I want to approximate with Bezier curve, but if there are more than 25 points (or something like that), factorial counting in number of combination causes number overflow.\nIs there a way of approximating such amount of points in a Bezier-like way (smooth curve without passing through all points, except first and last)?\nOr do I need to choose another approximation algorithm with the same effect?\nI'm using default swing drawing tools.\n\nP.S. English is not native for me, so probably I've used wrong math terms somewhere.\n    ", "Answer": "\r\nDo you want to get one Bezier curve fitting best in all 100 points? If that is the case Jim Herold has a very detailed explanation how to do it. A further optimisation could be reducing the amount of points using the Douglas-Peucker algorithm.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Are there algorithms to (approximately?) sort data that can change?\r\n                \r\nAll sorting algorithms I know require exclusive access to the data structure they work on. Are there any that can handle data that can change at any time?\n\nTo make this possible at all, we can certainly assume:\n\n\nThe rate of change is low, let's say i.e. we will repeatedly have enough time to walk through the whole structure and verify that it is currently sorted.\nAll changes are atomic and do not violate integrity, i.e. we won't deal with accidentally lost pointers etc., and all changes can be assumed to perform additional actions to ensure the structure is still connected (let's say in at most O(log n) time).\n\n\nI'm interested in any information, papers or implementations, also if they have more or less strict assumptions than those above.\n    ", "Answer": "\r\nMany, many data structures maintain data in sorted order. For example, any tree, skiplist, heap, etc., allows ordered access. In general, inserting, removing or updating a data item is O(log N) or better (N = number of items in the dataset). Therefore you can expect the cost of maintaining the sorted invariant for the dataset over a time interval to be O(M*log(N)), where M is the number of items you insert/delete/update in that time interval.\n\nSome sorting algorithms (insertion sort for example) perform better when data is partially sorted. At best, the cost of running such an algorithm is O(N), but this happens only in very limited circumstances. On average you can expect it to be closer to O(N*log(N)).\n\nTherefore, if the sorting invariant of the dataset needs to be maintained at all times you should use a data structure like an index or heap. However, if you only need to have the data sometimes, it might be more efficient to just buffer the updates in an array and re-sort the whole dataset whenever needed.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Find distorted rectangle in image (OpenCV)\r\n                \r\nI am looking for the right set of algorithms to solve this image processing problem:\n\n\nI have a distorted binary image containing a distorted rectangle\nI need to find a good approximation of the 4 corner points of this rectangle\n\n\nI can calculate the contour using OpenCV, but as the image is distorted it will often contain more than 4 corner points. \nIs there a good approximation algorithm (preferably using OpenCV operations) to find the rectangle corner points using the binary image or the contour description?\n\nThe image looks like this:\n\n\n\nThanks!\n\nDennis\n    ", "Answer": "\r\nUse cvApproxPoly function to eliminate number of nodes of your contour, then filter out those contours that have too many nodes or have angles which much differ from 90 degrees. See also similar answer \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How can I generate and run random lambda expressions?\r\n                \r\nI am working on a school project related to genetic algorithms. I need to create an approximation algorithm where I create random lambda expressions in order to approximate the correct value, so far I am able to create random lambda expressions thanks to an example I was provided with, however, I am stuck on how to run them, here's my code:\n\n```\n#lang racket\n(define operators '(+ *))\n(define operands '(a b x y))\n\n(define element\n  (lambda (L)\n    (list-ref L (random (length L)))))\n\n(define expression\n  (lambda (n)\n    (cond ((zero? n) (element operands))\n          (else\n           (list\n            (element operators)\n            (expression (random n))\n            (expression (random n)))\n           ))))\n\n;Generates lambda expression\n;n is the amount of expressions \n;p is the depth of each expression\n;Examples below\n(define generate\n  (lambda (n p)\n    (if (zero? n) empty\n        (cons\n         (list 'λ '(x y) (expression p))\n         (generate (- n 1) p))\n        )))\n\n(generate 1 1)\n'((λ (x y) (* b y)))\n(generate 1 3)\n'((λ (x y) (* (* y y) (* y y))))\n(generate 2 1)\n'((λ (x y) (+ b x)) (λ (x y) (+ b x)))\n(generate 2 3)\n'((λ (x y) (+ (+ a y) a)) (λ (x y) (* (+ (+ y y) (* a a)) (+ (+ y x) a))))\n```\n\n\nEach of these generated expressions is random and it's an approximation to the correct value. How can I run these generated expressions if they are Lists? \n    ", "Answer": "\r\nNormally if we just wanted to evaluate expressions with given values for x and y, we do not need eval. A more direct would be used with apply-formula, however in this case we are running procedurally generated, randomized expressions so eval would be needed.\n\n\n  The eval function takes a representation of an expression or definition (as a “quoted” form or syntax object) and evaluates it\n\n\nHere's an example, on how to run eval:\n\n```\n#lang racket\n(define ns (make-base-namespace))\n((eval '(lambda (x y) (+ x x)) ns) 1 1)\n```\n\n\nNote we are sending namespace in every eval call\n\nMy solution would look simply like this:\n\n```\n((eval (car (generate 2 2)) ns) 1 1)\n```\n\n\nIf you wish to test my code, modify the operands so it looks like this (Since it only validates x and y variables for now):\n\n```\n(define operands '(x y))\n```\n\n\nSource \n\n\nhttps://docs.racket-lang.org/guide/eval.html\nMysterious Racket error: define: unbound identifier; also, no #%app syntax transformer is bound in: define\nAnd some help in the comments\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Optimal (approximated) path Algorithm (distance + priority)\r\n                \r\nI need to write an algorithm for the following scenario:\n\n\n  Given a set of points (cities) on a map I have to visit. Each of these cities has a priority, from 1 (you need to go there ASAP) to 5 (no rush, but go there eventually). I have limited ressources so I can not visit all the priority-1 cities at first. (E.g if NY and SF have priority 1 and Washington has Priority 5, I'm looking for a path NY-Washington-SF not NY-SF-Washington). \n\n\nI don't know it it matters, but ```\nn```\n (# of cities) will usually be around 10-20. \n\nI found a presentation of \"The Hierarchical Traveling Salesman Problem\" (Panchamgam, Xiong, Golden and Wasi) which is kind of what I'm looking for but the corresponding article is not publicly accessible. \n\nCan you recommend existing algorithms for such scenarios? Or point me in the right direction what to search for? \n\nAn approximation would be alright. My scenario is not as life-threatening as in the scenario described by Panchamgam et. al. It's important to avoid unnecessary detours caused by the priorities without ignoring them completely. \n    ", "Answer": "\r\nIn standard TSP you want to minimize the total length of the route. In your case you want to basically optimize two metrics: the length of the route, and how early high priority cities appear on the route. You need to put these two metrics into a single metric, and how you do that might have an impact on the algorithm choice. For example, map the city priorities to penalties, e.g.\n\n```\n1  ->  16\n2  ->  8\n3  ->  4\n4  ->  2\n5  ->  1\n```\n\n\nThen use as the metric you want to minimize the total sum of (city_penalty * distance_to_city_from_start_on_route). This pushes the high priority cities to the beginning of the route in general but allows for out of priority order traversal if otherwise the route becomes too long. Obviously the penalty values should be experimentally tuned.\n\nWith this metric, you can then use e.g. standard stochastic search approach --- start with a route and then swap cities or edges on the route in order to decrease the metric (use simulated annealing or tabu search).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "approximate sorting Algorithm\r\n                \r\nDoes anyone know an Algorithm that sorts  k-approximately an array?\n\nWe were asked to find and Algorithm for k-approximate sorting, and it should run in O(n log(n/k)). but I can't seem to find any.\n\nK-approx. sorting means that an array and any 1 <= i <= n-k such that sum a[j] <= sum a[j] i<=j<= i+k-1 i+1<=j<= i+k\n    ", "Answer": "\r\nI know I'm very late to the question ... But under the assumption that k is some approximation value between 0 and 1 (when 0 is completely unsorted and 1 is perfectly sorted) surely the answer to this is quicksort (or mergesort).\n\nConsider the following array:\n\n```\n[4, 6, 9, 1, 10, 8, 2, 7, 5, 3]\n```\n\n\nLet's say this array is 'unsorted' - now apply one iteration of quicksort to this array with the (length[array]/2)th element as a pivot: length[array]/2 = 5. So the 5th element is our pivot (i.e. 8):\n\n```\n[4, 6, 2, 1, 3, 9, 7, 10, 8]\n```\n\n\nNow this is array is not sorted - but it is more sorted than one iteration ago, i.e. its approximately sorted but for a low approximation, i.e. a low value of k. Repeat this step again on the two halves of the array and it becomes more sorted. As k increases towards 1 - i.e. perfectly sorted - the complexity becomes ```\nO(N log(N/1)) = O(N log(N))```\n.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "algorithms for fast string approximate matching\r\n                \r\nGiven a source string ```\ns```\n and ```\nn```\n equal length strings, I need to find a quick algorithm to return those strings that have at most ```\nk```\n characters that are different from the source string ```\ns```\n at each corresponding position. \n\nWhat is a fast algorithm to do so?\n\nPS: I have to claim that this is a ```\nacademic```\n question. I want to find the most efficient algorithm if possible. \n\nAlso I missed one very important piece of information. The ```\nn```\n equal length strings form a dictionary, against which many source strings ```\ns```\n will be queried upon. There seems to be some sort of preprocessing step to make it more efficient.\n    ", "Answer": "\r\nMy gut instinct is just to iterate over each String ```\nn```\n, maintaining a counter of how many characters are different than ```\ns```\n, but I'm not claiming it is the most efficient solution. However it would be O(n) so unless this is a known performance problem, or an academic question, I'd go with that.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "what does n^O(1/ε) means?\r\n                \r\nI often find n^O(1/ε) in approximation algorithms. for example, in euclidean tsp, the number of portals(with its possible state) is equal to n^O(1/ε).\n\nhere is the link to the source\nhttp://algo2.iti.kit.edu/schultes/lebenslauf/publications/euclTSPsummary.pdf\ncan someone explain why 3^4m = n^O(1/ε)?\n    ", "Answer": "\r\nGenerally speaking, the complexity of approximation algorithms depends on the precision wanted, often noted by ε to say you want the ratio of your answer to the best one to differ by no more than ε.\nHere, this means that the dependence is in the exponent of the time complexity, as the algorithm is polynomial for every ε but with a bigger exponent the smaller ε is. Namely, here the exponent of the time complexity as a function of ε grows as a O(1/ε).\nIndeed, in this paper it is said that \"The parameter m must lie in the interval [k/ε, 2k/ε]\" and 4^k=O(n^4), so 3^(4m) = 3^(O(k/ε)) = 3^(O(log(n)/ε)) = n^(O(1/ε)).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "approximated algorithm of union two non-disjoint sets\r\n                \r\nWhats the best algorithm for Non Disjoint Set Union? is about the precise algorithm of non-disjoint set union.\nWith balanced trees, its union cost is n*log(n) where n is the average size of a set; and its membership cost is log(n); the space cost is n if we use balanced trees. With hashmaps, its union cost is O(n), find cost is log(n), and space cost is n.\nMy question is if we allow more space, can time cost be smaller?\nA native approach is like this\nIf two sets are s1=(a, b) and s2=(b, c), we create a new node s12 whose two children are s1 and s2. Given this, s1 is also a tree with leaves a and b; s2 is similar. So all the sets are like this.\n```\n       s21\n       /  \\\n      s1  s2\n     / \\  / \\\n     a b  b c\n```\n\nThis is basically a union-find on a non-disjoint set.\nTo check is v in a set, we traverse the whole tree of the set. So union's cost is O(1), and find's cost is log(n) on average. But the space cost can be a lot because this way we will be creating a lot of trees for the same set. For example,\n```\n          s\n         / \\\n        s  a\n        /\\ \n        a a\n     \n```\n\nAnother native approach would be using a long bit vector to mark which element is used in a set assuming set elements are limited. For example, if my sets can contain only 10k elements, the bit vector's length is 10k, and its i-th bit is 1 iff this element is in the set. With this, set union is turned into a bit vector or. However, this space cost is high for a sparse set, and run time union cost is also same to the number of total possible elements.\nIs there a better (using less space but still getting smaller time cost) approach than these?\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Any python library for Bezier curve approximation with biarcs?\r\n                \r\nI am in need of a simple python library which can perform Bezier curve approximation algorithm with biarcs (biarc interpolation). I am looking for something like the algorithm explained at approximating-bezier-curves-by-biarcs which also includes CSharp implementation. \n\nI have tried searching, for similar implementation in python. I found some but they were inside in CNC controllers code like gcodetools. Extracting just the part I need, seems to be complicated. And couldn't find any simple ones, which just implement the algorithm. \n\nBefore I try to convert CSharp code to python, I want to check here if any such python script already exists. Please share anything you think might be helpful.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximate Optimization using gurobi attributes in python\r\n                \r\nI'm trying to use python for an ILP optimization using gurobi.\nI wonder if primal-dual approximation has been implemented on gurobi and how should we use an approximation algorithm.\n\nI'm using m.optimize function and don't know which attributes to add for an approximate optimization.\n\nAny help will be appreciated,\nDiman\n    ", "Answer": "\r\nThere are several parameters you can set to use Gurobi as an approximation algorithm for mixed integer programming.  The most common ones are\n\n\nMIPGap Gurobi will stop when it finds a solution within a percentage of optimal\nTimeLimit Gurobi will stop after a certain amount of time.\nMIPGapAbs Gurobi will stop when if finds a solution within an absolute amount of optimal.\nIf Gurobi does find a solution, it will always report both the solution and the best known bound.\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Are there algorithms that are only feasible in a recursive manner?\r\n                \r\nAre there algorithms that are only feasible in a recursive manner? If yes, how can one identify these algorithms?\n\n\n\nConsider the following as a case study.\n\nIn order to approximate a exponential, one can use.\n\n\n  ef ≈ ```\n1 + f(1 + f/2(1 + f/3(1 + f/4)))```\n\n\n\nHere is a function that computes this approximation at a level of n steps using recursion.\n\n```\nexp_approximation = function(f, n, i = 2)\n{\n    if (n == 1)\n    {\n        return( 1 + f/i )\n    } else\n    {\n        1 + f/i * exp_approximation(f,n=n-1,i=i+1)\n    }\n\n}\n```\n\n\nIs it possible to write the same algorithm using iteration instead of recursion?\n\nI wrote my code in R but I welcome solution in pseudo code other some other commonly used language such as C, C++, Java, Python, ...\n    ", "Answer": "\r\nYes, technically it's possible. See, for example, this question for a generic conversion method. That said, many algorithms are much more intuitive in the recursive form (and might be more efficient in functional languages but not in imperative ones). That's especially true for math-related problems involving recurrences, where recursion is in fact a representation of a recurrence relation. \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Q-learning with linear function approximation\r\n                \r\nI would like to get some helpful instructions about how to use the Q-learning algorithm with function approximation. For the basic Q-learning algorithm I have found examples and I think I did understand it. In case of using function approximation I get into trouble. Can somebody give me an explanation through a short example how it works?\n\nWhat I know:\n\n\nIstead of using matrix for Q-values we use features and parameters.\nMake approximation with the linear combination of feauters and parameters.\nUpdate the parameters.\n\n\nI have checked this paper: Q-learning with function approximation\n\nBut I cant find any useful tutorial how to use it.\n\nThanks for help!\n    ", "Answer": "\r\nTo my view, this is one of the best references to start with. It is well written with several pseudo-code examples. In your case, you can simplify the algorithms by ignoring eligibility traces. \n\nAlso, to my experience and depending on your use case, Q-Learning might not work very well (sometimes it needs huge amounts of experience data). You can try Fitted-Q value for example, which is a batch algorithm.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximated closest pair algorithm\r\n                \r\nI have been thinking about a variation of the closest pair problem in which the only available information is the set of distances already calculated (we are not allowed to sort points according to their x-coordinates).\n\nConsider 4 points (A, B, C, D), and the following distances:\n\n```\ndist(A,B) = 0.5\ndist(A,C) = 5\ndist(C,D) = 2\n```\n\n\nIn this example, I don't need to evaluate ```\ndist(B,C)```\n or ```\ndist(A,D)```\n, because it is guaranteed that these distances are greater than the current known minimum distance. \n\n\nIs it possible to use this kind of information to reduce the O(n²) to something like O(nlogn)?\nIs it possible to reduce the cost to something close to O(nlogn) if I accept a kind of approximated solution? In this case, I am thinking about some technique based on reinforcement learning that only converges to the real solution when the number of reinforcements go to infinite, but provides a great approximation for small n.\nProcessing time (measured by the big O notation) is not the only issue. To keep a very large amount of previous calculated distances can also be an issue.\nImagine this problem for a set with 10⁸ points.\n\n\nWhat kind of solution should I look for? Was this kind of problem solved before?\n\nThis is not a classroom problem or something related. I have been just thinking about this problem. \n    ", "Answer": "\r\nI suggest using ideas that are derived from quickly solving k-nearest-neighbor searches.\n\nThe M-Tree data structure: (see http://en.wikipedia.org/wiki/M-tree and http://www.vldb.org/conf/1997/P426.PDF )  is designed to reduce the number distance comparisons that need to be performed to find \"nearest neighbors\".\n\nPersonally, I could not find an implementation of an M-Tree online that I was satisfied with  (see my closed thread Looking for a mature M-Tree implementation) so I rolled my own.\n\nMy implementation is here: https://github.com/jon1van/MTreeMapRepo\n\nBasically, this is binary tree in which each leaf node contains a HashMap of Keys that are \"close\" in some metric space you define.\n\nI suggest using my code (or the idea behind it) to implement a solution in which you:\n\n\nSearch each leaf node's HashMap and find the closest pair of Keys within that small subset.\nReturn the closest pair of Keys when considering only the \"winner\" of each HashMap.\n\n\nThis style of solution would be a \"divide and conquer\" approach the returns an approximate solution.\n\nYou should know this code has an adjustable parameter the governs the maximum number of Keys that can be placed in an individual HashMap.  Reducing this parameter will increase the speed of your search, but it will increase the probability that the correct solution won't be found because one Key is in HashMap A while the second Key is in HashMap B.\n\nAlso, each HashMap is associated a \"radius\".  Depending on how accurate you want your result you maybe able to just search the HashMap with the largest hashMap.size()/radius (because this HashMap contains the highest density of points, thus it is a good search candidate)\nGood Luck \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Rational approximation of double using int numerator and denominator in C++\r\n                \r\nA real world third party API takes a parameter of type ```\nfraction```\n which is a ```\nstruct```\n of an ```\nint```\n numerator and denominator. The value that I need to pass is known to me as a decimal string that is converted to a double.\nThe range of possible values are, let's say 10K to 300M but if there is a fraction part after the decimal point, it's significant.\nI have here code for two approximation approaches, one uses the extended euclidean algorithm while the other is brute-force. Both methods find a rational approximation using ```\nint```\n types for a given ```\ndouble```\n.\nThe brute-force is of course the more accurate of the two and is actually faster when the converted numbers are large. My questions is, can I say anything clever about the quality of the approximation using the euclidean algorithm.\nMore formally, can I put a bound on the approximation using the euclidean algorithm vs. the approximation of the brute-force algorithm (which I believe to be optimal).\nAn example for a bound:\nIf the error of the optimal approximation is ```\nr```\n, then the euclidean algorithm approximation would produce an error that is less than ```\n2*r```\n.\n(I'm not claiming this is the bound and I certainly can't prove it, it's just an example for what a good bound may look like).\nHere's the code an a test program:\n```\n#include <iostream>\n#include <iomanip>\n#include <cmath>\n#include <limits>\n#include <chrono>\n#include <random>\n\n// extended euclidian algorithm\n// finds the coefficients that produce the gcd\n// in u, we store m,n the coefficients that produce m*a - n*b == gcd.\n// in v, we store m,n the coefficients that produce m*a - n*b == 0.\n// breaks early if the coefficients become larger than INT_MAX\nint gcd_e(uint64_t a, int b, int u[2], int v[2])\n{\n    auto w = lldiv(a, b);\n\n    // u[0] * a' - u[1] * b' == a\n    // v[0] * a' - v[1] * b' == b\n\n    // a - w.quot * b == w.rem\n    // (u[0] * a' - u[1] * b') - w.quot * (v[0] * a' - v[1] * b') == w.rem\n    // (u[0] - w.quot * v[0]) * a' - u[1] * b' + w.quot * v[1] * b' == w.rem\n    // (u[0] - w.quot * v[0]) * a' + (w.quot * v[1] - u[1]) * b' == w.rem\n    // (u[0] - w.quot * v[0]) * a' - (u[1] - w.quot * v[1]) * b' == w.rem\n\n\n    auto m = u[0] - w.quot * v[0];\n    auto n = u[1] - w.quot * v[1];\n    u[0] = v[0];\n    u[1] = v[1];\n\n    constexpr auto L = std::numeric_limits<int>::max();\n    if (m > L || n > L)\n        throw 0;  // break early\n    if (m < -L || n < -L)\n        throw 0;  // break early\n\n    v[0] = int(m);\n    v[1] = int(n);\n\n    if (w.rem == 0)\n        return b;\n\n    return gcd_e(b, int(w.rem), u, v);\n}\n\n\ninline double helper_pre(double d, bool* negative, bool* inverse)\n{\n    bool v = (d < 0);\n    *negative = v;\n    if (v)\n        d = -d;\n\n    v = (d < 1);\n    *inverse = v;\n    if (v)\n        d = 1 / d;\n\n    return d;\n}\n\ninline void helper_post(int* m, int* n, bool negative, bool inverse)\n{\n    if (inverse)\n        std::swap(*n, *m);\n\n    if (negative)\n        *n = -(*n);\n}\n\n// gets a rational approximation for double d\n// numerator is stored in n\n// denominator is stored in m\nvoid approx(double d, int* n, int *m)\n{\n    int u[] = { 1, 0 };  // 1*a - 0*b == a\n    int v[] = { 0, -1 }; // 0*a - (-1)*b == b\n\n    bool negative, inverse;\n    d = helper_pre(d, &negative, &inverse);\n\n    constexpr int q = 1 << 30;\n    auto round_d = std::round(d);\n    if (d == round_d)\n    {\n        // nothing to do, it's an integer.\n        v[1] = int(d);\n        v[0] = 1;\n    }\n    else try\n    {\n        uint64_t k = uint64_t(std::round(d*q));\n        gcd_e(k, q, u, v);\n    }\n    catch (...)\n    {\n        // OK if we got here.\n        // int limits\n    }\n\n    // get the approximate numerator and denominator\n    auto nn = v[1];\n    auto mm = v[0];\n\n    // make them positive\n    if (mm < 0)\n    {\n        mm = -mm;\n        nn = -nn;\n    }\n\n    helper_post(&mm, &nn, negative, inverse);\n\n    *m = mm;\n    *n = nn;\n}\n\n\n// helper to test a denominator\n// returns the magnitude of the error\ndouble helper_rattest(double x, int tryDenom, int* numerator)\n{\n    double r = x * tryDenom;\n    double rr = std::round(r);\n    auto num = int(rr);\n    auto err = std::abs(r - rr) / tryDenom;\n    *numerator = num;\n    return err;\n}\n\n// helper to reduce the rational number\nint gcd(int a, int b)\n{\n    auto c = a % b;\n    if (c == 0)\n        return b;\n    return gcd(b, int(c));\n}\n\n// gets a rational approximation for double d\n// numerator is stored in n\n// denominator is stored in m\n// uses brute force by scanning denominator range\nvoid approx_brute(double d, int* n, int* m)\n{\n    bool negative, inverse;\n    d = helper_pre(d, &negative, &inverse);\n\n    int upto = int(std::numeric_limits<int>::max() / d);\n    int bestNumerator;\n    int bestDenominator = 1;\n    auto bestErr = helper_rattest(d, 1, &bestNumerator);\n    for (int kk = 2; kk < upto; ++kk)\n    {\n        int n;\n        auto e = helper_rattest(d, kk, &n);\n        if (e < bestErr)\n        {\n            bestErr = e;\n            bestNumerator = n;\n            bestDenominator = kk;\n        }\n        if (bestErr == 0)\n            break;\n    }\n\n    // reduce, just in case\n    auto g = gcd(bestNumerator, bestDenominator);\n    bestNumerator /= g;\n    bestDenominator /= g;\n\n    helper_post(&bestDenominator, &bestNumerator, negative, inverse);\n\n    *n = bestNumerator;\n    *m = bestDenominator;\n}\n\nint main()\n{\n    int n, m;\n\n    auto re = std::default_random_engine();\n    std::random_device rd;\n    re.seed(rd());\n\n    for (auto& u : {\n        std::uniform_real_distribution<double>(10000,    15000),\n        std::uniform_real_distribution<double>(100000,   150000),\n        std::uniform_real_distribution<double>(200000,   250000),\n        std::uniform_real_distribution<double>(400000,   450000),\n        std::uniform_real_distribution<double>(800000,   850000),\n        std::uniform_real_distribution<double>(1000000,  1500000),\n        std::uniform_real_distribution<double>(2000000,  2500000),\n        std::uniform_real_distribution<double>(4000000,  4500000),\n        std::uniform_real_distribution<double>(8000000,  8500000),\n        std::uniform_real_distribution<double>(10000000, 15000000)\n        })\n    {\n        auto dd = u(re);\n        std::cout << \"approx: \" << std::setprecision(14) << dd << std::endl;\n\n        auto before = std::chrono::steady_clock::now();\n        approx_brute(dd, &n, &m);\n        auto after = std::chrono::steady_clock::now();\n        std::cout << n << \" / \" << m << \"  dur: \" << (after - before).count() << std::endl;\n        before = std::chrono::steady_clock::now();\n        approx(dd, &n, &m);\n        after = std::chrono::steady_clock::now();\n        std::cout << n << \" / \" << m << \"  dur: \" << (after - before).count()\n            << std::endl\n            << std::endl;\n    }\n}\n```\n\nHere's some sample output:\n```\napprox: 13581.807792679\n374722077 / 27590  dur: 3131300\n374722077 / 27590  dur: 15000\n\napprox: 103190.31976517\n263651267 / 2555  dur: 418700\n263651267 / 2555  dur: 6300\n\napprox: 223753.78683426\n1726707973 / 7717  dur: 190100\n1726707973 / 7717  dur: 5800\n\napprox: 416934.79214075\n1941665327 / 4657  dur: 102100\n403175944 / 967  dur: 5700\n\napprox: 824300.61241502\n1088901109 / 1321  dur: 51900\n1088901109 / 1321  dur: 5900\n\napprox: 1077460.29557\n1483662827 / 1377  dur: 39600\n1483662827 / 1377  dur: 5600\n\napprox: 2414781.364653\n1079407270 / 447  dur: 17900\n1079407270 / 447  dur: 7300\n\napprox: 4189869.294816\n1776504581 / 424  dur: 10600\n1051657193 / 251  dur: 9900\n\napprox: 8330270.2432111\n308219999 / 37  dur: 5400\n308219999 / 37  dur: 10300\n\napprox: 11809264.006453\n1830435921 / 155  dur: 4000\n1830435921 / 155  dur: 10500\n```\n\n    ", "Answer": "\r\nThanks to all who commented and drew my attention to the concept of continued fractions.\nAccording to this paper by (William F. Hammond)\nThere is equivalence between the euclidean algorithm and the continued fractions method.\n\nThe sub-optimal results are due to the fact that the numerator is constrained as well as the denominator so if the non brute force algorithm only produces \"convergents\" it means that it neglects the range of denominators between the first convergent to violate the constraints and the one just before it.\nThe denominators after the returned convergent and the one that follows may approximate close to the latter convergent and the difference between subsequent convergents can be shown to be:\n\nSo I suppose this would be the bound on the difference between the brute-force and the euclidean algorithm. The ratio of the error between them can be practically anything.\n(can find examples of error ratios of more than 100 easily)\nI hope I read everything correctly. I'm no authority on this.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Difference between function approximator and optimization algorithm?\r\n                \r\nI just started learning about artificial neural networks and genetic algorithms and found that the difference between them is that ANN is a function approximator and that GA is an optimization algorithm (according to SO). Problem is I am not 100% sure where and how to draw the line between these definitions; is there a simpler way to explain what the difference is using e.g. analogies (assume I am a 10 year old)? What I found particularly confusing is that both types seem to be able to solve the same problem in some cases (e.g. Traveling Salesman Problem). \n    ", "Answer": "\r\nANNs approximate an unknown function that correlates input and output. The goal of ANNs is to find the mathematical relation between both: if is presented a new input, the modeling found by the net gives an approximation to the true value. Example: find the pressure of gas in a tube giving as input temperature, viscosity, density, section of tube, ecc., using a set of measurements for training.\n\nGAs are used often to find max or min of a function (optimization). For example: find the optimal net (minor error) for my previous example, using a set of nets, or solve the traveling salesman problem (given a set of cities, visit each city once and find the minimal path).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fuzzy/approximate checking of solutions from algorithms\r\n                \r\nWe have people who run code for simulations, testing etc. on some supercomputers that we have. What would be nice is, if as part of a build process we can check that not only that the code compiles but that the ouput matches some pattern which will indicate we are getting meaningful results.\n\ni.e. the researcher may know that the value of x must be within some bounds. If not, then a logical error has been made in the code (assuming it compiles and their is no compile time error). \n\nAre there any pre-written packages for this kind of thing. The code is written in FORTRAN, C, C++ etc.\n\nAny specific or general advice would be appreciated.\n    ", "Answer": "\r\nI expect most unit testing frameworks could do this; supply a toy test data set and see that the answer is sane in various different ways.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation of growth of a function\r\n                \r\nI am trying to learn about analysis of algorithms, but I find running times a little bit hard to understand. I have a problem where I am supposed to find the exact approximation of the growth of a function and the big-O notation. \n\nI am wondering if the order of growth of D is N, because function D is only executing step() at most n times?\n\nI have the function \n\n```\nvoid step() {\n        count++;\n    }\nvoid functionD(int n) {\n        if (n % 2 == 0) {\n            step();\n        }\n        else\n            step();\n    }\n```\n\n    ", "Answer": "\r\nIt seems that you've made a typo, since your claim\n\n\n  because ```\nfunctionD```\n is only executing ```\nstep()```\n at most ```\nn```\n times?\n\n\nsounds strange. Just have look. Both ```\nif```\n branches do the same:\n\n```\nvoid functionD(int n) {\n    if (n % 2 == 0) {  // either n is even \n        step();        //   ... do the step()\n    }                  \n    else               // or n is odd\n        step();        //   ... do exactly the same - step()\n}\n```\n\n\nso we can simplify the code into\n\n```\nvoid functionD(int n) {\n    // either n is even or odd do the step; \n    step();\n}\n```\n\n\nNow, since ```\nstep```\n does nothing but increment a ```\nconter```\n\n\n```\nvoid step() {\n    count++;\n} \n```\n\n\n```\nfunctionD```\n increments a ```\ncounter```\n and that's all:\n\n```\nvoid functionD(int n) {\n    count++;\n}\n```\n\n\nFinally, we have for ```\nfunctionD```\n: ignore ```\nn```\n, increment a ```\ncounter```\n, this operation has evident ```\nO(1)```\n time complexity (```\nfunctionD```\n ignores ```\nn```\n).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm for online approximation of a slowly-changing, real valued function\r\n                \r\nI'm tackling an interesting machine learning problem and would love to hear if anyone knows a good algorithm to deal with the following:\n\n\nThe algorithm must learn to approximate a function of N inputs and M outputs\nN is quite large, e.g. 1,000-10,000\nM is quite small, e.g. 5-10\nAll inputs and outputs are floating point values, could be positive or negative, likely to be relatively small in absolute value but no absolute guarantees on bounds\nEach time period I get N inputs and need to predict the M outputs, at the end of the time period the actual values for the M outputs are provided (i.e. this is a supervised learning situation where learning needs to take place online)\nThe underlying function is non-linear, but not too nasty (e.g. I expect it will be smooth and continuous over most of the input space)\nThere will be a small amount of noise in the function, but signal/noise is likely to be good - I expect the N inputs will expain 95%+ of the output values\nThe underlying function is slowly changing over time - unlikely to change drastically in a single time period but is likely to shift slightly over the 1000s of time periods range\nThere is no hidden state to worry about (other than the changing function), i.e. all the information required is in the N inputs\n\n\nI'm currently thinking some kind of back-propagation neural network with lots of hidden nodes might work - but is that really the best approach for this situation and will it handle the changing function?\n    ", "Answer": "\r\nWith your number of inputs and outputs, I'd also go for a neural network, it should do a good approximation. The slight change is good for a back-propagation technique, it should not have to 'de-learn' stuff.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Simple algorithm for approximation image contour for line segment\r\n                \r\nThe image contour is a array points. I need to approach two points on each side to create four line segments.\n\nImage contour\n    ", "Answer": "\r\nA possible approach.\n\nFind the convex hull of these points.\n\nThen find the two vertices which are the furthest apart. This gives a first diagonal.\n\nThen find the two other vertices that are the farthest from the first diagonal. This gives a second diagonal.\n\nNow you can just use the quadrilateral formed by these four vertices.\n\nIf you want a tighter fit, you can consider the subsets of points in the four quadrants defined by the diagonals, and compute best fit lines, for example by least squares.\n\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm for Approximate Social Security Number Match Search\r\n                \r\nI wrote an application to link files according the client's social security numbers, but sometimes the client wrote 1 or 2 digits incorrectly. What are the algorithms I can deploy to return an approximate match?\n\nCorrectly I implemented a wild card function, allow me to search through the database if the hand writing is bad. But sometimes the hand writing is fine (they just wrote it incorrectly).\n\n```\n; Function to get the new SSN\n; Keep in mind the searchCount is reset at the end of this function only.\n; =============================================================================\nGetSSN() {\n    Global searchCount\n    UserInput = Blank\n    Length := StrLen(UserInput)\n\n    while (Length < 9) \n    {\n        InputBox, UserInput, Please Enter the SSN,,, 350, 100\n        Length := StrLen(UserInput)\n    }\n\n    ; Use the SubStr method to extract the first character of the ssn\n    firstCharacterOfInput := SubStr(UserInput, 1, 1)\n\n    if (firstCharacterOfInput = 0) {\n        StringTrimLeft, UserInput, UserInput, 1\n    }\n\n    ; Replace the wild card key \"*\" with the implementation required \".\"\n    StringReplace, UserInput, UserInput, *,.\n\n    ; Depend if the key contains a wild card, use normal / wild card search\n    IfInString, UserInput, . \n    {\n        ; MsgBox, WildCardSearch\n        mFileName := WildCardSearch(UserInput)\n    } else {\n        ; MsgBox, NormalSearch\n        mFileName := GetFileName(UserInput)\n    }\n\n    DrawFileName(mFileName)\n    ; Reset searchCount for next time use\n    searchCount = 0\n}\n\n; Function for wild card implementation\n; Reference: http://www.adarshr.com/papers/wildcard\n; The implementation used the RegEXMatch Expression\n; =============================================================================\nWildCardSearch(key) {\n    Global dataCount, searchCount, DataBaseArray\n\n    Loop, %dataCount% {\n        currentLine := DataBaseArray%searchCount%_1\n        FoundPos := RegExMatch(currentLine, key)\n\n        if (FoundPos != 0) {\n            result := DataBaseArray%searchCount%_2\n            return result\n        }\n        searchCount += 1\n    }\n\n    notFoundMapName = Unable to find the member %A_Now%.jpg\n    return notFoundMapName\n}\n```\n\n    ", "Answer": "\r\nOne way is to build an approximate match out of exact lookup. Divide the SSN up into three fields. If there are only two errors, at least one of those fields must be free of errors. So build three tables, each of them allowing you to retrieve all of the SSNs that match exactly on one of the fields.\n\nGiven an approximate SSN, retrieve all of the exact matches from each of the three tables, indexed by the three fields you have broken it into, and check the SSNs retrieved to see if any of them has no more than two characters wrong.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm for approximate search in sorted integer list\r\n                \r\nConsider an array of integers (assumed to be sorted); I would like to find the array index of the integer that is closest to a given integer in the fastest possible way. And the case where there are multiple possibilities, the algorithm should identify all.\n\nExample: consider T=(3, 5, 24, 65, 67, 87, 129, 147, 166), and if the given integer is 144, then the code should identify 147 as the closest integer, and give the array index 7 corresponding to that entry. For the case of 66, the algorithm should identify 65 and 67. \n\nAre there O(1) or at least O(log N) algorithms to do this? The direct search algorithm (binary-search, tree-search, hashing etc.) implementation won't work since those would require perfect matching. Is there any way these can be modified to handle approximate search?\n\nI am developing a C code.\n\nThanks\n    ", "Answer": "\r\nDo binary search until you get down to a single element.\nIf there is a match, walk along your neighbors to find other matches.\nIf there is no match, look at your immediate neighbors to find the closest match. \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Is there a function or package in R that can get lambda of poisson distribution from quantile and probability?\r\n                \r\nfor example, ```\nF(X = 2 | lambda = 2.3) =  0.596```\n I know 2 and 0.596 and I would like to get lambda.\n\nI know I can do it by numerical approximation algorithms. \n\nHowever before I manually create a function, is there an existing function that I can simply use?\n\nUpdate\n\nIt is a shame that I have to use numerical method. I had thought there is a popular well known analytic method / close form.\n    ", "Answer": "\r\nOptimization is probably the way to go with this.\n\n```\nf <- function(lambda) abs(ppois(2, lambda) - 0.5960388)\noptimize(f, c(0, 10))\n```\n\n\nOr, as Ben B suggested,\n\n```\nf2 <- function(lambda) ppois(2, lambda) - 0.5960388\nuniroot(f2, c(0, 10))\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Vertices that are K away\r\n                \r\nGiven a graph G(V,E) and a vertex v, how do i find all the vertices that are reachable via simple paths ( no vertex on the path repeats) of length exactly k.\n\nPowers of adjacency matrix gives the number of paths between vertices but it includes non simple paths.\n\nIs this problem solvable in polynomial time ? If not are there any known approximation algorithms. Any pointers to literature would be great.\n    ", "Answer": "\r\nI'm answering only the first question: \"is it solvable in polynomial time?\".\n\nSuppose it is solvable in polynomial time. Then solve it for ```\nk=|V|-1```\n and pick any resulting vertex. Remove this vertex and solve this problem for ```\nk=|V|-2```\n. Resulting set of vertices should contain at least one vertex that was connected to the last removed vertex. Remove this vertex and continue process for ```\nk=|V|-i```\n until single starting vertex remains. You've just found a Hamiltonian path for the original graph with polynomial time algorithm.\n\nSince Hamiltonian path problem is NP-complete, the problem in OP is also very unlikely to be solvable in polynomial time.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Best machine-optimized polynomial minimax approximation to arctangent on [-1,1]?\r\n                \r\nFor the simple and efficient implementation of fast math functions with reasonable accuracy, polynomial minimax approximations are often the method of choice. Minimax approximations are typically generated with a variant of the Remez algorithm. Various widely available tools such as Maple and Mathematica have built-in functionality for this. The generated coefficients are typically computed using high-precision arithmetic. It is well-known that simply rounding those coefficients to machine precision leads to suboptimal accuracy in the resulting implementation.\n\nInstead, one searches for closely related sets of coefficients that are exactly representable as machine numbers to generate a machine-optimized approximation. Two relevant papers are:\n\nNicolas Brisebarre, Jean-Michel Muller, and Arnaud Tisserand, \"Computing Machine-Efficient Polynomial Approximations\", ACM Transactions on Mathematical Software, Vol. 32, No. 2, June 2006, pp. 236–256.\n\nNicolas Brisebarre and Sylvain Chevillard, \"Efficient polynomial L∞-approximations\", 18th IEEE Symposium on Computer Arithmetic (ARITH-18), Montpellier (France), June 2007, pp. 169-176.\n\nAn implementation of the LLL-algorithm from the latter paper is available as the ```\nfpminimax()```\n command of the Sollya tool. It is my understanding that all algorithms proposed for the generation of machine-optimized approximations are based on heuristics, and that it is therefore generally unknown what accuracy can be achieved by an optimal approximation. It is not clear to me whether the availability of FMA (fused multiply-add) for the evaluation of the approximation has an influence on the answer to that question. It seems to me naively that it should.\n\nI am currently looking at a simple polynomial approximation for arctangent on [-1,1] that is evaluated in IEEE-754 single-precision arithmetic, using the Horner scheme and FMA. See function ```\natan_poly()```\n in the C99 code below. For lack of access to a Linux machine at the moment, I did not use Sollya to generate these coefficients, but used my own heuristic that could be loosely described as a mixture of steepest decent and simulated annealing (to avoid getting stuck on local minima). The maximum error of my machine-optimized polynomial is very close to 1 ulp, but ideally I would like the maximum ulp error to be below 1 ulp. \n\nI am aware that I could change my computation to increase the accuracy, for example by using a leading coefficient represented to more than single-precision precision, but I would like to keep the code exactly as is (that is, as simple as possible) adjusting only the coefficients to deliver the most accurate result possible.\n\nA \"proven\" optimal set of coefficients would be ideal, pointers to relevant literature are welcome. I did a literature search but could not find any paper that advances the state of the art meaningfully beyond Sollya's ```\nfpminimax()```\n, and none that examine the role of FMA (if any) in this issue.\n\n```\n// max ulp err = 1.03143\nfloat atan_poly (float a)\n{\n    float r, s;\n    s = a * a;\n    r =              0x1.7ed1ccp-9f;\n    r = fmaf (r, s, -0x1.0c2c08p-6f);\n    r = fmaf (r, s,  0x1.61fdd0p-5f);\n    r = fmaf (r, s, -0x1.3556b2p-4f);\n    r = fmaf (r, s,  0x1.b4e128p-4f);\n    r = fmaf (r, s, -0x1.230ad2p-3f);\n    r = fmaf (r, s,  0x1.9978ecp-3f);\n    r = fmaf (r, s, -0x1.5554dcp-2f);\n    r = r * s;\n    r = fmaf (r, a, a);\n    return r;\n}\n\n// max ulp err = 1.52637\nfloat my_atanf (float a)\n{\n    float r, t;\n    t = fabsf (a);\n    r = t;\n    if (t > 1.0f) {\n        r = 1.0f / r;\n    }\n    r = atan_poly (r);\n    if (t > 1.0f) {\n        r = fmaf (0x1.ddcb02p-1f, 0x1.aee9d6p+0f, -r); // pi/2 - r\n    }\n    r = copysignf (r, a);\n    return r;\n}\n```\n\n    ", "Answer": "\r\nThe following function is a faithfully-rounded implementation of ```\narctan```\n on ```\n[0, 1]```\n:\n\n```\nfloat atan_poly (float a) {\n  float s = a * a, u = fmaf(a, -a, 0x1.fde90cp-1f);\n  float r1 =               0x1.74dfb6p-9f;\n  float r2 = fmaf (r1, u,  0x1.3a1c7cp-8f);\n  float r3 = fmaf (r2, s, -0x1.7f24b6p-7f);\n  float r4 = fmaf (r3, u, -0x1.eb3900p-7f);\n  float r5 = fmaf (r4, s,  0x1.1ab95ap-5f);\n  float r6 = fmaf (r5, u,  0x1.80e87cp-5f);\n  float r7 = fmaf (r6, s, -0x1.e71aa4p-4f);\n  float r8 = fmaf (r7, u, -0x1.b81b44p-3f);\n  float r9 = r8 * s;\n  float r10 = fmaf (r9, a, a);\n  return r10;\n}\n```\n\n\nThe following test harness will abort if the function ```\natan_poly```\n fails to be faithfully-rounded on ```\n[1e-16, 1]```\n and print \"success\" otherwise:\n\n```\nint checkit(float f) {\n  double d = atan(f);\n  float d1 = d, d2 = d;\n  if (d1 < d) d2 = nextafterf(d1, 1.0/0.0);\n  else d1 = nextafterf(d1, -1.0/0.0);\n  float p = atan_poly(f);\n  if (p != d1 && p != d2) return 0;\n  return 1;\n}\n\nint main() {\n  for (float f = 1; f > 1e-16; f = nextafterf(f, -1.0/0.0)) {\n    if (!checkit(f)) abort();\n  }\n  printf(\"success\\n\");\n  exit(0);\n}\n```\n\n\n\n\nThe problem with using ```\ns```\n in every multiplication is that the polynomial's coefficients do not decay rapidly.  Inputs close to 1 result in lots and lots of cancellation of nearly equal numbers, meaning you're trying to find a set of coefficients so that the accumulated roundoff at the end of the computation closely approximates the residual of ```\narctan```\n.\n\nThe constant ```\n0x1.fde90cp-1f```\n is a number close to 1 for which ```\n(arctan(sqrt(x)) - x) / x^3```\n is very close to the nearest float.  That is, it's a constant that goes into the computation of ```\nu```\n so that the cubic coefficient is almost completely determined.  (For this program, the cubic coefficient must be either ```\n-0x1.b81b44p-3f```\n or ```\n-0x1.b81b42p-3f```\n.)\n\nAlternating multiplications by ```\ns```\n and ```\nu```\n has the effect of reducing the effect of roundoff error in ```\nri```\n upon ```\nr{i+2}```\n by a factor of at most 1/4, since ```\ns*u < 1/4```\n whatever ```\na```\n is.  This gives considerable leeway in choosing the coefficients of fifth order and beyond.\n\n\n\nI found the coefficients with the aid of two programs:\n\n\nOne program plugs in a bunch of test points, writes down a system of linear inequalities, and computes bounds on the coefficients from that system of inequalities.  Notice that, given ```\na```\n, one can compute the range of ```\nr8```\n that lead to a faithfully-rounded result.  To get linear inequalities, I pretended ```\nr8```\n would be computed as a polynomial in the ```\nfloat```\ns ```\ns```\n and ```\nu```\n in real-number arithmetic; the linear inequalities constrained this real-number ```\nr8```\n to lie in some interval.  I used the Parma Polyhedra Library to handle these constraint systems.\nAnother program randomly tested sets of coefficients in certain ranges, plugging in first a set of test points and then all ```\nfloat```\ns from ```\n1```\n to ```\n1e-8```\n in descending order and checking that ```\natan_poly```\n produces a faithful rounding of ```\natan((double)x)```\n.  If some ```\nx```\n failed, it printed out that ```\nx```\n and why it failed.\n\n\nTo get coefficients, I hacked this first program to fix ```\nc3```\n, work out bounds on ```\nr7```\n for each test point, then get bounds on the higher-order coefficients.  Then I hacked it to fix ```\nc3```\n and ```\nc5```\n and get bounds on the higher-order coefficients.  I did this until I had all but the three highest-order coefficients, ```\nc13```\n, ```\nc15```\n, and ```\nc17```\n.\n\nI grew the set of test points in the second program until it either stopped printing anything out or printed out \"success\".  I needed surprisingly few test points to reject almost all wrong polynomials---I count 85 test points in the program.\n\n\n\nHere I show some of my work selecting the coefficients.  In order to get a faithfully-rounded ```\narctan```\n for my initial set of test points assuming ```\nr1```\n through ```\nr8```\n are evaluated in real arithmetic (and rounded somehow unpleasantly but in a way I can't remember) but ```\nr9```\n and ```\nr10```\n are evaluated in ```\nfloat```\n arithmetic, I need:\n\n```\n-0x1.b81b456625f15p-3 <= c3 <= -0x1.b81b416e22329p-3\n-0x1.e71d48d9c2ca4p-4 <= c5 <= -0x1.e71783472f5d1p-4\n0x1.80e063cb210f9p-5 <= c7 <= 0x1.80ed6efa0a369p-5\n0x1.1a3925ea0c5a9p-5 <= c9 <= 0x1.1b3783f148ed8p-5\n-0x1.ec6032f293143p-7 <= c11 <= -0x1.e928025d508p-7\n-0x1.8c06e851e2255p-7 <= c13 <= -0x1.732b2d4677028p-7\n0x1.2aff33d629371p-8 <= c15 <= 0x1.41e9bc01ae472p-8\n0x1.1e22f3192fd1dp-9 <= c17 <= 0x1.d851520a087c2p-9\n```\n\n\nTaking c3 = -0x1.b81b44p-3, assuming ```\nr8```\n is also evaluated in ```\nfloat```\n arithmetic:\n\n```\n-0x1.e71df05b5ad56p-4 <= c5 <= -0x1.e7175823ce2a4p-4\n0x1.80df529dd8b18p-5 <= c7 <= 0x1.80f00e8da7f58p-5\n0x1.1a283503e1a97p-5 <= c9 <= 0x1.1b5ca5beeeefep-5\n-0x1.ed2c7cd87f889p-7 <= c11 <= -0x1.e8c17789776cdp-7\n-0x1.90759e6defc62p-7 <= c13 <= -0x1.7045e66924732p-7\n0x1.27eb51edf324p-8 <= c15 <= 0x1.47cda0bb1f365p-8\n0x1.f6c6b51c50b54p-10 <= c17 <= 0x1.003a00ace9a79p-8\n```\n\n\nTaking c5 = -0x1.e71aa4p-4, assuming ```\nr7```\n is done in ```\nfloat```\n arithmetic:\n\n```\n0x1.80e3dcc972cb3p-5 <= c7 <= 0x1.80ed1cf56977fp-5\n0x1.1aa005ff6a6f4p-5 <= c9 <= 0x1.1afce9904742p-5\n-0x1.ec7cf2464a893p-7 <= c11 <= -0x1.e9d6f7039db61p-7\n-0x1.8a2304daefa26p-7 <= c13 <= -0x1.7a2456ddec8b2p-7\n0x1.2e7b48f595544p-8 <= c15 <= 0x1.44437896b7049p-8\n0x1.396f76c06de2ep-9 <= c17 <= 0x1.e3bedf4ed606dp-9\n```\n\n\nTaking c7 = 0x1.80e87cp-5, assuming ```\nr6```\n is done in ```\nfloat```\n arithmetic:\n\n```\n0x1.1aa86d25bb64fp-5 <= c9 <= 0x1.1aca48cd5caabp-5\n-0x1.eb6311f6c29dcp-7 <= c11 <= -0x1.eaedb032dfc0cp-7\n-0x1.81438f115cbbp-7 <= c13 <= -0x1.7c9a106629f06p-7\n0x1.36d433f81a012p-8 <= c15 <= 0x1.3babb57bb55bap-8\n0x1.5cb14e1d4247dp-9 <= c17 <= 0x1.84f1151303aedp-9\n```\n\n\nTaking c9 = 0x1.1ab95ap-5, assuming ```\nr5```\n is done in ```\nfloat```\n arithmetic:\n\n```\n-0x1.eb51a3b03781dp-7 <= c11 <= -0x1.eb21431536e0dp-7\n-0x1.7fcd84700f7cfp-7 <= c13 <= -0x1.7ee38ee4beb65p-7\n0x1.390fa00abaaabp-8 <= c15 <= 0x1.3b100a7f5d3cep-8\n0x1.6ff147e1fdeb4p-9 <= c17 <= 0x1.7ebfed3ab5f9bp-9\n```\n\n\nI picked a point close to the middle of the range for ```\nc11```\n and randomly chose ```\nc13```\n, ```\nc15```\n, and ```\nc17```\n.\n\n\n\nEDIT:  I've now automated this procedure.  The following function is also a faithfully-rounded implementation of ```\narctan```\n on ```\n[0, 1]```\n:\n\n```\nfloat c5 = 0x1.997a72p-3;\nfloat c7 = -0x1.23176cp-3;\nfloat c9 = 0x1.b523c8p-4;\nfloat c11 = -0x1.358ff8p-4;\nfloat c13 = 0x1.61c5c2p-5;\nfloat c15 = -0x1.0b16e2p-6;\nfloat c17 = 0x1.7b422p-9;\n\nfloat juffa_poly (float a) {\n  float s = a * a;\n  float r1 =              c17;\n  float r2 = fmaf (r1, s, c15);\n  float r3 = fmaf (r2, s, c13);\n  float r4 = fmaf (r3, s, c11);\n  float r5 = fmaf (r4, s, c9);\n  float r6 = fmaf (r5, s, c7);\n  float r7 = fmaf (r6, s, c5);\n  float r8 = fmaf (r7, s, -0x1.5554dap-2f);\n  float r9 = r8 * s;\n  float r10 = fmaf (r9, a, a);\n  return r10;\n}\n```\n\n\nI find it surprising that this code even exists.  For coefficients near these, you can get a bound on the distance between ```\nr10```\n and the value of the polynomial evaluated in real arithmetic on the order of a few ulps thanks to the slow convergence of this polynomial when ```\ns```\n is near ```\n1```\n.  I had expected roundoff error to behave in a way that was fundamentally \"untamable\" simply by means of tweaking coefficients.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximate function with genetic algorithm\r\n                \r\nAre there modules in python to approximate a given function(a) with a genetic algorithm to receive a function(b) which produces the same or similar outputs with the same inputs? Why approximate? The workings of function(a) are not known. So basically what the algorithm should do is minimizing the deviation from sample values produced by function(a) and mutating function(b). Any ideas?\n\nExample:\n\n```\n1.Iteration:\nf(a):  0 -> 5, 1 -> 3, 2 -> 7\nf(bi): 0 -> 4, 1 -> 6, 2 -> 3\ndevi:       1       3       4\nsum(devi):  8\n...\nf(bn): 0 -> 3, 1 -> 2, 2 -> 1\ndevn:       2       1       4\nsum(devn):  7   ------------> 'fitter function - use for mutation'\n\nmutate f(b):\n\n2.Iteration:\nf(a):  0 -> 5, 1 -> 3, 2 -> 7, ...\nf(bi): 0 -> 5, 1 -> 6, 2 -> 3, ...\ndevi:       0       3       4\n...\n```\n\n    ", "Answer": "\r\nWhat you're lookin for is called Extrapolation .\n\nThe are alot of algorithms that do that, and they're just mathematical so they can be easily implemented.\n\nIf you want to create a function that will simulate the first one with in the same range of input, you can use Interpolation which is kinda the same, but with better accuracy because of the limited range.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm for finding approximate area of the ground via elevation grid?\r\n                \r\nI have a 200X200 grid/array (i.e. 40,000 values) of 10mX10m \"pixels\" whose values indicate the average elevation of the land at that point.\n\nAcross the grid there are vast connected regions where the elevation values are 0m because they represent actual sea.\n\nQuestion: is there a quick algorithm for obtaining the approximate area of land? I understand that I may multiply 200^2*10^2 to obtain a rough approximation of the area, but some of the values vary quite a bit.\n\nI think I know a rather expensive way, i.e. by summing all of the triangles whose vertices lie at the elevation. But is there a faster/easier way?\n    ", "Answer": "\r\nNumPy and SciPy are the tools for this kind of problem. Here's a synthetic 200×200 landscape with points on a 10 metre grid and height ranging up to 40 metres above sea level:\n\n```\n>>> import numpy as np\n>>> xaxis = yaxis = np.arange(0, 2000, 10)\n>>> x, y = np.meshgrid(xaxis, yaxis)\n>>> z = np.maximum(40 * np.sin(np.hypot(x, y) / 350), 0)\n```\n\n\nWe can have a look at this in Matplotlib:\n\n```\n>>> import matplotlib.pyplot as plt\n>>> import mpl_toolkits.mplot3d.axes3d as axes3d\n>>> _, axes = plt.subplots(subplot_kw=dict(projection='3d'))\n>>> axes.plot_surface(x, y, z, cmap=plt.get_cmap('winter'))\n>>> plt.show()\n```\n\n\n\n\nNow, the number of points on land (that is, with height greater than 0) is trivial to compute, and you can multiply this by the size of a grid square (100 m² in your question) to get an estimate of the land area:\n\n```\n>>> (z > 0).sum() * 100\n1396500\n```\n\n\nBut from the question, I understand that you want a more accurate estimate, one that takes into account the slope of the land. One way to do that is to make a mesh of triangles covering the land, and add up the area of the triangles.\n\nFirst, turn the coordinate arrays into an array of points (a point cloud):\n\n```\n>>> points = np.vstack((x, y, z)).reshape(3, -1).T\n>>> points\narray([[  0.000000e+00,   0.000000e+00,   0.000000e+00],\n       [  1.000000e+01,   0.000000e+00,   1.142702e+00],\n       [  2.000000e+01,   0.000000e+00,   2.284471e+00],\n       ..., \n       [  1.970000e+03,   1.990000e+03,   3.957136e+01],\n       [  1.980000e+03,   1.990000e+03,   3.944581e+01],\n       [  1.990000e+03,   1.990000e+03,   3.930390e+01]])\n```\n\n\nSecond, use ```\nscipy.spatial.Delaunay```\n to triangulate in two dimensions, getting a surface mesh:\n\n```\n>>> from scipy.spatial import Delaunay\n>>> tri = Delaunay(points[:,:2])\n>>> len(tri.simplices)\n79202\n>>> tri.simplices\narray([[39698, 39899, 39898],\n       [39899, 39698, 39699],\n       [39899, 39700, 39900],\n       ..., \n       [19236, 19235, 19035],\n       [19437, 19236, 19237],\n       [19436, 19236, 19437]], dtype=int32)\n```\n\n\nThe values for each triangle in the triangulation are the indexes in the ```\npoints```\n array of the three points in the triangle.\n\nThird, select the triangles that have some land in them:\n\n```\n>>> land = (points[tri.simplices][...,2] > 0).any(axis=1)\n>>> triangles = tri.simplices[land]\n>>> len(triangles)\n27858\n```\n\n\nFourth, compute the areas of these triangles:\n\n```\n>>> v0, v1, v2 = (points[triangles[:,i]] for i in range(3))\n>>> areas = np.linalg.norm(np.cross(v1 - v0, v2 - v0), axis=1) / 2\n>>> areas\narray([ 50.325028,  50.324343,  50.32315 , ...,  50.308673,  50.313157, 50.313649])\n```\n\n\nFinally, add them up:\n\n```\n>>> areas.sum()\n1397829.2847141961\n```\n\n\nThat's not very different to the original estimate, which is to be expected because the slopes are shallow.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Quickcheck for non-boolean tests\r\n                \r\nI am using QuickCheck to test my code for some numeric calculations.  Basically I have an exact function and several approximations of it that are much more efficient.\n\nI'm currently implementing the properties I want to test something like:\n\n```\nprop_blah input = (abs $ (exact input)-(approx input)) < threshold\n```\n\n\nBut it would be really nice to know exactly how accurate each of the approximation algorithms is and compare them with each other.  One easy way to do this would be to get reports of the mean and standard deviation of the left hand side of the inequality.  Is this somehow possible?\n    ", "Answer": "\r\nIf you only need it to be printed out, you should check QuickCheck Callbacks which are performed after a single test. Their definition is located in Test.QuickCheck.Property\n\nOtherwise you could use the Function ```\ncollect :: (Show a, Testable prop) => a -> prop -> Property```\n located in Test.QuickCheck.Property.\n\n```\nlet a = (abs $ (exact input)-(approx input))\nin collect a (a < threshold)\n```\n\n\nThis way you geld at least a string representation of the approximation and also get to know how many of the single tests give the same approximation.\n\nYou could even get rid of the approximation quality and just list the factors by doing:\n\n```\nprop = collect (abs $ (exact input)-(approx input)) True\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Is there a decent algorithm to find the approximate wasted space in a Packing scenario?\r\n                \r\nSay we have a rectangular space of length L and width W. We now have to pack as many regular polygons of area A inside of this rectangle. This is a pretty well known problem and it's been asked a bit before, however what is different here is that I do not care the location of the packing, all that matters would be the approximate used or wasted space. \n\nAlso important to note is that we cannot change the size of anything.\n\nI'm not sure if this has been done before, or if this could make the algorithm/approximation more efficient, any help would be appreciated!\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Brute force (exact) algorithm for Vertex Cover\r\n                \r\nCan someone provide a detailed algorithm to implement a brute force (exact) algorithm for vertex cover. Currently, I know how to find a vertex cover using a log(n)-Approximation Algorithm, but cannot figure out how to go about the brute force.\n```\nlog(n)-Approximation Algorithm\n\nSmartGreedyVertexCover(G)\nInput: A graph G.\nOutput: A set of vertices that form a (not necessarily optimal) vertex cover.\n1:C←∅\n2:while G has at least one edge do\n3:  v←vertex in G with maximum degree\n4:  G←G\\v (This also removes all edges adjacent to v)\n5:  C←C∪v\n6:return C\n```\n\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fastest approximate counting algorithm\r\n                \r\nWhats the fastest way to get an approximate count of number of rows of an input file or std out data stream. FYI, this is a probabilistic algorithm, I can't find many examples online. \n\nThe data could just be one or 2 columns coming from an awk script of csv file! Lets say i want an aprox groupby on one of the columns. I would use a database group by but the number of rows are over 6-7 billion. I would like the first approx result In under 3 to 4 seconds. Then run a bayes or something after decisions are made on the prior. Any ideas on a really rough initial group count? \n\nIf you can provide the algorithm example in python, or java that would be very helpful. \n    ", "Answer": "\r\n@Ben Allison's answer is a good way if you want to count the total lines. Since you mentioned the Bayes and the prior, I will add an answer in that direction to calculate the percentage of different groups. (see my comments on your question. I guess if you have an idea of the total and if you want to do a ```\ngroupby```\n, to estimate the percentage of different groups makes more sense).\n\nThe recursive Bayesian update:\n\nI will start by assuming you have only two groups (extensions can be made to make it work for multiple groups, see later explanations for that.), ```\ngroup1```\n and ```\ngroup2```\n.\n\nFor ```\nm```\n ```\ngroup1```\ns out of the first ```\nn```\n lines(rows) you processed, we denote the event as ```\nM(m,n)```\n. Obviously you will see ```\nn-m```\n ```\ngroup2```\ns because we assume they are the only two possible groups. So you know the conditional probability of the event ```\nM(m,n)```\n given the percentage of ```\ngroup1```\n (```\ns```\n), is given by the binomial distribution with ```\nn```\n trials. We are trying to estimate ```\ns```\n in a bayesian way.\n\nThe conjugate prior for binomial is beta distribution. So for simplicity, we choose ```\nBeta(1,1)```\n as the prior (of course, you can pick your own parameters here for ```\nalpha```\n and ```\nbeta```\n), which is a uniform distribution on (0,1).  Therefor, for this beta distribution, ```\nalpha=1```\n and ```\nbeta=1```\n. \n\nThe recursive update formulas for a binomial + beta prior are as below:\n\n```\nif group == 'group1':\n    alpha = alpha + 1\nelse:\n    beta = beta + 1\n```\n\n\nThe posterior of ```\ns```\n is actually also a beta distribution:\n\n```\n                s^(m+alpha-1) (1-s)^(n-m+beta-1)\np(s| M(m,n)) = ----------------------------------- = Beta (m+alpha, n-m+beta)\n                      B(m+alpha, n-m+beta)\n```\n\n\nwhere ```\nB```\n is the beta function. To report the estimate result, you can rely on ```\nBeta```\n distribution's mean and variance, where:\n\n```\nmean = alpha/(alpha+beta)\nvar = alpha*beta/((alpha+beta)**2 * (alpha+beta+1))\n```\n\n\nThe python code: ```\ngroupby.py```\n\n\nSo a few lines of python to process your data from ```\nstdin```\n and estimate the percentage of ```\ngroup1```\n would be something like below:\n\n```\nimport sys\n\nalpha = 1.\nbeta = 1.\n\nfor line in sys.stdin:\n    data = line.strip()\n    if data == 'group1':\n        alpha += 1.\n    elif data == 'group2':\n        beta += 1.\n    else:\n        continue\n\n    mean = alpha/(alpha+beta)\n    var = alpha*beta/((alpha+beta)**2 * (alpha+beta+1))\n    print 'mean = %.3f, var = %.3f' % (mean, var)\n```\n\n\nThe sample data\n\nI feed a few lines of data to the code:\n\n```\ngroup1\ngroup1\ngroup1\ngroup1\ngroup2\ngroup2\ngroup2\ngroup1\ngroup1\ngroup1\ngroup2\ngroup1\ngroup1\ngroup1\ngroup2  \n```\n\n\nThe approximate estimation result\n\nAnd here is what I get as results:\n\n```\nmean = 0.667, var = 0.056\nmean = 0.750, var = 0.037\nmean = 0.800, var = 0.027\nmean = 0.833, var = 0.020\nmean = 0.714, var = 0.026\nmean = 0.625, var = 0.026\nmean = 0.556, var = 0.025\nmean = 0.600, var = 0.022\nmean = 0.636, var = 0.019\nmean = 0.667, var = 0.017\nmean = 0.615, var = 0.017\nmean = 0.643, var = 0.015\nmean = 0.667, var = 0.014\nmean = 0.688, var = 0.013\nmean = 0.647, var = 0.013\n```\n\n\nThe result shows that group1 is estimated to have 64.7% percent up to the 15th row processed (based on our beta(1,1) prior). You might notice that the variance keeps shrinking because we have more and more observation points. \n\nMultiple groups\n\nNow if you have more than 2 groups, just change the underline distribution from binomial to multinomial, and then the corresponding conjugate prior would be Dirichlet. Everything else you just make similar changes. \n\nFurther notes\n\nYou said you would like the approximate estimate in 3-4 seconds. In this case, you just sample a portion of your data and feed the output to the above script, e.g., \n\n```\nhead -n100000 YOURDATA.txt | python groupby.py\n```\n\n\nThat's it. Hope it helps. \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "seeking approximate algorithm to find largest clear circle in an area\r\n                \r\nRelated: Is there a simple algorithm for calculating the maximum inscribed circle into a convex polygon?\n\nI'm writing a graphics program whose goals are artistic rather than mathematical. It composes a picture step by step, using geometric primitives such as line segments or arcs of small angle. As it goes, it looks for open areas to fill in with more detail; as the available open areas get smaller, the detail gets finer, so it's loosely fractal.\n\nAt a given step, in order to decide what to do next, we want to find out: where is the largest circular area that's still free of existing geometric primitives?\n\nSome constraints of the problem\n\n\nIt does not need to be exact. A close-enough answer is fine.\nImprecision should err on the conservative side: an almost-maximal circle is acceptable, but a circle that's not quite empty isn't acceptable.\nCPU efficiency is a priority, because it will be called often.\nThe program will run in a browser, so memory efficiency is a priority too.\nI'll have to set a limit on level of detail, constrained presumably by memory space.\nWe can keep track of the primitives already drawn in any way desired, e.g. a spatial index. Exactness of these is not required; e.g. storing bounding boxes instead of arcs would be OK. However the more precision we have, the better, because it will allow the program to draw to a higher level of detail. But, given that the number of primitives can increase exponentially with the level of detail, we'd like storage of past detail not to increase linearly with the number of primitives.\n\n\nTo summarize the order of priorities\n\n\nMemory efficiency\nCPU efficiency\nPrecision\n\n\nP.S.\n\nI framed this question in terms of circles, but if it's easier to find the largest clear golden rectangle (or golden ellipse), that would work too.\n\nP.P.S.\n\nThis image gives some idea of what I'm trying to achieve. Here is the start of a tendril-drawing program, in which decisions about where to sprout a tendril, and how big, are made without regard to remaining open space. But now we want to know, where is there room to draw a tendril next, and how big? And where after that?\n\n\n    ", "Answer": "\r\nOne very efficient way would be to recursively divide your area into rectangular sub-areas, splitting them when necessary to divide occupied areas from unoccupied areas. Then you would simply need to keep track of the largest unoccupied area at each time. See https://en.wikipedia.org/wiki/Quadtree - but you needn't split into squares.\n\nGiven any rectangle, you can draw a line inside it, so that at least one of the rectangles to either side of the line is a golden rectangle. Therefore you can recursively erect partitions within a rectangle so that all but one of the rectangles formed by the partitions are golden rectangles, and the add rectangle left over is vanishingly small. You could do this to create a quadtree-like structure, where almost all of the rectangles left over were golden rectangles.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Find the best approximation of a subset for a value\r\n                \r\nI would like to get an algorithm that gives me the best approximation for a value based on subset.\nHere is an example:\n```\nN = 45\n\nsubset = [25,10,65,9,8]\n\noutput: [25,10,9]\n```\n\nThe important point is that the algorithm must give the best approximation (regardless the number of the element in the final result). The result must provide the association that gives the exact value of the nearest (but can not exceed the initial value).\nDo you know an algorithm that could do that with the minimal time cost ?\nThanks a lot for you help.\n    ", "Answer": "\r\nYou cannot to do so in polynomial time (unless P=NP)\nFinding out if there is a subset with sum exactly N is clearly easier than finding the subset with sum closest to N, and this former problem is called subset-sum which is known to be NP-complete.\nHowever, pseudo-polynomial time is possible. In fact, your problem is exactly equal to the 0/1 knapsack optimization problem if we take the values in ```\nsubset```\n to be both the values in weights for the translation to knapsack. This 0/1 knapsack problem has a dynamic programming solution that runs in ```\nO(nW)```\n where ```\nn```\n is the number of items in ```\nsubset```\n and ```\nW```\n is the target, which is ```\nN```\n in your code.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximating Pi in java using Gauss-Legendre algorithm [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     This question does not appear to be about programming within the scope defined in the help center.\r\n                \r\n                    \r\n                        Closed 9 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nJust started picking up java and tried to write a simple piece of code to display an approximation of Pi based on the Gauss-Legendre algorithm; the result I get in the command line from the code below is -33.343229... which seems very odd to me. I am aware that my code is not complete as I do not have a restriction on the number of digits after the decimal point, which I was going to try and get from using BigDecimal. I don't quite understand what I am supposed to do with it though after reading the documentation! \n\nDoes anyone have any idea if there are any current mistakes I can fix and also how to implement a restriction on the number of digits after the decimal point? Thank you!\n\n```\nclass calculatePi {\npublic static void main(String[] args) {\n    calculatePi x = new calculatePi();\n    double y = x.approxPi(3);       //3 iterations\n        System.out.print(y);\n}\ndouble approxPi(int i) {\n    double a = 1;           //The initial conditions for the iteration\n    double b = 1 / Math.sqrt(2);\n    double t = 1/4;\n    double p = 1;\n    double a1 = 0;          //The internal n+1 terms for the iteration\n    double b1 = 0;\n    double t1 = 0;\n    double p1 = 0;\n    while (i > 0) {\n        a1 = (a + b) / 2;\n        b1 = Math.sqrt(a*b);\n        t1 = t - p*(a - a1)*(a - a1);\n        p1 = 2*p;\n        a = a1;\n        b = b1;\n        t = t1;\n        p = p1;\n        i = i - 1;\n    }\ndouble applepie = ((a + b)*(a + b))/(4*t);\nreturn applepie;\n}\n```\n\n\n}\n    ", "Answer": "\r\n```\ndouble t = 1/4;```\n does an integer division for ```\n1/4```\n, which results in 0. Try ```\n1/4.0```\n.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "What would P=NP tell us for a proof?\r\n                \r\n\nAn exercise I've been working on is \"Prove that there is no efficient 5/4-approximation toChromaticNumber unless P=NP.\n\nMy question is what would P=NP tell us? Let's say we \"Suppose P=NP\" in our proof. Does that mean that there is a 1-approximation algorithm? Or that the polynomial-time verifier is the same as the algorithm itself? A little confused here, any responses are appreciated!\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximate, incremental nearest-neighbour algorithm for moving bodies\r\n                \r\nBounty\n\nThis question raises several issues. The bounty will go to an answer which addresses them holistically.\n\n\n\nHere's a problem I've been playing with.\n\nNOTE I'm especially interested in solutions that are not based in Euclidian space.\n\nThere is a set of Actors which form a crowd of size K. The distance ```\nd(ActorA,ActorB)```\n is easily computable for any two actors (solutions should work for various definitions of 'distance') and we can find the set of N nearest neighbours for any given Actor using any of a number of established algorithms.\n\nThis neighbour set is correct at the first instant but the Actors are always moving and I want to maintain the evolving list of N nearest neighbours for each Actor. What I am interested in is approximate solutions which are more efficient than perfect solutions.\n\n\nSolutions should converge to correctness after errors have been introduced.\nIt is acceptable to sometimes perform a full recomputation if the errors become too large but detecting these errors should be cheap.\n\n\nSo far I have been using a friend-of-a-friend algorithm:\n\n```\nrecompute_nearest (Actor A)\n{\n    Actor f_o_f [N*N];\n    for each Actor n in A.neighbours\n        append n to f_o_f if n != A and n not in f_o_f\n\n    Distance distances [N*N];\n    for 0 <= i < f_o_f.size\n        distances [i] = distance (A, f_o_f [i])\n\n    sort (f_o_f, distances)\n    A .neighbours = first N from f_o_f\n}\n```\n\n\nThis performs reasonably well when the crowd is slow-moving and N is suitably large. It converges after small errors, satisfying the first criteria, but\n\n\nI don't have good way to detect large errors,\nI have no quantitative description of the size and frequency of errors,\nit converges in practice but I can't prove that it always will.\n\n\nCan you help with any of these points?\n\nAlso, do you know of any alternative approaches which perform well\n\n\nwhen the crowd is fast-moving,\nwhen some actors are fast-moving,\nwhen N is small,\nwhen the crowd is sparse in some places and dense in others,\nor with particular spacial-indexing algorithms?\n\n\nThe extension I'm working on at the moment is to generalise friend-of-a-friend to take the friend-of-a-friend-of-a-friend in cases when a neighbour is fast-moving. I suspect that this doesn't scale to well and it's hard to derive the right parameters without a quantification of the errors.\n\nI welcome all suggestions! It's a fun little problem :-)\n\n\n\nNotable Suggestions So Far\n\nFexvez: sample random extra neighbours, sample size depending on the speed of the Agent. Sampling from the area it's about to move into would probably also help.\n\nResample the neighbours when an agents ```\nspeed*delta_time```\n exceeds the distance to the furthest known neighbour.\n\nMaintain the Delaunay triangulation which is a superset of the nearest-neighbour graph. Only accounts for one nearest neighbour.\n\nDavid Mount's ANN library Doesn't seem to handle moving bodies.\n    ", "Answer": "\r\nA very simple and very fast method from statistics is to use random linear projections.  These can help you determine clusters and neighbors very quickly.  With more projections, you get more accuracy (addressing your question about errors, I believe).\n\nThis paper offers an extensive quantitative analysis of several methods, including a new method (DPES) that is related to RLP.\n\nThis paper addresses use of RLP including for distance preservation even in the context of moving points.\n\nThis paper addresses RLP for motion planning and details several heuristics.\n\nRLP methods are:\n\n\nVery fast\nLead to approximations that are tunable for accuracy and speed\nDistance and angle preserving (provable)\nEasily scaled to large dimensions and large #s of objects\nUseful for dimensionality reduction\nLead to compact projections (e.g. one could project into a hierarchical binary partitioning)\nFlexible: you can project into whatever space you feel would be good for you - usually R^d, but projecting into 2^d (i.e. binary space of dimension d) is also allowable, only subject to a reduction in accuracy for a given # of projections.\nStatistically interesting\n\n\nAfter embedding into a lower dimensional space, neighbor calculations are very easy, as projections that are, say, binned in the same regions (if you bin the projections into a grid) are very likely to be close in the original space.\n\nAlthough the dimensionality of the original data is small (even 10 is small), the ability to rapidly project into a pre-selected grid is very useful for identifying and counting neighbors.\n\nFinally, you only need to update for those objects whose location (or relative location, if you're centering and scaling the data) have changed.\n\nFor related works, look into the Johnson-Lindenstrauss lemma.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "OpenCV Multilevel B-Spline Approximation\r\n                \r\nHi (sorry for my english) .. i'm working in a project for University in this project i need to use the MBA (Multilevel B-Spline Approximation) algorithm to get some points (control points) of a image to use in other operations.\n\nI'm reading a lot of papers about this algorithm, and i think i understand, but i can't writing.\n\nThe idea is: Read a image, process a image (OpenCV), then get control points of the image, use this points.\n\nSo the problem here is:\nThe algorithm use a set of points {(x,y,z)} , this set of points are approximated with a surface generated with the control points obtained from MBA. the set of points {(x,y,z)} represents de data we need to approximate (the image)..\n\nSo, the image is in a cv::Mat format , how can transform this format to an ordinary array to simply access to the data an manipulate...\n\nHere are one paper with an explanation of the method:\n(Paper) REGULARIZED MULTILEVEL B-SPLINE REGISTRATION\n(Paper)Scattered Data Interpolation with Multilevel B-splines\n(Matlab)MBA\n\nIf someone can help, maybe a guideline, idea or anything will be appreciate ..\n\nThanks in advance.\n\nEDIT: Finally i wrote the algorithm in C++ using armadillo and OpenCV ... \n    ", "Answer": "\r\nWell i'm using armadillo a C++ linear algebra library to works with matrix for the algorithm\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximative sorting algorithm\r\n                \r\nI've developed a predictive model using a deep neural network.\nThis model aims at comparing 2 inputs :\n```\n- if input 1 is better than input 2 then model returns 1\n- if input 2 is better than input 1 then model returns 2\n```\n\nI trained the model on 100 inputs and now, I want to compare a new_input to all of these 100 inputs in order to know approximatively how good new_input is.\nMy goal is to be able to approximate the rank of the new_input. For example : I would like to say, after having trained and used my model, the new_input is approximatively the 12th best of all 101 inputs.\nMy problem is : when i'm using the model to compare the new_input to all of 100 inputs, some results are inconsistent.\nLet's look an example :\n```\n- Inputs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] (here, there is only 10 inputs, not 100 as before)\n- new_input = 8\n- Model returns : \n8 > 1 : OK\n8 > 2 : OK\n8 > 3 : OK\n8 > 4 : OK\n8 < 5 : NOT OK\n8 < 6 : NOT OK\n8 > 7 : OK\n8 < 8 : OK\n8 < 9 : OK\n8 > 10 : NOT OK\n```\n\nHow could i do to know how to rank the new_input (= 8) properly ?\nPS : of course, in my real case, inputs are not numbers, but matrixes :)\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Vastly different output C++ monte carlo approximation\r\n                \r\ndoing a C++ approximation of Pi using a random number generator, output works exactly as expected on my AMD 64 machine running Ubuntu, however on my school machine the second algorithm I've implemented is broken, and would love some insight as to why. Code is as follows:\n\n```\n#ifndef RANDOMNUMBER_H_\n#define RANDOMNUMBER_H_\n\nclass RandomNumber {\npublic:\nRandomNumber() {\n    x = time(NULL);\n    m = pow(2, 19); //some constant value\n    M = 65915 * 7915; //multiply of some simple numbers p and q\n    method = 1;\n}\nRandomNumber(int seed) {\n    x = ((seed > 0) ? seed : time(NULL));\n    m = pow(2, 19); //some constant value\n    method = 1; //method number\n    M = 6543 * 7915; //multiply of some simple numbers p and q\n}\nvoid setSeed(long int seed) {\n    x = seed; //set start value\n}\n\nvoid chooseMethod(int method) {\n    this->method = ((method > 0 && method <= 2) ? method : 1); //choose one of     two method\n}\n\nlong int linearCongruential() { //first generator, that uses linear congruential method\n    long int c = 0; // some constant\n    long int a = 69069; //some constant\n    x = (a * x + c) % m; //solution next value\n    return x;\n}\n\nlong int BBS() { //algorithm Blum - Blum - Shub\n    x = (long int) (pow(x, 2)) % M;\n    return x;\n}\ndouble nextPoint() { //return random number in range (-1;1)\n    double point;\n    if (method == 1) //use first method\n        point = linearCongruential() / double(m);\n    else\n        point = BBS() / double(M);\n    return point;\n}\nprivate:\nlong int x; //current value\nlong int m; // some range for first method\nlong int M; //some range for second method\nint method; //method number\n};\n\n#endif /* RANDOMNUMBER_H_ */\n```\n\n\nand test class:\n\n```\n#include <iostream>\n#include <stdlib.h>\n#include <math.h>\n#include <iomanip>\n#include \"RandomNumber.h\"\nusing namespace std;\n\nint main(int argc, char* argv[]) {\ncout.setf(ios::fixed);\ncout.precision(6);\nRandomNumber random;\nrandom.setSeed(argc);\nsrand((unsigned) time(NULL));\ncout << \"---------------------------------\" << endl;\ncout << \"   Monte Carlo Pi Approximation\" << endl;\ncout << \"---------------------------------\" << endl;\ncout << \" Enter number of points: \";\nlong int k1;\ncin >> k1;\ncout << \"Select generator number: \";\nint method;\ncin >> method;\nrandom.chooseMethod(method);\ncout << \"---------------------------------\" << endl;\nlong int k2 = 0;\ndouble sumX = 0;\ndouble sumY = 0;\nfor (long int i = 0; i < k1; i++) {\n    double x = pow(-1, int(random.nextPoint() * 10) % 2)\n            * random.nextPoint();\n    double y = pow(-1, int(random.nextPoint() * 10) % 2)\n            * random.nextPoint();\n    sumX += x;\n    sumY += y;\n    if ((pow(x, 2) + pow(y, 2)) <= 1)\n        k2++;\n\n}\ndouble pi = 4 * (double(k2) / k1);\ncout << \"M(X)  = \" << setw(10) << sumX / k1 << endl; //mathematical expectation of x\ncout << \"M(Y)  = \" << setw(10) << sumY / k1 << endl; //mathematical expectation of y\ncout << endl << \"Pi = \" << pi << endl << endl; //approximate Pi\n\nreturn 0;\n}\n```\n\n\nThe second method returns 4.000 consistently on my lab machine, yet returns a rather close approximation on my personal machine.\n    ", "Answer": "\r\nFor one thing, the BBS generator as you're using it will always return ```\n1```\n.\n\nSince your program takes no arguments, presumably its ```\nargc```\n will be ```\n1```\n. You pass ```\nargc```\n as the seed (why?), so the initial value of ```\nx```\n is ```\n1```\n. \n\n```\nBBS()```\n has the following logic:\n\n```\nx = (long int) (pow(x, 2)) % M;\n```\n\n\nClearly, ```\n1```\n squared modulo ```\nM```\n gives ```\n1```\n, so ```\nx```\n never changes.\n\nWhen you run the simulation with such a generator, your program will always output ```\n4```\n.\n\nP.S. Wikipedia has the following to say about the initial value ```\nx0```\n for Blum Blum Shub:\n\n\n  The seed ```\nx0```\n should be an integer that's co-prime to ```\nM```\n (i.e. ```\np```\n and ```\nq```\n are not factors of ```\nx0```\n) and not ```\n1```\n or ```\n0```\n.\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation (fitting) method for an unknown function\r\n                \r\nI'm trying to approximate unknown function, given x and f(x) values. The function itself represents computational complexity of an algortihm, so it can be polynomial, logarithmic, exponential etc. I'm not sure whether such approximation method exists and if it's doable. I mostly find information about polynomial approximation everywhere, but that's not what I'm looking for I guess. I would be really grateful for any tips about method or algorithm that you think may be suitable. Or any approach to the problem actually.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Affine transformation approximation of two triangles using Eigen SVD\r\n                \r\nIn the code below, I am trying to implement the algorithm for affine transformation approximation presented here\n\n```\n#include <Eigen/Eigen>\n#include <iostream>\n\nusing namespace Eigen;\nusing namespace std;\n\nint main(int argc, char **argv)\n{\n  Vector3f x1 (3.0f, 0.0f, 0.0f);\n  Vector3f x2 (0.0f, 2.0f, 0.0f);\n  Vector3f x3 (0.0f, 0.0f, 1.0f);\n\n  Vector3f translation(1.0f, -2.0f, 2.0f);\n\n  Vector3f x_bar1 = x1 + translation;\n  Vector3f x_bar2 = x2 + translation;\n  Vector3f x_bar3 = x3 + translation;\n\n  std::cerr << \"x_bar1 = \\n\" << x_bar1 << std::endl;\n  std::cerr << \"x_bar2 = \\n\" << x_bar2 << std::endl;\n  std::cerr << \"x_bar3 = \\n\" << x_bar3 << std::endl;\n\n  Vector3f c     = (x1+x2+x3)/3.0f;\n\n  Vector3f c_bar = (x_bar1+x_bar2+x_bar3)/3.0f;\n\n  Vector3f y1,y2,y3, y_bar1,y_bar2,y_bar3;\n  y1  = x1 - c;\n  y2  = x2 - c;\n  y3  = x3 - c;\n  y_bar1 = x_bar1 - c_bar;\n  y_bar2 = x_bar2 - c_bar;\n  y_bar3 = x_bar3 - c_bar;\n\n  Matrix3f H;\n  H =  y1*y_bar1.transpose()+y2*y_bar2.transpose()+y3*y_bar3.transpose();\n\n  JacobiSVD<Matrix3f> svd(H, ComputeFullU | ComputeFullV);\n  Matrix3f R; R = svd.matrixV()*svd.matrixU().transpose();\n  Vector3f t; t = c-R*c_bar;\n\n  std::cerr << \"R = \\n\" << R << std::endl;\n  std::cerr << \"t = \\n\" << t << std::endl;\n}\n```\n\n\nBut I get wrong answer:\n\n```\nR = \n 0.836735 -0.244898 -0.489796\n-0.244898  0.632653 -0.734694\n-0.489796 -0.734694 -0.469388\nt = \n0.142857\n3.71429\n1.42857\n```\n\n\nIs the problem in the implementation or in the algorithm? If so, what is the correction?\n    ", "Answer": "\r\nYou can check your computation using e.g. this SVD-demonstration.\nFor your example I get\n\n```\nH =\n6   -2  -1\n-2  2.666666667 -0.666666667\n-1  -0.666666667    0.666666667\n\nU =\n1   0   0\n0   0.957092026 -0.289784149\n0   -0.289784149    -0.957092026\n\nV =\n1   0   0\n0   0.957092026 -0.289784149\n0   -0.289784149    -0.957092026\n\nR =\n1 0 0\n0 1 0\n0 0 1\n```\n\n\nSo the algorithm is correct! Probably your H is wrong.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithms:Max k-cut in a graph?\r\n                \r\nI have to develop the distributed algorithm for channel allocation in Wireless mesh network which is similar to Max K-cut problem in a graph.\nI have searched a lot for an algorithm of Max k-cut problem in a graph using greedy approximation but everywhere I found the mathematical analysis only.\n\nWould please anyone explain how we approach this problem for implementation?\n\nHere is the link for brief description-https://cs.stackexchange.com/questions/54028/a-greedy-approximation-algorithm-for-max-k-cut\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Incremental price graph approximation\r\n                \r\nI need to display a crypto currency price graph based similar to what is done on CoinMarketCap: https://coinmarketcap.com/currencies/bitcoin/\n\nThere could be gigabytes of data for one currency pair over a long period of time, so sending all the data to the client is not an option.\nAfter doing some research I ended up using a Douglas-Peucker Line Approximation Algorithm: https://www.codeproject.com/Articles/18936/A-C-Implementation-of-Douglas-Peucker-Line-Appro It allows to reduce the amount of dots that is sent to the client but there's one problem: every time there's new data I have to go through all the data on the server and as I'd like to update data on the client in real time, it takes a lot of resources.\n\nSo I'm thinking about a some kind of progressive algorithm where, let's say, if I need to display data for the last month I can split data into 5 minute intervals, preprocess only the last interval and when it's completed, remove the first one. I'm debating either customising the Douglas-Peucker algorithm (but I'm not sure if it fits this scenario) or finding an algorithm designed for this purpose (any hint would be highly appreciated)\n    ", "Answer": "\r\nConstantly re-computing the entire reduction points when the new data arrives would change your graph continuously. The graph will lack consistency. The graph seen by one user would be different from the graph seen by another user and the graph would change when the user refreshes the page(this shouldn't happen!), and even in case of server/application shutdown, your data needs to be consistent to what it was before.\n\n\nThis is how I would approach:\n\n\nYour reduced points should be as they are. For suppose, you are getting data for every second and you have computed reduced points for a 5-minute interval graph, save those data points in a limiting queue. Now gather all seconds data for next 5-minutes and perform the reduction operation on these 600 data points and add the final reduced point to your limiting queue.\n\nI would make the Queue synchronous and the main thread would return the data points in the queue whenever there is an API call. And the worker thread would compute the reduction point on the 5-minute data once the data for the entire 5-minute interval is available.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Mesh approximation with fixed-size irregular shapes\r\n                \r\nSuppose I have a mesh that I'd like to approximate the surface of by placing a bunch of irregular objects. Each irregular shape is of fixed-size, but can be rotated in increments of 15 degrees (around the \"up\" axis in 3D). Their positions are integer co-ordinates. The interior of the mesh does not need to be covered.\nSee my (poor) attempt to approximate a 2D example in the diagram below.\nI've looked into shape approximation, voxelization with geometric set cover, and surface similarity metrics, but find it quite overwhelming and I haven't found quite what I'm looking for. It doesn't need to be a fast algorithm. The criteria are some mix of minimizing object (or polygon) count and \"looking good from a distance\".\nWhat algorithms do you think would be most fruitful here? What are your go-to tools for tackling and visualizing this kind of problem? e.g. PyMesh, etc.\n\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to make faster approximation of data by sinusoids\r\n                \r\nI have a function that generates a sine wave\n```\nmy.sin <- function(vec,a,f,p)  a*sin(f*vec+p)\n```\n\nvec = vector indices\na = amplitude\nf = frequency\np = phase\nI also have some data ```\nmy_var```\n that I want to approximate with several sinusoids\n```\nset.seed(22)\nmy_var <- rnorm(100)\nplot(my_var,t=\"l\")\n```\n\n\nThere is also a fitness function that calculates the approximation error of the sum of two sinusoids,but there can be any number of sinusoids\n```\nfit <- function(x,test=F){\n  vec <- 1:length(my_var)\n  s1 <- my.sin(vec = vec,a = x[1],f = x[2],p = x[3])\n  s2 <- my.sin(vec = vec,a = x[4],f = x[5],p = x[6])\n  res_sin <- s1+s2\n  err <-  sqrt(sum((res_sin - my_var) ^ 2)) \n  if(test) return(res_sin)\n  return(  err/-1   )\n}\n```\n\nNext, I use a genetic algorithm to find the best solution.\n```\nlibrary(GA)\nGA <- ga(\"real-valued\", \n         fit, \n         lower = rep(-5,6),\n         upper = rep( 5,6), \n         maxiter = 300, \n         popSize = 300)\n\nsol <- tail(GA@solution,1)[1,]\nga_sin <- fit(sol,test = T)\n\nlines(ga_sin,col=4,lwd=2)\n```\n\n\n```\nbest_sin_comb <- matrix(sol,ncol = 3,byrow = T)\ncolnames(best_sin_comb) <- c(\"amplitude\",\"frequency\",\"phase\")\nprint(best_sin_comb)\n```\n\nresult\n```\n      amplitude  frequency     phase\n[1,] -0.3435402  1.5458888 1.8904578\n[2,] -0.4326791 -0.4886035 0.5606401\n```\n\nMy question is: can the approximation be made more efficient in terms of time spent. Perhaps a different search algorithm or something else ..\nAlso I would like to keep compatibility with the function ```\nmy.sin```\n\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Increase approximation probability in approxPolyDP() in OpenCV\r\n                \r\nI am making a system for shape identification in OpenCV-Python.\n```\napproxPolyDP()```\n is a best way to find a shape. I am using it for find square, pentagon, etc.\n\nBut, sometimes ```\napproxPolyDP()```\n is unable to get the polygon exactly. Therefore ```\nBlur()```\n and ```\nThresholding()```\n are used. However it is also not enough.\n\nIf I get a filtered image like this: \n\n\n\nAfter ```\nThreshold()```\n and ```\nBlur()```\n:\n\n\n\nHow can I improve approximation probability?\n\nPlease give me some idea/system/algorithm for increased approximation probability.\n    ", "Answer": "\r\nApproximation probability of 'approxPolyDP()' can be increased by using epsilon parameter.\n\ncv2.approxPolyDP(curve, epsilon, closed[, approxCurve]) \n\nThink,You have like this(black only),\n\n\n\nif you increase epsilon 'approxPolyDP()' ignore most nearest carve point and change detected shape.\nWhile you increasing epsilon 'approxPolyDP()' ignore A point first(because,A  point is nearest to red line than B point to green line).Then you increasing more it will ignore B point.\n\nSo,You also done threshold,blure and do more presetting,you may have some shaking point in detected polygon and you may be unable to  get exact shape.epsilonis very useful like that situation.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Can you please give me time approximation(like n^2 , n*2 , nlogn) of this New Selection Sort Algorithm made by me [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Big O, how do you calculate/approximate it?\r\n                            \r\n                                (24 answers)\r\n                            \r\n                    \r\n                Closed 3 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nNew Modified Selection Sort Algorithm (It outperforms Insertion Sort in some cases) goes like standard Selection Sort Algorithm , but instead searching for only a minimum in an Array , it searches for minimum and maximum in one iteration. Then it swaps minimum to start of an Array and maximum to end of an Array. Increments ```\nstart_pointer```\n, decrements ```\nend_pointer```\n, and then goes iterative again. I think time complexity it needs to sort N size array is : ```\nN + (N-2) + (N-4) + (N-6) + ... + 1```\n. Can someone give me approximation of this formula. I will be appreciated. \n\n```\npublic static void DoubleSelectionSort(int[] Array, int startpos, int endpos) {\n\n    /*Double Selection Sort Algorithm , made by Milan Domonji 1986 , MilanDomonji@yahoo.com*/\n\n    while(startpos < endpos) {\n\n        int min = Integer.MAX_VALUE;\n        int max = Integer.MIN_VALUE;\n\n        int minpos = startpos;\n        int maxpos = endpos;\n\n        for(int i = startpos; i <= endpos; i++) {\n            //main loop that finds minimum and maximum from an Array\n\n            if (Array[i] <= min) {\n                min = Array[i];\n                minpos = i;\n            }\n\n            if (Array[i] >= max) {\n                max = Array[i];\n                maxpos = i;\n            }   \n\n        }\n        /* I added missing part of algorithm so it works now (Edited) */\n        if (maxpos==startpos && minpos==endpos) {\n\n            Swap(Array, minpos, maxpos);\n\n        }else {\n             if (maxpos==startpos) {\n\n                Swap(Array,minpos,startpos);\n                Swap(Array,minpos,endpos);              \n\n             }else {\n\n               Swap(Array,minpos,startpos);\n               Swap(Array,maxpos,endpos);\n\n          }\n        }\n\n        startpos++;\n        endpos--;\n  }\n}\n\nprivate static void Swap(int[] Array, int A, int B) {\n\n    int tmp = Array[A];\n    Array[A] = Array[B];\n    Array[B] = tmp;\n\n}\n```\n\n\nAlgorithm Sorts all the times correctly.\n    ", "Answer": "\r\nIf you need sum ```\nS = N + (N-2) + (N-4) + ... + (N - (N-2))```\n (For example ```\nN```\n is even), it is equal to ```\nS = 2 + 4 + ... + N = 2 ( 1 + 2 + 3 + ... + N/2) = 2 * N/2 * (N/2 + 1)/2 = N/2 * (N/2 +1) =  Theta(N^2)```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "singular value decomposition and low rank tensorial approximation\r\n                \r\naccording this article\n\nhttp://www.wseas.us/e-library/conferences/2012/Vouliagmeni/MMAS/MMAS-07.pdf\n\nmatrix can be approximated by one rank   matrices using  tensorial  approximation,i know that in matlab  kronecker product plays  same role as  tensorial  product,function is  kron,now let us suppose that we have following matrix\n\n```\na=[2 1 3;4 3 5]\n\na =\n\n     2     1     3\n     4     3     5\n```\n\n\nSVD of this matrix is\n\n```\n[U E V]=svd(a)\n\nU =\n\n   -0.4641   -0.8858\n   -0.8858    0.4641\n\n\nE =\n\n    7.9764         0         0\n         0    0.6142         0\n\n\nV =\n\n   -0.5606    0.1382   -0.8165\n   -0.3913    0.8247    0.4082\n   -0.7298   -0.5484    0.4082\n```\n\n\nplease help me  to  implement algorithm with using tensorial approximation reconstructs original matrix in matlab languages,how can i  apply  tensorial product?like this \n\n```\nX=kron(U(:,1),V(:,1));\n```\n\n\nor?thanks in advance\n    ", "Answer": "\r\nI'm not quite sure about the Tensorial interpretation but the closest rank-1 approximation to the matrix is essentially the outer-product of the two dominant singular vectors amplified by the singular value. \n\nIn simple words, if ```\n[U E V] = svd(X)```\n, then the closest rank-1 approximation to ```\nX```\n is the outer-product of the first singular vectors multiplied by the first singular value.\n\nIn MATLAB, you could do this as:\n\n```\nU(:,1)*E(1,1)*V(:,1)'\n```\n\n\nWhich yields:\n\n```\nans =\n\n    2.0752    1.4487    2.7017\n    3.9606    2.7649    5.1563\n```\n\n\n\n\nAlso, mathematically speaking, the kronecker product of a row vector and a column vector is essentially their outer product. So, you could do the same thing using Kronecker products as:\n\n```\n(kron(U(:,1)',V(:,1))*E(1,1))'\n```\n\n\nWhich yields the same answer.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "What is the name of a decision algorithm answering 'yes' or 'maybe'?\r\n                \r\nI am wondering what the name of a decision algorithm is that does not return 'yes' or 'no', but can only decide 'yes' for a real subset of inputs and can not provide a definitive decision for the rest.\n\nA suitable example would be an algorithm for deciding invertibility of a matrix - my algorithm correctly answers 'yes' for a subclass of matrices, but can neither confirm nor deny it for the rest.\n\nIn my mind, this is (kind of) a sound under-approximation of the real answer, but Wikipedia defines an approximation algorithm only within the realms of optimization.\n\nThank you for your input!\n    ", "Answer": "\r\nYou may be referring to randomized / probabilistic algorithms or randomized data structures.\n\nAlgorithms that probabilistically determine if a number is prime or not (a primality test) are examples of such randomized algorithms. The Miller-Rabin algorithm is a concrete example.\n\nData structures can be built to use probability for some operations. A bloom filter is one such probabilistic data structure.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Implementing the Alon-Matias-Szegedy Algorithm For The Second Moment Stream Approximation\r\n                \r\nI'm trying to recreate a function in python for an estimate of the second moment of a data stream. \n\nAs stated by the Ullman Book, \"Mining of Massive Datasets\", a second moment:\n\n\n  Is the sum of the squares of the m_i ’s. It is some-\n  times called the surprise number, since it measures how uneven the distribution of elements in the stream is.\n\n\nWhere the m_i elements are the univocal elements in a stream.\n\nFor example, having this toy problem\\data stream:\n\n```\na, b, c, b, d, a, c, d, a, b, d, c, a, a, b\n```\n\n\nWe calculate the second moment like this:\n\n```\n5^2 + 4^2 + 3^2 + 3^2 = 59\n```\n\n\n(because 'a' occurs 5 times in the data stream, 'b' 4 times, and so on)\n\nBecause we cannot store all the data stream in memory, we can use an algorithm for the estimate of the second moment: \n\nThe Alon-Matias-Szegedy Algorithm (AMS algorithm), that estimate the second moment using this formula:\n\n```\nE(n *(2 * X.value − 1))\n```\n\n\nIn which X is an univocal element of the stream, randomically selected, and X.value is a counter, that, as we read the stream, add to 1 each\ntime we encounter another occurrence of the x element from the time we selected it.\n\nn represents the length of the data stream, and \"E\" is the mean notation.\n\nDoing an example with the previous data stream, let's assume we selected \"a\" at the 13th position of the data stream, \"d\" at the 8th and \"c\" at the 3th. We haven't selected \"b\".\n\n```\na, b, c, b, d, a, c, d, a, b, d, c, a, a, b\n1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n      x              x              x\n```\n\n\nSelected like this, we have:\n\n```\nX.element = \"a\"   X.value = 2\nX.element = \"c\"   X.value = 3\nX.element = \"d\"   X.value = 2\n```\n\n\nThe estimate by the AMS algorithm is:\n\n```\n(15*(2 * 2 - 1) + 15*(2 * 3 - 1) + 15*(2 * 2 - 1))/3 = 55 \n```\n\n\nWhich is pretty near the true value of the second moment calculated before (59).\n\nNow focusing on my code, I have written this function for the calculation of the \"true\" Second Moment, simulating the data stream by vector(1d array) and a for:\n\n```\ndef secondMoment(vector):\n    mydict = dict()\n    for el in vector:\n        if el not in mydict:\n            mydict[el] = 1\n        else:\n            mydict[el] += 1\n    return (sum([pow(value, 2) for key, value in mydict.items()]))\n```\n\n\nand the AMS function which calculate an estimate of the second moment:\n\n```\ndef AMSestimate(vector):\n    lenvect = len(vector)\n    elements = dict()\n    for el in vector:\n        if el in elements:\n            elements[el] += 1\n        elif random.choice(range(0, 10)) == 0:\n            elements[el] = 1\n    # E(n * (2 * x.value - 1))\n    lendict = len(elements)\n    estimateM2 = 0\n    for key, value in elements.items():\n        estimateM2 += lenvect * ((2 * value) - 1)\n    print(lendict)\n    if lendict > 0:\n        return estimateM2/lendict\n```\n\n\nThe problem is that, when I try to calculate the esteem of a little toy problem (like the one above) the values are somewhat correct, but when I try to extend the vector to, say, 10000 elements, the values, true Second Moment and esteem, are pretty different.\n\nI think that the problem is tied at the manner in which I generate the data-stream, and at the manner in which I decide to select the X.element.\n\nThat is:\n\n```\n[random.choice(string.ascii_letters) for x in range(size)]\n```\n\n\nFor the generation of a random vector\\data stream\n\nAnd \n\n```\nelif random.choice(range(0, 10)) == 0:\n    elements[el] = 1\n```\n\n\nFor the X.element selection (done in the code above, in the AMS function)\n\nFor the generation of a random vector\\data stream, a thought that the problem may be due to the lack of \"variability\" of the vector (string.ascii_letters got only 52 elements).\n    ", "Answer": "\r\nThis is an interesting question.\n\nSay we start with\n\n```\nimport random\nimport string\n\nsize = 100000\nseq = [random.choice(string.ascii_letters) for x in range(size)]\n```\n\n\nThen the first implementation is similar to yours (note the use of ```\ncollections.Counter```\n, though):\n\n```\nfrom collections import Counter\n\ndef secondMoment(seq):\n    c = Counter(seq)\n    return sum(v**2 for v in c.values())\n\n>>> secondMoment(seq)\n192436972\n```\n\n\nThe second implementation differs more significantly than yours, though. Note that first the random indices are found. Then, an element is counted only after its first occurrence (if any) at one of the indices:\n\n```\nfrom collections import defaultdict\n\ndef AMSestimate(seq, num_samples=10):\n    inds = list(range(len(seq)))\n    random.shuffle(inds)\n    inds = sorted(inds[: num_samples])\n\n    d = {}\n    for i, c in enumerate(seq):\n        if i in inds and c not in d:\n            d[c] = 0\n        if c in d:\n            d[c] += 1\n    return int(len(seq) / float(len(d)) * sum((2 * v - 1) for v in d.values()))\n\n>>> AMSestimate(seq)\n171020000\n```\n\n\n\n\nEdit Regarding The Original Code In The Question\n\nIn the code in the question, consider your loop\n\n```\nfor el in vector:\n    if el in elements:\n        elements[el] += 1\n    elif random.choice(range(0, 10)) == 0:\n        elements[el] = 1\n```\n\n\n(Minor) The sampling is problematic: it is hard-coded probabilistic at 0.1\n\nAlso consider:\n\n```\n    estimateM2 += lenvect * ((2 * value) - 1)\n```\n\n\nThis lacks a division by the number of sampled elements.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Game Tree Algorithms & Progressive Deepening: How to approximate an answer without reaching the leaf nodes?\r\n                \r\nI just saw this MIT lecture on Game Trees and MinMax algorithms where  Alpha Beta pruning and Progressive Deepening was discussed.\nhttps://www.youtube.com/watch?v=STjW3eH0Cik \n\nSo If I understand correctly progressive deepening is when you try to approximate the answer at every level and try to go deep towards the leaf nodes depending on the time limit you have for your move. It's important to have some answer at any point of time. \nNow, at 36:22 Prof discusses the case when we don't have enough time and we went only till the (d-1) th level, where d is the depth of the tree. And then he also suggests we can have an temporary answer at every level as we go down as we should have some approximate  answer at any point of time. \n\nMy question is how can we have any answer without going to the leaf nodes because it's only at the leaf nodes we can conclude who can win the game. Think this for tic-tac-toe game. At (d-1)th level we don't have enough information to decide if this series of moves till this node at (d-1) will win me or lose me the game. At higher levels say at (d-3) it's even more  blur! Everything is possible as we go down. Isn't it? So, if an algorithm decides to compute till (d-1) th level then all those path options are equal! Nothing guarantees a win and nothing guarantees a lose at (d-1)th level because if I understand correctly wins and losses can be calculated only at the leaf nodes. This is so true especially in pure MinMax algorithm. \n\nSo how exactly are we going to have an 'approximate answer' at (d-1)th level or say (d-5)th level?\n    ", "Answer": "\r\nI will try to explain that well.\nContext and importants of progressive deepening\nI need you to know that in the real-world game, the time that you will use to decide is limited! ( because user experience and other issue on human-computer interaction or about the problem/design in your game). You have a game tree and to use difference algorithm to optimization that travel all tree. But there are three problems:\n\n\nYou have a time constraints!\nYou need to calculate the best solution in your current game tree and that's time of calculating depending the deep of  tree!\nyou need to decide if go down in the tree to have a more precise answer without violate the time constraints.\n\n\nThe Answer of all problem is Progressive deepening: in current level you calculate the answer and try to pass the next level in the tree; but if you have not time you ready have a answer in the previous level and to get it out as answer\nThe answer your question \nyou can imagine the current level in your tree is \"the final level\" (you are supposing) in the game tree, but you will get a the best solution if you go to the next level in the tree, then if you can go to the next level: go now! but you need to calculate the optimal answer in the current game tree because it's \"the final level\" in the game tree as insurance policy if you don't finish the calcutation of the best answer in the next level by time constraint.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Tight example for MST heuristic for metric TSP\r\n                \r\nI'm looking for the tight example for MST heuristic which is and 2-approximation algorithm for metric Travelling Salesman Problem.\n\nThis algorithm can be found easily on the internet but I cannot find the tight example. By tight example I mean example in which given algorithm returns exactly 2 times worse solution than optimal.\n\nThanks.\n    ", "Answer": "\r\nThe closest example that I can think of is like this.\nConsider a star with n nodes: 1 node the center and n - 1 nodes surrounding it. every node is connected to center with cost 1.\n\nNow put this star inside a cycle of length (n - 1) where each two nodes are connected by cost 2. Note that the nodes of cycle are the non-center nodes. Now, MST will give cost n - 1.\nAnd TSP approx. using MST will give (n - 2)*2 + 1\n\nSo, the ratio of approx is (2n - 3) / (n - 1) which tends to 2 (what you want) as n grows large.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximating an unknown value in Python\r\n                \r\nI need to approximate an unknown value, a bound that separates divergent values from those that converge.\n\nI'm trying to do so like this:\n\n```\n# dont worry about the value of i, its one of many bounds checks\nbounds = 1.0\nfor j in range(niters):\n    if does_converge(i, bound):\n        bound += bound / 2.0\n    else:\n        bound /= 2.0\n```\n\n\nI've been googling for better approximation algorithms, but they all seem to assume I know something about the function, but I don't. All I get is a black box that tells me whether or not a value diverges or not.\n\nAny ideas would be appreciated!\n\nedit: I can't say for certain, but I would be fine assuming the function is continuous, and the boundary of convergence is most likely between 0 and 1. \n    ", "Answer": "\r\nWith the given information, there is nothing better available than some form of binary-search. \n\nEdit: See edit/remark at the end of this answer for a better solution (although without a rigorous theoretic explanation)!\n\nThis can be implemented using scipy's minimize_scalar. It is important to use ```\nmethod: golden```\n!\n\n\n  Method Golden uses the golden section search technique. It uses analog of the bisection method to decrease the bracketed interval.\n\n\nThe problem is the absence of any real-valued answer. Only yes/no does not allow to form any kind of gradient-information or surrogate-model.\n\nI'm assuming:\n\n\nwe are looking for the smallest value at which the black-box returns 1\nthe black-box is deterministic\n\n\nIdea: build some wrapper-function, which has a minimum at the smallest value for which 1 is returned. \n\nAs x should be in [0,1], trying to minimize x, we can formulate the wrapper-function as: ```\nx + 1 - black_box(x)```\n. Every solution with answer 0 is >= every solution with answer = 1 (probably some safeguard needed at the bound; e.g. ```\nx + (1 - eps) - black_box(x)```\n with eps very small!; might need to be chosen with ```\nxtol```\n in mind).\n\nCode:\n\n```\nfrom scipy import optimize\n\nSECRET_VAL = 0.7\n\ndef black_box(x):\n    if x > SECRET_VAL:\n        return 1.\n    else:\n        return 0.\n\ndef wrapper(x):\n    return x + 1 - black_box(x)\n\nres = optimize.minimize_scalar(wrapper, bracket=(0,1), method='golden')\n\nprint(res)\n```\n\n\nOutput:\n\n```\n     fun: 0.7000000042155881\n    nfev: 44\n     nit: 39\n success: True\n       x: 0.7000000042155881\n```\n\n\nOr with ```\nsecret_val=0.04```\n:\n\n```\n     fun: 0.04000000033008555\n    nfev: 50\n     nit: 45\n success: True\n       x: 0.040000000330085564\n```\n\n\nOr if you know what kind of accuracy you need (original secret 0.7):\n\n```\nres = optimize.minimize_scalar(wrapper, bracket=(0,1), method='golden',\n                            options={'xtol': 1e-2})\n```\n\n\nOutput:\n\n```\n     fun: 0.7000733152965655\n    nfev: 16                 !!!!!\n     nit: 11\n success: True\n       x: 0.7000733152965655\n```\n\n\nRemark:\n\nIt might be better to write a customized binary-search based solution here (not 100% sure). But one needs to be careful then given the assumptions like missing unimodality.\n\nEdit:\nOkay... i finally managed to transform this minimization-problem to a root-finding problem, which can be solved more efficiently!\n\nWarning: \nIt's clear, that ```\nwrapper```\n is never returning the value of ```\n0.0```\n (no exact root to find)!\n\nBut the bisection-method is about a ```\nzero crossing within the new interval```\n wiki.\n\nSo here it finds two points ```\na, b```\n, where the signs of the function are changing and is interpreting this as a root (given some tolerance!).\n\nThis analysis is not as rigorous as the one compared to the former method (not much analysis given, but easier to do in the pure minimization-approach given scipy's documentation).\n\n```\ndef wrapper_bisect(x):\n    return 1 - 2*black_box(x)\n\nres = optimize.bisect(wrapper_bisect, 0, 1, xtol=1e-2, full_output=True)\nprint(res)\n```\n\n\nOutput:\n\n```\n(0.6953125,       converged: True\n           flag: 'converged'\n function_calls: 9\n     iterations: 7\n           root: 0.6953125)\n```\n\n\nGiven the assumptions above (and only these), this should be the theoretically optimal algorithm (we reduced the number of function-evaluations from 16 to 9; the optimization-objective is worse, but within the bounds)!\n\nOne last test:\n\n```\nsecret: 0.9813; xtol: 1e-4```\n:\n\nGolden:\n\n```\n    fun: 0.9813254238281632\n    nfev: 25\n     nit: 20\n success: True\n       x: 0.9813254238291631\n```\n\n\nBisection:\n\n```\n(0.98126220703125,       converged: True\n           flag: 'converged'\n function_calls: 16\n     iterations: 14\n           root: 0.98126220703125)\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximating a conical frustum with binary search algorithm\r\n                \r\nI'm trying to solve this problem.\n\nThe description states that some friends have a bottle of coke and they want to equally divide it in cups and I need to help the group to split the Coke bottle. \n\nTo solve this I need the height h such that, if each cup is filled with Coke up to the height h, then each person gets the same amount of coke. You may assume that no cup will needed to be filled more than its capacity.\n\nThe information given to me is the radius of the top and bottom of the cup with its height. \n\nInput description:\n\n\n  The first line of input contains an integer C that determines the number of test cases. Each test case starts with a line containing two integers, N and L (1 ≤ N ≤ 100, 1 ≤ L ≤ 10^8), the number of friends in the group and the amount of Coke in the bottle, in milliliters. The second line contains three integers b, B and H (1 ≤ b ≤ B ≤ 100, 1 ≤ H ≤ 100), the radius of the smaller and the larger base, and the height of the cups, in centimeters.\n\n\nThe output description :\n\n\n  For each test case, print the value of h such that all cups must be filled up to the height h cm. Print the answer with 2 decimal places.\n\n\nExample of Input:\n\n\n  2\n  1 200\n  5 6 8\n  2 350\n  3 3 16   \n\n\nExample of Output:\n\n\n  2.40\n  6.19   \n\n\nMy solution.\n\nWith the data given in the question it was necessary to find the radius of the top of the coke amount (like a frustum inside a frustum O.o where the data given correspond to the outside frustum)\n\nWith the radius of the coke amount the binary search algorithm can 'guess' the best h.\n\n```\n#include <stdio.h>\n#include <math.h>\n\n#define EPS 0.001\n\ntypedef long long lld;\n\nint main(){\n    lld c,n,l,b,B,H; //names are described in the input\n    scanf(\"%lld\",&c);\n    for (lld i = 0; i < c; ++i) {\n        scanf(\"%lld %lld %lld %lld %lld\",&n,&l,&b,&B,&H);\n\n        double v = l/n;//volume expected for each cup\n        double ini = 0,fim = H, mid = 0.0; //mid will be the 'h'\n        double v_approximate = 0;\n        while(fabs(v - v_approximate) > EPS){\n            mid = (ini + fim)/2.0;\n\n            double tmp = b + (B-b)*mid/H; //this is the radius of the coke top\n            v_approximate = (M_PI*mid/3.0)*(tmp*tmp + tmp*b + b*b);\n\n            if(v_approximate == v)\n                break;\n            else if(v_approximate > v)\n                fim = mid;\n            else\n                ini = mid;\n        }\n        printf(\"%.2lf\\n\",mid);\n    }\n}\n```\n\n\nThis code gives 10% of wrong answer. This is the first time that i tried to use approximation to solve a math question. What am i missing ?\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Modify this algorithm for Nearest Neighbour Search (NNS) to perform Approximate-NNS\r\n                \r\nFrom the slides of a course, I found these:\n\nGiven a set P in R^D, and a query point q, it's NN is point p_0 in P, where:\n\n```\ndist(p_0, q) <= dist(p, q), for every p in P.\n```\n\n\nSimilarly, with an approximation factor 1 > ε > 0, the ε-NN is p_0, such that:\n\n```\ndist(p_0, q) <= (1+ε) * dist(p, q), for every p in P.\n```\n\n\n(I wonder why ε can't reach 1).\n\nWe build a KD-tree and then we search for the NN, with this algorithm:\n\nwhich is correct, as far as my mind goes and my testing.\n\nHow should I modify the above algorithm, in order to perform Approximate Nearest Neighbour Search (ANNS)?\n\nMy thought is to multiply the current best (at the part of the update in the leaf) with ε and leave the rest of the algorithm as is. I am not sure however, if this is correct. Can someone explain?\n\nPS - I understand how search for NN works.\n\nNote that I asked in the Computer Science site, but I got nothing!\n    ", "Answer": "\r\nThe one modification needed is to replace ```\ncurrent best distance```\n with ```\ncurrent best distance/(1+ε)```\n. This prunes the nodes that cannot contain a point violating the new inequality.\n\nThe reason that this works is that (assuming that ```\ncut-coor(q)```\n is on the left side) the test\n\n```\ncut-coor(q) + current best distance > node's cut-value\n```\n\n\nis checking to see if the hyperplane separating ```\nleft-child```\n and ```\nright-child```\n is closer than ```\ncurrent best distance```\n, which is a necessary condition for a point in ```\nright-child```\n to be closer than that to the query point ```\nq```\n, as the line segment joining ```\nq```\n and a point in ```\nright-child```\n passes through that hyperplane. By replacing ```\nd(p_0, q) = current best distance```\n with ```\ncurrent best distance/(1+ε)```\n, we're checking to see if any point ```\np```\n on the right side could satisfy\n\n```\nd(p, q) < d(p_0, q)/(1+ε),\n```\n\n\nwhich is equivalent to\n\n```\n(1+ε) d(p, q) < d(p_0, q),\n```\n\n\nwhich is a witness to the violation of the approximate nearest neighbor guarantee.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "spline surface interpolation\r\n                \r\nLet's say I have n number of points defining a surface on the z-axis\n\n```\nf(x1,y1) = 10\nf(x2,y2) = 12\nf(x3,y3) = 5\nf(x4,y4) = 2\n...\nf(xn,yn) = 21\n```\n\n\nnow I want to be able to approximate f(x,y). I am looking for an algorithm for a linear and especially a spline approximation. An example algorithms or at least some pointers would be great.\n    ", "Answer": "\r\nThis is a vague description of an approach to make a linear approximation.\n\n\nDetermine the Voronoi diagram of your points (for every point in the plane, find the nearest ```\n(x_i,y_i)```\n)\nTake the dual of this to get the Delaunay triangulation: connect ```\n(x_i,y_i)```\n and ```\n(x_j,y_j)```\n if there is a line segment of points so that ```\n(x_i,y_i)```\n and ```\n(x_j,y_j)```\n are equidistant (and closer than any other pair).\nOn each triangle, find the plane through the three vertices.  This is the linear interpolation you need.\n\n\n\n\nThe following implements the first two steps in Python.  The regularity of your\ngrid may allow you to speed things up (it may also mess up the triangulation).\n\n```\nimport itertools\n\n\"\"\" Based on http://stackoverflow.com/a/1165943/2336725 \"\"\"\ndef is_ccw(tri):\n    return ( ( tri[1][0]-tri[0][0] )*( tri[1][1]+tri[0][1] )\n            + ( tri[2][0]-tri[1][0] )*( tri[2][1]+tri[1][1] )\n            + ( tri[0][0]-tri[2][0] )*( tri[0][1]+tri[2][1] ) ) < 0\n\ndef circumcircle_contains_point(triangle,point):\n    import numpy as np\n    matrix = np.array( [ [p[0],p[1],p[0]**2+p[1]**2,1] for p in triangle+point ] )\n    return is_ccw(triangle) == ( np.linalg.det(matrix) > 0 )\n\ntriangulation = set()\n\"\"\"\nA triangle is in the Delaunay triangulation if and only if its circumscribing\ncircle contains no other point.  This implementation is O(n^4).  Faster methods\nare at http://en.wikipedia.org/wiki/Delaunay_triangulation\n\"\"\"\nfor triangle in itertools.combinations(points,3):\n    triangle_empty = True\n    for point in points:\n        if point in triangle:\n            next\n        if circumcircle_contains_point(triangle,point):\n            triangle_empty = False\n            break\n    if triangle_empty:\n        triangulation.add(triangle)\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Efficient approximate algorithm to determine the presence of k-sized cycle in graph\r\n                \r\nI have a very large sparse graph ```\nG```\n (about 100 million nodes, about 50 million edges) and I would like to find an efficient algorithm (hopefully ```\nO(1)```\n or sub-linear in the number of nodes + edges) that predicts with some probability the presence of a cycle of length ```\nk```\n in this graph. For practical use, ```\nk```\n will very small (between 30 and 90) relative to the size of ```\nG```\n. It is also guaranteed that ```\nk```\n will always be even. ```\nG```\n is also a random graph, so I don't expect any consistent clustering. \n\nThe algorithm doesn't need to enumerate the actual nodes contained in the cycle, it just needs to eliminate ```\nG```\n if it most likely don't have any cycles of length ```\nk```\n. \n\nI found a close solution with the answer presented here, where the ```\ntrace```\n and ```\nrank```\n of ```\nL```\n (where ```\nL```\n is the Laplacian of ```\nG```\n) could be compared to determine whether ```\nG```\n had any cycles at all. However, I couldn't find a relatively efficient way to compute ```\nrank```\n for ```\nG```\n. Another problem was that it doesn't take ```\nk```\n into account, which might be able to make a more efficient approach.\n\nGetting connected components is a possibility, but it is linear in the number of nodes + edges, which is not optimal for a graph of this size.\n    ", "Answer": "\r\nIf it's an Erdos--Renyi random graph, then since having such a cycle is a monotone property of a graph, there's a zero-one law (https://www.ams.org/journals/proc/1996-124-10/S0002-9939-96-03732-X/S0002-9939-96-03732-X.pdf), which implies that you can make a reasonably good guess by setting the right threshold. (Which threshold? I don't know offhand, but probably you can extrapolate from smaller graphs.)\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Reinforcement learning algorithms for continuous states, discrete actions\r\n                \r\nI'm trying to find optimal policy in environment with continuous states (dim. = 20) and discrete actions (3 possible actions). And there is a specific moment: for optimal policy one action (call it \"action 0\") should be chosen much more frequently than other two (~100 times more often; this two action more risky).\n\nI've tried Q-learning with NN value-function approximation. Results were rather bad: NN learns always choose \"action 0\". I think that policy gradient methods (on NN weights) may help, but don't understand how to use them on discrete actions.\n\nCould you give some advise what to try? (maybe algorithms, papers to read).\nWhat are the state-of-the-art RL algorithms when state space is continuous and action space is discrete?\n\nThanks.\n    ", "Answer": "\r\nApplying Q-learning in continuous (states and/or actions) spaces is not a trivial task. This is especially true when trying to combine Q-learning with a global function approximator such as a NN (I understand that you refer to the common multilayer perceptron and the backpropagation algorithm). You can read more in the Rich Sutton's page. A better (or at least more easy) solution is to use local approximators such as for example Radial Basis Function networks (there is a good explanation of why in Section 4.1 of this paper). \n\nOn the other hand, the dimensionality of your state space maybe is too high to use local approximators. Thus, my recommendation is to use other algorithms instead of Q-learning. A very competitive algorithm for continuous states and discrete actions is Fitted Q Iteration, which usually is combined with tree methods to approximate the Q-function.\n\nFinally, a common practice when the number of actions is low, as in your case, it is to use an independent approximator for each action, i.e., instead of a unique approximator that takes as input the state-action pair and return a Q value, using three approximators, one per action, that take as input only the state. You can find an example of this in Example 3.1 of the book Reinforcement Learning and Dynamic Programming Using Function Approximators\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Is a grid-approximation with A*-Search of a continuous motion planning problem resolution-optimal?\r\n                \r\nGiven a continuous motion planning problem of finding a collision-free path from A to B, the A* search is known to be optimal on a finite sized grid-approximation, where each grid-cell for example has 4 or 8 neighbours.\nNow, the A* path might be optimal, but it certainly doesn't have to be exactly the same as the shortest path in continuous space, since the A* path is confined to the grid-cells and integer grid coordinates. Now, I would expect that if you increase the resolution to infinity, the resulting A* path should be exact and equal to the continuous problem. But looking at the literature, I can only find \"resolution-completeness\" for the cell-based approximation. For example in \"A Survey of Motion Planning Algorithms\nfrom the Perspective of Autonomous UAV Guidance\" by C. Goerzen, Z. Kong, B. Mettler the Rectanguloid Approximate Cell Decompositon is non-optimal, but resolution-complete. I really don't understand how it's not resolution-optimal, meaning optimal if you increase the resolution to infinity.\nMy question is whether I am actually interpreting it wrongly or if such a grid-approximation is really never exactly optimal even for infinite resolutions.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Scipy Expm for Sparse Matrices\r\n                \r\nSo I know I can pass a sparse matrix object into scipy.sparse.linaglg expm, and it returns a spare matrix object as the matrix exponential... but does the actual Pade approximation algorithm that is implemented change whether the variable passed is a  dense or sparse matrix?  I am working on a problem which requires me to calculate the matrix exponential of a very large spare matrix object and I require the Pade approximation for accuracy purposes.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximating cosine on [0,pi] using only single precision floating point\r\n                \r\ni'm currently working on an approximation of the cosine. Since the final target device is a self-developement working with 32 bit floating point ALU / LU and there is a specialized compiler for C, I am not able to use the c library math functions (cosf,...). I'm aiming to code various methods that differ in terms of accuracy and number of instructions / cycles.\nI've already tried a lot of different approximation algorithms, starting from fdlibm, taylor expansion, pade approximation, remez algorithm using maple and so on....\nBut as soon as I implement them using only float precision, there is a significant loss of precision. And be sure: I know that with double precision, a much higher precision is no problem at all...\nRight now, i have some approximations which are exact up to a few thousand ulp around pi/2 (the range where the largest errors occur), and i feel that i am limited by the single precision conversions.\nTo address the topic argument reduction: input is in radian. i assume that an argument reduction will cause even more precision loss due to divisions / multiplications.... since my overall input range is only 0..pi, i decided to reduce the argument to 0..pi/2.\nTherefore my question is: Does anybody know a single precision approximation to cosine function with high accuracy (and in the best case high efficiency)? Are there any algorithms that optimize approximations for single precision? Do you know whether the built-in cosf function computes the values with single oder double precision internally?\n~\n```\nfloat ua_cos_v2(float x)\n{\n    float output;\n    float myPi = 3.1415927410125732421875f;\n    if (x < 0) x = -x;\n    int quad = (int32_t)(x*0.63661977236f);//quad = x/(pi/2) = x*2/pi\n    if (x<1.58f && x> 1.57f) //exclude approximation around pi/2\n    {\n        output = -(x - 1.57079637050628662109375f) - 2.0e-12f*(x - 1.57079637050628662109375f)*(x - 1.57079637050628662109375f) + 0.16666667163372039794921875f*(x - 1.57079637050628662109375f)*(x - 1.57079637050628662109375f)*(x - 1.57079637050628662109375f) + 2.0e-13f*(x - 1.57079637050628662109375f)*(x - 1.57079637050628662109375f)*(x - 1.57079637050628662109375f)*(x - 1.57079637050628662109375f)+ 0.000198412701138295233249664306640625f*(x - 1.57079637050628662109375f)*(x - 1.57079637050628662109375f)*(x - 1.57079637050628662109375f)*(x - 1.57079637050628662109375f)*(x - 1.57079637050628662109375f)*(x - 1.57079637050628662109375f)*(x - 1.57079637050628662109375f);\n        output -= 4.37E-08f;\n    }\n    else {\n        float param_x;\n        int param_quad = -1;\n        switch (quad)\n        {\n        case 0:\n            param_x = x;\n            break;\n        case 1:\n            param_x = myPi - x;\n            param_quad = 1;\n            break;\n        case 2:\n            param_x = x - myPi;\n            break;\n        case 3:\n            param_x = 2 * myPi - x;\n            break;\n        }\n        float c1 = 1.0f,\n            c2 = -0.5f,\n            c3 = 0.0416666679084300994873046875f,\n            c4 = -0.001388888922519981861114501953125f,\n            c5 = 0.00002480158218531869351863861083984375f,\n            c6 = -2.75569362884198199026286602020263671875E-7f,\n            c7 = 2.08583283978214240050874650478363037109375E-9f,\n            c8 = -1.10807162057025010426514199934899806976318359375E-11f;\n        float _x2 = param_x * param_x;\n        output = c1 + _x2*(c2 + _x2*(c3 + _x2*(c4 + _x2*(c5 + _x2*(c6 + _x2*(c7 \n        + _x2* c8))))));\n        if (param_quad == 1 || param_quad == 0)\n            output = -output;\n    }\n    return output;\n}\n```\n\n~\nif I have forgotten any information, please do not hesitate to ask!\nThanks in advance\n    ", "Answer": "\r\nIt is certainly possible to compute cosine on [0, π] with any desired error bound >= 0.5 ulp using just native precision operations. However, the closer the target is to a correctly rounded function, the more up-front design work and computational work at runtime is required.\nTranscendental functions implementations typically consist of argument reduction, core approximation(s), final fixup to counteract the argument reduction. In cases where the argument reduction involves subtraction, catastrophic cancellation needs to be avoided by explicitly or implicitly using higher precision. Implicit techniques can be designed to rely just on native precision computation, for example by splitting a constant like π into an unevaluated sum such as ```\n1.57079637e+0f - 4.37113883e-8f```\n when using IEEE-754 ```\nbinary32```\n (single precision).\nAchieving high accuracy with native precision computation is a lot easier when the hardware provides a fused multiply-add (FMA) operation. OP did not specify whether their target platform provides this operation, so I will first show a very simple approach offering moderate accuracy (maximum error < 5 ulps) relying just on multiplies and adds. I am assuming hardware that adheres to the IEEE-754 standard, and assume that ```\nfloat```\n is mapped to IEEE-754 ```\nbinary32```\n format.\nThe following is based on an archived blog post by Colin Wallace titled \"Approximating sin(x) to 5 ULP with Chebyshev polynomials\". It proposes to approximate sine on [-π, π] by using a polynomial in x² of sin(x)/(x*(x²-π²)), then multiplying this by x*(x²-π²). A standard trick to compute a²-b² more accurately is to rewrite it as (a-b) * (a+b). Representing π as an unevaluated sum of two floating-point numbers pi_high and pi_low avoids catastrophic cancellation during subtraction, which turns the computation x²-π² into ```\n((x - pi_hi) - pi_lo) * ((x + pi_hi) + pi_lo)```\n.\nThe polynomial core approximation should ideally use a minimax approximation, which minimizes the maximum error. I have done so here. Various standard tools like Maple or Mathematics can be used for this, or one create one's own code based on the Remez algorithm.\nFor a cosine computation on [0, PI] we can make use of the fact that cos (t) = sin (π/2 - t). Substituting x = (π/2 - t) into x * (x - π/2) * (x + π/2) yields (π/2 - t) * (3π/2 - t) * (-π/2 - t). The constants can be split into high and low parts (or head and tail, to use another common idiom) as before.\n```\n/* Approximate cosine on [0, PI] with maximum error of 5.081154 ulp */\nfloat cosine (float x)\n{\n    const float half_pi_hi       =  1.57079637e+0f; //  0x1.921fb6p+0\n    const float half_pi_lo       = -4.37113883e-8f; // -0x1.777a5cp-25\n    const float three_half_pi_hi =  4.71238899e+0f; //  0x1.2d97c8p+2\n    const float three_half_pi_lo = -1.19248806e-8f; // -0x1.99bc5cp-27\n    float p, s, hpmx, thpmx, nhpmx;\n\n    /* cos(x) = sin (pi/2 - x) = sin (hpmx) */\n    hpmx = (half_pi_hi - x) + half_pi_lo;               // pi/2 - x\n    thpmx = (three_half_pi_hi - x) + three_half_pi_lo;  // 3*pi/2 - x\n    nhpmx = (-half_pi_hi - x) - half_pi_lo;             // -pi/2 - x\n\n    /* P(hpmx*hpmx) ~= sin (hpmx) / (hpmx * (hpmx * hpmx - pi * pi)) */\n    s = hpmx * hpmx;\n    p =         1.32823530e-10f;//  0x1.241500p-33\n    p = p * s - 2.33173445e-8f; // -0x1.9096c4p-26 \n    p = p * s + 2.52237896e-6f; //  0x1.528c48p-19\n    p = p * s - 1.73501656e-4f; // -0x1.6bdbfep-13\n    p = p * s + 6.62087509e-3f; //  0x1.b1e7dap-8\n    p = p * s - 1.01321183e-1f; // -0x1.9f02f6p-4\n    return hpmx * nhpmx * thpmx * p;\n}\n```\n\nBelow I am showing a classical approach which first reduces the argument into [-π/4, π/4] while recording the quadrant. The quadrant then tells us whether we need to compute a polynomial approximation to the sine or the cosine on this primary approximation interval, and whether we need to flip the sign of the final result. This code assumes that the target platform supports the FMA operation specified by IEEE-754, and that it is mapped via the standard C function ```\nfmaf()```\n for single precision.\nThe code is straightforward except for the float-to-int conversion with rounding mode to-nearest-or-even that is used to compute the quadrant, which is performed by the \"magic number addition\" method and combined with the multiplication of 2/π (equivalent to division by π/2). The maximum error is less than 1.5 ulps.\n```\n/* compute cosine on [0, PI] with maximum error of 1.429027 ulp */\nfloat my_cosf (float a)\n{\n    const float half_pi_hi =  1.57079637e+0f; //  0x1.921fb6p+0\n    const float half_pi_lo = -4.37113883e-8f; // -0x1.777a5cp-25\n    float c, j, r, s, sa, t;\n    int i;\n\n    /* subtract closest multiple of pi/2 giving reduced argument and quadrant */\n    j = fmaf (a, 6.36619747e-1f, 12582912.f) - 12582912.f; // 2/pi, 1.5 * 2**23\n    a = fmaf (j, -half_pi_hi, a);\n    a = fmaf (j, -half_pi_lo, a);\n\n    /* phase shift of pi/2 (one quadrant) for cosine */\n    i = (int)j;\n    i = i + 1;\n\n    sa = a * a;\n    /* Approximate cosine on [-PI/4,+PI/4] with maximum error of 0.87444 ulp */\n    c =               2.44677067e-5f;  //  0x1.9a8000p-16\n    c = fmaf (c, sa, -1.38877297e-3f); // -0x1.6c0efap-10\n    c = fmaf (c, sa,  4.16666567e-2f); //  0x1.555550p-5\n    c = fmaf (c, sa, -5.00000000e-1f); // -0x1.000000p-1\n    c = fmaf (c, sa,  1.00000000e+0f); //  1.00000000p+0\n    /* Approximate sine on [-PI/4,+PI/4] with maximum error of 0.64196 ulp */\n    s =               2.86567956e-6f;  //  0x1.80a000p-19\n    s = fmaf (s, sa, -1.98559923e-4f); // -0x1.a0690cp-13\n    s = fmaf (s, sa,  8.33338592e-3f); //  0x1.111182p-7\n    s = fmaf (s, sa, -1.66666672e-1f); // -0x1.555556p-3\n    t = a * sa;\n    s = fmaf (s, t, a);\n\n    /* select sine approximation or cosine approximation based on quadrant */\n    r = (i & 1) ? c : s;\n    /* adjust sign based on quadrant */\n    r = (i & 2) ? (0.0f - r) : r;\n\n    return r;\n}\n```\n\nAs it turns out, in this particular case the use of FMA provides only a tiny benefit in terms of accuracy. If I replace calls to ```\nfmaf(a,b,c)```\n with ```\n((a)*(b)+(c))```\n, the maximum error increases minimally to 1.451367 ulps, that is, it stays below 1.5 ulps.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation Ratios\r\n                \r\nI've got a question about how best to classify the performance of an approximate algorithm. I'm trying to find the 'correct' value of a graph problem instance whose cost function has an objective function and a penalty function. I've configured my method such that the optimum solution of the problem has the highest value, fulfilling the objective criterion adds to the cost function value, and the penalty subtracts from it. This means that some poor solutions have negative values.\nTypically the performance of an approximate algorithm is measured as:\nfound_cost_function_value / best_possible_solution   = r .\nI'm a little confused where to set my baseline of 0 however. Should I translate my objective values such  that they are all positive, and so add the | most_negative_solution_cost_function_value| to all my cost function values and the best_possible_solution? Or is it best practise to calculate r as found ?\nEDIT: This is an optimisation problem. Each instance can be assigned an objective value. In my setup I've got a bit of an issue in that I found it best for the execution of the algorithm to have the penalty terms much larger than my reward terms and so I have a distribution of solutions with a small head of positive terms and a large tail of grossly negative solutions which skew things if I force everyting to be between 0 <r <1.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "how to find all edge-disjoint equi-cost paths in an undirected graph between a pair of nodes?\r\n                \r\nGiven an undirected graph G = (V, E), the edges in G have non-negative weights.\n\n[1] how to find all the edge-disjoint and equal-cost paths between two nodes s and t ? \n\n[2] how to find all the edge-disjoint paths between two nodes s and t ?  \n\n[3] how to find all the vertex-disjoint and equal-cost paths between two nodes s and t ? \n\n[4] how to find all the vertex-disjoint paths between two nodes s and t ?  \n\nAny approximation algorithms instead ?\n    ", "Answer": "\r\nBuild a tree where each node is a representation of a path through your unidirectional graph.\n\nI know, a tree is a graph too, but in this answer I will use the following terms with this meanings:\n\n\nvertex: is a vertex in your unidirectional graph. It is NOT a node in my tree.\nedge: is an edge in your unidirectional graph. It is NOT part of my tree.\nnode: a vertex in my tree, not in your graph.\nroot: the sole node on top of my tree that has no parent.\nleaf: any node in my tree that has no children.\n\n\nI will not talk about edges in my tree, so there is no word for tree-edges. Every edge in this answer is part of your graph.\n\n\n\nAlgorithm to build the tree \n\nThe root of the tree is the representation of a path that contains only of the vertex s and contains no edge. Let its weight be 0.\n\nDo for every node in that tree:\n\nTake the end-vertex of the path represented by that node (I call it the actual node) and find all edges that lead away from that end-vertex.\nIf: there are no edges that lead away from this vertex, you reached a dead end. This path never will lead to vertex t. So mark this node as a leaf and give it an infinite weight.\nElse:  \n\nFor each of those edges:\nadd a child-node to the actual node. Let it be a copy of the actual node. Attach the edge to the end of path and then attach the edges end-vertex to the path too. (so each path follows the pattern vertex-edge-vertex-edge-vertex-...)\nNow traverse up in the tree, back to the root and check if any of the predecessors has an end-vertex that is identic with the just added end-vertex.\nIf you have a match, the newly generated node is the representation of a path that contains a loop. Mark this node as a leaf and give it an infinite weight.\nElse If there is no loop, just add the newly added edges weight to the nodes weight.\nNow test, if the end-vertex of the newly generated node is the vertex t.\nIf it really is, mark this node as a leaf, since the path represented by this node is a valid path from s to t.\n\n\n\nThis algorithm always comes to an end in finite time. At the end you have 3 types of leafs in your tree:\n\n\nnodes that represent dead ends with an infinite weight\nnodes that represent loops, also with an infinite weight\nnodes that represent a valid path from s to t, with its weight beeing the sum of all edges weights that are part of this path.\n\n\nEach path represented by a leaf has its individual sequence of edges (but might contain the same sequence of vertexes), so the leafs with finite weights represent the complete set of edge-disjoint pathes from s to t. This is the solution of exercise [2].\n\nNow do for all leafs with finite weight:\nWrite its weight and its path into a list. Sort the list by the weights. Now paths with identic weight are neighbours in the list, and you can find and extract all groups of equal-cost paths in this sorted list. This is the solution of exercise [1].\n\nNow, do for each entry in this list:\nadd to each path in this list the list of its vertexes. After you have done this, you have a table with this columns:\n\n\nweight\npath\n1st vertex (is always s)\n2nd vertex\n3rd vertex\n...\n\n\nSort this table lexigraphic by the vertexes and after all vertexes by the weight (sort by 1st vertex, 2nd vertex, 3rd vertex ,... ,weight)\nIf one row in this sorted table has the same sequence of vertexes as the row before, then delete this row.\n\nThis is the list of all vertex-disjoint paths between two nodes s and t, and so it is the solution of exercise [4].\n\nAnd in this list you find all equal-cost paths as neighbours, so you can easily extract all groups of vertex-disjoint and equal-cost paths between two nodes s and t from that list, so here you have the solution of exercise [3].\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation Ratios\r\n                \r\nI've got a question about how best to classify the performance of an approximate algorithm. I'm trying to find the 'correct' value of a graph problem instance whose cost function has an objective function and a penalty function. I've configured my method such that the optimum solution of the problem has the highest value, fulfilling the objective criterion adds to the cost function value, and the penalty subtracts from it. This means that some poor solutions have negative values.\nTypically the performance of an approximate algorithm is measured as:\nfound_cost_function_value / best_possible_solution   = r .\nI'm a little confused where to set my baseline of 0 however. Should I translate my objective values such  that they are all positive, and so add the | most_negative_solution_cost_function_value| to all my cost function values and the best_possible_solution? Or is it best practise to calculate r as found ?\nEDIT: This is an optimisation problem. Each instance can be assigned an objective value. In my setup I've got a bit of an issue in that I found it best for the execution of the algorithm to have the penalty terms much larger than my reward terms and so I have a distribution of solutions with a small head of positive terms and a large tail of grossly negative solutions which skew things if I force everyting to be between 0 <r <1.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Php (int) parse weird result explanation [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Is floating point math broken?\r\n                            \r\n                                (33 answers)\r\n                            \r\n                    \r\n                    \r\n                        int((0.1+0.7)*10) = 7 in several languages. How to prevent this?\r\n                            \r\n                                (7 answers)\r\n                            \r\n                    \r\n                Closed 4 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nWhy does this code:\n\n```\necho (int)((0.1+0.7)*10);\n```\n\n\nReturn 7?\n\nIs this because of some weird approximation algorithm of math operations in php causing 0.1+0.7 to be almost 0.8 and not exact 0.8?\n\nIt confuses me because any other decimal value returns correct answer like with 0.1+0.8 and so on and so on.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Floating point reciprocal approximation using softfloat library\r\n                \r\nI am using the softfloat library (http://www.jhauser.us/arithmetic/SoftFloat.html) to implement a single precision division algorithm. \nI am trying to understand the reciprocal approximation function implemented\nas part of the softfloat library. Please see below the code. Could anyone\nexplain how they came up with the LUT? It looks like a combination of LUT and NR approximations, but a detailed explanation would definitely help.\n\n```\n/*\n  Returns an approximation to the reciprocal of the number represented by `a',\n  where `a' is interpreted as an unsigned fixed-point number with one integer\n  bit and 31 fraction bits.  The `a' input must be \"normalized\", meaning that\n  its most-significant bit (bit 31) must be 1.  Thus, if A is the value of\n  the fixed-point interpretation of `a', then 1 <= A < 2.  The returned value\n  is interpreted as a pure unsigned fraction, having no integer bits and 32\n  fraction bits.  The approximation returned is never greater than the true\n  reciprocal 1/A, and it differs from the true reciprocal by at most 2.006 ulp \n  (units in the last place).\n*/\n\nuint32_t softfloat_approxRecip32_1( uint32_t a )\n{\n    int index;\n    uint16_t eps;\n    static const uint16_t k0s[] = {\n      0xFFC4, 0xF0BE, 0xE363, 0xD76F, 0xCCAD, 0xC2F0, 0xBA16, 0xB201,\n      0xAA97, 0xA3C6, 0x9D7A, 0x97A6, 0x923C, 0x8D32, 0x887E, 0x8417\n    };\n    static const uint16_t k1s[] = {\n      0xF0F1, 0xD62C, 0xBFA1, 0xAC77, 0x9C0A, 0x8DDB, 0x8185, 0x76BA,\n      0x6D3B, 0x64D4, 0x5D5C, 0x56B1, 0x50B6, 0x4B55, 0x4679, 0x4211\n    };\n\n    uint16_t r0;\n    uint32_t delta0;\n    uint_fast32_t r;\n    uint32_t sqrDelta0;\n\n    index = a>>27 & 0xF;\n    eps = (uint16_t) (a>>11);\n    r0 = k0s[index] - ((k1s[index] * (uint_fast32_t) eps)>>20);\n    delta0 = ~(uint_fast32_t) ((r0 * (uint_fast64_t) a)>>7);\n    r = ((uint_fast32_t) r0<<16) + ((r0 * (uint_fast64_t) delta0)>>24);\n    sqrDelta0 = ((uint_fast64_t) delta0 * delta0)>>32;\n    r += ((uint32_t) r * (uint_fast64_t) sqrDelta0)>>48;\n    return r;\n\n}\n```\n\n    ", "Answer": "\r\nThe initial approximation ```\nr0```\n is computed by piece-wise linear approximation, using sixteen intervals, from [1, 17/16) to [15/16, 2), selected by the four most significant fractional bits of the 1.31 fixed-point argument. The initial estimate is then refined using the generalized Newton iteration for the reciprocal rnew = rold + rold * (1 - a * rold) + rold * (1 - a * rold)2 + ... + rold * (1 - a * rold)k [see paper by Liddicoat and Flynn]. ```\ndelta0```\n is (1 - a * r0). The first three terms of the expansion are used: r = r0 + r0 * delta0 + r0 * delta02. This iteration has cubic convergence, tripling the number of correct bits in every iteration. In this implementation, worst case relative error in ```\nr0```\n is about 9.44e-4, while the worst case relative error in the final result ```\nr```\n is about 9.32e-10.\n\nThe scale factors in the fixed-point computation are chosen to maximize the accuracy of intermediate computations (by retaining as many bits as possible), and to have the fixed-point conveniently fall on a word boundary, as in the computation of ```\ndelta0```\n in which the 1 therefore can be omitted.\n\nThe code requires ```\ndelta0```\n to be a positive quantity, therefore ```\nr0```\n must always be an underestimation of the mathematical result 1/a. The linear approximation for each interval therefore cannot be minimax approximations. Instead the slope between the function values 1/a of the endpoints of each interval is computed, and the absolute value of that scaled by 216 is stored in ```\nk0s```\n, meaning the array elements are 0.16 fixed-point numbers. Starting at the function value for the midpoint of each interval, the slope is then applied to find the intercept for the left endpoint of each interval. This value is likewise scaled by 216 and stored in ```\nk1s```\n which therefore also holds 0.16 fixed-point numbers.\n\nBased on my analysis, it seems that rounding towards 0 is employed in the floating-point to fixed-point conversion for entries in ```\nk0s```\n while rounding towards positive infinity is employed in the floating-point to fixed-point conversion for entries in ```\nk1s```\n. The following program implements the algorithm outlined above and produces table entries identical to those used in the code in the question.\n\n```\n#include <stdlib.h>\n#include <stdio.h>\n\nint main (void)\n{\n    printf (\"interval  k0    k1\\n\");\n    for (int i = 0; i < 16; i++) {\n        double x0 = 1.0+i/16.0;       // left endpoint of interval\n        double x1 = 1.0+(i+1)/16.0;   // right endpoint of interval\n        double f0 = 1.0 / x0;\n        double f1 = 1.0 / x1;\n        double df = f0 - f1;\n        double sl = df * 16.0;        // slope across interval\n        double mp = (x0 + x1) / 2.0;  // midpoint of interval\n        double fm = 1.0 / mp;\n        double ic = fm + df / 2.0;    // intercept at start of interval\n\n        printf (\"%5d     %04x  %04x\\n\",\n                i, (int)(ic * 65536.0 - 0.9999), (int)(sl * 65536.0 + 0.9999));\n    }\n    return EXIT_SUCCESS;\n}\n```\n\n\nThe output of the above program should be as follows:\n\n```\ninterval  k0    k1\n    0     ffc4  f0f1\n    1     f0be  d62c\n    2     e363  bfa1\n    3     d76f  ac77\n    4     ccad  9c0a\n    5     c2f0  8ddb\n    6     ba16  8185\n    7     b201  76ba\n    8     aa97  6d3b\n    9     a3c6  64d4\n   10     9d7a  5d5c\n   11     97a6  56b1\n   12     923c  50b6\n   13     8d32  4b55\n   14     887e  4679\n   15     8417  4211\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Decimal to Irrational fraction approximation\r\n                \r\nI have implemented an algorithm for floating point decimal to rational fraction approximation (example: 0.333 -> 1/3) and now I wonder, is there a way to find an irrational number which satisfies the condition. For example, given the input 0.282842712474 I want the result to be sqrt(2)/5 and not 431827/1526739 which my algorithm produces. The only condition is that the first digits of the result (converted back to floating point) should be the digits of the input, the rest doesn't matter. Thanks in advance!\n    ", "Answer": "\r\nI came up with solution, that from given set of possible denominators and nominators finds best approximation of given number.\n\nFor example this set can contain all numbers that can be created by:\n1 <= radicand <= 100000\n1 <= root_index <= 20\n\nIf set has N elements, than this solution finds best approximation in O(N log N).  \n\nIn this solution X represents denominator and Y nominator. \n\n\nsort numbers from set  \nfor each number X from set:\nusing binary find smallest Y such that Y/X >= input_number\ncompare Y/X with currently best approximation of input_number  \n\n\nI couldn't resist and I implemented it:\n\n```\n#include <cstdio>\n#include <vector>\n#include <algorithm>\n#include <cmath>\nusing namespace std;\n\nstruct Number {\n  // number value\n  double value;\n\n  // number representation\n  int root_index;\n  int radicand;\n\n  Number(){}\n  Number(double value, int root_index, int radicand)\n    : value(value), root_index(root_index), radicand(radicand) {}\n\n  bool operator < (const Number& rhs) const {\n    // in case of equal numbers, i want smaller radicand first\n    if (fabs(value - rhs.value) < 1e-12) return radicand < rhs.radicand;\n    return value < rhs.value;\n  }\n\n  void print() const {\n    if (value - (int)value < 1e-12) printf(\"%.0f\", value);\n    else printf(\"sqrt_%d(%d)\",root_index, radicand); \n  }\n};\n\nstd::vector<Number> numbers;\ndouble best_result = 1e100;\nNumber best_numerator;\nNumber best_denominator;\n\ndouble input;\n\nvoid compare_approximpation(const Number& numerator, const Number& denominator) {\n   double value = numerator.value / denominator.value;\n\n   if (fabs(value - input) < fabs(best_result - input)) {\n      best_result = value;\n      best_numerator = numerator;\n      best_denominator = denominator;\n   }\n}\n\nint main() {\n\n  const int NUMBER_LIMIT = 100000;\n  const int ROOT_LIMIT = 20;\n\n  // only numbers created by this loops will be used\n  // as numerator and denominator\n  for(int i=1; i<=ROOT_LIMIT; i++) {\n     for(int j=1; j<=NUMBER_LIMIT; j++) {\n        double value = pow(j, 1.0 /i);\n        numbers.push_back(Number(value, i, j));\n     }\n  }\n\n  sort(numbers.begin(), numbers.end());\n\n  scanf(\"%lf\",&input); \n\n  int numerator_index = 0;\n\n  for(int denominator_index=0; denominator_index<numbers.size(); denominator_index++) {\n    // you were interested only in integral denominators\n    if (numbers[denominator_index].root_index == 1) {\n      // i use simple sweeping technique instead of binary search (its faster)\n      while(numerator_index < numbers.size() && numbers[numerator_index].root_index &&\n    numbers[numerator_index].value / numbers[denominator_index].value <= input) {\n      numerator_index++;\n      }\n\n      // comparing approximations\n      compare_approximpation(numbers[numerator_index], numbers[denominator_index]);\n      if (numerator_index > 0) {\n    compare_approximpation(numbers[numerator_index - 1], numbers[denominator_index]);\n      }\n    }\n  }\n\n  printf(\"Best approximation %.12lf = \", best_numerator.value / best_denominator.value);\n  best_numerator.print();\n  printf(\" / \");\n  best_denominator.print();\n  printf(\"\\n\");\n}\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm: Longest Approximate interval\r\n                \r\nI am trying to solve this question:\n\n\n  When Xellos was doing a practice course in university, he once had to\n  measure the intensity of an effect that slowly approached equilibrium.\n  A good way to determine the equilibrium intensity would be choosing a\n  sufficiently large number of consecutive data points that seems as\n  constant as possible and taking their average. Of course, with the\n  usual sizes of data, it's nothing challenging — but why not make a\n  similar programming contest problem while we're at it?\n  \n  You're given a sequence of n data points a1, ..., an. There aren't any\n  big jumps between consecutive data points — for each 1 ≤ i < n, it's\n  guaranteed that |ai + 1 - ai| ≤ 1.\n  \n  A range [l, r] of data points is said to be almost constant if the\n  difference between the largest and the smallest value in that range is\n  at most 1. Formally, let M be the maximum and m the minimum value of\n  ai for l ≤ i ≤ r; the range [l, r] is almost constant if M - m ≤ 1.\n  \n  Find the length of the longest almost constant range.\n  \n  Input The first line of the input contains a single integer n\n  (2 ≤ n ≤ 100 000) — the number of data points.\n  \n  The second line contains n integers a1, a2, ..., an\n  (1 ≤ ai ≤ 100 000).\n  \n  Output Print a single number — the maximum length of an almost\n  constant range of the given sequence.\n\n\nhttps://codeforces.com/problemset/problem/602/B\n\nI see a solution here but I don't understand the algorithm - specifically the body of the loop. I am familiar with C++ syntax and I understand what's happening. I just don't understand  why the algorithm works. \n\n```\n#include<cstdio>\n#include<algorithm>\nusing namespace std;\nint a[1000005];\nint main()\n{\nint n,ans = 2,x;\nscanf(\"%d\",&n);\nfor(int i = 1; i <= n; i++)\n{\n\n    scanf(\"%d\",&x);\n    a[x] = i;\n    if(a[x-1] > a[x+1])\n        ans = max(ans,i-max(a[x+1],a[x-2]));\n    else\n        ans = max(ans,i-max(a[x+2],a[x-1]));\n}\nprintf(\"%d\\n\",ans);\nreturn 0;\n}\n```\n\n\nCould someone explain it to me? \n    ", "Answer": "\r\nArray ```\na```\n stands for the last position of value ```\nx```\n.\n\nNow let's calculate the left bound for each position(with value ```\nx```\n), if ```\na[x - 1]```\n is more close to ```\na[x]```\n compare to ```\na[x + 1]```\n, it means the position that will break the rule of almost constant is at ```\na[x + 1]```\n(because there is a ```\nx - 1```\n in between) or ```\na[x - 2]```\n(because there is a ```\nx - 1```\n in between).\n\nVice versa.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "N dimensional space search algorithms\r\n                \r\nI have an input N dimensional space. I also have a function f which given some point in the space, would return how likely this point is a \"good\" point [some measure between [0, 1]]. I know that \"good\" points are often close together in the space. But there could be clusters of these good points spread across the entire search space. So there could be regions which are excellent in producing these good points. \n\nWhat are some good approximate algorithms / statistics / techniques I could apply to get as many of these points as possible, and also as extensive as possible (covering as many cluster as possible). \n\nThanks\n    ", "Answer": "\r\nTake a look at the topics of cluster analysis and statistical classification. \n\nThe point here is that there are many different algorithms and it really depends a lot on your application and the structure of your input data if a certain method is appropriate. \n\nYou might want to use a data mining tool for evaluating different algorithms on your specific data. I have used RapidMiner in the past to do so and learned a lot about what worked well for my application and what did not. \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm for approximate number finding\r\n                \r\nConsider the following game:\n\n\nJohn and Peter agree on a number n.\nJohn chooses a number x between 1 and n.\nPeter makes a series of guesses k between 1 and n. For each guess:\n\n\nIf x/2 ≤ k ≤ 2x, then Peter wins.\nOtherwise, John tells Peter whether x is less than k.\n\n\n\nPeter wants to win in the fewest number of guesses.\n\nThere are obvious solutions requiring worst-case O(log n) guesses, but a friend told me that there's a solution with better asymptotic behavior than that. Is my friend right?\n    ", "Answer": "\r\nYour friend is right. The possible values of x can be partitioned into the ranges {1,2,3,4}, {5,6,…,19,20}, {21,22,…,83,84}, etc., where each range has a single \"central\" element that covers the whole range; for example, if x is anywhere between 21 and 84, then k = 42 is a winning guess. There are O(log n) such ranges, and Peter can use binary search to find the right range in O(log log n) guesses.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximate Counting Algorithm with limited updates / writes\r\n                \r\nProblem\nI want to count the number of page hits that my site gets, without relying on an external service, e.g. countapi.xyz, and without invading user privacy by using Google Analytics.\nIdea\nCloudflare offers a Key Value service, which allows me up to:\n\n100,000 reads per day\n1,000 writes per day\n\nSo, I would like to approximate numbers up to 100,000 using only 1,000 writes. This sounds like the approximate/morris counting algorithm + here, and the more recently improved counting algorithm.\nQuestion\nIs there a variant of the Morris counter that optimizes not for size, but for # of updates? If not, how can I find the optimal parameters to approximate 100000 using a counter up to 1000?\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Looking for an algorithm/example implementation for curve made out of arcs with control points on that curve [duplicate]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                This question already has answers here:\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                    \r\n                        Circular approximation of polygon (or its part)\r\n                            \r\n                                (2 answers)\r\n                            \r\n                    \r\n                Closed 5 months ago.\r\n        \r\n\r\n\r\n    \r\n\r\nLooking for an algorithm and or example implementation for curve made out of arcs with control points on that curve. There are algorithms for approximation of bezier curves with arcs, but technically I don't want to do any approximation, but rather some simpler (and faster to compute) solution that will use arcs (center point, start and end angle as input params).\nResearched internet, did some experiments, but no satisfactionary results\n    ", "Answer": "\r\nIf you want to interpolate between two points and their tangents, you generally need to use 2 arcs.  The resulting curve is called a \"biarc\", and biarc interpolation is reasonably common.\nI recently made a program for calculating gear shapes (source, live), that uses biarc approximations.  The part that determines the arcs from endpoints and tangents is here.\nThis version only works when the two endpoints and their tangents form a triangle.  It puts the connection between the two arcs at the triangle incenter, which makes the tangent at the connection point parallel to the line between the two control points.\nYou can find a pretty extensive explanation of biarc interpolation at this site.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Set Cover - A Couple Different Versions\r\n                \r\nI've been working with a bipartite graph with two sets of vertices L and R, and an edge-set E. There are a couple different problems I've been attempting to solve:\n\n1) The simple set cover problem (i.e. find the smallest cardinality subset of vertices in L, such that the neighborhood of that subset contains all of R). As I understand, this problem is known as the hitting set problem and is equivalent to the set cover problem and some approximation algorithms exist. I wanted to know what approximation algorithm you would recommend, I've found a couple different ones online.\n\n2) The second problem I wanted to solve is similar to the above, but instead of describing all of R, I want to describe only T, a subset of R, without any other elements of R. In addition, the allowed operations include set unions and set difference and set intersections of the neighborhoods of elements of L. Thus, I want to find the minimum number of elements in L that must be included in such a description. Apologies if this is unclear, I can explain further and respond to any questions.\n\nI would really appreciate any help.\n    ", "Answer": "\r\nFor 1, I recommend integer linear programming. There are many solvers out there; I've personally used GLPK (free) and CPLEX (commercial).\n\nFor 2, intuitively, we want the Venn diagram induced by the chosen sets to have the property that no area contains both an element in T and an element not in T. (I'm assuming in addition to set complement that we are allowed the empty set in the formula.) We can reduce this problem to a simple set cover problem as follows. The new elements to be covered are the two element sets {x, y} for all x in T and y in R - T. Each old set S gives rise to one new set ```\n{{x, y} | x in S and y in R - S}```\n.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm for approximating the 2D Look-up table\r\n                \r\nWhat would be a good algorithm or analytical method to approximate a discrete 2D look-up-table (LUT) to a continuous function? That is, given ```\nz=LUT[x1][x2]```\n and we wanna come up with a function such that ```\nz=f(x1, x2)```\n. \n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Neural Network (FFW, BP) - function approximation\r\n                \r\nis it possible to train NN to approximate this function:\n \n\nIf I tun approximation for x^2 or sin or something simple, it works fine, but for this sort of function i got only constant valued line.\nMy NN has 2 inputs (x, f(x)), one hidden layer (10 neurons), 1 output (f(x))\nFor training I am using BP, activation functions sigmoid -> tanh\n\nMy goal is to get \"smooth\" function without noise, that catch function on image above.\n\nOr is there any other way with NN or genetic algorithm, how to approximate this ?\n    ", "Answer": "\r\nYou're gping to have major problems because the input (x, f(x)) is discontinuous (not exactly, but sort of).\n\nTherefore, your NN will have to literally memorize the x-f(x) mapping given the large discontinuities.\n\nOne approach is to use a four-layer NN which can address the discontinuities.\n\nBut really, you may simply want to look at other smoothening methods rather than NN for thos problem.\n\nYou have a periodic function so first of all, only use one period, or you will memorize and not generalize.   \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Need help fixing an algorithm that approximates pi\r\n                \r\nI'm trying to write the C code for an algorithm that approximates pi. It's supposed to get the volume of a cube and the volume of a sphere inside that cube (the sphere's radius is 1/2 of the cube's side). Then I am supposed to divide the cube's volume by the sphere's and multiply by 6 to get pi.\n\nIt's working but it's doing something weird in the part that is supposed to get the volumes. I figure it's something to do the with delta I chose for the approximations.\nWith a cube of side 4 instead of giving me a volume of 64 it's giving me 6400. With the sphere instead of 33 it's giving me 3334. something.\n\nCan someone figure it out? Here is the code (I commented the relevant parts):\n\n```\n#include <stdio.h>      \n\nint in_esfera(double x, double y, double z, double r_esfera){\n    double dist = (x-r_esfera)*(x-r_esfera) + (y-r_esfera)*(y-r_esfera) + (z-r_esfera)*(z-r_esfera);\n\n    return  dist <= (r_esfera)*(r_esfera) ? 1 : 0;   \n}   \n\ndouble get_pi(double l_cubo){   \n    double r_esfera = l_cubo/2;   \n    double total = 0;\n    double esfera = 0;    \n//this is delta, for the precision. If I set it to 1E anything less than -1 the program continues endlessly. Is this normal?\n    double delta = (1E-1);   \n\n    for(double x = 0; x < l_cubo; x+=delta){\n        printf(\"x => %f; delta => %.6f\\n\",x,delta);\n        for(double y = 0; y <l_cubo; y+=delta){\n            printf(\"y => %f; delta => %.6f\\n\",y,delta);\n            for(double z = 0; z < l_cubo; z+=delta){\n                printf(\"z => %f; delta => %.6f\\n\",z,delta);\n                total+=delta;\n                if(in_esfera(x,y,z,r_esfera))\n                    esfera+=delta;\n            }\n        }\n    }\n\n    //attempt at fixing this\n        //esfera/=delta;\n        //total/=delta;\n    //\n\n//This printf displays the volumes. Notice how the place of the point is off. If delta isn't a power of 10 the values are completely wrong.   \n    printf(\"v_sphere = %.8f; v_cube = %.8f\\n\",esfera,total);   \n\n    return (esfera)/(total)*6;\n}   \n\nvoid teste_pi(){        \n    double l_cubo = 4;    \n    double pi = get_pi(l_cubo);\n\n    printf(\"%.8f\\n\",pi);\n}   \n\nint main(){   \n    teste_pi();\n}\n```\n\n    ", "Answer": "\r\n\n```\ntotal+=delta;\nif(in_esfera(x,y,z,r_esfera))\n    esfera+=delta;\n```\n\n\n\n```\ntotal```\n and ```\nesfera```\n are three-dimensional volumes whereas ```\ndelta```\n is a one-dimensional length. If you were tracking units you'd have m3 on the left and m on the right. The units are incompatible.\n\nTo fix it, cube ```\ndelta```\n so that you're conceptually accumulating tiny cubes instead of tiny lines.\n\n```\ntotal+=delta*delta*delta;\nif(in_esfera(x,y,z,r_esfera))\n    esfera+=delta*delta*delta;\n```\n\n\nDoing that fixes the output, and also works for any value of ```\ndelta```\n:\n\n```\nv_sphere = 33.37400000; v_cube = 64.00000000\n3.12881250\n```\n\n\nNote that this algorithm \"works\" for arbitrary ```\ndelta```\n values, but it has severe accuracy issues. It's incredibly prone to rounding problems. It works best when ```\ndelta```\n is a power of two: ```\n1/64.0```\n is better than ```\n1/100.0```\n, for example:\n\n```\nv_sphere = 33.50365448; v_cube = 64.00000000\n3.14096761\n```\n\n\nAlso, if you want your program to run faster get rid of all those printouts! Or at least the ones in the inner loops...\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation for random coloring graph with 2 colors [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     This question does not appear to be about programming within the scope defined in the help center.\r\n                \r\n                    \r\n                        Closed 1 year ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nHaving trouble figuring out how to show this approximation, hoping someone could lend some advice. I'm quite new to approximation (especially with randomization) and having trouble figuring out how to narrow this down.\nThe problem:\nSuppose we have a graph ```\nG = (V,E)```\n, each edge with a weight ```\nw```\n.\nWe want to color the graph with 2 colors, ```\nred```\n and ```\nblue```\n. We want to maximize the edge weight from each vertex from ```\nred```\n to ```\nblue```\n.\nWe randomly mark each vertex with either ```\nred```\n or ```\nblue```\n with probably ```\n1/2```\n for each. The coloring is done independently of every vertex.\nI need to show that this color assignment randomization algorithm is a ```\n4-approximaton```\n. However, not entirely sure where to start. Anyone have any ideas?\n    ", "Answer": "\r\nEven the simplest greedy algorithm will produce better approximations than randomly assigning colors.\nLike this:\n```\nMark all nodes uncolored\nMark all edges unprocessed\nSort edges into decreasing weight\nLOOP until all edges processed\n   Select heaviest unprocessed edge\n   IF both nodes uncoloured\n        color nodes on edge opposite colors\n   IF one node uncolored\n        color node opposite color to its partner\n   mark edge processed\n   ENDLOOP\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Selecting k sub-posets\r\n                \r\nI ran into the following algorithmic problem while experimenting with classification algorithms. Elements are classified into a polyhierarchy, what I understand to be a poset with a single root. I have to solve the following problem, which looks a lot like the set cover problem.\n\nI uploaded my Latex-ed problem description here.\n\nDevising an approximation algorithm that satisfies 1 & 2 is quite easy, just start at the vertices of G and \"walk up\" or start at the root and \"walk down\". Say you start at the root, iteratively expand vertexes and then remove unnecessary vertices until you have at least k sub-lattices. The approximation bound depends on the number of children of a vertex, which is OK for my application.\n\nDoes anyone know if this problem has a proper name, or maybe the tree-version of the problem? I would be interested to find out if this problem is NP-hard, maybe someone has ideas for a good NP-hard problem to reduce or has a polynomial algorithm to solve the problem. If you have both collect your million dollar price. ;)\n    ", "Answer": "\r\nThe DAG version is hard by (drum roll) a reduction from set cover. Set k = 2 and do the obvious: condition (2) prevents us from taking the root. (Note that (3) doesn't actually imply (2) because of the lower bound k.)\n\nThe tree version is a special case of the series-parallel poset version, which can be solved exactly in polynomial time. Here's a recursive formula that gives a polynomial p(x) where the coefficient of xn is the number of covers of cardinality n.\n\nSingle vertex to be covered: p(x) = x.\n\nOther vertex: p(x) = 1 + x.\n\nParallel composition, where q and r are the polynomials for the two posets: q(x) r(x).\n\nSeries composition, where q is the polynomial for the top poset and r, for the bottom: If the top poset contains no vertices to be covered, then p(x) = (q(x) - 1) + r(x); otherwise, p(x) = q(x).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Steiner Tree for Directed Graphs\r\n                \r\nI was using ```\nsteiner_tree```\n approximation algorithm in NetworkX library. While feeding it a directed graph it showed me an error that\n```\nNetworkXNotImplemented: not implemented for directed type```\n.\nI also tried to convert the following graph to an undirected type, and feed the same. But I'm losing some information like edge directions.\nHow can I extract Steiner Tree on a Directed Graph?\n    ", "Answer": "\r\nYou cannot get a solution to the directed Steiner tree problem from the undirected one. I would suggest to write out your directed graph and solve it with specialized software for directed Steiner tree problems. I don't know any\npython package that is able to do that, the only software I know is:\nhttps://scipjack.zib.de/\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fast algorithm for approximate lookup on multiple keys\r\n                \r\nI have formulated a solution to a problem where I am storing parameters in a set of tables, and I want to be able to look up the parameters based on multiple criteria.\n\nFor example, if criteria 1 and criteria 2 can each be either A or B, then I'd have four potential parameters - one for each combination A&A, A&B, B&A and B&B. For these sort of criteria I could concatenate the fields or something similar and create a unique key to look up each value quickly.\n\nUnfortunately not all of my criteria are like this. Some of the criteria are numerical and I only care about whether or not a result sits above or below a boundary. That also wouldn't be a problem on its own - I could maybe use a binary search or something relatively quick to find the nearest key above or below my value.\n\nMy problem is I need to include a number of each in the same table. In other words, I could have three criteria - two with A/B entries, and one with less-than-x/greater-than-x type entries, where x is in no way fixed. So in this example I would have a table with 8 entries. I can't just do a binary search for the boundary because the closest boundary won't necessarily be applicable due to the other criteria. For example, if the first two criteria are A&B, then the closest boundary might be 100, but if the if first two criteria are A&A, the closest boundary might be 50. If I want to look up A, A, 101, then I want it to recognise that 50 is the closest boundary that applies - not 100.\n\nI have a procedure to do the lookup but it gets very slow as the tables get bigger - it basically goes through each criteria, checks if a match is still possible, and if so it looks at more criteria - if not, it moves on to check the next entry in the table. So in other words, my procedure requires cycling through the table entries one by one and checking for a match. I have tried to optimise that by ensuring the tables that are input to the procedure are as small as possible and by making sure it looks at the criteria that are least likely to match first (so that it checks each entry as quickly as possible) but it is still very slow. \n\nThe biggest tables are maybe 200 rows with about 10 criteria to check, but many are much smaller (maybe 10x5). The issue is that I need to call the procedure many times during my application, so algorithms with some initial overhead don't necessarily make things better. I do have some scope to change the format of the tables before runtime but I would like to keep away from that as much as possible (while recognising it may be the only way forward).\n\nI've done quite a bit of research but I haven't had any luck. Does anyone know of any algorithms that have been designed to tackle this kind of problem? I was really hoping that there would be some clever hash function or something that means I won't have to cycle through the tables, but from my limited knowledge something like that would struggle here. I feel confident that I understand the problem well enough to gradually optimise the solution I have at the moment, but I want to be sure I've not missed a much better solution.\n\nApologies for the very long and abstract description of the problem - hopefully it's clear what I'm trying to do. I'll amend my question if it's unclear.\n\nThanks for any help.\n    ", "Answer": "\r\nthis is basically what a query optimizer does in SQL land. There are fast, free, in memory databases for exactly this purpose. Checkout sqlite https://www.sqlite.org/inmemorydb.html.\n\nIt sounds like you are doing what is called a 'full table scan' for each query, which is like the last resort for a query optimizer.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Do randomized algorithms always give an approximate result? Why? Why not?\r\n                \r\nI was wondering if they give approximate results all the time, I am unable to understand.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Minimum area geometric cover for a set of line segments\r\n                \r\nThe problem I am trying to solve is:\n\nGiven a set of M points on a plane where circles can be centered and a set of N line segments which need to be covered by the circles, find the minimum area circle cover for the line segments. That is, find the radii of the circles and the centers (chosen from the M points) such that all the N line segments are covered and the total area of the circles is minimized.\n\nNote that a line segment is covered if no part of it is OUTSIDE a circle.\n\nAny pointers to papers or code or approximation algorithms would be great. \n    ", "Answer": "\r\nEdit: Just realized that the original approach (moved to the end) probably doesn't cover the case where a line segment is best covered by multiple circles very well. So I think it's better to iterate points instead of line segments, cutting the line segments down at the circle boundaries:\n\n\nFind the \"worst\" point, i.e. the point requiring the largest additional circle area for its best circle center option with the corresponding line segment at least partially in the circle. Construct / extend the corresponding circle.\nRemove fully covered line segments from the set and cut partially covered segments at the circle boundary. \nIterate until no more line segments remain.\n\n\nThe main idea is to keep doing what's necessary in any case. How are overlapping circles counted? Do the areas add up, or are they merged? When going back to step one in later iterations, some kind of cost heuristics will probably be able to improve the result...\n\nThe original suggestion was:\n\n\nFind the \"worst\" line segment, i.e. the line segment requiring the largest circle for any of the circle center options and construct the corresponding circle.\nRemove covered line segments from the set. \nIterate until no more line segments remain.\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "how to prove my approximation ratio?\r\n                \r\nsuppose we are given a weighted graph G and we want to cluster its nodes into k clusters such that sum of weights of all edges between clusters be maximum. we want an approximation algorithm of ratio (1-1/k).\n\nmy solution : cause this is a maximization problem,first of all we must find an upper bound for OPT solution. let k be n and G is a complete graph so the OPT will be sum of all edges and its the maximum value we can reach to. so OPT <= SUM(all edges). the approach is described as : \n\nif k = n  the solution is trivial if k < n we consider all n node as a disjoint set and find the minimum weighted edge and union disjoint sets of two nodes which are connected through this edge. we repeat this procedure until number of disjoint sets equals to k. \n\nat the end we have removed at least (n-k) low weighted edge (in this case our result equals to OPT) and at the worst case only (k-1) high weighted edge has been remained to bee added into the result.\nat this case result is Sum(k-1 high weighted edge). to prove that our approach has approximation ratio (1-1/k) we should show that (1-1/k)Sum(All) <= Sum(k-1 high weighted edge). I doubt this is correct or not.can any one help me to prove it?\n    ", "Answer": "\r\nLet V1, ... , Vk be an initial random clustering on V with the same sizes.\nNow, it is sufficient that for each u in Vi and v in Vj with \n\nW(u,Vi)+W(v,Vj) > W(u,Vj)+W(v,Vi)\n\nmove u to Vj and move v to Vi.\n\nNow, we have:\n\nhttps://i.stack.imgur.com/8zeTI.png\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Set cover approximation in R\r\n                \r\nI'm trying to solve or implement an approximation to the set cover problem in R. Given a data frame like this.\n\n```\n  sets n\n1   s1 1\n2   s1 2\n3   s1 3\n4   s2 2\n5   s2 4\n6   s3 3\n7   s3 4\n8   s4 4\n9   s4 5\n```\n\n\nThe unique number of elements in column ```\nn```\n are:\n\n```\nunique(d$n)\n[1] 1 2 3 4 5\n```\n\n\nI'd like to calculate the smaller number of sets (column ```\nsets```\n) that cover all the unique elements in n (universe). In this example two sets: s1 {1, 2, 3} and s4 {4, 5}. I've read about it in wikipedia and on the Internet, and I know a greedy algorithm could be applied to find an approximation. I've checked too this link in which they mention two packages to solve such problems, ```\nLPsolve```\n and ```\nRsymphony```\n, but I don't know even how to start. In my real life example, I have more than 40,000 sets and each set between 1,000 and 10,000 elements and a unviverse or unique elements of 80,000. \nAny help or guide about how to start or proceed will be very much appreciated.\n\ndata\n\n```\nd <- structure(list(sets = structure(c(1L, 1L, 1L, 2L, 2L, 3L, 3L, \n4L, 4L), .Label = c(\"s1\", \"s2\", \"s3\", \"s4\"), class = \"factor\"), \n    n = c(1, 2, 3, 2, 4, 3, 4, 4, 5)), .Names = c(\"sets\", \"n\"\n), row.names = c(NA, -9L), class = \"data.frame\")\n```\n\n    ", "Answer": "\r\nThe ```\nlpSolve```\n package is available on CRAN for linear programing problems. Using your link which had a response from the very reputable Hans Borchers, as well as a slightly more complex example (starting on pg 4/5) in http://math.mit.edu/~goemans/18434S06/setcover-tamara.pdf as templates to understand teh proper structure of the setup, and then following along with modifications to the first example in ```\n?lp```\n:\n\n```\nlibrary( lpSolve)\n?lp\n# In Details: \"Note that every variable is assumed to be >= 0!\"\n# go from your long-form rep of the sets to a wide form for a matrix representation\n( items.mat<- t(table(d$sets,d$n))  )  # could have reversed order of args to skip t()\n#---------\n> dimnames(items.mat) = list( items=1:5, sets=paste0(\"s\", 1:4) )\n> items.mat\n     sets\nitems s1 s2 s3 s4\n    1  1  0  0  0\n    2  1  1  0  0\n    3  1  0  1  0\n    4  0  1  1  1\n    5  0  0  0  1\n#---------\nf.obj <-  rep(1,4)  # starting values of objective parameters by column (to be solved)\nf.dir <- rep(\">=\",5) # the constraint \"directions\" by row\nf.rhs <- rep(1,5)    # the inequality values by row (require all items to be present)\n\nlp (\"min\", f.obj, items.mat, f.dir, f.rhs)$solution\n#[1] 1 0 0 1\n```\n\n\nSo sets ```\ns1```\n and ```\ns4```\n are a minimal cover. The \"column coefficients\" determine choice of \"sets\".\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Dynamic T-SQL approach for combinatorics/knapsack\r\n                \r\nI guess my question has to do with a variant of the knapsack problem, but I can't really come up with a solution for this:\n\nLet's say you are in a hardware store and need to buy 21 screws.\nThey only offer them in bags:\n\n\nBag X - 16 Screws - 1.56$ per screw - 25$ Total\nBag Y - 8 Screws - 2.25$ per screw - 18$ Total   \nBag Z - 4 Screws - 1.75$ per screw - 7$ Total   \n\n\nNow you have to figure out which Bags you should buy to get your 21 screws (or more!) for the lowest possible price.\n\nSo what I got is a table with all the bags and a variable to define the required amount. What I need as a result should be a table with the bagname and the required amount.\n\nUnfortunately sqlfiddle is down.. But at least here's the example data:\n\n```\ndeclare @bags table (id int, qty int, price decimal(19,4))\ninsert into @bags values\n (10, 16, 25.00)\n,(20, 8, 18.00)\n,(30, 4, 7.00)\n\ndeclare @ReqQty int = 21\n```\n\n\nI really appreciate your help! Hope we can get this solved, as I need to customize our companys ERP System with this important function.\n\nThank you in advance!\n\nEdit:\nI read the whole wikipedia article about knapsack and there it says:\nOverfill approximation algorithm\nIt may be possible to generate an approximation algorithm where we can slightly overflow the allowed weight limit. You would want to achieve at least as high a total value as the given bound B, but you're allowed to exceed the weight limit...\nCurrently the solution is unknown for this approximation algorithm.\n\nSo it seems I better use a greedy algorithm instead of inventig the wheel? ;)\n    ", "Answer": "\r\nHere is a possible solution. I will see if I can finish it tomorrow as it's almost 3 AM here now. The main logic is there. All that's left to be done is to trace back using the ```\nprev_w```\n values. Just jump back (starting from the ```\nbest_price```\n row) till you reach the ```\nw=0```\n row. The differences between the ```\nw```\ns of current and the previous row give you the size of the bag you have to buy at each step.\n\nIn your example, the solution route obviously is:\n\"w=24, w=8, w=4, w=0\" which translates \"to buy bags: 16, 4, 4.\".\nThese 3 bags cost $39.  \n\nThis solution assumes the person is not going to buy\nmore than 1000 screws (this is what @limit is there for).   \n\nScript draft:\n\n```\n-- use TEST;\n\ndeclare @limit decimal(19,4);\nset @limit = 1000;\n\ncreate table #bags\n(\n    id int primary key,\n    qty int,\n    price decimal(19,4),\n    unit_price decimal(19,4),\n    w int, -- weight\n    v decimal(19,4) -- value\n);\n\ninsert into #bags(id, qty, price) \nvalues\n (10, 16, 25.00)\n,(20, 8, 18.00)\n,(30, 4, 7.00);\n\ndeclare @ReqQty int;\nset @ReqQty = 21;\n\nupdate #bags set unit_price = price / ( 1.0 * qty );\n\nupdate #bags set w = qty;\nupdate #bags set v = -price;\n\nselect * From #bags;\n\ncreate table #m(w int primary key, m int, prev_w int);\ndeclare @w int;\nset @w = 0;\nwhile (@w<=@limit)\nbegin\n    insert into #m(w) values (@w);\n    set @w = @w + 1;\nend;\n\nupdate #m\nset m = 0;\n\nset @w = 1;\n\ndeclare @x decimal(19,4);\ndeclare @y decimal(19,4);\n\n    update m1\n    set\n    m1.m = 0 \n    from #m m1\n    where\n    m1.w = 0;\n\nwhile (@w<=@limit)\nbegin\n\n    select \n        @x = max(b.v + m2.m) \n    from\n    #m m1 \n    join #bags b on m1.w >= b.w and m1.w = @w\n    join #m m2 on m2.w = m1.w-b.w;\n\n    select @y = min(m22.w) from\n    #m m11 \n    join #bags bb on m11.w >= bb.w and m11.w = @w\n    join #m m22 on m22.w = m11.w-bb.w\n    where\n    (bb.v + m22.m) = ( @x );\n\n\n\n    update m1\n    set\n    m1.m = @x,\n    m1.prev_w = @y\n    from #m m1\n    where\n    m1.w = @w;\n\n    set @w = @w + 1;\nend;\n\nselect * from #m;\n\nselect \n-m1.m as best_price\nfrom\n#m m1\nwhere\nm1.w = (select min(m2.w) from #m m2 where m2.w >= @ReqQty and (m2.m is not null));\n\ndrop table #bags;\ndrop table #m;\n```\n\n\nScript final version:\n\n```\n-- use TEST;\n\ndeclare @limit decimal(19,4);\nset @limit = 1000;\n\ndeclare @ReqQty int;\nset @ReqQty = 21;\n\ncreate table #bags\n(\n    id int primary key,\n    qty int,\n    price decimal(19,4),\n    unit_price decimal(19,4),\n    w int, -- weight\n    v decimal(19,4), -- value\n    reqAmount int,\n    CONSTRAINT UNQ_qty UNIQUE(qty) \n);\n\ninsert into #bags(id, qty, price) \nvalues\n (10, 16, 25.00)\n,(20, 7, 14.00)\n,(30, 4, 7.00);\n\n\nupdate #bags set unit_price = price / ( 1.0 * qty );\n\nupdate #bags set w = qty;\nupdate #bags set v = -price;\n\nupdate #bags set reqAmount = 0;\n\n-- Uncomment the next line when debugging!\n-- select * From #bags;\n\ncreate table #m(w int primary key, m int, prev_w int);\ndeclare @w int;\nset @w = 0;\nwhile (@w<=@limit)\nbegin\n    insert into #m(w) values (@w);\n    set @w = @w + 1;\nend;\n\nupdate #m\nset m = 0;\n\nset @w = 1;\n\ndeclare @x decimal(19,4);\ndeclare @y decimal(19,4);\n\n    update m1\n    set\n    m1.m = 0 \n    from #m m1\n    where\n    m1.w = 0;\n\nwhile (@w<=@limit)\nbegin\n\n    select \n        @x = max(b.v + m2.m) \n    from\n    #m m1 \n    join #bags b on m1.w >= b.w and m1.w = @w\n    join #m m2 on m2.w = m1.w-b.w;\n\n    select @y = min(m22.w) from\n    #m m11 \n    join #bags bb on m11.w >= bb.w and m11.w = @w\n    join #m m22 on m22.w = m11.w-bb.w\n    where\n    (bb.v + m22.m) = ( @x );\n\n    update m1\n    set\n    m1.m = @x,\n    m1.prev_w = @y\n    from #m m1\n    where\n    m1.w = @w;\n\n    set @w = @w + 1;\nend;\n\n-- Uncomment the next line when debugging!\n-- select * from #m;\n\ndeclare @z int;\nset @z = -1;\n\nselect \n@x = -m1.m, \n@y = m1.w ,\n@z = m1.prev_w\nfrom\n#m m1\nwhere\nm1.w =  \n\n-- The next line contained a bug. It's fixed now. \n-- (select min(m2.w) from #m m2 where m2.w >= @ReqQty and (m2.m is not null)); \n\n(\n    select top 1 best.w from \n    (\n        select m1.m, max(m1.w) as w\n        from \n        #m m1\n        where\n        m1.m is not null\n        group by m1.m\n    ) best where best.w >= @ReqQty and best.w < 2 * @ReqQty\n    order by best.m desc\n)\n\n\n\n-- Uncomment the next line when debugging!\n-- select * From #m m1 where m1.w = @y;\n\nwhile (@y > 0)\nbegin\n    update #bags\n    set reqAmount = reqAmount + 1\n    where\n    qty = @y-@z;\n\n    select \n    @x = -m1.m, \n    @y = m1.w ,\n    @z = m1.prev_w\n    from\n    #m m1\n    where\n    m1.w = @z;\n\nend;\n\nselect * from #bags;\n\nselect sum(price * reqAmount) as best_price\nfrom #bags;\n\ndrop table #bags;\ndrop table #m;\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How Were These Coefficients in a Polynomial Approximation for Sine Determined?\r\n                \r\nBackground: I'm writing some geometry software in Java. I need the precision offered by Java's BigDecimal class. Since BigDecimal doesn't have support for trig functions, I thought I'd take a look at how Java implements the standard Math library methods and write my own version with BigDecimal support.\n\nReading this JavaDoc, I learned that Java uses algorithms \"from the well-known network library netlib as the package \"Freely Distributable Math Library,\" fdlibm. These algorithms, which are written in the C programming language, are then to be understood as executed with all floating-point operations following the rules of Java floating-point arithmetic.\"\n\nMy Question: I looked up fblibm's sin function, k_sin.c, and it looks like they use a Taylor series of order 13 to approximate sine (edit - njuffa commented that fdlibm uses a minimax polynomial approximation). The code defines the coefficients of the polynomial as S1 through S6. I decided to check the values of these coefficients, and found that S6 is only correct to one significant digit! I would expect it to be 1/(13!), which Windows Calculator and Google Calc tell me is 1.6059044...e-10, not 1.58969099521155010221e-10 (which is the value for S6 in the code). Even S5 differs in the fifth digit from 1/(11!). Can someone explain this discrepancy? Specifically, how are those coefficients (S1 through S6) determined?\n\n```\n/* @(#)k_sin.c 1.3 95/01/18 */\n/*\n * ====================================================\n * Copyright (C) 1993 by Sun Microsystems, Inc. All rights reserved.\n *\n * Developed at SunSoft, a Sun Microsystems, Inc. business.\n * Permission to use, copy, modify, and distribute this\n * software is freely granted, provided that this notice \n * is preserved.\n * ====================================================\n */\n\n/* __kernel_sin( x, y, iy)\n * kernel sin function on [-pi/4, pi/4], pi/4 ~ 0.7854\n * Input x is assumed to be bounded by ~pi/4 in magnitude.\n * Input y is the tail of x.\n * Input iy indicates whether y is 0. (if iy=0, y assume to be 0). \n *\n * Algorithm\n *  1. Since sin(-x) = -sin(x), we need only to consider positive x. \n *  2. if x < 2^-27 (hx<0x3e400000 0), return x with inexact if x!=0.\n *  3. sin(x) is approximated by a polynomial of degree 13 on\n *     [0,pi/4]\n *                   3            13\n *      sin(x) ~ x + S1*x + ... + S6*x\n *     where\n *  \n *  |sin(x)         2     4     6     8     10     12  |     -58\n *  |----- - (1+S1*x +S2*x +S3*x +S4*x +S5*x  +S6*x   )| <= 2\n *  |  x                               | \n * \n *  4. sin(x+y) = sin(x) + sin'(x')*y\n *          ~ sin(x) + (1-x*x/2)*y\n *     For better accuracy, let \n *           3      2      2      2      2\n *      r = x *(S2+x *(S3+x *(S4+x *(S5+x *S6))))\n *     then                   3    2\n *      sin(x) = x + (S1*x + (x *(r-y/2)+y))\n */\n\n#include \"fdlibm.h\"\n\n#ifdef __STDC__\nstatic const double \n#else\nstatic double \n#endif\nhalf =  5.00000000000000000000e-01, /* 0x3FE00000, 0x00000000 */\nS1  = -1.66666666666666324348e-01, /* 0xBFC55555, 0x55555549 */\nS2  =  8.33333333332248946124e-03, /* 0x3F811111, 0x1110F8A6 */\nS3  = -1.98412698298579493134e-04, /* 0xBF2A01A0, 0x19C161D5 */\nS4  =  2.75573137070700676789e-06, /* 0x3EC71DE3, 0x57B1FE7D */\nS5  = -2.50507602534068634195e-08, /* 0xBE5AE5E6, 0x8A2B9CEB */\nS6  =  1.58969099521155010221e-10; /* 0x3DE5D93A, 0x5ACFD57C */\n\n#ifdef __STDC__\n    double __kernel_sin(double x, double y, int iy)\n#else\n    double __kernel_sin(x, y, iy)\n    double x,y; int iy;     /* iy=0 if y is zero */\n#endif\n{\n    double z,r,v;\n    int ix;\n    ix = __HI(x)&0x7fffffff;    /* high word of x */\n    if(ix<0x3e400000)           /* |x| < 2**-27 */\n       {if((int)x==0) return x;}        /* generate inexact */\n    z   =  x*x;\n    v   =  z*x;\n    r   =  S2+z*(S3+z*(S4+z*(S5+z*S6)));\n    if(iy==0) return x+v*(S1+z*r);\n    else      return x-((z*(half*y-v*r)-y)-v*S1);\n}\n```\n\n    ", "Answer": "\r\nWe can use trig identities to get everything down to 0≤x≤π/4, and then need a way to approximate sin x on that interval.  On 0≤x≤2-27, we can just stick with sin x≈x (which the Taylor polynomial would also give, within the tolerance of a double).\n\nThe reason for not using a Taylor polynomial is in step 3 of the algorithm's comment.  The Taylor polynomial gives (provable) accuracy near zero at the expense of less accuracy as you get away from zero.  By the time you get to π/4, the 13th order Taylor polynomial (divided by x) differs from (sin x)/x by 3e-14.  This is far worse than fblibm’s error of 2-58.  In order to get that accurate with a Taylor polynomial, you’d need to go until (π/4)n-1/n! < 2-58, which takes another 2 or 3 terms.\n\nSo why does fblibm settle for an accuracy of 2-58?  Because that’s past the tolerance of a double (which only has 52 bits in its mantissa).\n\nIn your case though, you’re wanting arbitrarily many bits of sin x.  To use fblibm’s approach, you’d need to recalculate the coefficients whenever your desired accuracy changes.  Your best approach seems to be to stick with the Taylor polynomial at 0, since it’s very easily computable, and take terms until (π/4)n-1/n! meets your desired accuracy.\n\nnjuffa had a useful idea of using identities to further restrict your domain.  For example, ```\nsin(x) = 3*sin(x/3) - 4*sin^3(x/3)```\n.  Using this would let you restrict your domain to 0≤x≤π/12.  And you could use it twice to restrict your domain to 0≤x≤π/36.  This would make it so that your Taylor expansion would have your desired accuracy much more quickly.  And instead of trying to get an arbitrarily accurate value of π for (π/4)n-1/n!, I’d recommend rounding π up to 4 and going until 1/n! meets your desired accuracy (or 3-n/n! or 9-n/n! if you’ve used the trig identity once or twice).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Find maximal subgraph containing only nodes of degree 2 and 3\r\n                \r\nI'm trying to implement a (Unweighted) Feedback Vertex Set approximation algorithm from the following paper: FVS-Approximation-Paper. One of the steps of the algorithm (described on page 4) is to compute a maximal 2-3 subgraph of the input graph.\n\nTo be precise, a 2-3 graph is one that has only vertices of degree either 2 or 3. \n\nBy maximal we mean that that no set of edges or vertices of the original graph can be added to the maximal subgraph without violating the 2-3 condition.\n\nThe authors of the paper claim that the computation can be carried out by a \"simple Depth First Search (DFS)\" on the graph. However, this algorithm seems to elude me. How can the maximal subgraph be computed?\n    ", "Answer": "\r\nI think I managed to figure out something like what the authors intended. I wouldn't call it simple, though.\n\nLet G be the graph and H be an initially empty 2-3 subgraph of G. The algorithm bears a family resemblance to a depth-first traversal, yet I wouldn't call it that. Starting from an arbitrary node, we walk around in the graph, pushing the steps onto a stack. Whenever we detect that the stack contains a path/cycle/sigma shape that would constitute a 2-3 super-graph of H, we move it from the stack to H and continue. When it's no longer possible to find such a shape, H is maximal, and we're done.\n\nIn more detail, the stack usually consists of a path having no nodes of degree 3 in H. The cursor is positioned at one end of the path. With each step, we examine the next edge incident to the end. If the only incident edge is the one by which we arrived, we delete it from both G and the stack and move the end back one. Otherwise we can possibly extend the path by some edge e. If e's other endpoint has degree 3 in H, we delete e from G and consider the next edge incident to the end. If e's other endpoint has degree 2 in H but is not currently on the stack, then we've anchored this end. If the other end is anchored too, then add the stack path to H and keep going. Otherwise, move the cursor to the other end of the stack, reversing the stack. The final case is if the stack loops back on itself. Then we can extract a path/cycle/sigma and keep going.\n\nTyping this out on mobile, so sorry for the terse description. Maybe I'll find time to implement it.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Implementing Bin Fu's approximate sum algorithm\r\n                \r\nI'm trying to implement Bin Fu's approximate sum algorithm\nin a real language to have a better feel of how it works.\nIn a nutshell, this is an algorithm to compute efficiently $(1+\\epsilon)$-bounds on the value of $s(x)=\\sum_{i=1}^n x_i$ where\n$x$ is a vector of sorted floats.\nHowever, I must be doing something wrong because running the algorithm results in a bug\n(I'm also not very versed in pseudo algorithm language and some things like array bound checking seem to be implicit in this code).\nHere is the non-working code I have so far and any hints/help with the problem would be welcome --I'm language agnostic, I just used R because it is a 1-index (the algo is 1-index) open source interpreted language:\n```\nApproxRegion<-function(x,n,b,delta){\n    if(x[n]<b)  return(NULL)\n    if(x[n-1]<b)    return(c(n,n))\n    if(x[1]>=b) reurn(c(1,n))\n    m1<-2\n    while(x[n-m1**2+1]>=b)  m1<-m1**2\n    i<-1\n    m1<-m1\n    r1<-m1\n    while(m1>(1+delta)){\n        m1<-sqrt(m1)\n        if(x[n-floor(m1*r1)+1]>=b){\n            r1<-m1*r1\n        } else {\n            r1=r1\n        }\n        i=i+1\n    }\n    return(c(n-floor(r1*m1)+1,n))\n}       \nApproxSum<-function(x,n,epsilon){\n    if(x[n]==0) return(0)\n    delta<-3*epsilon/4\n    r1p<-n\n    s<-0\n    i<-1\n    b1<-x[n]/(1+delta)\n    while(b1>=((delta*x[n])/(3*n))){\n        Ri<-ApproxRegion(x=x,n=r1p,b=b1,delta=delta)\n        r1p<-Ri[1]-1\n        b1<-x[r1p]/(1+delta)\n        s1<-(Ri[2]-Ri[1]+1)*b1\n        s<-s+s1\n        i<-i+1\n    }\n    return(s)\n}\nn<-100;\nx<-sort(runif(n));\nApproxSum(x=x,n=length(x),epsilon=1/10);\nsum(x)\n```\n\nThe author mentions a c++ version but I couldn't find it online (any help on front would also be good).\nModo: I put the question here (rather than at the theoretical CS stackexchange site) because it's about an implementation problem. Feel free to move.\nEDIT\nThe original code had an 'hairy' exit condition (x[i]=$-\\infty$ for $i\\leq 0$).\nFollowing Martin Morgan's suggestion, I replaced the occurrences of this by a proper break, yielding the following code:\n```\nApproxRegion<-function(x,b,delta,n){\n    if(n<=1)            return(NULL)\n    if(x[n]<b)          return(NULL)\n    if(x[n-1]<b)            return(c(n,n))\n    if(x[1]>=b)         return(c(1,n))\n    m<-2\n    xit<-0\n    while(!xit){\n        if(n-m**2+1<1)      break\n        if(x[n-m**2+1]<b)   break\n        m<-m**2\n    }\n    i<-1\n    r<-m\n    while(m>=(1+delta)){\n        m<-sqrt(m)  \n        if(n-floor(m*r)+1>0){\n            if(x[n-floor(m*r)+1]>=b)    r=m*r   \n        }   \n        i<-i+1      \n    }\n    return(c(n-floor(m*r)+1,n))\n}       \nApproxSum<-function(x,n,epsilon){\n    if(x[n]==0) return(0)\n    delta=3*epsilon/4\n    rp<-n\n    s<-0\n    i<-1\n    b<-x[n]/(1+delta)\n    while(b>=delta*x[n]/(3*n)){\n        R<-ApproxRegion(x,b,delta,rp)\n            if(is.null(R))  break   \n        if(R[1]<=1) break\n        rp<-R[1]-1\n        b<-x[rp]/(1+delta)\n        si<-(R[2]-R[1]+1)*b\n        s<-s+si\n        i<-i+1\n    }\n    return(s)\n}\n```\n\nNow, it works:\n```\nn<-100;\nset.seed(123)\nx<-sort(runif(n));\nApproxSum(x=x,n=length(x),epsilon=1/10);\nsum(x)\n```\n\n    ", "Answer": "\r\nBy way of a partial answer... There are edge conditions that are not handled explicitly by the algorithm. For instance in ```\nApproxRegion```\n one needs to guard againt n = 0 (return value should be NULL?) or 1 (c(n, n)?) otherwise the first or second conditions ```\nx[n] < b```\n, ```\nx[n - 1] < b```\n will not evaluate as expected (e.g., x[0] returns numeric(0)). Likewise the test in the loop has to guard against ```\nm1**2 > n + 1```\n, otherwise you'll subscript by a negative number.\n\nI think there are similar issues in ```\nApproxSum```\n, particularly when ```\nApproxRegion```\n returns, e.g., ```\nc(1, 1)```\n (hence r1p == 0, b1 = integer()). It would be interesting to see an updated implementation.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm for creating a school timetable\r\n                \r\nI've been wondering if there are known solutions for algorithm of creating a school timetable. Basically, it's about  optimizing \"hour-dispersion\" (both in teachers and classes case) for given class-subject-teacher associations. We can assume that we have sets of classes, lesson subjects and teachers associated with each other at the input and that timetable should fit between 8AM and 4PM. \n\nI guess that there is probably no accurate algorithm for that, but maybe someone knows a good approximation or hints for developing it. \n    ", "Answer": "\r\nThis problem is NP-Complete!\nIn a nutshell one needs to explore all possible combinations  to find the list of acceptable solutions.  Because of the variations in the circumstances in which the problem appears at various schools (for example: Are there constraints with regards to classrooms?, Are some of the classes split in sub-groups some of the time?, Is this a weekly schedule? etc.) there isn't a well known problem class which corresponds to all the scheduling problems.  Maybe, the Knapsack problem has many elements of similarity with these problems at large.\n\nA confirmation that this is both a hard problem and one for which people perennially seek a solution, is to check this (long) list of (mostly commercial) software scheduling tools\n\nBecause of the big number of variables involved, the biggest source of which are, typically, the faculty member's desires ;-)..., it is typically impractical to consider enumerating all possible combinations.  Instead we need to choose an approach which visits a subset of the problem/solution spaces.\n- Genetic Algorithms, cited in another answer is (or, IMHO, seems) well equipped to perform this kind of semi-guided search (The problem being to find a good evaluation function for the candidates to be kept for the next generation)\n- Graph Rewriting approaches are also of use with this type of combinatorial optimization problems.\n\nRather than focusing on particular implementations of an automatic schedule generator program, I'd like to suggest a few strategies which can be applied, at the level of the definition of the problem.\nThe general rationale is that in most real world scheduling problems, some compromises will be required, not all constraints, expressed and implied: will be satisfied fully.  Therefore we help ourselves by:\n\n\nDefining and ranking all known constraints\nReducing the problem space, by manually, providing a set of additional constraints.This may seem counter-intuitive but for example by providing an initial, partially filled schedule (say roughly 30% of the time-slots), in a way that fully satisfies all constraints, and by considering this partial schedule immutable, we significantly reduce the time/space needed to produce candidate solutions.   Another way additional constraints help is for example \"artificially\" adding a constraint which prevent teaching some subjects on some days of the week (if this is a weekly schedule...); this type of constraints results in reducing the problem/solution spaces, without, typically, excluding a significant number of good candidates.\nEnsuring that some of the constraints of the problem can be quickly computed.  This is often associated with the choice of data model used to represent the problem; the idea is to be able to quickly opt-for (or prune-out) some of the options.\nRedefining the problem and allowing some of the constraints to be broken, a few times, (typically towards the end nodes of the graph).  The idea here is to either remove some of constraints for filling-in the last few slots in the schedule, or to have the automatic schedule generator program stop shy of completing the whole schedule, instead providing us with a list of a dozen or so plausible candidates.  A human is often in a better position to complete the puzzle, as indicated, possibly breaking a few of the contraints, using information which is not typically shared with the automated logic  (eg \"No mathematics in the afternoon\" rule can be broken on occasion for the \"advanced math and physics\" class;  or \"It is better to break one of Mr Jones requirements than one of Ms Smith ... ;-) )\n\n\nIn proof-reading this answer , I realize it is quite shy of providing a definite response, but it none the less full of practical suggestions.  I hope this help, with what is, after all, a \"hard problem\".\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Implementing the square root method through successive approximation\r\n                \r\nDetermining the square root through successive approximation is implemented using the following algorithm:\n\n\nBegin by guessing that the square root is x / 2. Call that guess g.\nThe actual square root must lie between g and x/g. At each step in the successive approximation, generate a new guess by averaging g and x/g.\nRepeat step 2 until the values of g and x/g are as close together as the precision of the hardware allows. In Java, the best way to check for this condition is to test whether the average is equal to either of the values used to generate it.\n\n\nWhat really confuses me is the last statement of step 3. I interpreted it as follows:\n\n```\nprivate double sqrt(double x) {\n    double g = x / 2;\n    while(true) {\n        double average = (g + x/g) / 2;\n        if(average == g || average == x/g) break;\n        g = average;\n    }\n\n    return g;\n}\n```\n\n\nThis seems to just cause an infinite loop. I am following the algorithm exactly, if the average equals either g or x/g (the two values used to generate it) then we have our answer ?\n    ", "Answer": "\r\nWhy would anyone ever use that approach, when they could simply use the formulas for (2n^2) = 4n^2 and (n + 1)^2 = n^2 + 2n + 1, to populate each bit in the mantissa, and divide the exponent by two, multiplying the mantissa by two iff the the mod of the exponent with two equals 1?\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Reinforcement Learning function approximation with Neural Networks\r\n                \r\nI'm trying to implement the Episodic Semi-gradient Sarsa for estimating q* with a Neural Network as a function approximator.\nMy question is: does the weight vector w in q(S, A, w) refer to the weights in the Neural Network?\n\nSee:\n Sutton and Barto  page 197/198 for a concrete algorithm.\n\nIf yes: then how to deal with the fact that there are multiple weight vectors in a multilayer Neural Network?\n\nIf no: How would I use it in the algorithm?\nMy suggestion would be to append it to the state s and action a and plug it into the Neural Network to get an approximation of the state with the chosen action. Is this correct?\n\nHow is the dimension of the weight vector w determined?\n\nThanks in advance!\n    ", "Answer": "\r\nThe w in the pseudocode does not strictly have to be just a single weight vector. The text in the beginning of the chapter does refer to w as a \"weight vector\" a couple of times, but the pseudocode itself only mentions that w are the parameters of a differentiable action-value function approximator. A Neural Network perfectly fits that description.\n\nIn the case of a Neural Network, you can think of w as the combination of all weight matrices (alternatively; you can view it as a really really long vector constructed by unrolling all of the weight matrices into a single vector). You can view the lines of pseudocode performing the update on w as regular backpropagation in Neural Networks, optimizing all the parameters w to make the prediction ```\nq(S, A, w)```\n slightly closer to ```\nR + gamma*q(S', A', w)```\n. \n\nThat single line of pseudocode basically summarizes the entire backpropagation procedure in the case where w is a huge vector consisting of unrolled weight matrices of a Neural Network. In practice, it cannot be implemented in a single line of code, because partial derivatives of earlier layers of the network (components of that gradients-of-```\nq```\n vector) depend on partial derivatives in layers closer to the output layer, so those have to be computed sequentially (which is what backpropagation as you know it if you're familiar with Neural Networks does).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm to approximate an optimal solution for an integer allocation pro­blem\r\n                \r\nI have the following problem:\n\n\n  Given a set of sums of variables like { a + b, b + c, c + d, a + a + d, b }, find positive integer values for the variables such that all sums are distinct and the highest sum is as small as possible.\n\n\nIs there an algorithm to find or approximate a solution to this kind of problems?\n    ", "Answer": "\r\nI have created a possible solution and an implementation in C#. Hope it is what you need. It would be nice if someone proves it is correct/incorrect but it works and results look correct. The details on theory are below. Its complexity is something about ```\nO(N!*M^3*Log2(N))```\n where ```\nN```\n is number of variables and ```\nM```\n is number of summands all sums.\n\nBTW, for your example it gives this result:\n\n```\nc=3, a=2, d=2, b=1\n{a+b=3; b+c=4; c+d=5; a+a+d=6; b=1}\nmax=6\n```\n\n\nUPDATE\n\nTheory of the algorithm.\n\nAssume the variables are ordered, e.g. ```\na >= b >= c >= ....```\n\nLets say a set of sums is a Bag if all sums in it are distinct.\nAll sums in a Bag can be divided into two groups: sums that do not contain variable ```\na```\n and sums that do contain. Lets call the first group as Head and the second as Tail.\nNote that both are Bags because they contain distinct sums.\nWe can subtract ```\na```\n from each sum in Tail so that all sums remain distinct (i.e. the Tail is still a Bag). This way we get two bags both without variable ```\na```\n.\n\nSimilar way we exclude variable ```\nb```\n from two Bags and get four Bags.\nThis operation we repeat for each variable until we get sums with last variable (lets say it is ```\nd```\n). The smallest value of ```\nd```\n is 1.\n\nAfter that we can return to the previous step and include variable ```\nc```\n in sums from tails. Remember that we have many pairs Head-Tail and need to join them back. To do that we add ```\nc```\n to each sum in each Tail so that sums from the Tail have to be distinct from the Head.\n\nHow to calculate ```\nc```\n? The idea is to calculate its invalid values and then take the smallest value that is not invalid and also is equal or greater than ```\nd```\n. Calculating invalid values is trivial, using condition ```\nHeadSum != TailSum + c```\n => ```\nc != HeadSum - TailSum```\n. For each combination of tail sum and head sum we get all invalid values.\n\nCollapsing all pairs Head-Tail back and calculating each variable we get the solution for the case ```\na >= b >= c >= d```\n.\nThen we calculate a solution for each permutation of ```\na >= b >= c >= d```\n and find the smallest one.\n\nPS It would be great if someone provide better solution because I think my algorithm is somewhat of approximation (but a good approximation) or even better to prove it.\nThe worst thing is ```\nN!```\n complexity because of permutations and I still have no idea how to get rid of it.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "What is the algorithm used for approximating trigonometric functions in RStudio?\r\n                \r\nI've tried the below code on RStudio and was expecting 0, 1 and 0.5 to show up. However, it showed a very small number instead of 0 and I thought it must be using some algorithm to approximate the sin function.\n```\nsin(c(pi, pi/2, pi/6))\n```\n\nThis was the result\n```\n1.224606e-16 1.000000e+00 5.000000e-01\n```\n\nI wanted to know how they approximated the sin function in this case.\n    ", "Answer": "\r\nThough your question may seem simple at first, the reality is quite the opposite. Whenever you want to know what a function is doing, you just have to access the function as it were an object (it literally is an object in R):\n```\nsin # function (x)  .Primitive(\"sin\")\n```\n\n.Primitive is one of the ways R can call C. If you want to see the C-code, then you can use the pryr library as in:\n```\npryr::show_c_source(.Primitive(sin(x)))\n# do_math1 with op = 21\n```\n\nIt also opens a Github page with the code of arithmetic.c, the arithmetic heart of R. R computes sin with the do_math1 function with option 21. If you want to go any further, you will need to understand how the sin function is estimated in C. For that, I recommend the following post:\nHow does C compute sin() and other math functions?\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Bound Principle for Integer Linear Programming and Linear Programming\r\n                \r\nCurrently, I am learning approximation algorithms. When I learned Vertex Cover via LP, I encountered a principle called Bounding Principles.\nIt like this:\n\n(1) The maximum value for an ILP problem is always less than or equal to the maximum \nvalue for the LP relaxation: \n\nMAX for ILP ≤ MAX for LP relaxation \n\n(2) The minimum value for an ILP problem is always greater than or equal to the \nminimum for the LP relaxation: \n\nMIN for ILP ≥ MIN for LP relaxation\n\nI cannot figure out why \"MAX for ILP ≤ MAX for LP relaxation\" and \"MIN for ILP ≥ MIN for LP relaxation\". \n\nCan anyone explain, thx!\n    ", "Answer": "\r\nAn ILP has an extra constraint than LP problem. The constraint is that all variables should be integers.\n\nHence, the optimal solution for an ILP shall be at best as good as an optimal solution for an LP problem, it can never be better.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximate nearest neighbor algorithm for Octrees\r\n                \r\nDoes anyone know the origin (paper, book, etc.) of this approximate nearest neighbor technique for octrees?:\n\nhttp://www.cs.ucdavis.edu/~amenta/w11/nnLecture.pdf\n\nI am having trouble implementing it from the provided pseudo code. I am also having trouble finding the original publication of the technique, which I am hoping has a little more detail.\n\nThanks for any help.\n    ", "Answer": "\r\nThis is not the exact answer, but an approximate ( to use the terms of the subject :) ).\n\nIt was too big to write it an comment, and I think is good information for a start.\n\nThe paper mentions that Voronoi diagrams don't expand in higher dimensions than three and it implies that octrees do. That's wrong, in terms of terminology.\n\nAn octree is defined in R^3. Simply put it, you see this kind of data structure in 2D, where we have a quadtree. These kind of trees have 2^D children per node, where D is the dimension. This means:\n\n```\n 1. 2D: 2^D children per node, i.e. 4 children per node.\n\n 2. 3D: 2^D children per node, i.e. 8 children per node.\n\n 3. and so on.\n```\n\n\nFor exampe, ```\noctree```\n, comes from the Greek word octo, which means 8 and it implies that this tree has 8 children per node.\n\nI had implemented this kind of tree for ```\nNN```\n (Nearest Neighbor) and, even though I had made the tree a polymorphic one to not waste any amount of memory, this wouldn't scale above 10 dimensions.\n\nMoreover, the paper mentions ```\nkd-trees```\n. Notice, that when dimensions go high, the query time is no longer ```\nO(logn)```\n, but it becomes slightly less than brute force approach (i.e. check all points). The higher the dimensions, the worse ```\nkd-trees```\n will perform.\n\nA ```\nkd-tree```\n is actually a binary tree, embedded in geometry. By that, I mean, that every node has two children and at every level, we halve the dataset (usually in the median of the coordinate with the greatest variance, so that we can exploit the structure of the dataset). And this will result into a perfect tree.\n\n\nHere you can see a ```\nkd-tree```\n, a friend of mine made, of 64 points in 8D. In this version, we store 4 points per leaf.\n\nThe numbers in the boxes refer to the point number (starting with 1,\ni.e. line numbers in test.points file).\n\nThe notation \"8 @ 0.532\" refers to an inner node, where the data is\nsplit at 0.532 along the eighth dimension (again, dimensions starting with\n1, for easier human understanding).\n\nThat's why, we tend our interest in approximate NN, which means that we pay some loss in accuracy, but we obtain some speedup. (As you may know, everything is a trade-off).\n\nBy Box, it probably means a ```\nminimum bounding box```\n.\n\nThis is simple and here is an example:\n\nSuppose you have, in 2D, this dataset:\n\n```\n-1 -2\n 0  5\n 8 -5\n```\n\n\nIn order to construct the Bounding box, we need to find the minimum and the maximum coordinate in every dimension. Note, that for storing the Boudning box, it is enough to store its min and max corner.\n\nHere, we have ```\nmin = (-1, -5) and max = (8, 5)```\n. The bounding box is then, the rectangle, formed, in clockwise order -starting from max corner, the one that has as corners:\n\n```\n( 8,  5)  // ( max.x, max.y)\n( 8, -5)  // ( max.x, min.y)\n(-1, -5)  // ( min.x, min.y)\n(-1,  5)  // ( min.x, max.y)\n```\n\n\nObserve, that all the points of the dataset, lie inside this bounding box.\n\nAs for the paper, it's actually a lecture, not a paper. It doesn't explain how one should write the algorithm. Moreover, it doesn't provide any unique information, in order to try to find another .pdf, that explains in more details the .pdf in your link.\n\n[EDIT] for the OP's comment.\n\n1) Q: ```\ndequeue box B, containing representative point p```\n\n\nI would say, that dequeue, means extract the \"first\" element of the queue. Enqueue, means push back an element in the queue. The queue seams to hold Bounding boxes as elements.\n\n2) Q: ```\nr = d(q,B)```\n\n\nMaybe, he means from the representative point the box contains. Not clear.\n\nYou can compute the (Euclidean) distance from the query point to the closest corner of the box, or to the representative of the box.\n\n3) ```\nfor all children B' of B containing points in P```\n\n\n```\nP```\n is the dataset. Every box, is partitioned in 8 sub-boxes at every level (in the case    of octree).\n\n4) Q: ```\nwhile dN >= (1+e)r do```\n\n\nApproximation error ```\ne```\n, is actually, what we call ```\nepsilon```\n. It is usually a parameter and it means, that when you check:\n\n```\nwhile delta >= r do```\n\n\nyou are less strict and you do\n\n```\nwhile delta >= (1 + e)*r do```\n\n\nwhich means that you are going into the loop less times than the exact condition above.\nSo, I think it says, to insert every sub-box of box B, in the queue. This is not so clever, IMHO.\n\nAbout the last comment with e = 0.01, just do the math in the condition above. You will see that the answer is no, since as the link you posted state, ```\ne```\n is a multiplicative factor.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm to approximate non-linear equation system solution\r\n                \r\nI'm looking for an algorithm to approximate the solution of the following equation system: \n\nThe equations have to be solved on an embedded system, in C++.\n\nBackground:\n\n\nWe measure the 2 variables X_m and Y_m, so they are known\nWe want to compute the real values: X_r and Y_r\nX and Y are real numbers\nWe measure the functions f_xy and f_yx during calibration. We have maximal 18 points of each function. \nIt's possible to store the functions as a look-up table\nI tried to approximate the functions with 2nd order polynomials and compute the solution, but it was not accurate enough, because of the fitting error.\n\n\nI am looking for an algorithm to approximate the results in an embedded system in C++, but I don't even know what to search for. I found some papers on the theory link, but I think there must be an easier way to do it in my case.\n\nAlso: how can I determine during calibration, whether the functions can be solved with the algorithm?\n    ", "Answer": "\r\nFitting a second-order polynomial through ```\nf_xy```\n? That's generally not viable. The go-to solution would be Runga-Kutta interpolation. You pick two known values left and two to the right of your argument, with weights 1,2,2,1. This gets you an estimate ```\nd(f_xy)/dx```\n which you can then use for interpolation.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Concrete algorithm code for approximate string matching\r\n                \r\nApproximate string matching is not a stranger problem. \n\nI am learning and trying to understand how to solve it. I even now don't want to get too deep into it and just want to understand the brute-force way.\n\nIn its wiki page (Approximate string matching), it says \n\n\n  A brute-force approach would be to compute the edit distance to P (the pattern) for all substrings of T, and then choose the substring with the minimum distance. However, this algorithm would have the running time O(m * n^3), n is the length of T, m is the length of P\n\n\nOk. I understand this statement in the following way:\n\n\nWe find out all possible substrings of T\nWe compute the edit distance of each pair of strings {P, t1}, {P, t2}, ... \nWe find out which substring has the shortest distance from P and this substring is the answer.\n\n\nI have the following question:\n\na. I can use two for-loop to get all possible substrings and this requires O(n^2). So when I  try to compute the edit distance of one substring and the patter, does it need O(n*m)? Why?\n\nb. How exactly do I compute the distance of one pair (one substring and the patter)? I know I can insert, delete, substitute, but can anyone give me a algorithm that do just the calculation for one pair?\n\nThanks\n\n\n\nEdit\n\nOk, I should use Levenshtein distance, but I don't quite understand its method.\n\nHere is part of the code\n\n```\nfor j from 1 to n\n{\n    for i from 1 to m\n    {\n      if s[i] = t[j] then  \n        d[i, j] := d[i-1, j-1]       // no operation required\n      else\n        d[i, j] := minimum\n                   (\n                     d[i-1, j] + 1,  // a deletion\n                     d[i, j-1] + 1,  // an insertion\n                     d[i-1, j-1] + 1 // a substitution\n                   )\n    }\n  }\n```\n\n\nSo, assume I am now comparing ```\n{\"suv\", \"svi\"}```\n.\n\nSo ```\n'v' != 'i'```\n, then I have to see three other pairs:\n\n\n```\n{\"su\", \"sv\"}```\n\n```\n{\"suv\", \"sv\"}```\n\n```\n{\"su\", \"svi\"}```\n\n\n\nHow can I understand this part? Why I need to see these 3 parts?\n\nDoes the ```\ndistance between two prefixes```\n mean that we need ```\ndistance```\n number of changes in order to make the two prefixes (or strings) equal?\n\nSo, let's take a look at ```\n{\"su\", \"sv\"}```\n. We can see that distance of ```\n{\"su\", \"sv\"}```\n is 1. Then how can ```\n{\"su\", \"sv\"}```\n become ```\n{\"suv\", \"svi\"}```\n by just adding 1? I think we need to insert 'v' into \"su\" and 'v' into \"sv\" and then substitute the last 'i' with 'v', which has 3 operations involved, right?\n    ", "Answer": "\r\nThe standard way of measuring the edit distance between two strings is called Levenshtein distance - the wikipedia page contains pseudocode for the algorithm.\n\nAs for your edit: You need to look at ```\n{\"su\", \"sv\"}```\n because it is possible that the best way to change ```\n\"suv\"```\n into ```\n\"svi\"```\n is to replace the last ```\nv```\n by ```\ni```\n, whose cost will come on top of the cost for changing ```\n\"su\"```\n to ```\n\"sv\"```\n. Or, it could be that the best way is to change ```\n\"suv\"```\n into ```\n\"sv\"```\n somehow and then add an ```\ni```\n. Or, it could be that the best way is to first delete the ```\nv```\n from ```\n\"suv\"```\n and then change ```\n\"su\"```\n into ```\n\"svi\"```\n. The first way turns out to be best (or as good as the other options) in this case. The edit distance is indeed 2, and the operations are to change the ```\nu```\n into a ```\nv```\n and the ```\nv```\n into an ```\ni```\n.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Two Dimensional Curve Approximation\r\n                \r\nhere is what I want to do (preferably with Matlab):\n\nBasically I have several traces of cars driving on an intersection. Each one is noisy, so I want to take the mean over all measurements to get a better approximation of the real route. In other words, I am looking for a way to approximate the Curve, which has the smallest distence to all of the meassured traces (in a least-square sense). \n\nAt the first glance, this is quite similar what can be achieved with spap2 of the CurveFitting Toolbox (good example in section Least-Squares Approximation here).\nBut this algorithm has some major drawback: It assumes a function (with exactly one y(x) for every x), but what I want is a curve in 2d (which may have several y(x) for one x). This leads to problems when cars turn right or left with more then 90 degrees.\nFuthermore it takes the vertical offsets and not the perpendicular offsets (according to the definition on wolfram). \n\nHas anybody an idea how to solve this problem? I thought of using a B-Spline and change the number of knots and the degree until I reached a certain fitting quality, but I can't find a way to solve this problem analytically or with the functions provided by the CurveFitting Toolbox. Is there a way to solve this without numerical optimization?\n    ", "Answer": "\r\nmbeckish is right. In order to get sufficient flexibility in the curve shape, you must use a parametric curve representation (x(t), y(t)) instead of an explicit representation y(x). See Parametric equation.\n\nGiven n successive points on the curve, assign them their true time if you know it or just integers 0..n-1 if you don't. Then call spap2 twice with vectors T, X and T, Y instead of X, Y. Now for arbitrary t you get a point (x, y) on the curve.\n\nThis won't give you a true least squares solution, but should be good enough for your needs.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Square root by approximation\r\n                \r\nI am trying to solve this challenge but I don't know if my code is wrong, or if the phrasing of the challenge is wrong. The algorithm says:\n\n\nChoose two numbers ```\nS,E```\n. The ```\nX```\n square root must be in ```\n[S,E]```\n interval.\nChoose the precision desired ```\ne```\n.\nThe middle value of the current interval, ```\nM```\n, will be a good approximation.\nWhile the interval ```\n[S,E]```\n is greater than ```\ne```\n, do:\n\n\nfind the middle value of the current interval ```\nM```\n;  \n```\nif M^2 > x```\n, ```\nE = M```\n, otherwise, ```\nS = M```\n; \nWhen the length of \nour interval is smaller than ```\ne```\n, square root of ```\nX = M```\n.\n\n\n\nMy code below produces an infinite loop:\n\n```\ne = 0.001; //I want square root of 10\nn = \"10\";\nx = parseInt(n);\nE = (x / 2);\nS = 1;\nM = ((E - S) / 2);\ntam = (E - S);\nwhile (tam >= e) {\n console.log(M)\n if ((M * M) > x) {\n   E = M;\n } else {\n   S = M\n };\n M = ((E - S) / 2);\n tam = (E - S);\n}\nconsole.log(n + \": \" + M);\n```\n\n\nThanks\n    ", "Answer": "\r\nYou're not finding the midpoint of the interval correctly. You should be adding ```\nE```\n and ```\nS```\n and dividing by two instead of subtracting.\n\n\r\n\r\n```\ne=0.001; //I want square root of 10\r\n   n=\"10\";\r\n   x=parseInt(n);\r\n   E=(x/2);\r\n   S=1;\r\n   M=((E+S)/2);\r\n   tam = (E-S);\r\n   while(tam>=e){\r\n       console.log(M)\r\n       if ((M*M)>x){E=M;}else{S=M};\r\n       M=((E+S)/2);\r\n       tam = (E-S);\r\n   }   console.log(n+\": \"+M);```\n\r\n\r\n\r\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How do I select the node that minimizes the maximum shortest distance to the other nodes in a graph?\r\n                \r\nI have an undirected, connected, positively-weighted graph ```\nG = <V,E>```\n. I need to find one node ```\nv_min```\n in ```\nV```\n such that the maximum distance between ```\nv_min```\n and the other nodes are minimized. I've learned that this is a special problem of the k-center problem (i.e., with ```\nk=1```\n) which is already known to be NP-Hard. However, since we are restricting ```\nk```\n to be equal to 1, I assume that the problem can still be solved efficiently.\nThe approach I have now is: compute all-pairs distances among the nodes in ```\nV```\n, e.g., using Floyd-Warshall, or repeated calls of Dijkstra. Then, we go down the list of nodes to find the one that minimizes the maximum distance between the node and the other nodes. If there are more than one nodes that satisfy this, pick any one of them.\n\nIs this approach correct?\nIs there any better approach (i.e., more efficient)?\nNote that I am not interested in approximation algorithms, only exact algorithms.\n\n    ", "Answer": "\r\nThe nodes you're looking for are called the graph center or the Jordan center, and your approach of finding them is the common method. Floyd-Warshall is a quick way to find all distances between nodes, and iterating over the result to find the minimum maximum will take even less time.\nThis should be fast enough for most purposes, and it's impossible to do much better. If performance is of the utmost importance, you could take a look at this 2019 paper which introduces a new algorithm which they claim is better parallelizable, and usually slightly faster than Floyd-Warshall.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Calculating lower bound with stirlings approximation\r\n                \r\nWe have this exercise in school, where we are to calculate the lower bound of an algorithm.\nWe know that the lower bound is: Log_6((3*n)! / n!^3) and we are to use stirlings approximation to approximate n!. When appling stirlings approximation we get:\n\nlog_6((sqrt(2*pi*3*n)*((3*n)/e)^(3*n) * e^alpha)/(sqrt(2*pi*n)*(n/e)^n * e^alpha)^3)\n\nNow our problem is that every time we try expanding this formula with simple logarithm properties, such as log(a/b) = log(a)-log(b), log(a*b) = log(a)+log(b), log(a^b) = b*log(a) and lastly for sqrt log(sqrt(a)) = log(a^1/2) = 1/2 * log(a), we get a result where to dominating expression will be something with n*log(n) * constant. Now we know from the teacher that we have to find a linear lower bound, so this is wrong. \n\nWe have been using 2 days on this and are about to give up. Can anybody maybe help us?\n\nThanks in advance!\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Exploring an algorithm to approximate minimum N concentration of an area\r\n                \r\nImagine there is a farmer growing a large land. I have a device to measure the amount of N (Nitrogen) in a location of this land. Using this device I have measured the N values of several locations and I have coordinates of each location.\nThe problem is I need an algorithm to\n\napproximate the coordinates of the location where the N value is minimum.\napproximate the N value of that location\n\nHere is the class for keeping Measurement values. It includes coordinates and the corresponding N value of the location.\n```\npublic class Measurement {\n    private double latitude;\n    private double longitude;\n    private double N;\n\n    public Measurement(double latitude, double longitude, double n) {\n        this.latitude = latitude;\n        this.longitude = longitude;\n        N = n;\n    }\n\n    public double getLatitude() {\n        return latitude;\n    }\n\n    public double getLongitude() {\n        return longitude;\n    }\n\n    public double getN() {\n        return N;\n    }\n}\n\n```\n\nHere is the Main.java class.\n```\nimport java.util.ArrayList;\n\npublic class Main {\n    public static void main(String[] args) {\n        ArrayList<Measurement> measurements = new ArrayList<>();\n        //following data is not real.\n        measurements.add(new Measurement(4.1214,5.3445,4));\n        measurements.add(new Measurement(4.1224,5.3431,10));\n        measurements.add(new Measurement(4.1220,5.3450,15));\n        measurements.add(new Measurement(4.121,5.3440,1));\n        measurements.add(new Measurement(4.1211,5.3437,4));\n        measurements.add(new Measurement(4.1215,5.3475,24));\n\n        Measurement approximated = findMinimumConcentrationLocation(measurements);\n    }\n\n    private static Measurement findMinimumConcentrationLocation(ArrayList<Measurement> measurements) {\n        //TODO : implement this\n\n        return null;\n    }\n}\n\n```\n\nThis is a real practical problem I got when doing a project (mobile app), not homework or an assignment.\nEdit: Assume the area that I need to find the minimum concentration of N is like the convex hull polygon. (https://www.geeksforgeeks.org/convex-hull-set-1-jarviss-algorithm-or-wrapping/).I am going to get measurements from around the border, and the middle of the area so we can assume that the convex hull gives the target area.\nAlso, assume that the land is fully horizontal.\nI don't want the measurement with minimum values. It will be inaccurate in most cases.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fast approximate float division\r\n                \r\nOn modern processors, float division is a good order of magnitude slower than float multiplication (when measured by reciprocal throughput).\n\nI'm wondering if there are any algorithms out there for computating a fast approximation to ```\nx/y```\n, given certain assumptions and tolerance levels. For example, if you assume that ```\n0<x<y```\n, and are willing to accept any output that is within 10% of the true value, are there algorithms faster than the built-in FDIV operation?\n    ", "Answer": "\r\nI hope that this helps because this is probably as close as your going to get to what you are looking for.\n\n```\n__inline__ double __attribute__((const)) divide( double y, double x ) {\n                                    // calculates y/x\n    union {\n        double dbl;\n        unsigned long long ull;\n    } u;\n    u.dbl = x;                      // x = x\n    u.ull = ( 0xbfcdd6a18f6a6f52ULL - u.ull ) >> (unsigned char)1;\n                                    // pow( x, -0.5 )\n    u.dbl *= u.dbl;                 // pow( pow(x,-0.5), 2 ) = pow( x, -1 ) = 1.0/x\n    return u.dbl * y;               // (1.0/x) * y = y/x\n}\n```\n\n\n\nSee also:\n    Another post about reciprocal approximation.\n    The Wikipedia page.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "AppEngine Approximate Partial String Matching Algorithm\r\n                \r\nSo, I realize that this covers a wide array of topics and pieces of them have been covered before on StackOverflow, such as this question.  Similarly, Partial String Matching and Approximate String Matching are popular algorithmic discussions, it seems.  However, using these ideas in conjunction to suit a problems where both need to be discussed seems highly inefficient.  I'm looking for a way to combine the two problems in to one solution, efficiently.\n\nRight now, I'm using AppEngine with Java and the Persistent DataStore.  This is somewhat annoying, since it doesn't seem to have any arithmetic usage in the queries to make things easier, so I'm currently considering doing some precalculation and storing it as an extra field in the database.  Essentially, this is the idea that a friend and I were having on how to possibly implement a system for matching and I was more or less hoping for suggestions on how to make it more efficient.  If it needs to be scrapped in favor of something better that already exists, I can handle that, as well.\n\n\n\nLet's start off with a basic example of what I'd look to do a search for.  Consider the following nonsense sentence:\n\n\n  The isolating layer rackets the principal beneath your hypocritical rubbish.\n\n\nIf a user does a search for \n\n\n  isalatig pri\n\n\nI would think that this would be a fairly good starting match for the string, and the value should be returned.  The current method that we are considering using basically assigns a value to test divisibility.  Essentially, there is a table with the following data\n\n```\nA: 2        B: 3        C: 5\nD: 7        E: 11       F: 13\n...\n```\n\n\nwith each character being mapped to a prime number (multiple characters don't make a difference, only one character is needed).  And if the query string divides the string in the database, then the value is returned as a possible match.\n\nAfter this, keywords that aren't listed as stopwords are compared from the search string to see if they are starting substrings of words in the possible match under a given threshold of an edit distance (currently using the Levenshtein distance).\n\n```\ndistance(\"isalatig\", \"isolating\") == 2\ndistance(\"pri\", \"principal\") == 0 // since principal has a starting \n                                  // substring of pri it passes\n```\n\n\nThe total distance for each query is then ranked in ascending order and the top ```\nn```\n values are then returned back to the person doing the querying.\n\n\n\nThis is the basic idea behind the algorithm, though since this is my first time dealing with such a scenario, I realize that I'm probably missing something very important (or my entire idea may be wrong).  What is the best way to handle the current situation that I'm trying to implement.  Similarly, if there are any utilities that AppEngine currently offers to combat what I'm trying to do, please let me know.\n    ", "Answer": "\r\nFirst off, a clarification: App Engine doesn't allow arithmetic in queries because there's no efficient way to query on the result of an arbitrary arithmetic expression. When you do this in an SQL database, the planner is forced to select an inefficient query plan, which usually involves scanning all the candidate records one by one.\n\nYour scheme will not work for the same reason: There's no way to index an integer such that you can efficiently query for all numbers that are divisible by your target number. Other potential issues include words that translate into numbers that are too large to store in a fixed length integer, and being unable to distinguish between 'rental', 'learnt' and 'antler'.\n\nIf we discard for the moment your requirement for matching arbitrary prefixes of strings, what you are searching for is full-text indexing, which is typically implemented using an inverted index and stemming. Support for fulltext search is on the App Engine roadmap but hasn't been released yet; in the meantime your best option appears to be SearchableModel, or using an external search engine such as Google Site Search.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Using linear approximation to perform addition and subtraction | error barrier\r\n                \r\nI'm attempting my first solo project, after taking an introductory course to machine learning, where I'm trying to use linear approximation to predict the outcome of addition/subtraction of two numbers.\n\nI have 3 features: first number, subtraction/addition (0 or 1), and second number.\nSo my input looks something like this:\n\n```\n3 0 1\n4 1 2\n3 0 3\n```\n\n\nWith corresponding output like this:\n\n```\n2\n6\n0\n```\n\n\nI have (I think) successfully implemented logistic regression algorithm, as the squared error does gradually decrease, but in 100 values, ranging from 0 to 50, the squared error value flattens out at around 685.6 after about 400 iterations.\n\nGraph: Squared Error vs Iterations\n .\n\nTo fix this, I have tried using a larger dataset for training, getting rid of regularization, and normalizing the input values.\n\nI know that one of the steps to fix high bias is to add complexity to the approximation,  but I want to maximize the performance at this particular level. Is it possible to go any further on this level?\n\nMy linear approximation code in Octave:\n\n```\n% Iterate\nfor i = 1 : iter\n    % hypothesis\n    h = X * Theta;\n\n    % reg theta prep\n    regTheta = Theta;\n    regTheta(:, 1) = 0;\n\n    % cost calc\n    J(i, 2) = (1 / (2 * m)) * (sum((h - y) .^ 2) + lambda * sum(sum(regTheta .^ 2,1),2));\n\n    % theta calc\n    Theta = Theta -  (alpha / m) * ((h - y)' * X)' + lambda * sum(sum(regTheta, 1), 2);\nend\n```\n\n\nNote: I'm using 0 for lambda, as to ignore regularization. \n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Infinite series approximation of pi in C not terminating\r\n                \r\nHere is my code, when using print statements it appeared to be an infinite loop:\n\nsome potential issues i can troubleshoot are integer division, and perhaps figuring out if the algorithm terminates, and if it always has the correct output, perhaps there is some idiosyncrasy of the C language that I do not understand causing this issue?\n\nFrom my understanding as the sum tends to negative infinity it will cause the break statement to be triggered ending the algorithm once it reaches an approximation of epsilon precision.\n\n```\n#include <stdio.h>\n\nint main()\n{\n\n  double pi, sum, epsilon;\n  long long i=1;\n  long long max = 2147483647;\n  printf(\"Pleas enter the desired accuracy epsilon : \");\n  scanf(\"%f\", &epsilon);\n  while (i<max){\n    if (i%2 == 0){\n      sum = -4.0/(2.0*i-1);\n    }else if (i%2 ==1){\n      sum = (4.0/(2.0*i-1));\n    }\n    if (sum < epsilon){\n      break;\n    }\n    pi += sum;\n  }\n  printf(\"The value of pi approximated to epsion : %f is %f\\n\", epsilon, pi);\n  return 0;\n}\n```\n\n    ", "Answer": "\r\nYou are not increment the value of ```\ni```\n in your ```\nwhile```\n loop. As a result its value always remains 1.\n\nIncrement ```\ni```\n after your processing is done before the closing ```\n}```\n of the ```\nwhile```\n loop.\n\nInitialize the values of ```\npi, sum, epsilon```\n so that you do not run into undefined behavior when you try to read their values for the first time.\n\nAlso, with proper options enabled (```\n-Wall```\n with GCC) the compiler will warn you, that you are trying to read a ```\nfloat```\n into a ```\ndouble```\n.\n\n```\nwarning: format '%f' expects argument of type 'float *', but argument 2 has type 'double *' [-Wformat=]\n\n   scanf(\"%f\", &epsilon);\n\n          ~^   ~~~~~~~~\n```\n\n\nSo you have to use the correct conversion specifier: ```\n%lf```\n instead of ```\n%f```\n in your ```\nscanf```\n statement.\n\nIf you are not very particular about the number ```\n2147483647```\n and just want a large value, you can also use ```\nULLONG_MAX```\n (from ```\nlimits.h```\n) to initialize ```\nmax```\n instead of hard-coding the value.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Terrain triangulation algorithm\r\n                \r\nI have a terrain presented by large set of points in ```\n3D-space```\n. What is the best way to triangulate it?\n\nI can just to project all points on ```\n2D-space```\n, than make Delaunay triangulation in time ```\nO(n * log(n))```\n and lift it back to the previous heigth. But is it good enough? I've heard about Delaunay triangulation in time ```\nO(n * log(log(n))```\n) in some special cases. Is it possible in my case? Or maybe I should to use some approximation algorithm?\n    ", "Answer": "\r\nProjection and Delaunay triangulation in 2D is certainly a good solution which will generate well shaped triangles. For a terrain you might also need to enforce certain edges, so look for a constrained Delaunay triangulation. \n\nAs to the runtime: For real world data you can assume linear runtime. If performance is important, make sure your input data is not degenerate: Scanning devices often return points on a grid. You can improve the situation by adding some noise. \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Real time CPU data for algorithms of different complexity classes\r\n                \r\nCan anyone point me to some reliable resource/document where there is some authentic discussion on the time taken by algorithms of different complexity classes e.g. ```\nO (log n), O (n), O(n log n), O(n^2), O(n^3)```\n etc. etc. Particularly I am interested in some document/site which can answer to the following question:\n\nGiven a machine configuration (CPU, Memory) how much time (in miliseconds/seconds) does it take to run mergesort (or binary search or some other standard algorithm) with N instances as input where N can vary from 100 to 1 million. \n\nIt would be even be better if someone can point me towards a document that can not only give me the time in miliseconds but also can give me an approximation/heuristics of the energy cost that will be incurred in Joules/KJoules if some of the above mentioned algorithm is run on a mobile device (smart phone).\n    ", "Answer": "\r\nI've put some time into doing just what you are asking. This isn't at the level of university research or real-time/production level code but it may help.\n\nI've implemented a number of data structures and algorithms and run tests on them using sorted, unsorted, etc data.\nhttp://github.com/phishman3579/java-algorithms-implementation\n\nAlso, you could easily get the other information yourself since all the code is open source and on the site.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Finding Graph Partitions with Minimum Edges Crossing Partitions\r\n                \r\nI have recently been working on a few algorithms and have gotten stuck on the following problem:\n\nGiven an undirected unweighted possibly disconnected graph G(V, E) find k partitions of G such that each partition has the same number of vertices (assume the total number of vertices is a multiple of k) and that there is a minimum number of edges in G that connect vertices in different partitions.\n\nI am not looking for a polynomial time algorithm for this problem (as it is probably NP-Hard anyway), but rather an algorithm that will find a solution for 150 vertices and 1200 edges in under 24 hours. While any good approximation algorithms would be much appreciated as well, I would prefer an exact solution.\n\nI kept the problem as simple as possible, however a general solution for a directed weighted graph would be nice as well.\n\nThanks for any and all help!\n\nUpdate: I just did some more research and realized this can be reinterpreted as a modified connectivity problem. Maybe there is a solution along that line of thinking?\n    ", "Answer": "\r\nThis is indeed an NP-hard problem. If you're handy with convex optimization or can learn, my instinct is to formulate this problem as an exact cover integer program with many variables (one per subset of |V|/k vertices) and then actually solve that program by generating columns with an integer program solver to find partitions with many interior edges. The subproblem formulation will look like\n\n```\nmaximize sum_{vertices v} w_v x_v + sum_{edges uv} y_{uv}\nsubject to\nsum_{vertices v} x_v = |V|/k\ny_{uv} <= x_u for all edges uv\ny_{uv} <= x_v for all edges uv\nx_v in {0, 1} for all vertices v\ny_{uv} in {0, 1} for all edges uv\n```\n\n\nwhere ```\nw_v```\n are weights determined by the dual solution to the master problem, intuitively understood as how urgently a particular vertex needs coverage.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to map SAX (Symbolic Aggregrate Approximation) back to time series\r\n                \r\nI converted my long time series into Piecewise Polynomial Approximation and then into symbolic form using SAX representation. Basically, I am using these algorithm to detect motifs (repeating patterns in data). After identifying repeated patterns in the symbols, how do I map these symbols back to the time series so that I can isolate motifs within the time series?\n    ", "Answer": "\r\nThe authors of the paper, \"Visualizing Variable-Length Time Series Motifs\" solved this problem by storing the starting position of each SAX word and its corresponding length. \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm to approximate a unknown function with a given functional form?\r\n                \r\nSuppose i have a function f defined on the positives reals that i can only evaluate values from : for a given x, i have an algo that compute f(x) but that's it (and it's costly..)\n\nI want to approximate by another function $g$ with parameters a_i and b_i and the following form : \n\ng(x) = sum_{i=1}^n [ a_i/(b_i - x) ]\n\nFor a given number n of couples (a_i,b_i).\n\nWhat algorithm could i use to do such thing ? \n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "What is the difference between a 'combinatorial algorithm' and a 'linear algorithm'?\r\n                \r\nOr rather, what is the definition of a combinatorial algorithm and a linear algorithm, resp.?\n\nTo make it clear because obviously the first responders misunderstood the question: I am not looking for a definition of an algorithm running in linear time vs non-linear time. A linear algorithm is somehow related to linear programming, which is a technique for finding or approximating solutions to linear optimization problems.\n\nSince NP-hard problems are so hard, there is a whole field trying to find approximate solutions. The traveling salesman problem for instance has several approximate solutions which run in polynomial time and produce a solution which is within a given bound of the best solution.\n\nSome of these approximating algorithms are called a linear algorithm, others a combinatorial algorithm; and the latter seems to be preferred (Why?). These are the two concepts I would like to understand.\n    ", "Answer": "\r\nThe issue is one of problem formulation.  \n\nJust as you said Traveling Salesperson Problem (TSP) is NP-hard precisely because it has a discrete problem formulation (the salesperson either visits a city or not at a particular time).  This discrete formulation makes the problem, and it's algorithm, combinatorial.  (Note that not all combinatorial problems are NP-hard; consider sorting algorithms.)\n\nHowever, the Linear-Programming (LP) relaxation of TSP results in a linear algorithm.  This is because the problem has been reformulated such that the salesperson visits a city a certain proportion of the time.  The main reason for using an LP relaxation is because the relaxed version can be solved in polynomial time.  However, the solution to the LP relaxation is not necessarily a solution to the original problem.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Find all maximal complete bipartite subgraph from given bipartite graph\r\n                \r\nGiven is a bipartite graph, and we want to list all maximal complete bipartite sub-graph.\n\nFor instance, \n\nvertex set L = {A, B, C, D}\n\nvertex set R = {a, b, c, d, e}\n\nedges: A-a, A-b, B-a, B-b, C-c, C-d, D-c, D-d, D-e\n\nThe maximal complete bipartite are:\n\n{A,B}-{a, b}\n\n{C,D}-{c, d}\n\n{D} - {c, d, e}\n\nI have found a brute force algorithm, O(2^n).\nI don't know if some approximation algorithm or randomized algorithm.\n    ", "Answer": "\r\nYou can transform the problem to finding maximal cliques by adding edges between every pair of vertices in each part of the bipartite graph.\n\nThe Bron-Kerbosch algorithm can be used to list all maximal cliques in a graph (not necessarily bipartite). It is pretty easy to implement and has a slightly better worst-case time bound of O(3^(n/3)). There is also a fix-parameter tractable time bound in term of the degeneracy of the graph.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Optimizations for pow() with const non-integer exponent?\r\n                \r\nI have hot spots in my code where I'm doing ```\npow()```\n taking up around 10-20% of my execution time.\n\nMy input to ```\npow(x,y)```\n is very specific, so I'm wondering if there's a way to roll two ```\npow()```\n approximations (one for each exponent) with higher performance:\n\n\nI have two constant exponents: 2.4 and 1/2.4.\nWhen the exponent is 2.4, x will be in the range (0.090473935, 1.0].\nWhen the exponent is 1/2.4, x will be in the range (0.0031308, 1.0].\nI'm using SSE/AVX ```\nfloat```\n vectors.  If platform specifics can be taken advantage of, right on!\n\n\nA maximum error rate around 0.01% is ideal, though I'm interested in full precision (for ```\nfloat```\n) algorithms as well.\n\nI'm already using a fast ```\npow()```\n approximation, but it doesn't take these constraints into account.  Is it possible to do better?\n    ", "Answer": "\r\nAnother answer because this is very different from my previous answer, and this is blazing fast. Relative error is 3e-8. Want more accuracy? Add a couple more Chebychev terms. It's best to keep the order odd as this makes for a small discontinuity between 2^n-epsilon and 2^n+epsilon.\n\n```\n#include <stdlib.h>\n#include <math.h>\n\n// Returns x^(5/12) for x in [1,2), to within 3e-8 (relative error).\n// Want more precision? Add more Chebychev polynomial coefs.\ndouble pow512norm (\n   double x)\n{\n   static const int N = 8;\n\n   // Chebychev polynomial terms.\n   // Non-zero terms calculated via\n   //   integrate (2/pi)*ChebyshevT[n,u]/sqrt(1-u^2)*((u+3)/2)^(5/12)\n   //   from -1 to 1\n   // Zeroth term is similar except it uses 1/pi rather than 2/pi.\n   static const double Cn[N] = { \n       1.1758200232996901923,\n       0.16665763094889061230,\n      -0.0083154894939042125035,\n       0.00075187976780420279038,\n      // Wolfram alpha doesn't want to compute the remaining terms\n      // to more precision (it times out).\n      -0.0000832402,\n       0.0000102292,\n      -1.3401e-6,\n       1.83334e-7};\n\n   double Tn[N];\n\n   double u = 2.0*x - 3.0;\n\n   Tn[0] = 1.0;\n   Tn[1] = u;\n   for (int ii = 2; ii < N; ++ii) {\n      Tn[ii] = 2*u*Tn[ii-1] - Tn[ii-2];\n   }   \n\n   double y = 0.0;\n   for (int ii = N-1; ii >= 0; --ii) {\n      y += Cn[ii]*Tn[ii];\n   }   \n\n   return y;\n}\n\n\n// Returns x^(5/12) to within 3e-8 (relative error).\ndouble pow512 (\n   double x)\n{\n   static const double pow2_512[12] = {\n      1.0,\n      pow(2.0, 5.0/12.0),\n      pow(4.0, 5.0/12.0),\n      pow(8.0, 5.0/12.0),\n      pow(16.0, 5.0/12.0),\n      pow(32.0, 5.0/12.0),\n      pow(64.0, 5.0/12.0),\n      pow(128.0, 5.0/12.0),\n      pow(256.0, 5.0/12.0),\n      pow(512.0, 5.0/12.0),\n      pow(1024.0, 5.0/12.0),\n      pow(2048.0, 5.0/12.0)\n   };\n\n   double s;\n   int iexp;\n\n   s = frexp (x, &iexp);\n   s *= 2.0;\n   iexp -= 1;\n\n   div_t qr = div (iexp, 12);\n   if (qr.rem < 0) {\n      qr.quot -= 1;\n      qr.rem += 12;\n   }\n\n   return ldexp (pow512norm(s)*pow2_512[qr.rem], 5*qr.quot);\n}\n```\n\n\nAddendum: What's going on here?\nPer request, the following explains how the above code works.\n\nOverview\nThe above code defines two functions, ```\ndouble pow512norm (double x)```\n and ```\ndouble pow512 (double x)```\n. The latter is the entry point to the suite; this is the function that user code should call to calculate x^(5/12). The function ```\npow512norm(x)```\n uses Chebyshev polynomials to approximate x^(5/12), but only for x in the range [1,2]. (Use ```\npow512norm(x)```\n for values of x outside that range and the result will be garbage.)\n\nThe function ```\npow512(x)```\n splits the incoming ```\nx```\n into a pair ```\n(double s, int n)```\n such that ```\nx = s * 2^n```\n and such that 1≤```\ns```\n<2. A further partitioning of ```\nn```\n into ```\n(int q, unsigned int r)```\n such that ```\nn = 12*q + r```\n and ```\nr```\n is less than 12 lets me split the problem of finding x^(5/12) into parts:\n\n\n```\nx^(5/12)=(s^(5/12))*((2^n)^(5/12))```\n via (uv)^a=(u^a)(v^a) for positive u,v and real a.\n```\ns^(5/12)```\n is calculated via ```\npow512norm(s)```\n.\n```\n(2^n)^(5/12)=(2^(12*q+r))^(5/12)```\n via substitution.\n```\n2^(12*q+r)=(2^(12*q))*(2^r)```\n via ```\nu^(a+b)=(u^a)*(u^b)```\n for positive u, real a,b.\n```\n(2^(12*q+r))^(5/12)=(2^(5*q))*((2^r)^(5/12))```\n via some more manipulations.\n```\n(2^r)^(5/12)```\n is calculated by the lookup table ```\npow2_512```\n.\nCalculate ```\npow512norm(s)*pow2_512[qr.rem]```\n and we're almost there. Here ```\nqr.rem```\n is the ```\nr```\n value calculated in step 3 above. All that is needed is to multiply this by ```\n2^(5*q)```\n to yield the desired result.\nThat is exactly what the math library function ```\nldexp```\n does.\n\n\nFunction Approximation\nThe goal here is to come up with an easily computable approximation of f(x)=x^(5/12) that is 'good enough' for the problem at hand. Our approximation should be close to f(x) in some sense. Rhetorical question: What does 'close to' mean? Two competing interpretations are minimizing the mean square error versus minimizing the maximum absolute error.\n\nI'll use a stock market analogy to describe the difference between these. Suppose you want to save for your eventual retirement. If you are in your twenties, the best thing to do is to invest in stocks or stock market funds. This is because over a long enough span of time, the stock market on average beats any other investment scheme. However, we've all seen times when putting money into stocks is a very bad thing to do. If you are in your fifties or sixties (or forties if you want to retire young) you need to invest a bit more conservatively. Those downswings can wreak have on your retirement portfolio.\n\nBack to function approximation: As the consumer of some approximation, you are typically worried about the worst-case error rather than the performance \"on average\". Use some approximation constructed to give the best performance \"on average\" (e.g. least squares) and Murphy's law dictates that your program will spend a whole lot of time using the approximation exactly where the performance is far worse than average. What you want is a minimax approximation, something that minimizes the maximum absolute error over some domain. A good math library will take a minimax approach rather than a least squares approach because this lets the authors of the math library give some guaranteed performance of their library.\n\nMath libraries typically use a polynomial or a rational polynomial to approximate some function f(x) over some domain a≤x≤b. Suppose the function f(x) is analytic over this domain and you want to approximate the function by some polynomial p(x) of degree N. For a given degree N there exists some magical, unique polynomial p(x) such that p(x)-f(x) has N+2 extrema over [a,b] and such that the absolute values of these N+2 extrema are all equal to one another. Finding this magical polynomial p(x) is the holy grail of function approximators.\n\nI did not find that holy grail for you. I instead used a Chebyshev approximation. The Chebyshev polynomials of the first kind are an orthogonal (but not orthonormal) set of polynomials with some very nice features when it comes to function approximation. The Chebyshev approximation oftentimes is very close to that magical polynomial p(x). (In fact, the Remez exchange algorithm that does find that holy grail polynomial typically starts with a Chebyshev approximation.)\n\npow512norm(x)\nThis function uses Chebyshev approximation to find some polynomial p*(x) that approximates x^(5/12). Here I'm using p*(x) to distinguish this Chebyshev approximation from the magical polynomial p(x) described above. The Chebyshev approximation p*(x) is easy to find; finding p(x) is a bear. The Chebyshev approximation p*(x) is sum_i Cn[i]*Tn(i,x), where the Cn[i] are the Chebyshev coefficients and Tn(i,x) are the Chebyshev polynomials evaluated at x.\n\nI used Wolfram alpha to find the Chebyshev coefficients ```\nCn```\n for me. For example, this calculates ```\nCn[1]```\n. The first box after the input box has the desired answer, 0.166658 in this case. That's not as many digits as I would like. Click on 'more digits' and voila, you get a whole lot more digits. Wolfram alpha is free; there is a limit on how much computation it will do. It hits that limit on higher order terms. (If you buy or have access to mathematica you will be able to calculate those high-order coefficients to a high degree of precision.)\n\nThe Chebyshev polynomials Tn(x) are calculated in the array ```\nTn```\n. Beyond giving something very close to magical polynomial p(x), another reason for using Chebyshev approximation is that the values of those Chebyshev polynomials are easily calculated: Start with ```\nTn[0]=1```\n and ```\nTn[1]=x```\n, and then iteratively calculate ```\nTn[i]=2*x*Tn[i-1] - Tn[i-2]```\n. (I used 'ii' as the index variable rather than 'i' in my code. I never use 'i' as a variable name. How many words in the English language have an 'i' in the word? How many have two consecutive 'i's?)\n\npow512(x)\n```\npow512```\n is the function that user code should be calling. I already described the basics of this function above. A few more details: The math library function ```\nfrexp(x)```\n returns the significand ```\ns```\n and exponent ```\niexp```\n for the input ```\nx```\n. (Minor issue: I want ```\ns```\n between 1 and 2 for use with ```\npow512norm```\n but ```\nfrexp```\n returns a value between 0.5 and 1.) The math library function ```\ndiv```\n returns the quotient and remainder for integer division in one swell foop. Finally, I use the math library function ```\nldexp```\n to put the three parts together to form the final answer.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "features of segmented regions of the approximation matrix in matlab\r\n                \r\nI'm using multiresolution approach for segmentation based on wavelet transform and watershed algorithm. To merge the over-segmented regions, i need to calculate the features of each segmented region of the approximation matrix (such as the mean,second and third order central moments). My problem is i don't know how to define each region separately. In other words, i don't success writing these equations with matlab.\n\n\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Divide an array of numbers into the least number of arrays whose sum is less than or equal to a given number\r\n                \r\nIf the array is: [ 10,10,10,20,20,30,39,50,50 ]\n\nAnd maxSum is 80\n\nThe function should return a list with the least number of arrays whose sum is less than or equal to 80 (in this case a list that contains three arrays):\n\narray1: [10,20,50] (sum=80)\n\narray2: [10,20,50] (sum=80)\n\narray3: [10,30,39] (sum=79)\n\nI created an approximation algorithm, but there are cases that the following code creates more arrays than the optimal solution. Could you help me to find an algorithm that gets the optimal solution?\n\n```\n    // This is my code:\n    private List<Shipment> GetShipmentsByMaxSum(double[] orderedWeights, double maxSum)\n    {\n        List<Shipment> result = new List<Shipment>();\n        Shipment shipment = new Shipment();\n        double weight = 0;\n\n        for (int i = 0; i < orderedWeights.Length; i++)\n        {\n            shipment.Weights.Add(orderedWeights[i]);\n            weight += orderedWeights[i];\n\n            double nextWeight = weight;\n            if (i + 1 < orderedWeights.Length)\n            {\n                nextWeight += orderedWeights[i + 1];\n            }\n\n            if (nextWeight > maxSum || i + 1 >= orderedWeights.Length)\n            {\n                result.Add(shipment);\n                shipment = new Shipment();\n                weight = 0;\n            }\n        }\n\n        return result;\n    }\n```\n\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Is there a way to approximate the Laplacian matrix eigenvalues by using random walk like algorithm\r\n                \r\nIt seems to me that many advanced graph analysis algorithm are based on spectral graph analysis, relying more specifically on the Laplacian matrix properties.\n\nI know there are some alternative for clustering that are based on random-walk type algorithms, that make no use of the Laplacian matrix factorization. \n\nI am curious if there exists anything to go a bit further and determine the Laplacian matrix eigenvalues (especially the second one), without using spectral method, but more like wandering on the graph.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithms for the error function and its friends (erf, erfc etc) [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is opinion-based. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.\r\n                \r\n                    \r\n                        Closed 9 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI've been looking at algorithms used to calculate various functions, such as the CORDIC algorithm for tigonometric functions.\n\nI was wondering how the error function is usually calculated. Wikipedia has a number of approximations, but is there one algorithm that is generally preferred when coding the error functions for numerical computing?\n    ", "Answer": "\r\nYour best bet is to check what actual implementations do, here is a selection:\n\n\nBoost: http://www.boost.org/doc/libs/1_55_0/boost/math/special_functions/erf.hpp\nGNU Scientific Library: http://www.gnu.org/software/gsl/\nGLibc http://www.gnu.org/software/libc/index.html\n\n\nThere are probably others.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximately evaluate the time needed for running an algorithm by CPU frequency\r\n                \r\nI just want to know, is that possible to evaluate approximately (to 1/6 hour) the time needed for executing an program ( an algorithm for example) , knowing the all the source codes and the frequency of the CPU of my computer ? \n    ", "Answer": "\r\nNo, because there's more to speed than CPU frequencies.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Optimizing an algorithm that approximates the euler number (e) in C\r\n                \r\nFor starters, I'm reinventing the wheel here. I know there is a constant in C for Euler's number. As I find it easier to create scientific programs than any other type of problem, I use these problems to practice and become a good developer.\n```\n/* calculating euler number */\n\n#include <stdio.h>\nint fat(int x);\nint main(void)\n\n{\n    int n; int y; double en = 1.0; //en = euler number\n    \n    printf(\"Entre com o valor de n para a aproximação do número de Euler: \\n\");//prompt\n    scanf(\"%d\", &n);//read n\n    //n big enough -> aproximation for en is close of real value of euler number\n    if (n < 0){ //if the user input a negative value\n        printf(\"Error!\\n\");\n    }\n    else {//if the user is not a troll and enter a positive integer\n        for (y = 1; y <= n; y++){ \n            en = en + (1.0/(fat(y))); // en = summation(1/fat(y))\n            //printf(\"y = %d and en = %.9lf\\n\", y, en); this line is for studying when en return 'inf'\n\n        }\n        printf(\"O valor aproximado de e é aproximadamente %.9lf\\n\", en);//print euler number aproximation\n    }\n}\nint fat(int x){//defining the factorial function\n    int factorial = 1;\n    int f;\n    for (f = 1; f <= x; f++){\n        factorial = factorial * f;\n    }\n    return factorial;\n}\n```\n\nI compiled and ran the code a few times, and I noticed that the approximate value exceeds the Euler value by n=19, en=2.718281835 (the last two digits should be 28). For n greater than 33, the program returns en = inf. Probably because factorial of 34 is already too huge a value for my code.\nMy question is: how to take advantage of the idea behind this code and make a more optimized program, for example, an algorithm that does not exceed the value of the Euler number.\nI know, my code isn't very good, but it's what I managed to build without consulting anything but a math book on how to get the Euler number. Thanks in advance!\n    ", "Answer": "\r\nYou can avoid calculating the factorial completly, if you store this ```\n1.0/(fat(y))```\n in a variable, and divide it by progressing ```\ny```\ns.\nThat way you should only hit an obstacle when the precision of your datatypes starts failing.\n```\ndouble term = 1.0;\nfor (y = 1; y <= n; ++y){\n    term /= y;\n    en += term; // en = summation(1/fat(y))\n}\n```\n\nOtherwise the usual step would help, to use wider/preciser datatypes, like ```\nlong int```\n. But I supposed that is not the optimisation aspect you are looking for here.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Log2 approximation in fixed-point\r\n                \r\nI'v already implemented fixed-point log2 function using lookup table and low-order polynomial approximation but not quite happy with accuracy across the entire 32-bit fixed-point range [-1,+1). The input format is s0.31 and the output format is s15.16. \n\nI'm posting this question here so that another user can post his answer (some comments were exchanged in another thread but they prefer to provide comprehensive answer in a separate thread). Any other answers are welcome, I would much appreciate if you could provide some speed vs accuracy details of your algorithm and its implementation. \n\nThanks.\n    ", "Answer": "\r\nBy simply counting the leading zero bits in a fixed-point number ```\nx```\n, one can determine ```\nlog2(x)```\n to the closest strictly smaller integer. On many processor architectures, there is a \"count leading zeros\" machine instruction or intrinsic. Where this is not available, a fairly efficient implementation of ```\nclz()```\n can be constructed in a variety of ways, one of which is included in the code below.\n\nTo compute the fractional part of the logarithm, the two main obvious contenders are interpolation in a table and minimax polynomial approximation. In this specific case, quadratic interpolation in a fairly small table seems to be the more attractive option. x = 2i * (1+f), with 0 ≤ f < 1. We determine ```\ni```\n as described above and use the leading bits of ```\nf```\n to index into the table. A parabola is fit through this and two following table entries, computing the parameters of the parabola on the fly. The result is rounded, and a heuristic adjustment is applied to partially compensate for the truncating nature of fixed-point arithmetic. Finally, the integer portion is added, yielding the final result.\n\nIt should be noted that the computation involves right shifts of signed integers which may be negative. We need those right shifts to map to arithmetic right shifts at machine code level, something which is not guaranteed by the ISO-C standard. However, in practice most compilers do what is desired. In this case I used the Intel compiler on an x64 platform running Windows.\n\nWith a 66-entry table of 32-bit words, the maximum absolute error can be reduced to  8.18251e-6, so full ```\ns15.16```\n accuracy is achieved.\n\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <stdint.h>\n#include <math.h>\n\n#define FRAC_BITS_OUT (16)\n#define INT_BITS_OUT  (15)\n#define FRAC_BITS_IN  (31)\n#define INT_BITS_IN   ( 0)\n\n/* count leading zeros: intrinsic or machine instruction on many architectures */\nint32_t clz (uint32_t x)\n{\n    uint32_t n, y;\n\n    n = 31 + (!x);\n    if ((y = (x & 0xffff0000U))) { n -= 16;  x = y; }\n    if ((y = (x & 0xff00ff00U))) { n -=  8;  x = y; }\n    if ((y = (x & 0xf0f0f0f0U))) { n -=  4;  x = y; }\n    if ((y = (x & 0xccccccccU))) { n -=  2;  x = y; }\n    if ((    (x & 0xaaaaaaaaU))) { n -=  1;         }\n    return n;\n}\n\n#define LOG2_TBL_SIZE (6)\n#define TBL_SIZE      ((1 << LOG2_TBL_SIZE) + 2)\n\n/* for i = [0,65]: log2(1 + i/64) * (1 << 31) */\nconst uint32_t log2Tab [TBL_SIZE] =\n{\n    0x00000000, 0x02dcf2d1, 0x05aeb4dd, 0x08759c50, \n    0x0b31fb7d, 0x0de42120, 0x108c588d, 0x132ae9e2, \n    0x15c01a3a, 0x184c2bd0, 0x1acf5e2e, 0x1d49ee4c, \n    0x1fbc16b9, 0x22260fb6, 0x24880f56, 0x26e2499d, \n    0x2934f098, 0x2b803474, 0x2dc4439b, 0x30014ac6, \n    0x32377512, 0x3466ec15, 0x368fd7ee, 0x38b25f5a, \n    0x3acea7c0, 0x3ce4d544, 0x3ef50ad2, 0x40ff6a2e, \n    0x43041403, 0x450327eb, 0x46fcc47a, 0x48f10751, \n    0x4ae00d1d, 0x4cc9f1ab, 0x4eaecfeb, 0x508ec1fa, \n    0x5269e12f, 0x5440461c, 0x5612089a, 0x57df3fd0, \n    0x59a80239, 0x5b6c65aa, 0x5d2c7f59, 0x5ee863e5, \n    0x60a02757, 0x6253dd2c, 0x64039858, 0x65af6b4b, \n    0x675767f5, 0x68fb9fce, 0x6a9c23d6, 0x6c39049b, \n    0x6dd2523d, 0x6f681c73, 0x70fa728c, 0x72896373, \n    0x7414fdb5, 0x759d4f81, 0x772266ad, 0x78a450b8, \n    0x7a231ace, 0x7b9ed1c7, 0x7d17822f, 0x7e8d3846, \n    0x80000000, 0x816fe50b\n};\n\n#define RND_SHIFT     (31 - FRAC_BITS_OUT)\n#define RND_CONST     ((1 << RND_SHIFT) / 2)\n#define RND_ADJUST    (0x10d) /* established heuristically */\n\n/* \n   compute log2(x) in s15.16 format, where x is in s0.31 format\n   maximum absolute error 8.18251e-6 @ 0x20352845 (0.251622232)\n*/   \nint32_t fixed_log2 (int32_t x)\n{\n    int32_t f1, f2, dx, a, b, approx, lz, i, idx;\n    uint32_t t;\n\n    /* x = 2**i * (1 + f), 0 <= f < 1. Find i */\n    lz = clz (x);\n    i = INT_BITS_IN - lz;\n    /* normalize f */\n    t = (uint32_t)x << (lz + 1);\n    /* index table of log2 values using LOG2_TBL_SIZE msbs of fraction */\n    idx = t >> (32 - LOG2_TBL_SIZE);\n    /* difference between argument and smallest sampling point */\n    dx = t - (idx << (32 - LOG2_TBL_SIZE));\n    /* fit parabola through closest three sampling points; find coeffs a, b */\n    f1 = (log2Tab[idx+1] - log2Tab[idx]);\n    f2 = (log2Tab[idx+2] - log2Tab[idx]);\n    a = f2 - (f1 << 1);\n    b = (f1 << 1) - a;\n    /* find function value for argument by computing ((a*dx+b)*dx) */\n    approx = (int32_t)((((int64_t)a)*dx) >> (32 - LOG2_TBL_SIZE)) + b;\n    approx = (int32_t)((((int64_t)approx)*dx) >> (32 - LOG2_TBL_SIZE + 1));\n    approx = log2Tab[idx] + approx;\n    /* round fractional part of result */\n    approx = (((uint32_t)approx) + RND_CONST + RND_ADJUST) >> RND_SHIFT;\n    /* combine integer and fractional parts of result */\n    return (i << FRAC_BITS_OUT) + approx;\n}\n\n/* convert from s15.16 fixed point to double-precision floating point */\ndouble fixed_to_float_s15_16 (int32_t a)\n{\n    return a / 65536.0;\n}\n\n/* convert from s0.31 fixed point to double-precision floating point */\ndouble fixed_to_float_s0_31 (int32_t a)\n{\n    return a / (65536.0 * 32768.0);\n}\n\nint main (void)\n{\n    double a, res, ref, err, maxerr = 0.0;\n    int32_t x, start, end;\n\n    start = 0x00000001;\n    end =   0x7fffffff;\n    printf (\"testing fixed_log2 with inputs in [%17.10e, %17.10e)\\n\",  \n            fixed_to_float_s0_31 (start), fixed_to_float_s0_31 (end));\n\n    for (x = start; x < end; x++) {\n        a = fixed_to_float_s0_31 (x);\n        ref = log2 (a);\n        res = fixed_to_float_s15_16 (fixed_log2 (x));\n        err = fabs (res - ref);\n        if (err > maxerr) {\n            maxerr = err;\n        }\n    }\n\n    printf (\"max. err = %g\\n\", maxerr);\n    return EXIT_SUCCESS;\n}\n```\n\n\nFor completeness, I am showing the minimax polynomial approximation below. The coefficients for such approximations can be generated by several tools such as Maple, Mathematica, Sollya or with homebrew code using the Remez algorithm, which is what I used here. The code below shows the original floating-point coefficients, the dynamic scaling used to maximize accuracy in intermediate computation, and the heuristic adjustments applied to mitigate the impact of non-rounding fixed-point arithmetic.\n\nA typical approach for computation of ```\nlog2(x)```\n is to use x = 2i * (1+f) and use approximation of log2(1+f) for (1+f) in [√½, √2], which means that we use a polynomial ```\np(f)```\n on the primary approximation interval [√½-1, √2-1].\n\nThe intermediate computation scales up operands as far as feasible for improved accuracy under the restriction that we want to use a 32-bit ```\nmulhi```\n operation as its basic building block, as this is a native instruction on many 32-bit architectures, accessible either via inline machine code or as an intrinsic. As in the table-based code, there are right shifts of signed data which may be negative, and such right shifts must map to arithmetic right shifts, something that ISO-C doesn't guarantee but most C compilers do.\n\nI managed to get the maximum absolute error for this variant down to 1.11288e-5, so almost full ```\ns15.16```\n accuracy but slightly worse than for the table-based variant. I suspect I should have added one additional term to the polynomial. \n\n```\n/* on 32-bit architectures, there is often an instruction/intrinsic for this */\nint32_t mulhi (int32_t a, int32_t b)\n{\n    return (int32_t)(((int64_t)a * (int64_t)b) >> 32);\n}\n\n#define RND_SHIFT  (25 - FRAC_BITS_OUT)\n#define RND_CONST  ((1 << RND_SHIFT) / 2)\n#define RND_ADJUST (-2) /* established heuristically */\n\n/* \n    compute log2(x) in s15.16 format, where x is in s0.31 format\n    maximum absolute error 1.11288e-5 @ 0x5a82689f (0.707104757)\n*/   \nint32_t fixed_log2 (int32_t x)\n{\n    int32_t lz, i, f, p, approx;\n    uint32_t t;\n    /* x = 2**i * (1 + f), 0 <= f < 1. Find i */\n    lz = clz (x);\n    i = INT_BITS_IN - lz;\n    /* force (1+f) into range [sqrt(0.5), sqrt(2)] */\n    t = (uint32_t)x << lz;    \n    if (t > (uint32_t)(1.414213562 * (1U << 31))) {\n        i++;\n        t = t >> 1;\n    }\n    /* compute log2(1+f) for f in [-0.2929, 0.4142] */\n    f = t - (1U << 31);\n    p =              + (int32_t)(-0.206191055 * (1U << 31) -  1);\n    p = mulhi (p, f) + (int32_t)( 0.318199910 * (1U << 30) - 18);\n    p = mulhi (p, f) + (int32_t)(-0.366491705 * (1U << 29) + 22);\n    p = mulhi (p, f) + (int32_t)( 0.479811855 * (1U << 28) -  2);\n    p = mulhi (p, f) + (int32_t)(-0.721206390 * (1U << 27) + 37);\n    p = mulhi (p, f) + (int32_t)( 0.442701618 * (1U << 26) + 35);\n    p = mulhi (p, f) + (f >> (31 - 25));\n    /* round fractional part of the result */\n    approx = (p + RND_CONST + RND_ADJUST) >> RND_SHIFT;\n    /* combine integer and fractional parts of result */\n    return (i << FRAC_BITS_OUT) + approx;\n}\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How do you normalize weights q-learning with linear function approximation\r\n                \r\nI am developing simple game program to show q-learning with linear function approximation.\nscreen shot\n\nIn this game, there are uncountable state. I have to consider many factors like player's position, speed, and enemy's position (there are 12 ~ 15 enemy objects). I ended up changing my algorithm from using table to use linear function approximation.\n\nI decided around 20 ~ 22 features.(constant, player position, player speed, all of enemies position). and there is \n\nAfter implementing that algorithm, I got stuck in some problem.\n\nWeight value is overflowed in a few second after running my program. I found that I didn't normalize features and weight.\n\nIt was easy to normalize feature value because each feature has their bound .\nHowever, It wasn't enough to normalize only feature value. \nIt still end up overflow. \n\nMy problem is how do I normalize my weights. \n\nBelow is my code to implement to normalize features.\n\n```\n//f is feature \n\n    f[0] = 1;\n    f[1] = this.getNormMinMax(this.player.x,0,cc.winSize.width);\n    f[2] = this.getNormMinMax(this.player.vel,-80,80);\n\n    for(var i=0; i<pooList.length;++i)\n    {\n        f[3 + 2*i] = this.getNormMinMax(pooList[i].x,0,cc.winSize.width);\n        f[3 + 2*i+1] = this.getNormMinMax(pooList[i].y,0,cc.winSize.height*3);\n    }\n```\n\n\nAnd this below code is updating weight without any normalization.\n\n```\nfor(var i=0; i<this.featureSize; ++i)\n        {\n            var w = this.weightArray[this.doAction][i];\n            this.weightArray[this.doAction][i] =\n                w + this.learningRate*(this.reward + this.discountFactor*maxAction - this.updateQSA) * f[i];\n        }\n```\n\n    ", "Answer": "\r\nIt seems you're using Linear Regression without regularization, and there are collinear features. Try adding L1 or L2 regularization (use Ridge, Lasso or Elastic Net models).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "what is twitter's interest in abstract algebra? [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is off-topic. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\r\n                \r\n                    \r\n                        Closed 10 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nWhy would a company like Twitter be interest in algebraic concepts like groups, monoids and rings. https://github.com/twitter/algebird\n\nAll I could find is:\n\n\n  Implementations of Monoids for interesting approximation algorithms,\n  such as Bloom filter, HyperLogLog and CountMinSketch. These allow you\n  to think of these sophisticated operations like you might numbers, and\n  add them up in hadoop or online to produce powerful statistics and\n  analytics.\n\n\nand in another part of the GitHub page:\n\n\n  It was originally developed as part of Scalding's Matrix API, where\n  Matrices had values which are elements of \n  Monoids, Groups, or Rings. Subsequently, it was clear that the code had broader application\n  within Scalding and on other projects within Twitter.\n\n\nWhat could this broader application be? within Twitter and for general interest?\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "C++ Graph Vertex Coloring Library or Source Code\r\n                \r\nIs there a C++ (or any other language) library with a portfolio of algorithms for the problem of graph coloring?\n\nThere are of course naive greedy vertex coloring algorithms, but I'm interested in more interesting algorithms like:\n\n\nAlgorithms mentioned in the \"Exact Algorithms\" section of the wiki\nApproximation algorithms that take advantage of special graph properties like the graph being planar or a unit disk graph.\nAlgorithms that find the fractional coloring of a graph.\n\n\nThat last one is of particular importance to me.\n\nWhat I found so far is the list on this page but none of them have any of the above algorithms. Moreover, the best one is Joe Culberson's Graph Coloring code and that was implemented in late 90's, so is very much outdated in terms of not having a documented API (not that this is important for what this question is about, but I thought I'd mention it).\n\nThere's the Koala graph coloring library that has the spirit of what I'm looking for, but if you look at their source code it has not delivered on the promise just yet. It appears to be in very early stages of development.\n\nOther general graph libraries are mentioned in this stackoverflow question. They include:\n\n\nGraphviz\nBoost Graph Library\nLemon\nigraph\nOGDF\n\n\nI should note that I use the Boost Graph Library for a lot of things. In fact, it provides a naive vertex coloring implementation. Joe Culberson's code (mentioned above) does much more.\n\nThe following is a list of graph coloring code, I've found (and tested in most cases) but they still mostly fall short in terms of the three algorithm classes above.\n\n\nGraphCol - documentation is not in English, sigh.\nPlanarity - contains a coloring algorithm that guarantees a 5-coloring or better for planar graphs.\nGraph-Coloring - appears to be a re-implementation of a small number of algorithms already implemented by Joe Culberson (above).\n\n    ", "Answer": "\r\nThere's some good ones at http://rhydlewis.eu/gcol/. These correspond to a portfolio of algorithms reviewed and tested in my book:\nLewis, R. (2021) A Guide to Graph Colouring: Algorithms and Applications (second ed.). Springer. ISBN: 978-3-030-81053-5. https://link.springer.com/book/10.1007/978-3-030-81054-2\nThese include greedy, backtracking and metaheuristic approaches. I've included compilation instructions etc. in the above link.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Need Algorithm to group files of varying sizes into approximately equal blocks\r\n                \r\nI'm trying to figure out an algorithm that will help me group an assortment of files of varying sizes into say, 'n' groups of approximately equal size.\n\nAny ideas on how to achieve this?\n    ", "Answer": "\r\n```\nFind the target group size. This is the sum of all sizes divided by n.\nCreate a list of sizes.\nSort the files decreasing in size. \nfor each group\n    while the remaining space in your group is bigger than the first element of the list\n        take the first element of the list and move it to the group\n    for each element\n        find the elemnet for which the difference between group size and target group size is minimal\n    move this elemnt to the group\n```\n\n\nThis doesn't produce optimal results, but is easy to implement and gets you good results. For the optimal solution you need an exhaustive search which is NP complete.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "C# fast chart approximate comparison algorithm\r\n                \r\nI have 2 series of values. By showing them in a chart, they are quite similar (they have the same behavior, when one grows, the other one grows, when one has small values, the other has small values; BUT: It may occur that one is growing and the other is descending). Important is that they do not have strictly the same values. I am interested in the general behavior of these 2 charts.\n\nRight now, I am comparing them by doing the average on both graphs, and building 2 bool arrays. when a value is above the average, the corresponding value in bool[] is true, otherwise false. Then I compare these 2 bool[], by using Hamming distance.\nThis algorithm works, but not as good as I wished it would work. (It doesn't detect all matches between charts).\n\nDoes anyone have an idea of a better algorithm for performing this operation?\n    ", "Answer": "\r\nA simple way of calculating a distance between two lists of numbers is to calculate the sum of the squares of the differences between them. This used quite often in statistics.\n\n```\ndouble sum = 0.0;\nfor (int i = 0; i < N; i++) {\n    double diff = a[i] - b[i];\n    sum += diff*diff;\n}\n```\n\n\nBut the fundamental question is, what kind of statement you are expecting from such an analysis.\n\n\n\nAnother possibility is to calculate the Correlation Coefficient between the two series. A coefficient of ```\n+1```\n means that the two series fit 100%, ```\n0```\n means that there is no apparent relation between the two series and ```\n-1```\n means that they are the pure opposite of each other.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Effective multi-search algorithms\r\n                \r\nReason for posting question: My programming experience is somewhere between amateur and moderate (but I'm pretty rusty at this point). I'll probably be working with C++, but I would be willing to switch to Python. I have some experience with basic sort/search algorithms like binary, M, A*, etc. I've heard that A* can be a pretty efficient search algorithm, but that when multiple search targets start being desired that there are some big losses on efficiency in A* compared to other algorithms. I'm looking into a multiple search/optimization problems with a multidimensional problem space, so efficiency is really going to start to matter.\n\nMost multi-search algorithms I've seen are referred to as string searching algorithms. I wanted to inquire how well these algorithms work on other types of problems, or possibly be provided recommendations for other, more effective algorithms for the scenario I am providing. I do admit I need to do more research to understand the difference between optimization and multi-search algorithms, but the current ideas I have seem to work well with multi-search algorithms. I am looking into potential energy surfaces and finding rough approximations of local minima.\n\nImagine a something like a hilly surface. Now let's populate a 3D graph of the potential energies of an object at any x and y positions as a function of the two position coordinates. I am interested in finding all the local minima within a defined bound of this surface. I need algorithm(s) that allow for me to sample the surface at some resolution and then start searching the lowest points for local minima. In essence I was thinking of creating a low resolution mesh with some kind of constrained breadth-first search, then using another smart algorithm to increase the resolution of the mesh at the lower valued points. Ideally the algorithms would be used multi-threaded, but they need to be able to support an arbitrary dimensional PES. I have a black-box evaluation function that provides the potential energy. Here's a 2+1 dimensional illustration below.   \n\n    \n\nEnvision the redline was 1 of the dimensions of the initial sampling mesh, and it was actually crossing near the local minima. The algorithm then will start to increase mesh resolution around the valleys for a few steps prior to selecting the lowest value at that region. It will then move on to another low point.\n    ", "Answer": "\r\nYou are trying to solve a Global Optimization problem.\n\nhttp://en.wikipedia.org/wiki/Global_optimization is a good reference that has a number of links to methods used to solve this problem. I've used Simulated Annealing successfully before but the solution to fit your problem will really come down the the size and complexity of your problem space.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Suitable approximate string matching algorithm for name and addresses\r\n                \r\nI am working on a project that contains a large number of names and addresses in its database. Names such as \"John K Smith\" and \"Joe Smith\", and addresses such \"20 Theroad avenue\" or \"1345 Myplace st.\"\n\nIn this project once a user X enters the website, they will enter a name and address along with other details; the entered name and address is checked with what already exist in the database. If the name and address entered is similar enough with what exist in the database for user X, access is granted.\n\nInstead of exact string matching I need to perform approximate string matching to make the login more convenient. (I know this is a security concert but there is also username/pass which are exact matched).\n\nI am looking for a string matching algorithm that is suitable for names and addresses, in addition take into account acronyms, short forms and similar phrases such as 'ave' vs 'avenue' or 'mr' vs 'mr.' or 'street' vs 'avenue'. \n\nI have so far looked at edit distance, jarowinkler, ngram(qgram), cosine similarity and phonetic approaches. \n\nI thought maybe a hybrid approach with a custom normalization function (that does string replacement for shortforms/similar terms) is the way to go, but I am not certain yet.\n\nThis project eventually should work with other languages (Spanish and French), which may mean more custom text replacements.\n\nAny help is appreciated in finding the most suitable algorithm(s) to match names and addresses with high accuracy (with minimum number of false positives). \n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Error in π Approximation Causing Divergence on 29th Digit\r\n                \r\nI have a π approximation program that I've been working on. It uses the Rananujen-Chudnovsky series(https://stackoverflow.com/a/531/2058221) to approximate π. I test the number generated by having 2 variants of π statically loaded in memory from different sources which I test against. The empirical version that I compute consistently fails to get the number correct past 29 digits for some reason. I've used the ```\nMathContext```\n on just about everything I possibly can use it on. NOTE: ```\nbroadcastSystemMessage()```\n is essentially a ```\nSystem.out.println()```\n but makes the formatting \"nice\", essentially think of the first argument as the title + : and the second as the body.\n\nAlgorithm Implementation:\n\n```\n/**\n     * The much faster Ramanujan-Chudnovsky algorithm.\n     */\n    RAMANUJAN_CHUDNOVSKY {\n\n                private final long k1 = 545140134, k2 = 13591409, k3 = 640320,\n                k4 = 100100025, k5 = 327843840, k6 = 53360;\n\n                @Override\n                public String getAlgorithmName() {\n                    return \"Ramanujan-Chudnovsky\";\n                }\n\n                @Override\n                public String getAlgorithmFormula() {\n                    return \"S=Σ from 0 to ∞(-1^n *(6n)! * (k2 + n * k1) / ((n!) ^ 3 * (3n)! * 8 * k4 * k5) ^ n)\"\n                    + \"π = k6 * sqrt(k3) / S\";\n                }\n\n                @Override\n                public BigDecimal initCalculation(long iterations, long significantDigits) {\n                    //God, if you're real, please forgive me for this; Java didn't give me a choice.\n                    MathContext context = new MathContext((int) significantDigits);\n                    BigDecimal s = new BigDecimal(0, context);\n                    for (int n = 0; n < iterations; n++) {\n                        s = s.add(new BigDecimal(Math.pow(-1, n), context).multiply(new BigDecimal(factorial(6 * n), context), context)\n                                .multiply(new BigDecimal(BigInteger.valueOf(k2).add(BigInteger.valueOf(n).multiply(BigInteger.valueOf(k1))),\n                                                context), context).divide(new BigDecimal(factorial(n).pow(3), context).multiply(\n                                                new BigDecimal(factorial(new BigDecimal(3 * n, context).toBigInteger()), context), context)\n                                        .multiply(new BigDecimal(BigInteger.valueOf(8).multiply(BigInteger.valueOf(k4))\n                                                        .multiply(BigInteger.valueOf(k5)), context), context).pow(n, context), context), context);\n                    }\n                    Main.brodcastSystemMessage(\"Check\", k6 + \" || + \" + k3 + \" || \" + sqrt(new BigDecimal(k3, context), context));\n                    return new BigDecimal(k6, context).multiply(sqrt(new BigDecimal(k3, context), context),\n                            context).divide(s, context);\n                    //Square Root of k3 approximation: 800.19997500624804755833750301086\n                }\n            }\n```\n\n\nπ:\n\n```\npublic static final BigDecimal π = new BigDecimal(\"3.14159265358979323846264338327950288419716939937510582097494459230781640628620899862803482534211706798214808651328230664709384460955058223172535940812848111745028410270193852110555964462294895493038196442881097566593344612847564823378678316527120190914564856692346034861045432664821339360726024914127372458700660631558817488152092096282925409171536436789259036001133053054882046652138414695194151160943305727036575959195309218611738193261179310511854807446237996274956735188575272489122793818301194912983367336244065664308602139494639522473719070217986094370277053921717629317675238467481846766940513200056812714526356082778577134275778960917363717872146844090122495343014654958537105079227968925892354201995611212902196086403441815981362977477130996051870721134999999837297804995105973173281609631859502445945534690830264252230825334468503526193118817101000313783875288658753320838142061717766914730359825349042875546873115956286388235378759375195778185778053217122680661300192787661119590921642019893809525720106548586327886593615338182796823030195203530185296899577362259941389124972177528347913151557485724245415069595082953311686172785588907509838175463746493931925506040092770167113900984882401285836160356370766010471018194295559619894676783744944825537977472684710404753464620804668425906949129331367702898915210475216205696602405803815019351125338243003558764024749647326391419927260426992279678235478163600934172164121992458631503028618297455570674983850549458858692699569092721079750930295532116534498720275596023648066549911988183479775356636980742654252786255181841757467289097777279380008164706001614524919217321721477235014144197356854816136115735255213347574184946843852332390739414333454776241686251898356948556209921922218427255025425688767179049460165346680498862723279178608578438382796797668145410095388378636095068006422512520511739298489608412848862694560424196528502221066118630674427862203919494504712371378696095636437191728746776465757396241389086583264599581339\");\n```\n\n\nCalling Method:\n\n```\nBigDecimal π = PIAlgorithms.RAMANUJAN_CHUDNOVSKY.initCalculation(10,\n            1000);\n    brodcastSystemMessage(\"π Calculation Result\", π + \"\");\n    brodcastSystemMessage(\"\", StringUtils.getCommonPrefix(π.toPlainString(), PIAlgorithms.π.toPlainString()).length() + \"\");\n```\n\n\nMisc. Functions Used in Algorithm:\n\n```\n/**\n * Custom factorial function.\n *\n * @param integer The integer to use\n * @return The factorial of the number\n */\nprivate static BigInteger factorial(int integer) {\n    return factorial(BigInteger.valueOf(integer));\n}\n\n/**\n * Custom factorial function.\n *\n * @param integer The integer to use\n * @return The factorial of the number\n */\nprivate static BigInteger factorial(@NotNull BigInteger integer) {\n    if (integer.equals(BigInteger.ZERO)) {\n        return BigInteger.ONE;\n    } else if (integer.compareTo(BigInteger.ZERO) < 0) {\n        throw new IllegalArgumentException(\"Can't take the factorial of a number less than zero\");\n    }\n    BigInteger i = integer.equals(BigInteger.ONE) ? integer : integer.multiply(factorial(integer.subtract(BigInteger.ONE)));\n    System.out.println(integer + \"! --> \" + i);\n    return i;\n} \nprivate static BigDecimal sqrt(BigDecimal number, MathContext context) {\n    BigDecimal first = new BigDecimal(\"0\", context);\n    BigDecimal second = new BigDecimal(Math.sqrt(number.doubleValue()), context);\n    while (!first.equals(second)) {\n        first = second;\n        second = number.divide(first, context);\n        second = second.add(first, context);\n        second = second.divide(new BigDecimal(\"2\"), context);\n\n    }\n    return second;\n}\n```\n\n\nEdit:\nFirst 5 Terms: 3.141592653589793238462643383587297242678\nFirst 10 Terms: 3.141592653589793238462643383587297242678\nFirst 15 Terms: 3.141592653589793238462643383587297242678\nComparison<π>: 3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628\n    ", "Answer": "\r\nI think you've copied the formula incorrectly. I assume that your code implements the formula shown in ```\ngetAlgorithmFormula```\n. That formula differs from the one shown in the reference -- the parentheses for the denominator of the summand are incorrect.\n\nIn this output from Maxima, ```\ng```\n is your formula and ```\ng2```\n is the correct formula:\n\n```\n(%i118) g(N);\n          N - 1\n          ====                                n\n          \\     (545140134 n + 13591409) (- 1)  (6 n)!\n           >    --------------------------------------\n          /                         n   3 n       n\n          ====    262537412640768000  n!    (3 n)!\n          n = 0\n(%o118)   --------------------------------------------\n                       426880 sqrt(10005)\n(%i119) g2(N);\n          N - 1\n          ====                                n\n          \\     (545140134 n + 13591409) (- 1)  (6 n)!\n           >    --------------------------------------\n          /                           n   3\n          ====      262537412640768000  n!  (3 n)!\n          n = 0\n(%o119)   --------------------------------------------\n                       426880 sqrt(10005)\n```\n\n\nYour formula has the first 2 terms correct (n = 0 and n = 1). These 2 are enough to give you 28 digits correct. From then on (n >= 2) the terms are incorrect, so you are stuck with 28 digits.\n\nIs it necessary to use Java? It is pretty clumsy to handle arbitrary precision numbers. I know Maxima can handle that pretty easily, and I'm sure there are other packages too.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "AMD(approximate minimum degree) algorithm in R\r\n                \r\nIn matlab P = amd(A) returns the approximate minimum degree permutation vector for the sparse matrix A, in R does we have the same command?\n    ", "Answer": "\r\nTry the chol function in the Matrix package... not sure if this is exactly what you are looking for, but it's from the documentation for that package (\" Returns (and stores) the Cholesky decomposition of x. If pivot is true, the Approximate Minimal Degree (AMD) algorithm is used to create a reordering of the rows and columns of x so as to reduce fill-in.\")\n\n```\n> library(Matrix)\n> M2 <- toeplitz(as(c(1,.5, rep(0,12), -.1), \"sparseVector\"))\n> C2 <- chol(M2, pivot=TRUE)\n> C2\n15 x 15 sparse Matrix of class \"dtCMatrix\"\n\n\n [1,] 1 0.5000000 .         .         .         .         .         .         .        \n [2,] . 0.8660254 0.5773503 .         .         .         .         .         .        \n [3,] . .         0.8164966 0.6123724 .         .         .         .         .        \n [4,] . .         .         0.7905694 0.6324555 .         .         .         .        \n [5,] . .         .         .         0.7745967 0.6454972 .         .         .        \n [6,] . .         .         .         .         0.7637626 0.6546537 .         .        \n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "JS How make that function could not be start earlier than 10 seconds after the previous launch?\r\n                \r\n```\nfunction testworking(n){ \n   if(n == 1)\n      testuser();\n   else\n        testconfig();\n}\n\nsetInterval(function(){testworking(n)}, 1000);\n```\n\n\nHow do I make that function ```\ntestuser();```\n could not start earlier than 10 seconds after the previous launch?\n\nP.S.:\nan approximate algorithm: \n\n```\nif(n == 1){\n   if (first run function `testuser()` || \n         time after previous run `testuser();` == 10 seound){\n      testuser();\n   }\n}\n```\n\n    ", "Answer": "\r\nSet a flag using a timer:\n\n```\nvar is_waiting = false;\nfunction testuser() {\n  if (!is_waiting) {\n    //do your stuff here\n  } else {\n    alert('You must wait ten seconds before doing this again');\n  }\n  is_waiting = true;\n  setTimeout(function() {is_waiting = false}, 10000);\n}\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "spline approximation with specified number of intervals [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                     We don’t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\r\n                \r\n                    \r\n                        Closed 8 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nSo - edited because some of us thought that this question is off-topic.\n\nI need to build spline (approximation) on 100 points in one of environments listed in tags. But I need it with exact number of intervals (maximum of 6 intervals - separate equations - in whole domain). Packages / libraries in R and Maxima which I know let me for building spline on this points but with 25-30 intervals (separate equations). Does anyone know how to build spline with set number of intervals without coding whole algorithm all over again?\n    ", "Answer": "\r\nWhat you're looking for might be described as \"local regression\" or \"localized regression\"; searching for those terms might turn up some hits. \n\nI don't know if you can find exactly what you've described. But implementing it doesn't seem too complicated: (1) Split the domain into N intervals (say N=10). For each interval, (2) make a list of the data in the interval, (3) fit a low-order polynomial (e.g. cubic) to the data in the interval using least squares.\n\nIf that sounds interesting to you, I can go into details, or maybe you can work it out yourself.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "piecewise linear approximation\r\n                \r\nI'm studying the vertical profile of a railway. In a nutshell it is composed by a succession of straights and circles (tangents to the straights between which they are included). My data set is composed by survey points, measured along the railway with an uncertainty. My goal is to approximate this data set with a succession of straights and tangent circles (with the center on the bisector of the two straights in which it's included), such that certain constraints are respected (the most important concerns deviations, so i have to set lower and upper bounds).\nI'm developing an algorithm that recognize (or try to) if the survey points belong to a straight or a circle (simply, computing the change rate of gradient). After having cataloged all the points in the single lines and circles (I have the point group that belongs to the first straight, the point group that belongs to the second straights and the point group that belongs to the first circle, tangent to the first to straights.... and so on...). So, I need to approximate all the point group that belongs to tangents with a continuous piecewise linear function, respecting lower and upper bounds constraints.\nCould someone help me to implement in Matlab this continuous piecewise linear approximation, or maybe with some alternative ideas?\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "how to approximate time-series data\r\n                \r\nI'm not sure if this is the right term but I think I want to s̶m̶o̶o̶t̶h̶ ̶a̶n̶d̶/̶o̶r̶ approximate a data set. I have 30 data points as it is presented in the chart below (the red line with dots)\nI want to approximate the dataset so it can be described with fewer data points. The black line represents what I want to achieve.\n\nI want to be able to define an approximation level which will control how much the result data set will differ from the original one.\nThe approximated data set should contain a set of data points which I can connect together using straight lines.\n\nWhat is the right algorithm or a math function to solve this problem? I don't expect an implementation here, but rather some suggestions where to start.\n\nI wrote my implementation of the approximation algorithm. It works in most of the cases, but there are certain situations in which it returns non-optimal data.\nThe example below shows three dotted lines. Thin red line is the original dataset, a thick red-black dotted line is generated by my algorithm, the green line is what I'd like to achieve. \n\n \n\n```\n        var previousValue;\n        return array.map(function (dataPoint, index, fullArray) {\n            var approximation = dataPoint;\n\n            if (index > 0) {\n                if (Math.abs(previousValue - value) < tolerance) {\n                    approximation = previousValue;\n                } else {\n                    previousValue = dataPoint;\n                }\n\n            } else {\n                previousValue = dataPoint;\n            }\n\n            return approximation;\n        });\n```\n\n    ", "Answer": "\r\nThere are two options here:\n\n\nif the shown \"glitch\" in the data is significant, meaning that you cannot smooth it.\nif all data shown can be approximated and the \"glitch\" is insignificant\n\n\nIn (1) case, you may consider approximate by templates (e.g. wavelet) or use basic differential analysis to detect and keep the \"glitch\" (e.g. meshes).\nIn (2) case, you may use MA, ARIMA to fit, where the \"glitch\" can be analyzed further through the roots\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": ".NET Framework - Per Application Overhead\r\n                \r\nDoes anybody have any concrete information on the overhead of using the .NET Framework 2.0/3.0/3.5?\n\nI am mostly interested in per-instance overhead and whether there is a \"fixed cost\" regardless of the number of instances, e.g. in a Terminal Services environment with 300 instances of a .NET Framework application running is there only 1 instance of the Just-In-Time compiler?\n\nIt would be great if I get an approximation algorithm, eg 10mb per instance + 50mb for the JIT\n    ", "Answer": "\r\nIt works the exact same way as unmanaged code.  The CLR, the JIT compiler and the .NET framework assemblies are DLLs that are shared by any process that runs managed code.  Only one copy of their code is loaded in RAM, all processes map their virtual memory pages to that one copy.\n\nManaged code tends to have more private bytes than unmanaged code, the kind that cannot be shared.  That's first of all due to the JIT compiler, it generates machine code on-the-fly at addresses that won't be the same for one process vs another.  And the loader and garbage collected heaps tend to be a bit beefy.\n\nYou eliminate the JIT compiler overhead by using Ngen.exe.  That's why the .NET framework assemblies are shared, they were Ngen-ed when you installed the framework on the machine.  You can't do anything about the heaps, but that's not really different in unmanaged code.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Function approximation by ANN\r\n                \r\nSo I have something like this,\ny=l3*[sin(theta1)*cos(theta2)*cos(theta3)+cos(theta1)*sin(theta2)*cos(theta3)-sin(theta1)*sin(theta2)*sin(theta3)+cos(theta1)*cos(theta2)sin(theta3)]+l2[sin(theta1)*cos(theta2)+cos(theta1)*sin(theta2)]+l1*sin(theta1)+l0;\n\nand something similar for x. Where thetai is angles from specified interval and li some coeficients. Task is approximate inversion of equation, so you set x and y and result will be appropriate theta. So I random generate thetas from specified intervals, compute x and y. Then I norm x and y between <-1,1> and thetas between <0,1>. This data I used as training set in such way, inputs of network are normalized x and y, outputs are normalized thetas. \n\nI train the network, tried different configuration and absolute error of network was still around 24.9% after whole night of training. It's so much, so I don't know what to do.\n\n\nBigger training set?\nBigger network?\nExperiment with learning rate?\nLonger training?\n\n\nTechnical info\n\nAs training algorithm was used error back propagation. Neurons have sigmoid activation function, units are biased. I tried topology: [2 50 3], [2 100 50 3], training set has length 1000 and training duration was 1000 cycle(in one cycle I go through all dataset). Learning rate has value 0.2.\n\nError of approximation was computed as \n\nsum of abs(desired_output - reached_output)/dataset_lenght.\n\nUsed optimizer is stochastic gradient descent.\n\nLoss function,\n\n1/2 (desired-reached)^2\n\nNetwork was realized in my Matlab template for NN. I know that is weak point, but I'm sure my template is right because(successful solution of XOR problem, approximation of differential equations, approximation of state regulator). But I show this template, because this information may be useful.\n\nNeuron class\n\nNetwork class\n\nEDIT:\nI used 2500 unique data within theta ranges.\n\ntheta1<0, 180>, theta2<-130, 130>, theta3<-150, 150>\n\nI also experiment with larger dataset, but accuracy doesn't improve.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "approximate histogram for streaming string values (card catalog algorithm?)\r\n                \r\nI have a large list (or stream) of UTF-8 strings sorted lexicographically. I would like to create a histogram with approximately equal values for the counts, varying the bin width as necessary to keep the counts even. In the literature, these are sometimes called equi-height, or equi-depth histograms.\n\nI'm not looking to do the usual word-count bar chart, I'm looking for something more like an old fashioned library card catalog where you have a set of drawers (bins), and one might hold SAM - SOLD,and the next bin SOLE-STE, while all of Y-ZZZ fits in a single bin. I want to calculate where to put the cutoffs for each bin.\n\nIs there (A) a known algorithm for this, similar to approximate histograms for numeric values? or (B) suggestions on how to encode the strings in a way that a standard numeric histogram algorithm would work. The algorithm should not require prior knowledge of string population. \n\nThe best way I can think to do it so far is to simply wait until I have some reasonable amount of data, then form logical bins by:\n\nnumber_of_strings / bin_count = number_of_strings_in_each_bin\n\nThen, starting at 0, step forward by number_of_strings_in_each_bin to get the bin endpoints.\n\nThis has two weaknesses for my use-case. First, it requires two iterations over a potentially very large number of strings, one for the count, one to find the endpoints. More importantly, a good histogram implementation can give an estimate of where in a bin a value falls, and this would be really useful.\n\nThanks. \n    ", "Answer": "\r\nIf we can't make any assumptions about the data, you are going to have to make a pass to determine bin size.\n\nThis means that you have to either start with a bin size rather than bin number or live with a two-pass model. I'd just use linear interpolation to estimate positions between bins, then do a binary search from there.\n\nOf course, if you can make some assumptions about the data, here are some that might help:\n\nFor example, you might not know the exact size, but you might know that the value will fall in some interval ```\n[a, b]```\n. If you want at most ```\nn```\n bins, make the bin size ```\n== a/n```\n.\n\nAlternatively, if you're not particular about exactly equal-sized bins, you could do it in one pass by sampling every ```\nm```\n elements on your pass and dump it into an array, where ```\nm```\n is something reasonable based on context.\n\nThen, to find the bin endpoints, you'd find the element at ```\nsize/n/m```\n in your array.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Networkx Dijkstra Shortest Path exists but is way too long - algorithm that gives me an approximation upfront\r\n                \r\nI am computing a shortest path with networkx. Works fine most of the time, but sometimes the nodes are connected, but over a really weird very remote connection in the network. In this case the algorithm produces a memory error. My question is, if there is a nice way to check upfront if the connection between the nodes will make sense for a shortest path in terms of length, by a threshold which I define.\n    ", "Answer": "\r\nIf you are interested in general solution you can modify Dijkstras algorithm and limit it to a maximum number of nodes or a maximum length and just abort, once that threshold is broken.\n\nI don't know networkx so I don't know if this is available out of the box.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation of n points to the curve with the best fit\r\n                \r\nI have a list of n points(2D):  P1(x0,y0), P2(x1,y1), P3(x2,y2) …\nPoints satisfy the condition that each point has unique coordinates and also the coordinates of each point xi, yi> 0 and xi,yi are integers.\n\nThe task is to write an algorithm which make approximation of these points\n\n\nto the curve ```\ny = | Acos (Bx) |```\n  with the best fit (close or equal to 100%)\nand so that the coefficients A and B were as simple as possible.\n\n\nI would like to write a program in C # but the biggest problem for me is to find a suitable algorithm. Has anyone would be able to help me with this?\n    ", "Answer": "\r\nTaking ```\nB```\n as an independent parameter, you can solve the fitting for ```\nA```\n using least-squares, and compute the fitting residual.\n\nThe residue function is complex, with numerous minima of different value, and an irregular behavior. Anyway, if the ```\nXi```\n are integer, the function is periodic, with a period related to the ```\nLCM```\n of the ```\nXi```\n.\n\nThe plots below show the fitting residue for ```\nB```\n varying from ```\n0```\n to ```\n2```\n and from ```\n0```\n to ```\n10```\n, with the given sample points.\n\n\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Comparing MATLAB AR model algorithms\r\n                \r\nI'm currently trying to build an AR model to approximate the error process in a sensor system. And I'm comparing the different parameter estimators in MATLAB. I have sets of data that i'm trying to match a model to, but I'm not too sure on the benefits/disadvantages of the algorithms available in the signal processing toolbox.\n\n```\narburg: Autoregressive (AR) all-pole model parameters estimated using Burg method\narcov: Estimate AR model parameters using covariance method\narmcov: Estimate AR model parameters using modified covariance method\naryule: Estimate autoregressive (AR) all-pole model using Yule-Walker method\n```\n\n\nIf someone could give a more detailed description comparing the different algorithm and which one would best model existing data that would be very helpful.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Rational approximation of rational exponentiation root with error control\r\n                \r\nI am looking for an algorithm that would efficiently calculate ```\nb^e```\n where ```\nb```\n and ```\ne```\n are rational numbers, ensuring that the approximation error won't exceed given ```\nerr```\n (rational as well). Explicitly, I am looking for a function:\n\n```\nrational exp(rational base, rational exp, rational err)\n```\n\n\nthat would preserve law ```\n|exp(b, e, err) - b^e| < err```\n\n\nRational numbers are represented as pairs of big integers. Let's assume that all rationality preserving operations like addition, multiplication etc. are already defined.\n\nI have found several approaches, but they did not allow me to control the error clearly enough. In this problem I don't care about integer overflow. What is the best approach to achieve this?\n    ", "Answer": "\r\nThis one is complicated, so I'm going to outline the approach that I'd take.  I do not promise no errors, and you'll have a lot of work left.\n\nI will change variables from what you said to ```\nexp(x, y, err)```\n to be ```\nx^y```\n within error ```\nerr```\n.If ```\ny```\n is not in the range ```\n0 <= y < 1```\n, then we can easily multiply by an appropriate ```\nx^k```\n with ```\nk```\n an integer to make it so.  So we only need to worry about fractional `y\n\nIf all numerators and denominators were small, it would be easy to tackle this by first taking an integer power, and then taking a root using Newton's method.  But that naive idea will fall apart painfully when you try to estimate something like ```\n(1000001/1000000)^(2000001/1000000)```\n.  So the challenge is to keep that from blowing up on you.\n\nI would recommend looking at the problem of calculating ```\nx^y```\n as ```\nx^y = (x0^y0) * (x0^(y-y0)) * (x/x0)^y = (x0^y0) * e^((y-y0) * log(x0)) * e^(y * log(x/x0))```\n.  And we will choose ```\nx0```\n and ```\ny0```\n such that the calculations are easier and the errors are bounded.\n\nTo bound the errors, we can first come up with a naive upper bound ```\nb```\n on ```\nx0^y0```\n - something like \"next highest integer than ```\nx```\n to the power of the next highest integer than ```\ny```\n\".  We will pick ```\nx0```\n and ```\ny0```\n to be close enough to ```\nx```\n and ```\ny```\n that the latter terms are under ```\n2```\n.  And then we just need to have the three terms estimated to within ```\nerr/12```\n, ```\nerr/(6*b)```\n and ```\nerr/(6*b)```\n.  (You might want to make those errors tighter half that then make the final answer a nearby rational.)\n\nNow when we pick ```\nx0```\n and ```\ny0```\n we will be aiming for \"close rational with smallish numerator/denominator\".  For that we start calculating the continued fraction.  This gives a sequence of rational numbers that quickly converges to a target real.  If we just cut off the sequence fairly soon, we can quickly find a rational number that is within any desired distance of a target real while keeping relatively small numerators and denominators.\n\nLet's work from the third term backwards.\n\nWe want ```\ny * log(x/x0) < log(2)```\n.  But from the Taylor series if ```\nx/2 < x0 < 2x```\n then ```\nlog(x/x0) < x/x0 - 1```\n.  So we can search the continued fraction for an appropriate ```\nx0```\n.\n\nOnce we have found it, we can use the Taylor series for ```\nlog(1+z)```\n to calculate ```\nlog(x/x0)```\n to within ```\nerr/(12*y*b)```\n.  And then the Taylor series for ```\ne^z```\n to calculate the term to our desired error.\n\nThe second term is more complicated.  We need to estimate ```\nlog(x0)```\n.  What we do is find an appropriate integer ```\nk```\n such that ```\n1.1^k <= x0 < 1.1^(k+1)```\n.  And then we can estimate both ```\nk * log(1.1)```\n and ```\nlog(x0 / 1.1^k)```\n fairly precisely.  Find a naive upper bound to that ```\nlog```\n and use it to find a close enough ```\ny0```\n for the second term to be within 2.  And then use the Taylor series to estimate ```\ne^((y-y0) * log(x0))```\n to our desired precision.\n\nFor the first term we use the naive method of raising ```\nx0```\n to an integer and then Newton's method to take a root, to give ```\nx0^y0```\n to our desired precision.\n\nThen multiply them together, and we have an answer.  (If you chose the \"tighter errors, nicer answer\", then now you'd do a continued fraction on that answer to pick a better rational to return.)\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "is there a clustering algorithm in Spark where the resulting clusters are approximately the same size?\r\n                \r\ni am running a Spark GraphX pregel algorithm where the vertices are intersection coordinates in latitude/longitude, and edges are road segments - a road network. for argument's sake, let's assume uniform message load across the vertices while running the algorithm.\ni want to improve performance by providing a better partitioning. my first effort was to create a ```\nPartitionStrategy```\n which partitioned edges based on clustering their source coordinate. i used the Spark k-means algorithm. then, i realized, while this does a fine job clustering the euclidean points, it tends to do a poor job of load balancing the vertices - cluster sizes vary greatly.\nthere are agglomerative clustering techniques out there in the wild which will find an approximate solution for this. but, since i'm already in Spark, is there an implementation where it can produce clusters for some k that are (approximately) the same size? or, is there a way that i haven't discovered using the built-in mllib tools?\n    ", "Answer": "\r\nHere you find all the clustering algorithms that Spark currently supports.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "In R, is there an algorithm to create approximately equal sized clusters\r\n                \r\nThere seems to be a lot of information about creating either hierarchical or k-means clusters.  But I would like to know if there is an solution in R that would create K clusters of approximately equal sizes.  There is some stuff out there about doing this in other languages, but I have not been able to find anything from searching on the internet that suggests how to achieve the result in R.\n\nAn example would be\n\n```\nset.seed(123)\ndf <- matrix(rnorm(100*5), nrow=100)\nkm <- kmeans(df, 10)\nprint(sapply(1:10, function(n) sum(km$cluster==n)))\n```\n\n\nwhich results in\n\n```\n[1] 14 12  4 13 16  6  8  7 13  7\n```\n\n\nI would ideally like to see\n\n```\n[1] 10 10 10 10 10 10 10 10 10 10 \n```\n\n    ", "Answer": "\r\nI would argue that you shouldn't, in the first place. Why? When there are naturally well-formed clusters in your data, e.g., \n\n```\nplot(matrix(c(sample(1:10,10),sample(30:40, 7), sample(80:90,9)), ncol=2, byrow = F))\n```\n\n\nthen these will be clustered together anyway (assuming k equals the natural n of clusters; see this comprehensive answer on how to choose a good k). If they are uniform in size, then you will have clusters with ~equal size; if they are not, then forcing a uniform cluster size will surely deteriorate the fitness of the clustering solution. \nIf you do not have naturally pretty clusters in your data, e.g,\n\n```\nplot(matrix(c(sample(1:100, 100), ncol=2)))\n```\n\n\nthen forcing a cluster size will either be redundant (if the data is completely random, the cluster sizes will be ~equal - but then there is not much point in clustering anyhow), or, if there are some nice clusters in there, e.g.,\n\n```\nplot(matrix(c(sample(1:15,15),sample(20:100, 11)), ncol=2, byrow = T))\n```\n\n\nthen the forced size will almost certainly break them.\n\nThe Ward's method mentioned in the comments by JasonAizkalns will, however, give you more \"round\" shaped clusters compared to single-link for example, so that might be a way to go (cf. ```\nhelp(hclust)```\n for the difference between D and D2, it's not arbitrary).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "PartitionProblem variation - fixed size of subsets\r\n                \r\nI have a problem which is a variation of the partition problem which is NP-complete. This is an optimization problem, not a decision problem.\n\nProblem: Partition a list of numbers into two subsets such that their difference of sums is minimum, and find the two subsets. If ```\nn```\n even, then the sizes should be ```\nn/2```\n, and if odd, then ```\nfloor[n/2]```\n and ```\nceil[n/2]```\n.\n\nAssuming that the pseudo polynomial time DP algorithm is the best for an exact solution, how can it be modified to solve this? And what would be the best approximate algorithms to solve this?\n    ", "Answer": "\r\nSince you didn't specified which algorithm to use i'll assume you use the one defined here:\nhttp://www.cs.cornell.edu/~wdtseng/icpc/notes/dp3.pdf\n\nThen using this algorithm you add a variable to track the best result, initialize it to ```\nN```\n (sum of all the numbers in the list as you can always take one subset to be the empty set) and every time you update ```\nT```\n (e.g: T[i]=true) you do something like ```\nbestRes = abs(i-n/2)<bestRes : abs(i-n/2) : bestRes```\n. And you return ```\nbestRes```\n. This of course doesn't change the complexity of the algorithm.\n\nI've got no idea about your 2nd question.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to show Metric TSP in APX set of algorithms\r\n                \r\nI know that Metric Travelling Salesman Problem is in the APX (approximable) set of algorithms, but I'm not sure how to show this?\n\nI have been researching about it a little bit and have heard that you can show that Metric TSP is in APX by ```\ngiving a corresponding algorithm```\n.\n\nWhat are the steps for this?\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Matching algorithm with liking function\r\n                \r\nI have an algorithmic problem.\nI want to match two equally sized groups of people. There's a liking function which  assigns every pair (consisting of one person of group A and one person of group B) a liking score.\nI now want to match every person of group A with exactly one person of group B and I want the sum of the scores of all matches to be maximal.\nI designed a naive algorithm which tries out all possibilities and then chooses the best one, but it's runtime is n! (where n is the amount of people in each group).\nIs there a faster algorithm? Or at least a fast approximation algorithm?\n\nThanks in advance!\n    ", "Answer": "\r\nAssuming that each person is only to be matched once (both directions), this sound like a simple assignment problem (or: minimum weight perfect matching in bipartite graph) which can be solved in polynomial-time (and quite efficient in practice). There is also a lot of software available in many programming-languages.\n\nOpposed to the classic ```\nworker <-> job```\n view, your view would be: ```\ngroup A <-> group B```\n.\n\nAs most libraries are somewhat assuming:\n\n\nnon-negative costs\nminimization\n\n\nyou would need to translate your maximization-problem:\n\n```\nx = max(original_likings)\ntransformed_liking_i_j = x - original_liking_i_j\n... solve minimization problem (with transformed likings)\n```\n\n\nThis is often called opportunity loss.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Performance Testing for Calculation-Heavy Programs\r\n                \r\nWhat are some good tips and/or techniques for optimizing and improving the performance of calculation heavy programs. I'm talking about things like complication graphics calculations or mathematical and simulation types of programming where every second saved is useful, as opposed to IO heavy programs where only a certain amount of speedup is helpful.\n\nWhile changing the algorithm is frequently mentioned as the most effective method here,I'm trying to find out how effective different algorithms are in the first place, so I want to create as much efficiency with each algorithm as is possible. The \"problem\" I'm solving isn't something thats well known, so there are few if any algorithms on the web, but I'm looking for any good advice on how to proceed and what to look for.\n\nI am exploring the differences in effectiveness between evolutionary algorithms and more straightforward approaches for a particular group of related problems. I have written three evolutionary algorithms for the problem already and now I have written an brute force technique that I am trying to make as fast as possible.\n\nEdit: To specify a bit more. I am using C# and my algorithms all revolve around calculating and solving constraint type problems for expressions (using expression trees). By expressions I mean things like x^2 + 4 or anything else like that which would be parsed into an expression tree. My algorithms all create and manipulate these trees to try to find better approximations. But I wanted to put the question out there in a general way in case it would help anyone else.\n\nI am trying to find out if it is possible to write a useful evolutionary algorithm for finding expressions that are a good approximation for various properties. Both because I want to know what a good approximation would be and to see how the evolutionary stuff compares to traditional methods.\n    ", "Answer": "\r\nIt's pretty much the same process as any other optimization: profile, experiment, benchmark, repeat.\n\nFirst you have to figure out what sections of your code are taking up the time. Then try different methods to speed them up (trying methods based on merit would be a better idea than trying things at random). Benchmark to find out if you actually did speed them up. If you did, replace the old method with the new one. Profile again.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Finding Top Left and Bottom Right Points (C++)\r\n                \r\nI'm looking for some help with a project I'm working on. \n\nWhat I'm doing is a polygon approximation algorithm. I've got all of the points of my boundary, but in order to start the algorithm, I need to find the top left and bottom right points from the set of points. All of the points are stored in a structure array that has the x and y coordinates of each point. Any ideas on an easy way to loop through the array of points? \n\nAny help would be greatly appreciated. If you need more information, just ask and I'll provide all I can. \n    ", "Answer": "\r\nBased on your comment that bottom left is ```\nmin(x+y)```\n and top right is ```\nmax(x+y)```\n\n\nTop left: ```\nmin(x+max(y)-y)```\n\n\nBottom right: ```\nmax(max(x)-x+y)```\n\n\nWhere the inner ```\nmax```\n is a constant.\n\nThough this may not always give a result that agrees with your eyes.\n\nAlternative metrics can be constructed based on the distance from the corners of the bounding box of your object, or the square of the distance, etc.\n\nAnother technique is to translate the polygon around the origin and then top left is the point furthest from the origin but in the top left quadrant... That gives a whole heap of choices as to where to put ```\n(0,0)```\n could be average of all, could be weighted average based on some rule, etc. lot of variability in how you pick that, each may give results that differ for a very small number if polygons from what the eye would pick.\n\nFinally you could always train a neural network to pick the answer.... That might result in something that is (insert confidence limits from training)% likely to give an answer you agree with... But you and I may disagree\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Objective-C vs C++ for applied math algorithms [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI'm an engineering student and I have a couple classes about different complex math problems and how to solve them exactly and numerically (approximation) using various algorithms. We have focused more on the actual math and the ideas of the solutions than implementations of them. The most of these that I have seen are written in C++. For some reason I don't really like C++ and I have fallen completely in love with Objective-C. (I own an apple computer)\n\nIs it to much of a difference in efficiency between these to languages? I'm not planning on solving 40.000x40.000 navier-stokes equation systems, just some little/little-medium instances of practical problems. Most off the problems of interest are P-problems and for the NP-Hard good heuristics would be fine.\n\nI am obligated to learn yet another programming language, or should I just go with Objective-C, a language that I'm very familiar with nowadays.\n    ", "Answer": "\r\nFor Math you have another languages like:\n\n\nR for statistic\nOctave for matrices and vectors\nMaxima as CAS\nSciPy for numeric computations\nThe God of all mathematics computations FORTRAN\n\n\nUse right tool for right thing, and focus on idea not on algorithm (as you said).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Why is my Shortest Hamiltonian Path algorithm suboptimal?\r\n                \r\nI was trying to code up a brute force algorithm in Python from scratch that solves the Shortest Hamiltonian Path problem for a weighted complete graph as follows:\n\n```\ndef min_route(cities, distances):\n    \"\"\"Finds the Shortest Hamiltonian Path for a weighted complete graph.\n\n    Args:\n        cities (list of string):\n            The vertices of the graph.\n\n        distances (dict):\n            The distances between any two cities. Maps each origin city to a\n            dictionary that maps destination cities to distances, sort of like\n            an adjacency matrix. Type: Dict<string, Dict<string, int>>.\n\n    Returns:\n        (list of string, int):\n            The list of cities in the optimal route and its length.\n    \"\"\"\n    if len(cities) < 2:\n        return cities, 0\n\n    best_route, min_dist = None, float('inf')\n    for i in range(len(cities)):\n        first, rest = cities[i], cities[:i] + cities[i+1:]\n        sub_route, sub_dist = min_route(rest, distances)\n        route = [first] + sub_route\n        dist = sub_dist + distances[first][sub_route[0]]\n        if dist < min_dist:\n            best_route, min_dist = route, dist\n\n    return best_route, min_dist\n```\n\n\nIt turns out that this algorithm doesn't work and that it's sensitive to the order of the initial list of cities. This confused me, as I thought that it would enumerate all ```\nn!```\n possible city permutations, where ```\nn```\n is the number of cities. It seems that I'm pruning some of the routes too early; instead, I should do something like:\n\n```\ndef min_route_length(cities, distances):\n    routes = get_a_list_of_all_permutations_of(cities)\n    return min(compute_route_length(route, distances) for route in routes)\n```\n\n\n\n  Question: What is a simple counterexample that demonstrates why my algorithm is suboptimal?\n  \n  Follow Up: Is my suboptimal algorithm at least some kind of approximation algorithm that uses some kind of greedy heuristic? Or is it really just a terrible ```\nO(n!)```\n algorithm?\n\n    ", "Answer": "\r\nAssuming your graph is directed(can have different weights from A to B and from B to A), one of the counterexamples would be\n\n```\n   A  B  C\nA  x  1  5\nB 30  x 10\nC 30  9  x\n```\n\n\nPaths not starting from A have their costs of at least 30, so we dont need to consider them. For path starting with A, your code makes recursive call with ```\n[B, C]```\n. Their optimal arrangement is C>B with cost 9 and that is the return value from recursive call. However, an entire path A>C>B has cost 14, versus optimal path A>B>C with cost 11.\n\nYou're correct that it is ```\nO(n!)```\n. You just need to pass an extra argument down - starting point (possibly None for the first call) and consider it when calculating ```\ndist```\n.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Genetic Algorithm Approximating Images with Polygons - Fitness Calculating\r\n                \r\nSo I'm implementing GA. And I've got an array of polygons for a generation. Every polygon has a ```\ncolor```\n and some ```\npoint```\ns.\nI'm calculating the fitness right now like this:\n\n```\nPaint the polygons onto a 200x200 view```\n -> ```\nGet a snapshot of the view```\n -> ```\nGet 40000 pixel data from the snapshot```\n -> ```\nCompare the 40000 pixel data with the target image pixel data and returns the fitness```\n\n\nThis is actually a very bad implement because UI rendering takes a lot of time.\nWhat better method can I use for the fitness calculating? How do I get 40000 pixel data from an a array of polygons? Because you know, I cant get an exact RGBA pixel data at an exact coordinate from that array.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Is there an accepted \"current industry standard best\" of stochastic optimization? (Simulated annealing, Particle swarm optimization, etc)\r\n                \r\nSorting algorithms are well understood enough that Java Collections uses some flavor of MergeSort or Timsort.  (Even though it is possible to hand-craft collections that \"fight\" the algorithm and perform poorly, those choices are \"often enough ideal\" for most real world sorting situations)\n\nStatistical ML algorithms kinda/sorta have winners as well, e.g. \"You won't go wrong first trying Logistic Regression, Random Forests, and SVM.\"\n\nQ: Is there a similar \"best of breed\" choice between the various global optimum approximation functions?  For example, it seems that particle swarm optimization (PSO) is several simulated annealing processes running in parallel and sharing information...\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How could I optimize this algorithm for approximating pi?\r\n                \r\nI am quite inexperienced in coding, but I have managed to  write this:\n\n```\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\n\nnamespace PiApprox\n{\n\n    public class PiApprox\n    {\n        //side of the rectangle\n        public static int s;\n\n        //radius of the circle\n        public static int r;\n\n        //indexers of the coordinates, points\n        public static int ix;\n        public static int iy;\n\n        //current y\n        public static decimal cury;\n        //without rounding\n        public static decimal wcury;\n\n        //amounts of points relative to the circle\n        public static decimal inAmount;\n        public static decimal onAmount;\n        public static decimal outAmount;\n\n        //amount of all points\n        public static decimal allAmount;\n\n        //short for inAmount and on onAmount, \n        //used to make the calculations clearer in the final part\n        public static decimal inanon;\n\n        //final result, crude approximation of pi\n        public static decimal piApprox;\n\n\n        public static void Main()\n        {\n            while (true)\n            {\n                Calculate();\n            }\n        }\n\n        public static void Calculate ()\n        {\n            s = Convert.ToInt32(Console.ReadLine());\n\n\n            //calculate the radius of the circle\n            r = s / 2;\n\n            //calculate the total amount of points in the grid\n            //rectangle area\n            allAmount = (decimal) Math.Pow(s, 2);\n\n            //reset values\n            inAmount = 0;\n            onAmount = 0;\n            outAmount = 0;\n\n            //main loop\n            //iterate for y, from up to down \n            for (ix = -r; ix <= 0; ix++)\n            {\n                wcury = (decimal) Math.Sqrt(Math.Pow(r, 2) - Math.Pow(ix, 2));\n                cury = Math.Floor(wcury);\n\n\n                outAmount += r - (int)cury;\n\n                if (wcury == cury)\n                {\n                    onAmount++;\n                }\n\n                if (wcury == cury)\n                {\n                    inAmount += (int)cury;\n                }\n                else\n                {\n                    inAmount += (int)cury + 1;\n                }\n                Result();\n            }\n\n            Result();\n        }\n\n        public static void Result()\n        {\n            //total amount of points\n            inanon = 4 * (onAmount + inAmount - (r + 1)) + 1;\n\n            //proportion\n            piApprox = 4 * (inanon / allAmount);\n\n            Console.SetCursorPosition(1, 0);\n            Console.WriteLine(piApprox);\n        }\n    }\n\n}\n```\n\n\nThe Monte Carlo principle is simple; I calculate the y values \nfor the plot f(x) = sqrt(r^2 - ix^2) which represent the first quarter of a circle. I then calculate points within the circle and output it at the end. \nThe multiplication on the line piApprox = 4 * (inanon / allAmount);\ncomes from the proportions of the square and the circle:\n (pi * r^2) / ( (2r) ^ 2 )  ->\n (pi * r ^ 2) / (4 * r ^ 2)  ->  pi / 4\n\nIs there something I could do to speed up the computing?\n    ", "Answer": "\r\nI assume you're new to C# so I'll just give you a couple hints here.\n\nSeveral things have potential for improvement:\n\n\n```\ndecimal```\n is slow: it uses software computations. On the other hand, calculations on ```\nint```\n, ```\ndouble```\n and similar are implemented in hardware. Use ```\nint```\n here, you don't use the decimal part anyway.\n```\nMath.Pow```\n is slow. Don't use it for squaring: Replace ```\nMath.Pow(x, 2)```\n with ```\nx * x```\n\n```\nMath.Sqrt```\n is slow. Instead of comparing ```\nMath.Sqrt(x)```\n to ```\ny```\n, compare ```\nx```\n to ```\ny * y```\n instead. Or just call it once at the end.\n```\nMath.Floor```\n is slow :)\nYou could use parallelism to leverage multicore CPUs\nYou should use local variables as they're more prone for optimization\n\n\nBear in mind that when I mean slow it's relative. All of these operations are extremely fast in an absolute sense - I just mean you could use an even faster alternative.\n\nBut there's one thing which is painfully slow (such that it's noticeable for a human): ```\nConsole```\n. It got much better on Windows 10, but it's still slow and you're using the console in the hot path of your code. Get rid of these intermediate results.\n\nOne more thing, if you use ```\nint```\n in a division, you'll get an ```\nint```\n in C#. You need to cast one operand to, say, a ```\ndouble```\n before dividing if you want to get the fractional part (as in ```\n(double)x / y```\n).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to go about formatting 1200 to 1.2k in java\r\n                \r\nI'd like to format following numbers into the numbers next to them with java:\n\n```\n1000 to 1k\n5821 to 5.8k\n10500 to 10k\n101800 to 101k\n2000000 to 2m\n7800000 to 7.8m\n92150000 to 92m\n123200000 to 123m\n```\n\n\nThe number on the right will be long or integer the number on the left will be string.\nHow should I approach this. I already did little algorithm for this but I thought there might be already something invented out there that does nicer job at it and doesn't require additional testing if I start dealing with billions and trillions :)\n\nAdditional Requirements:\n\n\nThe format should have maximum of 4 characters   \nThe above means 1.1k is OK 11.2k is not. Same for 7.8m is OK 19.1m is not. Only one digit before decimal point is allowed to have decimal point. Two digits before decimal point means not digits after decimal point.  \nNo rounding is necessary.  (Numbers being displayed with k and m appended are more of analog gauge indicating approximation not precise article of logic. Hence rounding is irrelevant mainly due to nature of variable than can increase or decrees several digits even while you are looking at the cached result.)\n\n    ", "Answer": "\r\nHere is a solution that works for any long value and that I find quite readable (the core logic is done in the bottom three lines of the ```\nformat```\n method).\nIt leverages ```\nTreeMap```\n to find the appropriate suffix. It is surprisingly more efficient than a previous solution I wrote that was using arrays and was more difficult to read.\n```\nprivate static final NavigableMap<Long, String> suffixes = new TreeMap<> ();\nstatic {\n  suffixes.put(1_000L, \"k\");\n  suffixes.put(1_000_000L, \"M\");\n  suffixes.put(1_000_000_000L, \"G\");\n  suffixes.put(1_000_000_000_000L, \"T\");\n  suffixes.put(1_000_000_000_000_000L, \"P\");\n  suffixes.put(1_000_000_000_000_000_000L, \"E\");\n}\n\npublic static String format(long value) {\n  //Long.MIN_VALUE == -Long.MIN_VALUE so we need an adjustment here\n  if (value == Long.MIN_VALUE) return format(Long.MIN_VALUE + 1);\n  if (value < 0) return \"-\" + format(-value);\n  if (value < 1000) return Long.toString(value); //deal with easy case\n\n  Entry<Long, String> e = suffixes.floorEntry(value);\n  Long divideBy = e.getKey();\n  String suffix = e.getValue();\n\n  long truncated = value / (divideBy / 10); //the number part of the output times 10\n  boolean hasDecimal = truncated < 100 && (truncated / 10d) != (truncated / 10);\n  return hasDecimal ? (truncated / 10d) + suffix : (truncated / 10) + suffix;\n}\n```\n\n\nTest code\n```\npublic static void main(String args[]) {\n  long[] numbers = {0, 5, 999, 1_000, -5_821, 10_500, -101_800, 2_000_000, -7_800_000, 92_150_000, 123_200_000, 9_999_999, 999_999_999_999_999_999L, 1_230_000_000_000_000L, Long.MIN_VALUE, Long.MAX_VALUE};\n  String[] expected = {\"0\", \"5\", \"999\", \"1k\", \"-5.8k\", \"10k\", \"-101k\", \"2M\", \"-7.8M\", \"92M\", \"123M\", \"9.9M\", \"999P\", \"1.2P\", \"-9.2E\", \"9.2E\"};\n  for (int i = 0; i < numbers.length; i++) {\n    long n = numbers[i];\n    String formatted = format(n);\n    System.out.println(n + \" => \" + formatted);\n    if (!formatted.equals(expected[i])) throw new AssertionError(\"Expected: \" + expected[i] + \" but found: \" + formatted);\n  }\n}\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "SARSA implementation with tensorflow\r\n                \r\nI try to learn the concept of reinforcement learning at the moment. Hereby, I tried to implement the SARSA algorithm for the cart pole example using tensorflow. I compared my algorithm to algorithms which use a linear approximation function for the q-value function and find my algorithm to be very similar. Unfortunately, my implementation seems to be false or inefficient as the learning success is rather limited. Is there anyone who can tell me if I am doing something wrong and what it is? The code of my implementation is:\n```\nimport numpy as np\nimport matplotlib.pylab as plt\nimport random\nimport gym\n\n\n#define a neural network which returns two action dependent q-values given a state\nneural_net = tf.keras.Sequential([\n    tf.keras.layers.Dense(10, activation = 'relu', input_shape = [4]),\n    tf.keras.layers.Dense(2)\n])\n\n#return the neural network's q-value for a specific action\ndef q_value(state, action):\n    return neural_net(tf.convert_to_tensor([state]))[0, action]\n\n#act either randomly or choose the action which maximizes the q-value\ndef policy(state, epsilon):\n    values = neural_net(tf.convert_to_tensor([state]))\n    if np.random.rand() < epsilon:\n        return random.choice([0, 1])\n    else:\n        return np.argmax(values)\n\n#intialize gym environment\nenv = gym.make('CartPole-v0')\n\n#hyperparameters\ndiscount = 0.99\noptimizer = tf.keras.optimizers.Adam()\nepisodes = 1000\nepsilon = 0.30\n\n#collect reward for each episode\nrewards = []\nfor episode in range(episodes):\n\n    #start trajectory for episode\n    state = env.reset()\n\n    #record rewards during episode\n    sum_returns = 0\n\n    #decrease random action after the first 100 episodes\n    if episode == 100:\n        epsilon = 0.10\n\n    #Q-learning\n    while True:\n        action = policy(state, epsilon)\n        next_state, reward, done, _ = env.step(action)\n        next_action = policy(next_state, epsilon)\n        sum_returns += 1\n\n        if done:\n            with tf.GradientTape() as tape:\n                tape.watch(neural_net.trainable_variables)\n                q_hat = q_value(state, action)\n                y = reward\n                loss = tf.square(y - q_hat)\n\n            gradients = tape.gradient(loss, neural_net.trainable_variables)\n            optimizer.apply_gradients(zip(gradients, neural_net.trainable_variables))\n            break\n        else:\n            with tf.GradientTape() as tape:\n                tape.watch(neural_net.trainable_variables)\n                q_hat = q_value(state, action)\n                y = reward + discount * q_value(next_state, next_action)\n                loss = tf.square(y - q_hat)\n\n            gradients = tape.gradient(loss, neural_net.trainable_variables)\n            optimizer.apply_gradients(zip(gradients, neural_net.trainable_variables))\n            state = next_state\n\n    rewards.append(sum_returns)\n\n#plot learning over time\nplt.plot([episode for episode in range(episodes)], rewards)\nplt.show()```\n\n\n```\n\n    ", "Answer": "\r\nI took a quick look at your code and it seems that the neural network has no way of knowing that the new estimate for the Q value ```\ny```\n is related to the action ```\na```\n that was selected since you passed the same state regardless of the choice that is made afterwards.\nMy recommendation would be to concatenate the state twice - since you have two actions - and when you select the first action you only add the current state once to the first half to this intermediate representation, leaving the second half of the vector empty. You would do something similar when choosing the second action, but you'd leave the first half of the vector empty.\nCheck out this link on Coursera I found when I was implementing SARSA: https://www.coursera.org/lecture/prediction-control-function-approximation/episodic-sarsa-with-function-approximation-z9xQJ\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximate Minimum Feedback Arc Set implementation in Java\r\n                \r\nI'd like to find an implementation of an approximate algorithm for the Minimum Feedback Arc Set in Java but I did not find anything so far. Does anyone have something in mind? \n    ", "Answer": "\r\nIt appears that the simplest approximate algorithm one can implement (but with no minimality guarantees) is the one of this paper:\n\nA fast and effective heuristic for the feedback arc set problem, by P. Eades, X. Lin, W.F. Smyth.\n\nIt is very easy to implement and works quite fast for large graphs (I tried it on a graph of 2.5 million edges and around 100 thousand nodes and broke all cycles in less than a minute).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Minimum axis-parallel bounding box in linear time\r\n                \r\nProblem\nI have to calculate the diameter for a set of 2-dimensional points in linear time O(n).\n\nIn order to do this, I thought of using the minimum axis-parallel bounding box that can be computed in linear time with rotating calipers starting from a convex polygon.\nUnfortunately I don't have a convex polygon and computing it will require O(nlogn) time because of the convex hull.\n\nMy idea was to use the radix sort and then compute the convex hull through monotone chain algorithm (which requires linear time if the input is sorted).\n\nNow my questions are:\n\n\nhow can I be sure that radix sort will run in linear time?\ndo you have better ideas to compute the minimum axis-parallel bounding box in linear time?\n\n\nThank you in advance!\n\nEdit\nI specifically need the minimum bounding box because I have to design a sqrt(2) approximation algorithm for the diameter and this is the only way I know to prove this approximation.\n    ", "Answer": "\r\nIf you're looking for the diameter of a set of points, Welzl's algorithm is probably your best bet. It finds the minimum enclosing circle in linear time.\n\nEDIT: I didn't realize you needed to make a box. To find the axis-aligned boundary of the minimum bounding box, you'll just want to do a linear scan of your data and take the appropriate min/max values of the (x,y) coordinates.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Is it possible to find the closest point to all points in subquadratic time?\r\n                \r\nAn algorithmic question.\n\nInput:\n\n\nA list of data points ```\nX```\n\nA metric function for data points ```\ndist(x,y)```\n that takes O(1) time to evaluate and obeys the triangle inequality\n\n\nIs there an algorithm that can return a vector of data points ```\nY```\n such that ```\nY[i]```\n is the closest point in ```\nX```\n to ```\nX[i]```\n in subquadratic time? \n\nObviously this is possible in O(n^2), because you could just directly check every point. I'm wondering if it might be possible to leverage the triangle inequality to improve on this. I would also be interested in approximate algorithms with provable bounds (i.e. something like ```\nY[i]```\n is no more than (1 + log(n)) times the distance  from ```\nX[i]```\n as the minimum).\n    ", "Answer": "\r\nThere's no such algorithm. Consider a metric where all but one pair of points is at distance 1. That pair cannot be found without consulting its particular distance oracle entry, which requires Omega(n^2) queries in the worst case.\n\nCover trees can be used to solve the exact neighbors problem. The time bound depends on the so-called doubling dimension of the metric.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Is it possible to find the closest point to all points in subquadratic time?\r\n                \r\nAn algorithmic question.\n\nInput:\n\n\nA list of data points ```\nX```\n\nA metric function for data points ```\ndist(x,y)```\n that takes O(1) time to evaluate and obeys the triangle inequality\n\n\nIs there an algorithm that can return a vector of data points ```\nY```\n such that ```\nY[i]```\n is the closest point in ```\nX```\n to ```\nX[i]```\n in subquadratic time? \n\nObviously this is possible in O(n^2), because you could just directly check every point. I'm wondering if it might be possible to leverage the triangle inequality to improve on this. I would also be interested in approximate algorithms with provable bounds (i.e. something like ```\nY[i]```\n is no more than (1 + log(n)) times the distance  from ```\nX[i]```\n as the minimum).\n    ", "Answer": "\r\nThere's no such algorithm. Consider a metric where all but one pair of points is at distance 1. That pair cannot be found without consulting its particular distance oracle entry, which requires Omega(n^2) queries in the worst case.\n\nCover trees can be used to solve the exact neighbors problem. The time bound depends on the so-called doubling dimension of the metric.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to determine path from noisy X, Y data\r\n                \r\nI have an unsorted list of noisy X, Y points. They do, however, form a path through the world. I would like an algorithm to draw an approximation of this data using line segments.\n\nThis is similar to how you would use a line -fitting algorithm to pick an approximation of linear data. My problem is only harder because the path bends and winds around the world.\nalt text http://www.praeclarum.org/so/pathfinder.png\n\nDoes anyone know of any standard / robust / easy to comprehend algorithms to accomplish this?\n\nQ&A:\n\nWhat do you mean by noisy? If I had an ideal realization of the path, then my set of points would be sampled from that ideal path with gaussian noise added to the X and Y elements. I do not know the mean or standard deviation of that noise. I may be able to guess at the std dev...\n\nDo the points lie near, but not on, some ideal but complicated path which you seek to approximate? Yes.\n\nDo you have any a priori information about he shape of the path? Any other way to get such information? Unfortunately not.\n    ", "Answer": "\r\nBezier Interpolation may fit your problem.\n\n\n\nThis does not address the ordering of the points into a path, however; there are a number of approaches to consider: \n\n\nAny \"optimal\" type of path (e.g. smallest direction change at each point on the path, * Shortest path through all points) will likely boil down the NP complete Travelling Salesman Problem (TSP).\nA \"reasonable\" path to cluster the nodes and then route between clusters, and within clusters. Of course, the larger the cluster, or the larger the number of clusters the more this smaller problem looks like a large n TSP.\nOrdering the points by one axis. If there are much more than 2 axes, some dimensional reduction strategy may be useful. e.g. Independent Component Analysis.\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Halley's method in C or Octave\r\n                \r\nI have been working on a math project recently and I need some help. I got stuck in a point where I need to write a program that represents Halley's method which is something actually similar to Newton's method of approximation algorithm for finding a root of a function.\n\nthats what I have at the moment. Since I dont have much of programming knowledge could you tell me what could be improved to make it work?\n\nI assumed p, q being values of first and second derrivatives of a function.\n\n```\n function y = halley (f, x_0)\n x=x_0 + 10;\n x_new = x_0;\n while ( abs(x_new - x)> 0.00001)\n     x = x_new;\n     p = (f(x+0.00001) - f(x)) / 0.0001;\n     q = (f(x+0.00001) - 2*f(x) + f(x-0.00001)) / (0.0000000001);\n     x_new = x-((2* f(x)*p) / (((2*p)^2) -(q*f(x)))\n    end;\n    y =x_new\n end;\n```\n\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Solve bulb toggling puzzle in non-exponential time?\r\n                \r\nThe bulb on-off problem:\n\n\n  You are given few switches and few bulbs, not necessarily equal, and each button may connect to multiple bulbs, and when a switch is toggled on, it switches on the already off and off the already on bulbs. We need to find if all bulbs can be lighted up by switching some of these switches?\n\n\nWe can brute-force this problem in O(2^n.n.m.x) (n, the no. of switches;m, total bulbs;x, the maximum no. of bulbs that any switch switches on). Where we try all combinations of switches O(2^n) and check if all bulbs are on O(m) after switching on all bulbs from selected switches O(n.x)\n\nCan this be done in non-exponentiation time, maybe a heuristic.\nThis somewhat relates to set-packing and set-cover problem and both seem to have approximation algorithms only (NP-hard, IIRC)\n    ", "Answer": "\r\nThe answer is \"yes\", you can solve this kind of puzzle in O(min((m^2) n, (n^2) m)) time.\n\nYour problem can be viewed as a matrix of integers mod 2, where multiplication becomes a boolean \"and\", and addition is boolean \"xor\".\n\nTo make things more concrete:  you can represent the \"original\" bulb state (with all switches off) as a boolean column vector, the state of each switch as another boolean vector, and the set of bulbs toggled by each switch as the corresponding column of your boolean matrix:\n\n```\noutput     original         matrix      switches\n  [v]        [u]        [m m m m m m]     [x]\n  [v]   =    [u]   +    [m m m m m m]  *  [x]\n  [v]        [u]        [m m m m m m]     [x]\n                                          [x]\n                                          [x]\n```\n\n\nThis is a set of linear equations under the integers mod 2, and you can solve this kind of problem efficiently using an appropriately modified version of standard linear equation solvers.\n\nNote that the reason this works is that flipping a switch always toggles the same set of bulbs.  If this were not the case, then the equation would not be linear, and you could not solve it this way.  In general, if you have \"or\"'s mixed in with your \"and\"'s in this kind of problem, you will likely have some variation of the \"SAT\" problem, which is NP-complete.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "bin packing with overlapping objects\r\n                \r\nI have some bins with different capacities and some objects with specified size. The goal is to pack these objects in the bins. Until now it is similar to the bin-packing problem. But the twist is that each object has a partial overlap with another. So while object 1 and 2 has sizes s1 and s2, when I put them in the same bin the filled space is less than s1+s2. Supposing that I know this overlapping value for each pair of objects, is there any approximation algorithm like the ones for original bin-packing for this problem too?\n    ", "Answer": "\r\nThe answer is to use a kind of tree that captures the similarity of objects assuming that objects can be broken. Then run a greedy algorithm to fill the bins according to the tree. This algorithm has 3-x approximation bound. However, there should also be better answers.\n\nThis method is presented in  Michael Sindelar, Ramesh K. Sitaraman, Prashant J. Shenoy: Sharing-aware algorithms for virtual machine colocation. SPAA 2011: 367-378. \n\nI got this answer from this thread but just wanted to close this question by giving the answer.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How does Photoshop's magnetic lasso work?\r\n                \r\nSeems like GIMP's intelligent scissor is based on a paper from SIGGRAPH '95 on \"Intelligent Scissors for Image Composition\", as seen in one of the comments in the source.\n\nHowever Magnetic Lasso in Photoshop differs a lot, while GIMP's tool gives users an option to click vertices of desired figure and approximates along edges found in the image, photoshop's magnetic lasso on the other hand, gives users a way to run freehand and gives something between computerized approximation and what the user desired to draw.\n\nLooking at this behavior its quite obvious that Magnetic Lasso style selection would quite cool for selection in touch based interfaces. Any pointers on how magnetic lasso differs from the GIMP's tool? Any specific papers/algorithms to look into?\n    ", "Answer": "\r\nOne algorithm you can look into is Marching Squares.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation of arcsin in C\r\n                \r\nI've got a program that calculates the approximation of an arcsin value based on Taylor's series.\n\n\n\nMy friend and I have come up with an algorithm which has been able to return the almost \"right\" values, but I don't think we've done it very crisply. Take a look:\n\n```\ndouble my_asin(double x)\n{\n    double a = 0;\n    int i = 0;\n    double sum = 0;\n    a = x;\n    for(i = 1; i < 23500; i++)\n    {\n        sum += a;\n        a = next(a, x, i);\n    }\n}\n\ndouble next(double a, double x, int i)\n{\n    return a*((my_pow(2*i-1, 2)) / ((2*i)*(2*i+1)*my_pow(x, 2)));\n}\n```\n\n\nI checked if my_pow works correctly so there's no need for me to post it here as well. Basically I want the loop to end once the difference between the current and next term is more or equal to my EPSILON (0.00001), which is the precision I'm using when calculating a square root.\n\n\n\nThis is how I would like it to work:\n\n```\nwhile(my_abs(prev_term - next_term) >= EPSILON)\n```\n\n\nBut the function double next is dependent on i, so I guess I'd have to increment it in the while statement too. Any ideas how I should go about doing this?\n\n\n\nExample output for -1:\n\n```\n$ -1.5675516116e+00\n```\n\n\nInstead of:\n\n```\n$ -1.5707963268e+00\n```\n\n\n\n\nThanks so much guys.\n    ", "Answer": "\r\nIssues with your code and question include:\n\n\nYour image file showing the Taylor series for arcsin has two errors: There is a minus sign on the x5 term instead of a plus sign, and the power of x is shown as xn but should be x2n+1.\nThe x factor in the terms of the Taylor series for arcsin increases by x2 in each term, but your formula ```\na*((my_pow(2*i-1, 2)) / ((2*i)*(2*i+1)*my_pow(x, 2)))```\n divides by x2 in each term. This does not matter for the particular value -1 you ask about, but it will produce wrong results for other values, except 1.\nYou ask how to end the loop once the difference in terms is “more or equal to” your epsilon, but, for most values of x, you actually want less than (or, conversely, you want to continue, not end, while the difference is greater than or equal to, as you show in code).\nThe Taylor series is a poor way to evaluate functions because its error increases as you get farther from the point around which the series is centered. Most math library implementations of functions like this use a minimax series or something related to it.\nEvaluating the series from low-order terms to high-order terms causes you to add larger values first, then smaller values later. Due to the nature of floating-point arithmetic, this means that accuracy from the smaller terms is lost, because it is “pushed out” of the width of the floating-point format by the larger values. This effect will limit how accurate any result can be.\nFinally, to get directly to your question, the way you have structured the code, you directly update ```\na```\n, so you never have both the previous term and the next term at the same time. Instead, create another ```\ndouble b```\n so that you have an object ```\nb```\n for a previous term and an object ```\na```\n for the current term, as shown below.\n\n\nExample:\n\n```\ndouble a = x, b, sum = a;\nint i = 0;\ndo\n{\n    b = a;\n    a = next(a, x, ++i);\n    sum += a;\n} while (abs(b-a) > threshold);\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to find the approximate color of any color in a given palette?\r\n                \r\nI'm working on a pixel game that has a limited palette, but lighting and translucency can cause colors to go out of the palette. To make the overall picture uniform, I need a color approximation algorithm with low time complexity.\nI've searched about how to find the differences between colors, and find that the distance between colors is not consistent with the distance of their color space coordinates. So I may need a way to optimize the distance finding algorithm for non-uniform spaces, but I'm completely confused about it.\nThis project is an Webapp game which run on browser, but at this stage it is only planned to be compatible with electron.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm to convert a straight skeleton of an orthogonal poygon into an orthogonal straight skeleton\r\n                \r\nConsider the definition of an orthogonal polygon as a closed curve in 2 dimensions where all edges are aligned to two orthogonal axes.\nThe straight skeleton of such polygon is a set of straight-line segments that follow a property that every point in the skeleton is the same distance of the polygon edges.\nThis is an orthogonal polygon with a straight skeleton.\n\nWhat I need is an algorithm that takes this straight skeleton and approximates it using only orthogonal lines.\nOne example of approximation that is not ideal, but is something I would accept as a starting point, would be this skeleton:\n\nAre there algorithms that achieve similar goals in the literature?\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Load balancing and Stirling numbers of the second kind applied\r\n                \r\nI have, say, P threads and N > P tasks to perform using the aforementioned. I have a positive integer value associated with each tasks denoting how much work that particular task implies.\n\nI want to partition the N tasks among P threads such that if we consider the sums of \"work integers\" for each thread, they would be approximately the same.\n\nA naive but exact method for doing such \"scheduling\" would have to consider S(N,P) task partitions, where S(N,P) is a Stirling number of the second kind (should be impractically large in a real-world computing).\n\nQ: Is there any good and efficient approximation algorithm for computing such \"load-balanced\" task partition?\n    ", "Answer": "\r\nIf you have only 2 processor, the problem of dividing the task into two equal task is known as the partition problem. It is known to be NP-hard. So this may indicate that finding the best solution is probably a hard problem. I you only want an approximation, I would go for some kind of greedy algorithm: Assigning systematically the largest remaining task to the thread which has the less work. This will give you an easy to implement algorithm with time complexity O(number of task*number of thread).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How can I intercept this approximation error in my matlab script?\r\n                \r\nI am trying to find the minimum of a function using this algorithm.\nIt's not an optimal algorithm, but I don't care at the moment.\n\nAlso, you don't have to know how the algorithm works in order to reply, but if you're curious, I talk about it at the end of this post. It's really not that difficult.\n\nIncriminated Algorithm\n\n```\nfunction result = fmin(f,a,b,max_error)\nif abs(b-a) < max_error\n    result = (a+b)/2;\nelse\n    r1 = a+(b-a)*rand(1,1); r2 = a+(b-a)*rand(1,1);\n    c = min([r1,r2]); d = max([r1,r2]);\n    fc = f(c); fd = f(d);  \n    if fc <= fd\n        b = d;\n    else\n        a = c;\n    end\n    result = fmin(f,a,b,max_error);\nend\n```\n\n\nNow, the problem is this algorithm returns a minimum that is far from the actual minimum (computed via the matlab predefined function ```\nfminbnd```\n) for more than ```\nmax_error```\n, if I use it with values of ```\nmax_error <= 1e-10```\n. This situation, form a theoretical standpoint is not possible. \n\nBeing recursive, the algorithm would never return if the condition  ```\nabs(b-a) < max_error```\n is never satisfied.\n\nSo, I think there is some error arising form the approximation of the numbers. At first, I thought that ```\nr1```\n or ```\nr2```\n where not computed properly. At some point, the two numbers would go out of the ```\n[a,b]```\n interval, thus invalidating the hypothesis on which the algorithm is working.\n\nTo prove this, I modified the algorithm above to include a check on the interval that's computed at every iteration:\n\nIncriminated Algorithm 2 [Check on the extremes] \n\n```\nfunction result = fmin(f,a,b,max_error)\nif abs(b-a) < max_error\n    result = (a+b)/2;\nelse\n    r1 = a+(b-a)*rand(1,1); r2 = a+(b-a)*rand(1,1);\n    c = min([r1,r2]); d=max([r1,r2]);\n    % check that c and d are actually inside [a,b]\n    if ((c < a)||(d > b))\n        disp('Max precision reached');\n        result = (a+b)/2;\n        return;\n    end\n    fc = f(c); fd = f(d);  \n    if fc <= fd\n        b = d;\n    else\n        a = c;\n    end\n    result = fmin(f,a,b,max_error);\nend\n```\n\n\nBut I don't get any additional output from the console.\n\nSo, I am thinking there is some error in the computation of ```\nf(c)```\n or ```\nf(d)```\n, but I don't know how to prove it.\n\nQuestion\n\nFinally, my questions are\n\n\nDo we, at this point, can be sure that the error is committed in the computation of either one of ```\nf(c)```\n or ```\nf(d)```\n?\nCan we prove it with some line of code? Or better, can we write the algorithm so that it returns when it is supposed to?\n\n\nHow the algorithm works (not strictly inherent to the question)\n\nIt's an iterative algorithm. Basically, the idea is to generate a sequence of intervals containing the solution, starting from an initial interval [a,b] in which a given function ```\nf```\n is unimodal.\nAt every step, we randomly choose two number ```\nc```\n and ```\nd```\n so that ```\na <= c <= d <= b```\n. Now, if we find that ```\nf(c) > f(d)```\n it means we are sure that we can discard the values the function assumes before ```\nc```\n as valid candidates for a minimum, because of the unimodality. So we restrict the interval and repeat the procedure in the interval ```\n[c,b]```\n. On the contrary, if it's ```\nf(c) < f(d)```\n, we can discard the values from ```\nd```\n to ```\nb```\n, so we repeat the procedure in the interval ```\n[a,d]```\n.\n\nAt every iteration, the interval gets shorter. When its length is minor than the specified ```\nmax_error```\n value, the algorithm returns the medium point of the last interval as an approximation of the minimum value.\n\n\n\nEDIT\n\nI see there is one person that wants to close this question because it is too broad.\nPlease sir, can you elaborate in the comments?\n    ", "Answer": "\r\nThis subdivision method only works in the special case that your function is (quasi-)convex (one local minimum, monotonically falling on the left, raising on the right). In the case of several local minima it will often converge to one of them, but it is by no means guaranteed that the algorithm finds the global minimum. The reduction from ```\na```\n to ```\nc```\n resp. from ```\nb```\n to ```\nd```\n can jump over several local minima.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximate number of colors in Bitmap\r\n                \r\nI'm looking for an approximate algorithm returning an approximate number of colors in an Image - possibly stopping when a Min number is reached.\nThe language is C#, and I'm working on Bitmap objects. An idea is to iterate over all pixels, but this can be very time expensive.\nAny ideas?\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "finding maximum weight subgraph\r\n                \r\nMy graph is as follows:\n\n\n\nI need to find a maximum weight subgraph. \n\nThe problem is as follows:\n\nThere are n Vectex clusters, and in every Vextex cluster, there are some vertexes. For two vertexes in different Vertex cluster, there is a weighted edge, and in the same Vextex cluster, there is no edge among vertexes. Now I\nwant to find a maximum weight subgraph by finding only one vertex in each \nVertex cluster. And the total weight is computed by adding all weights of the edges between the selected vertex. I add a picture to explain the problem. Now I know how to model this problem by ILP method. However, I do not know how to solve it by an approximation algorithm and how to get its approximation ratio. \n\nCould you give some solutions and suggestions?\n\nThank you very much. If any unclear points in this description, \nplease feel free to ask.\n    ", "Answer": "\r\nI do not think you can find an ```\nalpha```\n-approx for this problem, for any ```\nalpha```\n. That is because if such an approximation exists, then it would also prove that the unique games conjecture(UGC) is false. And disproving (or proving) the UGC is a rather big feat :-)\n(and I'm actually among the UGC believers, so I'd say it's impossible :p)  \n\nThe reduction is quite straightforward, since any UGC instance can be described as your problem, with weights of ```\n0```\n or ```\n1```\n on edges.  \n\nWhat I can see as polynomial approximation is a ```\n1/k```\n-approx (```\nk```\n the number of clusters), using a maximum weight perfect matching (PM) algorithm (we suppose the number of clusters is even, if it's odd just add a 'useless' one with 1 vertex, 0 weights everywhere).  \n\nFirst, you need to build a new graph. One vertex per cluster. The weight of the edge ```\nu, v```\n has the weight ```\nmax w(e)```\n for ```\ne```\n edge from cluster ```\nu```\n to cluster ```\nv```\n. Run a max weight PM on this graph.  \n\nYou then can select one vertex per cluster, the one that corresponds to the edge selected in the PM.\nThe total weight of the solution extracted from the PM is at least as big as the weight of the PM (since it contains the edges of the PM + other edges).  \n\nAnd then you can conclude that this is a ```\n1/k```\n approx, because if there exists a solution to the problem that is more than ```\nk```\n times bigger than the PM weight, then the PM was not maximal.  \n\nThe explanation is quite short (lapidaire I'd say), tell me if there is one part you don't catch/disagree with.\n\nEdit: Equivalence with UGC: unique label cover explained.\nThink of a UGC instance. Then, every node in the UGC instance will be represented by a cluster, with as many nodes in the cluster as there are colors in the UGC instance. Then, create edge with weight ```\n0```\n if they do not correspond to an edge in the UGC, or if it correspond to a 'bad color match'. If they correspond to a good color match, then give it the weight 1.\nThen, if you find the optimal solution to an instance of your problem, it means it corresponds to an optimal solution to the corresponding UGC instance.\nSo, if UGC holds, it means it is NP-hard to approximate your problem.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm to get approximate equation of a graph based on a set of points [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                    \r\n                        \r\n                    \r\n                \r\n                    \r\n                            \r\n                                As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n                Closed 10 years ago.\r\n        \r\n\r\n\r\n    \r\n\r\nI have a MySQL database with a variety of constantly updating data sets that are generally matchable to a linear or quadratic-type pattern. Are there any existing libraries, preferentially with PHP, that I can use to estimate an equation from a set of data points for predicting future data points?\n    ", "Answer": "\r\nCan't answer about the libraries, but the basic process in \"Linear Regression\". The process isn't that difficult, so you can probably write your own fairly quickly. As an aside, Postgres can has statistic functions built in that do that, MySQL might has something like that. At least now you know what to look for.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Program an iteration which will be stopped when the mentioned circumstance is met\r\n                \r\nHi everyone I have encountered a problem in writing a programming code for the algorithm as shown below\n\nThis program is going to be terminated when the approximate error which is defined as (current approximation-previous approximation)/current approximation is less than 0.01. It can be simplified as (f(xr)i+1 - f(xr)i)/f(xr)i+1. Below are the code that I have written and I would really like to know how can I program an iteration which will be stopped when the mentioned circumstance is met. \n\n```\nxl = input('Enter lower limit : ');\n\nxu = input('Enter upper limit : ');\n\nxr = (xl+xu)/2;\n\nR = 3; V = 30;\n\nfl = (pi*R*xl^2)-(pi*(xl^3)/3)-V;    % between is there anyway can call these functions \n\nfu = (pi*R*xu^2)-(pi*(xu^3)/3)-V;      other than typing 3 times\n\nfh = (pi*R*xr^2)-(pi*(xr^3)/3)-V;\n\n\nwhile relative error is less than 0.01 then display value of xr\n\nif fl*fu<0\n\n    xu = xr;\n\n\nelseif fl*fu>0\n\n    xl = xr;\n\n\nend\n\nend\n```\n\n    ", "Answer": "\r\nI updated the code now that I could run it.  I tested it with f(x)=x^2-2.  It converges to 1.4141 in 6 iterations.  I suggest you compare that code with what you had to understand what was not working for you before.  This will be a good learning experience.\n\n```\n>> example(1,2);\nCrossing found after 6 iterations: 1.414062\n```\n\n\nwhere example.m is the following:\n\n```\nfunction xr = root(xl,xu)\n\nMAX_NUMBER_ITERATIONS = 1000;\nMAX_DELTA=.01;\n\nnumberIterations=0;\nxr_old=xu;\nxr = (xl+xu)/2;\n\nwhile ((numberIterations<MAX_NUMBER_ITERATIONS) & (abs(xr_old-xr)>=MAX_DELTA))\n    numberIterations=numberIterations+1;\n    xr_old = xr;;\n\n    product=f(xl)*f(xr);\n    if product<0\n        xu = xr;\n        xr = (xl+xu)/2;\n        continue;  \n    elseif product>0\n        xl = xr;\n        xr = (xl+xu)/2;\n        continue;\n    else\n        break;\n    end\nend\nfprintf('Crossing found after %d iterations: %f\\n',numberIterations,xr)\n\nend\n\n\nfunction y = f(x)\ny=x^2-2;\nend\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximate substring matching using a Suffix Tree\r\n                \r\nThis article discusses approximate substring matching techniques that utilize a suffix tree to improve matching time. Each answer addresses a different algorithm.\n\n\nApproximate substring matching attempts to find a substring (pattern) ```\nP```\n in a string ```\nT```\n allowing up to ```\nk```\n mismatches.\nTo learn how to create a suffix tree, click here. However, some algorithms require additional preprocessing.\n\n\nI invite people to add new algorithms (even if it's incomplete) and improve answers.\n    ", "Answer": "\r\nThis was the original question that started this thread.\n\nProfessor Esko Ukkonen published a paper: Approximate string-matching over suffix trees. He discusses 3 different algorithms that have different matching times:\n\n\nAlgorithm A: ```\nO(mq + n)```\n\nAlgorithm B: ```\nO(mq log(q) + size of the output)```\n\nAlgorithm C: ```\nO(m^2q + size of the output)```\n\n\n\nWhere ```\nm```\n is the length of the substring, ```\nn```\n is the length of the search string, and ```\nq```\n is the edit distance.\n\nI've been trying to understand algorithm B but I'm having trouble following the steps. Does anyone have experience with this algorithm? An example or pseudo algorithm would be greatly appreciated.\n\nIn particular:\n\n\nWhat does ```\nsize of the output```\n refer to in terms of the suffix tree or input strings? The final output phase lists all occurrences of ```\nKey(r)```\n in ```\nT```\n, for all states ```\nr```\n marked for output.\nLooking at Algorithm C, the function dp is defined (page four); I don't understand what index ```\ni```\n represents. It isn't initialized and doesn't appear to increment.\n\n\nHere's what I believe (I stand to be corrected):\n\n\nOn page seven, we're introduced to suffix tree concepts; a state is effectively a node in the suffix tree: let ```\nroot```\n denote the initial state.\n```\ng(a, c) = b```\n where ```\na```\n and ```\nb```\n are nodes in the tree and ```\nc```\n is a character or substring in the tree. So this represents a transition; from ```\na```\n, following the edges represented by ```\nc```\n, we move to node ```\nb```\n. This is referred to as the go-to transition. So for the suffix tree below, ```\ng(root, 'ccb') = red node```\n\n\n```\nKey(a) = edge sequence```\n where ```\na```\n represents a node in the tree. For example, ```\nKey(red node) = 'ccb'```\n. So ```\ng(root, Key(red node)) = red node```\n.\n```\nKeys(Subset of node S) = { Key(node) | node ∈ S}```\n\nThere is a suffix function for nodes ```\na```\n and ```\nb```\n, ```\nf(a) = b```\n: for all (or perhaps there may exist) ```\na```\n ≠ ```\nroot```\n, there exists a character ```\nc```\n, a substring ```\nx```\n, and a node ```\nb```\n such that ```\ng(root, cx) = a```\n and ```\ng(root, x) = b```\n. I think that this means, for the suffix tree example above, that ```\nf(pink node) = green node```\n where ```\nc = 'a'```\n and ```\nx = 'bccb'```\n.\nThere is a mapping ```\nH```\n that contains a node from the suffix tree and a value pair. The value is given by ```\nloc(w)```\n; I'm still uncertain how to evaluate the function. This dictionary contains nodes that have not been eliminated.\n```\nextract-min(H)```\n refers to attaining the entry with the smallest value in the pair ```\n(node, loc(w))```\n from ```\nH```\n.\n\n\nThe crux of the algorithm seems to be related to how ```\nloc(w)```\n is evaluated. I've constructed my suffix tree using the combined answer here; however, the algorithms work on a suffix trie (uncompressed suffix tree). Therefore concepts like the depth need to be maintained and processed differently. In the suffix trie the depth would represent the suffix length; in a suffix tree, the depth would simply represent the node depth in the tree.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Maximizing overlap of high-low set\r\n                \r\nI have a finite sequence of random positive integer numbers a1,…,aM  and I am looking for an approximation algorithm (not exact) that finds  sequence of  z1,z2,z3,z4,z5,z6 (zigzag) that starts with z1=a1 in order to maximize overlap of High-Low signal, set H=[z3-z2,z5-z4] and set L=[z4-z1,z6-z3] (one of the set is less than zero and another one is greater than zero) .\nHow do I approach this optimization problem? Ideally, I am looking for an efficient algorithm that can find the optimal sequence (zi) given any input sequence (ai), typically containing millions of elements.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "C++ (and maths) : fast approximation of a trigonometric function\r\n                \r\nI know this is a recurring question, but I haven't really found a useful answer yet. I'm basically looking for a fast approximation of the function ```\nacos```\n in C++, I'd like to know if I can significantly beat the standard one.\n\nBut some of you might have insights on my specific problem: I'm writing a scientific program which I need to be very fast. The complexity of the main algorithm boils down to computing the following expression (many times with different parameters):\n\n```\nsin( acos(t_1) + acos(t_2) + ... + acos(t_n) )\n```\n\n\nwhere the ```\nt_i```\n are known real (double) numbers, and ```\nn```\n is very small (like smaller than 6). I need a precision of at least 1e-10. I'm currently using the standard ```\nsin```\n and ```\nacos```\n C++ functions.\n\nDo you think I can significantly gain speed somehow? For those of you who know some maths, do you think it would be smart to expand that sine in order to get an algebraic expression in terms of the ```\nt_i```\n (only involving square roots)?\n\nThank you your your answers.\n    ", "Answer": "\r\nThe code below provides simple implementations of ```\nsin()```\n and ```\nacos()```\n that should satisfy your accuracy requirements and that you might want to try. Please note that the math library implementation on your platform is very likely highly tuned for the specific hardware capabilities of that platform and is probably also coded in assembly for maximum efficiency, so simple compiled C code not catering to specifics of the hardware is unlikely to provide higher performance, even when the accuracy requirements are somewhat relaxed from full double precision. As Viktor Latypov points out, it may also be worthwhile to search for algorithmic alternatives that do not require expensive calls to transcendental math functions. \n\nIn the code below I have tried to stick to simple, portable constructs. If your compiler supports the ```\nrint()```\n function [specified by C99 and C++11] you might want to use that instead of ```\nmy_rint()```\n. On some platforms, the call to ```\nfloor()```\n can be expensive since it requires dynamic changing of machine state. The functions ```\nmy_rint()```\n, ```\nsin_core()```\n, ```\ncos_core()```\n, and ```\nasin_core()```\n would want to be inlined for best performance. Your compiler may do that automatically at high optimization levels (e.g. when compiling with ```\n-O3```\n), or you could add an appropriate inlining attribute to these functions, e.g. inline or __inline depending on your toolchain.\n\nNot knowing anything about your platform I opted for simple polynomial approximations, which are evaluated using Estrin's scheme plus Horner's scheme. See Wikipedia for a description of these evaluation schemes:\n\nhttp://en.wikipedia.org/wiki/Estrin%27s_scheme ,\nhttp://en.wikipedia.org/wiki/Horner_scheme\n\nThe approximations themselves are of the minimax type and were custom generated for this answer with the Remez algorithm: \n\nhttp://en.wikipedia.org/wiki/Minimax_approximation_algorithm ,\nhttp://en.wikipedia.org/wiki/Remez_algorithm\n\nThe identities used in the argument reduction for ```\nacos()```\n are noted in the comments, for ```\nsin()```\n I used a Cody/Waite-style argument reduction, as described in the following book:\n\nW. J. Cody, W. Waite, Software Manual for the Elementary Functions. Prentice-Hall, 1980\n\nThe error bounds mentioned in the comments are approximate, and have not been rigorously tested or proven. \n\n```\n/* not quite rint(), i.e. results not properly rounded to nearest-or-even */\ndouble my_rint (double x)\n{\n  double t = floor (fabs(x) + 0.5);\n  return (x < 0.0) ? -t : t;\n}\n\n/* minimax approximation to cos on [-pi/4, pi/4] with rel. err. ~= 7.5e-13 */\ndouble cos_core (double x)\n{\n  double x8, x4, x2;\n  x2 = x * x;\n  x4 = x2 * x2;\n  x8 = x4 * x4;\n  /* evaluate polynomial using Estrin's scheme */\n  return (-2.7236370439787708e-7 * x2 + 2.4799852696610628e-5) * x8 +\n         (-1.3888885054799695e-3 * x2 + 4.1666666636943683e-2) * x4 +\n         (-4.9999999999963024e-1 * x2 + 1.0000000000000000e+0);\n}\n\n/* minimax approximation to sin on [-pi/4, pi/4] with rel. err. ~= 5.5e-12 */\ndouble sin_core (double x)\n{\n  double x4, x2, t;\n  x2 = x * x;\n  x4 = x2 * x2;\n  /* evaluate polynomial using a mix of Estrin's and Horner's scheme */\n  return ((2.7181216275479732e-6 * x2 - 1.9839312269456257e-4) * x4 + \n          (8.3333293048425631e-3 * x2 - 1.6666666640797048e-1)) * x2 * x + x;\n}\n\n/* minimax approximation to arcsin on [0, 0.5625] with rel. err. ~= 1.5e-11 */\ndouble asin_core (double x)\n{\n  double x8, x4, x2;\n  x2 = x * x;\n  x4 = x2 * x2;\n  x8 = x4 * x4;\n  /* evaluate polynomial using a mix of Estrin's and Horner's scheme */\n  return (((4.5334220547132049e-2 * x2 - 1.1226216762576600e-2) * x4 +\n           (2.6334281471361822e-2 * x2 + 2.0596336163223834e-2)) * x8 +\n          (3.0582043602875735e-2 * x2 + 4.4630538556294605e-2) * x4 +\n          (7.5000364034134126e-2 * x2 + 1.6666666300567365e-1)) * x2 * x + x; \n}\n\n/* relative error < 7e-12 on [-50000, 50000] */\ndouble my_sin (double x)\n{\n  double q, t;\n  int quadrant;\n  /* Cody-Waite style argument reduction */\n  q = my_rint (x * 6.3661977236758138e-1);\n  quadrant = (int)q;\n  t = x - q * 1.5707963267923333e+00;\n  t = t - q * 2.5633441515945189e-12;\n  if (quadrant & 1) {\n    t = cos_core(t);\n  } else {\n    t = sin_core(t);\n  }\n  return (quadrant & 2) ? -t : t;\n}\n\n/* relative error < 2e-11 on [-1, 1] */\ndouble my_acos (double x)\n{\n  double xa, t;\n  xa = fabs (x);\n  /* arcsin(x) = pi/2 - 2 * arcsin (sqrt ((1-x) / 2)) \n   * arccos(x) = pi/2 - arcsin(x)\n   * arccos(x) = 2 * arcsin (sqrt ((1-x) / 2))\n   */\n  if (xa > 0.5625) {\n    t = 2.0 * asin_core (sqrt (0.5 * (1.0 - xa)));\n  } else {\n    t = 1.5707963267948966 - asin_core (xa);\n  }\n  /* arccos (-x) = pi - arccos(x) */\n  return (x < 0.0) ? (3.1415926535897932 - t) : t;\n}\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Preserving shape local convexity/concavity with Matlab interpolation\r\n                \r\nI am trying to interpolate an array of scattered values (asterisks in the plots). The algorithms that worked best so far are:\nSpline\n\npchip\n\nHowever, they both fail in preserving a natural trend of the curve between the 30° and 360° directions. The right approximation should be somewhere in between. As it is clear, the two different behaviors depend on the slope of the interpolated line right before the known values.\nIs there some way to constrain the algorithm so that it doesn't show such an artificial trend?\nThank you in advance for any advice.\n    ", "Answer": "\r\nYou can \"unwrap\" your polar plot into a cartesian plot to better understand what's going on here. As shown below, the extrapolation to ```\n0```\n and ```\n360```\n degrees with ```\npchip```\n is very reasonable in x-y space given the inputs.\nIf you want to dictate the behaviour across the end points then you need to add artificial endpoints to the input data which force the interpolation to be cyclic.\ni.e.\n```\n% Input points\na = 30:30:330;\nb = [0.6, 0.8, 0.7, 0.3, 0.3, 0.0, 0.3, 0.3, 0.7, 0.8, 0.6];\n% Extend the inputs by wrapping f(330deg)=f(-30deg), f(390deg)=f(30deg)\na_ext = [-30, a, 390]; \nb_ext = [b(end), b, b(1)];\n% convert to radians    \na = deg2rad(a);\na_ext = deg2rad(a_ext);\n\nfigure();\n% Polar plot\nsubplot(2,1,1)\npolarplot( a, b, '.', 'markersize', 20 )\nhold on\nainterp = linspace(0,2*pi,100);\npolarplot( ainterp, pchip(a,b,ainterp), 'linewidth', 1 )\npolarplot( ainterp, pchip(a_ext,b_ext,ainterp), 'k', 'linewidth', 1 )\nrlim( [-0.2, 1] )\nlegend( {'Input points','pchip','pchip extended'} )\n% Cartesian plot\nsubplot(2,1,2)\nplot( rad2deg(a), b, '.', 'markersize', 20 )\nhold on\nplot( rad2deg(ainterp), pchip(a,b,ainterp), 'linewidth', 1.5 )\nplot( rad2deg(ainterp), pchip(a_ext,b_ext,ainterp), 'k', 'linewidth', 1.5 )\nylim( [-0.2, 1] ); xlim( [0, 360] ); grid on\n```\n\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Finding a polygonal approximation of a Closed Path\r\n                \r\nI'd like to be able to find the best fitting polygonal approximation of a closed path (could be any path as they're being pulled out of images) but am having issues with how to approach coding an algorithm to find it. \n\nI can think of a naive approach: every x amount of pixels along the path, choose the best fit line for those pixels, then brute force for different starting offsets and lengths and find the one that minimizes the least-square error with the minimum amount of lines.\n\nThere's got to be something more elegant. Anyone know of anything? Also, (cringe) but this is going to be implemented in javascript unless I get really desperate, so nice libraries that do things for you are pretty much ruled out, (opencv has a polygonal fitter for instance).\n    ", "Answer": "\r\nD3.js1 has some adaptive resampling code that you might be able to use. There's also an illustrated description of the algorithm used (Visvalingam’s algorithm).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "OpenGL Displaying Two Arrays\r\n                \r\nI'm needing some help with some c++/OpenGL coding. What I'm doing is a polygon approximation algorithm. \n\nMy code first pulls in a set of points from a .txt file, stores them all in an array and then displays that array. It then takes those points and performs the algorithm on them, and creates a new array of points. What I can't figure out how to do, is how to get that second set of points displayed on the same window as the first. Do I have to create a new display function and call that one? Or maybe modify the basic display function that I have now to accept an array? Here's the code for my display function:\n\n```\nvoid display(void){\n    glClearColor(0,0,0,0);\n    glClear(GL_COLOR_BUFFER_BIT);\n    glColor3f(1,1,1);\n\n    glBegin(GL_POINTS);\n    for(int i=0; i<2000; i++)\n        glVertex2i(pixel[i].x,pixel[i].y);\n    glEnd();\n\n    glFlush();\n}\n```\n\n    ", "Answer": "\r\nYou just need to draw the processed array. Considering that you just want to render the resulting points, like in your code sample, you could use:\n\n```\nvoid display(void){\n  glClearColor(0,0,0,0);\n  glClear(GL_COLOR_BUFFER_BIT);\n  glColor3f(1,1,1);\n\n  glBegin(GL_POINTS);\n  for(int i=0; i<2000; i++)\n    glVertex2i(pixel[i].x,pixel[i].y);\n  // here goes the rendering of the new set of points.\n  glColor3f(1,0,0); // change the color so we can see better the new points.\n  for(int i=0; i<2000; i++)\n    glVertex2i(result[i].x,result[i].y);\n  glEnd();\n\n  glFlush();\n}\n```\n\n\nThe variable ```\nresult```\n is your array with the results of the processing.\n\nYou can not modify the ```\ndisplay```\n function since it is called by the OpenGL and it does not know about your arrays. But nothing goes against you splitting your code into many functions called by your ```\ndisplay```\n function.\n\nHope it helps.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Is there an algorithm to reduce the sides of a Polygon while keeping all Points within the Polygon?\r\n                \r\nI'm trying to reduce a polygon with 4+ vertexes to a polygon with 4 sides to perform perspective transformation later on. I need to find the polygon with 4 sides which contains all the points the original polygon had. Basicly what i want is something like this: \n\n\nThe real problem here is that the polygon must only get bigger... if it gets smaller with let's say a polygon approximation algorithm it's not usefull anymore...\n\nEDIT: \nI need the optimal solution, that means that the resulting 4-sided polygon has as little area as possible!\n\nEDIT2: \nWhat would also work is an convex hull algorithm where I can determine the number of sides the resulting polygon must have!\n    ", "Answer": "\r\nThe easiest solution is to take the bounding box of your polygon, the rectangle defined by the min and max of the x values of your vertices, and the min and max of the y values.\n\nIf you need a 4-vertex polygon with smaller area, an idea could be:\n\n\nTake the convex hull of your polygon.\nSelect one side for deletion, and extend its neighboring sides to the point where they meet. Do this only if they really meet at the end where the deleted side was. Maybe you want to select the side to be deleted by the smallest area that this deletion adds to the polygon (that's the triangle formed by the deleted side and the new intersection point).\nRepeat until only 4 sides are left.\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Bicubic interpolation artifacts (image upscale)\r\n                \r\nI am using a bicubic interpolation algorithm in order to upscale an height map, and I am noticing some artifacts around the pixels boundaries.\nHowever, these artifacts don't seem to appear when I use a simple cubic interpolation (spline).\n\nCould it be because the bicubic interpolation doesn't guarantee the second derivative to be continuous, unlike the cubic spline ?\nIf so, is there known algorithms that have a continuous second derivative ?\nOtherwise, is there a way to deal with these artifacts ?\n\nLinear interpolation (shows the pixels boundaries):\n\n\nBicubic interpolation (artifacts visible at pixels boundaries):\n\n\nCubic interpolation (no noticeable artifacts):\n\n\nI tried several bicubic formulas, which gave me the same results. Here are some examples:\n\n\nhttp://paulbourke.net/miscellaneous/interpolation\nhttp://www.paulinternet.nl/?page=bicubic\n\n\n\n\nEdit:\nI made some searches and found that B-Spline have a continuous C2 (also suggested by Bharat). I implemented it and it looks fine, even if it's an approximation and not an interpolation (it doesn't go through the samples).\n\nB-spline (approximation):\n\n    ", "Answer": "\r\nsecond derivative of cubic B-Spline is continuous while that of bicubic interpolation is not.\n\nhttp://en.wikipedia.org/wiki/Spline_interpolation\nhttp://en.wikipedia.org/wiki/Cubic_interpolation\n\nhttps://math.stackexchange.com/questions/485935/piecewise-interpolation-with-derivatives-that-is-also-twice-differentiable\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm for partitioning set of sets into approximately equal-sized chunks?\r\n                \r\nConsider a set A of n finite sets, whose members are not necessarily disjoint. Let P={P[1], P[2], ..., P[m]} be a partition of A, and for each i in 1..m, let U[i] be the union of all of the elements of P[i]. So U={U[1], U[2], ..., U[m]}. I would like an algorithm to a find a P such that the corresponding U is a partition, and such that the difference in cardinality (i.e. size) between the smallest and largest elements of U is minimised.\n\nCharacteristics of the data:\n\n\nm is small (2 to 5) and n<10000\nTypically, there is a large proportion of 1-element sets in A\nIntersections between pairs of sets in A are typically small or empty\n\n    ", "Answer": "\r\nMy necklace analogy in the question comments suggests this solution:\n\n\nBuild an undirected graph G whose vertices are the elements of A, and where there is an edge from A[i] to A[j] iff A[i] intersects A[j].\nFind the connected components C of G. This can be done with a simple breadth-first or depth-first algorithm.\nFor each C[i], take the vertices of C[i] and union them together, yielding D[i]. You now have reduced the problem to a special case, because the set D is a partition of the union of the elements of A.\nUse a bin-packing algorithm to try and fit the elements of D into precisely m bins, each of size ceil(t/m) where t is the size of the union of all the elements of D. If that fails, increase the sizes of the bins repeatedly until either it succeeds or it's clear that it's never going to succeed. Bin-packing algorithms are typically heuristic, so a perfect solution might not be found. Also, this is more than a simple bin-packing problem, so even a perfect bin-packing algorithm might not find the optimal solution.\n\n\nI'd be interested to know if there is a more efficient solution. In particular, I have a hunch that the repeated use of the bin-packing algorithm in step 4 is not sensible.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "k-center algorithm in one-dimensional space\r\n                \r\nI'm aware of the general k-center approximation algorithm, but my professor (this is a question from a CS class) says that in a one-dimensional space, the problem can be solved (optimal solution found, not an approximation) in ```\nO(n^2)```\n polynomial time without depending on ```\nk```\n or using dynamic programming.\n\nAs you might expect, I can't figure out how this is possible. The part currently causing me problems is how the runtime can not rely on ```\nk```\n.\n\nThe nature of the problem causes me to try to step through the nodes on a sort of number line and try to find points to put boundaries, marking off the edges of each cluster that way. But this would require a runtime based on ```\nk```\n.\n\nThe ```\nO(n^2)```\n runtime though makes me think it might involve filling out an ```\nnxn```\n array with the distance between two nodes in each entry.\n\nAny explanation on how this is works or tips on how to figure it out would be very helpful.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Find closest approximation of Pi for given range of denominators\r\n                \r\nI am looking for an algorithm (preferably in Go or C) to find the closest common fraction n/d to Pi out of an inclusive range of possible denominators (dmin,dmax with 1 <= dmin <= dmax <= 1e15). If there are multiple common fractions with the same distance to Pi, i want to find the one with the smallest denominator.\n\nNote: A bruteforce approach is not efficient enough, therefore i am looking for a smarter / more efficient solution.\n\nExample: For dmin=1 and dmax=10 the closest common fraction is 22/7 with a distance to Pi of approx 0.001\n\nFirst thought: Looking at the farey sequence, we could find the closest approximation for all denominators up to dmax. Unfortunately that result does not fulfill the constraint of dmin.\n    ", "Answer": "\r\nI don't have time for a full answer, but here is a partial answer. This technique uses the concepts of continued fractions--there is much about them online. I'll ignore your value dmin, which is not used below.\n\nGet the continued fraction expansion of pi to as many places as you need. For your bound of dmax <= 1e15 you need only the first 28 numbers, which are\n\n```\n[3, 7, 15, 1, 292, 1, 1, 1, 2, 1, 3, 1, 14, 2, 1, 1, 2, 2, 2, 2, 1, 84, 2, 1, 1, 15, 3, 13]\n```\n\n\nUse a short loop to find the convergents for pi that have denominators just below and just above dmax. In Python that would be\n\n```\npi_cont_frac = [3, 7, 15, 1, 292, 1, 1, 1, 2, 1, \n                3, 1, 14, 2, 1, 1, 2, 2, 2, 2,\n                1, 84, 2, 1, 1, 15, 3, 13]\ndenomlo, denomhi = 1, 0\nnumlo, numhi = 0, 1\nfor q in pi_cont_frac:\n    denomlo, denomhi = denomhi, q * denomhi + denomlo\n    numlo, numhi = numhi, q * numhi + numlo\n    if denomhi > dmax:\n        break\n```\n\n\nSome software, such as Microsoft Excel, would use the fraction ```\nnumlo/denomlo```\n, but there may be better approximation than that. Now find the value of natural number r that makes ```\ndenomhi - r * denomlo```\n just below (or equal to) dmax.\n\nThen either ```\nnumlo/denomlo```\n or ```\n(denomhi - r * denomlo)/(denomhi - r * denomlo)```\n is your desired closest fraction to pi. Just check which one is closer.\n\nThis algorithm is of order log(dmax), and due to the properties of pi it is usually much lower. For dmax <= 1e15 it takes 28 loops but a few more clean-up statements.\n\nYou could make a faster algorithm by pre-calculating and storing the convergents (values of numhi and denomhi) and doing a search for the value of denomhi just above dmax. This also only takes 28 numbers, but you would need this for both the numerators and the denominators. A binary search would take at most 5 steps to find it--practically instantaneous. Yet another possibility using more storage and less calculation would be to store all intermediate fractions. That storage would go into the hundreds, at least three hundred. If you don't like that stored list for the continued fraction expansion of pi, you could use the value of pi to calculate that on the fly, but using double precision (in C) would get you only to the 28 numbers I showed you.\n\nFor more research, look up continued fractions and intermediate fractions.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm to find top 10 search terms\r\n                \r\nI'm currently preparing for an interview, and it reminded me of a question I was once asked in a previous interview that went something like this:\n\n\"You have been asked to design some software to continuously display the top 10 search terms on Google. You are given access to a feed that provides an endless real-time stream of search terms currently being searched on Google. Describe what algorithm and data structures you would use to implement this. You are to design two variations: \n\n(i) Display the top 10 search terms of all time (i.e. since you started reading the feed). \n\n(ii) Display only the top 10 search terms for the past month, updated hourly. \n\nYou can use an approximation to obtain the top 10 list, but you must justify your choices.\"\nI bombed in this interview and still have really no idea how to implement this. \n\nThe first part asks for the 10 most frequent items in a continuously growing sub-sequence of an infinite list. I looked into selection algorithms, but couldn't find any online versions to solve this problem. \n\nThe second part uses a finite list, but due to the large amount of data being processed, you can't really store the whole month of search terms in memory and calculate a histogram every hour. \n\nThe problem is made more difficult by the fact that the top 10 list is being continuously updated, so somehow you need to be calculating your top 10 over a sliding window.\n\nAny ideas?\n    ", "Answer": "\r\nFrequency Estimation Overview\n\nThere are some well-known algorithms that can provide frequency estimates for such a stream using a fixed amount of storage. One is Frequent, by Misra and Gries (1982). From a list of n items, it find all items that occur more than n / k times, using k - 1 counters. This is a generalization of Boyer and Moore's Majority algorithm (Fischer-Salzberg, 1982), where k is 2. Manku and Motwani's LossyCounting (2002) and Metwally's SpaceSaving (2005) algorithms have similar space requirements, but can provide more accurate estimates under certain conditions. \n\nThe important thing to remember is that these algorithms can only provide frequency estimates. Specifically, the Misra-Gries estimate can under-count the actual frequency by (n / k) items. \n\nSuppose that you had an algorithm that could positively identify an item only if it occurs more than 50% of the time. Feed this algorithm a stream of N distinct items, and then add another N - 1 copies of one item, x, for a total of 2N - 1 items. If the algorithm tells you that x exceeds 50% of the total, it must have been in the first stream; if it doesn't, x wasn't in the initial stream. In order for the algorithm to make this determination, it must store the initial stream (or some summary proportional to its length)! So, we can prove to ourselves that the space required by such an \"exact\" algorithm would be Ω(N).\n\nInstead, these frequency algorithms described here provide an estimate, identifying any item that exceeds the threshold, along with some items that fall below it by a certain margin. For example the Majority algorithm, using a single counter, will always give a result; if any item exceeds 50% of the stream, it will be found. But it might also give you an item that occurs only once. You wouldn't know without making a second pass over the data (using, again, a single counter, but looking only for that item).\n\nThe Frequent Algorithm\n\nHere's a simple description of Misra-Gries' Frequent algorithm. Demaine (2002) and others have optimized the algorithm, but this gives you the gist.\n\nSpecify the threshold fraction, 1 / k; any item that occurs more than n / k times will be found. Create an an empty map (like a red-black tree); the keys will be search terms, and the values will be a counter for that term. \n\n\nLook at each item in the stream. \nIf the term exists in the map, increment the associated counter. \nOtherwise, if the map less than k - 1 entries, add the term to the map with a counter of one.\nHowever, if the map has k - 1 entries already, decrement the counter in every entry. If any counter reaches zero during this process, remove it from the map.\n\n\nNote that you can process an infinite amount of data with a fixed amount of storage (just the fixed-size map). The amount of storage required depends only on the threshold of interest, and the size of the stream does not matter.\n\nCounting Searches\n\nIn this context, perhaps you buffer one hour of searches, and perform this process on that hour's data. If you can take a second pass over this hour's search log, you can get an exact count of occurrences of the top \"candidates\" identified in the first pass. Or, maybe its okay to to make a single pass, and report all the candidates, knowing that any item that should be there is included, and any extras are just noise that will disappear in the next hour.\n\nAny candidates that really do exceed the threshold of interest get stored as a summary. Keep a month's worth of these summaries, throwing away the oldest each hour, and you would have a good approximation of the most common search terms.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Why machine learning algorithms focus on speed and not accuracy?\r\n                \r\nI study ML and I see that most of the time the focus of the algorithms is run time and not accuracy. Reducing features, taking sample from the data set, using approximation and so on.\n\nIm not sure why its the focus since once I trained my model I dont need to train it anymore if my accuracy is high enough and for that if it will take me 1 hours or 10 days to train my model it does not really matter because I do it only 1 time and my goal is to predict as better as I can my outcomes (minimum loss).\n\nIf I train a model to differ between cats and dogs I want it to be the most accurate it can be and not the fasted since once I trained this model I dont need to train any more models. \nI can understand why models that depends on fasting changing data need this focus of speed but for general training models I dont understand why the focus is on speed.\n    ", "Answer": "\r\nSpeed is relative term. Accuracy is also relative depending on the difficulty of the task. Currently the goal is to achieve human-like performance for application at reasonable costs because this will replace human labor and cut costs.\n\nFrom what I have seen in reading papers, people usually focus on accuracy first to produce something that works. Then do ablation studies - studies where pieces of the models are removed or modified - to achieve the same performance in less time or memory requirements.\n\nThe field is very experimentally validated. There really isn't much of a theory that states why CNN work so well other than that it can model any function given non-linear activations functions. (https://en.wikipedia.org/wiki/Universal_approximation_theorem) There have been some recent efforts to explain why it works well. One I recall is MobileNetV2: Inverted Residuals and Linear Bottlenecks. The explaination of embedding data into a low dimensional space without losing information might be worth reading.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm to find top 10 search terms\r\n                \r\nI'm currently preparing for an interview, and it reminded me of a question I was once asked in a previous interview that went something like this:\n\n\"You have been asked to design some software to continuously display the top 10 search terms on Google. You are given access to a feed that provides an endless real-time stream of search terms currently being searched on Google. Describe what algorithm and data structures you would use to implement this. You are to design two variations: \n\n(i) Display the top 10 search terms of all time (i.e. since you started reading the feed). \n\n(ii) Display only the top 10 search terms for the past month, updated hourly. \n\nYou can use an approximation to obtain the top 10 list, but you must justify your choices.\"\nI bombed in this interview and still have really no idea how to implement this. \n\nThe first part asks for the 10 most frequent items in a continuously growing sub-sequence of an infinite list. I looked into selection algorithms, but couldn't find any online versions to solve this problem. \n\nThe second part uses a finite list, but due to the large amount of data being processed, you can't really store the whole month of search terms in memory and calculate a histogram every hour. \n\nThe problem is made more difficult by the fact that the top 10 list is being continuously updated, so somehow you need to be calculating your top 10 over a sliding window.\n\nAny ideas?\n    ", "Answer": "\r\nFrequency Estimation Overview\n\nThere are some well-known algorithms that can provide frequency estimates for such a stream using a fixed amount of storage. One is Frequent, by Misra and Gries (1982). From a list of n items, it find all items that occur more than n / k times, using k - 1 counters. This is a generalization of Boyer and Moore's Majority algorithm (Fischer-Salzberg, 1982), where k is 2. Manku and Motwani's LossyCounting (2002) and Metwally's SpaceSaving (2005) algorithms have similar space requirements, but can provide more accurate estimates under certain conditions. \n\nThe important thing to remember is that these algorithms can only provide frequency estimates. Specifically, the Misra-Gries estimate can under-count the actual frequency by (n / k) items. \n\nSuppose that you had an algorithm that could positively identify an item only if it occurs more than 50% of the time. Feed this algorithm a stream of N distinct items, and then add another N - 1 copies of one item, x, for a total of 2N - 1 items. If the algorithm tells you that x exceeds 50% of the total, it must have been in the first stream; if it doesn't, x wasn't in the initial stream. In order for the algorithm to make this determination, it must store the initial stream (or some summary proportional to its length)! So, we can prove to ourselves that the space required by such an \"exact\" algorithm would be Ω(N).\n\nInstead, these frequency algorithms described here provide an estimate, identifying any item that exceeds the threshold, along with some items that fall below it by a certain margin. For example the Majority algorithm, using a single counter, will always give a result; if any item exceeds 50% of the stream, it will be found. But it might also give you an item that occurs only once. You wouldn't know without making a second pass over the data (using, again, a single counter, but looking only for that item).\n\nThe Frequent Algorithm\n\nHere's a simple description of Misra-Gries' Frequent algorithm. Demaine (2002) and others have optimized the algorithm, but this gives you the gist.\n\nSpecify the threshold fraction, 1 / k; any item that occurs more than n / k times will be found. Create an an empty map (like a red-black tree); the keys will be search terms, and the values will be a counter for that term. \n\n\nLook at each item in the stream. \nIf the term exists in the map, increment the associated counter. \nOtherwise, if the map less than k - 1 entries, add the term to the map with a counter of one.\nHowever, if the map has k - 1 entries already, decrement the counter in every entry. If any counter reaches zero during this process, remove it from the map.\n\n\nNote that you can process an infinite amount of data with a fixed amount of storage (just the fixed-size map). The amount of storage required depends only on the threshold of interest, and the size of the stream does not matter.\n\nCounting Searches\n\nIn this context, perhaps you buffer one hour of searches, and perform this process on that hour's data. If you can take a second pass over this hour's search log, you can get an exact count of occurrences of the top \"candidates\" identified in the first pass. Or, maybe its okay to to make a single pass, and report all the candidates, knowing that any item that should be there is included, and any extras are just noise that will disappear in the next hour.\n\nAny candidates that really do exceed the threshold of interest get stored as a summary. Keep a month's worth of these summaries, throwing away the oldest each hour, and you would have a good approximation of the most common search terms.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Why machine learning algorithms focus on speed and not accuracy?\r\n                \r\nI study ML and I see that most of the time the focus of the algorithms is run time and not accuracy. Reducing features, taking sample from the data set, using approximation and so on.\n\nIm not sure why its the focus since once I trained my model I dont need to train it anymore if my accuracy is high enough and for that if it will take me 1 hours or 10 days to train my model it does not really matter because I do it only 1 time and my goal is to predict as better as I can my outcomes (minimum loss).\n\nIf I train a model to differ between cats and dogs I want it to be the most accurate it can be and not the fasted since once I trained this model I dont need to train any more models. \nI can understand why models that depends on fasting changing data need this focus of speed but for general training models I dont understand why the focus is on speed.\n    ", "Answer": "\r\nSpeed is relative term. Accuracy is also relative depending on the difficulty of the task. Currently the goal is to achieve human-like performance for application at reasonable costs because this will replace human labor and cut costs.\n\nFrom what I have seen in reading papers, people usually focus on accuracy first to produce something that works. Then do ablation studies - studies where pieces of the models are removed or modified - to achieve the same performance in less time or memory requirements.\n\nThe field is very experimentally validated. There really isn't much of a theory that states why CNN work so well other than that it can model any function given non-linear activations functions. (https://en.wikipedia.org/wiki/Universal_approximation_theorem) There have been some recent efforts to explain why it works well. One I recall is MobileNetV2: Inverted Residuals and Linear Bottlenecks. The explaination of embedding data into a low dimensional space without losing information might be worth reading.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Probabilistic selection algorithm\r\n                \r\nGiven is:\n\n\nAn array of length ```\nN```\n.\nThe array contains integers.\nThe integers are not necessarily sorted.\n\n\nFind an algorithm that:\n\n\nReturns (a close approximation of)the ```\nK```\n-th smallest array element.\nHas a runtime complexity of O(```\nN```\n log ```\nN```\n) and a space complexity of O(log ```\nN```\n).\nThe algorithm needn't be deterministic. In case of a probabilistic algorithm also provide a measure for the quality of the approximated result.\n\n    ", "Answer": "\r\nTreat the problem as something analogous to Quicksort.  Given an element in the array, you can get its rank in O(n) time and O(lg n) space.  You can use binary search to find an element with a given rank in O(lg n) iterations of that, for a total of O(lg n) space and O(n lg n) time.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to use an approximate string matching algorithm in android sqlite?\r\n                \r\ni want to create an android application that can search word in sqlite database using approximate string matching.\nfor example if someone misspelled the word \"switch\" with \" swithc\", the sistem will correct the word and show message \"did you mean 'switch' \".\n\nits like google that can correct wrong word. how can i do it ? \n    ", "Answer": "\r\nhave a look at this answer\n\nJava: how to find the most probable string in a list of strings?\n\nyou can use the way of string matching as you desire. there is also a github project for this at: \nhttps://github.com/northpoint/blur\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Normalize two arrays with second array the base for normalizing first array\r\n                \r\nIn order to find the quality indicators like Generational Distance, Inverted Generational Distance, Epsilon Indicator, and HyperVolume for a Pareto front I want to normalize the values of approximation front obtained on solving the algorithm based on reference front which I assume encloses the approximation front.\n```\nreference_front = np.array([[0.5, 2.0], [1, 1.0], [1.2, 0.833], [2.3, 0.435], [3, 0.333]])\napproximation_front = np.array([[0.8, 2.5], [1.0, 2.0], [2.1, 0.952], [2.8, 0.714]])\nreference_point = [max(approximation_front[:,0]),max(approximation_front[:,0])]\n```\n\nI have used the code below for normalization. However, it is for one array at a time\n```\nfrom sklearn.preprocessing import MinMaxScaler\n    min_max_scaler = MinMaxScaler()\nreference_front_norm = min_max_scaler.fit_transform(reference_front)\napproximation_front_norm = min_max_scaler.fit_transform(approximation_front)\nreference_point = [max(approximation_front[:,0]),max(approximation_front[:,0])]\n```\n\nHere, approximation front and reference front are normalized separately. Can we normalize the approximate front between 0 to 1 based on max and min values of reference front.\n    ", "Answer": "\r\nAfter applying ```\nfit_transform```\n, you can simply use ```\ntransform```\n. This will use the 'fit' from the ```\nfit_transform```\n call. In your case\n```\nreference_front_norm = min_max_scaler.fit_transform(reference_front)\napproximation_front_norm = min_max_scaler.transform(approximation_front)\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Hamiltonian path functional way\r\n                \r\nI am trying to work on a problem which gives me a hamiltonian path in a graph. I am aware of the algorithms used for it but they're all suited for imperative style. My confusion is if I have to use dynamic programming in scala to solve the problem what would be the best approach.Also is there a better algorithm which gives better efficiency than DP (both memory and space) ? Approximation is something I can think of but as far as my knowledge goes it requires a complete graph. Please enlighten me. Thanks!\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Any known linear algorithm to approximate a function with line segments?\r\n                \r\nI have a function given by a list of points, ex:\n\n```\nf = [0.03, 0.05, 0.02, 1.3, 1.0, 5.6, ..., 13.4, 12.45]\n```\n\n\nI need an algorithm (with linear complexity) to \"cut\" this function/list into K intervals/sublists so that each interval/sublist contains points that \"lie near a line segment\" (take a look at the image)\n\n\nThe number K may be decided either by the algorithm itself or be a parameter of the algorithm. (preferable is to be decided by the algorithm itself)\n\nIs there such a known algorithm I could use ?\n    ", "Answer": "\r\ni am writing with smartphone so this is short. Basically a function is nearly linear if the difference between two consecutive values is approximately equal see http://psn.virtualnerd.com/viewtutorial/PreAlg_13_01_0006\n\nAs an algorithm for traversing an unsorted array Sliding Window is nice ( https://www.geeksforgeeks.org/window-sliding-technique/ ) and can be implemented by a single pass (1-pass solution)\n\nUpdate because comment :\n\nSo with a sliding window you can implement the vagueness or fuzziness of the values you mentioned in the comment this is why nearly linear and approximately, i.e. \n\n```\nif(abs(abs(x[i]-x[i+1]) - abs(x[i+1]-x[i+2])) < 0.5)\n      {linearity_flag=1;} \nelse \n      {linearity_flag=0;}\n```\n\n\nwhere ```\nx[i]-x[i+1]```\n and ```\nx[i+1]-x[i+2]```\n are two consecutive differences of two consecutive values and 0.5 is a deliberately chosen threshold that fixes what you define as a straight line or linear function in an x-y graph (or what 'jittering' of the line you allow). So you have to use the difference of differences of consecutive values. Instead of 3 points you can also include more points with this approach (sliding window)\n\nIf you want a strict mathematical ansatz you could use other curve analysis techniques : https://openstax.org/books/calculus-volume-1/pages/4-5-derivatives-and-the-shape-of-a-graph (actually the difference of differences of consecutive values is a discrete realization of a 2nd derivative)\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm to get an approximate depth map from an embossed/relief image\r\n                \r\nI am attempting to read information from pictures of hard plastic ID cards.  As a first step, I've been trying to process the pictures to make the text more computer-readable.  The pictures are fairly clear, but they are tricky because they are light on one side and dark on the other.  It seems like it should be possible to use this information to create a depth map, which could then be converted to a black and white image.  Mainly, I'd like to know if there is some known algorithm (the simpler the better) I could implement.  I'm currently doing the rest of the processing using Python and PIL, but any implementation I could adapt would be great.\n\nA small example of the images I'm working with:\n\n\n    ", "Answer": "\r\nExample in Mathematica. If the result is satisfactory I could explain the procedure step by step.  \n\n```\nErosion[\n ColorNegate@\n  Thinning@\n   Dilation[\n    DeleteSmallComponents[\n     DeleteBorderComponents@\n      ColorNegate@\n       Binarize@Import[\"http://i.imgur.com/GLzvj.png\"],\n     150],\n    8],\n 8]\n```\n\n\n\n\nEdit\n\nStep by step ... \n\nStarting with   \n\n\n\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithms for reporting the progress of asynchronous function calls\r\n                \r\nWhat sort of algorithms are used to compute the progress of a function call? \n\nObviously its easy to do if you have a loop, something like:\n\n```\n Action<Double> notifier = d => Console.WriteLine(\"Completed: \" + d.ToString());\n\n Double _interval = 0.05;\n\n for (int i = 0; i < 1000; i++)\n            {\n                int rem = default(int); \n\n                Math.DivRem(i, _interval, out rem);\n\n                if (rem == 0)\n                {\n                    double progress = (double)i / 1000;\n                    notifier(progress); \n                }\n             }\n```\n\n\nBut what about when you just have some generic delegate and you'd like to run it asynchronously but also notify another thread of its progress, and you can't gaurantee that you can straightforwardly use a for-loop? Some other straightforward ways might be:\n\n1) Time the function first (very bad for performance though if its a long running task)\n\n2) Store past timings of the function in a log and use that (but this doesnt necessarily take into account extra variables - cpu, memory, etc. at the specific time of the task)\n\nHowever do any more advanced algorithms exist, even if only to approximate the progress? \n    ", "Answer": "\r\nThe ```\nBackgroundWorker```\n class has an update callback, but in answer to your question of a 'generic algorithm' for finding completion, Not really. The closest you can get is estimating based on Function Length (http://www.ndepend.com/) which will get you length in lines of code.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Approximation of a common divisor closest to some value?\r\n                \r\nSay we have two numbers (not necessarily integers) ```\nx1```\n and ```\nx2```\n. Say, the user inputs a number ```\ny```\n. What I want to find, is a number ```\ny'```\n close to ```\ny```\n so that ```\nx1 % y'```\n and ```\nx2 % y'```\n are very small (smaller than ```\n0.02```\n, for example, but lets call this number ```\nLIMIT```\n). In other words, I don't need an optimal algorithm, but a good approximation.\n\nI thank you all for your time and effort, that's really kind!\n\n\n\nLet me explain what the problem is in my application : say, a screen size is given, with a width of ```\nscreenWidth```\n and a height of ```\nscreenHeight```\n (in pixels). I fill the screen with squares of a length ```\ny'```\n. Say, the user wants the square size to be ```\ny```\n. If ```\ny```\n is not a divisor of ```\nscreenWidth```\n and/or ```\nscreenHeight```\n, there will be non-used space at the sides of the screen, not big enough to fit squares. If that non-used space is small (e.g. one row of pixels), it's not that bad, but if it's not, it won't look good. How can I find common divisors of ```\nscreenWidth```\n and ```\nscreenHeight```\n?\n    ", "Answer": "\r\nI don't see how you can ensure that x1%y' and x2%y' are both below some value - if x1 is prime, nothing is going to be below your limit (if the limit is below 1) except x1 (or very close) and 1.\n\nSo the only answer that always works is the trivial y'=1.\n\nIf you are permitting non-integer divisors, then just pick y'=1/(x1*x2), since the remainder is always 0.\n\nWithout restricting the common divisor to integers, it can be anything, and the whole 'greatest common divisor' concept goes out the window. \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Why branch-and-bound is approximate and not exact?\r\n                \r\nI have read that branch-and-bound paradigm is approximate. Can someone explain why this method is approximate and not exact? what it means that it's approximation? I implemented it for knapsack problem and always when I run this algorithm, it gives me exact answer.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Bundling neighbouring roads of a vector map\r\n                \r\nI'm trying to simplify a city map by bundling neighbouring parallel roads together by taking their average and by removing roads that are unnecessary for the essential road structure (e.g. short roads that are enclosed by other roads). Are there any well-known algorithms that address this problem?\nI'm aware of the Ramer–Douglas–Peucker algorithm, but I don't think it's a good-enough fit for the problem. I'm using Qgis and Openstreetmap maps. Thank you in advance.\nHere's an illustration of the problem. On the left: the current map information; on the right: approximation of the desired output.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Calculating the difference in percentage of two HTML files\r\n                \r\nI'm writing a tool in php which compares HTML files and shows the differences. Now I'm looking for an efficient way to calculate the difference in percentage between two HTML files. These files can be arbitrary long (the files I have can be as long as 300000 characters).\n\nAfter some research I stumbled upon the Levensthein distance which is an algorithm of O(n*m) and requires space O(n*m): the php version can only support up to 255 characters and my own implementation of O(n) space, was too slow.\nAfter that, I tried the php function similar_text, but that algorithm is also too slow for very large HTML files.\n\nSo now I'm looking for another, more efficient algorithm to compare HTML files. An approximation algorithm is also fine. Could anyone give me some advice on how to do this?\n    ", "Answer": "\r\nYou can setup the xdiff extension:\n\nhttp://www.php.net/manual/en/function.xdiff-file-diff.php\n\nThen get the diff of the two files, and based on that diff you can easily come with the percentage.\n\nExample:\n\n\nFirst File A: 400 words\nSecond File B: 400 words\n\n\nDiff Results: 200 words diff from A to B\n\nThat would give you a 50% similarity.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Max 3 color algorithm\r\n                \r\nIn this problem I'm given a Graph ```\nG = (V,E)```\n, the goal is to find a coloring of graph's vertices with 3 colors possible that maximize the quality function \n\nq(c) = the number of edges that their endpoints are colored differently. \n\nGive a probalistic 3/2 approximation, and show that the algorithm returns fail (meaning worse approximation) with probability at most ```\nd^-k```\n  for each natural number ```\nk```\n and for a fixed ```\nd>= 1```\n.\n\nNow I've given this following algorithm: I color each vertex at random, that means the expected probability of an edge to have different color edges is 2/3 which makes it 3/2 approximation.\n\nYet, I've no idea how to show that it returns fail with probability of at most ```\nd^-k```\n.\n\nCould use some help, thanks!\n    ", "Answer": "\r\nRather than try to untangle the quantifiers here, I'll just observe that the method of conditional expectations can be used to prove the following deterministically implementable greedy algorithm correct. \n\nWhile there exists an uncolored vertex, color it one of the colors that appears least among its neighbors. \n\nThe idea is that each edge with an uncolored endpoint is worth 2/3 in expectation regardless of what other choices have been made, assuming that we color the rest of the graph randomly. By using the most diverse choice, we get at least as much deterministic value as we lost in randomized value.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to iterate over grapheme clusters in Crystal?\r\n                \r\nThe Unicode standard defines a grapheme cluster as an algorithmic approximation to a \"user-perceived character\". A grapheme cluster more or less corresponds to what people think of as a single \"character\" in text. Therefore it is a natural and important requirement in programming to be able to operate on strings as sequences of grapheme clusters.\nThe best general-purpose grapheme cluster definition is the extended grapheme cluster; there are other grapheme cluster algorithms (a tailored grapheme cluster) meant for specific localized usages.\nIn Crystal, how can I iterate over (or otherwise operate on) a ```\nString```\n as a sequence of grapheme clusters?\n    ", "Answer": "\r\nThis answer is based on a thread in the Crystal forum.\nCrystal does not have a built-in way to do this (unfortunately) as of 1.0.0.\nHowever, the regex engine in Crystal does, with the ```\n\\X```\n pattern which matches a single extended grapheme cluster:\n```\n\"\\u0067\\u0308\\u1100\\u1161\\u11A8\".scan(/\\X/) do |match|\n  grapheme = match[0]\n  puts grapheme\nend\n\n# Output:\n# g̈\n# 각\n```\n\nRun it online\nYou can wrap this up in a nicer API as follows:\n```\ndef each_grapheme(s : String, &)\n  s.scan(/\\X/) do |match|\n    yield match[0]\n  end\nend\n\ndef graphemes(s : String) : Array(String)\n  result = Array(String).new\n  each_grapheme(s) do |g|\n    result << g\n  end\n  return result\nend\n\n# Example from https://docs.swift.org/swift-book/LanguageGuide/StringsAndCharacters.html\ns = \"\\u{E9}\\u{65}\\u{301}\\u{D55C}\\u{1112}\\u{1161}\\u{11AB}\"\neach_grapheme(s) do |g|\n  puts \"#{g}\\t#{g.codepoints}\"\nend\n\n# Output:\n# é [233]\n# é    [101, 769]\n# 한 [54620]\n# 한   [4370, 4449, 4523]\n```\n\nRun it online\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Histogram approximation for streaming data\r\n                \r\nThis question is a slight extension of the one answered here. I am working on re-implementing a version of the histogram approximation found in Section 2.1 of this paper, and I would like to get all my ducks in a row before beginning this process again. Last time, I used ```\nboost::multi_index```\n, but performance wasn't the greatest, and I would like to avoid the logarithmic in number of buckets insert/find complexity of a ```\nstd::set```\n. Because of the number of histograms I'm using (one per feature per class per leaf node of a random tree in a random forest), the computational complexity must be as close to constant as possible.\n\nA standard technique used to implement a histogram involves mapping the input real value to a bin number. To accomplish this, one method is to:\n\n\ninitialize a standard C array of size N, where N = number of bins; and\nmultiply the input value (real number) by some factor and floor the result to get its index in the C array.\n\n\nThis works well for histograms with uniform bin size, and is quite efficient. However, Section 2.1 of the above-linked paper provides a histogram algorithm without uniform bin sizes.\n\nAnother issue is that simply multiplying the input real value by a factor and using the resulting product as an index fails with negative numbers. To resolve this, I considered identifying a '0' bin somewhere in the array. This bin would be centered at 0.0; the bins above/below it could be calculated using the same multiply-and-floor method just explained, with the slight modification that the floored product be added to two or subtracted from two as necessary.\n\nThis then raises the question of merges: the algorithm in the paper merges the two closest bins, as measured from center to center. In practice, this creates a 'jagged' histogram approximation, because some bins would have extremely large counts and others would not. Of course, this is due to non-uniform-sized bins, and doesn't result in any loss of precision. A loss of precision does, however, occur if we try to normalize the non-uniform-sized bins to make the uniform. This is because of the assumption that m/2 samples fall to the left and right of the bin center, where m = bin count. We could model each bin as a gaussian, but this will still result in a loss of precision (albeit minimal)\n\nSo that's where I'm stuck right now, leading to this major question: What's the best way to implement a histogram accepting streaming data and storing each sample in bins of uniform size?\n    ", "Answer": "\r\nKeep four variables.\n\n```\nint N;  // assume for simplicity that N is even\nint count[N];\ndouble lower_bound;\ndouble bin_size;\n```\n\n\nWhen a new sample ```\nx```\n arrives, compute ```\ndouble i = floor(x - lower_bound) / bin_size```\n. If ```\ni >= 0 && i < N```\n, then increment ```\ncount[i]```\n. If ```\ni >= N```\n, then repeatedly double ```\nbin_size```\n until ```\nx - lower_bound < N * bin_size```\n. On every doubling, adjust the counts (optimize this by exploiting sparsity for multiple doublings).\n\n```\nfor (int j = 0; j < N / 2; j++) count[j] = count[2 * j] + count[2 * j + 1];\nfor (int j = N / 2; j < N; j++) count[j] = 0;\n```\n\n\nThe case ```\ni < 0```\n is trickier, since we need to decrease ```\nlower_bound```\n as well as increase ```\nbin_size```\n (again, optimize for sparsity or adjust the counts in one step).\n\n```\nwhile (lower_bound > x) {\n    lower_bound -= N * bin_size;\n    bin_size += bin_size;\n    for (int j = N - 1; j > N / 2 - 1; j--) count[j] = count[2 * j - N] + count[2 * j - N + 1];\n    for (int j = 0; j < N / 2; j++) count[j] = 0;\n}\n```\n\n\nThe exceptional cases are expensive but happen only a logarithmic number of times in the range of your data over the initial bin size.\n\nIf you implement this in floating-point, be mindful that floating-point numbers are not real numbers and that statements like ```\nlower_bound -= N * bin_size```\n may misbehave (in this case, if ```\nN * bin_size```\n is much smaller than ```\nlower_bound```\n). I recommend that ```\nbin_size```\n be a power of the radix (usually two) at all times.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Graph partitioning based on nodes and edges weights\r\n                \r\nI have a graph G=(V,E) that both edges and nodes have  weights. I want to partition this graph to create equal sized partitions. The definition of the size of partition is sum(vi)-sum(ej) where vi is a node inside that partition and ej is an edge between two nodes in that partition. In my problem the graph is very dense (almost complete). Is there any approximation algorithm for that? \n\nThis is somehow similar to the problem in bin packing with overlapping objects where bins have the same size. Weight of nodes are their size and weight of Edges show how much two object can overlap.\n    ", "Answer": "\r\nI think if you use METIS program solved problem.\nyou can download this program of this link\nhttp://glaros.dtc.umn.edu/gkhome/views/metis\nit has a good documentation and very fast program.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Significant error when approximating elliptical arcs with bezier curves on canvas with javascript\r\n                \r\nI'm trying to convert svg path to canvas in javascript, however it's really hard to map svg path elliptical arcs to canvas path. One of the ways is to approximate using multiple bezier curves.\n\nI have successfully implemented the approximation of elliptical arcs with bezier curves however the approximation isn't very accurate.\n\nMy code:\n\n```\nvar canvas = document.getElementById(\"canvas\");\nvar ctx = canvas.getContext(\"2d\");\n\ncanvas.width = document.body.clientWidth;\ncanvas.height = document.body.clientHeight;\nctx.strokeWidth = 2;\nctx.strokeStyle = \"#000000\";\nfunction clamp(value, min, max) {\n  return Math.min(Math.max(value, min), max)\n}\n\nfunction svgAngle(ux, uy, vx, vy ) {\n  var dot = ux*vx + uy*vy;\n  var len = Math.sqrt(ux*ux + uy*uy) * Math.sqrt(vx*vx + vy*vy);\n\n  var ang = Math.acos( clamp(dot / len,-1,1) );\n  if ( (ux*vy - uy*vx) < 0)\n    ang = -ang;\n  return ang;\n}\n\nfunction generateBezierPoints(rx, ry, phi, flagA, flagS, x1, y1, x2, y2) {\n  var rX = Math.abs(rx);\n  var rY = Math.abs(ry);\n\n  var dx2 = (x1 - x2)/2;\n  var dy2 = (y1 - y2)/2;\n\n  var x1p =  Math.cos(phi)*dx2 + Math.sin(phi)*dy2;\n  var y1p = -Math.sin(phi)*dx2 + Math.cos(phi)*dy2;\n\n  var rxs = rX * rX;\n  var rys = rY * rY;\n  var x1ps = x1p * x1p;\n  var y1ps = y1p * y1p;\n\n  var cr = x1ps/rxs + y1ps/rys;\n  if (cr > 1) {\n    var s = Math.sqrt(cr);\n    rX = s * rX;\n    rY = s * rY;\n    rxs = rX * rX;\n    rys = rY * rY;\n  }\n\n  var dq = (rxs * y1ps + rys * x1ps);\n  var pq = (rxs*rys - dq) / dq;\n  var q = Math.sqrt( Math.max(0,pq) );\n  if (flagA === flagS)\n    q = -q;\n  var cxp = q * rX * y1p / rY;\n  var cyp = - q * rY * x1p / rX;\n\n  var cx = Math.cos(phi)*cxp - Math.sin(phi)*cyp + (x1 + x2)/2;\n  var cy = Math.sin(phi)*cxp + Math.cos(phi)*cyp + (y1 + y2)/2;\n\n  var theta = svgAngle( 1,0, (x1p-cxp) / rX, (y1p - cyp)/rY );\n\n  var delta = svgAngle(\n    (x1p - cxp)/rX, (y1p - cyp)/rY,\n    (-x1p - cxp)/rX, (-y1p-cyp)/rY);\n\n  delta = delta - Math.PI * 2 * Math.floor(delta / (Math.PI * 2));\n\n  if (!flagS)\n    delta -= 2 * Math.PI;\n\n  var n1 = theta, n2 = delta;\n\n\n  // E(n)\n  // cx +acosθcosη−bsinθsinη\n  // cy +asinθcosη+bcosθsinη\n  function E(n) {\n    var enx = cx + rx * Math.cos(phi) * Math.cos(n) - ry * Math.sin(phi) * Math.sin(n);\n    var eny = cy + rx * Math.sin(phi) * Math.cos(n) + ry * Math.cos(phi) * Math.sin(n);\n    return {x: enx,y: eny};\n  }\n\n  // E'(n)\n  // −acosθsinη−bsinθcosη\n  // −asinθsinη+bcosθcosη\n  function Ed(n) {\n    var ednx = -1 * rx * Math.cos(phi) * Math.sin(n) - ry * Math.sin(phi) * Math.cos(n);\n    var edny = -1 * rx * Math.sin(phi) * Math.sin(n) + ry * Math.cos(phi) * Math.cos(n);\n    return {x: ednx, y: edny};\n  }\n\n  var n = [];\n  n.push(n1);\n\n  var interval = Math.PI/4;\n\n  while(n[n.length - 1] + interval < n2)\n    n.push(n[n.length - 1] + interval)\n\n  n.push(n2);\n\n  function getCP(n1, n2) {\n    var en1 = E(n1);\n    var en2 = E(n2);\n    var edn1 = Ed(n1);\n    var edn2 = Ed(n2);\n\n    var alpha = Math.sin(n2 - n1) * (Math.sqrt(4 + 3 * Math.pow(Math.tan((n2 - n1)/2), 2)) - 1)/3;\n\n    console.log(en1, en2);\n\n    return {\n      cpx1: en1.x + alpha*edn1.x,\n      cpy1: en1.y + alpha*edn1.y,\n      cpx2: en2.x - alpha*edn2.x,\n      cpy2: en2.y - alpha*edn2.y,\n      en1: en1,\n      en2: en2\n    };\n  }\n\n  var cps = []\n  for(var i = 0; i < n.length - 1; i++) {\n    cps.push(getCP(n[i],n[i+1]));\n  }\n  return cps;\n}\n\n// M100,200\nctx.moveTo(100,200)\n// a25,100 -30 0,1 50,-25\nvar rx = 25, ry=100 ,phi =  -30 * Math.PI / 180, fa = 0, fs = 1, x = 100, y = 200, x1 = x + 50, y1 = y - 25;\n\nvar cps = generateBezierPoints(rx, ry, phi, fa, fs, x, y, x1, y1);\n\nvar limit = 4;\n\nfor(var i = 0; i < limit && i < cps.length; i++) {\n  ctx.bezierCurveTo(cps[i].cpx1, cps[i].cpy1,\n                    cps[i].cpx2, cps[i].cpy2,\n                    i < limit - 1 ? cps[i].en2.x : x1, i < limit - 1 ? cps[i].en2.y : y1);\n}\nctx.stroke()\n```\n\n\nWith the result:\n\n\n\nThe red line represents the svg path elliptical arc and the black line represents the approximation\n\nHow can I accurately draw any possible elliptical arc on canvas?\n\nUpdate:\n\nForgot to mention the original source of the algorithm: https://mortoray.com/2017/02/16/rendering-an-svg-elliptical-arc-as-bezier-curves/\n    ", "Answer": "\r\nSo both bugs are simply:\n\n\nn2 should be declare ```\nn2 = theta + delta;```\n\nThe E and Ed functions should use ```\nrX```\n ```\nrY```\n rather than ```\nrx```\n ```\nry```\n.\n\n\nAnd that fixes everything. Though the original should have obviously opted to divide up the arcs into equal sized portions rather than pi/4 sized elements and then appending the remainder. Just find out how many parts it will need, then divide the range into that many parts of equal size, seems like a much more elegant solution, and because error goes up with length it would also be more accurate.\n\nSee: https://jsfiddle.net/Tatarize/4ro0Lm4u/ for working version. \n\n\n\nIt's not just off in that one respect it doesn't work most anywhere. You can see that depending on phi, it does a lot of variously bad things. It's actually shockingly good there. But, broken everywhere else too.\n\nhttps://jsfiddle.net/Tatarize/dm7yqypb/\n\nThe reason is that the declaration of n2 is wrong and should read:\n\n```\nn2 = theta + delta;\n```\n\n\nhttps://jsfiddle.net/Tatarize/ba903pss/\nBut, fixing the bug in the indexing, it clearly does not scale up there like it should. It might be that arcs within the svg standard are scaled up so that there can certainly be a solution whereas in the relevant code they seem like they are clamped. \n\nhttps://www.w3.org/TR/SVG/implnote.html#ArcOutOfRangeParameters\n\n\n  \"If rx, ry and φ are such that there is no solution (basically, the\n  ellipse is not big enough to reach from (x1, y1) to (x2, y2)) then the\n  ellipse is scaled up uniformly until there is exactly one solution\n  (until the ellipse is just big enough).\"\n\n\nTesting this, since it does properly have code that should scale it up, I changed it green when that code got called. And it turns green when it screws up. So yeah, it's failure to scale for some reason:\n\nhttps://jsfiddle.net/Tatarize/tptroxho/\n\nWhich means something is using rx rather than the scaled rX and it's the E and Ed functions:\n\n```\nvar enx = cx + rx * Math.cos(phi) * Math.cos(n) - ry * Math.sin(phi) * Math.sin(n);\n```\n\n\nThese ```\nrx```\n references must read ```\nrX```\n and ```\nrY```\n for ```\nry```\n.\n\n```\nvar enx = cx + rX * Math.cos(phi) * Math.cos(n) - rY * Math.sin(phi) * Math.sin(n);\n```\n\n\nWhich finally fixes the last bug, QED.\n\nhttps://jsfiddle.net/Tatarize/4ro0Lm4u/\n\n\n\nI got rid of the canvas, moved everything to svg and animated it.\n\n```\nvar svgNS = \"http://www.w3.org/2000/svg\";\nvar svg = document.getElementById(\"svg\");\nvar arcgroup = document.getElementById(\"arcgroup\");\nvar curvegroup = document.getElementById(\"curvegroup\");\n\nfunction doArc() {\n  while (arcgroup.firstChild) {\n    arcgroup.removeChild(arcgroup.firstChild);\n  } //clear old svg data. -->\n  var d = document.createElementNS(svgNS, \"path\");\n  //var path = \"M100,200 a25,100 -30 0,1 50,-25\"\n  var path = \"M\" + x + \",\" + y + \"a\" + rx + \" \" + ry + \" \" + phi + \" \" + fa + \" \" + fs + \" \" + \" \" + x1 + \" \" + y1;\n  d.setAttributeNS(null, \"d\", path);\n  arcgroup.appendChild(d);\n}\n\nfunction doCurve() {\n  var cps = generateBezierPoints(rx, ry, phi * Math.PI / 180, fa, fs, x, y, x + x1, y + y1);\n\n  while (curvegroup.firstChild) {\n    curvegroup.removeChild(curvegroup.firstChild);\n  } //clear old svg data. -->\n  var d = document.createElementNS(svgNS, \"path\");\n  var limit = 4;\n  var path = \"M\" + x + \",\" + y;\n  for (var i = 0; i < limit && i < cps.length; i++) {\n    if (i < limit - 1) {\n      path += \"C\" + cps[i].cpx1 + \" \" + cps[i].cpy1 + \" \" + cps[i].cpx2 + \" \" + cps[i].cpy2 + \" \" + cps[i].en2.x + \" \" + cps[i].en2.y;\n    } else {\n      path += \"C\" + cps[i].cpx1 + \" \" + cps[i].cpy1 + \" \" + cps[i].cpx2 + \" \" + cps[i].cpy2 + \" \" + (x + x1) + \" \" + (y + y1);\n    }\n  }\n  d.setAttributeNS(null, \"d\", path);\n  d.setAttributeNS(null, \"stroke\", \"#000\");\n  curvegroup.appendChild(d);\n}\n\nsetInterval(phiClock, 50);\n\nfunction phiClock() {\n  phi += 1;\n  doCurve();\n  doArc();\n}\ndoCurve();\ndoArc();\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Optimization algorithms for piecewise-constant and similar ill-defined functions\r\n                \r\nI have a function which takes as inputs n-dimensional (say n=10) vectors whose components are real numbers varying from 0 to a large positive number A say 50,000, ends included. For any such vector the function outputs an integer from 1 to say B=100. I have this function and want to find its global minima.\n\nBroadly speaking there are algorithmic, iterative and heuristics based approaches to tackle such optimization problem. Which are the best techniques suggested to solve this problem? I am looking for suggestions to algorithms or active research papers that i can implement from scratch to solve such problems. I have already given up hope on existing optimization functions that ship with Matlab/python. I am hoping to read experience of others working with approximation/heuristic algorithms to optimize such ill-defined functions.\n\nI ran ```\nfmincon```\n, ```\nfminsearch```\n, ```\nfminunc```\n in Matlab but they fail to optimize the function. The function is ill-defined according to their definitions. Matlab says this for ```\nfmincon```\n:\n\n```\nInitial point is a local minimum that satisfies the constraints.\n\nOptimization completed because at the initial point, the objective function is non-decreasing \nin feasible directions to within the selected value of the optimality tolerance, and \nconstraints are satisfied to within the selected value of the constraint tolerance.\n```\n\n\nProblem arises because this function has piecewise-constant behavior. If a vector V is assigned to a number say 65, changing its components very slightly may not have any change. Such ill-defined behavior is to be well-expected because of pigeon-hole principle. The domain of function is unlimited whereas range is just a bunch of numbers.\n\nI also wish to clarify one issue that may arise. Suppose i do gradient descent on a starting point ```\nx0```\n and my next ```\nx```\n that i get from GD-iteration has some components lie outside the domain [0,50000], then what happens? So actually the domain is circular. So a vector of size 3 like [30;5432;50432] becomes [30;5432;432]. This is automatically taken care of so that there is no worry about iterations finding a vector outside the domain.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Pseudo LRU tree algorithm\r\n                \r\nA lot of descriptions of Pseudo LRU algorithms involve using a binary search tree, and setting flags to \"point away\" from the node you're searching for every time you access the tree.\n\nThis leads to a reasonable approximation of LRU. However, it seems from the descriptions that all of the nodes deemed LRU would be leaf nodes. Is there a pseudo-LRU algorithm that deals with a static tree that will still perform reasonably well, while determining that non-leaf nodes are suitable LRU candidates?\n\nedit:\nI've already implemented an LRU using hashmaps and linkedlists. I'm interested in seeing the performance implications of using a pseudo lru tree (especially on concurrent reads).\nThat is why I specifically asked about pseudo lru tree algorithms, but I should have made that clearer.\n    ", "Answer": "\r\nYou can always push your internal node down to the leaf using rotates ala red-black trees.\n\nKeeping the tree balanced while doing that might be tough.  But you do have a choice of which subtree to push down, so maybe not impossible...\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "If I relax some constraints, can I get an algorithmic shortcut on Approximate Nearest Neighbors?\r\n                \r\nI'm looking for an algorithm with the fastest time per query for a problem similar to nearest-neighbor search, but with two differences:\n\nI need to only approximately confirm (tolerating Type I and Type II error) the existence of a neighbor within some distance k or return the approximate distance of the nearest neighbor.\nI can query many at once\n\nI'd like better throughput than the approximate nearest neighbor libraries out there (https://github.com/erikbern/ann-benchmarks) which seem better designed for single queries. In particular, the algorithmic relaxation of the first criteria seems like it should leave room for an algorithmic shortcut, but I can't find any solutions in the literature nor can I figure out how to design one.\nHere's my current best solution, which operates at about 10k queries / sec on per CPU. I'm looking for something close to an order-of-magnitude speedup if possible.\n```\nsample_vectors = np.random.randint(low=0, high=2, size=(10000, vector_size))\nnew_vectors = np.random.randint(low=0, high=2, size=(100000, vector_size))\n\nimport annoy\nann = annoy.AnnoyIndex(vector_size, metric='hamming')\nfor i, v in enumerate(sample_vectors):\n    ann.add_item(i, v)\nann.build(20)\n\nfor v in new_vectors:\n    print(ann.get_nns_by_vector(v, n=1, include_distances=True))\n```\n\n    ", "Answer": "\r\nI'm a bit skeptical of benchmarks such as the one you have linked, as in my experience I have found that the definition of the problem at hand far outweighs in importance the merits of any one algorithm across a set of other (possibly similar looking) problems.\nMore simply put, an algorithm being a high performer on a given benchmark does not imply it will be a higher performer on the problem you care about. Even small or apparently trivial changes to the formulation of your problem can significantly change the performance of any fixed set of algorithms.\nThat said, given the specifics of the problem you care about I would recommend the following:\n\nuse the cascading approach described in the paper [1]\nuse SIMD operations (either SSE on intel chips or GPUs) to accelerate, the nearest neighbour problem is one where operations closer to the metal and parallelism can really shine\ntune the parameters of the algorithm to maximize your objective; in particular, the algorithm of [1] has a few easy to tune parameters which will dramatically trade performance for accuracy, make sure you perform a grid search over these parameters to set them to the sweet spot for your problem\n\nNote: I have recommended the paper [1] because I have tried many of the algorithms listed in the benchmark you linked and found them all inferior (for the task of image reconstruction) to the approach listed in [1] while at the same time being much more complicated than [1], both undesirable properties. YMMV depending on your problem definition.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Polynomial-time algorithm for travelling salesman in a grid\r\n                \r\nI have read that the classic travelling salesman problem (TSP) is NP-Hard. And there are some approximation algorithms and also a specific algorithm running in O(N^2 * 2^N) time. But AFAIK, these are for TSP in a general graph.\n\nSo my question, is there a better (preferable polynomial time) algorithm for solving the TSP in a M x N grid?\n\nFor example, say there's a grid of 3x4 and there are different costs of travelling from one cell to each of the 2 adjacent (bottom and right) cells. So I want to find the minimum cost to visit all the cells, starting from cell (0, 0) and returning to cell (0, 0).\n\nEDIT: Just to clear things up, I'm pretty sure this not an Euclidean TSP. For simplicity, think of the below example. A rectangle is divided in to M rows and N columns. The salesman is at cell 0, 0 (the top-left cell). He has to visit all the cells and still come back to his starting cell (0, 0). But he can only travel from one cell to each of its 4 adjacent cells (top, left, bottom, right). And the cost from one cell to any one of its adjacent cells may not be the same.\n\nThanks.\n    ", "Answer": "\r\nIt is a very old post, but it seems that all answers are not accurate, so I will try to clear things up (mainly for future readers).\nIt is true (AFAIK) that there is no known polynomial solution to TSP when input is restricted to be a grid.\nFrom here, to say the problem in NP-Hard - this jump is not justify and might be incorrect.\nThe fact that the degrees of nodes is greater than 2, makes the naive algorithm indeed to run in an exponential time complexity. As @Eyal Schneider mentioned, this doesn't prove it is NP-Hard.\nProving TSP in NP-Hard (from the method I know), is done using the fact that Hamiltonian-Path is NP-Hard. On grids graphs, Hamiltonian-Path is in P, hence the same method to prove it is NP-Hard won't work.\nI provide here as well an example why the degree of each node is not necessarily a good indication if the problem is hard.\nTake the TSP on the 2 layers grid graphs, all grid graphs of size 2 * n for some n. In this problem (which is easier the the general case of TSP on grids) the degree of all nodes (except for corner) is 3, which by most answers here means the naive algorithm time complexity will still be exponential with n (because there are O(n) nodes with degree greater than 2).\nIn fact, for 2 * n grids there are exactly 2 Hamiltonian paths (same circle in the opposite direction). Hence there exists a polynomial algorithm to solve TSP on 2 * n grids - check the 2 circles and return True if the minimum of them is lower than k (Given input of graph G and maximum value k).\nFor summary - there is no proof that TSP on grids is NP-Hard (AFAIK). There is also no any known polynomial algorithm to solve TSP on grids (AFAIK).\nAs I see it, TSP on grids might be in NP-intermediate, but this class is a bit tricky, and it is quiet impossible to prove something is in NP-intermediate (it will be equivalent to prove P is not equal to NP).\nBest,\nShahar\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Block layout algorithm\r\n                \r\nI'm looking for help with improving an algorithm for placing blocks of odd shapes. My problem domain is odd, but the best analogy for my blocks is Tetris pieces, except that they can have more than four pieces. The blocks still are composed of only right angles, but they can be long and winding, they can branch, etc.\nI'm trying to arrange multiple large arbitrarily shaped blocks in minimal space (I know, a bin-packing problem) but my current solution looks ugly. I'm basically placing one, then brute forcing the rest by trying to place them at the origin of my grid and then slowly pushing them in different directions until they don't collide anymore. It's not slow, but it doesn't make any attempt to fit pieces nicely so they don't waste overall space.\nThe only thing I can think of trying is ordering the blocks by size, placing the largest first, then fitting the smallest in at the end into any holes remaining. But there are certainly ways that can backfire.\nAre there any heuristics or approximation algorithms that can help me here?\nThe results would look something like the following:\n\nAlso, perhaps my gravatar gives away that this is Mega Man related...\n    ", "Answer": "\r\nThis (polyomino shape-packing) generally seems to be a nontrivial math problem, and I'll point you to the expertise of some others who have worked on it. This guy has a bunch of polyomino examples on his website where others can submit solutions. He also has solver software in Java:\n\nhttp://gp.home.xs4all.nl/Site/Polyomino_Solver.html.\n\nhttp://gp.home.xs4all.nl/PolyominoSolver/downloadsolver.htm\n\nThere are also some algorithms written for this by Stephen Montgomery-Smith, which seem to be more comprehensive than the above (it solved some problems that weren't solvable with that) eventually made it into an xscreensaver (solves in real-time and cool to watch!). The following screenshot, from the screensaver, only shows shapes up to pentominoes, but it works on general shapes with general containers.\n\nhttp://www.math.missouri.edu/~stephen/software/\n\nI am unsure if either of these software approximates the best fit of polyominoes allowing for holes. But it's definitely 'decidable' this way, in the sense that you could certainly insert extra 1x1 polyominoes into your solution and see if it can find a particular result that fits, then remove the 1x1 pieces to get the result.\n\n\n\nFor your application, it might be more efficient to work backwards. All of these algorithms have complexity in the number of unit cells in each block. A good way to lay out your blocks would be to think of them as \"subdivisions\" in larger cells, so that a 3x3 square in your block corresponds to a 1x1 square in a rescaled version. Then, pad the blocks with empty space so they all consist of the larger blocks, run the algorithm, and strip away the extra space. Not only will this be much faster to execute, but it will also generate the space between blocks that you require.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Optimization When Objective Function Can Only Be Approximately Computed\r\n                \r\nI have some graph algorithms that depend on a moderate number of parameters (say 2-6), and which don't always succeed in finding what they want (they want 'good enough' solutions to problems known to be hard, like ```\nmincut/maxflow```\n). I also have a very large family of graphs that I'd like to use the algorithms on. \n\nMy current goal is to find the parameter values for which a given algorithm most often succeeds. Unfortunately, the only way I know how to calculate 'success' is to take a graph from my large family and actually run the algorithm. This has two problems: it is computationally expensive, and it gives only an approximation to my real objective function, the true percentage of graphs on which the algorithm succeeds.\n\nThe first isn't the end of the world; Nelder-Mead or something similar could work. Is there a variant of this algorithm which would work in my situation? I expect success probabilities far from 0 or 1.\n    ", "Answer": "\r\n(Sorry, switched computers and don't have the ability to edit - this is the original poster. In response to Shahbaz, I made a mistake. I meant to say sparsest cut, which is NP complete. The actual problem I'm working on is, as is often the case, rather messier. I really just meant to say that there's no hope for a clean solution, but ended up saying the opposite by accident.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Gauss-Jacobi iteration method\r\n                \r\nI'm trying to write a programm that solves system of equations Ax=B using Gauss-Jacobi iteration method.\n```\n#include <math.h>\n#include <stdlib.h>\n#include <stdio.h>\n\nint main(void) {\n    double **a, *b, *x, *f, eps = 1.e-2, c;  \n    int n = 3, m = 3, i, j, bool = 1, d = 3;\n    /* printf(\"n=\") ; scanf(\"%d\", &n);\n       printf(\"m=\") ; scanf(\"%d\", &n) */\n   \n   \n    a =malloc(n * sizeof *a);\n    for (i = 0; i < n; i++) \n        a[i] = (double*)malloc(m * sizeof(double));\n\n    b = malloc(m * sizeof *b);\n    x = malloc(m * sizeof *x) ;  \n    f = malloc(m * sizeof *f) ;\n    for (i = 0; i < n; i++) {\n        for (j = 0; j < m; j++) { \n            printf(\"a[%d][%d]=\", i, j); \n            scanf(\"%le\", &a[i][j]); \n            if(fabs(a[i][i])<1.e-10) return 0 ; \n        }\n\n        printf(\"\\n\") ;\n    }\n        \n    printf(\"\\n\") ;\n        \n    for (i = 0; i < n; i++) {\n        for (j = 0; j < m; j++) { \n            printf(\"a[%d][%d]=%le  \", i, j, a[i][j]); \n        }\n         \n        printf(\"\\n\") ;\n    }\n    \n    for (j = 0; j < m; j++) { \n        printf(\"x[%d]=\", j); \n        scanf(\"%le\", &x[j]); \n    } //intial guess\n    \n    printf(\"\\n\") ;\n    \n    for (j = 0; j < m; j++) { \n        printf(\"b[%d]=\", j); \n        scanf(\"%le\", &b[j]); \n    }\n    \n    printf(\"\\n\")  ;\n\n    while (1) {\n        bool = 0;\n        for (i = 0; i < n; i++) {\n            c = 0.0;\n            for (j = 0; j < m; j++) \n                if (j != i) \n                    c += a[i][j] * x[j];  \n            f[i] = (b[i] - c) / a[i][i];\n        }\n       \n        for (i = 0; i < m; i++)  \n            if (fabs(f[i] - x[i]) > eps) \n                bool = 1;\n       \n        if (bool == 1) \n            for (i = 0; i < m; i++) \n                x[i] = f[i];\n        else if (bool == 0) \n            break;\n    }\n\n    for (j = 0; j < m; j++) \n        printf(\"%le\\n\", f[j]);\n\n    return 0;\n}\n```\n\nThe condition of stoping the loop is that previous approximation minus current approximation for all x is less than epsilon.\nIt seems like i did everything according to algorithm,but the programm doesn't work.\nWhere did i make a mistake?\n    ", "Answer": "\r\nWhile not the most strict condition, the usual condition requiered to guarantee convergence in the Jacobi and Gauss-Seidel methods is diagonal dominance,\n```\nabs(a[i][i]) > sum( abs(a[i][j]), j=0...n-1, j!=i)\n```\n\nThis test is also easy to implement as a check to run before the iteration.\nThe larger the relative gap in all these inequalities, the faster the convergence of the method.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Color approximation\r\n                \r\nLet's suppose we have a regular RGB image. Now we would want to approximate the color of each individual pixel of our source image with a color out of a small set of colors.\n\nFor example, all tones of red should be converted to that specific red out of my set of colors, same goes for green, blue, etc.\n\nIs there any elegant way/algorithm to achieve this?\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Allocation minimum cost with quantity constraint in Python\r\n                \r\nI'm trying to solve an allocation problem:\n\n\nI have 350 000 parcels with size S_i, each parcel can only have a state, and for each parcel I have a set of probability\nfor each state, I have a quantity to reach\n\n\nI found the Vogel approximation method but with 350 000 rows (parcels) and 15 columns (possible states), the computation time should be too long.\n\nDo you know minimum-cost allocation algorithms that could solve this kind of problem ?\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to partition undirected graph into balanced partition?\r\n                \r\nI have an undirected graph G=(V, E) where each vertex represents a segment in a large road network map. Each edge represents a road segment from one place to another place. Therefore, all edges have the same weight. I wish to partition this road network into k different sets clusters.\nMotivations: The idea is to divide the edges into k-sets of partition, such that each partition can be replicated into n-number of machines. Each machine will perform the distance approximation algorithm.\nIs there any graph partitioning technique, that is easy to understand and implement? The graph I  am trying to partition consists of 25000 nodes.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Evaluating time complexity for the binomial coefficient\r\n                \r\nI'm new to Theoretical Computer Science, and I would like to calculate the time complexity of the following algorithm that evaluates the binomial coefficient defined as\n\n```\nnf = 1; \nfor i = 2 to n do nf = nf * i; \nkf = 1; \nfor i = 2 to k do kf = kf * i; \nnkf = 1; \nfor i = 2 to n-k do nkf = nkf * i; \nc = nf / (kf * nkf);\n```\n\nMy textbook suggests to use Stirling's approximation\n\nHowever, I can get the same result by considering that ```\nfor i = 2 to n do nf = nf * i;```\n have complexity O(n-2)=O(n), that is predominant.\nStirling's approximation seems a little bit overkill. Is my approach wrong?\n    ", "Answer": "\r\nIn your first approach you calculate n!, k! and (n-k)! separately and then calculate the binomial coefficient. Therefore since all of those terms can be calculated with at most operations you have O(n) time complexity.\nHowever, you are wrong about the time complexity of calculating the Stirling's formula. You only need log(n) in base 2 operations to calculate it. This is because when trying to calculate p'th power of some real number, instead of multiplicating it p times, you can instead keep squaring the number to calculate it quickly. For example:\nIf you want to calculate 2^17, instead of doing 17 operations like this:\n```\nreturn 2*2*2*2*2*2*2*2*2*2*2*2*2*2*2*2*2\n```\n\nyou can do this:\n```\na = 2*2\nb = a*a\nc = b*b\nd = c*c\nreturn d * 2\n```\n\nwhich is only 5 operations.\nNote: However keep in mind that the Stirling's formula is not equal to the factorial. It is only an approximation but a good one.\nEdit: Also you can consider a^n as e^(log(a)*n) and then calculate it by the  quickly converging series expansion\n1 + (log(a)n) + ((log(a)n)^2)/2! + ((log(a)n)^3)/3! + ...\nSince the series converges very quickly you can get really close approximations in no time.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How do printf and scanf handle floating point precision formats?\r\n                \r\nConsider the following snippet of code:\n\n```\nfloat val1 = 214.20;\ndouble val2 = 214.20;\n\nprintf(\"float : %f, %4.6f, %4.2f \\n\", val1, val1, val1);\nprintf(\"double: %f, %4.6f, %4.2f \\n\", val2, val2, val2);\n```\n\n\nWhich outputs:\n\n```\nfloat : 214.199997,  214.199997, 214.20 | <- the correct value I wanted \ndouble: 214.200000,  214.200000, 214.20 |\n```\n\n\nI understand that ```\n214.20```\n has an infinite binary representation. The first two elements of the first line have an approximation of the intended value, but the the last one seems to have no approximation at all, and this led me to the following question:\n\nHow do the ```\nscanf```\n, ```\nfscanf```\n, ```\nprintf```\n, ```\nfprintf```\n (etc.) functions treat the precision formats?\n\nWith no precision provided, ```\nprintf```\n printed out an approximated value, but with ```\n%4.2f```\n it gave the correct result. Can you explain me the algorithm used by these functions to handle precision?\n    ", "Answer": "\r\nThe thing is, 214.20 cannot be expressed exactly with binary representation. Few decimal numbers can. So an approximation is stored. Now when you use printf, the binary representation is turned into a decimal representation, but it again cannot be expressed exactly and is only approximated.\n\nAs you noticed, you can give a precision to printf to tell it how to round the decimal approximation. And if you don't give it a precision then a precision of 6 is assumed (see the man page for details).\n\nIf you use ```\n%.40f```\n for the float and ```\n%.40lf```\n for the double in your example above, you will get these results:\n\n```\n214.1999969482421875000000000000000000000000\n214.1999999999999886313162278383970260620117\n```\n\n\nThey are different because with double, there are more bits to better approximate 214.20. But as you can see, they are still very odd when represented in decimal.\n\nI recommend to read the Wikipedia article on floating point numbers for more insights about how floating point numbers work. An excellent read is also What Every Computer Scientist Should Know About Floating-Point Arithmetic\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to calculate order (big O) for more complex algorithms (eg quicksort)\r\n                \r\nI know there are quite a bunch of questions about big O notation, I have already checked:\n\n\nPlain english explanation of Big O\nBig O, how do you calculate/approximate it?\nBig O Notation Homework--Code Fragment Algorithm Analysis?\n\n\nto name a few.\n\nI know by \"intuition\" how to calculate it for ```\nn```\n, ```\nn^2```\n, ```\nn!```\n and so, however I am completely lost on how to calculate it for algorithms that are ```\nlog n```\n , ```\nn log n```\n, ```\nn log log n```\n and so.\n\nWhat I mean is, I know that Quick Sort is ```\nn log n```\n (on average).. but, why? Same thing for merge/comb, etc.\n\nCould anybody explain me in a not too math-y way how do you calculate this?\n\nThe main reason is that Im about to have a big interview and I'm pretty sure they'll ask for this kind of stuff. I have researched for a few days now, and everybody seem to have either an explanation of why bubble sort is n^2 or the  unreadable explanation (for me) on Wikipedia \n    ", "Answer": "\r\nThe logarithm is the inverse operation of exponentiation.  An example of exponentiation is when you double the number of items at each step.  Thus, a logarithmic algorithm often halves the number of items at each step.  For example, binary search falls into this category.\n\nMany algorithms require a logarithmic number of big steps, but each big step requires O(n) units of work.  Mergesort falls into this category.\n\nUsually you can identify these kinds of problems by visualizing them as a balanced binary tree.  For example, here's merge sort:\n\n```\n 6   2    0   4    1   3     7   5\n  2 6      0 4      1 3       5 7\n    0 2 4 6            1 3 5 7\n         0 1 2 3 4 5 6 7\n```\n\n\nAt the top is the input, as leaves of the tree.  The algorithm creates a new node by sorting the two nodes above it.  We know the height of a balanced binary tree is O(log n) so there are O(log n) big steps.  However, creating each new row takes O(n) work.  O(log n) big steps of O(n) work each means that mergesort is O(n log n) overall.\n\nGenerally, O(log n) algorithms look like the function below.  They get to discard half of the data at each step.\n\n```\ndef function(data, n):\n    if n <= constant:\n       return do_simple_case(data, n)\n    if some_condition():\n       function(data[:n/2], n / 2) # Recurse on first half of data\n    else:\n       function(data[n/2:], n - n / 2) # Recurse on second half of data\n```\n\n\nWhile O(n log n) algorithms look like the function below.  They also split the data in half, but they need to consider both halves.\n\n```\ndef function(data, n):\n    if n <= constant:\n       return do_simple_case(data, n)\n    part1 = function(data[n/2:], n / 2)      # Recurse on first half of data\n    part2 = function(data[:n/2], n - n / 2)  # Recurse on second half of data\n    return combine(part1, part2)\n```\n\n\nWhere do_simple_case() takes O(1) time and combine() takes no more than O(n) time.\n\nThe algorithms don't need to split the data exactly in half.  They could split it into one-third and two-thirds, and that would be fine.  For average-case performance, splitting it in half on average is sufficient (like QuickSort).  As long as the recursion is done on pieces of (n/something) and (n - n/something), it's okay.  If it's breaking it into (k) and (n-k) then the height of the tree will be O(n) and not O(log n).\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Stack overflow during evaluation in OCaml\r\n                \r\nI encountered a stack overflow issue when implementing a square root approximation algorithm by Heron of Alexandria, given as follows:\n\n\n  We start with an initial (poor) approximate answer that the square root is 1.0 and then continue improving the guess until we're within delta of the real answer.  The improvement is achieved by averaging the current guess with x/guess.  The answer is accurate to within delta = 0.0001.\n\n\nMy attempt at an implementation was as follows:\n\n```\nlet squareRoot (x : float) : float =\n  let rec aux input guess =\n    if abs_float(guess**2. -. input) < 0.0001 then guess\n    else aux input (guess +. input/.guess)/.2. in\n  aux x 1.;;\n```\n\n\nHowever, this raises a ```\n# Stack overflow during evaluation (looping recursion?).```\n error in the OCaml REPL. I tried implementing an identical algorithm in Python as follows:\n\n```\ndef squareRoot(x):\n  def aux (s, guess):\n    if abs(pow(guess,2) - s) < 0.0001:\n      return guess\n    else:\n      return aux (s, (guess + s/guess)/2)\n  return aux (x, 1)\n```\n\n\n...which ran just fine. So I played around with the OCaml code, and changed my original attempt to:\n\n```\nlet squareRoot (x : float) : float =\n  let improve i g = (g +. i/.g)/.2. in\n  let rec aux input guess =\n    if abs_float(guess ** 2. -. input) < 0.0001 then guess\n    else aux input (improve input guess) in\n  aux x 1.;;\n```\n\n\nAll I changed was wrapping the improvement portion of the algorithm in a separate function, but now the code runs successfully, without any stack overflow error! \n\nI'd appreciate if someone could explain why this is so, and the mechanism behind the OCaml REPL/compiler possibly not recognizing a terminating condition in the recursive call in the my first iteration of code, etc.\n    ", "Answer": "\r\n\n```\naux input (guess +. input/.guess)/.2. \n```\n\n\n\n(the application of ```\naux```\n happens before the division by ```\n2.```\n ...)\n\nis parsed as\n\n```\n  (aux input (guess +. input/.guess))/.2\n```\n\n\n\n\nYou really want\n\n```\n  aux input ((guess +. input/.guess)/.2.)\n```\n\n\nor even (read about A-normal forms)\n\n```\n  let newguess = (guess +. input/.guess)/.2. \n  in\n      aux input newguess\n```\n\n\n(which could be more readable, some people use names like ```\nguess'```\n)\n\nBTW, some people would code\n\n```\n  let guess =  aux input ((guess +. input/.guess)/.2.)\n  in aux input guess\n```\n\n\n(there is no recursion, but lexical scoping)\n\nbut I don't like coding like that (reusing the same name ```\nguess```\n)\n\nAs a rule of thumb, don't be shy in using parenthesis (or ```\nbegin```\n ... ```\nend```\n which is the same) and intermediate ```\nlet```\n bindings. Both makes your code more readable.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Python algorithm to approximate closest parallel equivalence of resistors from a list\r\n                \r\nThe formula for series equivalence of resistors:\nseries equivalence = sum(resistors)\nFor parallel it is 1/(sum(1/resistors[i]))\nI wrote code to return a list of resistors that is closest to a specified target value from a list, within a specified tolerance.\n```\npercentage_difference = lambda xi,xf: 100*(xf-xi)/xi\n\ndef series_equivalance(R,target,tolerance):\n\"\"\"\nR = list of resistors present\ntarget = target value\ntolerance = range += % of target that is acceptable \nThis function returns a list of resistors\n\"\"\"\n\ntol = tolerance/100 #converting tolerance to decimal\n\nif target < min(R):\n    return \"Your target is too small for series equivalence, Try parallel equivalence\"\n\nelse:\n\n    r = R #dummy/copy R\n    toriginal = target #dummy values for arguments made to not change arguments \n    approximate = 0 #this is for exit condition, target in and of itself could be used but that would make algo unstable\n    resistors_list = [] #list to return at the end\n    \n    while True: #Infinite loop because multiple exit conditions\n    \n        if (approximate >= (1-tol)*target and approximate <= (1+tol)*target)  :#exit condition\n            break\n        \n        if len(R) == 0: #If all values are used up\n            return \"All values used up, list: {}, approximate: {}\".format(resistors_list,series_sum(resistors_list))\n \n        difference_from_target = [abs(toriginal-i) for i in R] #finding absolute difference of target from list of R values\n        \n        for i,v in enumerate(difference_from_target):\n            if v == min(difference_from_target): #adding lowest differences to list\n                approximate += r[i] #increment approximate by value from resistors with least difference\n                toriginal -= r[i] #remove that from target dummy target\n                resistors_list.append(r[i]) #adding to list to be returned\n                r.remove(r[i]) \n                break\n        \n    return \"Resistors to use are {}, Approximated value: {}, %Δ of {}%\".format(resistors_list,sum(resistors_list),percentage_difference(target,int(sum(resistors_list))))\n    \n```\n\nSo for example ```\nseries_equivalance([1,2,3,4,5],7,0)```\nwill return ```\n[5,2]```\n.\nI want to a function that can do the same for parallel equivalence. How would I go about it?\n    ", "Answer": "\r\nEdit: I made a blog post which expands on the mip solution and can solve for minimum resistors satisfying a tolerance.\nI've solved this two ways\n\nUsing your function and feeding it inverse values\nUsing a mixed integer linear program\n\nUsing your function and feeding it inverse values\nThis is a half way solution that just feeds in 1/R for the resistor\nvalues and gives it a target resistance of 1/target.\nThen, take reciprocal of the result and you have your resistor values.\nDoesn't work with the tolerance value properly. Need to comment out the\n\"Your target is too small\" check for it to work.\n```\ndef parallel_equivalance(R, target, tolerance):\n\n    R_recip = [1/x for x in R]\n    target_recip = 1/target\n    tolerance_recip = tolerance  # TODO: have a think about how to handle this.\n\n    result_recip = series_equivalance(R_recip, target_recip, tolerance_recip)\n    # resistors_to_use = [1/x for x in result_recip]\n```\n\n```\nprint(parallel_equivalance([1, 2, 3, 4, 5, 6, 7], 1.5555, 1e-2))```\n gives ```\nResistors to use are [5, 2], Approximated value: 7, %Δ of 0.0%```\n\nUsing a mixed integer linear program\nFor this method I use a mixed integer linear program to pick out which\nresistors to use, such that (1/r1 + 1/r2 + ...) is as close\nas possible to 1/target. Solves very quickly (<1s) even when given\nten thousand resistors to choose from.\nYou could modify this to pick the least number of resistors\n(minimise ```\nsum(R_in_use)```\n) with the constraint that the\nerror value must be within some margin (say, ```\nerror >= -eps```\n and ```\nerror <= +eps```\n)\nYou'll need to ```\npip install mip```\n\n```\nimport mip\n\n\ndef parallel_equivalance(R, target):\n\n    R_recip = [1/x for x in R]\n    target_recip = 1/target\n\n    m = mip.Model()  # Create new mixed integer/linear model.\n\n    # Will take value of 1 when corresponding resistor is in use, otherwise 0.\n    R_in_use = [m.add_var(var_type=mip.BINARY) for _ in R_recip]\n    opt_r = sum([b * r for b, r in zip(R_in_use, R_recip)])  # This will be the optimal resistance\n    error = opt_r - target_recip  # Want to minimise the absolute value of this error.\n\n    # create a variable which is greater than than the absolute value of the error.\n    # Because we will be minimizing, this will be forced down to equal the\n    # absolute value. Common trick, google \"linear programming absolute value\".\n    abs_eror = m.add_var(lb=0)\n    m += abs_eror >= error\n    m += abs_eror >= -1 * error\n\n    # Objective of the optimisation is to minimise the absolute error.\n    m.objective = mip.minimize(abs_eror)\n    m.verbose = False  # Turn off verbose logging output.\n    sol_status = m.optimize()\n    print(sol_status)  # This should be `optimal`.\n\n    # Get the solution values telling us which resistors are in use.\n    R_in_use_sol = [float(v) for v in R_in_use]\n\n    # Pick out the values of the resistors corresponding to the resistors\n    # that the optimiser decided to use.\n    R_to_use = [r for r, i in zip(R, R_in_use_sol) if i > 0]\n\n    solved_resistance = 1/sum(1/x for x in R_to_use)\n    solved_error = 100 * (solved_resistance - target) / target\n    print(f'Resistors {R_to_use} in parallel will produce '\n          f'R={solved_resistance:.3f}. '\n          f'Aiming for R={target:.3f}, '\n          f'error of {solved_error:.2f}%')\n    return R_to_use\n\n\ndef main():\n    print(f'mip version {mip.version}')\n    sol = parallel_equivalance([1, 2, 3, 4, 5, 6, 7], 1.5555)\n    sol = parallel_equivalance([1, 2, 3, 4, 5, 6, 7], 1.9)\n    sol = parallel_equivalance(list(range(1, 100)), 123)\n    sol = parallel_equivalance(list(range(1, 1000)), 5.954520294)\n    sol = parallel_equivalance(list(range(1, 10_000)), 5.954520294)\n\n\nif __name__ == '__main__':\n    main()\n```\n\n```\nmip version 1.13.0\nOptimizationStatus.OPTIMAL\nResistors [2, 7] in parallel will produce R=1.556. Aiming for R=1.556, error of 0.00%\nOptimizationStatus.OPTIMAL\nResistors [3, 5] in parallel will produce R=1.875. Aiming for R=1.900, error of -1.32%\nOptimizationStatus.OPTIMAL\nResistors [99] in parallel will produce R=99.000. Aiming for R=123.000, error of -19.51%\nOptimizationStatus.OPTIMAL\nResistors [27, 40, 41, 68, 69, 83, 123, 166, 172, 219, 277, 384, 391, 435, 453, 782, 837] in parallel will produce R=5.954. Aiming for R=5.955, error of -0.01%\nOptimizationStatus.OPTIMAL\nResistors [7, 2001, 2021, 2065, 2130, 2152, 2160, 2176, 2191, 2202, 2216, 2245, 2270, 2279, 2282, 2283, 2313, 2342, 2351, 2381, 2414, 2417, 2497, 2728, 2789, 3449, 3514, 3566, 3575, 3621, 3701, 3789, 3812, 3868, 3879, 3882, 3903, 3936, 3952, 3959, 4128, 4145, 4152, 4158, 4183, 4373, 4382, 4430, 4441, 4498, 4525, 4678, 4722, 4887, 4953, 5138, 5178, 5253, 5345, 5358, 5543, 5593, 5620, 5774, 6002, 6247, 6364, 6580, 6715, 6740, 6819, 6904, 7187, 7293, 7380, 7468, 7533, 7782, 7809, 7846, 7895, 7914, 8018, 8067, 8242, 8309, 8414, 8507, 8515, 8590, 8627, 8872, 8893, 8910, 8952, 9171, 9282, 9311, 9376, 9477, 9550, 9657, 9736, 9792, 9822, 9876, 9982, 9988] in parallel will produce R=5.957. Aiming for R=5.955, error of 0.04%\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm to find a node with particular properties in a tree given a starting node and the approximate path\r\n                \r\nI am looking for a logic which predicts where a particular element lies in the DOM of a specific page, given that we know some general properties of the element, and the approximate path from a few fixed nodes in the template to the element (obtained by analyzing a few pages of the similar type).\n\nSpecific Example:\n\nThere are a few Wikipedia pages to be analyzed:\n\nhttp://en.wikipedia.org/wiki/Econometrics\n\nhttp://en.wikipedia.org/wiki/History_of_economic_thought\n\netc\n.\n.\n.\n\nThe algorithm must get the right navigation box (```\nclass=\"vertical-navbox nowraplinks plainlist\"```\n) in these pages, given the following conditions:\n\n\nThe class name of the element might not be same in all the pages\nThe path to the navbox from the header (```\nid=\"firstHeading\"```\n), and some other fixed nodes, in a few pages(test cases) is available\nThe header(and the other fixed nodes) always has the same id in each page\nSome pages might have a few extra nodes in the path (```\nclass=\"hatnote\"```\n in the second link)\nA few properties of the box(it is in blue color, it is a table etc..) are known\n\n\nIs there an algorithm for this purpose?\n    ", "Answer": "\r\nSo, let's make some assumption and see if they are compatible with your situation.\n\nLet's say you have a test page and in that test page you can do complete dom tree visits. \n\nIn this case we could do a series of reversal path walks, from every leaf to the root, assuming a node has a score of 0 at the beginning and adding +1 if the branch from where we came up contained the wanted node. \n\nAfter we have done this for all possible paths from leafs to root, we do another full visit and we divide the previously calculated score by the number of children (sub-trees or leaves) each node has.\n\nThis means that for every node now you have a percentage telling you the probability of a random sub-tree of that node containing the desired nodes.\n\nNow, for the prediction part, you need some way to match a node in another page to one of the nodes for which you have probabilities (and for this I'm afraid I don't have any idea how it could be done).\n\nOnce you have such a match, and assuming the test page is really predictive, you have automatically a probability factor for each node of the new page that should be meaningful, notwithstanding any possible intermediate additional node.\n\nNote that with the matching algorithm you could do the same calculation for multiple test pages and at the end of each process calculate an overall probability for each node that, hopefully, is more precise than your original one.\n\nHope this is what you needed.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "trying to create an algorithm similar to an approximate subset-sum table in PHP\r\n                \r\nI've been trying to construct an algorithm in PHP that in some ways resembles a subset-sum problem, but in this case, i'm not looking for an exact match, only the closest matches, as in most cases there is unlikely to be an exact match.\n\nLet me elaborate in a little more detail. Say I own various hypothetical investments, and need to raise the sum X from existing investments. Say X = 40000.\n\nThe array of inputs are (4500, 8750, 12900) and the number of each held are (8, 10, 10).\n\nNow, I can intuitively work out that 4500 * 8 = 36000, 12900 * 3 = 38700, but a closer match would be (4500 * 6) + 12900 = 39900.\n\nI soon found that try to loop through each possible combination rapidly creates 10's of million of possible arrays, even more if additional inputs are added. That would not be a problem if it was just for my own purposes, but not viable for a web-application.\n\nI'm not asking for anybody to write code for me. I'm not a mathematician, so I'm just wondering if there is a different way I could tackle this problem, or are least short-cut parts of it ?\n\nThanks.\n    ", "Answer": "\r\nIt's a well known problem called the \"Knapsack Problem\" that can be solved with a dynamic programming approach. \n\nhttps://en.wikipedia.org/wiki/Knapsack_problem\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fastest algorithm of getting precise answer (not approximated) when square-rooting\r\n                \r\nSorry for unclear title, but I don't know how to state it properly (feel free to edit), so I will give example:\n\nsqrt(108) ~ 10.39... BUT I want it to be like this sqrt(108)=6*sqrt(3) so it means expanding into two numbers\n\nSo that's my algorithm\n\n```\ni = floor(sqrt(number))                  //just in case, floor returns lowest integer value :)\nwhile (i > 0)                            //in given example number 108\n  if (number mod (i*i) == 0)\n    first = i                            //in given example first is 6\n    second = number / (i*i)              //in given example second is 3\n    i = 0\n  i--\n```\n\n\nMaybe you know better algorithm?\n\nIf it matters I will use PHP and of course I will use appropriate syntax\n    ", "Answer": "\r\nThere is no fast algorithm for this.  It requires you to find all the square factors.  This requires at least some factorizing.\n\nBut you can speed up your approach by quite a bit.  For a start, you only need to find prime factors up to the cube root of n, and then test whether n itself is a perfect square using the advice from Fastest way to determine if an integer's square root is an integer.\n\nNext speed up, work from the bottom factors up.  Every time you find a prime factor, divide n by it repeatedly, accumulating out the squares.  As you reduce the size of n, reduce your limit that you'll go to.  This lets you take advantage of the fact that most numbers will be divisible by some small numbers, which quickly reduces the size of the number you have left to factor, and lets you cut off your search sooner.\n\nNext performance improvement, start to become smarter about which numbers you do trial divisions by.  For instance special case 2, then only test odd numbers.  You've just doubled the speed of your algorithm again.\n\nBut be aware that, even with all of these speedups, you're just getting more efficient brute force.  It is still brute force, and still won't be fast.  (Though it will generally be much, much faster than your current idea.)\n\nHere is some pseudocode to make this clear.\n\n```\ninteger_sqrt = 1\nremainder = 1\n\n# First we special case 2.\nwhile 0 == number % 4:\n    integer_sqrt *= 2\n    number /= 4\n\nif 0 == number / 2:\n    number /= 2\n    remainder *= 2\n\n# Now we run through the odd numbers up to the cube root.\n# Note that beyond the cube root there is no way to factor this into\n#    prime * prime * product_of_bigger_factors\nlimit = floor(cube_root(number + 1))\ni = 3\nwhile i <= limit:\n    if 0 == number % i:\n        while 0 == number % (i*i):\n            integer_sqrt *= i\n            number /= i*i\n        if 0 == number % (i*i):\n            number /= i\n            remainder *= i\n        limit = floor(cube_root(number + 1))\n    i += 2\n\n# And finally check whether we landed on the square of a prime.\n\npossible_sqrt = floor(sqrt(number + 1))\nif number == possible_sqrt * possible_sqrt:\n    integer_sqrt *= possible_sqrt\nelse:\n    remainder *= number\n\n# And the answer is now integer_sqrt * sqrt(remainder)\n```\n\n\nNote that the various +1s are to avoid problems with the imprecision of floating point numbers.\n\nRunning through all of the steps of the algorithm for 2700, here is what happens:\n\n```\nnumber = 2700\ninteger_sqrt = 1\nremainder = 1\n\nenter while loop\n    number is divisible by 4\n        integer_sqrt *= 2 # now 2\n        number /= 4 # now 675\n\n    number is not divisible by 4\n        exit while loop\n\nnumber is not divisible by 2\n\nlimit = floor(cube_root(number + 1)) # now 8\ni = 3\nenter while loop\n    i < =limit # 3 < 8\n        enter while loop\n            number is divisible by i*i # 9 divides 675\n                integer_sqrt *= 3 # now 6\n                number /= 9 # now 75\n\n            number is not divisible by i*i # 9 does not divide 75\n                exit while loop\n\n        i divides number # 3 divides 75\n            number /= 3 # now 25\n            remainder *= 3 # now 3\n\n        limit = floor(cube_root(number + 1)) # now 2\n\n    i += 2 # now 5\n\n    i is not <= limit # 5 > 2\n        exit while loop\n\npossible_sqrt = floor(sqrt(number + 1)) # 5\n\nnumber == possible_sqrt * possible_sqrt # 25 = 5 * 5\n    integer_sqrt *= possible_sqrt # now 30\n\n# and now answer is integer_sqrt * sqrt(remainder) ie 30 * sqrt(3)\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "finding very close points on plane - approximate clustering algorithm needed\r\n                \r\nI have many points (latitudes and longitudes) on a plane (a city) and I want to find two clusters. Cluster 1 is points cluttered close together and Cluster 2 is everything else.\n\nI know the definition of the problem is not exact. The only thing defined is that I need exactly 2 clusters. Out of N points, how many end up in cluster 1 or cluster 2 is not defined.\n\nThe main aim is to identify points which are very close to each other and separate them from the rest (which are more more evenly spread out)\n\nThe best I can think of is the following algorithm:\n\n```\n1. For each point, Calculate the sum of the square distances to all other points.\n2. Run the k-means with k=2 on these square distances\n```\n\n\nThe squaring (or maybe even higher order) of the distance should help by raising the dimensionality. However this algorithm will be biased towards points near the center of the city. It will struggle to find clusters at the edges of the city.\n\nAny suggestions on how to avoid this problem? And any other suggestions to improve this algorithm\n    ", "Answer": "\r\ni'd suggest something along the following lines:\n\nkey concept\n\ncount number of neighbouring points at distance less than a given value.\n\nsemiformal description\n\n\ncount number ```\nnc(P)```\n of neighbouring points at distance less than a given value ```\nd_cutoff```\n for each point ```\nP```\n.\ncluster all points ```\nP_i```\n with ```\nnc(P_i)```\n greater than a threshold ```\nthres_count```\n into cluster #1.\nfor each ```\nP_i```\n in cluster #1 add its close neighbors, i.e. points ```\nQ```\n with ```\nd(Q, P_i) < d_cutoff```\n to the very same cluster #1.\nset cluster #2 to the complement of cluster #1.\n\n\nalgorithmic angle\n\n\nbuild an undirected graph ```\nG=(V, E)```\n with your points being the vertex set ```\nV```\n and an edge between every pair of points at a distance less than ```\nd_cutoff```\n from each other.\ndelete all edges ```\ne=(v,w)```\n from the graph where ```\ndeg(v) < thres_count```\n and ```\ndeg(w) < thres_count```\n.\n```\nG```\n's isolated vertices form cluster #2, the complement is cluster #1.\n\n\nheuristic on how to choose ```\nd_cutoff```\n\n\nbuild a minimum spanning tree (mst) of your point set. the frequency distribution of edge lengths should hint at suitable cutoff values. short pairwise distances will be incorporated into the mst first. thus there should be at least one pronounced gap in the ordered sequence of edge lengths for point sets with a natural clustering. so partition the set of mst edge lengths into a small number of adjacent intervals, ordering these intervals in the natural way. count how many actual distance values fall into each interval. consider the map between an interval's ordinal number and its count of distance values. large deltas between functions values for successive arguments would suggest to take the upper bound of distances in the lower interval as ```\nd_cutoff```\n.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Does atan() provide any computational advantage over pnorm() in R?\r\n                \r\nThis article describes an analytical approximation of normal CDF:\n\n\n\nThe approximation uses the arctangent function, which is also numerically approximated. I found some discussions about the algorithm of arctan functions in general, and it seems pretty convoluted. In comparison, the source code of ```\npnorm()```\n in R seems pretty straight forward, though it may not be as efficient.\n\nIs there any computational advantage of using ```\natan()```\n instead of ```\npnorm()```\n in R, especially with large data and high parameter space when there is already a bunch of other numerical calculations based off the normal PDF already?\n\nThanks!\n    ", "Answer": "\r\nTried to look at it out of curiosity\n\nFirst define the function\n\n```\nPNORM <- function(x) { 1/(exp(-358/23*x + 111*atan(37*x/294)) + 1) }\n```\n\n\nThen let us look at differences over the range of [-4, 4]\n\n```\nx <- seq(-4, 4, .01)\nplot(x, pnorm(x)-PNORM(x), type=\"l\", lwd=3, ylab=\"Difference\")\n```\n\n\nwhich results in this graph\n\n\n\nSo the difference is small but maybe not small enough to ignore in some applications. YMMV. If we look at computing time then they are roughly equal with the approximation appearing to be slightly faster\n\n```\n> microbenchmark::microbenchmark(pnorm(x), PNORM(x))\nUnit: microseconds\n     expr    min      lq     mean  median      uq    max neval cld\n pnorm(x) 34.703 34.8785 36.54254 35.1820 38.3150 47.786   100   b\n PNORM(x) 24.293 24.4625 27.07660 24.8875 28.9035 59.216   100  a \n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "fault-tolerant K-median problem on an undirected graph [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 5 months ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nWe know that the K-median problem is proved to be NP-Hard. In fault-tolerant K-median problem on an undirected graph G=(V, E):\nWe are given a set of facilities F⊆ V and a set of demands (or clients) D ⊆ V in a metric space. We can open at most k facilities and then assign each client j to the  r_j≥1 facility. Assigning demand j to facility i incurs an assignment cost of d(i, j), where d(i, j) is the shortest path between i and j. Our goal is to minimize the sum of the assignment costs for all clients.\nIf r_j is uniform, e.g. r_j=3 for all clients, each client should be connected to three facilities. Why the below algorithm does not solve the problem in polynomial time; let's assume r_j =3.\n\nGo through all the vertices in F\n\nFor each v ∈ F Calculate the sum of the distance to all clients D\nStore this sum value in a variable\n\n\nSelect the first vertext v with minimum sum variable as the first facility to be opened\nSelect the second vertex v' with minimum sum variable as the second facility to be opened\nSelect the third vertex v'' with minimum sum variable as the third facility to be opened\nConnect all clients to facilities calculated in steps 2,3,4.\n\nNote: This problem was considered in this article , and the authors provided an approximation algorithm for it. Hence, it should be a hard problem. They did not specifically mention the problem to be solved on the graph but mentioned that it should be on metric space. Moreover, they mention the existence of an approximation algorithm even in the uniform case of r_j.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "fault-tolerant K-median problem on an undirected graph [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 5 months ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nWe know that the K-median problem is proved to be NP-Hard. In fault-tolerant K-median problem on an undirected graph G=(V, E):\nWe are given a set of facilities F⊆ V and a set of demands (or clients) D ⊆ V in a metric space. We can open at most k facilities and then assign each client j to the  r_j≥1 facility. Assigning demand j to facility i incurs an assignment cost of d(i, j), where d(i, j) is the shortest path between i and j. Our goal is to minimize the sum of the assignment costs for all clients.\nIf r_j is uniform, e.g. r_j=3 for all clients, each client should be connected to three facilities. Why the below algorithm does not solve the problem in polynomial time; let's assume r_j =3.\n\nGo through all the vertices in F\n\nFor each v ∈ F Calculate the sum of the distance to all clients D\nStore this sum value in a variable\n\n\nSelect the first vertext v with minimum sum variable as the first facility to be opened\nSelect the second vertex v' with minimum sum variable as the second facility to be opened\nSelect the third vertex v'' with minimum sum variable as the third facility to be opened\nConnect all clients to facilities calculated in steps 2,3,4.\n\nNote: This problem was considered in this article , and the authors provided an approximation algorithm for it. Hence, it should be a hard problem. They did not specifically mention the problem to be solved on the graph but mentioned that it should be on metric space. Moreover, they mention the existence of an approximation algorithm even in the uniform case of r_j.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Finding the constant c in the time complexity of certain algorithms\r\n                \r\nI need help finding and approximating the constant c in the complexity of insertion sort (cn^2) and merge sort (cnlgn) by inspecting the results of their running times.\n\nA bit of background, my purpose was to \"implement insertion sort and merge sort (decreasing order) algorithms and measure the performance of these two algorithms. For each algorithm, and for each n = 100, 200, 300, 400, 500, 1000, 2000, 4000, measure its running time when the input is \n\n\nalready sorted, i.e. n, n-1, …, 3, 2,1; \nreversely sorted 1, 2, 3, … n; \nrandom permutation of 1, 2, …, n. \n\n\nThe running time should exclude the time for initialization.\" \n\nI have done the code for both algorithms and put the measurements (microseconds) in a spreadsheet. Now, I'm not sure how to find this c due to differing values for each condition of each algorithm.\n\nFor reference, the time table:\n\n\n\n          InsertionSort                    MergeSort      \n n      AS    RS   Random              AS     RS    Random\n100     12    419     231              192    191     211\n200     13   2559    1398             1303   1299    1263\n300     20    236      94              113    113     123\n400     25    436     293              536    641     556\n500     32    504     246               91     81     105\n1000    65   1991     995              169    246     214\n2000     9   8186    4003              361    370     454\n4000    17  31777   15797              774    751     952\n\n\n\nI can provide the code if necessary.\n    ", "Answer": "\r\nIt's hardly possible to determine values of these constants, especially for modern processors that uses caches, pipelines, and other \"performance things\".\n\nOf course, you can try to find an approximation, and then you'll need Excel or any other spreadsheet.\n\nEnter your data, create chart, and then add trendline. The spreadsheet calculates the values of constants for you.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "number approximation in python\r\n                \r\nI have a list of floating points numbers which represent x and y coordinates of points. \n\n```\n(-379.99418604651157, 47.517234218543351, 0.0) #representing point x\n```\n\n\nan edge contains two such numbers. \n\nI'd like to use a graph traversal algorithm, such as dijkstra, but using floating point numbers such as the ones above don't help. \nWhat I'm actually looking for is a way of approximating those numbers:\n\n```\n(-37*.*, 4*.*, 0.0)\n```\n\n\nis there a python function that does that?\n    ", "Answer": "\r\n\"...using floating point numbers such as the ones above don't help...\" - why not?  I don't recall integers as a requirement for Dijkstra.  Aren't you concerned with the length of the edge?  That's more likely to be a floating point number, even if the endpoints are expressed in integer values.\n\nI'm quoting from Steve Skiena's \"Algorithm Design Manual\":\n\n\n  Dijkstra's algorithm proceeds in a\n  series of rounds, where each round\n  establishes the shortest path from s\n  to some new vertex.  Specifically, x\n  is the vertex that minimizes dist(s,\n  vi) + w(vi, x) over all unfinished 1\n  <= i <= n...\n\n\nDistance - no mention of integer.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "PCA in large (p>>n) big data sets in R\r\n                \r\nI have a data set of n = 100,000 observations by p = 2 millions variables.\nI cannot load all the data at once in the memory and the covariance matrix would not fit either (2 millions x 2 millions).\nIs there a way in R to get most of the relevant principal components (~5,000 to 10,000 I think, explaining 99% of total variation) ?\n\nI am trying to find if there is a good implementation of an iterative algorithm. The packages I found seem either discontinued or for the approximation of the few first principal components.\n\nIf there is no package with precompiled algorithms, which iterative algorithm would you suggest to get most of the PCs ? (that I can code myself)\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "A simple algorithm for polygon intersection\r\n                \r\nI'm looking for a very simple algorithm for computing the polygon intersection/clipping.\nThat is, given polygons ```\nP```\n, ```\nQ```\n, I wish to find polygon ```\nT```\n which is contained in ```\nP```\n and in ```\nQ```\n, and I wish ```\nT```\n to be maximal among all possible polygons.\n\nI don't mind the run time (I have a few very small polygons), I can also afford getting an approximation of the polygons' intersection (that is, a polygon with less points, but which is still contained in the polygons' intersection).\n\nBut it is really important for me that the algorithm will be simple (cheaper testing) and preferably short (less code).\n\nedit: please note, I wish to obtain a polygon which represent the intersection. I don't need only a boolean answer to the question of whether the two polygons intersect.\n    ", "Answer": "\r\nI understand the original poster was looking for a simple solution, but unfortunately there really is no simple solution.\nNevertheless, I've recently created an open-source freeware clipping library (written in Delphi, C++ and C#) which clips all kinds of polygons (including self-intersecting ones). This library is pretty simple to use: https://github.com/AngusJohnson/Clipper2\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to tell CVXPY that x^T * x is SDP?\r\n                \r\n```\n    x = cp.Variable((m, n)) \n```\n\n\nx is some rectangular matrix\n\n```\n    z = cp.Variable((n, n), PSD=True)\n```\n\n\nz should be equal to X^T * X which is always PSD\n\n```\n    e_1 = cp.Variable()\n    e_2 = cp.Variable()\n    obj = cp.Minimize(cp.sum_squares(x - obj_matrix) + omega_k * e_1 + omega_k * e_2)\n    constr1 = (e_1 * np.eye(n - rank) - np.transpose(v_k_prev_1) @ z @ v_k_prev_1) >> 0\n    aux = (np.eye(m + n, m) @ (np.eye(m, m + n) + x @ np.eye(n, m + n, k=m)) +\n           np.eye(m + n, n, k=-m) @ (cp.transpose(x) @ np.eye(m, m + n) +\n                                     z @ np.eye(n, m + n, k=m)))\n    constr2 = (e_2 * np.eye(n) - np.transpose(v_k_prev_2) @ aux @ v_k_prev_2) >> 0\n    constr3 = e_1 <= e_k_prev_1\n    constr4 = e_2 <= e_k_prev_2\n    constr5 = z == cp.transpose(x) @ x\n    prob = cp.Problem(obj, constraints=[constr1, constr2, constr3, constr4, constr5])\n    prob.solve()\n```\n\n\nThis is an iteration of a low-rank approximation algorithm.\nFirst I tried it on PSD matrices only, but when I tried to change it to work on rectangular matrices, this error occurred. I guess CVXPY doesn't know that X^T * X is always PSD and I don't know how to code that.\n\n\nThis is the error that I got\nThe following constraints are not DCP: var1 == var0.T @ var0 , because\nthe following subexpressions are not: |--  var0.T @ var0 var0.T @ var0\n\nPromote(-0.0, (10, 10)) >> 0 , because the following subexpressions are not: |--  var0.T @ var0\n\n\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Non-monotonic time complexity algorithm\r\n                \r\nAs a thought exercise, I am trying to think of an algorithm which has a non-monotonic complexity curve. The only thing I could think of was some algorithm with asymptotic solution in extremities.\n\nIs there such algorithm, which has non-monotonic complexity curve, which does not rely on asymptotic approximation?\n    ", "Answer": "\r\nThe discrete Fourier transform comes to mind; if it was applied as follows it would be non-monotonic (and discontinuous):\n\n```\nif is_power_of_2(len(data)):\n    return fft(data)\nreturn dft(data)\n```\n\n\nsince dft runs in O(N**2) and fft runs in O(N log N).\n\nDesigning an algorithm, one would probably find a way to pad the input data to remove non-monotonic behavior (i.e. accelerate smaller inputs), as is commonly done with fft.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Understanding Matlab linsolve\r\n                \r\n1.What is the difference between  A\\b and linsolve(A,b) (different algorithms?) ?\n\n2.What is the difference solving A*x=b and A'*A*x=A'*b, which is more precise ?\n\nSecond equation goes from Least squares approximation\n\nSimple matlab test code:\n\n```\nA=[1,2,3;4,5,6;7,8,9]\nb=[1;2;3]\n\nx1= A\\b\nx1 =\n\n   -0.3333\n    0.6667\n         0\nx2=linsolve(A,b)\nx2 =\n\n   -0.3333\n    0.6667\n         0\nx3=linsolve(A'*A,A'*b)\nx3 =\n\n    0.2487\n   -0.4974\n    0.5820\nx4=(A'*A)\\(A'*b)\nx4 =\n\n   -0.8182\n    1.6364\n   -0.4848\n```\n\n\nreading linsolve documentation I found that \n\n\n  [X,R] = linsolve(A,B) solves the matrix equation AX = B and returns\n  the reciprocal of the condition number of A if A is a square matrix,\n  and the rank of A otherwise.\n\n\nso using R we can test precision(2nd question)?\n    ", "Answer": "\r\nRegarding your first question: one can consider ```\nmldivde```\n (```\nx = A\\B```\n) as a wrapper of the ```\nlinsolve```\n function. The function ```\nlinsolve```\n allows the user to specify information about the matrix ```\nA```\n which can help Matlab to select a more appropriate (faster) algorithm to solve the system. Nevertheless, by using linsolve it is easy to screw up. Quoting from Matlab's documentation:\n\n\n  If A does not have the properties that you specify in opts, linsolve returns incorrect results and does not return an error message. If you are not sure whether A has the specified properties, use mldivide instead.\n\n\nIf you can assess with 100% of certainty the type of your matrix ```\nA```\n while executing your algorithm, then go for ```\nlinsolve```\n. Otherwise use ```\nmldivide```\n.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "What is the simplest, easiest algorithm for finding EMST of a complete graph of order 10^5\r\n                \r\nI just want to be clear that EMST stands for Euclidean Minimum Spanning Tree.\n\nEssentially, I have been given a file with 100k 4D vertices (one vertex on each line). The goal is to visit every vertex in the file while minimizing the total distance traveled. The distance from a point to another point is simply the Euclidean Distance (Distance if you draw a Straight Line between two points\".\n\nI already know that this is pretty much the Traveling Salesman Problem, which is NP Complete, so I am looking to approximate the solution.\n\nThe first approximation algorithm that came to my mind is by finding the MST from a graph constructed from the file... But that would take O(N^2) to even just construct all the edges from the file given the fact that it's a complete graph ( I can go from any point to another ). And given that my input is N = 10^5, my algorithm will have a huge running time, which is too slow...\n\nAny ideas on how I can plan on approximating the solution? Thank you very much..\n    ", "Answer": "\r\nI know it's quadratic-time, but I think you should consider Prim with an implicit graph. The structure of the algorithm is\n\n```\nfor each vertex v\n    mindist[v] := infinity\n    visited[v] := false\nchoose a root vertex r\nmindist[r] := 0\nrepeat |V| times\n    let w be the minimizer of d[w] such that not visited[w]\n    visited[w] := true\n    for each vertex v\n        if not visited[v] and distance(w, v) < mindist[v]:\n            mindist[v] := distance(w, v)\n            parent[v] := w\n```\n\n\nSince the storage used is linear, it will likely stay resident in cache, and there are no fancy data structures, so this algorithm should run pretty fast.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Is there a more efficient way to find the smallest Vector3 combination from a single number?\r\n                \r\nI'm trying to find the smallest combination of vector3 from a single number, I have working code so far but it's really not efficient.\nTo demonstrate, let's say a user enters the number n, the function should output the combination of 3 numbers (x, y, z) with the smallest sum while still being able to multiply to the original number n.\nSo if the user enters 100 as n the x, y, and z should be 4, 5 and 5. (or (5, 5, 4); (5, 4, 5)).\nI'm doing 3 for loops to calculate separate values of x, y, and z. It works great with small numbers but it becomes incredibly calculation heavy as n increases. I'm looking for any ways I can change the method of calculation that would make this faster. I'm open to approximation algorithms as this doesn't need to be 100% accurate.\nI originally wrote it in Lua but the problem isn't directly related to one language.\n```\nfunction CalculateVector(Size)\n    local Vectors = {}\n    local Lowest = math.huge\n    local Index = nil\n    for x = 0, Size, 1 do\n        for y = 0, Size, 1 do\n            for z = 0, Size, 1 do\n                if Size - (x * y * z) == 0 then\n                    table.insert(Vectors, Vector3.new(x, y, z))\n                end\n            end\n        end \n    end\n    table.foreachi(Vectors, function(i, v)\n        local Combined = v.X + v.Y + v.Z\n        if Combined < Lowest then\n            Lowest = Combined\n            Index = i\n        end\n    end)\n    return Vectors[Index]\nend\n```\n\nSame code in Python in case someone doesn't know Lua syntax.\n```\nclass Vector3:\n    def __init__(self, x, y, z):\n        self.X = x\n        self.Y = y\n        self.Z = z\n\ndef CalculateVector(Size):\n    Vectors = []\n    Lowest = Size + 3\n    Index = None\n    for x in range(Size):\n        for y in range(Size):\n            for z in range(Size):\n                if Size - (x * y * z) == 0:\n                    Vectors.append(Vector3(x, y, z))\n    for i,v in enumerate(Vectors):\n        Combined = v.X + v.Y + v.Z\n        if Combined < Lowest:\n            Lowest = Combined\n            Index = i\n    return Vectors[Index]\n```\n\n    ", "Answer": "\r\nFactorize ```\nn```\n and test every split of all its prime factors into 3 sets\n```\nfunction split_number_into_factors_having_min_sum(n, factors)\n   assert(n > 0 and factors > 0)\n   local primes = {}\n   local degrees = {}\n   local terms = {}\n   local p = 2\n   local step = {4, 1, 2, 0, 2}\n   local m = 0\n   while n > 1 do\n      if p * p > n then\n         p = n\n      end\n      if n % p == 0 then\n         local d = 0\n         repeat\n            d = d + 1\n            n = n / p\n         until n % p ~= 0\n         m = m + 1\n         primes[m] = p\n         degrees[m] = d\n         terms[m] = {}\n      end\n      p = p + step[p % 6]\n   end\n   local parts = {}\n   for j = 1, factors do\n      parts[j] = 1\n   end\n   local best_sum = math.huge\n   local best_parts = {}\n   local process_next_prime\n\n   local function split_in_terms(sum, qty, k)\n      if qty < factors then\n         local max_val = parts[qty] == parts[qty + 1] and sum > terms[k][qty] and terms[k][qty] or sum\n         qty = qty + 1\n         local min_val = qty == factors and sum or 0\n         for val = min_val, max_val do\n            terms[k][qty] = val\n            split_in_terms(sum - val, qty, k)\n         end\n      else\n         local p = primes[k]\n         for j = 1, factors do\n            parts[j] = parts[j] * p^terms[k][j]\n         end\n         process_next_prime(k)\n         for j = 1, factors do\n            parts[j] = parts[j] / p^terms[k][j]\n         end\n      end\n   end\n\n   function process_next_prime(k)\n      if k < m then\n         split_in_terms(degrees[k + 1], 0, k + 1)\n      else\n         local sum = 0\n         for j = 1, factors do\n            sum = sum + parts[j]\n         end\n         if sum < best_sum then\n            best_sum = sum\n            for j = 1, factors do\n               best_parts[j] = parts[j]\n            end\n         end\n      end\n   end\n\n   process_next_prime(0)\n   table.sort(best_parts)\n   return best_parts\nend\n```\n\nUsage:\n```\nlocal t = split_number_into_factors_having_min_sum(100, 3)\nprint(unpack(t))  --> 4 5 5\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Dummy node for TSP and finding shortest Hamiltonian Path\r\n                \r\nCan anyone provide assistance to the problem I'm facing? I'm working on implementing a delivery system algorithm. I have attempted to solve it using a graph, where addresses are represented as nodes and the distance between nodes as the weight of the edges. One approach I'm currently exploring involves adapting the Traveling Salesman Problem using Christofides' algorithm(https://www.youtube.com/watch?v=dNCwtFJLsKI&t=211s), which utilizes a Minimum Spanning Tree and Eulerian Cycle algorithm. However, this approach is not accurate enough for my needs. The TSP finds the Shortest Hamiltonian Cycle, whereas I only require the Shortest Hamiltonian Path.\nI have come across suggestions to introduce a \"dummy\" node with a weight of zero, but this causes my approximation algorithm to lose context, resulting in paths that are far from optimal.\nHas anyone encountered a similar problem and can provide assistance?\nExample of paths. keep in mind that this algorithm creates the shortest paths for 4 different routes\n\nOptimal paths\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Cover a polygon with K rectangles minimizing the rectangles area\r\n                \r\nGiven a general polygon P (might contain holes) and a number of rectangles K, I want to find K rectangles {r1,r2,...,rk} such that the polygon is contained in the union of these rectangles. Also, I want to minimize the area of the union of the rectangles.\n\nNote that the rectangles may overlap each other. Also, they're axis-aligned.\n\nAs an example, for K=1, the algorithm should report the minimum bounding box.\n\nI've read some papers on the subject, namely \"A linear-time approximation algorithm for minimum rectangular covering\" and \"Covering a polygonal region by rectangles\".\n\nThe first article approaches the problem by dividing the polygon into simpler polygons, but it considers only rectangles that are within the polygon.\nThe second article assumes that the rectangles dimension are fixed and simply translates them around and tries to cover the polygon with the minimum number of rectangles.\n\nI know that there is an article named \"covering a polygon with few rectangles\" that has what I believe to be exactly what I'm looking for, but I would have to pay in order to have access to it and I'd like to dig depeer before spending any money.\n\nSince this is an np-complete or np-hard (not sure) problem, i'm not expecting fast exact algorithms, therefore approximations are also relevant. \n\nDoes anyone have knowledge of previous work on this particular problem? Any work that might relate is also welcome. Also, if you have ideas on how to address the problem it would be great!\n\nThanks in advance.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Integral approximation python [closed]\r\n                    \r\n            \r\n        \r\n            \r\n                \r\n                    \r\n                            \r\n                                Closed. This question needs to be more focused. It is not currently accepting answers.\r\n                                \r\n                            \r\n                    \r\n                \r\n            \r\n        \r\n            \r\n        \r\n            \r\n                \r\n                        \r\n                            \r\n                        \r\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\r\n                \r\n                    \r\n                        Closed 3 years ago.\r\n                    \r\n                \r\n\r\n                \r\n            \r\n        \r\n            \r\n                    \r\n                        Improve this question\r\n                    \r\n            \r\n\r\n\r\n    \r\n\r\nI saw this formula:\nhttp://tutorial.math.lamar.edu/Classes/CalcI/ProofIntProp.aspx#Extras_IntPf_AvgVal\nand tried to implement python algorithm to approximate integrals. It kinda works, but does not make sense to me, so if any1 can explain why, it will be nice :) This is my code:\n\n```\nimport random\n\ndef monte_carlo(function, a, b, iter = 100000):\n    \"\"\"\n    function - 2d array of numbers, example: [[2, 1], [5, 4]] 2x^1 + 5x^4\n    a, b - boundaries\n    Approximates integral\n    \"\"\" \n    answer = 0\n    for i in range(0, iter): \n        rpt = random.randrange(a, b+1)\n        print(i , 'th ' , 'iteration')\n        answer += evall(function, rpt) \n\n    return (1/(b-a))*answer\ndef evall(function, point):\n    result = 0\n    for term in function:\n        result += term[0]*pow(point, term[1])\n    return result\n\n\nprint('Answer is: ', monte_carlo([[1, 2]], 1, 100))\n```\n\n\nand it works. But the formula says that we need the delta X in there, so if I make:\n\n```\ndeltaX = (b-a)/iter\n```\n\n\nand then multiply evall(function, rpt) by it, it should work as well, but it does not. The example I used is for the function x^2.\n    ", "Answer": "\r\nChange your ```\nmonte_carlo```\n function to:\n\n```\ndef monte_carlo(function, a, b, iter = 100000):\n    \"\"\"\n    function - 2d array of numbers, example: [[2, 1], [5, 4]] 2x^1 + 5x^4\n    a, b - boundaries\n    Approximates integral\n    \"\"\" \n    answer = 0\n    for i in range(0, iter): \n        rpt = random.random()*(b-a) + a  # Change to continuous uniform\n        print(i , 'th ' , 'iteration')   # Probably don't want this\n        answer += evall(function, rpt) \n\n    return (b-a)*answer/iter             # Corrects the Monte Carlo part\n```\n\n\nMonte Carlo integration involves taking an average. You might have noticed that your approximation changes based on the number of iterations. The only thing that should change with the number of iterations is the Monte Carlo error.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Load balancing and scheduling algorithms\r\n                \r\nso here is my problem:\n\nI have several different configuarion servers.  I have different calculations (jobs); I can predict how long approximately each job will take to be caclulated.  Also, I have priorities.  My question is how to keep all machines loaded 99-100% and schedule the jobs in the best way. \n\nEach machine can do several calculations at a time.  Jobs are pushed to the machine.  The central machine knows the current load of each machine.  Also, I would like to to assign some kind of machine learning here, because I will know statistics of each job (started, finished, cpu load etc.).\n\nHow can I distribute jobs (calculations) in the best possible way, keeping in mind the priorities?\n\nAny suggestions, ideas, or algorithms?\n\nFYI: My platform .NET.\n    ", "Answer": "\r\n\nLook at Dryad linq. It already in academic release and may be useful.\nWin HPC server - enterprise solution for distributed computing from Microsoft.\nSome code samples which can help to build load balancing by analyzing performance counters.\nMicrosoft has StockTrader sample application (with sources), which is example of distributable SOA with hand-written RoundRobin load balancing.\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Is there any method for multiplying matrices having O(n) complexity?\r\n                \r\nI want to multiply two matrices but the triple loop has O(n3) complexity. Is there any algorithm in dynamic programming to multiply two matrices with O(n) complexity?\n\nok fine we can't get best than O(n2.81 ) \n\nedit: but is there any solution that can even approximate the result upto some specific no. of columns and rows of matrix\n\ni mean we get the best of O(n2.81 ) with a complex solution but perfect results but if there is any solution for even an approximation of multiplication of matrices as we have formulas for factorial approximation etc.\n\nif there is any you know it will help me \n\nregards.\n    ", "Answer": "\r\nThe best Matrix Multiplication Algorithm known so far is the \"Coppersmith-Winograd algorithm\" with  O(n2.38 ) complexity but it is not used for practical purposes.\n\nHowever you can always use \"Strassen's algorithm\"  which has O(n2.81 ) complexity but there is no such known algorithm for matrix multiplication with O(n) complexity.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "More work like Judea Pearl's Heuristics?\r\n                \r\nI am researching formal and informal search heuristics. One of the best books on the subject I've found is Judea Pearl's Heuristics. Embarrassingly, I find myself unable to find a good search strategy that returns more material in this vein.\nThings I am looking for:\n\n\nSummary papers about advances in search\nBooks/papers that cover some of the history of the development of methods\nSome idea about who is currently producing research in this space and their specialization\nAdditional keywords, search methods, and items that should appear on this list to broaden the search\n\n\nI'm looking for non-technical material. Most works have a bunch of specific implementation detail and small, short bits about where the research came from and what it lead to (which leads to me chasing citation trails). This is totally fine, but hoping to find works that include more of the non-technical info all in one place.\nSome works I've canvassed so far:\n\n\nSearch and Optimization by Metaheuristics. Techniques and Algorithms Inspired by Nature\nMetaheuristics: from design to implementation\nArtificial Intelligence, Evolutionary Computing and Metaheuristics: In the Footsteps of Alan Turing\nEssays and Surveys in Metaheuristics\nEssentials of Metaheuristics\nHandbook of approximation algorithms and metaheuristics\nHeuristics, Metaheuristics and Approximate Methods in Planning and Scheduling\nRecent Advances on Meta-Heuristics and Their Application to Real Scenarios\nAdvances in Knowledge Representation\nApplications of Conceptual Spaces: The Case for Geometric Knowledge Representation\nConcepts, Ontologies, and Knowledge Representation\nHandbook of Knowledge Representation\n\n\nI realize this is more an academically oriented question and am also open to suggestions of where else to post such a question.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithm for splitting a connected graph into two components\r\n                \r\nSuppose I am given a weighted, connected graph. I'd like to find a list of edges that can be removed from the graph leaving it split into two components and so that the sum of the weights of the removed edges is small. Ideally I'd like to have the minimal sum, but I'd settle for a reasonable approximation.\n\nThis seems like a hard problem.  Are there any good algorithms for doing this?\n\nIf it helps, in my case the number of nodes is about 50 and the graph may be dense, so that most pairs of nodes will have an edge between them.\n    ", "Answer": "\r\nI think you are looking for a minimum cut algorithm. Wikipedia\n\nBefore the Edmunds-Karp algorithm came the Ford-Fulkerson algorithm. For what it's worth, the Algorithms book [Cormen, Rivest] cites these two algorithms in the chapter on graph theory.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "How to get approximate time, when an algorithm takes zero second/milli-second times?\r\n                \r\nI need times for \n\n\n1000\n5000\n10000\n15000\n20000 etc no. of datas.\nBy using Quick Sort algorithm for verifying time-complexity from No. of Data vs Times graph. But I have still zero second time for 1000 data's and also 20000 data's. If I measured time in milli or nano second, but time is still zero. Is there any way to find approximate or comparative time for different No. of Data's?\n\n\nMy Quick Sort process are here - \n\n```\n#include <bits/stdc++.h>\nusing namespace std;\n\nint A[50000], i, j, V, store;\n\nint part(int left, int right, int P)\n{\n    V = A[P];\n    swap(A[P], A[right]);\n    store = left;\n    for(i = left; i < right; i++)\n    {\n        if(A[i] <= V)\n        {\n            swap(A[i], A[store]);\n            store++;\n        }\n    }\n    swap(A[store], A[right]);\n\n    return store;\n}\n\nvoid qSort(int left, int right)\n{\n    if(left < right)\n    {\n        j = part(left, right, left);\n        qSort(left, j-1);\n        qSort(j+1, right);\n    }\n}\n\nmain()\n{\n    int nData, k, minValue, maxValue;\n    cout<<\"No. of Data: \";\n    cin>>nData;\n    cout<<\"\\nRange (min, max): \";\n    cin>>minValue>>maxValue;\n    for(k=0; k<nData; k++)\n    {\n        A[k] = minValue + (rand()%(int) (maxValue - minValue + 1));\n    }\n    clock_t t1 = clock();\n    qSort(0, nData-1);\n    clock_t t2 = clock();\n    cout<<\"\\n\\nTime: \"<<(double)(t2-t1)/CLOCKS_PER_SEC<<endl;    \n}\n```\n\n\n[N.B: My operating System is Windows]\n    ", "Answer": "\r\n```\nmain()\n{\n    ...\n    clock_t t1 = clock();\n    qSort(0, nData-1);\n    clock_t t2 = clock();\n    cout<<\"\\n\\nTime: \"<<(double)(t2-t1)/CLOCKS_PER_SEC<<endl;    \n}\n```\n\n\nThe problem here is that the compiler is too smart for this kind of simple test. The compiler sees code that has no effect in the program, it optimizes the program by removing unnecessary code. You have to disable optimization (running the program in debug mode may do that) or modify the program so that the result of sort operation is used in some ways.\n\nAlso, ```\nclock()```\n has different accuracy on Windows and POSIX systems. It's simpler to use ```\nstd::chrono```\n instead. For example\n\n```\n#include <iostream>\n#include <chrono>\n\nint main()\n{\n    std::chrono::time_point<std::chrono::system_clock> start, end;\n    start = std::chrono::system_clock::now();\n    qSort(0, nData-1);\n    end = std::chrono::system_clock::now();\n\n    std::chrono::duration<double> elapsed_seconds = end - start;\n    std::cout << \"count:\" << elapsed_seconds.count() << \"\\n\";\n\n    return 0;\n}\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Smoothing spline De Boor's approach\r\n                \r\nIs it possible to find smoothing spline based on De Boor's approach in Python? For data approximation.\n\nEarlier I used Smoothing spline in matlab, I need exactly same algorithm in python.\n    ", "Answer": "\r\nYou might want to use scipy.interpolate.CubicSpline. The example below is directly taken from the documentation:\n\n\n\nIn this example, the cubic spline is used to interpolate a sampled sinusoid. One can see that the spline continuity property holds for the first and second derivatives and violates only for the third derivative. There is also another example available; not sure what exactly you need. \n\nThis is the code needed to produce the figure:\n\n```\nfrom scipy.interpolate import CubicSpline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.arange(10)\ny = np.sin(x)\ncs = CubicSpline(x, y)\nxs = np.arange(-0.5, 9.6, 0.1)\nplt.figure(figsize=(6.5, 4))\nplt.plot(x, y, 'o', label='data')\nplt.plot(xs, np.sin(xs), label='true')\nplt.plot(xs, cs(xs), label=\"S\")\nplt.plot(xs, cs(xs, 1), label=\"S'\")\nplt.plot(xs, cs(xs, 2), label=\"S''\")\nplt.plot(xs, cs(xs, 3), label=\"S'''\")\nplt.xlim(-0.5, 9.5)\nplt.legend(loc='lower left', ncol=2)\nplt.show()\n```\n\n\nYou can also check scipy.interpolate.splrep and scipy.interpolate.InterpolatedUnivariateSpline. Furthermore, there is a github repository which might be of help. You can compare these methods to the Matlab function you use and select the appropriate one.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Newton root finding function does not work with sqrt(x) in R\r\n                \r\nCurrently doing a homework exercise based on root finding algorithms: \n\nA root finding algorithm can also be used to approximate certain functions. Show mathematically how the evaluation of the square root function f(x) = √x can be expressed as a root finding problem.4 Use both Newton’s method and the bisection method to approximate √x for different values of x. Compare your approximations with the R function sqrt. For which values of x does the approximation work well? Does Newton’s method or the bisection method perform better? How do the answers to these questions depend\non your starting value?\n\nI have the following code that worked for every function so far: \n\n```\nnewton.function <- function(f, fPrime, nmax, eps, x0){\n  n <- 1\n  x1 <- x0\n  result <- c()\n  while((n <= nmax) && (abs(f(x1)) >= eps)){\n    x1 <- (x0 - (f(x0)/fPrime(x0)))\n    result <- c(result, x1)\n    n <- n + 1\n    x0 <- x1\n  }\n  iterations <- n - 1\n  return(c(iterations, result[length(result)]))\n}\n```\n\n\nSqrt functions:\n\n```\ng <- function(x){\n  x^(1/2)\n}\n\ngPrime <- function(x){\n  1/(2*x^(1/2))\n}\n```\n\n\nWhen I execute the function I either get Error in if (abs(f(x1)) <= eps) break : \n  missing value where TRUE/FALSE needed or if the x0 = 0 I get 1 and 0 as a result.\n\n```\nnewton.function(f = g, fPrime = gPrime, nmax = 1000, eps = 1E-8, x0 = 0)\n```\n\n\nMy bisection function works equally as bad, I am stuck answering the question. \n    ", "Answer": "\r\nFrom a programming point of view, your code works as expected.\n\nIf you start with 0, which is the exact solution, you get 0, fine.\n\nNow look what happens when starting with any other number:\n\n```\nx1 <- (x0 - (f(x0)/fPrime(x0))) = (x0 - (x0^(1/2)/(1/(2*x^(1/2)))))\n= x0-2x0 = -x0\n```\n\n\nSo if you start with a positive number, ```\nx1```\n will be negative after the first iteration, and the next call to ```\nf(x1)```\n returns ```\nNaN```\n since you ask the square root of a negative number.\n\nThe error message tells you that R can not evaluate ```\nabs(f(x1)) >= eps```\n to TRUE or FALSE, indeed, ```\nabs(f(x1))```\n returns NaN and the ```\n>=```\n operator returns also ```\nNaN```\n in this case. This is exactly what the error message tells you.\n\nSo I advice you to look at some mathematics source to check you algorithm, but the R part is ok. \n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fast equivalent to sin() for DSP referenced in STK\r\n                \r\nI'm using bits of Perry Cook's Synthesis Toolkit (STK) to generate saw and square waves. STK includes this BLIT-based sawtooth oscillator:\n\n```\ninline STKFloat BlitSaw::tick( void ) {\n  StkFloat tmp, denominator = sin( phase_ );\n  if ( fabs(denominator) <= std::numeric_limits<StkFloat>::epsilon() )\n      tmp = a_;\n  else {\n      tmp = sin( m_ * phase_ );\n      tmp /= p_ * denominator;\n  }\n\n  tmp += state_ - C2_;\n  state_ = tmp * 0.995;\n\n  phase_ += rate_;\n  if ( phase_ >= PI ) \n     phase_ -= PI;\n\n  lastFrame_[0] = tmp;\n     return lastFrame_[0];\n}\n```\n\n\nThe square wave oscillator is broadly similar. At the top, there's this comment:\n\n```\n// A fully  optimized version of this code would replace the two sin \n// calls with a pair of fast sin oscillators, for which stable fast \n// two-multiply algorithms are well known.\n```\n\n\nI don't know where to start looking for these \"fast two-multiply algorithms\" and I'd appreciate some pointers. I could use a lookup table instead, but I'm keen to learn what these 'fast sin oscillators' are. I could also use an abbreviated Taylor series, but thats way more than two multiplies. Searching hasn't turned up anything much, although I did find this approximation:\n\n```\n#define AD_SIN(n) (n*(2.f- fabs(n))) \n```\n\n\nPlotting it out shows that it's not really a close approximation outside the range of -1 to 1, so I don't think I can use it when ```\nphase_```\n is in the range -pi to pi:\n\n\n\nHere, Sine is the blue line and the purple line is the approximation.\n\nProfiling my code reveals that the calls to ```\nsin()```\n are far and away the most time-consuming calls, so I really would like to optimise this piece.\n\nThanks\n\nEDIT Thanks for the detailed and varied answers. I will explore these and accept one at the weekend.\n\nEDIT 2 Would the anonymous close voter please kindly explain their vote in the comments? Thank you.\n    ", "Answer": "\r\nEssentially the sinusoidal oscilator is one (or more) variables that change with each DSP step, rather than getting recalculated from scratch.\n\nThe simplest are based on the following trig identities: (where ```\nd```\n is constant, and thus so is ```\ncos(d)```\n and ```\nsin(d)```\n )\n\n```\nsin(x+d) = sin(x) cos(d) + cos(x) sin(d)\ncos(x+d) = cos(x) cos(d) - sin(x) sin(d)\n```\n\n\nHowever this requires two variables (one for sin and one for cos) and 4 multiplications to update. However this will still be far faster than calculating a full sine at each step.\n\nThe solution by Oli Charlesworth is based on solutions to this general equation\n\n```\nA_{n+1} = a A_{n} + A_{n-1}\n```\n\n\nWhere looking for a solution of the form ```\nA_n = k e^(i theta n)```\n gives an equation for ```\ntheta```\n.\n\n```\ne^(i theta (n+1) ) = a e^(i theta n ) + b e^(i theta (n-1) )\n```\n\n\nWhich simplifies to \n\n```\ne^(i theta) - e^(-i theta ) = a\n2 cos(theta) = a\n```\n\n\nGiving\n\n```\nA_{n+1} = 2 cos(theta) A_{n} + A_{n-1}\n```\n\n\nWhichever approach you use you'll either need to use one or two of these oscillators for each frequency, or use another trig identity to derive the higher or lower frequencies.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "General purpose algorithm for triangulating an undirected graph?\r\n                \r\nI am playing around with implementing a junction tree algorithm for belief propagation on a Bayesian Network.  I'm struggling a bit with triangulating the graph so the junction trees can be formed.\n\nI understand that finding the optimal triangulation is NP-complete, but can you point me to a general purpose algorithm that results in a 'good enough' triangulation for relatively simple Bayesian Networks?  \n\nThis is a learning exercise (hobby, not homework), so I don't care much about space/time complexity as long as the algorithm results in a triangulated graph given any undirected graph.  Ultimately, I'm trying to understand how exact inference algorithms work before I even try doing any sort of approximation.\n\nI'm tinkering in Python using NetworkX, but any pseudo-code description of such an algorithm using typical graph traversal terminology would be valuable.\n\nThanks!\n    ", "Answer": "\r\nIf Xi is a possible variable (node) to be deleted then,\n\n\nS(i) will be the size of the clique created by deleting this variable\nC(i) will be the sum of the size of the cliques of the subgraph given by Xi and its adjacent nodes\n\n\nHeuristic:\n\nIn each case select a variable Xi among the set of possible variables to be deleted with minimal S(i)/C(i)\n\nReference: Heuristic Algorithms for the Triangulation of Graphs\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "LBFGS: Accuracy of Hessian approximation\r\n                \r\nDoes anybody know how useful LBFGS is for estimating the Hessian matrix in the case of many (>10 000) dimensions? When running scipy's implementation on a simple 100D quadratic form the algorithm does already seem to struggle. Are there any general results about special cases (i.e. a dominant diagonal) in which the approximated Hessian is reasonably trustworthy?\nFinally, one immediate drawback in scipy's implementation to me seems that the initial estimate of the Hessian is the identity matrix which might lead to a slower convergence. Do you know how important this effect is, i.e. how would the algorithm be affected if I would have a good idea of what the diagonal elements would be?\nHere are two sets of example plots for a rather diagonal dominant form, as well as for a case with strong off-diagonals. The first one shows the original covariance matrix and the latter one gives the approximated results using m=50 and m=500.\n\n\n\n\n\n\nCode for running the experiment:\n```\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# Parameters\nndims = 100 # Dimensions for our problem\na = .2 # Relative importance of non-diagonal elements in covariance\nm = 500 # Number of updates we allow in lbfgs\nx0=1*np.random.rand(ndims) # Initial starting point for LBFGS\n\n# Generate covariance matrix\nA = np.matrix([np.random.randn(ndims) + np.random.randn(1)*a for i in range(ndims)])\nA = A*np.transpose(A)\nD_half = np.diag(np.diag(A)**(-0.5))\ncov= D_half*A*D_half\ninvcov = np.linalg.inv(cov)\nassert(np.all(np.linalg.eigvals(cov) > 0))\n\n# Define quadratic form and its derivative\ndef gauss(x,invcov):\n    res = 0.5*x.T@invcov@x\n    return res[0,0]\n\ndef gaussgrad(x,invcov):\n    res = np.asarray(x.T@invcov)\n    return res[0]\n\n# Put function in lambda shape\nfgauss = lambda x: gauss(x,invcov=invcov)\nfprimegauss = lambda x: gaussgrad(x,invcov=invcov)\n\n# Run the lbfgs variant and retrieve the inverse Hessian approximation\nx, f, d, s, y = fmin_l_bfgs_b(func=fgauss,x0=x0,fprime=fprimegauss,m=m,approx_grad=False)\ninvhess = LbfgsInvHess(s, y)\n\n# Plot the results\nplt.imshow(cov)\nplt.colorbar()\nplt.show()\n\nplt.imshow(invhess.todense(),vmin=np.min(cov),vmax=np.max(cov))\nplt.colorbar()\nplt.show()\n\nplt.imshow(invhess.todense()-cov)\nplt.colorbar()\nplt.show()\n```\n\nAs scipy does not give the vectors from which the Hessian is reconstructed we need to call a marginally modified function (based on scipy.optimize.lbfgsb.py):\n```\nimport numpy as np\nfrom numpy import array, asarray, float64, zeros\nfrom scipy.optimize import _lbfgsb\nfrom scipy.optimize.optimize import (MemoizeJac, OptimizeResult,\n                       _check_unknown_options, _prepare_scalar_function)\nfrom scipy.optimize._constraints import old_bound_to_new\n\nfrom scipy.sparse.linalg import LinearOperator\n\n__all__ = ['fmin_l_bfgs_b', 'LbfgsInvHessProduct']\n\n\ndef fmin_l_bfgs_b(func, x0, fprime=None, args=(),\n                  approx_grad=0,\n                  bounds=None, m=10, factr=1e7, pgtol=1e-5,\n                  epsilon=1e-8,\n                  iprint=-1, maxfun=15000, maxiter=15000, disp=None,\n                  callback=None, maxls=20):\n    \"\"\"\n    Minimize a function func using the L-BFGS-B algorithm.\n    Parameters\n    ----------\n    func : callable f(x,*args)\n        Function to minimize.\n    x0 : ndarray\n        Initial guess.\n    fprime : callable fprime(x,*args), optional\n        The gradient of `func`. If None, then `func` returns the function\n        value and the gradient (``f, g = func(x, *args)``), unless\n        `approx_grad` is True in which case `func` returns only ``f``.\n    args : sequence, optional\n        Arguments to pass to `func` and `fprime`.\n    approx_grad : bool, optional\n        Whether to approximate the gradient numerically (in which case\n        `func` returns only the function value).\n    bounds : list, optional\n        ``(min, max)`` pairs for each element in ``x``, defining\n        the bounds on that parameter. Use None or +-inf for one of ``min`` or\n        ``max`` when there is no bound in that direction.\n    m : int, optional\n        The maximum number of variable metric corrections\n        used to define the limited memory matrix. (The limited memory BFGS\n        method does not store the full hessian but uses this many terms in an\n        approximation to it.)\n    factr : float, optional\n        The iteration stops when\n        ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,\n        where ``eps`` is the machine precision, which is automatically\n        generated by the code. Typical values for `factr` are: 1e12 for\n        low accuracy; 1e7 for moderate accuracy; 10.0 for extremely\n        high accuracy. See Notes for relationship to `ftol`, which is exposed\n        (instead of `factr`) by the `scipy.optimize.minimize` interface to\n        L-BFGS-B.\n    pgtol : float, optional\n        The iteration will stop when\n        ``max{|proj g_i | i = 1, ..., n} <= pgtol``\n        where ``pg_i`` is the i-th component of the projected gradient.\n    epsilon : float, optional\n        Step size used when `approx_grad` is True, for numerically\n        calculating the gradient\n    iprint : int, optional\n        Controls the frequency of output. ``iprint < 0`` means no output;\n        ``iprint = 0``    print only one line at the last iteration;\n        ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;\n        ``iprint = 99``   print details of every iteration except n-vectors;\n        ``iprint = 100``  print also the changes of active set and final x;\n        ``iprint > 100``  print details of every iteration including x and g.\n    disp : int, optional\n        If zero, then no output. If a positive number, then this over-rides\n        `iprint` (i.e., `iprint` gets the value of `disp`).\n    maxfun : int, optional\n        Maximum number of function evaluations.\n    maxiter : int, optional\n        Maximum number of iterations.\n    callback : callable, optional\n        Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n        current parameter vector.\n    maxls : int, optional\n        Maximum number of line search steps (per iteration). Default is 20.\n    Returns\n    -------\n    x : array_like\n        Estimated position of the minimum.\n    f : float\n        Value of `func` at the minimum.\n    d : dict\n        Information dictionary.\n        * d['warnflag'] is\n          - 0 if converged,\n          - 1 if too many function evaluations or too many iterations,\n          - 2 if stopped for another reason, given in d['task']\n        * d['grad'] is the gradient at the minimum (should be 0 ish)\n        * d['funcalls'] is the number of function calls made.\n        * d['nit'] is the number of iterations.\n    See also\n    --------\n    minimize: Interface to minimization algorithms for multivariate\n        functions. See the 'L-BFGS-B' `method` in particular. Note that the\n        `ftol` option is made available via that interface, while `factr` is\n        provided via this interface, where `factr` is the factor multiplying\n        the default machine floating-point precision to arrive at `ftol`:\n        ``ftol = factr * numpy.finfo(float).eps``.\n    Notes\n    -----\n    License of L-BFGS-B (FORTRAN code):\n    The version included here (in fortran code) is 3.0\n    (released April 25, 2011). It was written by Ciyou Zhu, Richard Byrd,\n    and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following\n    condition for use:\n    This software is freely available, but we expect that all publications\n    describing work using this software, or all commercial products using it,\n    quote at least one of the references given below. This software is released\n    under the BSD License.\n    References\n    ----------\n    * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound\n      Constrained Optimization, (1995), SIAM Journal on Scientific and\n      Statistical Computing, 16, 5, pp. 1190-1208.\n    * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,\n      FORTRAN routines for large scale bound constrained optimization (1997),\n      ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.\n    * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,\n      FORTRAN routines for large scale bound constrained optimization (2011),\n      ACM Transactions on Mathematical Software, 38, 1.\n    \"\"\"\n    # handle fprime/approx_grad\n    if approx_grad:\n        fun = func\n        jac = None\n    elif fprime is None:\n        fun = MemoizeJac(func)\n        jac = fun.derivative\n    else:\n        fun = func\n        jac = fprime\n\n    # build options\n    if disp is None:\n        disp = iprint\n    opts = {'disp': disp,\n            'iprint': iprint,\n            'maxcor': m,\n            'ftol': factr * np.finfo(float).eps,\n            'gtol': pgtol,\n            'eps': epsilon,\n            'maxfun': maxfun,\n            'maxiter': maxiter,\n            'callback': callback,\n            'maxls': maxls}\n\n    res, s, y = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n                           **opts)\n    d = {'grad': res['jac'],\n         'task': res['message'],\n         'funcalls': res['nfev'],\n         'nit': res['nit'],\n         'warnflag': res['status']}\n    f = res['fun']\n    x = res['x']\n    \n    return x, f, d, s, y\n\n\ndef _minimize_lbfgsb(fun, x0, args=(), jac=None, bounds=None,\n                     disp=None, maxcor=10, ftol=2.2204460492503131e-09,\n                     gtol=1e-5, eps=1e-8, maxfun=15000, maxiter=15000,\n                     iprint=-1, callback=None, maxls=20,\n                     finite_diff_rel_step=None, **unknown_options):\n    \"\"\"\n    Minimize a scalar function of one or more variables using the L-BFGS-B\n    algorithm.\n    Options\n    -------\n    disp : None or int\n        If `disp is None` (the default), then the supplied version of `iprint`\n        is used. If `disp is not None`, then it overrides the supplied version\n        of `iprint` with the behaviour you outlined.\n    maxcor : int\n        The maximum number of variable metric corrections used to\n        define the limited memory matrix. (The limited memory BFGS\n        method does not store the full hessian but uses this many terms\n        in an approximation to it.)\n    ftol : float\n        The iteration stops when ``(f^k -\n        f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.\n    gtol : float\n        The iteration will stop when ``max{|proj g_i | i = 1, ..., n}\n        <= gtol`` where ``pg_i`` is the i-th component of the\n        projected gradient.\n    eps : float or ndarray\n        If `jac is None` the absolute step size used for numerical\n        approximation of the jacobian via forward differences.\n    maxfun : int\n        Maximum number of function evaluations.\n    maxiter : int\n        Maximum number of iterations.\n    iprint : int, optional\n        Controls the frequency of output. ``iprint < 0`` means no output;\n        ``iprint = 0``    print only one line at the last iteration;\n        ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;\n        ``iprint = 99``   print details of every iteration except n-vectors;\n        ``iprint = 100``  print also the changes of active set and final x;\n        ``iprint > 100``  print details of every iteration including x and g.\n    callback : callable, optional\n        Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n        current parameter vector.\n    maxls : int, optional\n        Maximum number of line search steps (per iteration). Default is 20.\n    finite_diff_rel_step : None or array_like, optional\n        If `jac in ['2-point', '3-point', 'cs']` the relative step size to\n        use for numerical approximation of the jacobian. The absolute step\n        size is computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``,\n        possibly adjusted to fit into the bounds. For ``method='3-point'``\n        the sign of `h` is ignored. If None (default) then step is selected\n        automatically.\n    Notes\n    -----\n    The option `ftol` is exposed via the `scipy.optimize.minimize` interface,\n    but calling `scipy.optimize.fmin_l_bfgs_b` directly exposes `factr`. The\n    relationship between the two is ``ftol = factr * numpy.finfo(float).eps``.\n    I.e., `factr` multiplies the default machine floating-point precision to\n    arrive at `ftol`.\n    \"\"\"\n    #_check_unknown_options(unknown_options)\n    m = maxcor\n    pgtol = gtol\n    factr = ftol / np.finfo(float).eps\n\n    x0 = asarray(x0).ravel()\n    n, = x0.shape\n\n    if bounds is None:\n        bounds = [(None, None)] * n\n    if len(bounds) != n:\n        raise ValueError('length of x0 != length of bounds')\n\n    # unbounded variables must use None, not +-inf, for optimizer to work properly\n    bounds = [(None if l == -np.inf else l, None if u == np.inf else u) for l, u in bounds]\n    # LBFGSB is sent 'old-style' bounds, 'new-style' bounds are required by\n    # approx_derivative and ScalarFunction\n    new_bounds = old_bound_to_new(bounds)\n\n    # check bounds\n    if (new_bounds[0] > new_bounds[1]).any():\n        raise ValueError(\"LBFGSB - one of the lower bounds is greater than an upper bound.\")\n\n    # initial vector must lie within the bounds. Otherwise ScalarFunction and\n    # approx_derivative will cause problems\n    x0 = np.clip(x0, new_bounds[0], new_bounds[1])\n\n    if disp is not None:\n        if disp == 0:\n            iprint = -1\n        else:\n            iprint = disp\n\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n                                  bounds=new_bounds,\n                                  finite_diff_rel_step=finite_diff_rel_step)\n\n    func_and_grad = sf.fun_and_grad\n\n    fortran_int = _lbfgsb.types.intvar.dtype\n\n    nbd = zeros(n, fortran_int)\n    low_bnd = zeros(n, float64)\n    upper_bnd = zeros(n, float64)\n    bounds_map = {(None, None): 0,\n                  (1, None): 1,\n                  (1, 1): 2,\n                  (None, 1): 3}\n    for i in range(0, n):\n        l, u = bounds[i]\n        if l is not None:\n            low_bnd[i] = l\n            l = 1\n        if u is not None:\n            upper_bnd[i] = u\n            u = 1\n        nbd[i] = bounds_map[l, u]\n\n    if not maxls > 0:\n        raise ValueError('maxls must be positive.')\n\n    x = array(x0, float64)\n    f = array(0.0, float64)\n    g = zeros((n,), float64)\n    wa = zeros(2*m*n + 5*n + 11*m*m + 8*m, float64)\n    iwa = zeros(3*n, fortran_int)\n    task = zeros(1, 'S60')\n    csave = zeros(1, 'S60')\n    lsave = zeros(4, fortran_int)\n    isave = zeros(44, fortran_int)\n    dsave = zeros(29, float64)\n\n    task[:] = 'START'\n\n    n_iterations = 0\n\n    while 1:\n        # x, f, g, wa, iwa, task, csave, lsave, isave, dsave = \\\n        _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,\n                       pgtol, wa, iwa, task, iprint, csave, lsave,\n                       isave, dsave, maxls)\n        task_str = task.tobytes()\n        if task_str.startswith(b'FG'):\n            # The minimization routine wants f and g at the current x.\n            # Note that interruptions due to maxfun are postponed\n            # until the completion of the current minimization iteration.\n            # Overwrite f and g:\n            f, g = func_and_grad(x)\n        elif task_str.startswith(b'NEW_X'):\n            # new iteration\n            n_iterations += 1\n            if callback is not None:\n                callback(np.copy(x))\n\n            if n_iterations >= maxiter:\n                task[:] = 'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n            elif sf.nfev > maxfun:\n                task[:] = ('STOP: TOTAL NO. of f AND g EVALUATIONS '\n                           'EXCEEDS LIMIT')\n        else:\n            break\n\n    task_str = task.tobytes().strip(b'\\x00').strip()\n    if task_str.startswith(b'CONV'):\n        warnflag = 0\n    elif sf.nfev > maxfun or n_iterations >= maxiter:\n        warnflag = 1\n    else:\n        warnflag = 2\n\n    # These two portions of the workspace are described in the mainlb\n    # subroutine in lbfgsb.f. See line 363.\n    s = wa[0: m*n].reshape(m, n)\n    y = wa[m*n: 2*m*n].reshape(m, n)\n    print(x.shape)\n    \n    \n\n    # See lbfgsb.f line 160 for this portion of the workspace.\n    # isave(31) = the total number of BFGS updates prior the current iteration;\n    n_bfgs_updates = isave[30]\n\n    n_corrs = min(n_bfgs_updates, maxcor)\n    inv_hess = LbfgsInvHess(s[:n_corrs], y[:n_corrs])\n\n    task_str = task_str.decode()\n    return OptimizeResult(fun=f, jac=g, nfev=sf.nfev,\n                          njev=sf.ngev,\n                          nit=n_iterations, status=warnflag, message=task_str,\n                          x=x, success=(warnflag == 0), hess_inv=inv_hess), s[:n_corrs], y[:n_corrs]\n    \n\nclass LbfgsInvHess(LinearOperator):\n    \"\"\"Linear operator for the L-BFGS approximate inverse Hessian.\n    This operator computes the product of a vector with the approximate inverse\n    of the Hessian of the objective function, using the L-BFGS limited\n    memory approximation to the inverse Hessian, accumulated during the\n    optimization.\n    Objects of this class implement the ``scipy.sparse.linalg.LinearOperator``\n    interface.\n    Parameters\n    ----------\n    sk : array_like, shape=(n_corr, n)\n        Array of `n_corr` most recent updates to the solution vector.\n        (See [1]).\n    yk : array_like, shape=(n_corr, n)\n        Array of `n_corr` most recent updates to the gradient. (See [1]).\n    References\n    ----------\n    .. [1] Nocedal, Jorge. \"Updating quasi-Newton matrices with limited\n       storage.\" Mathematics of computation 35.151 (1980): 773-782.\n    \"\"\"\n\n    def __init__(self, sk, yk):\n        \"\"\"Construct the operator.\"\"\"\n        if sk.shape != yk.shape or sk.ndim != 2:\n            raise ValueError('sk and yk must have matching shape, (n_corrs, n)')\n        n_corrs, n = sk.shape\n\n        super().__init__(dtype=np.float64, shape=(n, n))\n\n        self.sk = sk\n        self.yk = yk\n        self.n_corrs = n_corrs\n        self.rho = 1 / np.einsum('ij,ij->i', sk, yk)\n\n    def _matvec(self, x):\n        \"\"\"Efficient matrix-vector multiply with the BFGS matrices.\n        This calculation is described in Section (4) of [1].\n        Parameters\n        ----------\n        x : ndarray\n            An array with shape (n,) or (n,1).\n        Returns\n        -------\n        y : ndarray\n            The matrix-vector product\n        \"\"\"\n        s, y, n_corrs, rho = self.sk, self.yk, self.n_corrs, self.rho\n        q = np.array(x, dtype=self.dtype, copy=True)\n        if q.ndim == 2 and q.shape[1] == 1:\n            q = q.reshape(-1)\n\n        alpha = np.empty(n_corrs)\n\n        for i in range(n_corrs-1, -1, -1):\n            alpha[i] = rho[i] * np.dot(s[i], q)\n            q = q - alpha[i]*y[i]\n\n        r = q\n        for i in range(n_corrs):\n            beta = rho[i] * np.dot(y[i], r)\n            r = r + s[i] * (alpha[i] - beta)\n\n        return r\n\n    def todense(self):\n        \"\"\"Return a dense array representation of this operator.\n        Returns\n        -------\n        arr : ndarray, shape=(n, n)\n            An array with the same shape and containing\n            the same data represented by this `LinearOperator`.\n        \"\"\"\n        s, y, n_corrs, rho = self.sk, self.yk, self.n_corrs, self.rho\n        I = np.eye(*self.shape, dtype=self.dtype)\n        Hk = I\n\n        for i in range(n_corrs):\n            A1 = I - s[i][:, np.newaxis] * y[i][np.newaxis, :] * rho[i]\n            A2 = I - y[i][:, np.newaxis] * s[i][np.newaxis, :] * rho[i]\n\n            Hk = np.dot(A1, np.dot(Hk, A2)) + (rho[i] * s[i][:, np.newaxis] *\n                                                        s[i][np.newaxis, :])\n        return Hk\n```\n\nEdit: Typo in code.\n    ", "Answer": "", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Fuzzy autocomplete textbox control in ASP.NET\r\n                \r\nWhat is the best approximation algorithm to implement full-text fuzzy search. For example we have a dropdownlist with the following data (from SQL datasource):\n\n\nCompany Policy\nProduct Catelog\nOur Partners\n\n\nNow I want to replace it with an autocomplete textbox, such that when the letter \"p\" is typed the list shows all three results. It should start matching the first letter of the first word or second word and so on. Also, it should highlight or make the matched letters bold in the suggestions dropdown.\n\nIs there a readymade control for ASP.NET (with JS or jQuery) to deliver all the aforementioned functionality? Otherwise if I have to implement it, is there a tutorial/blog which point me in the right direction?\n    ", "Answer": "\r\nI believe this is what you're looking for. \n\nIt's jquery ui it has the autocomplete functionality described.\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Algorithms to normalize finger touch data (reduce the number of points)\r\n                \r\nI'm working on an app that lets users select regions by finger painting on top of a map. The points then get converted to a latitude/longitude and get uploaded to a server.\n\nThe touch screen is delivering way too many points to be uploaded over 3G. Even small regions can accumulate up to ~500 points. \n\nI would like to smooth this touch data (approximate it within some tolerance). The accuracy of drawing does not really matter much as long as the general area of the region is the same.\n\nAre there any well known algorithms to do this? Is this work for a Kalman filter?\n    ", "Answer": "\r\nThere is the Ramer–Douglas–Peucker algorithm (wikipedia). \n\n\n  The purpose of the algorithm is, given\n  a curve composed of line segments, to\n  find a similar curve with fewer\n  points. The algorithm defines\n  'dissimilar' based on the maximum\n  distance between the original curve\n  and the simplified curve. The\n  simplified curve consists of a subset\n  of the points that defined the\n  original curve.\n\n\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "My bisection code in python doesnt return the root\r\n                \r\nI am trying to find a good approximation to the root of a function using a bisection algorithm, however, when I run the code it doesnt return the root (c).  Here is my code. \n\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.linspace(-10,10,201)\n\ndef f(x): \n    return np.cos(x) \n\nplt.figure()\nplt.plot(x,f(x),label = 'cosx')\nplt.xlabel('independent variable x')\nplt.ylabel('dependent variable y')\nplt.title('')\n\nplt.show()\n\nTOL = 10**-6\na = 0\nb = 1\n\ndef bisection(a,b,TOL):\n    c = (a+b)/2.0  \n    while (b-a)/2.0 > TOL:\n        if f(c)==0:\n            return c\n        elif f(a)*f(c)<0:\n            b = c\n        else:\n            a = c\n    c = (a+b)/2.0\n    return c\n```\n\n    ", "Answer": "\r\nAs posted your code  don't change the values of ```\nc```\n  in the loop  as it is outside the while loop so the value of ```\na```\n and ```\nb```\n never change causing an infinite loop:\n\n```\ndef bisection(a,b,TOL):\n    c = (a+b)/2.0  \n    while (b-a)/2.0 > TOL:\n        if f(c)==0:\n            return c\n        elif f(a)*f(c)<0:\n            b = c\n        else:\n            a = c\n        c = (a+b)/2.0 # put inside the while\n    return c\n```\n\n\nTo see the value of c through the loop add a print statement:\n\n```\ndef bisection(a, b, TOL):\n    c = (a + b) / 2.0\n    while (b - a) / 2.0 > TOL:\n        if f(c) == 0:\n            return c\n        elif f(a) * f(c) < 0:\n            b = c\n        else:\n            a = c\n        c = (a + b) / 2.0\n        print (\"Current value of c is {}\".format(c)) \n    return c\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "String Approximation (Fetching the nearest matched String from Dictionary)\r\n                \r\nIs there any String Matching code or Algorithm which gives us the approximately matched string from the dictionay(contains pre-defined Set of Strings)?\n\nFor example: If there are 10 String in the dictionary (Set of Strings), if user input some String, then the algorithm should tell you the near matched string from the dictionary. Its would be great, if i get the matched string with the matched value (or percentage).\n    ", "Answer": "\r\nI think it's better to use lucene library it has a package called ```\norg.apache.lucene.search.spell```\n you can use it easily. it provide 3 algorithms  NGramDistance, LevensteinDistance, JaroWinklerDistance. try this\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Converting a floating-point decimal value to a fraction\r\n                \r\nGiven a decimal floating-point value, how can you find its fractional equivalent/approximation? For example:\n```\nas_fraction(0.1) -> 1/10\nas_fraction(0.333333) -> 1/3\nas_fraction(514.0/37.0) -> 514/37\n```\n\nIs there a general algorithm that can convert a decimal number to fractional form? How can this be implemented simply and efficiently in C++?\n    ", "Answer": "\r\nFirst get the fractional part and then take the gcd. Use the Euclidean algorithm http://en.wikipedia.org/wiki/Euclidean_algorithm\n\n```\nvoid foo(double input)\n{\n    double integral = std::floor(input);\n    double frac = input - integral;\n\n    const long precision = 1000000000; // This is the accuracy.\n\n    long gcd_ = gcd(round(frac * precision), precision);\n\n    long denominator = precision / gcd_;\n    long numerator = round(frac * precision) / gcd_;\n\n    std::cout << integral << \" + \";\n    std::cout << numerator << \" / \" << denominator << std::endl;\n}\n\nlong gcd(long a, long b)\n{\n    if (a == 0)\n        return b;\n    else if (b == 0)\n        return a;\n\n    if (a < b)\n        return gcd(a, b % a);\n    else\n        return gcd(b, a % b);\n}\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
{"Question": "Initial value for Newton-Raphson Division algorithm when input is close to 0\r\n                \r\nI am implementing a Newton-Raphson Division algorithm for values from ```\n[-16:16]```\n in S15.16 over an FPGA. For values from ```\n|[1:16]|```\n I achieved a MSE of ```\n10e-9```\nwith 3 iterations. The way that I have initialized the value ```\na0```\n is doing the inverse of the middle point in each range:\n\nSome examples are:\n\nFrom range 1 <= x  < 2: a0  = 2/3\nFrom range 6 <= x  < 7: a0  = 2/13\nFrom range 15 <= x < 16: a0 = 2/31\n\nThis approximation works well, as can be see in the following plot:\n\nSo, the problem here is in the range contained within ```\n[0:1]```\n. How to find the optimal initial value or an approximation for the initial value?\nIn the wikipedia says that:\nFor the subproblem of choosing an initial estimate X0, it is convenient to apply a bit-shift to the divisor D to scale it so that 0.5 ≤ D ≤ 1; by applying the same bit-shift to the numerator N, one ensures the quotient does not change. Then one could use a linear approximation in the form\n\nto initialize Newton–Raphson. To minimize the maximum of the absolute value of the error of this approximation on interval [0.5,1], one should use\n\nOk, this approximation works well for the range ```\n[0.5:1]```\n, but:\n\nwhats happens when the value tends to get smaller, close to 0. E.g:\n0.1, 0.01, 0.001, 0.00001... and so on? I see a problem here because I think is necessary a initial value for each range between ```\n[0.001:0.01]```\n, ```\n[0.01:0.1]```\n... etc.\nFor smaller values, is better apply other algorithm such as Goldschmidt division algorithm?\n\nThese are the codes that I have implemented to emulate the Newton-Raphson division in fixed point:\n```\ni = 0\n# 16 fractional bits\nSHIFT = 2 ** 16 \n# Lut with \"optimal?\" values to init NRD algorithm\nLUT = np.round(1 / np.arange(0.5, 16, 1) * SHIFT).astype(np.int64)\nLUT_f = 1 / np.arange(0.5, 16, 1)\n# Function to simulates the NRD algorithm in S15.16 over a FPGA\ndef FIXED_RECIPROCAL(x):\n    # Smart adressing to the initial iteration value\n    adress = x >> 16\n    # Get the value from LUT\n    a0 = LUT[adress]\n    # Algorithm with only 3 iterations\n    for i in range(3):\n        s1 = (a0*a0) >> 16\n        s2 = (x*s1) >> 16\n        a0 = (a0 << 1) - s2\n    # Return rescaled value (Only for analysis purposes)\n    return(a0 / SHIFT)\n\n# ----- TEST ----- #\nt = np.arange(1, 16, 0.1)\nteor = 1 / t\nt_fixed = (t * SHIFT).astype(np.int32)\nprac = np.zeros(len(t))\nfor value in t_fixed:\n    prac[i] = FIXED_RECIPROCAL(value)\n    i = i + 1\n\n# Get and print Errors\nerrors = abs(prac - teor)\nmse = ((prac - teor)**2).mean(axis=None)\n\nprint(\"Max Error : %s\" % ('{:.3E}'.format(np.max(errors))))\nprint(\"MSE:      : %s\" % ('{:.3E}'.format(mse)))\n\n# Print the obtained values:\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.plot(t, teor, label='Theorical division')\nplt.plot(t, prac, '.', label='Newton-Raphson Division')\nplt.legend(fontsize=16)\nplt.title('Float 32 theorical division Vs. S15.16 Newton-Raphson division', fontsize=22)\nplt.xlabel('x', fontsize=20)\nplt.ylabel('1 / x', fontsize=20)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\n```\n\n    ", "Answer": "\r\nThe ISO-C99 code below demonstrates how one can achieve an almost correctly rounded implementation of Newton-Raphson based s15.16 division, using a 2 kilobit lookup table for the initial reciprocal approximation, and a number of 32x32-bit multipliers capable of delivering both the low 32 bits of the full product and the high 32 bits. For ease of implementation the signed s15.16 division is mapped back to unsigned 16.16 division for operands in [0, 231].\nWe need to make full use of the 32-bit data path throughout by keeping operands normalized. In essence we are converting the computation into a quasi floating-point format. This requires a priority encoder to find the most significant set bit in both the dividend and the divisor in the initial normalization step. For software convenience, this is mapped to a CLZ (count leading zeros) operation, present in many processors, in the code below.\nAfter computing the reciprocal of the divisor ```\nb```\n we multiply by the dividend ```\na```\n to determine the raw quotient ```\nq = (1/b)*a```\n. In order to round correctly to nearest or even we need to compute the remainder for the quotient a well as its increment and decrement. The correctly rounded quotient corresponds to the candidate quotient with the remainder of smallest magnitude.\nIn order for this to work perfectly, we would need a raw quotient that is within 1 ulp of the mathematical result. Unfortunately, this is not the case here, since the raw quotient is occasionally off by ±2 ulps. We would need effectively 33 bits in some of the intermediate computation, which could be simulated in software but I don't have time to puzzle this out right now. The code \"as-is\" delivers the correctly rounded result in more than 99.999% of random test cases.\n```\n#include <stdio.h>\n#include <stdlib.h>\n#include <stdint.h>\n\n#define TAB_BITS_IN   (8) /* 256 entry LUT */\n#define TAB_BITS_OUT  (9) /* 9 bits effective, 8 bits stored */\n#define TRUNC_COMP    (1) /* compensate truncation in fixed-point multiply */\n\nint clz (uint32_t a);  // count leadzing zeros: a priority encoder\nuint32_t umul32_hi (uint32_t a, uint32_t b); // upper half of 32x32-bit product\n\n/* i in [0,255]: (int)(1.0 / (1.0 + 1.0/512.0 + i / 256.0) * 512 + .5) & 0xff \n   In a second step tuned to minimize the number of incorrect results with the\n   specific implementation of the two refinement steps chosen.\n*/\nstatic uint8_t rcp_tab[256] = \n{\n    0xff, 0xfd, 0xfb, 0xf9, 0xf7, 0xf5, 0xf3, 0xf1,\n    0xf0, 0xee, 0xec, 0xea, 0xe8, 0xe6, 0xe5, 0xe3,\n    0xe1, 0xdf, 0xdd, 0xdc, 0xda, 0xd8, 0xd7, 0xd5,\n    0xd3, 0xd2, 0xd0, 0xce, 0xcd, 0xcb, 0xc9, 0xc8,\n    0xc6, 0xc5, 0xc3, 0xc2, 0xc0, 0xbf, 0xbd, 0xbc,\n    0xba, 0xb9, 0xb7, 0xb6, 0xb4, 0xb3, 0xb1, 0xb0,\n    0xae, 0xad, 0xac, 0xaa, 0xa9, 0xa7, 0xa6, 0xa5,\n    0xa4, 0xa2, 0xa1, 0x9f, 0x9e, 0x9d, 0x9c, 0x9a,\n    0x99, 0x98, 0x96, 0x95, 0x94, 0x93, 0x91, 0x90,\n    0x8f, 0x8e, 0x8c, 0x8b, 0x8a, 0x89, 0x88, 0x87,\n    0x86, 0x84, 0x83, 0x82, 0x81, 0x80, 0x7f, 0x7e,\n    0x7c, 0x7b, 0x7a, 0x79, 0x78, 0x77, 0x76, 0x74,\n    0x74, 0x73, 0x71, 0x71, 0x70, 0x6f, 0x6e, 0x6d,\n    0x6b, 0x6b, 0x6a, 0x68, 0x67, 0x67, 0x66, 0x65,\n    0x64, 0x63, 0x62, 0x61, 0x60, 0x5f, 0x5e, 0x5d,\n    0x5c, 0x5b, 0x5b, 0x59, 0x58, 0x58, 0x56, 0x56,\n    0x55, 0x54, 0x53, 0x52, 0x51, 0x51, 0x50, 0x4f,\n    0x4e, 0x4e, 0x4c, 0x4b, 0x4b, 0x4a, 0x48, 0x48,\n    0x48, 0x46, 0x46, 0x45, 0x44, 0x43, 0x43, 0x42,\n    0x41, 0x40, 0x3f, 0x3f, 0x3e, 0x3d, 0x3c, 0x3b,\n    0x3b, 0x3a, 0x39, 0x38, 0x38, 0x37, 0x36, 0x36,\n    0x35, 0x34, 0x34, 0x33, 0x32, 0x31, 0x30, 0x30,\n    0x2f, 0x2e, 0x2e, 0x2d, 0x2d, 0x2c, 0x2b, 0x2a,\n    0x2a, 0x29, 0x28, 0x27, 0x27, 0x26, 0x26, 0x25,\n    0x24, 0x23, 0x23, 0x22, 0x21, 0x21, 0x21, 0x20,\n    0x1f, 0x1f, 0x1e, 0x1d, 0x1d, 0x1c, 0x1c, 0x1b,\n    0x1a, 0x19, 0x19, 0x19, 0x18, 0x17, 0x17, 0x16,\n    0x16, 0x15, 0x14, 0x13, 0x13, 0x12, 0x12, 0x11,\n    0x11, 0x10, 0x0f, 0x0f, 0x0e, 0x0e, 0x0e, 0x0d,\n    0x0c, 0x0c, 0x0b, 0x0b, 0x0a, 0x0a, 0x09, 0x08,\n    0x08, 0x07, 0x07, 0x07, 0x06, 0x05, 0x05, 0x04,\n    0x04, 0x03, 0x03, 0x02, 0x02, 0x01, 0x01, 0x01\n};\n\n/* Divide two u16.16 fixed-point operands each in [0, 2**31]. Attempt to round \n   the result to nearest of even. Currently this does not always succeed. We\n   would need effectively 33 bits in intermediate computation for that, so the\n   raw quotient is within +/- 1 ulp of the mathematical result.\n*/\nuint32_t div_core (uint32_t a, uint32_t b)\n{\n    /* normalize dividend and divisor to [1,2); bit 31 is the integer bit */\n    uint8_t lza = clz (a);\n    uint8_t lzb = clz (b);\n    uint32_t na = a << lza;\n    uint32_t nb = b << lzb;\n    /* LUT is addressed by most significant fraction bits of divisor */\n    uint32_t idx = (nb >> (32 - 1 - TAB_BITS_IN)) & 0xff;\n    uint32_t rcp = rcp_tab [idx] | 0x100; // add implicit msb\n    /* first NR iteration */\n    uint32_t f = (rcp * rcp) << (32 - 2*TAB_BITS_OUT);\n    uint32_t p = umul32_hi (f, nb);\n    rcp = (rcp << (32 - TAB_BITS_OUT)) - p;\n    /* second NR iteration */\n    rcp = rcp << 1;\n    p = umul32_hi (rcp, nb);\n    rcp = umul32_hi (rcp, 0 - p);\n    /* compute raw quotient as (1/b)*a; off by at most +/- 2ulps */\n    rcp = (rcp << 1) | TRUNC_COMP;\n    uint32_t quot = umul32_hi (rcp, na);\n    uint8_t shift = lza - lzb + 15;\n    quot = (shift > 31) ? 0 : (quot >> shift);\n    /* round quotient using 4:1 mux */\n    uint32_t ah = a << 16;\n    uint32_t prod = quot * b;\n    uint32_t rem1 = abs (ah - prod);\n    uint32_t rem2 = abs (ah - prod - b);\n    uint32_t rem3 = abs (ah - prod + b);\n    int sel = (((rem2 < rem1) << 1) | ((rem3 < rem1) & (quot != 0)));\n    switch (sel) {\n    case 0:\n    default:\n        quot = quot;\n        break;\n    case 1: \n        quot = quot - 1;\n        break;\n    case 2: /* fall through */\n    case 3: \n        quot = quot + 1;\n        break;\n    }\n    return quot;\n}\n\nint32_t div_s15p16 (int32_t a, int32_t b)\n{\n    uint32_t aa = abs (a);\n    uint32_t ab = abs (b);\n    uint32_t quot = div_core (aa, ab);\n    quot = ((a ^ b) & 0x80000000) ? (0 - quot) : quot;\n    return (int32_t)quot;\n}\n\nuint64_t umul32_wide (uint32_t a, uint32_t b)\n{\n    return ((uint64_t)a) * b;\n}\n\nuint32_t umul32_hi (uint32_t a, uint32_t b)\n{\n    return (uint32_t)(umul32_wide (a, b) >> 32);\n}\n\n#define VARIANT  (1)\nint clz (uint32_t a)\n{\n#if VARIANT == 1\n    static const uint8_t clz_tab[32] = {\n        31, 22, 30, 21, 18, 10, 29,  2, 20, 17, 15, 13, 9,  6, 28, 1,\n        23, 19, 11,  3, 16, 14,  7, 24, 12,  4,  8, 25, 5, 26, 27, 0\n    };\n    a |= a >> 16;\n    a |= a >> 8;\n    a |= a >> 4;\n    a |= a >> 2;\n    a |= a >> 1;\n    return clz_tab [0x07c4acddu * a >> 27] + (!a);\n#elif VARIANT == 2\n    uint32_t b;\n    int n = 31 + (!a);\n    if ((b = (a & 0xffff0000u))) { n -= 16;  a = b; }\n    if ((b = (a & 0xff00ff00u))) { n -=  8;  a = b; }\n    if ((b = (a & 0xf0f0f0f0u))) { n -=  4;  a = b; }\n    if ((b = (a & 0xccccccccu))) { n -=  2;  a = b; }\n    if ((    (a & 0xaaaaaaaau))) { n -=  1;         }\n    return n;\n#elif VARIANT == 3\n    int n = 0;\n    if (!(a & 0xffff0000u)) { n |= 16;  a <<= 16; }\n    if (!(a & 0xff000000u)) { n |=  8;  a <<=  8; }\n    if (!(a & 0xf0000000u)) { n |=  4;  a <<=  4; }\n    if (!(a & 0xc0000000u)) { n |=  2;  a <<=  2; }\n    if ((int32_t)a >= 0) n++;\n    if ((int32_t)a == 0) n++;\n    return n;\n#elif VARIANT == 4\n    uint32_t b;\n    int n = 32;\n    if ((b = (a >> 16))) { n = n - 16;  a = b; }\n    if ((b = (a >>  8))) { n = n -  8;  a = b; }\n    if ((b = (a >>  4))) { n = n -  4;  a = b; }\n    if ((b = (a >>  2))) { n = n -  2;  a = b; }\n    if ((b = (a >>  1))) return n - 2;\n    return n - a;\n#endif\n}\n\nuint32_t div_core_ref (uint32_t a, uint32_t b)\n{\n    int64_t quot = ((int64_t)a << 16) / b;\n    /* round to nearest or even */\n    int64_t rem1 = ((int64_t)a << 16) - quot * b;\n    int64_t rem2 = rem1 - b;\n    if (llabs (rem2) < llabs (rem1)) quot++;\n    if ((llabs (rem2) == llabs (rem1)) && (quot & 1)) quot &= ~1;\n    return (uint32_t)quot;\n}\n\n// George Marsaglia's KISS PRNG, period 2**123. Newsgroup sci.math, 21 Jan 1999\n// Bug fix: Greg Rose, \"KISS: A Bit Too Simple\" http://eprint.iacr.org/2011/007\nstatic uint32_t kiss_z=362436069, kiss_w=521288629;\nstatic uint32_t kiss_jsr=123456789, kiss_jcong=380116160;\n#define znew (kiss_z=36969*(kiss_z&65535)+(kiss_z>>16))\n#define wnew (kiss_w=18000*(kiss_w&65535)+(kiss_w>>16))\n#define MWC  ((znew<<16)+wnew )\n#define SHR3 (kiss_jsr^=(kiss_jsr<<13),kiss_jsr^=(kiss_jsr>>17), \\\n              kiss_jsr^=(kiss_jsr<<5))\n#define CONG (kiss_jcong=69069*kiss_jcong+1234567)\n#define KISS ((MWC^CONG)+SHR3)\n\nint main (void)\n{\n    uint64_t count = 0ULL, stats[3] = {0ULL, 0ULL, 0ULL};\n    uint32_t a, b, res, ref;\n    do {\n        /* random dividend and divisor, avoiding overflow and divison by zero */\n        do {\n            a = KISS % 0x80000001u;\n            b = KISS % 0x80000001u;\n        } while ((b == 0) || ((((uint64_t)a << 16) / b) > 0x80000000ULL));\n\n        /* compute function under test and reference result */\n        ref = div_core_ref (a, b);\n        res = div_core (a, b);\n\n        if (llabs ((int64_t)res - (int64_t)ref) > 1) {\n            printf (\"\\nerror: a=%08x b=%08x res=%08x ref=%08x\\n\", a, b, res, ref);\n            break;\n        } else {\n            stats[(int64_t)res - (int64_t)ref + 1]++;\n        }\n        count++;\n        if (!(count & 0xffffff)) {\n            printf (\"\\r[-1]=%llu  [0]=%llu  [+1]=%llu\", stats[0], stats[1], stats[2]);\n        }\n    } while (count);\n    return EXIT_SUCCESS;\n}\n```\n\n    ", "Knowledge_point": "Approximation Algorithms", "Tag": "算法分析"}
