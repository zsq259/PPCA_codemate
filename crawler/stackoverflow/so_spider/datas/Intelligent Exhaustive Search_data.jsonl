{"Question": "Exhaustive website verifier\r\n                \r\nI have this grand idea to basically employ some brute force attack to test/verify that my web application doesn't crash.\n\nDon't get me started on unit testing, and IoC stuff, this is something else entirely.\n\nWhat I'm doing, and what I'm asking for help with is to create an intelligent exhaustive search, that explore parts of the program state.\n\nWhat I have is a web page with things I can do, clicking is one thing, text input is another, some inputs like radio buttons and drop down lists are constrained to certain values. Pretty basic things. What I end up with a finite set of events and values and what I want to model is a progression of state. Maybe this is FSM optimization in a way, but the goal is to systematically go through arbitrary permutations of events and values and see what happens.\n\nWhen a problem is found I want to try and provoke that error with as little effort as possible to be able to present a clear test case.\n\nThis relates to formal verification methods and I'm asking for help or insight from people with experience.\n    ", "Answer": "\r\nWhat you want to do sounds a little like model-checking, on the one hand, and automated test case generation on the other hand (in the latter category check out Concolic testing, a technique to avoid wasting time with unfeasible execution paths).\n\nModel-checking would be the preferred method if you assume your web application is correct and want to prove that it is. But in the case of a warning, you may have to work to understand if the problem is real or not. Test case generation is oriented towards bug-finding: it does not prove that you app is correct, but if it finds a problem, it gives you an input vector to produce it so you don't need to wonder if the problem is real.\n\nI am not aware of any existing tools for web apps, but that doesn't mean that they don't exist.\n    ", "Knowledge_point": "Intelligent Exhaustive Search", "Tag": "算法分析"}
{"Question": "Zip-file that contains nothing but itself?\r\n                \r\nJust out of curiosity, does there exist a valid zip-file (according to format spec) that, contains nothing but itself?\n\nPut another way, does the function implemented by ```\nunzip```\n have a fix-point?\n\nCan I write a program to search for such a fix-point in a intelligent (not-exhaustive) way?\n\nI've thought about the opposite as well, i.e. if ```\nzip```\n has a fix-point, but presumably a file can be compressed in different ways (different algorithms, different levels of compression and so on), thus whether or not ```\nf = zip(f)```\n holds for some file ```\nf```\n is probably implementation dependent. Since the zip-compression is loss-less however, the case for ```\nunzip```\n should probably have a \"formal\" answer.\n\nHas anyone explored this? Any pointers to related URLs are appreciated.\n    ", "Answer": "\r\nYou can read about it here: http://research.swtch.com/2010/03/zip-files-all-way-down.html\n\nHere you will find r.zip, which contains itself.\n    ", "Knowledge_point": "Intelligent Exhaustive Search", "Tag": "算法分析"}
{"Question": "Testing ASP.NET webforms applications\r\n                \r\nIf you're in my position you have a big WebForms applications which have escalated to this unmaintainable thing. Things break when you add new features and you need an inexpensive maintainable way to do some kind of automated testing.\n\nNow, from my understanding, the right thing to do would be to try building an abstraction layout of the page and user control model present in ASP.NET WebForms however, seeing as it would require a major investment in an existing application it is not an option.\n\nI'm trying and pushing for a REST-like development as much as possible because it has some nice properties. And while doing this I've written a simple spider bot that crawls all URLs it can find and tries, simply getting them.\nThis allowed my to quickly find bad data that was causing problems and avoid having my end-users clicking on broken things, however, this is of course not enough.\n\nI continued work on my crawler and it's developed into a simple REST client that tries different input combination, looking for a probable bug or crash. It's more intelligent that just an exhaustive search (because it knows about the ASP.NET WebForms application layer) and my goal here is to basically explore the state of the web application, hoping to hit all the corner cases before our users.\n\nDoes anyone have any experience doing something similar?\n\nAlso, for you test gurus out there. Is this a complete waste of time, or will I be able to actually say something about the quality here? From my perspective it seems to hit a sweet spot in that it will try things a potential end user would though a browser.\n\nAs I said before, we're stuck in a bad place. And we need a simple way out of it, right now.\n\nWe've tried things like Selenium, but it mandates a lot of extra work and we change things all the time, it's just no possible to maintain multiple selenium test suits for 50 different applications.\n    ", "Answer": "\r\nOf all the types of testing to implement, unit testing is both the easiest and the most likely to yield results, in terms of less bugs and more maintainable code.  Get that worked out before you deal with automated integration testing\n\n\nPick an IOC Container - I like Ninject for this personally\nFind a convenient place to inject \"service\" classes into your Page (the consturctor of a base Page class or override the module that loads pages, whatever works for you)\nPick a unit test framework and if you don't have an automated build then set one up; include running a full suite of unit tests in that build\nEvery time you go near a piece of logic in an aspx.cs file, see if you can't isolate it in a service and wrap unit tests around it\nTake a look at whether the MVP Pattern would be good for you - we found it decreased productivity as much as it increased testability (it did both a lot), but it works for some people\nSee about slowly migrating your app over to MVC, a page at a time if necessary\n\n\nAnd remember, you are not going to fix this problem overnight, you don't have time.  Just keep improving test coverage and you'll see the benefits over time.\n    ", "Knowledge_point": "Intelligent Exhaustive Search", "Tag": "算法分析"}
{"Question": "How can I parse nested source files with ANTLR4?\r\n                \r\nI've asked this question before (slightly differently) but didn't understand the answers enough at the time to give intelligent feedback (sigh).\n\nI need to be able to include files inside other files at arbitrary points so I need to be able to have a stack of files with a single parse tree. \n\nIf I was writing this myself (and I have done this in the past), my parser would recognize the \"Include xyz\" or \"Import abc\", and would cause the lexer to suspend reading from the current file, push that file on a stack, and continue reading characters from the new file until exhausted.\n\nHowever, when using ANTLR4 (where so far I've avoided inserting any code into the grammar file itself) and using the visitor pattern, all I see is the created tree which of course is too late.\n\nI've found references to PUSHSTREAM as something that can be done in the lexer but I cannot find an actual example and would really appreciate some help (either a pointer to an actual example that I perhaps missed when searching or a short code sample if someone has one).\n\nNote that I'm writing code in C++, not Java.\n\nThanks in advance\n    ", "Answer": "\r\nYears ago I developed a solution for ANTLR 2.7, to parse Windows resource files (*.rc). Such files are structured very much like C/C++ header files and support preprocessor directives like #if/#end/#pragma/#include.\n\nFor that I created a special character input stream (with a nested char input stream) which implements a stack based approach for include files. Whenever a new include directive is found in the char input a new stack entry is created with the current actual input stream, its position and line/column information (to provide local source locations, in case a parsing problem was found). That entry is pushed onto a stack and a new input stream is created. Once this is exhausted the TOS is popped off the stack and serving chars continued from the last position (after the #include statement). The lexer only sees a continuous stream of characters.\n    ", "Knowledge_point": "Intelligent Exhaustive Search", "Tag": "算法分析"}
{"Question": "jQuery, A search for a smarter way to Initialize\r\n                \r\nI'm looking for a better way to manage global component/plugin/widget initialization in a large project. It has many jQuery-powered components that I would like to initialize quickly & efficiently and after scouring the internet, I've only really found short-sighted examples that are only realistic/performant in smaller sites.\n\nThe Problem\n\nI Want to find a smart & elegant way of getting rid of this:\n\n```\n$(function() { $('.widget-one').widgetOne(); });\n$(function() { $('.widget-two').widgetTwo(); });\n$(function() { $('.widget-three').widgetThree(); });\n$(function() { $('.widget-four').widgetFour(); });\n```\n\n\nNow before you smack me for it, let me state that I know that in most cases (but not all) ```\n.widget-one```\n is a bad selector as it will on older browsers get all elements in the dom and check for the class.\n\nThe issue is, these widgets are not one-off's, and I may not know about thier existance ahead of time (generated in a web app view, maybe 2-3 times based on logic or a loop of products or something).\n\nSo the following solutions are out:\n\n```\n$(function() { $('#WidgetOne').widgetOne(); });\n```\n\n\nand\n\n```\n<span id=\"WidgetOne_12345\">...</span>\n<script type=\"javascript\">\n   $(function() { $('#WidgetOne_12345').widgetOne(); });\n</script>\n```\n\n\nThoughts\n\nThis is not a new problem, It's been around since day one. It still baffles me how this is so difficult to solve with jQuery even at this level of maturity. Either that or I'm missing something blatantly obvious.\n\nUnfortunately google-fu turns out badly on this subject as everyone suggests one of two things:\n\n\njQuery's ```\n.live()```\n or ```\n.delegate()```\n catch-all event handlers. This is just scary on a fundamental level. ```\n.delegate()```\n wouldn't be so bad, but this requires that the plugin/widget/control/whatever be entirely event driven. This would work in a lot of cases for sure, but not in others. It also makes following & organizing the code very complex. I won't even go into ```\n.live()```\n, for large complex sites, the event bubbling is slow & when you get enough components together, the list of queries to match becomes large making each click/focus/whatever event incrementally slower on the whole for the entire page.\nWork arounds that involve plugins like liveQuery, which is a very cool plugin for sure but it primarily seems to have been designed to solve a different issue (the issue of new items being brought in by ajax/dom creation), and still will get incrementally slower the more queries that stack up that need to be checked against.\n\n\nConclusion\n\nThere has to be a better way, I know there has to be one. I've exhausted my google fu on the subject, and still cannot find ideas/concepts/examples/discussion that is newer than jQuery 1.3.2 or with a grand picture in mind. In a perfect world this wouldn't be a problem because everyone would use intelligent browsers with modern standards & a decent javascript engine, and ```\n.class```\n queries wouldn't take eons and eons, but unfortunately this is not the case.\n\nI'm looking for ideas on how to tackle this, after going through many SO questions similar to this, and many articles on various jquery techniques I feel that if the information is out there it's buried underneath the many false positives & \"101 best jquery plugin\" results that turn up in any search with jQuery. I know someone out there has ran into this predicament.\n\nIdeas, links, examples, anything is welcome, there just has to be a better way.\n    ", "Answer": "\r\njust brainstorming off the top of my head I can think of one thing that would decrease your \"select and initialize\" time, but may be a trade off in pain during development. furthermore, you can't really use params with this method, but if you have a situation exactly like the one you describe above, this may be worth doing.\n\nif you maintain that each of your initializing elements all have the same class (like widget for example), you can create a single selector system of initializing your widgets.\n\nstructure the classes instead like this in your HTML:\n\n```\n<div class=\"widget\" widgetFunc=\"widgetOne\"></div>\n<div class=\"widget\" widgetFunc=\"widgetTwo\"></div>\n```\n\n\netc.\n\nand your jquery init function will look like:\n\n```\nvar $widgetInitializerElements = $('.widget');\n\n$.each($widgetInitializerElements,function(){ \n    var initialize = window[$(this).attr('widgetFunc')];\n\n    if(typeof initialize === 'function'){\n        $(this).initialize(); \n    }\n});\n```\n\n\nso this would reduce your selection to one time, and keep your jquery pretty clean. you just have to worry about all those HTML elements which may not be feasible and you will have trouble if you need to add params to initialize calls :/\n    ", "Knowledge_point": "Intelligent Exhaustive Search", "Tag": "算法分析"}
{"Question": "How does kernel know, which pages in the virtual address space correspond to a swapped out physical page frame?\r\n                \r\nConsider the following situation: the kernel has exhausted the physical RAM and needs to swap out a page. It picks least recently used page frame and wants to swap its contents out to the disk and allocate that frame to another process. \n\nWhat bothers me is that this page frame was already mapped to, generally speaking, several (identical) pages of several processes. The kernel has to somehow find all of those processes and mark the page as swapped out. How does it carry that out?\n\nThank you.\n\nEDIT: Illustrations to the question:\n\nBefore the swapping processes 1 and 2 had a shared Page 1, which resided in the physical memory frame 1:\n\n\n\nNow, the memory in the system is exhausted and kernel allocates memory for  process 3 by swapping out Page 1 from frame 1 and replacing it with Page 2.\nIn order to do that, it has to \n\n1) find all the processes, referring to Page 1 (Process 1 and Process 2 in our case)\n\n2) modify their Page Table Entries, setting \"Present\" bit to 0 and setting the Page 1 location in Swap\n\n\n\nSo, I don't get, how the step 1 is carried out. Kernel couldn't be just iteratively looking into every process's Page Tables in order to find the Page Table Entry, pointing to frame 1. There should be some kind of reverse mapping from page frames to Page Table Entries.\n\nTHE ANSWER IS:\n\n\"The most significant and important change to page table management is the introduction of Reverse Mapping (rmap). Referring to it as “rmap” is deliberate as it is the common usage of the “acronym” and should not be confused with the -rmap tree developed by Rik van Riel which has many more alterations to the stock VM than just the reverse mapping.\n\nIn a single sentence, rmap grants the ability to locate all PTEs which map a particular page given just the struct page. In 2.4, the only way to find all PTEs which map a shared page, such as a memory mapped shared library, is to linearaly search all page tables belonging to all processes. This is far too expensive and Linux tries to avoid the problem by using the swap cache (see Section 11.4). This means that with many shared pages, Linux may have to swap out entire processes regardless of the page age and usage patterns. 2.6 instead has a PTE chain associated with every struct page which may be traversed to remove a page from all page tables that reference it. This way, pages in the LRU can be swapped out in an intelligent manner without resorting to swapping entire processes.\"\n\nfrom Understanding the Linux Memory Management, \"what's new in Linux2.6\"\n    ", "Answer": "\r\nLinux:\n\nWhen swap file is used the Page Table Entry gets updated with one marked as invalid and holding information about where it is saved in the swap file. That is: an index to the ```\nswap_info```\n array and an offset within the ```\nswap_map```\n.\n\nExample from (an a bit old) Page Table Entry type (```\npte_t```\n) on a x86. Some\nof the bits are used as flags by the hardware:\n\n```\nBit         Function\n_PAGE_PRESENT   Page is resident in memory and not swapped out\n_PAGE_PROTNONE  Page is resident but not accessable\n_PAGE_RW        Set if the page may be written to\n_PAGE_USER      Set if the page is accessible from user space\n_PAGE_DIRTY     Set if the page is written to\n_PAGE_ACCESSED  Set if the page is accessed\n```\n\n\nTable 3.1: Page Table Entry Protection and Status Bits\n\nSee also another SO answer with a diagram of the x86-64 page table format.  When the low bit = 0, the hardware ignores all the other bits, so the kernel can use them for anything.  Even in a \"present\" entry, there are some guaranteed-ignored bits that aren't reserved for future hardware use, so the kernel can use them for its own purposes.\n\nPresumably other architectures are similar.\n\n\n\nIn simple terms: A process points to a page, the page get updated. Thus the processes are, in effect, also updated. When the physical page get requested it is swapped in and thus all processes as well. The point being that the Page Table Entry is not removed when memory is swapped out.\n\nYou might find some of this useful: \n\n\nGustavo Duarte: How The Kernel Manages Your Memory.\n\n\nThe kernel documentation included book of Mel Gorman (2007):\n\n\n11.2 Mapping Page Table Entries to Swap Entries\n3.2 Describing a Page Table Entry \nRed Hat on VM's Life of a page.\n\n    ", "Knowledge_point": "Intelligent Exhaustive Search", "Tag": "算法分析"}
