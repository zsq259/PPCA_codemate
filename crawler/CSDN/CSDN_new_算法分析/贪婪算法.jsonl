{"Answer": "A*=耗散g+启发式h，当g=0时，A*与GBFS等价", "Konwledge_Point": "贪婪算法", "Question": "路径规划问题 A*算法和GBFS算法的联系\n请问在路径规划问题中 A*算法和贪婪最佳优先GBFS算法的联系是什么？", "Tag": "算法分析"}
{"Answer": "你可以看看CGAL的表面重建https://doc.cgal.org/latest/Manual/tuto_reconstruction.html", "Konwledge_Point": "贪婪算法", "Question": "开放式表面三维点云重建\n开放式表面（正面例子：室内扫描，反面例子：史坦佛兔）重建的最佳解法是不是得用非隐式曲面的写法\n\n\n问题一：想问是否有人也碰巧在看pcl中的贪婪投影演算法，对里面的gp3.h gp3.hpp或演算法中可见不可见的选点方式有研究( 还是小妹直接去私点云侠！\n问题二：若不用贪婪投影演算法，是否开放式表面还有更好的点云重建方法？泊松问题在于封闭式表面，ball pivot 问题在于不适应不均匀点云\n\n\n想找人讨论却发现网上没怎么找到资料⋯", "Tag": "算法分析"}
{"Answer": "你把gym换成0.25.2版本就行了。  pip install gym==0.25.2", "Konwledge_Point": "贪婪算法", "Question": "强化学习DQN:AttributeError: 'CartPoleEnv' object has no attribute 'seed'\n运行动手学强化学习中DQN算法时出现问题，求帮助啊\n\n\n\n\nimport\n random\n\nimport\n gym\n\nimport\n numpy \nas\n np\n\nimport\n collections\n\nfrom\n tqdm \nimport\n tqdm\n\nimport\n torch\n\nimport\n torch.nn.functional \nas\n F\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n rl_utils\n\n\n\nclass\n \nReplayBuffer\n:\n    \n''' 经验回放池 '''\n\n\n    \ndef\n \n__init__\n(\nself, capacity\n):\n        self.buffer = collections.deque(maxlen=capacity)  \n# 队列,先进先出\n\n\n    \ndef\n \nadd\n(\nself, state, action, reward, next_state, done\n):  \n# 将数据加入buffer\n\n        self.buffer.append((state, action, reward, next_state, done))\n\n    \ndef\n \nsample\n(\nself, batch_size\n):  \n# 从buffer中采样数据,数量为batch_size\n\n        \n# random.sample(x,size) 随机截取列表x指定size长度，顺序不变\n\n        transitions = random.sample(self.buffer, batch_size)\n        \n# transitions 包含很多transition，而transition中又包含state, action, reward, next_state, done\n\n        \n# *transitions 是将transition的参数解包出来state, action, reward, next_state, done\n\n        \n# zip(*transitions)是将属于一种属性的封装在一起，如所有state(s1,s2,s3,...)\n\n        state, action, reward, next_state, done = \nzip\n(*transitions)\n        \nreturn\n np.array(state), action, reward, np.array(next_state), done\n\n    \ndef\n \nsize\n(\nself\n):  \n# 目前buffer中数据的数量\n\n        \nreturn\n \nlen\n(self.buffer)\n\n\n\nclass\n \nQnet\n(torch.nn.Module):\n    \n''' 只有一层隐藏层的Q网络 '''\n\n\n    \ndef\n \n__init__\n(\nself, state_dim, hidden_dim, action_dim\n):\n        \nsuper\n(Qnet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n\n    \ndef\n \nforward\n(\nself, x\n):\n        x = F.relu(self.fc1(x))  \n# 隐藏层使用ReLU激活函数\n\n        \nreturn\n self.fc2(x)\n\n\n\nclass\n \nDQN\n:\n    \n''' DQN算法 '''\n\n\n    \ndef\n \n__init__\n(\nself, state_dim, hidden_dim, action_dim, learning_rate, gamma,\n                 epsilon, target_update, device\n):\n        self.action_dim = action_dim\n        self.q_net = Qnet(state_dim, hidden_dim,\n                          self.action_dim).to(device)  \n# Q网络\n\n        \n# 目标网络\n\n        self.target_q_net = Qnet(state_dim, hidden_dim,\n                                 self.action_dim).to(device)\n        \n# 使用Adam优化器\n\n        self.optimizer = torch.optim.Adam(self.q_net.parameters(),\n                                          lr=learning_rate)\n        self.gamma = gamma  \n# 折扣因子\n\n        self.epsilon = epsilon  \n# epsilon-贪婪策略\n\n        self.target_update = target_update  \n# 目标网络更新频率\n\n        self.count = \n0\n  \n# 计数器,记录更新次数\n\n        self.device = device\n\n    \ndef\n \ntake_action\n(\nself, state\n):  \n# epsilon-贪婪策略采取动作\n\n        \nif\n np.random.random() < self.epsilon:\n            action = np.random.randint(self.action_dim)\n        \nelse\n:\n            state = torch.tensor([state], dtype=torch.\nfloat\n).to(self.device)\n            action = self.q_net(state).argmax().item()\n        \nreturn\n action\n\n    \ndef\n \nupdate\n(\nself, transition_dict\n):\n        states = torch.tensor(transition_dict[\n'states'\n],\n                              dtype=torch.\nfloat\n).to(self.device)\n        actions = torch.tensor(transition_dict[\n'actions'\n]).view(-\n1\n, \n1\n).to(\n            self.device)\n        rewards = torch.tensor(transition_dict[\n'rewards'\n],\n                               dtype=torch.\nfloat\n).view(-\n1\n, \n1\n).to(self.device)\n        next_states = torch.tensor(transition_dict[\n'next_states'\n],\n                                   dtype=torch.\nfloat\n).to(self.device)\n        dones = torch.tensor(transition_dict[\n'dones'\n],\n                             dtype=torch.\nfloat\n).view(-\n1\n, \n1\n).to(self.device)\n\n        q_values = self.q_net(states).gather(\n1\n, actions)  \n# Q值\n\n        \n# 下个状态的最大Q值\n\n        max_next_q_values = self.target_q_net(next_states).\nmax\n(\n1\n)[\n0\n].view(\n            -\n1\n, \n1\n)\n        q_targets = rewards + self.gamma * max_next_q_values * (\n1\n - dones\n                                                                )  \n# TD误差目标\n\n        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  \n# 均方误差损失函数\n\n        self.optimizer.zero_grad()  \n# PyTorch中默认梯度会累积,这里需要显式将梯度置为0\n\n        dqn_loss.backward()  \n# 反向传播更新参数\n\n        self.optimizer.step()\n\n        \nif\n self.count % self.target_update == \n0\n:\n            self.target_q_net.load_state_dict(\n                self.q_net.state_dict())  \n# 更新目标网络\n\n        self.count += \n1\n\n\n\nlr = \n2e-3\n\nnum_episodes = \n500\n\nhidden_dim = \n128\n\ngamma = \n0.98\n\nepsilon = \n0.01\n\ntarget_update = \n10\n\nbuffer_size = \n10000\n\nminimal_size = \n500\n\nbatch_size = \n64\n\ndevice = torch.device(\n\"cuda\"\n) \nif\n torch.cuda.is_available() \nelse\n torch.device(\n    \n\"cpu\"\n)\n\nenv_name = \n'CartPole-v1'\n\nenv = gym.make(env_name)\nrandom.seed(\n0\n)\nnp.random.seed(\n0\n)\nenv.seed(\n0\n)\ntorch.manual_seed(\n0\n)\nreplay_buffer = ReplayBuffer(buffer_size)\nstate_dim = env.observation_space.shape[\n0\n]\naction_dim = env.action_space.n\nagent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,\n            target_update, device)\n\nreturn_list = []\n\nfor\n i \nin\n \nrange\n(\n10\n):\n    \nwith\n tqdm(total=\nint\n(num_episodes / \n10\n), desc=\n'Iteration %d'\n % i) \nas\n pbar:\n        \nfor\n i_episode \nin\n \nrange\n(\nint\n(num_episodes / \n10\n)):\n            episode_return = \n0\n\n            state = env.reset()\n            done = \nFalse\n\n            \nwhile\n \nnot\n done:\n                action = agent.take_action(state)\n                next_state, reward, done, _ = env.step(action)\n                replay_buffer.add(state, action, reward, next_state, done)\n                state = next_state\n                episode_return += reward\n                \n# 当buffer数据的数量超过一定值后,才进行Q网络训练\n\n                \nif\n replay_buffer.size() > minimal_size:\n                    b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n                    transition_dict = {\n                        \n'states'\n: b_s,\n                        \n'actions'\n: b_a,\n                        \n'next_states'\n: b_ns,\n                        \n'rewards'\n: b_r,\n                        \n'dones'\n: b_d\n                    }\n                    agent.update(transition_dict)\n            return_list.append(episode_return)\n            \nif\n (i_episode + \n1\n) % \n10\n == \n0\n:\n                pbar.set_postfix({\n                    \n'episode'\n:\n                        \n'%d'\n % (num_episodes / \n10\n * i + i_episode + \n1\n),\n                    \n'return'\n:\n                        \n'%.3f'\n % np.mean(return_list[-\n10\n:])\n                })\n            pbar.update(\n1\n)\n\n\n\n\n\n运行结果及报错内容\n\n\n\nTraceback (most recent \ncall\n last):\n  File \"E:\\graduate student\\Python\\Reinforcement learning\\HANDS-ON Reinforcement learning\\04_DQN\\01_DQN.py\", \nline\n \n120\n, \nin\n \n    env.seed(\n0\n)\n  File \"D:\\Python39\\lib\\site-packages\\gym\\core.py\", \nline\n \n241\n, \nin\n __getattr__\n    \nreturn\n getattr(self.env, \nname\n)\n  File \"D:\\Python39\\lib\\site-packages\\gym\\core.py\", \nline\n \n241\n, \nin\n __getattr__\n    \nreturn\n getattr(self.env, \nname\n)\n  File \"D:\\Python39\\lib\\site-packages\\gym\\core.py\", \nline\n \n241\n, \nin\n __getattr__\n    \nreturn\n getattr(self.env, \nname\n)\nAttributeError: \n'CartPoleEnv'\n \nobject\n has \nno\n \nattribute\n \n'seed'\n\n", "Tag": "算法分析"}
{"Answer": "应该是没有读取进来数据。在getstate函数里，for循环没有运行，所以state是None。也可能len(block)刚好等于1，而你又用len(block)-1,所以循环没有进行。我不知道你是不是想用for去枚举block，如果是，应该是for i in range(len(block))", "Konwledge_Point": "贪婪算法", "Question": "请各位大神帮帮忙,因为这是毕业论文所用的代码,我想问问到底是否是对的,看看是我代码错了,还是逻辑错了\n首先我这里有一个yunfuzai_main.py:\n\n\n\n\n\nfrom dqn_agent import Agent\nfrom model import QNetwork\nimport matplotlib.pyplot as plt\nimport  numpy as np\nimport torch\nimport xlrd\nfrom openpyxl import load_workbook\nimport gc\n\n\nSTATE_SIZE = 10\nEPISODE_COUNT = 1000\n\n# def dqn(n_episodes=EPISODE_COUNT,eps_start=2.0,eps_end=0.03,eps_decay=0.990):\n#     scores = []\n#     for i_episode in range(1, n_episodes + 1):\n#         print(\"Episode\" + str(i_episode))\n#         state = getState(stockData, 0, STATE_SIZE + 1)\n#         total_profit = 0\n#         agent.inventory = []\n#         eps = eps_start\n#\n#         for t in range(l):\n#             action = agent.act(state, eps)\n#             next_state = getState(stockData, t + 1, STATE_SIZE + 1)\n#             reward = 0\n#\n#             if action == 1:  # 买入\n#                 agent.inventory.append(stockData[t])\n#                 # print(\"buy\" + str(stockData[t]))\n#             elif action == 2 and len(agent.inventory) > 0:  # 卖出\n#                 bought_price = agent.inventory.pop(0)\n#                 total_profit += stockData[t] - bought_price\n#                 # reward = max(stockData[t] - bought_price, 0)\n#                 reward = stockData[t] - bought_price\n#                 # print(\"Sell: \" + str(stockData[t]) + \" | Profit: \" + str(stockData[t] - bought_price))\n#             done = 1 if t == l - 1 else 0\n#             agent.step(state, action, reward, next_state, done)\n#             eps = max(eps_end, eps * eps_decay)\n#             state = next_state\n#\n#             # if done:\n#             #     print(\"------------------------------\")\n#             #     print(\"total_profit = \" + str(total_profit))\n#             #     print(\"------------------------------\")\n#         scores.append(total_profit)\n#     return scores\n\ndef dqn1(n_episodes1=EPISODE_COUNT, eps_start1=1.0, eps_end1=0.01, eps_decay1=0.9995):\n    scores1 = []\n    for i_episode in range(1,n_episodes1+1):\n        print(\"Episode\" + str(i_episode))\n\n        state = getState(stockData, 0, STATE_SIZE + 1)\n        agent.inventory = []\n        eps = eps_start1\n        #global loss\n        print(1)\n        for t in range(l):\n            print(2)\n            action = agent.act(state,eps)\n            print(1)\n            next_state = getState(stockData, t + 1, STATE_SIZE + 1)\n            reward = 0\n            done = 1 if t == l - 1 else 0\n            global loss\n            loss = agent.step(state, action, reward, next_state, done)\n\n            print(\"loss\", + str(loss))\n            scores1.append(loss)\n            gc.collect()\n        gc.collect()\n\n    return scores1\n\n            # if action == 1:# 过载\n            #     agent.inventory.append(stockData[t])\n            # elif action == 2 and len(agent.inventory) > 0:\n\n\ndef getState(data, t, n):\n    d = t - n + 1\n    # block = data[d:t + 1] if d>= 0 else -d * [data[0]]+ data[0:t+1]\n    block = data[d:t + 1]\n    #res = [0 for x in range(0, n)]\n    #res = []\n    buffer = []\n    for i in range(len(block) - 1):\n        print(\"res=\",buffer[i])\n        buffer.append(block[i + 1]-block[i])\n        #print(\"res=\",res[i])\n        return np.array([buffer])\n    #return np.array([res])\n\nif __name__== '__main__':\n    print(1)\n    #stockData = []\n    #stockData = []\n    stockData = [None]*801\n    datas1 =xlrd.open_workbook(r'C:\\Users\\86138\\Desktop\\zi_ding_yi.xlsx',{'constant_memory':True})\n\n    #datas1 = xlrd.open_workbook(r'C:\\Users\\86138\\Desktop\\zi_ding_yi.xlsx')\n    print(2)\n    table = datas1.sheets()[0]\n    row_num = 0\n    print(3)\n    # for item in table[1:]:\n    #      stockData.append(float(table.col_values(6)))\n    col = table.col_values(5)\n    while row_num <= 800 :\n        # stockData[row_num] = table.col_values(5,0,row_num)\n        #stockData[row_num] = table.cell_value(row_num,5)\\\n        stockData.append(table.cell_value(row_num,5))\n        row_num += 1\n     # stockData{row_num} = table.col_values(5,0,row_num)\n     # row_num =+1\n\n    agent = Agent(state_size=STATE_SIZE, action_size=3)\n    print(4)\n    l = len(stockData) - 1\n\n    # scores = dqn()\n    scores1 =dqn1()\n\n\n\n它说我的错误是：\n\n\n\n\n\nC:\\Users\\86138\\anaconda3\\python.exe C:/Users/86138/Desktop/stockPrediction-master/yunfuzai_main.py\n1\n2\n3\n4\nEpisode1\n1\n2\nTraceback (most recent call last):\n  File \"C:/Users/86138/Desktop/stockPrediction-master/yunfuzai_main.py\", line 123, in \n    scores1 =dqn1()\n  File \"C:/Users/86138/Desktop/stockPrediction-master/yunfuzai_main.py\", line 61, in dqn1\n    action = agent.act(state,eps)\n  File \"C:\\Users\\86138\\Desktop\\stockPrediction-master\\dqn_agent.py\", line 136, in act\n    state = torch.tensor(state).float.unsqueeze(0).to(device)\nRuntimeError: Could not infer dtype of NoneType\n\n\n\n\n而我的dqn_agent.py:\n\n\n\n\n\nimport numpy as np\nimport random\nfrom collections import namedtuple, deque\n\nfrom model import QNetwork\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n#初始化超参数\nBUFFER_SIZE = int(1e5)\n#缓冲去大小，重播缓冲区大小\nBATCH_SIZE = 64\n#批处理大小，最小批量大小，minbatch size\nGAMMA = 0.99\n##折扣率\nTAU = 1e-3\n#用于目标参数的软更新\nLR = 5e-4\n#学习率\nUPDATE_EVERY = 4\n#更新网络的快慢\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#首先看下你的设备有无cuda可用：\n\nclass ReplayBuffer:\n    def __init__(self, action_size, buffer_size, batch_size):# 初始化记忆库\n        self.action_size = action_size\n        self.memory = deque(maxlen=buffer_size)\n        #使用 deque(maxlen=N) 构造函数会创建一个固定大小的队列。当新的元素加入并且这个队列已满的时候，最老的元素会自动被移除掉\n        self.batch_size = batch_size\n        #batch字面上是批量的意思，在深度学习中指的是计算一次cost需要的输入数据个数。\n        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n        #经验池的作用就是把每次进行的游戏回合transition（episode，step）记录下来存储起来。\n        # 在训练的时候则是在经验池中随机取一组transition batch对Q网络进行优化。同时，也需要及时丢掉过老的记录，及时更新。\n        #首先，定义了一个名为Experience的namedtuple。包括内容除了上面算法中提到的（s,a,r,s'）.还有结束的标识’done’。\n    def add(self, state, action, reward, next_state, done):# 向记忆库中加入一个记忆\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n\n    def sample(self):# 随机取出一个minibatch\n        experiences = random.sample(self.memory, k=self.batch_size)\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).float().to(device)\n\n        return (states, actions, rewards, next_states, dones)\n\n    def __len__(self):\n        return len(self.memory)\n\n\nclass Agent:\n\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n\n        # Q-Network\n        #在学习过程中，我们使用两个不相关的Q网络（Q_network_local和Q_network_target）来计算预测值（权重θ）和目标值（权重θ’）。\n        # 经过若干步骤后，目标网络会被冻结，然后拷贝实际的Q网络的权重到目标网络权重。\n        # 冻结目标Q网络一段时间再用实际Q网络的权重更新其权重，可以稳定训练过程\n        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n        self.qnetwork_target = QNetwork(state_size, action_size).to(device)#目标策略，智能体要学习的策略\n        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n        #在线性回归或者监督学习中，我们会计算预测值与真实值之间的差距，也就是loss\n        #在计算得出loss之后，通常会使用Optimizer对所构造的数学模型/网络模型进行参数优化，\n        #通常情况下，优化的最终目的是使得loss趋向于最小。\n\n\n        # Replay Buffer，所以我们设置一个replay_buffer，获得新的交互数据，抛弃旧的数据，\n        # 并且每次从这个replay_buffer中随机取一个batch，来训练我们的系统\n        self.memory = ReplayBuffer(action_size, buffer_size=BUFFER_SIZE,batch_size=BATCH_SIZE)\n        # 初始化迭代步数\n        self.t_step = 0\n        # 初始化持仓\n        self.inventory = []\n    # Experience Replay就是这样的一种技术，在游戏数据的采集过程中，所有的经验数据都被存储到一个回放池(replay memory)中。当训练网络时，从回放池中随机地取一小撮数据，\n    # 而不是最新连续的几条转换数据，进行训练。\n\n    def step(self, state, action, reward, next_state, done):\n        # 每一步需要先存储记忆库\n        self.memory.add(state, action, reward, next_state, done)\n\n        # 每隔若干步学习一次\n        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n        if self.t_step == 0:\n            if len(self.memory) > BATCH_SIZE:\n                experience = self.memory.sample()\n                self.learn(experience, GAMMA)\n\n    def learn(self, experience, gamma):\n        # 更新迭代\n        states, actions, rewards, next_states, dones = experience\n\n        # target network:compute and minimize the loss.计算并最小化损失\n        # Get max predicted Q values(for next states) from target model:从目标模型得到最大的预测Q值(下一个状态)\n        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n        # compute Q target for current states:计算当前状态的Q目标。\n        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n\n        Q_expected = self.qnetwork_local(states).gather(1, actions.long())# 固定行号，确认列\n        # Compute loss\n        loss = F.mse_loss(Q_expected, Q_targets)\n        # Minimize the loss\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        # update target network\n        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=TAU)\n        return loss\n\n\n    def soft_update(self, local_model, target_model, tau):\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n    #实现Q'到Q的逼近 use .data and .data.copy#\n    def act(self, state, eps = 0.):\n        #Returns actions for given state as per current policy.\n        #Params\n        # state (array_like): current state\n        # eps (float): epsilon, for epsilon-greedy action selection\n        #参数个数\n        #状态(array_like):当前状态\n        #eps (float):用于epsilon-贪婪动作选择\n\n\n        # 返回动作值orch.unsqueeze()这个函数主要是对数据维度进行扩充\n        #state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        state = torch.tensor(state).float.unsqueeze(0).to(device)\n        self.qnetwork_local.eval()\n        with torch.no_grad():\n            action_values = self.qnetwork_local(state)\n        self.qnetwork_local.train()\n# # Epsilon-greedy action selection\n        if random.random() > eps:\n            return np.argmax(action_values.cpu().data.numpy())\n        else:\n            return random.choice(np.arange(self.action_size))\n        #eps:根据当前策略返回给定状态的操作参数个数\n\n\n\n我也是真没辙了，所以我想问问，到底是怎么回事，因为我真实想法是把那个loss输出来。但似乎不行，想请各位大神帮我看看，因为是毕业论文用的代码，所以就想问问。拜托了", "Tag": "算法分析"}
