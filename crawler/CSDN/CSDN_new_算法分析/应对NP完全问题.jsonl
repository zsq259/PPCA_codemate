{"Answer": "怎样证明一个问题C是NP完全问题呢？首先，要证明C是NP问题，也就是C的解的正确性容易验证；然后要证明有一个NP完全问题B，能够在多项式时间内归约到C。这就要求必须先存在至少一个NPC问题。Cook证明了NP完全问题的祖先就是SAT。SAT问题是指给定一个包含n个布尔变量的逻辑式，问是否存在一个取值组合，使得该式被满足。Cook证明了SAT是一个NPC问题，如果SAT容易解决，那么所有NP都容易解决。", "Konwledge_Point": "应对NP完全问题", "Question": "为什么说旅行商问题是NP Hard的？\n为什么说旅行商问题是NP Hard的？网上看了很多文章还是比较迷糊，有谁能清晰地讲解下。怎么样判断一个算法是不是NP Hard？", "Tag": "算法分析"}
{"Answer": "没有返回值，可以检查一下源或者在调用之前先判断一下是否为空", "Konwledge_Point": "应对NP完全问题", "Question": "在python中运行image = image.astype(np.float32)时候发生错误。\nAttributeError: 'NoneType' object has no attribute 'astype'请问下这是什么原因呢？有什么解决办法", "Tag": "算法分析"}
{"Answer": "np.where(a == 1)返回的是a=1对应的位置，需要用“行”和“列”表示，原矩阵a是个一维矩阵（行数只有一行）。\r\n第一行的[ 0 0 0 0 0 0 0 0 0 0 0]代表的是对应点的行下标\r\n第二行的[ 5 6 13 19 28 29 34 44 45 46 48]代表的是列下标\r\n合起来，就是（0，5），（0，6），（0，13）.......", "Konwledge_Point": "应对NP完全问题", "Question": "np.where的用法 np.array(np.where(a == 1))的输出问题\ny - 实际的标签 p - 预测\n\n\n\n    p = [[1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1.\n1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1.\n0. 0.]]\n  y=[[1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0\n0 1 0 0 1 1 1 0 0 0 1 1 1 0]]\n  a = [[2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 0. 2. 0. 2. 2. 1. 2. 0. 0. 2.\n2. 2. 2. 0. 1. 1. 2. 2. 2. 2. 1. 0. 0. 2. 0. 0. 2. 2. 2. 0. 1. 1. 1. 2.\n1. 0.]]\n    a = p + y\nmislabeled_indices = np.array(np.where(a == 1))\n\n\n\n\n为什么输出的mislabeled_indices = [[ 0  0  0  0  0  0  0  0  0  0  0]\n\n [ 5  6 13 19 28 29 34 44 45 46 48]]\n\n 怎么是（2，x）的列表？ 第0行为什么都是0？", "Tag": "算法分析"}
{"Answer": "\nwaveData = np.frombuffer(strData, dtype='int16')", "Konwledge_Point": "应对NP完全问题", "Question": "Python中声音信号导入后显示出问题（函数fromstring和frombuffer的使用区别）\n\n\n关于Python中声音信号的导入及显示；\n\n\n相关代码：\n\n\nimport wave     # 导入音频处理包\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nf = wave.open(r'D:\\1.wav', 'rb')\nparams = f.getparams()\nnchannels, sampwidth, framerate, nframes = params[:4]\n# nchannels:声道数;sampwidth:量化位数（byte）;framerate:采样频率;nframes:采样点数\nprint('channel:', nchannels, 'sampwidth:', sampwidth, 'framerate:', framerate, 'numframes:', nframes)\n\nstrData = f.readframes(nframes)  # 读取音频，字符串格式\nf.close()\n\n# waveData = np.frombuffer(strData, dtype='S1', offset=0)  \nwaveData = np.fromstring(strData, dtype=np.short)  # 将字符串转化为int\n\nwaveData.shape = -1,2   # 将waveData数组改为2列，行数自动匹配。\nwaveData = waveData.T\ntime = np.arange(0, nframes) * (1.0 / framerate)\n\nplt.figure(1)\nplt.subplot(2,1,1)\nplt.plot(time, waveData[0])\nplt.subplot(2,1,2)\nplt.plot(time, waveData[1], c='r')\nplt.xlabel(\"Time(s)\")\nplt.show()\n\n\n\n\n\n\n\n报错信息：\n\nDeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n\nwaveData = np.fromstring(strData, dtype=np.short)  # 将字符串转化为int\n\n\n系统建议改用frombuffer，因为fromstring提示将会被弃用，但是使用frombuffer后得出的数组经过转换后不能和time的维度相匹配，而fromstring后的数据是左右声道的数据。想要知道如何使用frombuffer来代替fromstring！\n\n\n图像可以正常显示，不会报错但是会报警告，多谢大神的帮忙\n\n\n声音信号 1.wav\n\n\n\n", "Tag": "算法分析"}
{"Answer": "你这里提示错误是因为数组的长度为19429344 分割后的五维数组为(53018,1,17,25,25) 原始数组不能达到这个分割后的数组长度,所以不能分割.你需要检查x_1_25_final_array_new.npy是否在内部加载了文件或者数组,或者你加载的文件数组数据是否符合要求", "Konwledge_Point": "应对NP完全问题", "Question": "np.load报错 ValueError: cannot reshape array\n问题遇到的现象和发生背景\n\n\n就简简单单一句加载npy文件代码，不知道为啥报错了reshape？\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport\n os\n\nfrom\n osgeo \nimport\n gdal\n\nfrom\n osgeo \nimport\n ogr\n\nimport\n sys\n\nimport\n random\n\nimport\n numpy \nas\n np\n\nimport\n pandas \nas\n pd\n\n\nx_train_array\n = np.load('../x_1_25_final_array_new.npy')\n\n\n\n运行结果及报错内容\n\n\nTraceback (most recent \ncall\n last):\n  File \"/home/wangweiming/下载/FirePredict1031/me/test.py\", \nline\n \n10\n, \nin\n \n    x_train_array = np.\nload\n(\n'../x_1_25_final_array_new.npy'\n)\n  File \"/home/wangweiming/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\", \nline\n \n440\n, \nin\n \nload\n\n    \nreturn\n \nformat\n.read_array(fid, allow_pickle=allow_pickle,\n  File \"/home/wangweiming/anaconda3/lib/python3.8/site-packages/numpy/lib/format.py\", \nline\n \n783\n, \nin\n read_array\n    \narray\n.shape = shape\nValueError: cannot reshape \narray\n \nof\n size \n19429344\n \ninto\n shape (\n53018\n,\n1\n,\n17\n,\n25\n,\n25\n)\n\n\n\n我的解答思路和尝试过的方法\n\n\n我并没有使用reshape语句，检查了源文件，我也没有修改过文件或查到reshape语句\n\n\n我想要达到的结果\n\n\n要怎么解决或避开这问题成功加载npy文件", "Tag": "算法分析"}
{"Answer": "构建一个 5125123的数组，每个值都是0相当于生成了一张黑色的 512*512大小的图片。", "Konwledge_Point": "应对NP完全问题", "Question": "opencv中np的有关数组的问题\nimg=np.zeros((512,512,3),np.uint8）这个咋理解 ，求解", "Tag": "算法分析"}
{"Answer": "这个问题你可以去看看np.arange的官方文档，里面提到说如果step不是整数时，建议使用np.linespace函数，因为此时np.arange函数存在着数值不稳定的问题。实际上np.arange函数采用的step是dtype(start+step)-dtype(start)，当dtype参数没有显式给出的时候会根据其他输入参数进行推断，输入的为float类型，计算出的实际step就也为float型，且不稳定一般不等于原始输入step。见下述代码：\nfloat(1.58+0.1)-float(1.58)\nOut[20]: 0.10000000000000009\n\n官方文档：https://numpy.org/doc/stable/reference/generated/numpy.arange.html", "Konwledge_Point": "应对NP完全问题", "Question": "python 的numpy中arange()函数终值包含问题\nimport numpy as np\nab=np.arange(1.58,1.61,0.01)\nabc=np.arange(1.58,1.65,0.01)\nprint(ab)\nprint(abc)\n\n\n结果：\n[1.58 1.59 1.6  1.61]\n[1.58 1.59 1.6  1.61 1.62 1.63 1.64]\n\n\n问题：arange()函数一个包含终值，一个不包含终值，是为啥呢？", "Tag": "算法分析"}
{"Answer": "会报错,改成;  或者A=np.array([[10,11,12],[13,14,15],[16,17,18]])和A[[[2,1],[1,2]]]分成两行。A=np.array([[10,11,12],[13,14,15],[16,17,18]])得到一个二维数组Aarray([[10, 11, 12],       [13, 14, 15],       [16, 17, 18]])A[[2,1]]得到的是array([[16, 17, 18],       [13, 14, 15]])A[[2,1],[1,2]]得到的是array([17, 15])", "Konwledge_Point": "应对NP完全问题", "Question": "A=np.array的输出结果是（）\n\n\nA\n=np.array([[\n10\n,\n11\n,\n12\n],[\n13\n,\n14\n,\n15\n],[\n16\n,\n17\n,\n18\n]]),A[[[\n2\n,\n1\n],[\n1\n,\n2\n]]]\n\n\n\n的输出结果是（）", "Tag": "算法分析"}
{"Answer": "你生成了一个二维数值Aarray([[10, 11, 12],       [13, 14, 15],       [16, 17, 18]])\nA[[2,1]]：取了A中的第三行和第三行array([[16, 17, 18],       [13, 14, 15]])\nA[[2,1],[1,2]]：取了A中的第三行的第二个元素和第三行第三个元素", "Konwledge_Point": "应对NP完全问题", "Question": "A=np.array输出问题\nA\n=np.array([[\n10\n,\n11\n,\n12\n],[\n13\n,\n14\n,\n15\n],[\n16\n,\n17\n,\n18\n]]),A[[[\n2\n,\n1\n],[\n1\n,\n2\n]]]的输出结果是（）\n\n\n\n\narray([17, 15])\n\n\n为什么？", "Tag": "算法分析"}
{"Answer": "1维数组叫矢量，2维数组叫矩阵，3维及大于3维的数组就叫多维数组了", "Konwledge_Point": "应对NP完全问题", "Question": "np.zeros((2,2,3))为什么是三个参数，怎么理解？用在哪里？\narray([[[ 0.,  0.,  0.],\n\n        [ 0.,  0.,  0.]],\n\n       [[ 0.,  0.,  0.],\n\n        [ 0.,  0.,  0.]]])\n\n    产生2个2行3列矩阵？一直不理解。", "Tag": "算法分析"}
{"Answer": "可以将这段代码改下\r\nx = np.linspace(-4,4,num=50,dtype=np.complex)\r\ny = np.linspace(-4,4,num=50,dtype=np.complex)\r\n不改dtype的话，在出现负数的根时会出现问题。", "Konwledge_Point": "应对NP完全问题", "Question": "Python的运算符的一类问题，Z = np.cos((X**3+Y**3)**(1/3))\nimport numpy as np\n\nx = np.linspace(-4,4,num=50)\n\ny = np.linspace(-4,4,num=50)\n\nX,Y = np.meshgrid(x,y)\n\nZ = np.cos((X**3+Y**3)**(1/3))\n\n程序提示报错：\n\nD:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in power\n\n  \"\"\"\n\n\n\n但是当我将程序中的Z = np.cos((X**3+Y**3)**(1/3))改成Z = np.cos((X**2+Y**2)**(1/3))，程序就能正常运行，请问各位这是怎么回事啊？\n", "Tag": "算法分析"}
{"Answer": "a=np.arange(2)=[0,1]b=np.arange(3,5)=[3,4]np.lexsort([a,b])=[0,1]np.lexsort是对b先进行排序，排序好的索引有相同的再按a的顺序进行排序\nhttps://blog.csdn.net/Mxeron/article/details/113405566\nhttps://blog.csdn.net/weixin_38145317/article/details/90370558", "Konwledge_Point": "应对NP完全问题", "Question": "a=np.arange(2)，b=np.arange(3,5)，print(np.lexsort([a,b]))的输出结果是()。\na=np.arange(2)，b=np.arange(3,5)，print(np.lexsort([a,b]))的输出结果是()。", "Tag": "算法分析"}
{"Answer": "作用：求两个数组相同的个数。\r\n这段代码可以简单的求IOU，利用元素个数求取。当然正常的做法是按照区域求取。", "Konwledge_Point": "应对NP完全问题", "Question": "np.sum(np.array(test_labels)==np.array(test_labels_copy)）求解释代码意思\nnp.sum(np.array(test_labels)==np.array(test_labels_copy))\n\n有大佬能解释一下这行代码的意思吗？感激不尽", "Tag": "算法分析"}
{"Answer": "\nNumPy source code can be tricky to navigate, because it has so many functions for so many data types. You can find the C-level source code for the absolute value function in the file scalarmath.c.src. This file is actually a template with function definitions that are later replicated by the build system for several data types. Note each function is the \"kernel\" that is run for each element of the array (looping through the array is done somewhere else). The functions are always called <name of the type>_ctype_absolute, where <name of the type> is the data type it applies to and is generally templated. Let's go through them.\n/**begin repeat\n * #name = ubyte, ushort, uint, ulong, ulonglong#\n */\n\n#define @name@_ctype_absolute @name@_ctype_positive\n\n/**end repeat**/\n\nThis one is for unsigned types. In this case, the absolute value is the same as np.positive, which just copies the value without doing anything (it is what you get if you have an array a and you do +a).\n/**begin repeat\n * #name = byte, short, int, long, longlong#\n * #type = npy_byte, npy_short, npy_int, npy_long, npy_longlong#\n */\nstatic void\n@name@_ctype_absolute(@type@ a, @type@ *out)\n{\n    *out = (a < 0 ? -a : a);\n}\n/**end repeat**/\n\nThis one is for signed integers. Pretty straightforward.\n/**begin repeat\n * #name = float, double, longdouble#\n * #type = npy_float, npy_double, npy_longdouble#\n * #c = f,,l#\n */\nstatic void\n@name@_ctype_absolute(@type@ a, @type@ *out)\n{\n    *out = npy_fabs@c@(a);\n}\n/**end repeat**/\n\nThis is for floating-point values. Here npy_fabsf, npy_fabs and npy_fabsl functions are used. These are declared in npy_math.h, but defined through templated C code in npy_math_internal.h.src, essentially calling the C/C99 counterparts (unless C99 is not available, in which case fabsf and fabsl are emulated with fabs). You might think that the previous code should work as well for floating-point types, but actually these are more complicated, since they have things like NaN, infinity or signed zeros, so it is better to use the standard C functions that deal with everything reliably.\nstatic void\nhalf_ctype_absolute(npy_half a, npy_half *out)\n{\n    *out = a&0x7fffu;\n}\n\nThis is actually not templated, it is the absolute value function for half-precision floating-point values. Turns out you can change sign by just doing that bitwise operation (set the first bit to 0), since half-precision is simpler (if more limited) than other floating-point types (it's usually the same for those, but with special cases).\n/**begin repeat\n * #name = cfloat, cdouble, clongdouble#\n * #type = npy_cfloat, npy_cdouble, npy_clongdouble#\n * #rtype = npy_float, npy_double, npy_longdouble#\n * #c = f,,l#\n */\nstatic void\n@name@_ctype_absolute(@type@ a, @rtype@ *out)\n{\n    *out = npy_cabs@c@(a);\n}\n/**end repeat**/\n\nThis last one is for complex types. These use npy_cabsf, npycabs and npy_cabsl functions, again declared in npy_math.h but in this case template-implemented in npy_math_complex.c.src using C99 functions (unless that is not available, in which case it is emulated with np.hypot).\n", "Konwledge_Point": "应对NP完全问题", "Question": "numpy：np.abs到底是如何工作的？\n\n\n\nI'm trying to implement my own absolute function for gonum dense vectors in Go. I'm wandering if there's a better way of getting the absolute value of an array than squaring and then square rooting?\n\n\n\nMy main issue is that I've had to implement my own element wise Newtonian square-root function on these vectors and there's a balance between implementation speed and accuracy. If I could avoid using this square-root function I'd be happy.\n\n    ", "Tag": "算法分析"}
{"Answer": "运行下就知道了输出结果是 [0.5 2.5]\n\nimport numpy as np\n\nx=np.arange(4).reshape(2,2)\nprint(np.mean(x,axis=1))\n", "Konwledge_Point": "应对NP完全问题", "Question": "x=np.arange(4).reshape(2,2)，print(np.mean(x,axis=1))的输出结果是（）\nx=np.arange(4).reshape(2,2)，print(np.mean(x,axis=1))的输出结果是（）", "Tag": "算法分析"}
{"Answer": "将图片保存为png格式就可以了，jpg使用的一种失真压缩标准方法，多次上传下载jpg图片会逐渐失真，这也是为什么两次数据矩阵不一样的缘故，而png则是无损的压缩方式，不会出现上述情况", "Konwledge_Point": "应对NP完全问题", "Question": "numpy.array()读取图片不准确问题\n需求：\n将信息按像素存储在图片里，再完整地读出来（矩阵、图片互转）\n\n\n代码：\n\n\n    # 随机生成一个\n3\n*\n3\n的矩阵,保存到图片里\n    \ndata\n = np.random.randint(1, 255, (3, 3, 3), dtype='uint8')\n\n    print(\n\"image1: \"\n,\ndata\n)\n\n    im = \nImage\n.fromarray(\ndata\n)\n\n    im.save(\n\"picture1.jpg\"\n)\n\n    # 再读出来\n    im = \nImage\n.open(\n\"picture1.jpg\"\n)\n    \ndata\n = np.array(\nim\n)\n\n    print(\n\"image2: \"\n,\ndata\n)\n\n\n    # 为了直观对比，再写入新图片\n    im = \nImage\n.fromarray(\ndata\n)\n\n    im.save(\n\"picture2.jpg\"\n)\n\n\n\n结果：\n两次print的矩阵天差地别！\n两次的图片仔细看能发现差别\n\n\n其他尝试：\n\n\n将上述第一行代码换成：\n\n\n \ndata\n = np.full((\n3\n,\n3\n,\n3\n),\n255\n,dtype='uint8')\n \ndata2\n = np.zeros((\n3\n,\n3\n,\n3\n),dtype='uint8')\n \ndata\n = np.concatenate((data,data2),axis = \n1\n)\n\n\n\n结果如下：\n读取的值不准确\n\n\n将上述读取图片代码换成：\n\n\n \nim\n = Image.\nopen\n(\n\"picture1.jpg\"\n).convert(\n'RGB'\n)\n \nfor\n i in \nrange\n (\n0\n,\n3\n):\n     \nfor\n \nj\n in \nrange\n (\n0\n,\n3\n):\n         \nprint\n(\nim\n.getpixel((i, \nj\n)))\n\n\n\n同样不行\n\n\n将上述读取图片代码换成：\n\n\nim = cv2.cvt\nColor(\nnp\n.\narray\n(\nim\n)\n,cv2.COLOR_RGB2BGR)\n\n\n\n同样不行，结果跟直接np.array()一样，说明可能问题就出在np.array()上\n\n", "Tag": "算法分析"}
{"Answer": "第一个导入的是np，使用要加上np，比如要np.int32;第二个相当于from numpy import int32,double，float16(...),所以可以直接使用int32，float16等等", "Konwledge_Point": "应对NP完全问题", "Question": "为什么import numpy as np和from numpy import *在定义数组时会有差异呢\n\n\n为什么import numpy as np和from numpy import *在定义数组时会有差异呢。\nimport numpy as np 定义np.array时输入dtyp(dtype)不能自动补全。\n而from numpy import * 定义数组array(),就能自动补全\n有没有朋友指点一下", "Tag": "算法分析"}
{"Answer": "数组，", "Konwledge_Point": "应对NP完全问题", "Question": "np.mgrid[-1:1 : 2j][:, np.newaxis]运算结果是啥\nnp.mgrid[-1:1 : 2j][:, np.newaxis]的运算结果是啥 用到了python的numpy库", "Tag": "算法分析"}
{"Answer": "就在第一幅图头导入numpy模块，没错的。\r\n还是行不通的原因应该是：之前使用MyTransformer时，会在对应目录下生成一个对应的缓存文件，下次调用的时候会直接调用该缓存文件，\r\n所以你即使你改过源码，还是没用的，删除掉机器学习实践目录下MyTransformer模块的缓存文件就ok了。", "Konwledge_Point": "应对NP完全问题", "Question": "调用另一个ipynb中的函数，NameError: name 'np' is not defined\n在MyTransformer.ipynb中定义了一个Class\n\n在另外一个ipynb中导入该文件，并调用该Class中定义的transform函数报错\n\n第二个图片的ipynb在上面导入了 numpy\n\n\n\n求大神帮忙看看什么问题", "Tag": "算法分析"}
{"Answer": "\nI just found out that there is nothing like np.where() in gocv. \nAll I have to do is:\n1. get []byte using Mat.ToBytes()\n2. writing a for loop to check each pixel in []byte and changing it if meet the condition.\n3. get Mat from gocv.NewMatFromBytes()\n", "Konwledge_Point": "应对NP完全问题", "Question": "在gocv中是否有类似python中的np.where（）的类似功能？\n\n\n\nIs there any similar function in Gocv like np.where() in Python? I want to specify some specific pixel values to 0, and others to 255. As follows, in Python I can do:\n\n\n\n        img = cv2.imread(\"test.png\", cv2.IMREAD_GRAYSCALE)\n        img_ = np.where(img == 144 , img*0, np.where(img == 170 , img*0, np.where(img == 178 , img*0, np.where(img == 187 , img*0, 255))))\n\n\n\n\nthe pixel values which are 187, 178, 170, 144 will be set to 0, and others to 255. How can I do this job in Golang with Gocv?\n\n    ", "Tag": "算法分析"}
{"Answer": "删除线表达的是函数在未来的版本的中会被弃用,鼠标悬停会有对应的提示,会提示你目前推荐的方式", "Konwledge_Point": "应对NP完全问题", "Question": "np.matrix在Python中有取消线无法使用怎么办\nnp.matrix在Python中添加\nimport numpy as np\nimport xlrd的情况下依然显示有取消线无法使用怎么办", "Tag": "算法分析"}
{"Answer": "![图片说明](https://img-ask.csdn.net/upload/201908/05/1564983890_378193.png)\r\n-2147483648是32位系统里int类型的下界\r\nnp.nan是浮点数，而arange生成的array里是整数。\r\n应该是整型转成浮点型出问题了", "Konwledge_Point": "应对NP完全问题", "Question": "python新手 np.nan赋值后显示具体数字\n\n不知道怎么回事啊", "Tag": "算法分析"}
{"Answer": "升级sklearn和numpy 到新版本，经测试版本分别为0.24.2 和1.21.2运行正常，没有弃用提示信息。", "Konwledge_Point": "应对NP完全问题", "Question": "在一个py文件中导入一个算法库，然后运行np.array()就出现如下警告，怎么解决求解？（如果不导入算法库，只导入numpy就不会有警告）\nfrom sklearn\n.neighbors\n import NearestNeighbors\nimport numpy as np\n\nX = np\n.array\n(\n[[-1, -1]\n, \n[-2, -1]\n, \n[-3, -2]\n, \n[1, 1]\n, \n[2, 1]\n, \n[3, 2]\n])\n\nprint\n(X)\n\n\n\n\n运行结果：\nD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: \nnp.float\n is a deprecated alias for the builtin \nfloat\n. To silence this warning, use \nfloat\n by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use \nnp.float64\n here.\nDeprecated in NumPy 1.20; for more details and guidance: \n\n  \nNumPy 1.20.0 Release Notes — NumPy v1.22.dev0 Manual\n\n  \n\n  \n\n  \n\n    \n\n      \nhttps://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n\n    \n\n  \n  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\nD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: \nnp.float\n is a deprecated alias for the builtin \nfloat\n. To silence this warning, use \nfloat\n by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use \nnp.float64\n here.\nDeprecated in NumPy 1.20; for more details and guidance: \n\n  \nNumPy 1.20.0 Release Notes — NumPy v1.22.dev0 Manual\n\n  \n\n  \n\n  \n\n    \n\n      \nhttps://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n\n    \n\n  \n  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\nD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: \nnp.float\n is a deprecated alias for the builtin \nfloat\n. To silence this warning, use \nfloat\n by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use \nnp.float64\n here.\nDeprecated in NumPy 1.20; for more details and guidance: \n\n  \nNumPy 1.20.0 Release Notes — NumPy v1.22.dev0 Manual\n\n  \n\n  \n\n  \n\n    \n\n      \nhttps://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n\n    \n\n  \n  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\nD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: \nnp.float\n is a deprecated alias for the builtin \nfloat\n. To silence this warning, use \nfloat\n by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use \nnp.float64\n here.\nDeprecated in NumPy 1.20; for more details and guidance: \n\n  \nNumPy 1.20.0 Release Notes — NumPy v1.22.dev0 Manual\n\n  \n\n  \n\n  \n\n    \n\n      \nhttps://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n\n    \n\n  \n  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\nD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: \nnp.float\n is a deprecated alias for the builtin \nfloat\n. To silence this warning, use \nfloat\n by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use \nnp.float64\n here.\nDeprecated in NumPy 1.20; for more details and guidance: \n\n  \nNumPy 1.20.0 Release Notes — NumPy v1.22.dev0 Manual\n\n  \n\n  \n\n  \n\n    \n\n      \nhttps://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n\n    \n\n  \n  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\nD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: \nnp.float\n is a deprecated alias for the builtin \nfloat\n. To silence this warning, use \nfloat\n by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use \nnp.float64\n here.\nDeprecated in NumPy 1.20; for more details and guidance: \n\n  \nNumPy 1.20.0 Release Notes — NumPy v1.22.dev0 Manual\n\n  \n\n  \n\n  \n\n    \n\n      \nhttps://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n\n    \n\n  \n  eps=np.finfo(np.float).eps, positive=False):\nD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: \nnp.float\n is a deprecated alias for the builtin \nfloat\n. To silence this warning, use \nfloat\n by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use \nnp.float64\n here.\nDeprecated in NumPy 1.20; for more details and guidance: \n\n  \nNumPy 1.20.0 Release Notes — NumPy v1.22.dev0 Manual\n\n  \n\n  \n\n  \n\n    \n\n      \nhttps://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n\n    \n\n  \n  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\nD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: \nnp.float\n is a deprecated alias for the builtin \nfloat\n. To silence this warning, use \nfloat\n by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use \nnp.float64\n here.\nDeprecated in NumPy 1.20; for more details and guidance: \n\n  \nNumPy 1.20.0 Release Notes — NumPy v1.22.dev0 Manual\n\n  \n\n  \n\n  \n\n    \n\n      \nhttps://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n\n    \n\n  \n  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\nD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: \nnp.float\n is a deprecated alias for the builtin \nfloat\n. To silence this warning, use \nfloat\n by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use \nnp.float64\n here.\nDeprecated in NumPy 1.20; for more details and guidance: \n\n  \nNumPy 1.20.0 Release Notes — NumPy v1.22.dev0 Manual\n\n  \n\n  \n\n  \n\n    \n\n      \nhttps://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n\n    \n\n  \n  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\nD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:29: DeprecationWarning: \nnp.float\n is a deprecated alias for the builtin \nfloat\n. To silence this warning, use \nfloat\n by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use \nnp.float64\n here.\nDeprecated in NumPy 1.20; for more details and guidance: \n\n  \nNumPy 1.20.0 Release Notes — NumPy v1.22.dev0 Manual\n\n  \n\n  \n\n  \n\n    \n\n      \nhttps://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n\n    \n\n  \n\n\n\n\nEPS\n = np.finfo(np.float).eps\n\n[[-1 -1]\n\n \n[-2 -1]\n\n \n[-3 -2]\n\n \n[ 1  1]\n\n \n[ 2  1]\n\n \n[ 3  2]]\n\n\n\n\nProcess finished with exit code 0", "Tag": "算法分析"}
{"Answer": "multiply能这样接受三个参数么你这样写是不是只是前两个矩阵做了对应元素的相乘的操作", "Konwledge_Point": "应对NP完全问题", "Question": "python中np.dot与np.multiply 的使用问题\npython中np.dot与np.multiply 的使用问题\n三个矩阵相乘，两个是单位矩阵，另一个不是单位矩阵，但为什么得到的结果还是个单位矩阵？\n我试了试使用np.dot与np.multiply得到的都是这个结果", "Tag": "算法分析"}
{"Answer": "要你的电脑里面有这个包呐，numpy好像是拓展包，要你自己下的来着", "Konwledge_Point": "应对NP完全问题", "Question": "为什么我输入import numpy as np会报错？说电脑中不存在numpy 这个模块？\n", "Tag": "算法分析"}
{"Answer": "用photoshop打开png看下你的图像有没有alpha通道，也就是有没有透明图层。", "Konwledge_Point": "应对NP完全问题", "Question": "numpy.array()打开图片时像素点表示问题\n在应用numpy.array()打开图片\n\n代码如下：\n\n\n\ni = Image.open('images/numbers/0.1.png')\niar = np.array(i, dtype='int64')\nprint(iar)\n\n\n\n\n输出结果中大部分为\n\n[[[255 255 255 255]\n\n ......\n\n (即alpha数值有表示出来)\n\n 而另一张图片，可能就会输出\n\n[[[255 255 255]\n\n......\n\n(即alpha数值没有表示出来)\n\n想请教一下这是为什么呢，有没有什么办法可以把它们统一起来吗？", "Tag": "算法分析"}
{"Answer": "np.random.random((a,b))     （a,b）是形状   返回的是 a行 b列的array          每个元素的取值服从[0.0, 1.0)的均匀分布\r\n例如 \r\n\r\n\r\n```\r\nnp.random.random((3, 4))\r\narray([[ 0.08968149,  0.73049827,  0.90173847,  0.052132  ],\r\n             [ 0.89645737,  0.09354475,  0.85583081,  0.96655849],\r\n             [ 0.43413746,  0.12536754,  0.94566403,  0.07564477]])\r\n \r\n```\r\n\r\n\r\n\r\n\r\n[参考自](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.random.html \"\")", "Konwledge_Point": "应对NP完全问题", "Question": "python中的Random问题\nnp.random.random((layers[i-1]+1,layers[i]+1))其中layers是数组。求问这句话的语法是什么？没看到有\n\nrandom.random的这种函数形式哇？求解", "Tag": "算法分析"}
{"Answer": "题主你的报错原因是map函数的第二个参数应该是列表，而你传入的四列数据相当于是一个二维的dataframe，这样每列就相当于一个Series,而函数接受的参数应该是四个数字组成的列表，也就是一行数据，就是说你一次性传入所有的数据是不行的。必须一行一行的计算，每行数据计算得到一个距离，针对你的问题，我们可以使用pandas的apply函数每次传一行数据进入函数，具体可以将代码改成如下（题主你的sin cos sqrt函数都没有加np.这个我不知道是为什么，应该是要加的）：\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n\r\ndef haversine(df):   # 经度1，纬度1，经度2，纬度2\r\n    \"\"\"函数作用：计算两个经纬度之间的距离\"\"\"\r\n    # 将十进制度数转化为弧度\r\n\t\t# df.tolist()作用是将传入的一行数据转化为列表\r\n    lon1, lat1, lon2, lat2 = map(np.radians, df.tolist())\r\n    # haversine公式\r\n    dlon = lon2 - lon1\r\n    dlat = lat2 - lat1\r\n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\r\n    c = 2 * a * np.sin(np.sqrt(a))\r\n    r = 6371  # 地球平均半径，单位为公里\r\n    return c * r * 1000\r\n\r\n\r\ntest_data = pd.read_excel(\"test.xlsx\")\r\n# 这里的apply函数作用是将dataframe中每行的四个值传入函数haversine，然后计算距离，这样每行数据能返回一个距离\r\ntest_data[\"距离（米）\"] = test_data[['经度', '纬度', 'Longitude', 'Latitude']].apply(haversine, axis=1)\r\nprint(test_data)", "Konwledge_Point": "应对NP完全问题", "Question": "python pandas用函数算距离时报错\n在网上copy来的一个算经纬度的方法，但在实际操作运行到  lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2]) 的时候到报错：”TypeError: cannot convert the series to <class 'float'》，显示series不能转换为float，请问下各位大佬这个问题怎么解决  需要怎么修改脚本。\n\n\n\nimport numpy as np\nimport pandas as pd \n\ntest_data = pd.DataFrame(pd.read_excel(\"test.xlsx\"))\n\ndef haversine(lon1, lat1, lon2, lat2):   # 经度1，纬度1，经度2，纬度2 \n  \"\"\"函数作用：计算两个经纬度之间的距离\"\"\"\n  # 将十进制度数转化为弧度\n  lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n  # haversine公式\n  dlon = lon2 - lon1 \n  dlat = lat2 - lat1 \n  a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n  c = 2 * asin(sqrt(a)) \n  r = 6371 # 地球平均半径，单位为公里\n  return c * r * 1000\n\ntest_data[\"距离（米）\"] = test_data.eval('@haversine(经度,纬度,Longitude,Latitude)')\n\n\n\"\"\"\n表内容举例：\nname    经度  纬度  Longitude   Latitude\nA   113.545761  23.106667   113.09461   23.39367\nB   113.545761  23.106667   113.173833  23.336054\nC   113.545761  23.106667   113.1799    23.43946\nD   113.545761  23.106667   113.1975278 23.38591667\nE   113.545761  23.106667   113.1975278 23.38591667\nF   113.545761  23.106667   113.219664  23.100377\nG   113.545761  23.106667   113.219664  23.100377\nH   113.545761  23.106667   113.22253   23.18483\nI   113.545761  23.106667   113.22253   23.18483\n\n\"\"\"\n\n\n\n\n程序运行到", "Tag": "算法分析"}
{"Answer": "\n>>> import numpy as np\n>>> x = np.array([1,2,3,4,5,6])\n>>> r = np.random.choice(np.arange(x.size), 3, replace=False) # 从x中随机无重复地抽取3个数\n>>> r # 这次选中的是0，2，4\narray([0, 2, 4])\n>>> x[r] += 1 # 选中的数字加1\n>>> x\narray([2, 2, 4, 4, 6, 6])\n ", "Konwledge_Point": "应对NP完全问题", "Question": "如何让一个np数组中随机几个数字+1\n比如我现在有[1,2,3,4,5,6]这个数组，\n\n\n\n如何将该数组中随机n个数字都+1，\n\n\n\n比如将原有数组变成[2,2,4,4,5,6],\n\n\n\n这里面就随机到了第0第2个数加一了。", "Tag": "算法分析"}
{"Answer": "如有帮助请给个采纳！\n>>> import numpy as np\n>>> np.linspace(1, 9, num=10)\narray([1.        , 1.88888889, 2.77777778, 3.66666667, 4.55555556,\n       5.44444444, 6.33333333, 7.22222222, 8.11111111, 9.        ])\n", "Konwledge_Point": "应对NP完全问题", "Question": "使用np.linspace函数在1-9区间内生成10个包含9的等间隔数，不显示间隔\n使用np.linspace函数在1-9区间内生成10个包含9的等间隔数，不显示间隔", "Tag": "算法分析"}
{"Answer": "labels=np.empty((0,5))", "Konwledge_Point": "应对NP完全问题", "Question": "python中numpy格式数组维度\n##python中numpy格式数组维度问题\n\n\n\nlabels = np.empty((0,4))\nxml_names = os.listdir(path)\nfor xml_name in xml_names:\n        image_path, label = parse_xml(os.path.join(path, xml_name))\n        print(label)\n        label = np.array(label)\n        print(label)\n        labels = np.append(labels, label, axis=0)\n\n\n\n\nlabel是这个格式：\n\n\n\n[[128 230 188 328   0]\n\n\n\n[297 199 366 299   0]]\n\n\n\n然后和labels   append的时候报错，提示维度不匹配，请问为什么", "Tag": "算法分析"}
{"Answer": "import numpy as np\nn = int(input())\nx = np.linspace(0, 2*np.pi, n)\ny = np.sin(x)\nprint(y)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python 利用numpy求sin函数值\n\n\n我的解答\nimport numpy as np\n\n\nx = np.linspace(0, 2*np.pi, 10)\n\n\n对数组x中的每个元素进行正弦计算，返回一个同样大小的新数组\n\n\ny = np.sin(x)\nprint(y\n不正确怎么改", "Tag": "算法分析"}
{"Answer": "这个是取倒数第7个数到倒数第一个数，所以输出这个", "Konwledge_Point": "应对NP完全问题", "Question": "a=np.arange(12)  a[-7:-1]\na=np.arange(12)\n\n\na[-7:-1]\n这个结果是多少", "Tag": "算法分析"}
{"Answer": "运行下就知道了输出结果是[7 5 3]\nimport numpy as np\n\na=np.arange(1,10)\nprint(a[2:8:2][::-1])\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "a=np.arange(1,10)，print(a[2:8:2][::-1])输出结果是（）\na=np.arange(1,10)，print(a[2:8:2][::-1])输出结果是多少？[7，5，3]\n为啥呢？为啥能输出三个数？", "Tag": "算法分析"}
{"Answer": "\nPandas-df.apply() - 知乎\n批量操作：df.apply()关于可以在数据表上进行批量操作的函数： （1）有些函数是元素级别的操作，比如求平方 np.square()，针对的是每个元素。有些函数则是对元素集合级别的操作，这里元素集合指的是以列为单位，或…\n\n\n\nhttps://zhuanlan.zhihu.com/p/148743842\n\ndf.apply参数规定只写函数名即可，调用的函数参数写在后面有帮助望采纳", "Konwledge_Point": "应对NP完全问题", "Question": "为什么np.sum后没有加（）？\n", "Tag": "算法分析"}
{"Answer": "  [7,5,3]  1  5s['toyota', 'subaru', 'bmw', 'audi']['bmw', 'audi', 'toyota', 'subaru']\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python函数问题，人工智能问题，请给出解答过程\n1、a=np.arange(1,10)，print(a[2:8:2][::-1])输出结果是（）\n\n\n2、np.exp(x).round(5)的结果是2.71828，x的值是（）\n\n\n3、为了充分发挥GPU算力，需要尽可能多的将任务交给GPU执行。现在有一个任务数组T=[1,2,3,4,5]，数组元素表示在这1秒内新增的任务个数，且每秒都有新增任务，直到数结束。假设GPU最多一次执行N=3个任务，一次执行耗时1秒，在保证GPU尽量不空闲的情况下，一个GPU最少需要多长时间执行完成？\n\n\n4、在Python中，令cars=['bmw','audi','toyota','subaru']\n运行cars.sort(reverse=True)后，cars的值为（）；运行sorted(cars,reverse=True)后，cars的值为（）\n\n\n请给出解答过程！", "Tag": "算法分析"}
{"Answer": "你说的是大数因式分解？大质数的判定是NP Hard问题。这个是现代密钥算法体系的基石。", "Konwledge_Point": "应对NP完全问题", "Question": "大数分解问题是NP问题吗？\n现在对于大数分解问题是否是NP问题，非专业人员依据NP问题的定义回答是。资深人员在小心地寻找公式确定性的证明。请问这种证明己经有了吗？", "Tag": "算法分析"}
{"Answer": "numpy.ndarray 没有append这个方法\r\n试试这个命令\r\nprint(np.concatenate((a,b),axis = 0))", "Konwledge_Point": "应对NP完全问题", "Question": "想把两个13维数组合并成一个26维数组，用np.append之后报错，用extend和append好像也不行，为什么？\nExpected binary or unicode string, got tf.Tensor 'Relu_30:0' shape=(1, 13) dtype=float32\n\n\n\n我是在用tensorflow把两个网络各自生成的13维数组合并之后输入到另一个神经网络里面时遇到的问题，大家都是怎么解决的？\n\n\n\nWeights0 = tf.Variable(tf.random_normal([12, 13]))\n\nbiases0 = tf.Variable(tf.zeros([1, 13]) + 0.1)\n\nWx_plus_b0 = tf.matmul(xs0, Weights0) + biases0\n\nl0 = tf.nn.relu(Wx_plus_b0)\n\n\n\nWeights1 = tf.Variable(tf.random_normal([12, 13]))\n\nbiases1 = tf.Variable(tf.zeros([1, 13]) + 0.1)\n\nWx_plus_b1 = tf.matmul(xs1, Weights1) + biases1\n\nl1 = tf.nn.relu(Wx_plus_b1)\n\n\n\nL1 = np.append(l0,l1)\n\n\n\n隐层2\n\n\n\nWeights2 = tf.Variable(tf.random_normal([26, 10]))\n\nbiases2 = tf.Variable(tf.zeros([1, 10]) + 0.1)\n\nWx_plus_b2 = tf.matmul(L1, Weights2) + biases2\n\nl2 = tf.nn.relu(Wx_plus_b2)", "Tag": "算法分析"}
{"Answer": "你好,这里a是rehshape成了(2,3)的形式，使用a[i]应该是不行的，可以调试看看indix的结构是什么，然后进行调整", "Konwledge_Point": "应对NP完全问题", "Question": "numpy的argwhere,索引的使用错误\n刚学习玩numpy的argwhere的用法,根据字面意思返回的应当是符合条件的索引即\n\n\na\n = np\n.arange\n(\n6\n)\n.reshape\n(\n2\n,\n3\n)\nindix = np\n.argwhere\n(a>\n3\n)\n\n#输出的应该是\n[1,1]\n,\n[1,2]\n\n\n\n\n问题是在已知索引的情况下如何根据索引来寻找到对应的元素\n\n\nimport numpy as np\n\n\na\n = np\n.arange\n(\n6\n)\n.reshape\n(\n2\n,\n3\n)\nindix = np\n.argwhere\n(a>\n3\n)\n\nfor\n \ni\n \nin\n indix:\n    print(i)\n    print(\na\n[i]\n)\n\n\n\n\n结果报错，正常情况下应当如何根据索引寻找元素？", "Tag": "算法分析"}
{"Answer": "return最为常见，十分简单，其返回运行一次函数所得到的结果，且在return语句之后python解释器会直接跳出函数，函数剩下的其它所有语句或者循环都不再被运行。yield是返回一个生成器(generator)对象。python中的生成器，可以使用next()来逐个获取yield返回的值。同时运行机制为在运行包含有生成器的函数的时候，只要碰到yield就暂停，这时候会保存当前运行的信息，也就是之前所产生的变量等都不会改变。yileld和return的区别就是一个函数不在运行，一个函数暂停运行，用next()方法继续运行，看自己的需求在上面的代码中，换为return，返回的是遍历完以后的整个运行结果", "Konwledge_Point": "应对NP完全问题", "Question": "yield 实现什么功能？\n请问一下，这里的yied实现什么功能呢？     两个yield np.array(imgs_list), np.array(labels_list)都返回到哪里的呢？\n可以不用yield么？\n\n\n# 定义数据生成器，返回批次数据\ndef data_generator():\n    imgs_list = []\n    labels_list = []\n    \nfor\n i \nin\n index_list:\n        # 将数据处理成希望的类型\n        img = \nnp\n.\narray\n(imgs[i]).astype('float32')\n        \nlabel\n = \nnp\n.\narray\n(\nlabels\n[i]).astype('float32')\n        imgs_list.\nappend\n(img) \n        labels_list.\nappend\n(\nlabel\n)\n        \nif\n len(imgs_list) == BATCHSIZE:\n            # 获得一个batchsize的数据，并返回\n            yield \nnp\n.\narray\n(imgs_list), \nnp\n.\narray\n(labels_list)\n            # 清空数据读取列表\n            imgs_list = []\n            labels_list = []\n\n    # 如果剩余数据的数目小于BATCHSIZE，\n    # 则剩余数据一起构成一个大小为len(imgs_list)的mini-\nbatch\n\n    \nif\n len(imgs_list) > \n0\n:\n        yield \nnp\n.\narray\n(imgs_list), \nnp\n.\narray\n(labels_list)\n    \nreturn\n data_generator\n\n", "Tag": "算法分析"}
{"Answer": "有一种pandas的方法可以实现（你问的应该是rank）,需要先转换为dataframe，再用rank解决：arr=np.array(pd.DataFrame(array).rank(axis=0,method='min'))这是从1-n的排序，如果想从0开始，后面直接减去一即可目前我还没有搜查到numpy有直接的方法，需要再查一下", "Konwledge_Point": "应对NP完全问题", "Question": "使用numpy的argsort函数对二维数组按行（列）排序，返回排序索引时出错\n问题遇到的现象和发生背景\n\n\n版本：numpy    1.22.3    。使用numpy的argsort函数对二维数组按行（列）排序，返回排序索引时出错\n\n\n问题相关代码，请勿粘贴截图\n\n\n>>> array = \n[[0, 1, 2, 3, 4, 5], \n         [444, 4, 8, 3, 1, 10], \n         [2, 5, 8, 999, 1, 4]]\n\n>>> np.argsort(array, axis=\n0\n)\narray(\n[[0, 0, 0, 0, 1, 2],\n       [2, 1, 1, 1, 2, 0],\n       [1, 2, 2, 2, 0, 1]]\n, dtype=int64)\n\n\n\n运行结果及报错内容\n\n\n\n\n没有报错，但是，第五、六列按列排序的索引有很明显的问题\n\n\n我的解答思路和尝试过的方法\n\n\n我的思路，以第六列为例：\n\n\n\n\n我尝试过转置之后一行一行地执行再拼接，但仍然有问题\n\n\n我想要达到的结果\n\n\n返回正确的排序索引", "Tag": "算法分析"}
{"Answer": "np.append(b81[i]['标题'])  这个np是什么？ 不会是numpy吧？ ", "Konwledge_Point": "应对NP完全问题", "Question": "列表添加项时出错：TypeError: _append_dispatcher() missing 1 required positional argument: 'values'\n代码：\nnb = \n[\n'离开'\n]\n\nb81 = c寻找邮件({\n'发件人'\n:\n'老吴'\n})\n\nprint\n(b81[\n0\n][\n'标题'\n])\n\n\ni\n = \n0\n\n\nfor\n \ni\n \nin\n range(len(b81)):\n    np\n.append\n(b81\n[i]\n[\n'标题'\n]\n)\n    a1\n.buttonbox\n(\n'我的邮箱'\n,np)\n\n\n\n\n报错：\nnp.\nappend\n(b81[i][\n'标题'\n])\n  File \n\"<__array_function__ internals>\"\n, \nline\n \n4\n, in \nappend\n\nTypeError: _append_dispatcher() missing \n1\n required positional \nargument\n: \n'values'\n\n\n", "Tag": "算法分析"}
{"Answer": "倒数第二行 im2=Image.fromarray(a2,np.astype(\"uint8\"))\nfromarray后面指的是Mode参数，比如'rgb'", "Konwledge_Point": "应对NP完全问题", "Question": "module 'numpy' has no attribute 'astype'\n源代码：\n\n\n\n\n\nfrom PIL import Image\nimport numpy as np\nvec_el=np.pi/2.2\nvec_az=np.pi/4\ndepth=10\nim=Image.open(\"D:/视频源/graphics/python/aniya.jpg\").convert(\"L\")\na=np.asarray(im).astype(\"float\")\ngrad=np.gradient(a)\ngrad_x,grad_y=grad\ngrad_x=grad_x*depth/100\ngrad_y=grad_y*depth/100\ndx=np.cos(vec_el)*np.cos(vec_az)\ndy=np.cos(vec_el)*np.sin(vec_az)\ndz=np.sin(vec_el)\nA=np.sqrt(grad_x**2+grad_y**2+1)\nuni_x=grad_x/A\nuni_y=grad_y/A\nuni_z=1/A\na2=255*(dx*uni_x+dy*uni_y+dz*uni_z)\na2=a2.clip(0,255)\nim2=Image.fromarray(a2,np.astype(\"uint8\"))\nim2.save(\"draw_aniya.jpg\")\n\n\n\n ", "Tag": "算法分析"}
{"Answer": "delimiter 参数\n一旦文件被定义并打开阅读，genfromtxt将每个非空行拆分为一个字符串序列。刚刚跳过空行或注释行。delimiter关键字用于定义拆分应如何进行。\n通常，单个字符标记列之间的间隔。例如，逗号分隔文件（CSV）使用逗号（,）或分号（;）作为分隔符：Numpy 切片操作,将从第一行（在skip_header之后）读取名称.\n", "Konwledge_Point": "应对NP完全问题", "Question": "python中np.genfromtxt的使用小问题\n\nsnapshots = [\n    np.genfromtxt(\n'data/velocity0.{}.csv'\n.format(i), \ndelimiter\n=\n','\n, \nskip_header\n=1)[:, 0]\n    \nfor\n i \nin\n range(20, 40)\n]\n\npts = np.genfromtxt(\n'data/velocity0.20.csv'\n, \ndelimiter\n=\n','\n, \nskip_header\n=1)[:, -3:-1]\n\n\n\n这段代码中的[:, 0]与[:, -3:-1]代表什么意思呀", "Tag": "算法分析"}
{"Answer": "官方给的说法是np.ones是生成一个矩阵，所有值初始化为1，np.empty一般大家都说是生成一个空矩阵。但是要注意这里的空不是咱们生活中理解的什么都没有的空，是不进行初始化，原来这块内存是什么就是什么，不进行任何处理，理论上可以是任何值。（当然这种操作是不应该的）\n比如我在np.zero后面调用他就全是0\n但是我直接调用empty就是随机数值\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "为什么我的np.empty输出和np.ones一样啊？\n为什么我的np.empty输出和np.ones一样啊？按理说np.empty不应该不赋值吗？", "Tag": "算法分析"}
{"Answer": "一般情况下reset()就是重新初始化环境，除非你需要获取初始化时产生的某些参数，那就要有返回值，如果仅仅是初始化的话，完全可以不设返回值，你这4个随机数就更没必要了", "Konwledge_Point": "应对NP完全问题", "Question": "强化学习，gym.reset（）重置环境为什么不是返回一组为0 的数据，而是返回一定范围的数组？\n在学习强化学习，为什么强化学习的gym.reset() 返回的是一个不为零的数组，我理解的重置不就是归零吗？ 比如\nCartPole-v0 环境。为什么def reset()那儿要返回4个-0.05到0.05的随机数呢？\ndef reset(\n        self,\n        *,\n        seed: Optional[int] = None,\n        return_info: bool = False,\n        options: Optional[dict] = None,\n    ):\n        super().reset(seed=seed)\n**        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n**        self.steps_beyond_done = None\n        if not return_info:\n            return np.array(self.state, dtype=np.float32)\n        else:\n            return np.array(self.state, dtype=np.float32), {}\n\n\n\"\"\"\nClassic cart-pole system implemented by Rich Sutton et al.\nCopied from http://incompleteideas.net/sutton/book/code/pole.c\npermalink: https://perma.cc/C9ZM-652R\n\"\"\"\n\n\nimport\n math\n\nfrom\n typing \nimport\n \nOptional\n, \nUnion\n\n\n\nimport\n numpy \nas\n np\n\n\nimport\n gym\n\nfrom\n gym \nimport\n logger, spaces\n\nfrom\n gym.error \nimport\n DependencyNotInstalled\n\n\n\nclass\n \nCartPoleEnv\n(gym.Env[np.ndarray, \nUnion\n[\nint\n, np.ndarray]]):\n    \n\"\"\"\n    ### Description\n\n    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n     in the left and right direction on the cart.\n\n    ### Action Space\n\n    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n     of the fixed force the cart is pushed with.\n\n    | Num | Action                 |\n    |-----|------------------------|\n    | 0   | Push cart to the left  |\n    | 1   | Push cart to the right |\n\n    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n\n    ### Observation Space\n\n    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n\n    | Num | Observation           | Min                 | Max               |\n    |-----|-----------------------|---------------------|-------------------|\n    | 0   | Cart Position         | -4.8                | 4.8               |\n    | 1   | Cart Velocity         | -Inf                | Inf               |\n    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n\n    **Note:** While the ranges above denote the possible values for observation space of each element,\n        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n       if the cart leaves the `(-2.4, 2.4)` range.\n    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n\n    ### Rewards\n\n    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n    including the termination step, is allotted. The threshold for rewards is 475 for v1.\n\n    ### Starting State\n\n    All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n\n    ### Episode Termination\n\n    The episode terminates if any one of the following occurs:\n    1. Pole Angle is greater than ±12°\n    2. Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n    3. Episode length is greater than 500 (200 for v0)\n\n    ### Arguments\n\n    ```\n    gym.make('CartPole-v1')\n    ```\n\n    No additional arguments are currently supported.\n    \"\"\"\n\n\n    metadata = {\n\"render_modes\"\n: [\n\"human\"\n, \n\"rgb_array\"\n], \n\"render_fps\"\n: \n50\n}\n\n    \ndef\n \n__init__\n(\nself\n):\n        self.gravity = \n9.8\n\n        self.masscart = \n1.0\n\n        self.masspole = \n0.1\n\n        self.total_mass = self.masspole + self.masscart\n        self.length = \n0.5\n  \n# actually half the pole's length\n\n        self.polemass_length = self.masspole * self.length\n        self.force_mag = \n10.0\n\n        self.tau = \n0.02\n  \n# seconds between state updates\n\n        self.kinematics_integrator = \n\"euler\"\n\n\n        \n# Angle at which to fail the episode\n\n        self.theta_threshold_radians = \n12\n * \n2\n * math.pi / \n360\n\n        self.x_threshold = \n2.4\n\n\n        \n# Angle limit set to 2 * theta_threshold_radians so failing observation\n\n        \n# is still within bounds.\n\n        high = np.array(\n            [\n                self.x_threshold * \n2\n,\n                np.finfo(np.float32).\nmax\n,\n                self.theta_threshold_radians * \n2\n,\n                np.finfo(np.float32).\nmax\n,\n            ],\n            dtype=np.float32,\n        )\n\n        self.action_space = spaces.Discrete(\n2\n)\n        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n\n        self.screen = \nNone\n\n        self.clock = \nNone\n\n        self.isopen = \nTrue\n\n        self.state = \nNone\n\n\n        self.steps_beyond_done = \nNone\n\n\n    \ndef\n \nstep\n(\nself, action\n):\n        err_msg = \nf\"\n{action!r}\n (\n{\ntype\n(action)}\n) invalid\"\n\n        \nassert\n self.action_space.contains(action), err_msg\n        \nassert\n self.state \nis\n \nnot\n \nNone\n, \n\"Call reset before using step method.\"\n\n        x, x_dot, theta, theta_dot = self.state\n        force = self.force_mag \nif\n action == \n1\n \nelse\n -self.force_mag\n        costheta = math.cos(theta)\n        sintheta = math.sin(theta)\n\n        \n# For the interested reader:\n\n        \n# https://coneural.org/florian/papers/05_cart_pole.pdf\n\n        temp = (\n            force + self.polemass_length * theta_dot**\n2\n * sintheta\n        ) / self.total_mass\n        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n            self.length * (\n4.0\n / \n3.0\n - self.masspole * costheta**\n2\n / self.total_mass)\n        )\n        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n\n        \nif\n self.kinematics_integrator == \n\"euler\"\n:\n            x = x + self.tau * x_dot\n            x_dot = x_dot + self.tau * xacc\n            theta = theta + self.tau * theta_dot\n            theta_dot = theta_dot + self.tau * thetaacc\n        \nelse\n:  \n# semi-implicit euler\n\n            x_dot = x_dot + self.tau * xacc\n            x = x + self.tau * x_dot\n            theta_dot = theta_dot + self.tau * thetaacc\n            theta = theta + self.tau * theta_dot\n\n        self.state = (x, x_dot, theta, theta_dot)\n\n        done = \nbool\n(\n            x < -self.x_threshold\n            \nor\n x > self.x_threshold\n            \nor\n theta < -self.theta_threshold_radians\n            \nor\n theta > self.theta_threshold_radians\n        )\n\n        \nif\n \nnot\n done:\n            reward = \n1.0\n\n        \nelif\n self.steps_beyond_done \nis\n \nNone\n:\n            \n# Pole just fell!\n\n            self.steps_beyond_done = \n0\n\n            reward = \n1.0\n\n        \nelse\n:\n            \nif\n self.steps_beyond_done == \n0\n:\n                logger.warn(\n                    \n\"You are calling 'step()' even though this \"\n\n                    \n\"environment has already returned done = True. You \"\n\n                    \n\"should always call 'reset()' once you receive 'done = \"\n\n                    \n\"True' -- any further steps are undefined behavior.\"\n\n                )\n            self.steps_beyond_done += \n1\n\n            reward = \n0.0\n\n\n        \nreturn\n np.array(self.state, dtype=np.float32), reward, done, {}\n\n    \ndef\n \nreset\n(\n\n        self,\n        *,\n        seed: \nOptional\n[\nint\n] = \nNone\n,\n        return_info: \nbool\n = \nFalse\n,\n        options: \nOptional\n[\ndict\n] = \nNone\n,\n    \n):\n        \nsuper\n().reset(seed=seed)\n        self.state = self.np_random.uniform(low=-\n0.05\n, high=\n0.05\n, size=(\n4\n,))\n        self.steps_beyond_done = \nNone\n\n        \nif\n \nnot\n return_info:\n            \nreturn\n np.array(self.state, dtype=np.float32)\n        \nelse\n:\n            \nreturn\n np.array(self.state, dtype=np.float32), {}\n\n    \ndef\n \nrender\n(\nself, mode=\n\"human\"\n):\n        \ntry\n:\n            \nimport\n pygame\n            \nfrom\n pygame \nimport\n gfxdraw\n        \nexcept\n ImportError:\n            \nraise\n DependencyNotInstalled(\n                \n\"pygame is not installed, run `pip install gym[classic_control]`\"\n\n            )\n\n        screen_width = \n600\n\n        screen_height = \n400\n\n\n        world_width = self.x_threshold * \n2\n\n        scale = screen_width / world_width\n        polewidth = \n10.0\n\n        polelen = scale * (\n2\n * self.length)\n        cartwidth = \n50.0\n\n        cartheight = \n30.0\n\n\n        \nif\n self.state \nis\n \nNone\n:\n            \nreturn\n \nNone\n\n\n        x = self.state\n\n        \nif\n self.screen \nis\n \nNone\n:\n            pygame.init()\n            pygame.display.init()\n            self.screen = pygame.display.set_mode((screen_width, screen_height))\n        \nif\n self.clock \nis\n \nNone\n:\n            self.clock = pygame.time.Clock()\n\n        self.surf = pygame.Surface((screen_width, screen_height))\n        self.surf.fill((\n255\n, \n255\n, \n255\n))\n\n        l, r, t, b = -cartwidth / \n2\n, cartwidth / \n2\n, cartheight / \n2\n, -cartheight / \n2\n\n        axleoffset = cartheight / \n4.0\n\n        cartx = x[\n0\n] * scale + screen_width / \n2.0\n  \n# MIDDLE OF CART\n\n        carty = \n100\n  \n# TOP OF CART\n\n        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n        cart_coords = [(c[\n0\n] + cartx, c[\n1\n] + carty) \nfor\n c \nin\n cart_coords]\n        gfxdraw.aapolygon(self.surf, cart_coords, (\n0\n, \n0\n, \n0\n))\n        gfxdraw.filled_polygon(self.surf, cart_coords, (\n0\n, \n0\n, \n0\n))\n\n        l, r, t, b = (\n            -polewidth / \n2\n,\n            polewidth / \n2\n,\n            polelen - polewidth / \n2\n,\n            -polewidth / \n2\n,\n        )\n\n        pole_coords = []\n        \nfor\n coord \nin\n [(l, b), (l, t), (r, t), (r, b)]:\n            coord = pygame.math.Vector2(coord).rotate_rad(-x[\n2\n])\n            coord = (coord[\n0\n] + cartx, coord[\n1\n] + carty + axleoffset)\n            pole_coords.append(coord)\n        gfxdraw.aapolygon(self.surf, pole_coords, (\n202\n, \n152\n, \n101\n))\n        gfxdraw.filled_polygon(self.surf, pole_coords, (\n202\n, \n152\n, \n101\n))\n\n        gfxdraw.aacircle(\n            self.surf,\n            \nint\n(cartx),\n            \nint\n(carty + axleoffset),\n            \nint\n(polewidth / \n2\n),\n            (\n129\n, \n132\n, \n203\n),\n        )\n        gfxdraw.filled_circle(\n            self.surf,\n            \nint\n(cartx),\n            \nint\n(carty + axleoffset),\n            \nint\n(polewidth / \n2\n),\n            (\n129\n, \n132\n, \n203\n),\n        )\n\n        gfxdraw.hline(self.surf, \n0\n, screen_width, carty, (\n0\n, \n0\n, \n0\n))\n\n        self.surf = pygame.transform.flip(self.surf, \nFalse\n, \nTrue\n)\n        self.screen.blit(self.surf, (\n0\n, \n0\n))\n        \nif\n mode == \n\"human\"\n:\n            pygame.event.pump()\n            self.clock.tick(self.metadata[\n\"render_fps\"\n])\n            pygame.display.flip()\n\n        \nif\n mode == \n\"rgb_array\"\n:\n            \nreturn\n np.transpose(\n                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(\n1\n, \n0\n, \n2\n)\n            )\n        \nelse\n:\n            \nreturn\n self.isopen\n\n    \ndef\n \nclose\n(\nself\n):\n        \nif\n self.screen \nis\n \nnot\n \nNone\n:\n            \nimport\n pygame\n\n            pygame.display.quit()\n            pygame.quit()\n            self.isopen = \nFalse\n\n\n\n", "Tag": "算法分析"}
{"Answer": "![图片说明](https://img-ask.csdn.net/upload/201807/30/1532881307_117357.gif)", "Konwledge_Point": "应对NP完全问题", "Question": "用random.randint()方法定义一个整数型随机二维矩阵，返回布尔值。\n用np.random.randint()方法定义一个整数型随机二维矩阵，并判断其中有没有一整列数为0，返回布尔值,写成一个函数，求大神指点。\n\n用python写，谢谢。", "Tag": "算法分析"}
{"Answer": "可能是内存速度瓶颈，也可能是其他因素导致的。在多核并行计算中，除了CPU核心数和内存大小外，还有很多因素会影响计算速度，比如硬盘速度、缓存大小、算法复杂度等等。因此，要想准确分析速度瓶颈，需要对整个系统进行综合评估，包括硬件和软件方面。\n另外，从1核到10核速度增加不是成倍增加，这也是正常现象。多核并行计算的速度增加并不是线性的，随着核心数的增加，计算效率会逐渐降低，因为多核并行计算需要更多的协调和同步，而这些操作也需要消耗一定的时间和资源。\n总之，要想提高多核并行计算的速度，需要从多个方面入手，包括优化算法、提高硬件性能、合理调整系统参数等等。", "Konwledge_Point": "应对NP完全问题", "Question": "python多进程与np数组与talib计算金融指标问题\ncpu24核心笔记本电脑2个内存条16g 3200m x2，用1核运行速度是50+每秒it。用10个应该是500+。但是只有170+，我尝试从1核到10核测试，发现到3核后就不是成倍增加了，后面增加核心速度变快的很少。是不是内存速度瓶颈。\n\n\n with Pool(\nprocesses\n=workers) as pool:  #\n            # 使用并行批量获得\n            results = pool.imap_unordered(\n                # 先测试单个核心一秒多少个，用于设置chunksize参数\n                partial(calculate_by_one_loop, \nduo_lock\n=duo_lock), para_list, \nchunksize\n=50)\n            # 显示进度条\n            \nfor\n i, result \nin\n tqdm(enumerate(results),\n                                  \ntotal\n=total, \ndesc\n=\n'总任务进度'\n, \nmininterval\n=1, \nmaxinterval\n=1):\n                df_list.append(result)\n\n\n\n\n其中calculate_by_one_loop是目标函数，里面就执行np数组的计算\n\n\nx = talib.SMA(close, timeperiod=\n10\n)\n\n\n@jit(\n'float64[:,:](float64[:], float64[:], float64[:], float64[:])'\n)\n\n\ndef\n \nnumpy_chaoji_qushi\n(\natr, close, high, low\n):\n    \n\"\"\"\n    超级趋势计算\n\n    :param atr: 真实波动幅度\n    :param close: 收盘\n    :param high: 最高\n    :param low: 最低\n    :return: 超级趋势\n    \"\"\"\n\n\n    src = np.full_like(close, np.nan, dtype=\n'float64'\n)  \n# 生成空白的数组，结构和close一样\n\n    up = np.full_like(close, np.nan, dtype=\n'float64'\n)  \n# 生成空白的数组，结构和close一样\n\n\n    dn = np.full_like(close, np.nan, dtype=\n'float64'\n)  \n# 生成空白的数组，结构和close一样\n\n\n    \nfor\n i \nin\n \nrange\n(close.shape[\n0\n]):\n        \n# 价格源hlc3\n\n        src[i] = (close[i] + high[i] + low[i]) / \n3\n\n        \n# 计算超级趋势指标的上涨趋势线\n\n        up[i] = src[i] - atr[i]\n        \n# 过去的收盘价 大于 up[1]\n\n        \nif\n close[i - \n1\n] > up[i - \n1\n]:\n            \n# up就等于 up 与up1 中最大的\n\n            up[i] = \nmax\n(up[i], up[i - \n1\n])\n        \nelse\n:  \n# 不是就返回up自己\n\n            up[i] = up[i]\n        \n# 计算超级趋势指标的下跌趋势线\n\n        dn[i] = src[i] + atr[i]\n        \n# 过去的收盘价 大于 up[1]\n\n        \nif\n close[i - \n1\n] < dn[i - \n1\n]:\n            \n# up就等于 up 与up1 中最大的\n\n            dn[i] = \nmin\n(dn[i], dn[i - \n1\n])\n        \nelse\n:  \n# 不是就返回up自己\n\n            dn[i] = dn[i]\n\n    \nreturn\n np.vstack((up, dn))\n\n\n\n很多这种使用talib的代码，基本用5列数组会生成几十上百列。再循环判断逻辑。我测试过单次计算50多ms\n之前我是使用pandas的df\n例如：df['x'] = talib.SMA(df['close'], timeperiod=10)\n后面改成np数组，没使用一点df。速度提升了一点点。我重新写了一个只计算数学问题的程序，速度是随着进程数成倍增加的。我20核全开速度就比1核快将近20倍。所以我怀疑内存速度，但是我不知道怎么优化，已经从df数据改成np数组了。每计算一个金融指标（均线这些）就有一个单独的变量存起来。我看别人用服务器cpu和主板和内存，开几十上百个线程，速度飞快\n帮我优化一下", "Tag": "算法分析"}
{"Answer": "反向传播部分计算公式是对的\n吴恩达深度学习第二次作业_牛客博客\n带有一个隐藏层的平面数据分类 解释一下，就是一个二维平面有两种不同的花色，我们通过花色将这个平面进行划分 预备知识 我们先来看一下我们需要的包 import numpy as npimport ma\n\n\n\nhttps://blog.nowcoder.net/n/c4635a8beeb84e7e91986b8c0da7a313?from=nowcoder_improve\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "Python BP神经网络两分类 反向传播代码问题\n问题遇到的现象和发生背景\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import radviz\n'''\n    构建一个具有1个隐藏层的神经网络，隐层的大小为10\n    输入层为4个特征，输出层为3个分类\n    (1,0,0)为第一类，(0,1,0)为第二类，(0,0,1)为第三类\n'''\n\n\n1.初始化参数\n\n\ndef initialize_parameters(n_x, n_h, n_y):\n    np.random.seed(2)\n\n\n# 权重和偏置矩阵\n\nw1 = np.\nrandom\n.randn(n_h, n_x) * \n0.01\n\nb1 = np.zeros(shape=(n_h, \n1\n))\nw2 = np.\nrandom\n.randn(n_y, n_h) * \n0.01\n\nb2 = np.zeros(shape=(n_y, \n1\n))\n\n\n# 通过字典存储参数\n\nparameters = {\n'w1'\n: w1, \n'b1'\n: b1, \n'w2'\n: w2, \n'b2'\n: b2}\n\n\nreturn\n parameters\n\n\n\n2.前向传播\n\n\ndef forward_propagation(X, parameters):\n    w1 = parameters['w1']\n    b1 = parameters['b1']\n    w2 = parameters['w2']\n    b2 = parameters['b2']\n\n\n# 通过前向传播来计算a2\n\nz1 = np.dot(w1, X) + b1     \n# 这个地方需注意矩阵加法：虽然(w1*X)和b1的维度不同，但可以相加\n\na1 = np.tanh(z1)            \n# 使用tanh作为第一层的激活函数\n\nz2 = np.dot(w2, a1) + b2\na2 = \n1\n / (\n1\n + np.\nexp\n(-z2))  \n# 使用sigmoid作为第二层的激活函数\n\n\n\n# 通过字典存储参数\n\ncache = {\n'z1'\n: z1, \n'a1'\n: a1, \n'z2'\n: z2, \n'a2'\n: a2}\n\n\nreturn\n a2, cache\n\n\n\n3.计算代价函数\n\n\ndef compute_cost(a2, Y):\n    m = Y.shape[1]      # Y的列数即为总的样本数\n\n\n# 采用交叉熵（cross-entropy）作为代价函数\nlogprobs = \nnp\n.multiply(\nnp\n.\nlog\n(a2), Y) + \nnp\n.multiply(\nnp\n.\nlog\n(\n1\n - a2),(\n1\n - Y))\ncost = - \nnp\n.\nsum\n(logprobs) / m\n\n\nreturn\n cost\n\n\n\n4.反向传播（计算代价函数的导数）\n\n\ndef backward_propagation(parameters, cache, X, Y):\n    m = Y.shape[1]\n\n\nw2 = \nparameters\n[\n'w2'\n]\n\na1 \n= cache[\n'a1'\n]\n\na2 \n= cache[\n'a2'\n]\n\n\n# 反向传播，计算dw1、db1、dw2、db2\ndz2 \n= a2 - Y\n\ndw2 \n= (1\n / m) * np.dot(dz2, a1.T)\ndb2 = (\n1\n / \nm) * np.sum(dz2, axis=1, keepdims=True)\n\ndz1 \n= np.multiply(np.dot(w2.T, dz2), 1 - np.power(a1, 2))\n\ndw1 \n= (1\n / m) * np.dot(dz1, X.T)\ndb1 = (\n1\n / \nm) * np.sum(dz1, axis=1, keepdims=True)\n\n\ngrads \n= {\n'dw1'\n: dw1,\n \n'db1'\n: db1,\n \n'dw2'\n: dw2,\n \n'db2'\n: db2}\n\n\nreturn \ngrads\n\n\n\n\n5.更新参数\n\n\ndef update_parameters(parameters, grads, learning_rate=0.4):\n    w1 = parameters['w1']\n    b1 = parameters['b1']\n    w2 = parameters['w2']\n    b2 = parameters['b2']\n\n\ndw1 = grads[\n'dw1'\n]\ndb1 = grads[\n'db1'\n]\ndw2 = grads[\n'dw2'\n]\ndb2 = grads[\n'db2'\n]\n\n# 更新参数\nw1 = w1 - dw1 * learning_rate\nb1 = b1 - db1 * learning_rate\nw2 = w2 - dw2 * learning_rate\nb2 = b2 - db2 * learning_rate\n\nparameters = {\n'w1'\n: w1, \n'b1'\n: b1, \n'w2'\n: w2, \n'b2'\n: b2}\n\nreturn parameters\n\n\n\n建立神经网络\n\n\ndef nn_model(X, Y, n_h, n_input, n_output, num_iterations=10000, print_cost=False):\n    np.random.seed(3)\n\n\nn_x = n_input           \n# 输入层节点数\n\nn_y = n_output          \n# 输出层节点数\n\n\n\n# 1.初始化参数\n\n\nparameters\n = initialize_parameters(n_x, n_h, n_y)\n\n\n# 梯度下降循环\n\n\nfor\n i \nin\n range(\n0\n, num_iterations):\n    \n# 2.前向传播\n\n    a2, cache = forward_propagation(X, \nparameters\n)\n    \n# 3.计算代价函数\n\n    cost = compute_cost(a2, Y)\n    \n# 4.反向传播\n\n    grads = backward_propagation(\nparameters\n, cache, X, Y)\n    \n# 5.更新参数\n\n    \nparameters\n = update_parameters(\nparameters\n, grads)\n\n    \n# 每1000次迭代，输出一次代价函数\n\n    \nif\n print_cost and i % \n1000\n == \n0\n:\n        \nprint\n('迭代第%i次，代价函数为：%f' % (i, cost))\n\n\nreturn\n \nparameters\n\n\n\n\n运行结果及报错内容\n\n\n我的解答思路和尝试过的方法\n\n\n想问一下各位这个代码的反向传播部分里面的计算公式是对的嘛，尤其是dw1，dw2部分，现在需要学习率很高才能快速收敛\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "该回答引用ChatGPT \n在 Python 中，如果你想进行对应位置相乘，可以使用 Numpy 提供的 multiply 函数。例如：\n\nimport numpy as np\n\na = np.array([[1, 2], [3, 4]])\nb = np.array([[5, 6], [7, 8]])\n\n# 对应位置相乘\nc = np.multiply(a, b)\n\nprint(c)\n\n\n这段代码将输出一个2x2的矩阵，每个元素都是对应位置相乘的结果。\n至于将数组转化为一维的方法，你可以使用 Numpy 的 flatten 方法，例如：\n\n\nimport numpy as np\n\na = np.array([[1, 2], [3, 4]])\n\n# 将数组展平为一维\nb = a.flatten()\n\nprint(b)\n\n\n这段代码将输出一个包含所有元素的一维数组。", "Konwledge_Point": "应对NP完全问题", "Question": "np.mat()生成的矩阵问题\n请问生成的矩阵没有对应位置相乘的方法吗？只能转化为ndarray才行吗？有没有类似matlab的\na(:)直接输出a所有元素的办法呢？（除了np.reshape(a,-1)这种有点长的）", "Tag": "算法分析"}
{"Answer": "原来是我安装的nagios-plugins版本是2.2.1，而yum安装的OpenSSL的版本只有1.0.1e，而1.0.1e是一个比较落后的版本，一些新功能函数没包含。\r\n所以我升级了我的OpenSSL到1.1.1b之后就正常安装了。", "Konwledge_Point": "应对NP完全问题", "Question": "nagios在make过程中报错\nlibnpcommon.a(sslutils.o): In function `np_net_ssl_init_with_hostname_version_and_cert':\n/root/nagios-plugins-2.2.1/plugins/sslutils.c:125: undefined reference to `SSLv23_client_method'\n/root/nagios-plugins-2.2.1/plugins/sslutils.c:129: undefined reference to `SSL_library_init'\n/root/nagios-plugins-2.2.1/plugins/sslutils.c:130: undefined reference to `SSL_load_error_strings'\n/root/nagios-plugins-2.2.1/plugins/sslutils.c:131: undefined reference to `OPENSSL_add_all_algorithms_conf'\n/root/nagios-plugins-2.2.1/plugins/sslutils.c:68: undefined reference to `SSLv3_client_method'\n/root/nagios-plugins-2.2.1/plugins/sslutils.c:60: undefined reference to `SSLv2_client_method'\ncollect2: ld returned 1 exit status\n\n\n\n\n已经执行过yum -y gd gd-devel *openssl*\n\n\n\n但重新编译任然报上述错误。", "Tag": "算法分析"}
{"Answer": "np.poly1d方法生成的多项式本身带有求导的方法deriv\n\nn = np.poly1d([2,3,5,7]) \nprint(n.deriv())\nprint(n.deriv()(1))\n", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的知识点：关于求得拟合曲线函数的斜率的问题\n最近在学Python，现在遇到一个问题。由一组数据绘出一条曲线，再通过numpy的poly1d函数拟合出曲线的多元式函数，现在想对函数求导得到各个点的斜率，但sympy的diff函数好像无法作用于numpy类型的函数，有什么方法可以将numpy类型函数转化成sympy函数。或者有其他求得曲线上各点斜率的方法。因为数据有波动，根据相邻两点dy/dx来求斜率的方法不太行，希望寻找到更佳方法", "Tag": "算法分析"}
{"Answer": "打印row['open'], row['high'] 看看", "Konwledge_Point": "应对NP完全问题", "Question": "arange 遍历数据报错\ndef get_history_data(self):\n    bar_path = 'e:\\600036.csv'\n    history_data = pd.read_csv(bar_path)\n    for index, row in history_data.iterrows():\n        step = 0.01\n        arr = np.arange(row['open'], row['high'], step)\n        arr = np.append(arr, row['high'])\n        arr = np.append(arr, np.arange(row['open'] - step, row['low'], -step))\n        arr = np.append(arr, row['close'])\n        dt = parser.parse(row['datetime'])\n        i = 0\n        tick = []\n        for item in arr:\n            tick.append(((dt + timedelta(seconds=1 * i)) , item))\n            i += 1\n\n\narr = np\n.arange\n(row\n[\n'open'\n]\n, row\n[\n'high'\n]\n, step)\n\n\n\nValueError: arange: cannot compute length", "Tag": "算法分析"}
{"Answer": "\nThis is the new best way to upgrade npm on Windows.\nRun PowerShell as Administrator\nSet-ExecutionPolicy Unrestricted -Scope CurrentUser -Force\nnpm install -g npm-windows-upgrade\nnpm-windows-upgrade\n\nNote: Do not run npm i -g npm. Instead use npm-windows-upgrade to update npm going forward. Also if you run the NodeJS installer, it will replace the node version.\n\nUpgrades npm in-place, where node installed it.\nEasy updating, update to the latest by running npm-windows-upgrade -p -v latest.\nDoes not modify the default path.\nDoes not change the default global package location.\nAllows easy upgrades and downgrades.\nOfficially recommended by the NPM team.\nA list of versions matched between NPM and NODE (https://nodejs.org/en/download/releases/) - but you will need to download NODE INSTALLER and run that to update node (https://nodejs.org/en/) \n\n", "Konwledge_Point": "应对NP完全问题", "Question": "如何在 Windows 上更新 npm？\n\n\n\nI tried \nthis\n:\n\n\n\nsudo npm cache clean -f\nsudo npm install -g n\nsudo n stable\n\n\n\n\n...but it didn't work.\n\n\n\nHow do I do this on Windows?\n\n    \n\n\n\n转载于:https://stackoverflow.com/questions/18412129/how-can-i-update-npm-on-windows", "Tag": "算法分析"}
{"Answer": "A = [7,9,1,5,9,6,1,2,9]\nB = [x for x in A if x!=max(A) and x!=min(A)]\nprint(B)\n#或者：\nC = [x for x in A if x not in [max(A),min(A)]]\nprint(C)\n", "Konwledge_Point": "应对NP完全问题", "Question": "如何删除列表中的最大值和最小值\n想要删除D列表中的最大值和最小值  尝试了np.remove和np.delete(D,argmax())都报错了", "Tag": "算法分析"}
{"Answer": "元组的索引越界，打印一下len(dataset)，n取值已经超过了a,b元组元素个数。", "Konwledge_Point": "应对NP完全问题", "Question": "python运行层次聚类Agnes算法报错\n问题遇到的现象和发生背景\n\n\n问题相关代码，请勿粘贴截图\n\n\n#描述: 基于组平均的AGNES算法，支持多维数组，距离用欧式距离\n\n\n\n\nimport\n numpy \nas\n np\n\nimport\n pandas \nas\n pd\n\nfrom\n scipy.cluster.hierarchy \nimport\n dendrogram,linkage\n\nfrom\n scipy.spatial.distance \nimport\n squareform\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n pylab \nas\n pl\n\n\n\n#从excel中读取数据并转换为矩阵\n\ndatA=pd.read_excel(\nr'C:\\Users\\49175\\Desktop\\jzgb.xlsx'\n)\ndata=np.array(datA,dtype=np.int64)\n\n\n\n#数据处理 dataset是样本的列表\n\na = np.array_split(datA,\n78\n,axis=\n0\n)\ndataset = [(a[i], a[i+\n1\n]) \nfor\n i \nin\n \nrange\n(\n1\n, \nlen\n(a)-\n1\n, \n44\n)]\n\n\n\n#计算欧几里得距离,a,b分别为两个元组\n\n\ndef\n \ndist\n(\na, b\n):\n    t = \n0\n\n    n=\n44\n\n    \nfor\n i \nin\n \nrange\n(n):\n        t = t + np.power(a[i]-b[i], \n2\n)\n    \nreturn\n np.sqrt(t)\n\n\n#dist_min\n\n\ndef\n \ndist_min\n(\nCi, Cj\n):\n    \nreturn\n \nmin\n(dist(i, j) \nfor\n i \nin\n Ci \nfor\n j \nin\n Cj)\n\n#dist_max\n\n\ndef\n \ndist_max\n(\nCi, Cj\n):\n    \nreturn\n \nmax\n(dist(i, j) \nfor\n i \nin\n Ci \nfor\n j \nin\n Cj)\n\n#dist_avg\n\n\ndef\n \ndist_avg\n(\nCi, Cj\n):\n    \nreturn\n \nsum\n(dist(i, j) \nfor\n i \nin\n Ci \nfor\n j \nin\n Cj)/(\nlen\n(Ci)*\nlen\n(Cj))\n\n\n#找到距离最小的下标\n\n\ndef\n \nfind_Min\n(\nM\n):\n    \nmin\n = \n1000\n\n    x = \n0\n; y = \n0\n\n    \nfor\n i \nin\n \nrange\n(\nlen\n(M)):\n        \nfor\n j \nin\n \nrange\n(\nlen\n(M[i])):\n            \nif\n i != j \nand\n M[i][j] < \nmin\n:\n                \nmin\n = M[i][j];x = i; y = j\n    \nreturn\n (x, y, \nmin\n)\n\n\n#算法模型\n\n\ndef\n \nAGNES\n(\ndataset, dist, k\n):\n    \n#初始化C和M\n\n    C = [];M = []\n    \nfor\n i \nin\n dataset:\n        Ci = []\n        Ci.append(i)\n        C.append(Ci)\n    \nfor\n i \nin\n C:\n        Mi = []\n        \nfor\n j \nin\n C:\n            Mi.append(dist(i, j))\n        M.append(Mi)\n    q = \nlen\n(dataset)\n    \n#合并更新\n\n    \nwhile\n q > k:\n        x, y, \nmin\n = find_Min(M)\n        C[x].extend(C[y])\n        C.remove(C[y])\n        M = []\n        \nfor\n i \nin\n C:\n            Mi = []\n            \nfor\n j \nin\n C:\n                Mi.append(dist(i, j))\n            M.append(Mi)\n        q -= \n1\n\n    \nreturn\n C\n\n#画图\n\n\ndef\n \ndraw\n(\nC\n):\n    colValue = [\n'r'\n, \n'y'\n, \n'g'\n, \n'b'\n, \n'c'\n, \n'k'\n, \n'm'\n]\n    \nfor\n i \nin\n \nrange\n(\nlen\n(C)):\n        coo_X = []    \n#x坐标列表\n\n        coo_Y = []    \n#y坐标列表\n\n        \nfor\n j \nin\n \nrange\n(\nlen\n(C[i])):\n            coo_X.append(C[i][j][\n0\n])\n            coo_Y.append(C[i][j][\n1\n])\n        pl.scatter(coo_X, coo_Y, marker=\n'x'\n, color=colValue[i%\nlen\n(colValue)], label=i)\n\n    pl.legend(loc=\n'upper right'\n)\n    pl.show()\n\nC = AGNES(dataset, dist_avg, \n8\n)\ndraw(C)\n\n\n\n\n\n\n\n运行结果及报错内容\n\n\nTraceback (most recent call last):\n  \nFile\n \n\"C:/各种东西练习/1/4.py\"\n, \nline\n 91, \nin\n \n    C = AGNES(dataset, dist_avg, 8)\n  \nFile\n \n\"C:/各种东西练习/1/4.py\"\n, \nline\n 61, \nin\n AGNES\n    Mi.\nappend\n(dist(i, j))\n  \nFile\n \n\"C:/各种东西练习/1/4.py\"\n, \nline\n 38, \nin\n dist_avg\n    \nreturn\n \nsum\n(dist(i, j) \nfor\n i \nin\n \nCi\n \nfor\n j \nin\n Cj)/(len(\nCi\n)*len(Cj))\n  \nFile\n \n\"C:/各种东西练习/1/4.py\"\n, \nline\n 38, \nin\n \n    \nreturn\n \nsum\n(dist(i, j) \nfor\n i \nin\n \nCi\n \nfor\n j \nin\n Cj)/(len(\nCi\n)*len(Cj))\n  \nFile\n \n\"C:/各种东西练习/1/4.py\"\n, \nline\n 27, \nin\n dist\n    t = t + np.power(a[i]-b[i], 2)\nIndexError: tuple index \nout\n of \nrange\n\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "为了满足框架中模型的输入格式要求", "Konwledge_Point": "应对NP完全问题", "Question": "机器学习自制数据集，关于将图片处理成数值过程中array数组reshape的问题\n训练集有60000张图片，测试集10000，图片大小是28×28=784\n关于代码中    x_train_save = np.reshape(x_train, (len(x_train), -1))这句\n我不明白为什么还要对x_train进行形状重塑，这个x_train在函数generateds()中就已经被弄成array类型的二维数组了（60000×784）\n\n\n\n\n\n\nimport\n tensorflow as tf\nfrom PIL \nimport\n Image\n\nimport\n numpy as np\n\nimport\n os\n\n\ntrain_path\n = './mnist_image_label/mnist_train_jpg_60000/'\n\ntrain_txt\n = './mnist_image_label/mnist_train_jpg_60000.txt'\n\nx_train_savepath\n = './mnist_image_label/mnist_x_train.npy'\n\ny_train_savepath\n = './mnist_image_label/mnist_y_train.npy'\n\n\ntest_path\n = './mnist_image_label/mnist_test_jpg_10000/'\n\ntest_txt\n = './mnist_image_label/mnist_test_jpg_10000.txt'\n\nx_test_savepath\n = './mnist_image_label/mnist_x_test.npy'\n\ny_test_savepath\n = './mnist_image_label/mnist_y_test.npy'\n\n\ndef generateds(path, txt):\n    \nf\n = open(txt, 'r')  \n# 以只读形式打开txt文件\n\n    \ncontents\n = f.readlines()  \n# 读取文件中所有行\n\n    f.close()  \n# 关闭txt文件\n\n    x, \ny_\n = [], []  \n# 建立空列表\n\n    for content \nin\n contents:  \n# 逐行取出\n\n        \nvalue\n = content.split()  \n# 以空格分开，图片路径为value[0] , 标签为value[1] , 存入列表\n\n        \nimg_path\n = path + value[\n0\n]  \n# 拼出图片路径和文件名\n\n        \nimg\n = Image.open(img_path)  \n# 读入图片\n\n        \nimg\n = np.array(img.convert('L'))  \n# 图片变为8位宽灰度值的np.array格式\n\n        \nimg\n = img / \n255\n.  \n# 数据归一化 （实现预处理）\n\n        x.append(img)  \n# 归一化后的数据，贴到列表x\n\n        y_.append(value[\n1\n])  \n# 标签贴到列表y_\n\n        print('loading : ' + content)  \n# 打印状态提示\n\n\n    \nx\n = np.array(x)  \n# 变为np.array格式\n\n    \ny_\n = np.array(y_)  \n# 变为np.array格式\n\n    \ny_\n = y_.astype(np.int64)  \n# 变为64位整型\n\n    return x, y_  \n# 返回输入特征x，返回标签y_\n\n\n\n\nif\n os.path.exists(x_train_savepath) \nand\n os.path.exists(y_train_savepath) \nand\n os.path.exists(\n        x_test_savepath) \nand\n os.path.exists(y_test_savepath):\n    print('-------------Load Datasets-----------------')\n    \nx_train_save\n = np.load(x_train_savepath)\n    \ny_train\n = np.load(y_train_savepath)\n    \nx_test_save\n = np.load(x_test_savepath)\n    \ny_test\n = np.load(y_test_savepath)\n    \nx_train\n = np.reshape(x_train_save, (len(x_train_save), \n28\n, \n28\n))\n    \nx_test\n = np.reshape(x_test_save, (len(x_test_save), \n28\n, \n28\n))\n\nelse\n:\n    print('-------------Generate Datasets-----------------')\n    x_train, \ny_train\n = generateds(train_path, train_txt)\n    x_test, \ny_test\n = generateds(test_path, test_txt)\n\n    print('-------------Save Datasets-----------------')\n    \nx_train_save\n = np.reshape(x_train, (len(x_train), -\n1\n))  \n# 此处-1表示列数由行数连带确定\n\n    \nx_test_save\n = np.reshape(x_test, (len(x_test), -\n1\n))\n    np.save(x_train_savepath, x_train_save)\n    np.save(y_train_savepath, y_train)\n    np.save(x_test_savepath, x_test_save)\n    np.save(y_test_savepath, y_test)\n\n\nmodel\n = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(\n128\n, \nactivation='relu'),\n\n    tf.keras.layers.Dense(\n10\n, \nactivation='softmax')\n\n])\n\nmodel.compile(\noptimizer='adam',\n\n              \nloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n\n              \nmetrics=['sparse_categorical_accuracy'])\n\n\nmodel.fit(x_train, y_train, \nbatch_size=32,\n \nepochs=5,\n \nvalidation_data=(x_test,\n y_test), \nvalidation_freq=1)\n\nmodel.summary()\n\n", "Tag": "算法分析"}
{"Answer": "代码第二行，路径写错了，不是环境的问题！", "Konwledge_Point": "应对NP完全问题", "Question": "已经安装numpy，却 显示No module named 'numpy'\n用gprMax产生bp成像结果。源代码如下：\n\n\nimport\n sys \nsys.path.append(\n'D:/mygprmax/gprMax'\n) \n# 把 gprMax 安装路径添加至系统，使 import 可以找到 gprMax 模块 \n\n\nimport\n numpy \nas\n np\n\nfrom\n tools.plot_Bscan \nimport\n get_output_data, mpl_plot \n\nimport\n matplotlib.pyplot \nas\n plt \n\nfrom\n numba \nimport\n jit\n\nfilename_target = \nr\"D:\\soil_cylinder_merged.out\"\n#获取有目标回波数据文件 \n\nrxnumber = \n1\n \nrxcomponent = \n'Ez'\n \n\n# 获取回波数据\n\n\noutputdata_t, dt = get_output_data(filename_target, rxnumber, rxcomponent) \nplt.imshow(outputdata_t, \nextent=[\n0\n, outputdata_t.shape[\n1\n], outputdata_t.shape[\n0\n], \n0\n], \ninterpolation=\n'nearest'\n, \n                aspect=\n'auto'\n, cmap=\n'gray'\n, \n                vmin=-np.amax(np.\nabs\n(outputdata_t)), \nvmax=np.amax(np.\nabs\n(outputdata_t))) \nplt.show()\n\nfilename_back = \nr\"D:\\soil_background_merged.out\"\n#获取背景回波数据文件 \n\nrxnumber = \n1\n \nrxcomponent = \n'Ez'\n \n\n# 获取回波数据 \n\noutputdata_b, dt = get_output_data(filename_back, rxnumber, rxcomponent) \nplt.imshow(outputdata_b, extent=[\n0\n, outputdata_b.shape[\n1\n], outputdata_b.shape[\n0\n], \n0\n], \ninterpolation=\n'nearest'\n, \n                aspect=\n'auto'\n, cmap=\n'gray'\n, \n                vmin=-np.amax(np.\nabs\n(outputdata_b)), \nvmax=np.amax(np.\nabs\n(outputdata_b))) \nplt.show()\n\ntarget_back = outputdata_t-outputdata_b\n#有目标的数据减去背景数据，去除直达波 \n\nplt.imshow(target_back, extent=[\n0\n, target_back.shape[\n1\n], target_back.shape[\n0\n], \n0\n], \ninterpolation=\n'nearest'\n, \n                aspect=\n'auto'\n, cmap=\n'gray'\n, \n                vmin=-np.amax(np.\nabs\n(target_back)), \nvmax=np.amax(np.\nabs\n(target_back))) \nplt.show()\n\n\n@jit(\nnopython=\nTrue\n) \n\n\ndef\n \nbp\n(\noutputdata, soil, timewindow, cell, exception\n):\n#BP 成像算法 \n\n    \n''' \n    :param outputdata: B-scan 数据 \n    :param soil: 探测区域介质相对介电常数 \n    :param timewindow: 时窗:param cell: 单元格大小，一般是 dx \n    :param exception: 默认为 0 \n    :return: BP 结果 \n    '''\n \n    \n# time_rows -- 时间维采样点数 scans -- 扫描次数 \n\n    time_rows, scans = outputdata.shape \n    \n# 时间维步进 \n\n    dt = timewindow / time_rows \n    \n# 电磁波 \n\n    c = \n3e8\n \n    v = c / np.sqrt(soil) \n    \n# 实际探测区域的 x 长度 cell -- 天线步进，作为成像区域划分单位 \n\n    domain_x = cell * scans \n    x_vec = np.arange(\n0\n, domain_x, cell) \n    \n# 根据时窗和波速算出 y 的实际长度 \n\n    domain_y = cell * np.ceil(timewindow * v / \n2\n / cell) \n    y_vec = np.arange(\n0\n, domain_y, cell) \n    rows = y_vec.shape[\n0\n] \n# 成像区域行数 \n\n    cols = x_vec.shape[\n0\n] \n# 成像区域列数 \n\n    ans = np.zeros((rows, cols)) \n# 存储 bp 结果，空矩阵 \n\n    \nfor\n row \nin\n \nrange\n(rows): \n        \nfor\n col \nin\n \nrange\n(cols): \n            ascan_curve = \n0\n \n# 存储当前单元格幅值和 \n\n            \nfor\n scan \nin\n \nrange\n(scans): \n                d = \n2\n * np.sqrt(np.power((y_vec[row] - \n0\n), \n2\n) + np.power((x_vec[col] - x_vec[scan]), \n2\n)) \n                time = d / v \n# 双程传播时间 \n\n                serie = time / dt + exception \n# 时间索引 \n\n                fix = \nint\n(serie) \n                ceil = \nint\n(np.ceil(serie)) \n                \nif\n serie < time_rows - \n1\n \nand\n serie > \n0\n: \n                    ascan_curve += outputdata[fix][scan] + outputdata[ceil][scan] - (serie - fix) * outputdata[fix][scan] \n            ans[row][col] = ascan_curve \n    \nreturn\n ans\n\ntarget_back_bp = bp(target_back,\n80\n,\n5e-9\n,\n0.002\n,\n50\n)\n#对去除直达波的数据进行 BP 成像 \n\nplt.imshow(target_back_bp, extent=[\n0\n, target_back_bp.shape[\n1\n], target_back_bp.shape[\n0\n], \n0\n], \ninterpolation=\n'nearest'\n, \n                aspect=\n'auto'\n, cmap=\n'gray'\n, \n                vmin=-np.amax(np.\nabs\n(target_back_bp)), \nvmax=np.amax(np.\nabs\n(target_back_bp))) \nplt.show()\n\n\n\n在命令窗口运行，返回错误\n\n\n重装numpy\n\n\n\n\n出现冲突，尚未解决\n\n\n抱着试一试的想法重新运行，于是出现了这种错误，找不到numpy包，又无法安装因为已存在\n\n\n使用pip list查看\n\n\n请问该如何解决？", "Tag": "算法分析"}
{"Answer": "timedelta64你是不是多打了一个mtimemdelta64", "Konwledge_Point": "应对NP完全问题", "Question": "module 'numpy' has no attribute 'timemdelta64'\n在使用np.timemdelta64时出现错误：\n\n\nrfm['R'] = abs(rfm['order_dt'] - rfm['order_dt'].max())/np.timemdelta64(1,'D')\n\n\n报错如下：\n\n\n重装了numpy后仍然没有得到解决\n\n\n谢谢大家的解答！", "Tag": "算法分析"}
{"Answer": "可能的原因：浮点数太大，需要精度限制，将小数点进行限制后，才可求得！根号下必须为正数，如以下修改", "Konwledge_Point": "应对NP完全问题", "Question": "RunitmeWarning:invalid value encountered in double_scalars\n问题遇到的现象和发生背景\n\n\n读取栅格数据转换为了数组，然后传入该数组调用函数的时候，发出警告并一直在重复运行\nWarning (from warnings module):\n    File \"C: \\Python27\\AreGIS10. 7\\lib\\site-packages\\scipy\\stats\n_stats_mstats_ common.py\", line97\n                 sterrest=np.sqrt((1 - r**2) * ssym 1/ssxm / df)\nRunitmeWarning:invalid value encountered in double_scalars\n\n\n问题相关代码，请勿粘贴截图\n\n\n-\n- coding:utf-8 -\n-\n\n\nimport glob, os, sys\nimport numpy as np\nimport numpy.ma as ma\nimport time\nimport datetime\nfrom osgeo import gdal                     #导入osgeo包的gdal模块,GDAL用于读栅格数据，函数返回Dataset对象\nfrom scipy import stats, linalg\nfrom scipy.stats import mstats\n\n\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\n\n\ndef Calculate_trend(inFol, outFol, factor='p', inFormat = \".tif\"):\n\n\nfactor_finList = []\nfor files in os.listdir(inFol):                                                 \n#\nlistdir\n(\npath\n)\n:列举目录下的所有文件\n    for year in range(1988, 1990):\n        if (factor in files) and (inFormat== files[-4:]) and (str(year) in files):            \n            fileIn =  os.path.join(inFol, files)                                  #加入目录下的所有栅格文件\n            dataset = gdal.Open(fileIn, gdal.GA_ReadOnly)                         #读取栅格数据\n            #print\"fileIn是：\",fileIn\n            #print\"dataset是：\",dataset\n\n            if dataset is None:\n                print ('Could not open raster file'), fileIn\n                sys.exit(1)                                                         \n#\nexit\n(1)\n：有错误退出           \n            factor_array = dataset.ReadAsArray().astype(np.float32)              # 将这个数组转化为 float32 位的数组\n            print\"factor_array是：\",factor_array\n            factor_array = factor_array.astype('float')                \n            factor_finList.append(factor_array)                               \n#\nappend\n()\n方法用于在列表末尾添加新的对象\n            print\"factor_finList是：\",factor_finList\n    factor_array = np.array(factor_finList)                                   #创建数组\n    #print\"factor_array是：\",factor_array\n\nkwargs = {\"fillvalue\": -9999.0, \"plot\": False}                              #\noutArray= np.apply_along_axis(Func_single_linear_reg, 0, factor_array, **kwargs)    #np.apply_along_axis将一个函数沿一个轴作用到数组中 调用“Func_single_linear_reg”\n\n#\nexport_array_trend\n(\noutFol\n, \noutArray\n, \ngeoTran\n, \ngeoProj\n, \ncols\n, \nrows\n,  \nvariable\n = '\nlineareg\n', \nfactor\n = \nfactor\n, \nfillvalue\n= -9999.0, \ndriverName\n='\nGTiff\n')\n   #调用“export_array_trend”\n\n\n\ninFol = r\"D:\\py\" #读入的文件夹路径\noutFol = r\"D:\\py\\r\"    #输出的新文件的路径\nif not os.path.exists(outFol):                             #os.path.exists()函数用来检验给出的路径是否真地存在 返回bool\n    os.makedirs(outFol)                                    #makedirs(path):递归式的创建文件夹，注：创建已存在的文件夹将异常\n\n\nget raster tiff infomation(GeoTransform, Projection)\n\n\nglobal geoTran, geoProj, cols, rows, nodatav   \n\n\nfor allRasters in os.listdir(inFol):                      #listdir(path):列举目录下的所有文件\n    print\"第一个栅格数据：\",allRasters\n    if allRasters.endswith(\".tif\"):\n        firstRasPath = os.path.join(inFol, allRasters)    #加入目录下的所有栅格文件\n        break\n\n\nprint\"第一个栅格数据的路径：\",firstRasPath\n\n\nfirstdataset = gdal.Open(firstRasPath, gdal.GA_ReadOnly)          #读取栅格数据\nprint\"栅格数据：\",firstdataset\ncols= firstdataset.RasterXSize                                   #读取列数\nprint\"列数：\",cols\nrows= firstdataset.RasterYSize                                   #读取行数\nprint\"行数：\",rows\nbandnum =  firstdataset.RasterCount                              #读取波段数\nprint\"波段数：\",bandnum\ndriver = firstdataset.GetDriver()                                #读取驱动（返回当前的磁盘驱动器？）\nprint\"驱动器：\",driver\ngeoTran = firstdataset.GetGeoTransform()                         #读取坐标转换参数\nprint\"坐标转换参数：\",geoTran\ngeoProj= firstdataset.GetProjection()                            #读取空间参照系\nprint\"空间坐标系：\",geoProj\nnodatav = firstdataset.GetRasterBand(1).GetNoDataValue()         #栅格数值替换？？？\nprint\"nodata值：\",nodatav\n\n\nCalculate_trend(inFol, outFol, factor='p', inFormat = \".tif\")\n\n\n运行结果及报错内容\n\n\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n是因为数据精度的问题嘛\n\n\n我想要达到的结果\n\n\n不报错", "Tag": "算法分析"}
{"Answer": "- 什么是索引？\r\n\r\n假设有一个二维数组，给定** i **行和**j**列，这才是索引。比如numpy.where()\r\n\r\n- 为什么numpy的arg系列不能索引？\r\n\r\n你可以把arg打印出来就知道了，这是一个相对值，在axis上的索引。numpy argmax argmin argsort......\r\n要么你直接排序获得排序的数组，要么你去每一行的索引（这时可以使用arg的值了）", "Konwledge_Point": "应对NP完全问题", "Question": "根据numpy的argsort（）得到的结果，索引得到的数据是否正确？\nx = np.array([[1, 5, 7], [3, 2, 4]])\n\nindex = np.argsort(x, axis=0)\n\nprint(x[index])\n\n\n\n[[[1 5 7]\n\n  [3 2 4]\n\n  [3 2 4]]\n\n\n\n[[3 2 4]\n\n  [1 5 7]\n\n  [1 5 7]]]\n\n\n\nindex既然得到的是索引。那么根据索引可以得到原数组X的从小到大的值，为什么输出的是一个看不懂的数据 ，哪儿操作有失误或是理解不对吗？", "Tag": "算法分析"}
{"Answer": "http://blog.csdn.net/a197p/article/details/46336235", "Konwledge_Point": "应对NP完全问题", "Question": "Being a Predictor                        \n问题描述 :\n\n\n\nLet A(x) = Sigma(Ai * x^i) (0<=i<=N-1). Given A(1), A(2),…, A(N), You are asked to calculate A(N+1) mod 112233.\n\nIt is guaranteed that A(1), A(2), …, A(N), A(N+1) are all integers. \n\n输入:\n\n\n\nThere are multiple test cases, ended with an EOF.\n\nFor each case:\n\nLine 1 contains a positive integer N (N <= 10^6).\n\nLine 2 to Line N+1: each contains a non-negative integer less than 65536. The integer in Line i is A(i-1). \n\n输出:\n\n\n\nThere are multiple test cases, ended with an EOF.\n\nFor each case:\n\nLine 1 contains a positive integer N (N <= 10^6).\n\nLine 2 to Line N+1: each contains a non-negative integer less than 65536. The integer in Line i is A(i-1).\n\n样例输入:\n\n\n\n1\n\n18605\n\n5\n\n19543\n\n19998\n\n12266\n\n27854\n\n2103\n\n样例输出:\n\n\n\n18605\n\n110887", "Tag": "算法分析"}
{"Answer": "选B筛选切片后每一行的最小值对应的index,因为每一行的最小值都是在0的位置,所以最终生成打的array为[0,0,0]有帮助望采纳~", "Konwledge_Point": "应对NP完全问题", "Question": "关于#arr数组#的问题，如何解决？\n定义 arr 数组, arr = np.array([[1,2,3],[4,5,6],[7,8,9]]) np.argmin(arr[1:],axis=0) 怎么看输出结果\nA、 array([0, 1, 0], dtype=int64)\nB、 array([0, 0, 0], dtype=int64)\nC、 array([1, 1, 1], dtype=int64)\nD、 array([0, 1, 1], dtype=int64)", "Tag": "算法分析"}
{"Answer": "对Python列表使用乘法，对于不可变对象（如数字、字符串）而言是复制值，但对可变对象（如列表、字典）而言则是复制引用，因此对于包含可变对象的列表切莫使用列表乘法，可使用列表生成式代替。", "Konwledge_Point": "应对NP完全问题", "Question": "如何给变量批量赋值ndarray数组？\n\n\n如图，这样批量赋值的话，改变a的大小为什么会同步改变b的大小？\n\n\na,b = np.ones(2) , np.ones(2)\n这样写的话就没有上述问题了，但我要是想同时赋值10个变量，这样写代码会很冗长，有没有什么办法呢？", "Tag": "算法分析"}
{"Answer": "空值是 np.NaN，不是 ''", "Konwledge_Point": "应对NP完全问题", "Question": "python替换replace及空值fillna填充问题\n使用python处理数据时，第一步将df指定列2中含有G字母使用replace替换为空值；\n\n\n\n第二步，替换的空值使用fillna进行填充，实际运行结果没有填充，还是显示空值\n\n\n\n\n\ndf[2].str.replace('G(.*)','',regex = True)\n\ndf[2].fillna(df[1],inplace=True)\n\n\n\n\n这是什么情况呢？\n\n\n\n最终的目的就是df中第2列数据包含字母G的数据填充为第1列的数据", "Tag": "算法分析"}
{"Answer": "需要使用Numpy自带的方法np.isnan()，is表达式，in表达式进行判断\nimport numpy as np\nnp.nan is np.nan # True\nnp.isnan(np.nan) # True\nnp.nan in [np.nan] # True\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "如何判断dataframe里某一位置元素为NAN\n如何判断dataframe里某一位置元素为NAN？求指点一下\n尝试：data.iloc[0,0] is np.nan不行\n尝试代码：\n\n\nimport\n numpy \nas\n np\n\nimport\n pandas \nas\n pd\n\ndata\n = pd.\nDataFrame\n(\ndata\n=\nnp\n.\nones\n([4,4]))\n\n\ndata\n.iloc[0,0] =np.nan\n\n\nprint\n(\ndata\n.iloc[0,0])\n\n\nprint\n(\ndata\n.iloc[0,0] is np.nan)\n\n\nprint\n(np.nan is np.nan)\n\n\n\n运行结果：\nnan\nFalse\nTrue", "Tag": "算法分析"}
{"Answer": "我自己知道了谢谢", "Konwledge_Point": "应对NP完全问题", "Question": "为啥我的j和lat_index和lon-index都是0呀\nextract variable(given region by coord) from .nc4 file\n\n\nimport numpy as np\nfrom netCDF4 import Dataset\nimport os\nimport pandas as pd\n\n\nimport time\n\n\nimport re\n\n\ndef extract_nc(path, coord_path, variable_name, precision=2):\n    \"\"\"extract variable(given region by coord)from.nc file\n    input:\n        path: path of the source nc file\n        coord_path: path of the coord extracted by fishnet:OID,lat,lon\n        variable_name: name of the variable need to read\n        precision: the minimum precision of lat/lon,to match the lat/lon of source nc file\n\n\noutput：\n    {variable_name}.txt [i,\nj\n]:i(\nfile\n \nnumber\n) \nj\n(grid point \nnumber\n)\n    lat_index.txt/lon_index.txt\n    coord.txt\n\n\"\"\n\"\n\n\nprint\n(\nf\n\"variable:{variable_name}\"\n)\ncoord = pd.read_csv(coord_path, sep=\n\",\"\n)  # \nread\n coord(extract by fishnet)\n\nprint\n(\nf\n\"grid point number:{len(coord)}\"\n)\ncoord = coord.\nround\n(precision)  # coord precision correlating with .nc \nfile\n \nlat\n/lon\nresult = [path + \n\"/\"\n + d \nfor\n d in os.listdir(path) \nif\n d[-\n4\n:] == \n\".nc4\"\n]\n\nprint\n(\nf\n\"file number:{len(result)}\"\n)\nvariable = np.zeros((\nlen\n(result), \nlen\n(coord) + \n1\n))  # save the path correlated with \nread\n order\n\n# calculate the \nindex\n of \nlat\n/lon in coord from \nsource\n nc \nfile\n\nf1 = Dataset(result[\n0\n], \n'r'\n)\nDataset.set_auto_mask(f1, False)\nlat_index = []\nlon_index = []\n\nlat\n = f1.variables[\n\"lat\"\n][:]\nlon = f1.variables[\n\"lon\"\n][:]\n\ncount\n = \nlen\n(coord)\nindexs = \nrange\n(\ncount\n)\n\nfor\n \nj\n in indexs:\n    lat_index.\nappend\n(np.where(\nlat\n == coord[\n\"lat\"\n][\nj\n])[\n0\n][\n0\n])\n    lon_index.\nappend\n(np.where(lon == coord[\n\"lon\"\n][\nj\n])[\n0\n][\n0\n])\nf1.\nclose\n()\n\n\n\n到这一步运行完显示错误：\n lat_index.append(np.where(lat == coord[\"lat\"][j])[0][0])\nIndexError: index 0 is out of bounds for axis 0 with size 0\n\n\nProcess finished with exit code 1\n\n\n一直到for j in indexs之前的语句都能正常运行，这一句理论上j应该是跟count一样是92671，但是就显示是0\n\n\n我的coord是四列92671行，四列分别是FID_ OID lat lon ", "Tag": "算法分析"}
{"Answer": "可以用numpy的column_stack和where函数来获取像素点在某阈值范围内的坐标。例子：\nimport numpy as np\nfrom PIL import Image \n\nimage = Image.open(\"tqc.jpg\")\npixels = np.asarray(image)\n# Set threshold level\nthreshold_level = 50\n# Find coordinates of all pixels below threshold\ncoords = np.column_stack(np.where(pixels < threshold_level))\nprint(coords)\n\n或者要获取每个像素点坐标，参考这里的代码：\nhttps://stackoverflow.com/questions/60782965/extract-x-y-coordinates-of-each-pixel-from-an-image-in-python\n\n\n\nhttps://stackoverflow.com/questions/60782965/extract-x-y-coordinates-of-each-pixel-from-an-image-in-python\n\n\n如果对你有帮助，请点击我回答的右上角采纳按钮给予采纳。", "Konwledge_Point": "应对NP完全问题", "Question": "Python如何提取图片像素点坐标值？\nPython提问，Python如何提取图片像素点坐标值？。给个代码例子，各位！", "Tag": "算法分析"}
{"Answer": "x_train['abstract2']，可作为字典键取值或者选取数据框的列。如果x_train是一个数组（或列表），不能用字符去作为索引，应该是整数或切片。", "Konwledge_Point": "应对NP完全问题", "Question": "如何解决IndexError: only integers, slices这类问题\n想用Roberta进行文本分类，找到了代码代入后，出现了下面问题。我的数据集里的x_train设定为文本的'abstract2', y_train 是Label，进行分类。\n\n\nx_train[0]\n\"['inflammation', 'constitute', 'concerted', 'series', 'cellular', 'molecular', 'response', 'follow', 'disturbance', 'systemic', 'homeostasis', 'either', 'toxin']\"\n\n\nX_train是这种tokenize的文本，进入下面后出现了问题：\n\n\nseqlen = x_train['abstract2'].apply(lambda x: len(x.split()))\n\n\nsns.set_style(\"darkgrid\")\nplt.figure(figsize=(8,5))\nsns.distplot(seqlen)\n\n\nIndexError: only integers, slices (\n:\n), ellipsis (\n...\n), numpy.newaxis (\nNone\n) and integer or boolean arrays are valid indices.\n\n\n不知道如何解决？请大家不吝赐教！", "Tag": "算法分析"}
{"Answer": "np.where返回的就是坐标了啊\nimport numpy as np\na = np.array([[1,2,3],[4,5,6]])\nprint(np.where(a==2))\n返回\n(array([0], dtype=int64), array([1], dtype=int64))就是第2行第1列", "Konwledge_Point": "应对NP完全问题", "Question": "Python如何使用numpy.where在图片上提取某一位置的坐标\n遍历所有点耗费太耗费时间了，怎样通过nump.where获取图片的某一位置坐标", "Tag": "算法分析"}
{"Answer": "ubuntu联网后输入打开控制台输入\nsudo apt-get install python-pip\nsudo apt-get install python-numpy python-scipy python-matplotlib ipython ipython-notebook python-pandas python-sympy python-nose\n\n\n\n\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "ImportError: No module named numpy\nImportError: No module named numpy\n\n\npython使用过程中出现\nimport numpy as np\nImportError: No module named numpy", "Tag": "算法分析"}
{"Answer": "代码46行\nfor idx in range(x.size)改for idx in range(x.shape[0]):", "Konwledge_Point": "应对NP完全问题", "Question": "python错误解决\n\n\nclass Net:\n    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.001):\n        self.params={}\n        self.params['W1']=weight_init_std* \\\n                          np.random.randn(input_size,hidden_size)#高斯分布初始化\n        self.params['W2']=weight_init_std* \\\n                          np.random.randn(hidden_size,output_size)\n        self.params['b1']=np.zeros(hidden_size)\n        self.params['b2']=np.zeros(output_size)\n    def sigmoid(self,x):\n        x=np.array(x)\n        return 1/(1+np.exp(-x))\n\n    def softmax(self,x):\n        c=np.max(x)\n        exp_a=np.exp(x-c)#溢出对策\n        sum_exp=np.sum(exp_a)\n        y=exp_a/sum_exp\n        return y\n    def predict(self,x):\n        W1,W2=self.params['W1'],self.params['W2']\n        b1,b2=self.params['b1'],self.params['b2']\n        a1=np.dot(x,W1)+b1\n        z1=self.sigmoid(a1)\n        a2=np.dot(z1,W2)+b2\n        z2=self.sigmoid(a2)\n        y=self.softmax(z2)\n        return y\n    def loss(self,x,t):\n        y=self.predict(x)\n        if y.ndim==1:\n            t=t.reshape(1,t.size)\n            y=y.reshape(1,y.size)\n        batch_size=y.shape[0]\n        return -np.sum(t*np.log(y+1e-7))/batch_size\n    def accuracy(self,x,t):\n        y=self.predict(x)\n        y=np.argmax(y,axis=1)\n        t=np.argmax(t,axis=1)\n        accuracy=np.sum(y==t)/float(x.shape[0])\n        return accuracy\n    def numerical_gradient(self,f,x):#求偏导\n        h=1e-4\n        grad=np.zeros_like(x)\n        tmp_val=np.zeros_like(x[0])\n        for idx in range(x.size):\n            tmp_val=x[idx]#初始值\n            x[idx]=tmp_val+h\n            fxh1=f(x)\n            x[idx]=tmp_val-h\n            fxh2=f(x)\n            grad[idx]=(fxh1-fxh2)/(2*h)\n            x[idx]=tmp_val#还原x\n        return grad\n    def ng(self,x,t):\n        f=lambda w:self.loss(x,t)\n        grads={}\n        grads['W1']=self.numerical_gradient(f,self.params['W1'])\n        grads['b1']=self.numerical_gradient(f,self.params['b1'])\n        grads['W2']=self.numerical_gradient(f,self.params['W2'])\n        grads['b2']=self.numerical_gradient(f,self.params['b2'])\n        return grads\nnet=Net(input_size=784,hidden_size=225,output_size=15)\nx=np.random.rand(100,784)\ny=net.predict(x)\nt=np.random.rand(100,15)\ngrads=net.ng(x,t)\n\n\n\n错误提示：\n\n\n", "Tag": "算法分析"}
{"Answer": "这是因为在执行函数时，没有左键单击时，程序就开始执行第二条if 语句，读取参数a时因没有定义和赋值，当然会报错，应该将第一个判断语句块后面的if...elif...整个代码块缩进一个tab，作为第一个判断语句的一部分，这样就不会报错了。", "Konwledge_Point": "应对NP完全问题", "Question": "openCV绘画变量异常\nimport cv2\n\n\n\nimport numpy as np\n\n\n\nthickness=-1\n\n\n\nmode=1\n\n\n\nd=400\n\n\n\ndef draw_circle(event,x,y,flags,param):\n\n\n\n    if event==cv2.EVENT_LBUTTONDOWN:\n\n\n\n        a=np.random.randint(1,d-50)\n\n\n\n        r=np.random.randint(1,d/5)\n\n\n\n        angle = np.random.randint(0,361)\n\n\n\n        color = np.random.randint(0,high = 256,size = (3,)).tolist()\n\n\n\n    if mode==1:\n\n\n\n        cv2.rectangle(img,(x,y),(a,a),color,thickness)\n\n\n\n    elif mode==2:\n\n\n\n        cv2.circle(img,(x,y),r,color,thickness)\n\n\n\n    elif mode==3:\n\n\n\n        cv2.line(img,(a,a),(x,y),color,3) \n\n\n\n    elif mode==4:\n\n\n\n        cv2.ellipse(img, (x,y), (100,150), angle, 0, 360,color,thickness) \n\n\n\n    elif mode==5:\n\n\n\n        cv2.putText(img,'OpenCV',(0,round(d/2)), \n\n\n\n            cv2.FONT_HERSHEY_SIMPLEX, 2,color,5) \n\n\n\nimg=np.ones((d,d,3),np.uint8)*255\n\n\n\ncv2.namedWindow('image')\n\n\n\ncv2.setMouseCallback('image',draw_circle)\n\n\n\nwhile(1):\n\n\n\n    cv2.imshow('image',img)\n\n\n\n    k=cv2.waitKey(1)&0xFF\n\n\n\n    if k==ord('r'):\n\n\n\n        mode=1\n\n\n\n    elif k==ord('c'):\n\n\n\n        mode=2\n\n\n\n    elif k==ord('l'):\n\n\n\n        mode=3\n\n\n\n    elif k==ord('e'):\n\n\n\n        mode=4\n\n\n\n    elif k==ord('t'):\n\n\n\n        mode=5\n\n\n\n    elif k==27:\n\n\n\n        break \n\n\n\ncv2.destroyAllWindows()", "Tag": "算法分析"}
{"Answer": "代码没问题，你的w, h错了，根据这个图数一下", "Konwledge_Point": "应对NP完全问题", "Question": "关于用python实现张正友标定法中的设置棋盘格的点，换成其他的h*w的矩阵就会报错，应该如何解决这个问题\n问题遇到的现象和发生背景\n\n\n我觉得是在ret的角点提取的地方，我试着打印出来结果是false，只有一张图片可以使用，换成其他的图片ret都是false\n\n\n有问题的代码\n\n\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, \n30\n, \n0.001\n)\n#棋盘格模板规格\nw = \n9\n\nh = \n9\n\n# 世界坐标系中的棋盘格点,例如(\n0\n,\n0\n,\n0\n), (\n1\n,\n0\n,\n0\n), (\n2\n,\n0\n,\n0\n) ....,(\n8\n,\n5\n,\n0\n)，去掉Z坐标，记为二维矩阵\nobjp = np.zeros((w*h,\n3\n), np.float32)\nobjp\n[:,:\n2\n]\n = np.mgrid\n[\n0\n:\nw\n,\n0\n:\nh\n]\n.\nT\n.\nreshape(-\n1\n,\n2\n)\n# 储存棋盘格角点的世界坐标和图像坐标对\nobjpoints = \n[]\n # 在世界坐标系中的三维点\nimgpoints = \n[]\n # 在图像平面的二维点\n#images = cv2.imread('D:\\\\postgraduate\\\\Python\\\\learnpython\\\\picture_1.jpg')\nimages = glob.glob('D:\\\\postgraduate\\\\Python\\\\learnpython\\\\picture_9.jpg')\n\nfor\n fname \nin\n images:\n    img = cv2.imread(fname)\n    gray = cv2.cvt\nColor(\nimg\n,\ncv2\n.COLOR_BGR2GRAY)\n\n    # 找到棋盘格角点\n    ret, corners = cv2.find\nChessboardCorners(\ngray\n, (\nw\n,\nh\n)\n,None)\n    print(ret)\n    # 如果找到足够点对，将其存储起来\n    \nif\n ret\n == \nTrue:\n        cv2.corner\nSubPix(\ngray\n,\ncorners\n,(11,11)\n,(-\n1\n,-\n1\n),criteria)\n        objpoints.append(objp)\n        imgpoints.append(corners)\n        # 将角点在图像上显示\n        cv2.draw\nChessboardCorners(\nimg\n, (\nw\n,\nh\n)\n, corners, ret)\n        cv2.named\nWindow(\n\"findCorners\"\n, \ncv2\n.WINDOW_NORMAL)\n\n        cv2.resize\nWindow('\nfindCorners\n', 600, 600)\n\n        cv2.imshow('findCorners',img)\n        cv2.wait\nKey(0)\n\ncv2.destroy\nAllWindows()\n\nret, mtx, dist, rvecs, tvecs = cv2.calibrate\nCamera(\nobjpoints\n, \nimgpoints\n, \ngray\n.\nshape\n[::-1], None, None)\n\n\n\n\n运行结果及报错内容\n\n\ncalibration.cpp_3694_ error_ (-\n215\n_Assertion failed) nimages _ \n0\n \nin\n \nfunction\n \n'cv__calibrateCameraRO'\n\nFalse\n\nProcess finished with \nexit\n code \n1\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果\n\n\n希望给一张图片就可以对其进行标定和处理", "Tag": "算法分析"}
{"Answer": "改为float64就正常了\nimport numpy as np\na=np.float64(495853088.0)\nprint(a)\nb=f'{a}'\nprint(b)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "Python np.float32类型转str为什么数据会变\n比如a=np.float32(495853088.0)\n    b=f'{a}'\n结果a=495853100.0\n   b='495853088.0'\n问题1:b为什么不是'495853100.0'\n问题2: 请问这里的x可以填多少  495853088.0=np.float32(x)\n小白提问  谢谢各位大佬  好人一生平安", "Tag": "算法分析"}
{"Answer": "上次那个不好用么", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的问题：import matplotlib.pyplot as plt\n\n \n\nimport\n matplotlib\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n numpy \nas\n np\n\ndef\n \nmain\n():\n    matplotlib.rcParams[\n'font.family'\n] = \n'SimHei'\n\n    stuScore = np.loadtxt(\n'student_score.csv'\n, delimiter=\n','\n)  \n# 读入成绩文件,返回数组\n\n    sumEach = np.\nsum\n(stuScore[:, \n1\n:], axis=\n1\n)  \n# 返回每个学生3门课程总分\n\n    avgEach = np.average(stuScore[:, \n1\n:], axis=\n0\n)  \n# 返回每个学生每门课程平均分\n\n    \n# 取出各科成绩\n\n    mathScore = stuScore[:, \n1\n]\n    engScore = stuScore[:, \n2\n]\n    pythonScore = stuScore[:, \n3\n]\n    \n# Performanceanalysis(avgEach, stuScore, sumEach)\n\n    \nwhile\n \nTrue\n:\n        \nprint\n(\n\"\"\"成绩分析与可视化系统  \n 1: 基本信息显示      \n 2: 成绩分析          \n 3: 可视化         \n 4: 退出系统\"\"\"\n)\n        operation = \ninput\n(\n\"请输入你的操作\"\n)\n        \nif\n operation.isdigit():\n            operation = \nint\n(operation)\n            \nif\n operation == \n1\n:\n                \nprint\n(\n\" 学号  高数  英语  python\"\n)\n                \nfor\n i \nin\n stuScore:\n                    \nprint\n(\nf\"\n{\nint\n(i[\n0\n])}\n \n{i[\n1\n]}\n \n{i[\n2\n]}\n \n{i[\n3\n]}\n\"\n)\n            \nelif\n operation == \n2\n:\n                Performanceanalysis(avgEach, stuScore, sumEach)\n            \nelif\n operation == \n3\n:\n                \n# name= input(\"请输入课程名\")\n\n                \n# if name=='xxx':\n\n                \n# 由于不清楚你的课程名是啥,你这里自己填 if elif else结构就可以\n\n                Highnumberhistogram(mathScore)\n                Englishhistogram(engScore)\n                Scorehistogram(pythonScore)\n            \nelif\n operation == \n4\n:\n                \nimport\n sys\n                sys.exit(\n0\n)\n            \nelse\n:\n                \nprint\n(\n\"输入错误,请重新输入\"\n)\n\ndef\n \nPerformanceanalysis\n(\navgEach, stuScore, sumEach\n):\n    \n# 返回最高分和最低分\n\n    maxMath = np.\nmax\n(stuScore[:, \n1\n])\n    maxEng = np.\nmax\n(stuScore[:, \n2\n])\n    maxPython = np.\nmax\n(stuScore[:, \n3\n])\n    minMath = np.\nmax\n(stuScore[:, \n1\n])\n    minEng = np.\nmax\n(stuScore[:, \n2\n])\n    minPython = np.\nmax\n(stuScore[:, \n3\n])\n    \nprint\n(\n\"个人总分情况是：\"\n)\n    \nprint\n(sumEach)\n    \nprint\n(\n\"个人平均分情况是：\"\n)\n    \nprint\n(avgEach)\n    \nprint\n(\n\"班级每门课程最高分：\"\n)\n    \nprint\n(maxMath, maxEng, maxPython)\n    \nprint\n(\n\"班级每门课程最低分：\"\n)\n    \nprint\n(minMath, minEng, minPython)\n\ndef\n \nHighnumberhistogram\n(\nmathScore\n):\n    \n# 绘制高数直方图\n\n    plt.suptitle(\n\"成绩分布直方图\"\n)\n    plt.subplot(\n3\n, \n1\n, \n1\n)\n    plt.hist(mathScore, bins=\n10\n, \nrange\n=(\n0\n, \n100\n), color=\n'red'\n)  \n# 0-100分,分成10段\n\n    plt.xlabel(\n\"高数成绩分数段\"\n)  \n# 设置x轴标签\n\n    plt.ylabel(\n\"人数\"\n)  \n# 设置y轴标签\n\n    plt.xlim(\n0\n, \n100\n)  \n# 设置x轴区间\n\n    plt.xticks([\n0\n, \n10\n, \n20\n, \n30\n, \n40\n, \n50\n, \n60\n, \n70\n, \n80\n, \n90\n, \n100\n])  \n# 设置x轴刻度\n\n    plt.yticks([\n0\n, \n10\n, \n20\n, \n30\n, \n40\n, \n50\n, \n60\n, \n70\n, \n80\n, \n90\n, \n100\n])  \n# 设置y轴刻度\n\n    \n# plt.grid()\n\n    plt.show()\n\ndef\n \nEnglishhistogram\n(\nengScore\n):\n    \n# 绘制英语直方图\n\n    plt.subplot(\n3\n, \n1\n, \n2\n)\n    plt.hist(engScore, bins=\n10\n, \nrange\n=(\n0\n, \n100\n), color=\n'blue'\n)  \n# 0-100分,分成10段\n\n    plt.xlabel(\n\"英语成绩分数段\"\n)  \n# 设置x轴标签\n\n    plt.ylabel(\n\"人数\"\n)  \n# 设置y轴标签\n\n    plt.xlim(\n0\n, \n10\n)  \n# 设置x轴区间\n\n    plt.xticks([\n0\n, \n10\n, \n20\n, \n30\n, \n40\n, \n50\n, \n60\n, \n70\n, \n80\n, \n90\n, \n100\n])  \n# 设置x轴刻度\n\n    plt.yticks([\n0\n, \n10\n, \n20\n, \n30\n, \n40\n, \n50\n, \n60\n, \n70\n, \n80\n, \n90\n, \n100\n])  \n# 设置y轴刻度\n\n    \n# plt.grid()\n\n    plt.show()\n\ndef\n \nScorehistogram\n(\npythonScore\n):\n    \n# 绘制python直方图\n\n    plt.suptitle(\n\"成绩分布直方图\"\n)\n    plt.subplot(\n3\n, \n1\n, \n3\n)\n    plt.hist(pythonScore, bins=\n10\n, \nrange\n=(\n0\n, \n100\n), color=\n'green'\n)  \n# 0-100分,分成10段\n\n    plt.xlabel(\n\"Python成绩分数段\"\n)  \n# 设置x轴标签\n\n    plt.ylabel(\n\"人数\"\n)  \n# 设置y轴标签\n\n    plt.xlim(\n0\n, \n100\n)  \n# 设置x轴区间\n\n    plt.xticks([\n0\n, \n10\n, \n20\n, \n30\n, \n40\n, \n50\n, \n60\n, \n70\n, \n80\n, \n90\n, \n100\n])  \n# 设置x轴刻度\n\n    plt.yticks([\n0\n, \n10\n, \n20\n, \n30\n, \n40\n, \n50\n, \n60\n, \n70\n, \n80\n, \n90\n, \n100\n])  \n# 设置y轴刻度\n\n    \n# plt.grid()\n\n    plt.show()\n\nif\n __name__ == \n'__main__'\n:\n    main()\n \n\n\n\n\n\n加一个python的饼图 不要重复的\n\n\n就要最后一列的那列数字  加到饼图里", "Tag": "算法分析"}
{"Answer": "你好！还是不知怎么改啊？", "Konwledge_Point": "应对NP完全问题", "Question": "‘PTHREAD_MUTEX_RECURSIVE’ undeclared \n报错‘PTHREAD_MUTEX_RECURSIVE’ undeclared (first use in this function)：\n\n\n\n程序如下：\n\n\n\n1 #include \n\n  2 #include \n\n  3 #include \n\n  4 #include \n\n  5\n\n  6 int main(int argc,char *argv[]){\n\n  7     pthread_mutex_t  mutex;\n\n  8\n\n  9     if(argc < 2){\n\n 10         printf(\"-usage:%s [error|normal|recursive]\\n\",argv[0]);\n\n 11         exit(1);\n\n 12     }\n\n 13     pthread_mutexattr_t    mutexattr;\n\n 14     pthread_mutexattr_init(&mutexattr);\n\n 15     if(!strcmp(argv[1],\"error\")){\n\n 16         pthread_mutexattr_settype(&mutexattr,PTHREAD_MUTEX_RECURSIVE);\n\n 17     }/*else if(!strcmp(argv[1],\"normal\")){\n\n 18         pthread_mutexattr_settype(&mutexattr,PTHREAD_MUTEX_NORMAL);\n\n 19     }else if(!strcmp(argv[1],\"recursive\")){\n\n 20         pthread_mutexattr_settype(&mutexattr,PTHREAD_MUTEX_RECURSIVE);\n\n 21     }\n\n 22     pthread_mutex_init(&mutex,&mutexattr);\n\n 23     if(pthread_mutex_lock(&mutex) != 0){\n\n 24         printf(\"lock failure\\n\");\n\n 25     }else {\n\n 26         printf(\"lock success\\n\");\n\n 27     }\n\n 28     if(pthread_mutex_lock(&mutex) != 0){\n\n 29         printf(\"lock failure\\n\");\n\n 30     }else {\n\n 31         printf(\"lock success\\n\");\n\n 32     }\n\n 33 \n\n 34     pthread_mutex_unlock(&mutex);\n\n 35     pthread_mutex_unlock(&mutex);\n\n 36     pthread_mutexattr_destroy(&mutexattr);\n\n 37     pthread_mutex_destroy(&mutex);     */\n\n 38 \n\n 39 \n\n 40     return 0;\n\n 41 }\n\n\n\n请问怎么回事啊？谢谢", "Tag": "算法分析"}
{"Answer": "是不是σΔp[i] = np.std(((Δp[i])), ddof=1)这一句里的Δp[i]格式不对啊，我看你前一行转float来着", "Konwledge_Point": "应对NP完全问题", "Question": "python求标准差返回NAN\n问题遇到的现象和发生背景\n\n\n对列表求标准差的时候为什么返回NAN，应该是正常的数值呀\n\n\n问题相关代码，请勿粘贴截图\n\n\n\nΔ\np\n = pd\n.read_csv\n(\n\"C:/Users/DELL/Desktop/dt3.csv\"\n, header=\n0\n, usecols=\n[\n'Δp'\n]\n)\nΔp= np\n.array\n(Δp)\n\nσΔp=\n[]\n\n\nfor\n \ni\n \nin\n range(len(Δp)):\n    σΔ\np\n.append\n((\nfloat\n(Δp[i])))  \n    σΔ\np\n[i]\n = np\n.std\n(\nfloat\n((Δp[i])), ddof=\n1\n)\n\nprint\n(σΔp)\n\n\n\n\n运行结果及报错内容\n\n\n\n\n返回结果全为nan\n\n\n我的解答思路和尝试过的方法", "Tag": "算法分析"}
{"Answer": "个人习惯问题，或者说国外跟国内的写法也不一样", "Konwledge_Point": "应对NP完全问题", "Question": "正态分布是np模块的还是nd模块的\n为什么CSDN中都是np模块而书上是nd模块?", "Tag": "算法分析"}
{"Answer": "http://blog.csdn.net/kingbeful/article/details/3078306?locationNum=14", "Konwledge_Point": "应对NP完全问题", "Question": "Sending Gift                \nSending company has to send gift in the coming Christmas. One of the building of HangZhou which has 100 floors, there are many people working in the building will recieve gift in the coming Christmas. One of the staffer in the Sending company is dispatched to send gift to the tall building. It's so tired to send so many gift(but he has to do for money). Even worse there has no lift. Go up one floor cost 5 calories energy, Go down one floor cost 3 calories energy. The sending company aways use column to pack the gift, you can only fetch the gift at the two end of the column. Now the company pack these gift which sending to the tall building in two columns. Given the position(which floor) every gift has to send, now you are to find the min energy the staffer has to consume. \n\n\n\nThe staffer at the first floor at start.\n\n\n\nInput\n\n\n\nThe first line if one integer P which is the test case. \n\nThen P test follows,each test the first line if one integer N(N<=30) which is the number of gifts packed in the first column, the next line is N integers(between 1 and 100) which is the floor every gift (from one end to another) in the first column has to send.The next is one integer M(M<=30)which is the number of gifts packed in the second column, then next line is M integers (between 1 and 100) which is the floor every gift(from one end to another) in the second column has to send. \n\n\n\nOutput\n\n\n\nJust one integer the min energy(calories) the staffer has to consume. \n\n\n\nSample Input\n\n\n\n1 \n\n2 \n\n1 2 \n\n3 \n\n1 2 3 \n\n\n\nSample Output\n\n\n\n10 ", "Tag": "算法分析"}
{"Answer": "array = N.loadtxt(csvdir,delimiter = ',',usecols = (3),skiprows = 1)\r\nnp.sum(array[1000:5000], axis=0)", "Konwledge_Point": "应对NP完全问题", "Question": "numpy 范围 求和 sum，条件如何编写\narray = N.loadtxt(csvdir,delimiter = ',',usecols = (3),skiprows = 1)\n\n\n\n\n\n问题：\n\narray里面都是数据，想对array的数据进行区间求和，获得1000<x<5000的x的和\n\n\n\n请问用Numpy,sum()如何进行条件编写？\n\n\n\n在线等！", "Tag": "算法分析"}
{"Answer": "注释//换成#号。python中的注释是#号，不是//\n", "Konwledge_Point": "应对NP完全问题", "Question": "为什么我引用  import pandas as pd后，运行程序 这一句有invalid syntax 错误呢（头痛）\n", "Tag": "算法分析"}
{"Answer": "\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as pc\nimport numpy as np\n# 创建窗口\nfig1 = plt.figure()\nax1 = fig1.add_subplot(121) # 创建子图1\nplt.xlim(0,20)\nplt.ylim(0,20)\nplt.axis(\"equal\")\n# 定义圆心\ncenter = np.array([9,9])\nax1.add_patch(pc.Circle(center,5)) # 画圆\nax2 = fig1.add_subplot(122) # 创建子图2\n# 画等边三角形的过程就是确定3个点,然后画直线即可\n# 设左下角的点为(5,5),变成为10\n# 可推测右下角的点(15,5)\n# 顶点(10,sqrt(10**2 - 5 ** 2))\np1 = (5,5)\np2 = (15,5)\np3 = (10,(10 ** 2 - 5 ** 2) ** 0.5)\n# 画四条直线,根据三个顶点\ntriangleX = [p1[0],p2[0],p3[0],p1[0]] \ntriangleY = [p1[1],p2[1],p3[1],p1[0]]\nplt.axis(\"equal\")\nplt.plot(triangleX,triangleY,\"green\")\nplt.show() # 显示\n\n结果:\n\n如果觉得答案对你有帮助,请点击下采纳,谢谢~", "Konwledge_Point": "应对NP完全问题", "Question": "要求在下面图片中，需要有注释\n越简单越好，写下注释。谢谢。如果对一定及时采纳，感谢各位。一定采纳的我。", "Tag": "算法分析"}
{"Answer": "我怀疑是这里问题if ((c->a) < 0 || (c->b) > 100)输入错误应该是重新输入，而不是返回，你试试。\n    ", "Konwledge_Point": "应对NP完全问题", "Question": "数据结构入门题，计算a+b的值\n输入n组数据；\n下面n组，每组两个数a，b；（0<=a,b<=100）\n输出a+b的值。\n\n\n代码是在visual studio 2019上写的也能运行起来，但是没过OJ，不知道哪个地方出了问题，谢谢！\n\n\n#\ninclude\n \n\n\n\n#\ninclude\n \n\n\n\n \nstruct\n \narrye\n {\n    \nint\n a;\n    \nint\n b;\n    \nstruct\n \narrye\n* next;\n};\n \ntypedef\n \nstruct\n \narrye\n   arry;\n \ntypedef\n arry* link;\n\n \nint\n \nmain\n(\nvoid\n)\n\n \n{\n     link ab = \nNULL\n;\n     link c = \nNULL\n;\n     \nint\n i=\n0\n;\n     \nint\n n=\n0\n;\n     ab = (link)\nmalloc\n(\nsizeof\n(arry));\n     \nif\n (ab == \nNULL\n)\n         \nreturn\n \n0\n;\n     ab->next = \nNULL\n;\n           c = ab;\n     \nscanf_s\n(\n\"%d\"\n, &n);\n//这个已经改成了scanf\n\n    \n     \nwhile\n (i < n) \n     {\n         c->next = (link)\nmalloc\n(\nsizeof\n(arry));\n         c = c->next;\n         c->next = \nNULL\n;\n         \nscanf_s\n(\n\"%d %d\"\n, &c->a, &c->b);\n         \nif\n ((c->a) < \n0\n || (c->b) > \n100\n)\n             \nreturn\n \n0\n;\n         i++;\n\n     }\n     c = ab->next;\n     i = \n1\n;\n     \nwhile\n (c != \nNULL\n)\n     {\n         \nprintf\n(\n\"case #%d:\\n\"\n, i);\n         \nprintf\n(\n\"%d\\n\"\n, c->a + c->b);\n         c = c->next;\n         i++;\n      }\n    \n     \nwhile\n (ab != \nNULL\n)\n     {\n         c = ab->next;\n         \nfree\n(ab);\n         ab = c;\n\n     }\n         \nreturn\n \n0\n;\n }\n\n", "Tag": "算法分析"}
{"Answer": "暂时没有解决方法。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。", "Konwledge_Point": "应对NP完全问题", "Question": "跪求大神帮我检查一下BP算法计算异或问题程序的错误\n感觉是权值更新部分出错了 \n\n            运行出来都是0.5左右  不知道为啥\n\n\n\nimport math\nimport numpy as np\nimport random as rd\ndef sigmoid(x):\n    f = 1/(1+np.exp(-1*x))\n    return f\n\ndef sigmoid_(x):\n\n    f = [[sigmoid(x[i][j])*(1-sigmoid(x[i][j]))  for j in range(len(x[i]))] for i in range(len(x))]\n    return f\n\ndef differentail_matrix(x):\n    f = np.diag(x[0])\n    return f\n\ninput_X = np.array([[0,0],[1,0],[0,1],[1,1]])\n#print(input_X.shape)\ninput_Y = np.array([[0],[1],[1],[0]])\n#print(input_Y.shape)\n\nW1 = np.random.rand(2,2) #later input,former output\nb1 = np.zeros([2,1])\n#W1 = np.array([[2,2],[-1,-1]])\n#b1 = np.array([[-1],[1.5]])\nW2 = np.random.rand(1,2)\nb2 = np.random.rand(1,1)\n# W2 = np.array([1,1])\n# W2.shape = 1,2\n# b2 = np.array([-1.0])\n#print(W1,b1)\n\nalpha = 0.05 #learn rate\nfor k in range(10000):\n\n    r = rd.sample([0,1,2,3], 1)           #随机抽取\n\n\n\n    X = np.array(input_X[r])\n    X.shape = 2,1                        #transpose\n    Y = np.array(input_Y[r])\n\n    out1 = sigmoid(np.dot(W1,X) + b1)\n    pred_y = sigmoid(np.dot(W2,out1) + b2)\n    #print('predy:',pred_y)\n    err = Y-pred_y\n    #print('err:',err)\n\n\n    #back propagation\n\n    s2 = -2*sigmoid(pred_y)*(1-sigmoid(pred_y))*err         #计算敏感度\n\n    temp = sigmoid_(out1)\n\n    temp = np.array(temp)\n    temp.shape = 1, 2\n    temp2 = np.array(differentail_matrix(temp))\n    s1 = np.dot(temp2, W2.T)*s2\n\n    W2 = W2-alpha*s2*np.transpose(out1)               #权值更新\n    b2 = b2 - alpha*s2\n\n    W1 = W1 - alpha*np.dot(s1,np.transpose(X))\n    b1 = b1 - alpha*s1\n\n\n    #print('第:',i,'次迭代','\\n','权值1',W1,'\\n',b1)\n    #print()\n\n\n    if k%500 == 0:\n        out1 = sigmoid(np.dot(W1,np.transpose(input_X)) + b1)\n        pred_y = sigmoid(np.dot(W2,out1) + b2)\n        print(pred_y)\n", "Tag": "算法分析"}
{"Answer": "num+1改成num+=1,要不然知识第一张图片添加了6次，其次你调试的时候直接打印出列表而不是列表的值会更好", "Konwledge_Point": "应对NP完全问题", "Question": "训练识别器时候出的问题，报错是样本和标签不匹配，都是我用print（len（））打出来是一个结果\n训练识别器时候出的问题，报错是样本和标签不匹配，都是我用print（len（））打出来是一个结果，样本和标签数量都是6.\n\n\nimport pymysql\nimport cv2\nimport numpy \nas\n np\n\nfrom\n cv2 import face\nimport os\nimport pymysql\n\n#连接数据库\n\nconnect = pymysql.connect(host=\n\"localhost\"\n, user=\n\"root\"\n, password=\n\"root\"\n, database=\n\"face\"\n)\n\n#创建一个游标\n\ncur = connect.cursor()\n\n#查询MySQL中的face表格\n\nuserid = \n\"123\"\n\nsql = \n\"select ids,sname from face\"\n\ncur.execute(sql)\n\n#获取face表中的数据\n\n\nresult\n= cur.fetchall()\n\n#将从MySQL中读取的数据元组转为字典\n\n\nresult\n=dict(\nresult\n)\n\n#提取字典中的key值并转化为列表\n\nkeyvalues = list(map(int,\nresult\n.\nkeys\n()))\n\n#样本图像列表\n\nphotos =list()\n\n#标签列表\n\nlables =list()\nwenjianjia = os.listdir(\n\"D:/python/opencv/shuju\"\n)\n\nnum\n=\n1\n\n\nfor\n ids \nin\n wenjianjia:\n    \nif\n \nnum\n<\nlen\n(ids)+\n1\n:\n        photos.append(cv2.imread(\n\"D:/python/opencv/shuju/\"\n+ids+\n\"/\"\n+str(\nnum\n)+\n\".png\"\n,\n0\n))\n        lables.append(keyvalues)\n        \nnum\n+\n1\n\nprint(\nlen\n(lables))\nprint(\nlen\n(photos))\n\n#加载识别器（LBPH）\n\nrecognizer=cv2.face.LBPHFaceRecognizer_create()\n\n#识别器训练\n\nrecognizer.train(photos,np.array(lables))\n\n#训练结果录入\n\nrecognizer.\nwrite\n(\n'./trainer_face.yml'\n)\n\n\n运行结果：\nrecognizer.train(photos,np.array(lables))\ncv2.error: OpenCV(\n4.5\n.4\n) D:\\\na\n\\opencv-python\\opencv-python\\opencv_contrib\\modules\\face\\src\\lbph_faces.cpp:\n375\n: error: (\n-5\n:Bad argument) The \nnumber\n \nof\n samples (src) must equal \nthe\n \nnumber\n \nof\n labels (labels). Was \nlen\n(samples)=\n6\n, \nlen\n(labels)=\n0.\n \nin\n \nfunction\n \n'cv::face::LBPH::train'\n\n\n6\n\n\n6\n\n\n", "Tag": "算法分析"}
{"Answer": "毫无关系", "Konwledge_Point": "应对NP完全问题", "Question": "这个是不是检测模拟器的代码\n帮忙看一下这个代码是不是检测模拟器的，我用模拟器玩游戏闪退拦截了这个代码\n\n\n//\n// Decompiled by Jadx (from NP Manager)\n//\npackage com.unity3d.player;\n\n\nimport\n java.lang.Thread.UncaughtExceptionHandler;\n\nfinal class m implements UncaughtExceptionHandler {\n    \nprivate volatile UncaughtExceptionHandler a;\n\n    m() {\n    }\n\n    final synchronized boolean a() {\n        boolean z;\n        m defaultUncaughtExceptionHandler = Thread.getDefaultUncaughtExceptionHandler();\n        if (defaultUncaughtExceptionHandler == this) {\n            z = false;\n        } else {\n            this.a = defaultUncaughtExceptionHandler;\n            Thread.setDefaultUncaughtExceptionHandler(this);\n            z = true;\n        }\n        return z;\n    }\n\n    /*  JADX ERROR\n: NullPointerException in pass: BlockSplitter\n        java\n.lang\n.NullPointerException\n: Attempt to invoke virtual method 'boolean jadx\n.core\n.dex\n.nodes\n.BlockNode\n.contains\n(jadx\n.core\n.dex\n.attributes\n.AType\n)' on a null object reference\n            at jadx\n.core\n.dex\n.visitors\n.blocksmaker\n.BlockSplitter\n.connectExceptionHandlers\n(Unknown Source:64)\n            at jadx\n.core\n.dex\n.visitors\n.blocksmaker\n.BlockSplitter\n.setupConnections\n(Unknown Source:58)\n            at jadx\n.core\n.dex\n.visitors\n.blocksmaker\n.BlockSplitter\n.splitBasicBlocks\n(Unknown Source:23)\n            at jadx\n.core\n.dex\n.visitors\n.blocksmaker\n.BlockSplitter\n.visit\n(Unknown Source:13)\n            at jadx\n.core\n.dex\n.visitors\n.DepthTraversal\n.visit\n(Unknown Source:9)\n            at jadx\n.core\n.dex\n.visitors\n.DepthTraversal\n.visit\n(Unknown Source:41)\n            at jadx\n.core\n.ProcessClass\n.process\n(Unknown Source:54)\n            at jadx\n.api\n.JadxDecompiler\n.processClass\n(Unknown Source:4)\n            at jadx\n.api\n.JavaClass\n.decompile\n(Unknown Source:19)\n        */\n    public final synchronized void uncaughtException(java\n.lang\n.Thread\n r8, java\n.lang\n.Throwable\n r9) {\n        /*\n        r7 = this;\n        \nreturn r0;\n        monitor-enter(r7);\n        r0 = new java.lang.Error;     Catch\n:{ Throwable -> 0x0066 }\n        r1 = new java\n.lang\n.StringBuilder\n;     \nCatch\n:{ Throwable -> 0x0066 }\n        r1.();     \nCatch\n:{ Throwable -> 0x0066 }\n        r2 = \"FATAL EXCEPTION [%s]\\n\";     \nCatch\n:{ Throwable -> 0x0066 }\n        r3 = 1;     \nCatch\n:{ Throwable -> 0x0066 }\n        r4 = new java\n.lang\n.Object\n[r3];     \nCatch\n:{ Throwable -> 0x0066 }\n        r5 = r8\n.getName\n();     \nCatch\n:{ Throwable -> 0x0066 }\n        r6 = 0;     \nCatch\n:{ Throwable -> 0x0066 }\n        r4[r6] = r5;     \nCatch\n:{ Throwable -> 0x0066 }\n        r2 = java\n.lang\n.String\n.format\n(r2, r4);     \nCatch\n:{ Throwable -> 0x0066 }\n        r1\n.append\n(r2);     \nCatch\n:{ Throwable -> 0x0066 }\n        r2 = \"Unity version     : %s\\n\";     \nCatch\n:{ Throwable -> 0x0066 }\n        r4 = new java\n.lang\n.Object\n[r3];     \nCatch\n:{ Throwable -> 0x0066 }\n        r5 = \"2018.4.30f1\";     \nCatch\n:{ Throwable -> 0x0066 }\n        r4[r6] = r5;     \nCatch\n:{ Throwable -> 0x0066 }\n        r2 = java\n.lang\n.String\n.format\n(r2, r4);     \nCatch\n:{ Throwable -> 0x0066 }\n        r1\n.append\n(r2);     \nCatch\n:{ Throwable -> 0x0066 }\n        r2 = \"Device model      : %s %s\\n\";     \nCatch\n:{ Throwable -> 0x0066 }\n        r4 = 2;     \nCatch\n:{ Throwable -> 0x0066 }\n        r4 = new java\n.lang\n.Object\n[r4];     \nCatch\n:{ Throwable -> 0x0066 }\n        r5 = android\n.os\n.Build\n.MANUFACTURER\n;     \nCatch\n:{ Throwable -> 0x0066 }\n        r4[r6] = r5;     \nCatch\n:{ Throwable -> 0x0066 }\n        r5 = android\n.os\n.Build\n.MODEL\n;     \nCatch\n:{ Throwable -> 0x0066 }\n        r4[r3] = r5;     \nCatch\n:{ Throwable -> 0x0066 }\n        r2 = java\n.lang\n.String\n.format\n(r2, r4);     \nCatch\n:{ Throwable -> 0x0066 }\n        r1\n.append\n(r2);     \nCatch\n:{ Throwable -> 0x0066 }\n        r2 = \"Device fingerprint: %s\\n\";     \nCatch\n:{ Throwable -> 0x0066 }\n        r3 = new java\n.lang\n.Object\n[r3];     \nCatch\n:{ Throwable -> 0x0066 }\n        r4 = android\n.os\n.Build\n.FINGERPRINT\n;     \nCatch\n:{ Throwable -> 0x0066 }\n        r3[r6] = r4;     \nCatch\n:{ Throwable -> 0x0066 }\n        r2 = java\n.lang\n.String\n.format\n(r2, r3);     \nCatch\n:{ Throwable -> 0x0066 }\n        r1\n.append\n(r2);     \nCatch\n:{ Throwable -> 0x0066 }\n        r1 = r1\n.toString\n();     \nCatch\n:{ Throwable -> 0x0066 }\n        r0.(r1);     \nCatch\n:{ Throwable -> 0x0066 }\n        r1 = new java\n.lang\n.StackTraceElement\n[r6];     \nCatch\n:{ Throwable -> 0x0066 }\n        r0\n.setStackTrace\n(r1);     \nCatch\n:{ Throwable -> 0x0066 }\n        r0\n.initCause\n(r9);     \nCatch\n:{ Throwable -> 0x0066 }\n        r1 = r7\n.a\n;     \nCatch\n:{ Throwable -> 0x0066 }\n        r1\n.uncaughtException\n(r8, r0);     \nCatch\n:{ Throwable -> 0x0066 }\n        monitor-exit(r7);\n        \nreturn;\n        r8 = move-exception;\n        goto L_0x006d;\n    L_0x0066\n:\n        r0 = r7\n.a\n;     \nCatch\n:{ all -> 0x0064 }\n        r0\n.uncaughtException\n(r8, r9);     \nCatch\n:{ all -> 0x0064 }\n        monitor-exit(r7);\n        \nreturn;\n    L_0x006d\n:\n        monitor-exit(r7);\n        \nthrow r8;\n        */\n        throw new UnsupportedOperationException(\"Method not decompiled\n: com\n.unity\n3d\n.player\n.m\n.uncaughtException\n(java\n.lang\n.Thread\n, java\n.lang\n.Throwable\n):void\");\n    }\n}\n\n", "Tag": "算法分析"}
{"Answer": "labels = [left,right,top,bottom]，左右上下\r\n\r\nlabels=[1,1,0,1]即labels=[true,true,false,true]\r\n即经纬度标签标识地图位置，在地图的左右上下进行标识\r\n\r\n实例代码是说在地图左右和下方标识纬度label信息\r\n\r\n\r\n以下是 Jeffrey Whitaker的源码：\r\n\r\n\r\n```\r\nfrom mpl_toolkits.basemap import Basemap\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n# setup Lambert Conformal basemap.\r\nm = Basemap(width=12000000,height=9000000,projection='lcc',\r\n            resolution='c',lat_1=45.,lat_2=55,lat_0=50,lon_0=-107.)\r\n# draw coastlines.\r\nm.drawcoastlines()\r\n# draw a boundary around the map, fill the background.\r\n# this background will end up being the ocean color, since\r\n# the continents will be drawn on top.\r\nm.drawmapboundary(fill_color='aqua')\r\n# fill continents, set lake color same as ocean color.\r\nm.fillcontinents(color='coral',lake_color='aqua')\r\n# draw parallels and meridians.\r\n# label parallels on right and top\r\n# meridians on bottom and left\r\nparallels = np.arange(0.,81,10.)\r\n# labels = [left,right,top,bottom]\r\nm.drawparallels(parallels,labels=[False,True,True,False])\r\nmeridians = np.arange(10.,351.,20.)\r\nm.drawmeridians(meridians,labels=[True,False,False,True])\r\nplt.show()\r\n\r\n```", "Konwledge_Point": "应对NP完全问题", "Question": "python 画经纬度的函数drawparallel\n我想知道里面的label是怎么用的？这里的【1，1，0，1】是什么意思\n\n\n\nm.drawparallels(np.arange(24,41,0.05),labels=[1,1,0,1])\n", "Tag": "算法分析"}
{"Answer": "极点？你想求这个边缘的坐标吗？pos = np.where(img)这个函数返回就是所有的x,y坐标 min(pos[0])就是最小的x坐标，以此类推", "Konwledge_Point": "应对NP完全问题", "Question": "python-opencv如何寻找二值图的极点？\n如图，已经弄得了图片的边缘，想知道上下左右的极点，请问该如何实现", "Tag": "算法分析"}
{"Answer": "没有发现修改的地方，推荐使用第二种方式，pickle模块在python3内置，不需要传递额外的参数", "Konwledge_Point": "应对NP完全问题", "Question": "python numpy 中ndarry转成string后怎么转回来\n我用request.post（）发送请求，data中数组不会传，所以想把数组转成string，得知\n\nndarray可以通过方式tostring将ndarry形式的数据转成string ,但不知道怎么转回来。，\n\n求大神指教", "Tag": "算法分析"}
{"Answer": "else if(index2 = 0)\n改为else if(index2 == 0)\n==才是判断是否相等。=是赋值。由于if(index2 = 0)是赋值，所以index的值就被改为0啦。任何数的0次方都是1", "Konwledge_Point": "应对NP完全问题", "Question": "为什么这个函数power（）不能计算 负幂？\ndouble power(double dot2 , int index2);\nint main(void)\n{\n    double dot1;\n    int index1;\n\n\ndouble answer\n;\n\n\nscanf(\n\"%Lf%d\"\n,\n&dot1\n,\n&index1\n)\n;\n\nanswer = power(\ndot1\n,index1)\n;\n\nprintf(\n\"%.3g\"\n,answer)\n;\n\n\nreturn \n0\n;\n\n\n\n\n}\n\n\ndouble power(double dot2 , int index2)\n{\n    int num;\n    double answer2 = 1;\n\n\nif(index2 > \n0\n )\n    for(num \n=\n \n0\n \n; num < index2 ; num ++)\n\n        answer2 *\n=\n dot2\n;\n\n\nelse if(index2 \n=\n \n0\n)\n    answer2 \n=\n \n1\n;\n\n    \nelse(index2 < \n0\n)\n    for(num \n=\n \n0\n \n; num < -index2 ; num ++)\n\n        answer2 *\n=\n \n1\n/dot2\n;\n\n\nreturn answer2\n;\n\n\n\n\n}\n\n\n为什么这个函数power（）不能计算 负幂？\n每次第二个数输入负数总数输出1又是为什么？", "Tag": "算法分析"}
{"Answer": "当调用 setTypeface() 你可以设置外观：\r\n\r\n    textView.setTypeface(APP_FONT_REGULAR, Typeface.BOLD);", "Konwledge_Point": "应对NP完全问题", "Question": "在 xml 中以程序化的方式设置 Typeface 和 text style\n我在xml中定义了一个textview，我按照下面的方法设置了Typeface\n\n\n\ntextView.setTypeface(APP_FONT_REGULAR); // 在 assests 中自定义字体 (.ttf file).\n\n\n\n\n在xml中设置了 \nandroid:textStyle=\"bold\"\n\n但是为什么没有变成粗体字？\n\n\n\n如何给字体设置粗体？", "Tag": "算法分析"}
{"Answer": "http://hant.ask.helplib.com/mip/12821422", "Konwledge_Point": "应对NP完全问题", "Question": "keras自定义metric计算精确率和召回率\n在keras中，在model compile的时候可以通过设置metrics的方式确定预测指标，但是keras已经写好的函数中没有精确率和召回率的实现，因此需要自定义这两个指标，哪位大大能帮忙实现一下？拜谢！", "Tag": "算法分析"}
{"Answer": "def Quadratic(x):\n    \"\"\"定义二次函数\"\"\"\n    return 2 * x ** 2 + 3 * x + 4\n\n\nx = np.linspace(-5, 5, 30)  # 将积分区间30等分\nprint(x)\ny = Quadratic(x)\nprint(y)\nplt.plot(x, y)  # 绘制函数曲线\nplt.plot(x, [0]*len(x), color=\"b\")\nfor i in range(len(x)):  # 绘制x轴\n    plt.plot([x[i], x[i]], [0, y[i]])  # 绘制梯形的上底和下底\nfor i in range(len(x) - 1):\n    plt.plot([x[i], x[i + 1]], [y[i], y[i + 1]])  # 将梯形的斜腰连起来\nplt.show()\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "matplotlib画图题\n\n\n这两个空要填写什么(ﾟoﾟ;看不懂，是要填写范围还是什么东西", "Tag": "算法分析"}
{"Answer": "img1,img2从gpu拿到cpu还是tensor，要img.cpu().numpy(),然后进行格式转换。如果有帮助请采纳。", "Konwledge_Point": "应对NP完全问题", "Question": "pytorch中计算灰度图的psnr\n\n\n\n\n######这里是计算psnr方式\ndef calc_psnr(img1,img2):   \n    img1 = Variable( img1, requires_grad=False)    \n    img2 = Variable( img2, requires_grad = False)\n    img1 = img1.cpu()\n    img1 = np.array(img1).astype(np.float32)\n\n    img1 = torch.from_numpy(img1).float().unsqueeze(0)\n   \n    img2 = img2.cpu()\n    img2 = np.array(img2).astype(np.float32)\n   \n    img2 = torch.from_numpy(img2).float().unsqueeze(0)\n\n    PSNR = peak_signal_noise_ratio(img1,img2).item()\n\n    return PSNR\n\n\n\n#####提示错误\nAttributeError: 'torch.dtype' object has no attribute 'type'\n\n\n\n\n########这里是前面训练过程中调用psnr更新，不会出错\nepoch_psnr.update(calc_psnr(preds, labels), len(inputs))\n\n\n\n我在训练神经网络时，需要用灰度图数据集，使用传统的RGB转ycbcr的方法，psnr会输出nan。使用from skimage.metrics import peak_signal_noise_ratio计算的话，经过测试两幅图片可以计算。但是在训练过程中，给对应的preds和lable训练时，会出现numpy数组很tensor张量转换之间的问题，萌新不太懂，请问各位大佬这里应该怎么改呢？", "Tag": "算法分析"}
{"Answer": "我使用的是循环遍历列表，然后逐个判断是否需要添加到相应列表来完成题目要求。代码如下：\n参考链接：\npython中的array是什么意思_python中数组（numpy.array）的基本操作_BE东欲的博客-CSDN博客Python教程：在Python中遍历列表详解_站长在线的博客-CSDN博客_python 遍历列表\nimport numpy as np  #https://blog.csdn.net/weixin_42577243/article/details/112864277\na=[1,2,0,4,5,6,0,0,0,0,7,0,8,9]\nb=np.array(a)\n\n#print(a)\n\nc=[]  #存放最终结果的列表\nt=[] #存放被0分隔的数字的列表t\n\n#https://blog.csdn.net/u010292470/article/details/121090914\nfor i in a:  #遍历原始列表\n    \n    if(i!=0):   #如果原始列表当前值不是0，则添加到分隔数字的列表t\n        t.append(i)\n        #print(\"t.append(i)\",i)\n    else :  #如果当前是数字是0，则判断与执行是否将分隔数字的列表t添加到最终结果列表c\n    #https://www.jianshu.com/p/3a7ffd6884d8\n        if len(t)!=0: #如果分隔数字的列表c有值，则添加到最终结果列表，同时将分隔数字的列表t置为空列表，以存放下一组数字\n            #print(t)\n            \n            c.append(t)\n            t=[]\n            \n#判断最后一组分隔数字列表t是否有值，有值则将最后一组数字添加到最终结果列表c\nif len(t)!=0:\n    c.append(t)\n    \n#打印结果    \nprint(c)\n    \n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python，运用np，内置函数的切片\na是可变的数组，给a，或者b切片。\n\n\na\n=[\n1\n,\n2\n,\n0\n,\n4\n,\n5\n,\n6\n,\n0\n,\n0\n,\n0\n,\n0\n,\n7\n,\n0\n,\n8\n,\n9\n]\n\nb\n=np.array(a)\n\n\n\n输出结果是：被0隔开的非0数组。\n例，a的输出结果是：\n\n\nc\n=[[\n1\n,\n2\n],[\n4\n,\n5\n,\n6\n],[\n7\n],[\n8\n,\n9\n]]\n", "Tag": "算法分析"}
{"Answer": "if flag then\r\n\t V_PRO := 'select * from table_name t where t.name like '%_% ';\r\n\t else \r\n\t  V_PRO := 'select * from table_name t where t.name like  '%escape%';\r\n      EXECUTE IMMEDIATE V_PRO;\r\n\t\t\t\r\n\t\t\t你看这样行么。动态语句", "Konwledge_Point": "应对NP完全问题", "Question": "SQL问题求解决！！！！！！！！！！\n模糊查询带下划线和普通模糊查询能公用一条SQL吗？怎么写？（下面的只能查到带下划线的）\n\n\n\n                select * from table_name t where t.name like '%\\_%'escape'\\'\n", "Tag": "算法分析"}
{"Answer": "```\r\n(*L).next=LL;\r\n优先级的问题。\r\n\r\n比如\r\nint a[10];\r\nint * p = &a[0];\r\na[0] = 10;\r\na[1] = 20;\r\n*p++; //这是 *(p++)而不是(*p)++\r\nprintf(\"%d\", *p); //20，如果是 (*p)++，输出应该是11\r\n```", "Konwledge_Point": "应对NP完全问题", "Question": "这是数据结构一个简单的链表问题\n\n#include\n#include\n#include\n#define OK 1\n#define MAXSIZE 100\nusing namespace std;\ntypedef  int Status;\n\ntypedef  struct  \n{\nchar name[5];\nfloat price;    \n}Book;\ntypedef Book ElemType;\n\ntypedef  struct LNode\n{\nElemType data;\nstruct LNode *next;\n\n\n}LNode,*LinkList;\n\nint main()\n{\n\n\n\nLNode *L=new LNode();\nLNode *LL;\n*L.next=LL;\n\n\n\n\n\n\ngetch();\n\n\nreturn 0;\n}\n\n\n\n\n\n我想弄个含两个结点的单链表，*L是第一个，*LL是第二个，但是在写地址相连代码的时候（即*L.next=LL;），报了这个错误\n\n\n\n\n\n\n我感觉很奇怪，定义的指针L也开辟空间了，然后选取了地址next属性，完成对另一个指针的赋值，代码应该没毛病啊。求教", "Tag": "算法分析"}
{"Answer": "data1.append((start_num[i], end_num[i], data[i][75]))中data[i][75]是取每行的第76个字段, 你数据库中有76个字段吗没有就是tuple index out of range元组的下标75超过了索引的范围\n\n如有帮助，请点击我的回答下方的【采纳该答案】按钮帮忙采纳下，谢谢!\n", "Konwledge_Point": "应对NP完全问题", "Question": "python运行问题IndexError: tuple index out of range\nIndexError: tuple index out of range\n\n\n\n\nimport pymysql\nimport operator\n\n''\n'创建数据库'\n''\n\nimport pymysql\nimport numpy as np\n#  打开数据库连接，不需要指定数据库，因为需要创建数据库\n#  conn = pymysql.connect(\n'localhost'\n,user = \n\"root\"\n,passwd = \n\"639801\"\n,db=\n'maintenance'\n)\nconn = pymysql.connect(host=\n\"127.0.0.1\"\n, user=\n\"root\"\n, password=\n\"639801\"\n, database=\n\"user1\"\n, port=\n3306\n, autocommit=\nTrue\n)\n#  获取游标\ncursor=conn.cursor()\n#  cursor.execute()\n#  查询数据\ncursor.execute(\n\"select * from maintenance1\"\n)\ndata = cursor.fetchall()\nrow = cursor.rowcount  # 取得记录个数，用于设置表格的行数\nvol = len(data[\n0\n])  # 取得字段数，用于设置表格的列数\nprint(row)\nstart_num=[\n0.0\n]*row\nend_num=[\n0.0\n]*row\ndata1=[]\nfor i in range(row):\n    if data[i][\n1\n] != \nNone\n and \n\"+\"\n in data[i][\n1\n]:\n        start_str=str(data[i][\n1\n])\n        if \n\"k\"\n in start_str:\n            start_int1 = start_str[start_str.index(\n\"k\"\n) + \n1\n:start_str.index(\n\"+\"\n)]\n        if \n\"K\"\n in start_str:\n            start_int1 = start_str[start_str.index(\n\"K\"\n) + \n1\n:start_str.index(\n\"+\"\n)]\n        start_int2 = start_str[start_str.index(\n\"+\"\n) + \n1\n:]\n        start_num1 = float(start_int1)*\n1000\n+float(start_int2)\n    else:\n        start_num1 = data[i][\n1\n]\n\n    if data[i][\n2\n]!= \nNone\n and \n\"+\"\n in data[i][\n2\n]:\n        end_str = str(data[i][\n2\n])\n        if \n\"k\"\n in end_str:\n            end_int1 = end_str[end_str.index(\n\"k\"\n) + \n1\n:end_str.index(\n\"+\"\n)]\n        if \n\"K\"\n in end_str:\n            end_int1 = end_str[end_str.index(\n\"K\"\n) + \n1\n:end_str.index(\n\"+\"\n)]\n        end_int2 = end_str[end_str.index(\n\"+\"\n) + \n1\n:]\n        end_num1 = float(end_int1)*\n1000\n+float(end_int2)\n    else:\n        end_num1 = data[i][\n2\n]\n    start_num[i] = start_num1\n    end_num[i] = end_num1\n    data1.append((start_num[i], end_num[i], data[i][\n75\n]))\n\nprint(data1)\nsql=\n\"UPDATE maintenance1 SET start_number=(%s),end_number =(%s) WHERE id=(%s)\"\n\n\ncursor.executemany(sql, data1)\ncursor.close()\nconn.commit()\nconn.close()\nprint(\n'sql执行成功'\n)\n\n\n\n\n执行结果\n\n\n 928\nTraceback (most recent call last):\n  File \"E:/python1xuexi/pythonProject5/zhuanghao2.py\", line 45, in \n    data1.append((start_num[i], end_num[i], data[i][75]))\nIndexError: tuple index out of range\n\n\nProcess finished with exit code 1\n\n\n换一个数据库连接就行了，很奇怪，找不到解决方案，求帮助啊\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "要把两个拟合并的数据从ndarray转成dataframe,,df=pd.DataFrame(array)再合并。示例：\n\nimport pandas as pd \nimport numpy as np\ndfa=pd.DataFrame(np.random.rand(10,7),columns=list('abcdefg'))\nprint(type(dfa.values))\nprint(dfa)\ndfb=pd.DataFrame({'h':np.random.randint(10,20,10)})\ndf=pd.concat([dfa,dfb],axis=1)\nprint(df.shape)\n\n\n#结果是（10，8）\n ", "Konwledge_Point": "应对NP完全问题", "Question": "pandas 两个Dataframe 如何合并\n第一个dataframe：A\n\n\n\n\n\n\n第二个dataframe：B\n\n\n\n\n\n\n \n\n\n\n尝试用了concat和append都没成功\n\n\n\nconcat码： result = pd.concat([A,B],axis=1)\n\n\n\n出来的数据是33960x33960\n\n\n\n不是很懂\n\n\n\n希望大佬不吝赐教", "Tag": "算法分析"}
{"Answer": "从命令行显示信息看python版本信息及系统信息正常，建议选择对应的numpy版本重新安装。也可从这里下载安装：https://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy->>numpy‑1.22.2+mkl‑cp310‑cp310‑win_amd64.whl", "Konwledge_Point": "应对NP完全问题", "Question": "Python安装numpy时报错，如何解决？\n问题遇到的现象和发生背景 ：开始显示pip版本需要更新，然后成功更新了，到import numpy as np这一步时报错\n\n\n问题相关代码，请勿粘贴截图\n\n\n>>>\n \nimport\n numpy \nas\n np\n\n\n\n\n运行结果及报错内容\n\n\nTraceback\n (most recent call last):\n  \nFile\n \n\"\"\n, line \n1\n, in <\nmodule\n>\n  \nFile\n \n\"D:\\Appilication\\Python\\lib\\site-packages\\numpy\\__init__.py\"\n, line \n138\n, in <\nmodule\n>\n    from . \nimport\n _distributor_init\n  \nFile\n \n\"D:\\Appilication\\Python\\lib\\site-packages\\numpy\\_distributor_init.py\"\n, line \n26\n, in <\nmodule\n>\n    \nWinDLL\n(os.path.\nabspath\n(filename))\n  \nFile\n \n\"D:\\Appilication\\Python\\lib\\ctypes\\__init__.py\"\n, line \n374\n, in __init__\n    self._handle = _dlopen(self._name, mode)\nOSError: [WinError \n193\n] %\n1\n 不是有效的 Win32 应用程序。\n>>>\n\n\n\n我的解答思路和尝试过的方法 ：想过会不会是因为安装的python是64位的哪里版本不一致，还怀疑我电脑是不是32位的然后看了电脑系统是64位\n\n\n为什么cmd里安装的python显示：\nC:\\Users\\Oreo>python\nPython 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)] on win32  \n\n\n我想要达到的结果 只想要安装个numpy库，属实是有些不知道该咋搞了，求指教！", "Tag": "算法分析"}
{"Answer": "这个证明不了，别费心思了。", "Konwledge_Point": "应对NP完全问题", "Question": "如果证明了大数分解问题存在多项式时间的算法，是否相当于证明了P=NP？\n    关于大数分解问题，有人认为是\nP\n问频，有人认为不是\nP\n问题。同样有趣的是，有人认为这一问题是NP问题，也有人认为不是NP问题。大多数人认为大数分解问是NP问题，并在信息安全系统广泛使用，充分的理由是分解大数的过程与NP问题的定义是一致的。事实上，这仅仅是通过语言的表述性说明大数分解问题是NP问题，并没有通过公式的确定性来证明这一问题。也许是本人的阅读范围有限，因而未看到证明的过程，请求有相关资源的老师能提供帮助。\n     谢谢大家！\n", "Tag": "算法分析"}
{"Answer": "python矩阵坐标一般是0开始的。调用了numpy库，如下示意代码请参考。 \n\nimport numpy as np\n\narr=np.array([[0, 0, 0, 0, 1],\n              [1, 1, 0, 1, 0],\n              [1, 0, 1, 1, 0],\n              [1, 0, 0, 1, 0],\n              [1, 0, 0, 0, 1]])\nidx=np.array(np.where(arr==1))\nif idx.shape[1]>0:\n    print('where arr==1, index is: ')\n    for i in range(idx.shape[1]):\n        print((idx[0,i],idx[1,i]))\nelse:\n    print('no exits index where arr==1')\n ", "Konwledge_Point": "应对NP完全问题", "Question": "如何获取dataframe或array中 值为1的元素的坐标\n例如我有一个5*5的矩阵\n\n\n\n\narray([[0, 0, 0, 0, 1],\n       [1, 1, 0, 1, 0],\n       [1, 0, 1, 1, 0],\n       [1, 0, 0, 1, 0],\n       [1, 0, 0, 0, 1]])\n\n\n\n第一行第五列的元素值就为1，那么把它的坐标记成（1，5），\n\n\n\n第二行第一列的元素值也为1，记为（2，1）我想要获取矩阵中所有值为1的元素的坐标要怎么做。或者用pandas做也行", "Tag": "算法分析"}
{"Answer": "你可以再试一下，是不是不小心写成zeros了，我的是可以的\n", "Konwledge_Point": "应对NP完全问题", "Question": "为什么np.empty((3,2))元素不是随机的？\n\n\n唯独只有(3,2)传进去就不是随机元素\n我重复运行了几遍np.empty((3,2))\n但是里面的元素始终只有0\n这是为何？", "Tag": "算法分析"}
{"Answer": "```\r\nimport  numpy as np\r\nimport  matplotlib.pyplot as plt\r\n\r\n\r\nx=np.random.uniform(-2,2,500)\r\ny=np.random.uniform(-3,3,500)\r\ne=(x**2/4+y**2/9)<1\r\nratio=e.sum()/len(e)\r\nratio=\"ratio:%.2f\" % ratio\r\n\r\n\r\ncoler=e.astype(np.str)\r\ncoler[coler=='True']='r'\r\ncoler[coler=='False']='g'\r\nplt.scatter(x,y,c=coler)\r\nplt.title(ratio)\r\n\r\nplt.show()\r\n\r\n```", "Konwledge_Point": "应对NP完全问题", "Question": "国外大学编程题，求助\n小弟国外大学大一，有几道python编程题实在不会了，恳请\n\n各位大佬能帮忙。如果采纳，必有重谢。", "Tag": "算法分析"}
{"Answer": "该回答通过自己思路及引用到GPTᴼᴾᴱᴺᴬᴵ搜索,得到内容具体如下：np.arange 函数用于生成一个等差数列，左闭右开区间，即终点值通常不会被包含在内。但是由于浮点数的精度问题，有时候会出现终点值被包含在内的情况。\n在第一个例子中，使用 np.arange(0.91, 0.93, 0.01) 生成的等差数列应该是 [0.91, 0.92]，但是由于浮点数的精度问题，0.92 这个数可能会被表示成 0.9199999999999999，从而导致 0.93 也被包含在内。这就是为什么 0.93 会被输出的原因。\n在第二个例子中，使用 np.arange(0.91, 0.99, 0.02) 生成的等差数列应该是 [0.91, 0.93, 0.95, 0.97]，这个结果是正确的，因为终点值 0.99 不会被包含在内。\n在第三个例子中，使用 np.arange(0.91, 0.99, 0.01) 生成的等差数列应该是 [0.91, 0.92, 0.93, ..., 0.98]，但是由于浮点数的精度问题，0.98 这个数可能会被表示成 0.9799999999999999，从而导致 0.99 也被包含在内。这就是为什么 0.99 会被输出的原因。\n在第四个例子中，使用 np.arange(99.91, 99.93, 0.01) 生成的等差数列应该是 [99.91, 99.92]，但是由于浮点数的精度问题，99.92 这个数可能会被表示成 99.91999999999999，从而导致 99.93 也被包含在内。这就是为什么 99.93 会被输出的原因。\n为了避免这种精度问题，可以使用 np.linspace 函数代替 np.arange 函数，它可以生成指定数量的等分数列，可以避免出现终点值被包含的问题。例如，可以使用 np.linspace(0.91, 0.93, 3) 生成 [0.91, 0.92, 0.93]，这样可以确保终点值不会被包含在内。\n\n如果以上回答对您有所帮助，点击一下采纳该答案～谢谢", "Konwledge_Point": "应对NP完全问题", "Question": "np.arange输出结果异常\n使用np.arange，终点值应该不被输出，但是如图所示：\n\n\nimport\n numpy as np\n\nM\n = np.arange(\n0\n.\n91\n, \n0\n.\n93\n, \n0\n.\n01\n)\n\nprint\n(M)\n\n\n\n\n\nimport\n numpy as np\n\nM\n = np.arange(\n0\n.\n91\n, \n0\n.\n99\n, \n0\n.\n02\n)\n\nprint\n(M)\n\n\n\n\n\n终点值0.99又没有被输出，很奇怪的问题，不太懂为啥。\n\n\nimport\n numpy as np\n\nM\n = np.arange(\n0\n.\n91\n, \n0\n.\n99\n, \n0\n.\n01\n)\n\nprint\n(M)\n\n\n\n\n\n但是有的组合就是会出现问题:\n\n\nimport\n numpy as np\n\nM\n = np.arange(\n99\n.\n91\n, \n99\n.\n93\n, \n0\n.\n01\n)\n\nprint\n(M)\n\n\n\n99.93又被输出：\n\n\n\n\n感觉很奇怪的问题", "Tag": "算法分析"}
{"Answer": "max([len(x.split(\" \")) for x in x_data])，你的x的数据类型可能是numpy，她是没有.split属性的，你把x转换为字符串，如str(x)", "Konwledge_Point": "应对NP完全问题", "Question": "AttributeError: 'numpy.ndarray' object has no attribute 'split'\n问题遇到的现象和发生背景\n\n\nAttributeError: 'numpy.ndarray' object has no attribute 'split'\n\n\n问题相关代码，请勿粘贴截图\n\n\nmax_document_length = max([len(x.split(\" \")) for x in x_data])\nvocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n\n\n运行结果及报错内容\n\n\nAttributeError: 'numpy.ndarray' object has no attribute 'split'\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "仅供参考：\n                        image = imagedata.reshape((height, width, 3)) #3 is Actually channels\n\n                        imagebin=base64.b64decode(base64str)\n                        imagedata=np.frombuffer((imagebin), dtype=(np.uint8))\n                        image = cv2.imdecode(imagedata, cv2.IMREAD_COLOR)\n\n                        base64str = base64.b64encode(imagedata.tobytes()).decode('utf8',\"ignore\")\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "Python中如何将数组类型的图片转换成BASE64格式\n问题遇到的现象和发生背景\n\n\n我想使用百度智能云上的人脸识别api\n自己在本地试了几种方法调用api，均有返回结果\n\n\n\n\n于是我想把调用这个api的方法应用替换到别的程序里,却发现程序提供的image变量输出出来的是数组，不明白为什么。有点搞不懂这个程序，所以想问下该怎么把这个数组变成BASE64格式的图片\n\n\n\n\n人脸识别SDK地址: \nhttps://cloud.baidu.com/doc/FACE/s/ek37c1qiz", "Tag": "算法分析"}
{"Answer": "从没有这样写过，我猜题主的困惑应该是numpy数组的shape属性吧。shape为(10,)，表示一个长度为10的一维数组；shape为(10,1)，表示一个10行1列的二维数组。比如：\nimport numpy as np\na = np.arange(6).reshape(6,)\nb = np.arange(6).reshape(6,1)\na.shape, b.shape\n((6,), (6, 1))\n", "Konwledge_Point": "应对NP完全问题", "Question": "在python里面，np array (10,)和（10,1），这两个有区别么\n在python里面，np array (10,)和（10,1），这两个有区别么？为什么不都写成一样的？读写起来有区别么？", "Tag": "算法分析"}
{"Answer": "结合的代码，在计算2月时，没有根据年份判断是否是有闰月的年份，少加了条件. 应该是少了 两个if判断\nif i in years1:\n  for k in range(1,29):\n      pass\nif i in years2:\n  for k in range(1,30):\n      pass\n", "Konwledge_Point": "应对NP完全问题", "Question": "python中if,elif,else循环遇到问题,不知如何解决\n Python中想要利用一个方程计算1-12月各个月份的热扩散值，但是现只会分别计算2月份和其他除2月外月份的值，这是我分别计算的代码，是可以正常运行的，先放上数据格式\n\n\n\n\n下面是我计算1、3-12月的代码\n\n\n[]([](\n\n\nimport pandas as pd\nimport numpy as np\n\ncsv=pd\n.read_csv\n(\n\"C:\\\\Users\\86139\\Desktop/NADORS.csv\"\n,header=\n0\n,encoding=\n\"unicode_escape\"\n)\ncsv= csv\n.values\n\nyear=np\n.array\n(csv\n[:, 0]\n)\nmonth=np\n.array\n(csv\n[:, 1]\n)\nday=np\n.array\n(csv\n[:, 2]\n)\ntem20=np\n.array\n(csv\n[:, 3]\n)\ntem50=np\n.array\n(csv\n[:, 4]\n)\ntem100=np\n.array\n(csv\n[:, 5]\n)\ntem200=np\n.array\n(csv\n[:, 6]\n)\n\ndep=\n[0.2,0.5,1.0,2.0]\n \nd2050=\n[]\n \nd50100=\n[]\n \nd100200=\n[]\n \nyears=np\n.arange\n(\n2010\n,\n2017\n)\nyears1=\n[2010,2011,2013,2014,2015]\n\nyears2=\n[2012,2016]\n\nmonths=np\n.arange\n(\n1\n,\n13\n)\ndays=np\n.arange\n(\n1\n,\n32\n)\na=\n[1,3,5,7,8,10,12]\n\nb=\n[4,6,9,11]\n\nc=\n[2]\n\n\nyears\n\n\nfor\n \ni\n \nin\n years: \n    \nfor\n j \nin\n range(\n1\n,\n13\n): \n        temm20=\n[]\n\n        temm50=\n[]\n \n        temm100=\n[]\n \n        temm200=\n[]\n\n        \nif\n j \nin\n \na\n :\n            \nfor\n k \nin\n range(\n1\n,\n32\n):\n                temm20\n.append\n(np\n.mean\n(tem20\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm50\n.append\n(np\n.mean\n(tem50\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm100\n.append\n(np\n.mean\n(tem100\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm200\n.append\n(np\n.mean\n(tem200\n[np.where((year==i)&(month==j)&(day==k))]\n))\n            d2050\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm50)-np\n.min\n(temm50))/(np\n.max\n(temm20)-np\n.min\n(temm20)))/(-\n1\n)/(dep\n[1]\n-dep\n[0]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d50100\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm100)-np\n.min\n(temm100))/(np\n.max\n(temm50)-np\n.min\n(temm50)))/(-\n1\n)/(dep\n[2]\n-dep\n[1]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d100200\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm200)-np\n.min\n(temm200))/(np\n.max\n(temm100)-np\n.min\n(temm100)))/(-\n1\n)/(dep\n[3]\n-dep\n[2]\n))**\n2\n)*(\n10\n**\n6\n))\n        \nif\n j \nin\n \nb\n:\n            \nfor\n k \nin\n range(\n1\n,\n31\n):\n                temm20\n.append\n(np\n.mean\n(tem20\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm50\n.append\n(np\n.mean\n(tem50\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm100\n.append\n(np\n.mean\n(tem100\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm200\n.append\n(np\n.mean\n(tem200\n[np.where((year==i)&(month==j)&(day==k))]\n))\n            d2050\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm50)-np\n.min\n(temm50))/(np\n.max\n(temm20)-np\n.min\n(temm20)))/(-\n1\n)/(dep\n[1]\n-dep\n[0]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d50100\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm100)-np\n.min\n(temm100))/(np\n.max\n(temm50)-np\n.min\n(temm50)))/(-\n1\n)/(dep\n[2]\n-dep\n[1]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d100200\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm200)-np\n.min\n(temm200))/(np\n.max\n(temm100)-np\n.min\n(temm100)))/(-\n1\n)/(dep\n[3]\n-dep\n[2]\n))**\n2\n)*(\n10\n**\n6\n))\n\nframe2={\n'd20-50'\n:d2050,\n'd50-100'\n:d50100,\n'd100-200'\n:d100200} \ndataframe2=pd\n.DataFrame\n(frame2)\ndataframe2\ndataframe2\n.to_csv\n(\n\"C:\\\\Users\\\\86139\\\\Desktop/NADORSdataframeWITHOUT2.csv\"\n)\n```))\n\n下面是我计算\n2\n月份的代码后边的计算部分\n```python\n\nfor\n \ni\n \nin\n years: \n    \nfor\n j \nin\n c: \n        temm20=\n[]\n\n        temm50=\n[]\n \n        temm100=\n[]\n \n        temm200=\n[]\n \n        \nif\n \ni\n \nin\n years1 :\n            \nfor\n k \nin\n range(\n1\n,\n29\n):\n                temm20\n.append\n(np\n.mean\n(tem20\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm50\n.append\n(np\n.mean\n(tem50\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm100\n.append\n(np\n.mean\n(tem100\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm200\n.append\n(np\n.mean\n(tem200\n[np.where((year==i)&(month==j)&(day==k))]\n))\n            d2050\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm50)-np\n.min\n(temm50))/(np\n.max\n(temm20)-np\n.min\n(temm20)))/(-\n1\n)/(dep\n[1]\n-dep\n[0]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d50100\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm100)-np\n.min\n(temm100))/(np\n.max\n(temm50)-np\n.min\n(temm50)))/(-\n1\n)/(dep\n[2]\n-dep\n[1]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d100200\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm200)-np\n.min\n(temm200))/(np\n.max\n(temm100)-np\n.min\n(temm100)))/(-\n1\n)/(dep\n[3]\n-dep\n[2]\n))**\n2\n)*(\n10\n**\n6\n))\n        \nif\n \ni\n \nin\n years2:\n            \nfor\n k \nin\n range(\n1\n,\n30\n):\n                temm20\n.append\n(np\n.mean\n(tem20\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm50\n.append\n(np\n.mean\n(tem50\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm100\n.append\n(np\n.mean\n(tem100\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm200\n.append\n(np\n.mean\n(tem200\n[np.where((year==i)&(month==j)&(day==k))]\n))\n            d2050\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm50)-np\n.min\n(temm50))/(np\n.max\n(temm20)-np\n.min\n(temm20)))/(-\n1\n)/(dep\n[1]\n-dep\n[0]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d50100\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm100)-np\n.min\n(temm100))/(np\n.max\n(temm50)-np\n.min\n(temm50)))/(-\n1\n)/(dep\n[2]\n-dep\n[1]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d100200\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm200)-np\n.min\n(temm200))/(np\n.max\n(temm100)-np\n.min\n(temm100)))/(-\n1\n)/(dep\n[3]\n-dep\n[2]\n))**\n2\n)*(\n10\n**\n6\n))\n\nframe3={\n'd20-50'\n:d2050,\n'd50-100'\n:d50100,\n'd100-200'\n:d100200} \ndataframe3=pd\n.DataFrame\n(frame3)\ndataframe3\ndataframe3\n.to_csv\n(\n\"C:\\\\Users\\\\86139\\\\Desktop/newNADORSdataframe2.csv\"\n)\n\n\n\n下面是我想结合到一起写的代码，前面是一致的，后边计算的部分我改成了如下\n\n\nfor\n \ni\n \nin\n years: \n    \nfor\n j \nin\n range(\n1\n,\n13\n): \n        temm20=\n[]\n\n        temm50=\n[]\n \n        temm100=\n[]\n \n        temm200=\n[]\n\n        \nif\n j \nin\n \na\n :\n            \nfor\n k \nin\n range(\n1\n,\n32\n):\n               temm20\n.append\n(np\n.mean\n(tem20\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm50\n.append\n(np\n.mean\n(tem50\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm100\n.append\n(np\n.mean\n(tem100\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm200\n.append\n(np\n.mean\n(tem200\n[np.where((year==i)&(month==j)&(day==k))]\n))\n            d2050\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm50)-np\n.min\n(temm50))/(np\n.max\n(temm20)-np\n.min\n(temm20)))/(-\n1\n)/(dep\n[1]\n-dep\n[0]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d50100\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm100)-np\n.min\n(temm100))/(np\n.max\n(temm50)-np\n.min\n(temm50)))/(-\n1\n)/(dep\n[2]\n-dep\n[1]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d100200\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm200)-np\n.min\n(temm200))/(np\n.max\n(temm100)-np\n.min\n(temm100)))/(-\n1\n)/(dep\n[3]\n-dep\n[2]\n))**\n2\n)*(\n10\n**\n6\n))\n        elif j \nin\n \nb\n:\n            \nfor\n k \nin\n range(\n1\n,\n31\n):\n               temm20\n.append\n(np\n.mean\n(tem20\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm50\n.append\n(np\n.mean\n(tem50\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm100\n.append\n(np\n.mean\n(tem100\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm200\n.append\n(np\n.mean\n(tem200\n[np.where((year==i)&(month==j)&(day==k))]\n))\n            d2050\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm50)-np\n.min\n(temm50))/(np\n.max\n(temm20)-np\n.min\n(temm20)))/(-\n1\n)/(dep\n[1]\n-dep\n[0]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d50100\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm100)-np\n.min\n(temm100))/(np\n.max\n(temm50)-np\n.min\n(temm50)))/(-\n1\n)/(dep\n[2]\n-dep\n[1]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d100200\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm200)-np\n.min\n(temm200))/(np\n.max\n(temm100)-np\n.min\n(temm100)))/(-\n1\n)/(dep\n[3]\n-dep\n[2]\n))**\n2\n)*(\n10\n**\n6\n))\n        \nelse\n:\n            \nfor\n k \nin\n range(\n1\n,\n29\n):\n                temm20\n.append\n(np\n.mean\n(tem20\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm50\n.append\n(np\n.mean\n(tem50\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm100\n.append\n(np\n.mean\n(tem100\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm200\n.append\n(np\n.mean\n(tem200\n[np.where((year==i)&(month==j)&(day==k))]\n))\n            d2050\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm50)-np\n.min\n(temm50))/(np\n.max\n(temm20)-np\n.min\n(temm20)))/(-\n1\n)/(dep\n[1]\n-dep\n[0]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d50100\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm100)-np\n.min\n(temm100))/(np\n.max\n(temm50)-np\n.min\n(temm50)))/(-\n1\n)/(dep\n[2]\n-dep\n[1]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d100200\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm200)-np\n.min\n(temm200))/(np\n.max\n(temm100)-np\n.min\n(temm100)))/(-\n1\n)/(dep\n[3]\n-dep\n[2]\n))**\n2\n)*(\n10\n**\n6\n))\n            \nfor\n k \nin\n range(\n1\n,\n30\n):\n                temm20\n.append\n(np\n.mean\n(tem20\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm50\n.append\n(np\n.mean\n(tem50\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm100\n.append\n(np\n.mean\n(tem100\n[np.where((year==i)&(month==j)&(day==k))]\n)) \n                temm200\n.append\n(np\n.mean\n(tem200\n[np.where((year==i)&(month==j)&(day==k))]\n))\n            d2050\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm50)-np\n.min\n(temm50))/(np\n.max\n(temm20)-np\n.min\n(temm20)))/(-\n1\n)/(dep\n[1]\n-dep\n[0]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d50100\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm100)-np\n.min\n(temm100))/(np\n.max\n(temm50)-np\n.min\n(temm50)))/(-\n1\n)/(dep\n[2]\n-dep\n[1]\n))**\n2\n)*(\n10\n**\n6\n)) \n            d100200\n.append\n((\n2\n*np.pi/\n86400\n./\n365\n.)/\n2\n./((np\n.log\n((np\n.max\n(temm200)-np\n.min\n(temm200))/(np\n.max\n(temm100)-np\n.min\n(temm100)))/(-\n1\n)/(dep\n[3]\n-dep\n[2]\n))**\n2\n)*(\n10\n**\n6\n))\n\nframe2={\n'd20-50'\n:d2050,\n'd50-100'\n:d50100,\n'd100-200'\n:d100200} \ndataframe2=pd\n.DataFrame\n(frame2)\ndataframe2\ndataframe2\n.to_csv\n(\n\"C:\\\\Users\\\\86139\\\\Desktop/123NADORSdataframe.csv\"\n)\n\n\n\n倒是可以运行出来 但运行结果显示如下\n正确结果应如\n想请问问题究竟出在哪里呢？若能得到解答，感激不尽", "Tag": "算法分析"}
{"Answer": "imported but unused 是说你导入了模块, 但代码中没有使用到这个模块这个导入的模块是多于的.你把导入模块的import 删除即可\n\n如有帮助，请点击我的回答下方的【采纳该答案】按钮帮忙采纳下，谢谢!\n", "Konwledge_Point": "应对NP完全问题", "Question": "pyflakes E：imported but unused\nimport 模块但是显示imported but unused，这个问题怎么解决呀？谢谢🙏", "Tag": "算法分析"}
{"Answer": "Mat 好像有个reshape()成员函数，具体的你可以查一查API，是可以用的。但是你的这个你确定没有问题吗？经过opencv的blob之后一般都是NCHW的输入,也就是你的blob是（batch，channel,28,28），如果你再执行reshape，你的就变成了CHWN了，你确定你的这输入正确？", "Konwledge_Point": "应对NP完全问题", "Question": "如何在C++中实现python的np.reshape\n在python下实现了一个模型识别28，28的数字数据集，预处理如下\n\n\nblob\n = cv2.dnn.blobFromImage(x, scalefactor=\n1\n.\n0\n/\n255\n.\n0\n, size=(\n28\n, \n28\n), crop=False, swapRB=False)\n\ninput_blob\n = blob.reshape(-\n1\n, \n28\n, \n28\n, \n1\n)\n\nnet\n.setInput(input_blob)  # 设置模型输入\n\nout\n = net.forward()\n\n\n\n\n现在的问题是想在C中用opencvdnn部署，但是应该如何实现同样的预处理，C++中有等效于reshape的方法吗", "Tag": "算法分析"}
{"Answer": "基于Monster 组和GPT的调写：\n大概率就是因为这个浮点数的精度问题\n浮点数采用二进制表示，而在二进制中无限循环小数是无法准确表示的，所以会出现精度误差。\n在你的第二个例子中，0.15的二进制表示是一个无限循环小数，而计算机只能用一个近似值来表示它，所以在输出时会出现一个很小的误差。\n如果你需要更精确的计算，可以使用 Python 的 Decimal 类型或者 NumPy 提供的 decimal128 数据类型。", "Konwledge_Point": "应对NP完全问题", "Question": "疑惑np.arange（）函数\n\n\nb\n=np.arange(\n0\n.\n05\n, \n0\n.\n20\n, \n0\n.\n05\n)\n\nprint\n(b)\n\n\n\n为什么输出是0.05  0.1  0.15  0.2 呢？\n还有：\n\n\nfor\n ff in np.arange(\n0\n.\n05\n, \n0\n.\n20\n, \n0\n.\n05\n):  ##\n3\n\n    \nprint\n(ff)\n\n\n\n\n这样输出的是\n0.05\n0.1\n0.15000000000000002\n0.2\n这是为什么呢？疑惑的是 这个函数不是不包括终止值吗？\n如果是\n\n\na\n=np.arange(\n5\n, \n20\n, \n5\n)\n\n\n\n\n输出是5 10 15 就正常了", "Tag": "算法分析"}
{"Answer": "\n \na = mean(getnum())\nprint(\"平均值：{},标准差：{:.2}\".format(a,biaozhuncha(a))))\n", "Konwledge_Point": "应对NP完全问题", "Question": "Python求标准差，值一直为0\nPython求平均值和标准差，平均值可以求出来了，标准差运行结果一直为0，也不报错，哪里出问题了这是，希望小伙伴指出错误，给出正确解答，谢谢啦\n\n", "Tag": "算法分析"}
{"Answer": "这个应该是精度问题吧试试math.fsum", "Konwledge_Point": "应对NP完全问题", "Question": "python：对dataframe拆分使用透视表的np.sum之后，小数位数改变了\n我想得到不同城市不同产品的年产量，现在我有企业所在城市、产品类型和月产量的数据，所以我想用透视表呈现\nsj=pd.read_csv('2015.csv',dtype={'ycl':str}).loc[:,['city','type','ycl']]\npt=pd.pivot_table(sj,values='ycl',index=['city','type'],aggfunc=np.sum)\n但是这样，我最初读取的数据和透视表分别对总产量求和，小数位数会发生改变\n\n\n有没有什么办法能让透视表的总和和最初读取的数据一致", "Tag": "算法分析"}
{"Answer": "请题主补充一下遇到了什么问题，有没有运行出错的信息提示？", "Konwledge_Point": "应对NP完全问题", "Question": "这个Python怎么回事？\n", "Tag": "算法分析"}
{"Answer": "不知道你是不是这个意思：\nz = '''\n短视频：20.5\n唱歌：30\n网心云：19\nb站：2\n头条：10.5\n写小说：270\n'''\n\nday = {}\nfor row in z.strip('\\n').split('\\n'):\n    name, price = row.split('：')\n    day[name] = float(price)\nprint('名称', end='\\t\\t')\nfor k in day.keys():\n    print(k, end='\\t\\t')\nprint()\nfor title, rate in zip(['每年', '每月', '每日'], [365, 30, 1]):\n    print(title, end='\\t\\t')\n    for k, v in day.items():\n        print(v * rate, end='\\t\\t')\n    print()\n", "Konwledge_Point": "应对NP完全问题", "Question": "不怎么使用np，怎么使用字符串转多行列表，完成年月日的赚多少银子的数组运算？\n不怎么使用np，\n怎么使用字符串转多行列表，\n完成年月日的赚多少银子的数组运算？\n\nz = \n'''\n短视频：20.5\n唱歌：30\n网心云：19\nb站：2\n头条：10.5\n写小说：270\n'''\n\n\n\n# 分别计算年月日，与时薪\n\n\n名称  短视频 唱歌 网心云 b站 头条 写小说\n每年\n每月\n每天\n....\n", "Tag": "算法分析"}
{"Answer": "python中abs函数的参数是复数的返回值 是 复数的模\n设复数z=a+bi(a,b∈R) 则复数z的模|z|=√(a²+b²)", "Konwledge_Point": "应对NP完全问题", "Question": "python中abs函数的参数是复数的返回值\n这个abs函数返回的不是绝对值吗？那上面的那个是怎么得到10.0的呀？看不懂它是怎么转化的，求一个解析", "Tag": "算法分析"}
{"Answer": "灰度值拉伸，把 通道 2（如果 RGB 格式则是 蓝色分量 B）的颜色分量 从现有的 [0,maxGray] 拉伸到 [0,255]，是该颜色分量的区分度更大。\n例如，现有图片中的颜色（如蓝色）分量的最大值只有 50，即所有蓝色分量都在 [0,50] 之间，这样很起来蓝色的差异不大，不容易分辨。对其进行线性拉伸，将颜色分量拉伸到 [0,255]，看起来颜色的深浅就会更明显。", "Konwledge_Point": "应对NP完全问题", "Question": "label是json生成的标签图片label.png   请问这段代码是什么意思啊：label = label / np.max(label[:, :, 2]) * 255\nlabel是json生成的标签图片label.png   请问这段代码是什么意思啊：\n\n\n######label = label / np.max(label[:, :, 2]) * 255  ", "Tag": "算法分析"}
{"Answer": "这个代码的问题可能是缺少必要的依赖库，建议先检查是否安装了必要的库，如networkx、matplotlib和numpy等。如果没有安装，请在终端中使用pip install命令安装。\n另外，在运行代码时需要注意，代码中存在一些中文字符，可能会导致编码问题。如果出现编码问题，可以将代码中的中文字符转换为英文字符，或者在文件开头添加# coding=utf-8，以指定编码格式。\n关于如何出现节点图，你可以在代码中添加以下代码来显示节点图：\npos = nx.spring_layout(infected_graph)\nnx.draw_networkx_nodes(infected_graph, pos, node_color='r', node_size=100)\nnx.draw_networkx_edges(infected_graph, pos)\nnx.draw_networkx_labels(infected_graph, pos, font_size=10, font_family='Arial')\nplt.axis('off')\nplt.show()\n\n这段代码使用了networkx库中的spring_layout函数来生成节点的布局，然后使用draw_networkx_nodes、draw_networkx_edges和draw_networkx_labels函数来绘制节点、边和标签，最后使用plt.show()函数显示节点图。请注意，这段代码应该在for循环中的plt.show()函数之后添加。", "Konwledge_Point": "应对NP完全问题", "Question": "遇到这种问题应该如何解决，代码来自xiao黄，但是复制后放到我的编译器中，同时我的import numpy as np，已经安装numpy，但还是灰色状态显示\n想问一下，遇到这种问题应该如何解决\n代码来自xiao黄，但是复制后放到我的编译器中，却运行不了，同时我的import numpy as np，已经安装numpy，但还是灰色状态显示\n\n\n\n\nimport\n random\n\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n networkx \nas\n nx\n\nimport\n numpy \nas\n np\n\nmax_iter_num = \n5\n \n# 模拟的次数\n\nG = nx.karate_club_graph() \n# 空手道俱乐部\n\n\n\nfor\n edge \nin\n G.edges:\n    G.add_edge(edge[\n0\n], edge[\n1\n], weight=random.uniform(\n0\n,\n1\n)) \n# 可不可以作为权值 病毒的感染能力\n\n\nfor\n node \nin\n G:\n    G.add_node(node, state = \n0\n) \n# 用state标识状态 state=0 未激活，state=1 激活\n\n\nseed = \n33\n \n# 选定33作为传染源\n\nG.node[seed][\n'state'\n] = \n1\n \n# 表示33是感染的\n\n\nall_infect_nodes = [] \n# 所有被感染的节点放在这里\n\nall_infect_nodes.append(seed)\n\ninfected_graph = nx.Graph() \n# 被激活的图\n\ninfected_graph.add_node(seed)\n\n\nfor\n i \nin\n \nrange\n(max_iter_num):\n    new_infect = \nlist\n() \n# 新被感染的\n\n    t1 = \n'%s time'\n % i + \n' %s nodes'\n % \nlen\n(all_infect_nodes)\n    \nprint\n(t1) \n# 当前有多少个节点被感染\n\n    \n    \n# 画图\n\n    plt.title(t1)\n    nx.draw(infected_graph, with_labels=\nTrue\n)\n    plt.show()\n\n    \n# 感染的机会不止一次\n\n    \nfor\n v \nin\n all_infect_nodes:\n        \nfor\n nbr \nin\n G.neighbors(v):\n            \nif\n G.node[nbr][\n'state'\n] == \n0\n: \n# 如果这个邻居节点没被感染\n\n                edge_data = G.get_edge_data(v, nbr)\n                \nif\n random.uniform(\n0\n, \n1\n) < edge_data[\n'weight'\n]:\n                    G.node[nbr][\n'state'\n] = \n1\n\n                    new_infect.append(nbr)\n                    infected_graph.add_edge(v, nbr) \n# 画图 添加边\n\n    \n    all_infect_nodes.extend(new_infect) \n# 将新感染的添加到\n\n    \nprint\n(\n'all_active_nodes:'\n, all_infect_nodes)\n\n\n\n\n同时还想请教如何出现节点图，谢谢Thanks♪(･ω･)ﾉ", "Tag": "算法分析"}
{"Answer": "a = [1, 2, 3]\nb = [i*-1 for i in a]", "Konwledge_Point": "应对NP完全问题", "Question": "python列表怎么快速的使其中的元素都乘上（-1）\npython列表怎么快速的使其中的元素都乘上（-1）", "Tag": "算法分析"}
{"Answer": "直接切片，或者用np.hsplit()函数，都可以吧。\n\n>>> import numpy as np\n>>> a = np.random.random((5,20))\n>>> a.shape\n(5, 20)\n>>> a[:,0] # 直接切片示例，这是第0列\narray([0.35594877, 0.23039528, 0.12543397, 0.44093741, 0.87915851])\n>>> b = np.hsplit(a, 20) # 一次性切出20列，返回list\n>>> type(b)\n<class 'list'>\n>>> len(b)\n20\n>>> type(b[0])\n<class 'numpy.ndarray'>\n>>> b[0].shape\n(5, 1)\n>>> b[0]\narray([[0.35594877],\n       [0.23039528],\n       [0.12543397],\n       [0.44093741],\n       [0.87915851]])\n>>> b[0].ravel() # 展平\narray([0.35594877, 0.23039528, 0.12543397, 0.44093741, 0.87915851])", "Konwledge_Point": "应对NP完全问题", "Question": "如何在python中将矩阵按列分割为数个矩阵\n我想将一个矩阵按列分割为不同的矩阵，即从n*20的矩阵变为20个n*1的矩阵。请问要怎么实现呢？\n\n\n\n \n\n\n\n这是我的部分数据，一共有数千行，20列，数据为浮点数\n\n\n", "Tag": "算法分析"}
{"Answer": "data_file_1.readlines()[1:0]这是什么操作，从下标为1的位置开始直到下标为0的位置？？", "Konwledge_Point": "应对NP完全问题", "Question": "python追加扩增数据代码无法写入文件的问题\n\n\n【不报错但却写不进文件】\n\n\n这段目的是使用文件追加扩增数据，数据采样范围在(mean-STDEV, mean+STDEV)。东拼西凑了代码运行后没有报错但却无法写入文件，恳求帮助我改一下代码\n\n\n\n\nimport\n csv\n\nimport\n random\n\nimport\n numpy as np\n\nimport\n pandas as pd\n\n# load the total data CSV file into a list\n\n\ndata_file_1\n = open('RSDS_AI_10.csv', 'rb') \n\ndata_list_1\n = data_file_1.readlines()[\n1\n:\n0\n]\ndata_file_1.close()\n\n# augment training datasets\n\nfor record \nin\n data_list_1:\n    \n# split the record by the ',' commas\n\n    \nrecord\n = record.decode()\n    type(record)\n    \narray\n = record.split(',')\n    \nall_values\n = record.split(',')\n    \narr_std1\n = np.std(float(all_values[\n1\n]))   \n#标准差\n\n    \narr_std2\n = np.std(float(all_values[\n2\n])) \n    \narr_std3\n = np.std(float(all_values[\n3\n])) \n    \narr_std4\n = np.std(float(all_values[\n4\n])) \n    \narr_std5\n = np.std(float(all_values[\n5\n])) \n    \narr_std6\n = np.std(float(all_values[\n6\n]))\n    \narr_std7\n = np.std(float(all_values[\n7\n])) \n    \narr_std8\n = np.std(float(all_values[\n8\n])) \n    \narr_std9\n = np.std(float(all_values[\n9\n])) \n    \narr_std10\n = np.std(float(all_values[\n10\n])) \n    \narr_std11\n = np.std(float(all_values[\n11\n]))\n    \narr_std12\n = np.std(float(all_values[\n12\n]))  \n    \narr_std13\n = np.std(float(all_values[\n13\n])) \n    \narr_std14\n = np.std(float(all_values[\n14\n])) \n    \narr_std15\n = np.std(float(all_values[\n15\n])) \n    \narr_std16\n = np.std(float(all_values[\n16\n])) \n    \narr_std17\n = np.std(float(all_values[\n17\n]))\n    \narr_std18\n = np.std(float(all_values[\n18\n])) \n    \narr_std19\n = np.std(float(all_values[\n19\n])) \n    \narr_std20\n = np.std(float(all_values[\n20\n])) \n    \narr_std21\n = np.std(float(all_values[\n21\n])) \n    \narr_std22\n = np.std(float(all_values[\n22\n]))\n    \narr_std23\n = np.std(float(all_values[\n23\n]))\n    \narr_std24\n = np.std(float(all_values[\n24\n]))  \n    \narr_std25\n = np.std(float(all_values[\n25\n])) \n    \narr_std26\n = np.std(float(all_values[\n26\n])) \n    \narr_std27\n = np.std(float(all_values[\n27\n])) \n    \narr_std28\n = np.std(float(all_values[\n28\n])) \n    \narr_std29\n = np.std(float(all_values[\n29\n]))\n    \narr_std30\n = np.std(float(all_values[\n30\n])) \n    \narr_std31\n = np.std(float(all_values[\n31\n])) \n    \narr_std32\n = np.std(float(all_values[\n32\n])) \n    \narr_std33\n = np.std(float(all_values[\n33\n])) \n    \narr_std34\n = np.std(float(all_values[\n34\n]))\n  \n    \narr_mean1\n = np.mean(float(all_values[\n1\n]))   \n#平均值\n\n    \narr_mean2\n = np.mean(float(all_values[\n2\n])) \n    \narr_mean3\n = np.mean(float(all_values[\n3\n])) \n    \narr_mean4\n = np.mean(float(all_values[\n4\n])) \n    \narr_mean5\n = np.mean(float(all_values[\n5\n])) \n    \narr_mean6\n = np.mean(float(all_values[\n6\n]))\n    \narr_mean7\n = np.mean(float(all_values[\n7\n])) \n    \narr_mean8\n = np.mean(float(all_values[\n8\n])) \n    \narr_mean9\n = np.mean(float(all_values[\n9\n])) \n    \narr_mean10\n = np.mean(float(all_values[\n10\n])) \n    \narr_mean11\n = np.mean(float(all_values[\n11\n]))\n    \narr_mean12\n = np.mean(float(all_values[\n12\n]))  \n    \narr_mean13\n = np.mean(float(all_values[\n13\n])) \n    \narr_mean14\n = np.mean(float(all_values[\n14\n])) \n    \narr_mean15\n = np.mean(float(all_values[\n15\n])) \n    \narr_mean16\n = np.mean(float(all_values[\n16\n])) \n    \narr_mean17\n = np.mean(float(all_values[\n17\n]))\n    \narr_mean18\n = np.mean(float(all_values[\n18\n])) \n    \narr_mean19\n = np.mean(float(all_values[\n19\n])) \n    \narr_mean20\n = np.mean(float(all_values[\n20\n])) \n    \narr_mean21\n = np.mean(float(all_values[\n21\n])) \n    \narr_mean22\n = np.mean(float(all_values[\n22\n]))\n    \narr_mean23\n = np.mean(float(all_values[\n23\n]))\n    \narr_mean24\n = np.mean(float(all_values[\n24\n]))  \n    \narr_mean25\n = np.mean(float(all_values[\n25\n])) \n    \narr_mean26\n = np.mean(float(all_values[\n26\n])) \n    \narr_mean27\n = np.mean(float(all_values[\n27\n])) \n    \narr_mean28\n = np.mean(float(all_values[\n28\n])) \n    \narr_mean29\n = np.mean(float(all_values[\n29\n]))\n    \narr_mean30\n = np.mean(float(all_values[\n30\n])) \n    \narr_mean31\n = np.mean(float(all_values[\n31\n])) \n    \narr_mean32\n = np.mean(float(all_values[\n32\n])) \n    \narr_mean33\n = np.mean(float(all_values[\n33\n])) \n    \narr_mean34\n = np.mean(float(all_values[\n34\n]))\n    \n# build new CSV file to save new data\n\n    \np\n = open('D:fansile.csv', 'ab', \nencoding='utf-8',\n \nnewline='')\n\n    \ncsv_writer\n = csv.writer(p)\n    for i \nin\n range(\n10\n): \n# generate 10 times training data\n\n        \ninput1\n = np.random.uniform(arr_mean1 - arr_std1, arr_mean1 + arr_std1) \n# randomly sampling from (mean-STDEV, mean+STDEV)\n\n        \ninput2\n = np.random.uniform(arr_mean2 - arr_std2, arr_mean2 + arr_std2)\n        \ninput3\n = np.random.uniform(arr_mean3 - arr_std3, arr_mean3 + arr_std3)\n        \ninput4\n = np.random.uniform(arr_mean4 - arr_std4, arr_mean4 + arr_std4)\n        \ninput5\n = np.random.uniform(arr_mean5 - arr_std5, arr_mean5 + arr_std5)\n        \ninput6\n = np.random.uniform(arr_mean6 - arr_std6, arr_mean6 + arr_std6)\n        \ninput7\n = np.random.uniform(arr_mean7 - arr_std7, arr_mean7 + arr_std7)\n        \ninput8\n = np.random.uniform(arr_mean8 - arr_std8, arr_mean8 + arr_std8)\n        \ninput9\n = np.random.uniform(arr_mean9 - arr_std9, arr_mean9 + arr_std9)\n        \ninput10\n = np.random.uniform(arr_mean10 - arr_std10, arr_mean10 + arr_std10)\n        \ninput11\n = np.random.uniform(arr_mean11 - arr_std11, arr_mean11 + arr_std11)\n        \ninput12\n = np.random.uniform(arr_mean12 - arr_std12, arr_mean12 + arr_std12)\n        \ninput13\n = np.random.uniform(arr_mean13 - arr_std13, arr_mean13 + arr_std13)\n        \ninput14\n = np.random.uniform(arr_mean14 - arr_std14, arr_mean14 + arr_std14)\n        \ninput15\n = np.random.uniform(arr_mean15 - arr_std15, arr_mean15 + arr_std15)\n        \ninput16\n = np.random.uniform(arr_mean16 - arr_std16, arr_mean16 + arr_std16)\n        \ninput17\n = np.random.uniform(arr_mean17 - arr_std17, arr_mean17 + arr_std17)\n        \ninput18\n = np.random.uniform(arr_mean18 - arr_std18, arr_mean18 + arr_std18)\n        \ninput19\n = np.random.uniform(arr_mean19 - arr_std19, arr_mean19 + arr_std19)\n        \ninput20\n = np.random.uniform(arr_mean20 - arr_std20, arr_mean20 + arr_std20)\n        \ninput21\n = np.random.uniform(arr_mean21 - arr_std21, arr_mean21 + arr_std21)\n        \ninput22\n = np.random.uniform(arr_mean22 - arr_std22, arr_mean22 + arr_std22)\n        \ninput23\n = np.random.uniform(arr_mean23 - arr_std23, arr_mean23 + arr_std23)\n        \ninput24\n = np.random.uniform(arr_mean24 - arr_std24, arr_mean24 + arr_std24)\n        \ninput25\n = np.random.uniform(arr_mean25 - arr_std25, arr_mean25 + arr_std25)\n        \ninput26\n = np.random.uniform(arr_mean26 - arr_std26, arr_mean26 + arr_std26)\n        \ninput27\n = np.random.uniform(arr_mean27 - arr_std27, arr_mean27 + arr_std27)\n        \ninput28\n = np.random.uniform(arr_mean28 - arr_std28, arr_mean28 + arr_std28)\n        \ninput29\n = np.random.uniform(arr_mean29 - arr_std29, arr_mean29 + arr_std29)\n        \ninput30\n = np.random.uniform(arr_mean30 - arr_std30, arr_mean30 + arr_std30)\n        \ninput31\n = np.random.uniform(arr_mean31 - arr_std31, arr_mean31 + arr_std31)\n        \ninput32\n = np.random.uniform(arr_mean32 - arr_std32, arr_mean32 + arr_std32)\n        \ninput33\n = np.random.uniform(arr_mean33 - arr_std33, arr_mean33 + arr_std33)\n        \ninput34\n = np.random.uniform(arr_mean34 - arr_std34, arr_mean34 + arr_std34)\n        csv_writer.writerow([input1, input2, input3, input4, input5, input6, input7, input8, input9, input10,input11,input12,\n                             input13,input14,input15,input16,input17,input18,input19,input20,input21,input22,input23,input24\n                             ,input25,input26,input27,input28,input29,input30,input31,input32,input33,input34]) \n# decided by your structure of dataset\n\n    p.close()\n\n\n\n\n\n无报错但写不进去\n\n\n问了一个人他说我是把一个文件用两个权限不同的指针去指，每个指针有着不同的操作，不能这样写，需要借助临时文件，但具体怎么做没有说\n\n\n最终想要数据扩增迭代10次，并追加到csv文件中", "Tag": "算法分析"}
{"Answer": "没看到你history输出的代码，感觉曲线图是可信的，而你输出val_loss的时候输出错了。", "Konwledge_Point": "应对NP完全问题", "Question": "使用keras进行分类问题时，验证集loss,accuracy 显示0.0000e+00，但是最后画图像时能显示出验证曲线\ndata_train, data_test, label_train, label_test = train_test_split(data_all, label_all, test_size= 0.2, random_state = 1)\n\n\n\ndata_train, data_val, label_train, label_val = train_test_split(data_train,label_train, test_size = 0.25)\n\n\n\ndata_train = np.asarray(data_train, np.float32)\n\ndata_test = np.asarray(data_test, np.float32)\n\ndata_val = np.asarray(data_val, np.float32)\n\nlabel_train = np.asarray(label_train, np.int32)\n\nlabel_test = np.asarray(label_test, np.int32)\n\nlabel_val = np.asarray(label_val, np.int32)\n\n\n\ntraining = model.fit_generator(datagen.flow(data_train, label_train_binary, batch_size=200,shuffle=True), validation_data=(data_val,label_val_binary), samples_per_epoch=len(data_train)*8, nb_epoch=30, verbose=1)\n\n\n\ndef plot_history(history):\n\n    plt.plot(training.history['acc'])\n\n    plt.plot(training.history['val_acc'])\n\n    plt.title('model accuracy')\n\n    plt.xlabel('epoch')\n\n    plt.ylabel('accuracy')\n\n    plt.legend(['acc', 'val_acc'], loc='lower right')\n\n    plt.show()\n\n    plt.plot(training.history['loss'])\n\n    plt.plot(training.history['val_loss'])\n\n    plt.title('model loss')\n\n    plt.xlabel('epoch')\n\n    plt.ylabel('loss')\n\n    plt.legend(['loss', 'val_loss'], loc='lower right')\n\n    plt.show()\n\n\n\nplot_history(training)\n\n\n", "Tag": "算法分析"}
{"Answer": "你这个变量是在另一个方法里面定义的，你把df在main函数里定义就好了", "Konwledge_Point": "应对NP完全问题", "Question": "用Jupyter出现了name 'df' is not defined的问题\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io as sio\nimport seaborn as sb\nimport pandas as pd\nfrom scipy.io import loadmat#abc\nfrom scipy import stats\nfrom ipykernel import kernelapp as app\nfrom scipy.optimize import minimize\n\n\n\n\n\ndef load_data():\n    df = loadmat('C:/Users/fiq/Desktop/ex5data1.mat')#载入数据\n    return df['X'],df['y'],df['Xval'],df['yval'],df['Xtest'],df['ytest']\n\n\n\n\n\nX,y,Xval,yval,Xtest,ytest = load_data()\n\n\n\n\n\ndef computeCost (X,y,theta):\n    inner=np.power((X*theta.T)-y,2)\n    #theta.T就是矩阵theta的转置矩阵\n    #np.power(A,B)   ## 对A中的每个元素求B次方\n    return np.sum(inner)/(2*len(X))\ndf.insert(0,'ONE',1)  #在第0列插入表头为“ONE”的列，数值为1\n\n\n\n\n\n\ndf.insert(0,'ONE',1)  #在第0列插入表头为“ONE”的列，数值为1\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n in \n----> 1 df.insert(0,'ONE',1)  #在第0列插入表头为“ONE”的列，数值为1\n\nNameError: name 'df' is not defined\n\n\n\n\n这里出现了NameError: name 'df' is not defined\n\n\n\n请问如何解决", "Tag": "算法分析"}
{"Answer": "https://blog.csdn.net/lzdcsdn/article/details/9491677", "Konwledge_Point": "应对NP完全问题", "Question": "关于mpi集群的，在mpirun -np 6 -f nodes ./cpi 出现这样的错误，图示\n", "Tag": "算法分析"}
{"Answer": "试试这样写：\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport pyaudio\n\nCHUNK = 1024\nFORMAT = pyaudio.paInt16\nCHANNELS = 1\nRATE = 44100\n\np = pyaudio.PyAudio()\nstream = p.open(\n    format=FORMAT,\n    channels=CHANNELS,\n    rate=RATE,\n    input=True,\n    output=True,\n    frames_per_buffer=CHUNK\n)\n\nfig, ax = plt.subplots()\n \nx = np.arange(CHUNK)\nline, = ax.plot(x, np.random.rand(CHUNK), lw=1, animated=True)\nax.set_ylim(0, 255)\nax.set_xlim(0, CHUNK)\n\ndef update_data(i):\n    data = stream.read(CHUNK)\n    data = np.frombuffer(data, dtype=np.int16)\n    plt.setp(line, 'ydata', data[:CHUNK])\n    return [line]\n\nani = FuncAnimation(fig, update_data, blit=True, interval=25, frames=1000)\nplt.show()\n\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "接上一个问题，出不来图像\nimport\n struct\n\nimport\n numpy as np\n\nimport\n matplotlib.pyplot as plt\n\nCHUNK\n = \n1024\n * \n4\n\n\nFORMAT\n = pyaudio.paInt16\n\nCHANNELS\n = \n1\n\n\nRATE\n = \n44100\n\n\np\n = pyaudio.PyAudio()\n\nstream\n = p.open(\n    \nformat=FORMAT,\n\n    \nchannels=CHANNELS,\n\n    \nrate=RATE,\n\n    \ninput=True,\n\n    \noutput=True,\n\n    \nframes_per_buffer=CHUNK\n\n)\n\nfig, \nax\n = plt.subplots()\n\n\n\nx\n = np.arange(\n0\n, \n2\n * CHUNK, \n2\n)\nline, = ax.plot(x, np.random.rand(CHUNK))\nax.set_ylim(\n0\n, \n255\n)\nax.set_xlim(\n0\n, CHUNK)\n\nwhile True:\n    \ndata\n = stream.read(CHUNK)\n    \ndata_int\n = np.array(struct.unpack(str(\n2\n * CHUNK) + 'B', data), \ndtype='b')[::2]\n + \n127\n\n    line.set_ydata(data_int)\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "重装tqdm就好，看一下这个要和python版本吻合的", "Konwledge_Point": "应对NP完全问题", "Question": "AttributeError: tqdm的错误，该如何解决？\n问题遇到的现象和发生背景\n\n\n运用了一个网上找的网络，结果出现了 AttributeError\n\n\n问题相关代码，请勿粘贴截图\n\n\n  File \"E:\\python\\lib\\site-packages\\tqdm\n_tqdm.py\", line 835, in \ndel\n    self.close()\n  File \"E:\\python\\lib\\site-packages\\tqdm\n_tqdm.py\", line 1053, in close\n    pos = self.pos\nAttributeError: 'tqdm' object has no attribute 'pos'\n\n\n运行结果及报错内容\n\n\n我的解答思路和尝试过的方法\n\n\n尝试重装了tqdm包，但也没达到想要的效果\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "可能是pandas版本的问题，get不到版本属性。你可以在pycharm用的python环境中重新安装以下pandas试一试。", "Konwledge_Point": "应对NP完全问题", "Question": "一段机器学习的Python代码，放在Jupyter Notebook上运行没什么问题，但是放在PyCharm上就报错了。\n一段机器学习的代码，具体是回归方面的：\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nimport matplotlib.pyplot as plt\nplt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\nplt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n\n\npath='data/regress_data1.csv'\ndata=pd.read_csv(path)\ndata.head()\ndata.describe()\n\n\n这是Jupyter Notebook上的运行结果\n\n\n但是放在PyCharm上，就出现了下面的错误：\n\n\n\n\n值得一提的是，我已经在PyCharm中配置了机器学习的环境：\n我搜了搜解决方法，和我这个相关的是说Python文件命名的问题。我看了看我也没用Python的保留字，命名也符合规范。\n\n\n所以就僵住了好久。\n求指点！", "Tag": "算法分析"}
{"Answer": "错在这行，y_distance = ([0, 1, 2, 3, 4])，这个是一个列表，应该是y_distance = choice([0, 1, 2, 3, 4])", "Konwledge_Point": "应对NP完全问题", "Question": "python报错unsupported operand type for+int and list\n求问python使用matplotlib时遇到的问题\n\n\n\n\n\nfrom random import choice\n\n\n\nclass RandomWalk:\n\t\"\"\"一个随机生成漫步数据的类\"\"\"\n\tdef __init__(self, num_points=5000):\n\t\t\"\"\"初始化要漫步的属性\"\"\"\n\t\tself.num_points = num_points\n\n\t\t#所有随机漫步都始于(0,0)\n\t\tself.x_values = [0]\n\t\tself.y_values = [0]\n\n\tdef fill_walk(self):\n\t\t\"\"\"计算随机漫步包含的所有点\"\"\"\n\n\t\t#不断漫步直到列表达到指定的长度\n\t\twhile len(self.x_values) < self.num_points:\n\n\t\t\t#决定前进方向以及沿着这个方向前进的距离\n\t\t\tx_direction = choice([1, -1])\n\n\t\t\tx_distance = choice([0, 1, 2, 3, 4])\n\t\t\tx_step = x_direction * x_distance\n\n\t\t\ty_direction = choice([1, -1])\n\t\t\ty_distance = ([0, 1, 2, 3, 4])\n\t\t\ty_step = y_direction * y_distance\n\n\t\t\t#拒绝原地踏步\n\t\t\tif x_step == 0 and y_step == 0:\n\t\t\t\tcontinue\n\n\t\t\t#计算下一个点的x和y值\n\t\t\tx = self.x_values[-1] + x_step\n\t\t\ty = self.y_values[-1] + y_step\n\n\t\t\tself.x_values.append(x)\n\t\t\tself.y_value.append(y)\n\n\n\n\n 调用上图代码\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfrom aaaaaa import RandomWalk\n\nrw = RandomWalk()\nrw.fill_walk()\n\nplt.style.use('classic')\nfig, ax = plt.subplots()\nax.scatter(rw.x_values, rw.y_values, s=15)\n\nplt.show()\n\n\n\n运行出现Traceback (most recent call last):\n\n  File \"C:\\Users\\86199\\Desktop\\rw_visual.py\", line 6, in \n\n    rw.fill_walk()\n\n  File \"C:\\Users\\86199\\Desktop\\aaaaaa.py\", line 37, in fill_walk\n\n    y = self.y_values[-1] + y_step\n\nTypeError: unsupported operand type(s) for +: 'int' and 'list'", "Tag": "算法分析"}
{"Answer": "用print(pyglet.version)检查一下pyglet版本，使用1.5.19版本测试题中代码运行正常，无题中的报错信息。尝试更改pyglet版本。如有帮助，请采纳。", "Konwledge_Point": "应对NP完全问题", "Question": "pyglet批处理添加中出现的问题。\n错误：\n\n\nTraceback (most recent call last):\n  File \n\"D:\\SoftWare\\AI\\Python3.7\\lib\\site-packages\\pyglet\\graphics\\__init__.py\"\n, line 511, \nin\n _get_domain\n    domain = domain_map[key]\nKeyError: ((\n'v2f'\n, \n'c3B'\n), 7, \nFalse\n, 4)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \n\"D:/SoftWare/Project/Python/robot_arm/env/env.py\"\n, line 148, \nin\n \n    env.render()\n  File \n\"D:/SoftWare/Project/Python/robot_arm/env/env.py\"\n, line 53, \nin\n render\n    self.viewer = Viewer(self.arm_info, self.goal)\n  File \n\"D:/SoftWare/Project/Python/robot_arm/env/env.py\"\n, line 85, \nin\n __init__\n    (\n'c3B'\n, (86, 109, 249)\n*4\n))  # color\n  File \n\"D:\\SoftWare\\AI\\Python3.7\\lib\\site-packages\\pyglet\\graphics\\__init__.py\"\n, line 425, \nin\n \nadd\n\n    domain = self._get_domain(\nFalse\n, mode, group, formats)\n  File \n\"D:\\SoftWare\\AI\\Python3.7\\lib\\site-packages\\pyglet\\graphics\\__init__.py\"\n, line 514, \nin\n _get_domain\n    domain = vertexdomain.create_domain(shader_program, \n*f\normats, \nindexed\n=indexed)\n  File \n\"D:\\SoftWare\\AI\\Python3.7\\lib\\site-packages\\pyglet\\graphics\\vertexdomain.py\"\n, line 131, \nin\n create_domain\n    attribute_usages = [create_attribute_usage(shader_program, f) \nfor\n f \nin\n attribute_usage_formats]\n  File \n\"D:\\SoftWare\\AI\\Python3.7\\lib\\site-packages\\pyglet\\graphics\\vertexdomain.py\"\n, line 131, \nin\n \n    attribute_usages = [create_attribute_usage(shader_program, f) \nfor\n f \nin\n attribute_usage_formats]\n  File \n\"D:\\SoftWare\\AI\\Python3.7\\lib\\site-packages\\pyglet\\graphics\\vertexdomain.py\"\n, line 113, \nin\n create_attribute_usage\n    attribute = vertexattribute.create_attribute(shader_program, attribute_format)\n  File \n\"D:\\SoftWare\\AI\\Python3.7\\lib\\site-packages\\pyglet\\graphics\\vertexattribute.py\"\n, line 225, \nin\n create_attribute\n    assert attribute_meta, f\n\"No '{name}' attribute found in {shader_program}.\\n\"\n\\\nAssertionError: \nNo\n \n'v'\n attribute found \nin\n ShaderProgram(\nid\n=4).\nValid attibutes are: {\n'colors'\n: Attribute(\n'colors'\n, \nprogram\n=4, \nlocation\n=1, \ncount\n=4, \nformat\n=f), \n'position'\n: Attribute(\n'position'\n, \nprogram\n=4, \nlocation\n=0, \ncount\n=3, \nformat\n=f), \n'tex_coords'\n: Attribute(\n'tex_coords'\n, \nprogram\n=4, \nlocation\n=2, \ncount\n=3, \nformat\n=f)}\n\n\n\n\n源代码为下\n\n\n\n\n\n\n\n\nimport\n pyglet\n\nimport\n numpy \nas\n np\n\n\nclass\n \nArmEnv\n(\nobject\n):\n    viewer = \nNone\n\n    dt = \n0.1\n                                \n# 转动的速度和 dt 有关\n\n    action_bound = [-\n1\n, \n1\n]                  \n# 转动的角度范围\n\n    goal = {\n'x'\n: \n100.\n, \n'y'\n: \n100.\n, \n'l'\n: \n40\n}  \n# 蓝色 goal 的 x,y 坐标和长度 l\n\n    state_dim = \n2\n                           \n# 两个观测值 两个角度\n\n    action_dim = \n2\n                          \n# 两个动作 两个关节\n\n\n    \ndef\n \n__init__\n(\nself\n):\n        self.arm_info = np.zeros(\n            \n2\n, dtype=[(\n'l'\n, np.float32), (\n'r'\n, np.float32)])\n        \n# 生成出 (2,2) 的矩阵\n\n        self.arm_info[\n'l'\n] = \n100\n        \n# 两段手臂都 100 长\n\n        self.arm_info[\n'r'\n] = np.pi/\n6\n    \n# 两段手臂的端点角度\n\n\n    \ndef\n \nstep\n(\nself, action\n):\n        done = \nFalse\n\n        r = \n0.\n\n\n        \n# 计算单位时间 dt 内旋转的角度, 将角度限制在360度以内\n\n        action = np.clip(action, *self.action_bound)\n        self.arm_info[\n'r'\n] += action * self.dt\n        self.arm_info[\n'r'\n] %= np.pi * \n2\n  \n# normalize\n\n\n        \n# 我们可以将两截手臂的角度信息当做一个 state (之后会变)\n\n        s = self.arm_info[\n'r'\n]\n\n        \n# 如果手指接触到蓝色的 goal, 我们判定结束回合 (done)\n\n        \n# 所以需要计算 finger 的坐标\n\n        (a1l, a2l) = self.arm_info[\n'l'\n]  \n# radius, arm length\n\n        (a1r, a2r) = self.arm_info[\n'r'\n]  \n# radian, angle\n\n        a1xy = np.array([\n200.\n, \n200.\n])  \n# a1 start (x0, y0)\n\n        a1xy_ = np.array([np.cos(a1r), np.sin(a1r)]) * a1l + a1xy  \n# a1 end and a2 start (x1, y1)\n\n        finger = np.array([np.cos(a1r + a2r), np.sin(a1r + a2r)]) * a2l + a1xy_  \n# a2 end (x2, y2)\n\n\n        \n# 根据 finger 和 goal 的坐标得出 done and reward\n\n        \nif\n self.goal[\n'x'\n] - self.goal[\n'l'\n] / \n2\n < finger[\n0\n] < self.goal[\n'x'\n] + self.goal[\n'l'\n] / \n2\n:\n            \nif\n self.goal[\n'y'\n] - self.goal[\n'l'\n] / \n2\n < finger[\n1\n] < self.goal[\n'y'\n] + self.goal[\n'l'\n] / \n2\n:\n                done = \nTrue\n\n                r = \n1.\n  \n# finger 在 goal 以内\n\n        \nreturn\n s, r, done\n\n    \ndef\n \nreset\n(\nself\n):\n        \n# reset转角信息\n\n        self.arm_info[\n'r'\n] = \n2\n * np.pi * np.random.rand(\n2\n)\n        \nreturn\n self.arm_info[\n'r'\n]\n\n    \ndef\n \nrender\n(\nself\n):\n        \nif\n self.viewer \nis\n \nNone\n:\n            self.viewer = Viewer(self.arm_info, self.goal)\n        self.viewer.render()\n\n    \ndef\n \nsample_action\n(\nself\n):\n        \nreturn\n np.random.rand(\n2\n)-\n0.5\n    \n# two radians\n\n\n\n\nclass\n \nViewer\n(pyglet.window.Window):\n    bar_thc = \n5\n     \n# 手臂的厚度\n\n\n    \ndef\n \n__init__\n(\nself, arm_info, goal\n):\n        \n# 创建窗口的继承\n\n        \n# vsync 如果是 True, 按屏幕频率刷新, 反之不按那个频率\n\n        \nsuper\n(Viewer, self).__init__(width=\n400\n, height=\n400\n, resizable=\nFalse\n, caption=\n'Arm'\n, vsync=\nFalse\n)\n\n        \n# 窗口背景颜色\n\n        pyglet.gl.glClearColor(\n1\n, \n1\n, \n1\n, \n1\n)\n\n        \n# 添加 arm 信息\n\n        self.arm_info = arm_info\n        \n# 添加窗口中心点, 手臂的根\n\n        self.center_coord = np.array([\n200\n, \n200\n])\n        \n# 将手臂的作图信息放入这个 batch\n\n        self.batch = pyglet.graphics.Batch()  \n# display whole batch at once\n\n\n        \n# 蓝色 goal 的信息包括他的 x, y 坐标, goal 的长度 l\n\n        self.point = self.batch.add(\n            \n4\n, pyglet.gl.GL_QUADS, \nNone\n,  \n# 4 corners\n\n            (\n'v2f'\n, [goal[\n'x'\n] - goal[\n'l'\n] / \n2\n, goal[\n'y'\n] - goal[\n'l'\n] / \n2\n,\n                     goal[\n'x'\n] - goal[\n'l'\n] / \n2\n, goal[\n'y'\n] + goal[\n'l'\n] / \n2\n,\n                     goal[\n'x'\n] + goal[\n'l'\n] / \n2\n, goal[\n'y'\n] + goal[\n'l'\n] / \n2\n,\n                     goal[\n'x'\n] + goal[\n'l'\n] / \n2\n, goal[\n'y'\n] - goal[\n'l'\n] / \n2\n]),\n            (\n'c3B'\n, (\n86\n, \n109\n, \n249\n)*\n4\n))  \n# color\n\n\n        \n# 添加一条手臂\n\n        self.arm1 = self.batch.add(\n            \n4\n, pyglet.gl.GL_QUADS, \nNone\n,\n            (\n'v2f'\n, [\n250\n, \n250\n,              \n# 同上, 点信息\n\n                     \n250\n, \n300\n,\n                     \n260\n, \n300\n,\n                     \n260\n, \n250\n]),\n            (\n'c3B'\n, (\n249\n, \n86\n, \n86\n) * \n4\n,))    \n# color\n\n\n        \n# 按理添加第二条手臂...\n\n        self.arm2 = self.batch.add(\n            \n4\n, pyglet.gl.GL_QUADS, \nNone\n,\n            (\n'v2f'\n, [\n100\n, \n250\n,  \n# 同上, 点信息\n\n                     \n100\n, \n160\n,\n                     \n200\n, \n160\n,\n                     \n200\n, \n150\n]),\n            (\n'c3B'\n, (\n249\n, \n86\n, \n86\n) * \n4\n,))  \n# color\n\n\n\n\n    \ndef\n \nrender\n(\nself\n):\n        self._update_arm()  \n# 更新手臂内容 (暂时没有变化)\n\n        self.switch_to()\n        self.dispatch_events()\n        self.dispatch_event(\n'on_draw'\n)\n        self.flip()\n\n    \ndef\n \non_draw\n(\nself\n):\n        self.clear()        \n# 清屏\n\n        self.batch.draw()   \n# 画上 batch 里面的内容\n\n\n    \ndef\n \n_update_arm\n(\nself\n):\n        \n# 更新手臂的位置信息\n\n        (a1l, a2l) = self.arm_info[\n'l'\n]  \n# radius, arm length\n\n        (a1r, a2r) = self.arm_info[\n'r'\n]  \n# radian, angle\n\n        a1xy = self.center_coord  \n# a1 start (x0, y0)\n\n        a1xy_ = np.array([np.cos(a1r), np.sin(a1r)]) * a1l + a1xy  \n# a1 end and a2 start (x1, y1)\n\n        a2xy_ = np.array([np.cos(a1r + a2r), np.sin(a1r + a2r)]) * a2l + a1xy_  \n# a2 end (x2, y2)\n\n\n        \n# 第一段手臂的4个点信息\n\n        a1tr, a2tr = np.pi / \n2\n - self.arm_info[\n'r'\n][\n0\n], np.pi / \n2\n - self.arm_info[\n'r'\n].\nsum\n()\n        xy01 = a1xy + np.array([-np.cos(a1tr), np.sin(a1tr)]) * self.bar_thc\n        xy02 = a1xy + np.array([np.cos(a1tr), -np.sin(a1tr)]) * self.bar_thc\n        xy11 = a1xy_ + np.array([np.cos(a1tr), -np.sin(a1tr)]) * self.bar_thc\n        xy12 = a1xy_ + np.array([-np.cos(a1tr), np.sin(a1tr)]) * self.bar_thc\n\n        \n# 第二段手臂的4个点信息\n\n        xy11_ = a1xy_ + np.array([np.cos(a2tr), -np.sin(a2tr)]) * self.bar_thc\n        xy12_ = a1xy_ + np.array([-np.cos(a2tr), np.sin(a2tr)]) * self.bar_thc\n        xy21 = a2xy_ + np.array([-np.cos(a2tr), np.sin(a2tr)]) * self.bar_thc\n        xy22 = a2xy_ + np.array([np.cos(a2tr), -np.sin(a2tr)]) * self.bar_thc\n\n        \n# 将点信息都放入手臂显示中\n\n        self.arm1.vertices = np.concatenate((xy01, xy02, xy11, xy12))\n        self.arm2.vertices = np.concatenate((xy11_, xy12_, xy21, xy22))\n\n\nif\n __name__ == \n\"__main__\"\n:\n    env = ArmEnv()\n    \nwhile\n \nTrue\n:\n        s = env.reset()\n        \nfor\n i \nin\n \nrange\n(\n40\n):\n            env.render()\n            env.step(env.sample_action())\n", "Tag": "算法分析"}
{"Answer": "你这个其实近似是单位矩阵了，因为e-17近似为0，之所以出现这个现象，主要原因的话，应该是数据精度的问题，用decimal试一下。\n望点一下下采纳", "Konwledge_Point": "应对NP完全问题", "Question": "python中矩阵与其逆相乘并不是单位矩阵\n如下图，将矩阵的逆与矩阵相乘，并没有得到单位矩阵\n\n\n\n\n这行代码是照书上敲的，但是书上跑出来又是单位矩阵，请问这是为啥呀？\n\n", "Tag": "算法分析"}
{"Answer": "你执行的是1021.py这个脚本，这里面只是定义了函数，并没有调用执行函数，肯定不会显示图片啊", "Konwledge_Point": "应对NP完全问题", "Question": "关于#numpy#的问题，如何解决？\n想做频谱变换，代码都没问题，但是就是不出图，想不明白哪里错了\n\n\nimport\n cv2 \nas\n cv\n\nimport\n numpy \nas\n np\n\nfrom\n matplotlib \nimport\n pyplot \nas\n plt\n\n\n# 显示中文\n\nplt.rcParams[\n'font.family'\n] = [\n'sans-serif'\n]\nplt.rcParams[\n'font.sans-serif'\n] = [\n'SimHei'\n]\n\nnp.seterr(divide=\n'ignore'\n, invalid=\n'ignore'\n)\nimg = cv.imread(\n'high.jpg'\n, \n0\n)  \n# 读入第一张图片\n\nimg2 = cv.imread(\n'low.jpg'\n, \n0\n)  \n# 读入第二张图片\n\nf = np.fft.fft2(img, axes=(\n0\n, \n1\n))\nfshift = np.fft.fftshift(f)\nres = np.log(np.\nabs\n(fshift))  \n# 幅度谱\n\nag = np.angle(fshift)  \n# 相位谱\n\n\nishift1 = np.fft.ifftshift(res)  \n# 利用幅度谱逆变化\n\niimg1 = np.fft.ifft2(ishift1)\niimg1 = np.\nabs\n(iimg1)\n\nishift = np.fft.ifftshift(ag)  \n# 利用相位谱逆变换\n\niimg = np.fft.ifft2(ishift)\niimg = np.\nabs\n(iimg)\n\nishift2 = np.fft.ifftshift(fshift)  \n# 整体逆变换\n\niimg2 = np.fft.ifft2(ishift2)\niimg2 = np.\nabs\n(iimg2)\n\nf2 = np.fft.fft2(iimg2, axes=(\n0\n, \n1\n))\nfshift2 = np.fft.fftshift(f2)\nres2 = np.log(np.\nabs\n(fshift2))  \n# 幅度谱\n\nag2 = np.angle(fshift2)  \n# 相位谱\n\n\n\n\n# 分离\n\n\ndef\n \nmagnitude_phaes_split\n(\nimg\n):\n    \n# 分离幅度谱与相位谱\n\n    dft = np.fft.fft2(img)\n    dft_shift = np.fft.fftshift(dft)\n    \n# 幅度谱\n\n    magnitude_spectrum = np.bs(dft_shift)\n    \n# 相位谱\n\n    phase_spectrum = np.angle(dft_shift)\n    \nreturn\n magnitude_spectrum, phase_spectrum\n\n\n\n# 交换相位\n\n\ndef\n \nmagnitude_phase_combine\n(\nimg_m, img_p\n):\n    \n# 幅度谱与相位谱结合\n\n    img_mandp = img_m * np.e ** (\n1j\n * img_p)\n    img_mandp = np.uint8(np.\nabs\n(np.fft.ifft2(img_mandp)))\n    img_mandp = img_mandp / np.\nmax\n(img_mandp) * \n225\n\n    \nreturn\n img_mandp\n    img1_m, img1_p = magnitude_phase_split(img)\n    img2_m, img2_p = magnitude_phase_split(img2)\n    \n# 将图像1的幅度谱与图像2的相位谱结合\n\n    img_1mAnd2p = magnitude_phaes_combine(img1_m, img1_p)\n    \n# 将图像2的幅度谱与图像1的相位谱结合\n\n    img_2mAnd1p = magnitude_phase_combine(img2_m, img1_p)\n\n    plt.subplot(\n421\n), plt.imshow(img, \n'gray'\n), plt.title(\n'原图'\n)  \n# 原图\n\n    plt.axis(\n'off'\n)\n    plt.subplot(\n422\n), plt.imshow(res, \n'grey'\n), plt.title(\n'幅度谱变换'\n)  \n# 幅度谱变换\n\n    plt.axis(\n'off'\n)\n    plt.subplot(\n423\n), plt.imshow(ag, \n'grey'\n), plt.title(\n'相位谱变换'\n)  \n# 相位谱变换\n\n    plt.axis(\n'off'\n)\n    plt.subplot(\n424\n), plt.imshow(iimg, \n'gray'\n), plt.title(\n'相位谱逆变换'\n)  \n# 相位谱逆变换\n\n    plt.axis(\n'off'\n)\n    plt.subplot(\n425\n), plt.imshow(iimg1, \n'gray'\n), plt.title(\n'幅度谱逆变换'\n)  \n# 幅度谱逆变换\n\n    plt.axis(\n'off'\n)\n    plt.subplot(\n426\n), plt.imshow(iimg2, \n'gray'\n), plt.title(\n'整体逆变换'\n)  \n# 整体逆变换\n\n    plt.axis(\n'off'\n)\n    plt.subplot(\n427\n), plt.imshow(img_1mAnd2p, \n'gray'\n), plt.title(\n'1幅度加2相位'\n)  \n# 1幅度加2相位\n\n    plt.axis(\n'off'\n)\n    plt.subplot(\n428\n), plt.imshow(img_2mAnd1p, \n'gray'\n), plt.title(\n'2幅度加1相位'\n)  \n# 2幅度加1相位\n\n    plt.axis(\n'off'\n)\n    plt.show()\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "把代码中这句plt.xticks(rotation=50)改成下面，\r\n\r\n```\r\n for xtick in ax1.get_xticklabels():\r\n    xtick.set_rotation(50)\r\n```\r\nx轴的坐标就rotate了", "Konwledge_Point": "应对NP完全问题", "Question": "matplotlib共用x轴标签旋转问题\n目标是把这个图的x轴标签数字全部旋转90度。\n\n\n\n import numpy as np\nimport matplotlib.pyplot as plt\n\nt = np.arange(0.01, 10.0, 0.01)\ndata1 = np.exp(t)\ndata2 = np.sin(2 * np.pi * t)\n\nfig, ax1 = plt.subplots()\n\ncolor = 'tab:red'\nax1.set_xlabel('time (s)')\nax1.set_ylabel('exp', color=color)\nax1.plot(t, data1, color=color)\nax1.tick_params(axis='y', labelcolor=color)\n\nax2 = ax1.twinx()  \n\ncolor = 'tab:blue'\nax2.set_ylabel('sin', color=color)  \nax2.plot(t, data2, color=color)\nax2.tick_params(axis='y', labelcolor=color)\nplt.xticks(rotation=50) #如果不是共用轴，就可以设置。现在共用X轴就无法使用xticks的rotation来对x周标签旋转\nfig.tight_layout()  \nplt.show()\n", "Tag": "算法分析"}
{"Answer": "你要明白c == 3表示什么，这里c = [3,6] ，那么c == 3 的结果是[True, False]，所以b[c==3] 等价于 b[[True, False]]，结果嘛很明显了", "Konwledge_Point": "应对NP完全问题", "Question": "python中numpy有问题不懂，求帮助？\nimport numpy as np\na=np.array([(1,2,3),(4,5,6)])\nb=a[:,0:2]\nc=a[:,2]\nd=b[c==3]\n\n\n\n\n这是结果\n\n\n\n\n\n\n我不懂b这个矩阵里都没有c这列，它是怎么做到输出结果的", "Tag": "算法分析"}
{"Answer": "如果传给np.dstack()的参数是一个空的列表或元组，就会抛出题主给出的错误信息。建议在数组深度合并前，检查一下X的长度，大概率是空的。\n\n>>> import numpy as np\n>>> np.dstack([])\nTraceback (most recent call last):\n  File \"<pyshell#65>\", line 1, in <module>\n    np.dstack([])\n  File \"<__array_function__ internals>\", line 6, in dstack\n  File \"C:\\Users\\xufive\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\lib\\shape_base.py\", line 723, in dstack\n    return _nx.concatenate(arrs, 2)\n  File \"<__array_function__ internals>\", line 6, in concatenate\nValueError: need at least one array to concatenate\n ", "Konwledge_Point": "应对NP完全问题", "Question": "python :need at least one array to concatenate\n#!/usr/bin/python\n\n#-*- coding:cp936 -*-\n\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport os\n\ndef load_file(filepath):\n\n    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n\n    return dataframe.values\n\n\n\ndef load_dataset(data_rootdir, dirname, group):\n\n    '''\n\n    该函数实现将训练数据或测试数据文件列表堆叠为三维数组\n\n    '''\n\n    filename_list = []\n\n    filepath_list = []\n\n    X = []\n\n    \n\n    # os.walk() 方法是一个简单易用的文件、目录遍历器，可以高效的处理文件、目录。\n\n    for rootdir, dirnames, filenames in os.walk(data_rootdir + dirname):\n\n        for filename in filenames:\n\n            filename_list.append(filename)\n\n            filepath_list.append(os.path.join(rootdir, filename))\n\n        #print(filename_list)\n\n        #print(filepath_list)\n\n    \n\n    # 遍历根目录下的文件，并读取为DataFrame格式；\n\n    for filepath in filepath_list:\n\n        X.append(load_file(filepath))\n\n    \n\n    X = np.dstack(X) # dstack沿第三个维度叠加，两个二维数组叠加后，前两个维度尺寸不变，第三个维度增加；\n\n    y = load_file(data_rootdir+'/y_'+group+'.txt')\n\n    print('{}_X.shape:{},{}_y.shape:{}\\n'.format(group,X.shape,group,y.shape))\n\n    return X, y\n\n\n\ntrain_rootdir = 'F:/桌面/毕设/python/wanzheng/data/train/'\n\ntest_rootdir = 'F:/桌面/毕设/python/wanzheng/data/test/'\n\ndata_dirname = '/Inertial Signals/'\n\ntrainX, trainy = load_dataset(train_rootdir, data_dirname, 'train')\n\ntestX, testy = load_dataset(test_rootdir, data_dirname, 'test')\n\n\n\n \n\n\n\n \n\n\n", "Tag": "算法分析"}
{"Answer": "此类问题要么单步自己调试，要么做成可下载工程。。\r\n\r\n问答已经把代码给完全破坏了，代码也太长了。。。码农都习惯编译 --调试---解决BUG！！！！", "Konwledge_Point": "应对NP完全问题", "Question": "c++openmp问题 程序正常运行可以，放在openmp中不运行！！急急急\n#pragma hdrstop\n\n\n\n//---------------------------------------------------------------------------\n\n\n\n//#pragma argsused\n\n#include \n\n#include \n\n#include \n\n#include \n\n#include \n\n#include\n\n//#include \n\n#define PI 3.1415926535\n\n#define e 2.718281828\n\n\n\nfloat \nspace2d(int nr, int nc);\n\nfloat ***space3d(int nr, int ny, int nc);\n\nvoid free_space2d(float **a, int nr);\n\nvoid free_space3d(float ***b, int nr, int ny);\n\nvoid wfile(char filename[], float **data, int nr, int nc);\n\nvoid wfile3d(char filename[], float \n**data, int nr, int ny, int nc);\n\nvoid wfile4d(char filename[], float *\n*data, int nr, int ny, int nc);\n\nvoid create_model(float \n**vp, float *\n*vs, float \n**rho, float *\n*vf, float \n**rhof, float *\n*lamda, float \n**lamda2u, float *\n*mu, float\n*por, int nr, int ny, int nc);\n\n\n\nfloat \n**extmodel(float *\n*init_model, int nr, int ny, int nc, int np);\n\n\n\nint main()\n\n{\n\n    //给定参数\n\n\n\nint NX = 100;       //x方向网格点数%%%%%\nint NY = 100;     //y方向网格点数%%%%%\nint NZ = 300;       //z方向网格点数%%%%\nint NP = 20;        //pml层网格点数\nint i = 0;\n\nint sx = NX / 2 + NP;           //震源坐标点号\nint sy = NY / 2 + NP;\nint sz = NZ /6 + NP;\n\nint NX_ext = NX + 2 * NP;    //\nint NY_ext = NY + 2 * NP;\nint NZ_ext = NZ + 2 * NP;\n\nint NT = 8000;       //空间步长5%%%\nfloat H = 0.01f;\nfloat RC = 0.000001f;\nfloat DP = NP*H;\n\n//  double DT=0.0002;       //时间步长\ndouble DT = 1.0*pow(10, -6);        //时间步长%%%%\n                                    //  double F0=30.0;     //震源主频\ndouble F0 = 3.0*pow(10, 3);     //震源主频,主频太高会造成频散\n                                    //double T0=1.2/F0;  \nfloat T0 = 1.0 / F0;\ndouble Vpmax = 4270.0;    //模型最大纵波速度,用于稳定性计算\ndouble Vpmin = 1500.0;     //模型最小纵波速度,用于控制数值频散\ndouble Vsmax = 2650.0;\ndouble Vsmin = 1500.0;\n\nfloat ***vs;        //初始模型横波速度\nfloat ***vp;        //初始模型纵波速度\nfloat ***rho;      //初始模型密度\nfloat ***vf;        //孔隙流体速度\nfloat ***rhof;  //孔隙流体密度\nfloat ***mu;        //剪切模量\nfloat ***lamda;  //拉梅系数\nfloat ***lamda2u;\n//float **Q;\n//float **R;\nfloat ***por;\n//  float **D11,**D12,**D22;\n//  float **b;\nfloat **sis_x;  //地震记录x分量     波形记录-二维数据\nfloat **sis_y;  //地震记录y分量\nfloat **sis_z;  //地震记录z分量\n\n\n\nfloat ***vx;\nfloat ***vxx;\n\nfloat ***vy;\n\nfloat ***vz;\n/*float **Vx_x;//流相速度\nfloat **Vx_z;\nfloat **Vx;\nfloat **Vz_x;\nfloat **Vz_z;\nfloat **Vz;\n*/\n\nfloat ***txx;//应力分量\n\nfloat ***tyy;//应力分量\n\n\nfloat ***tzz;\n\nfloat ***txy;\nfloat ***txz;\nfloat ***tyz;\nfloat ***source;\n/*float **ssx;//流体应力\nfloat **ssz;\nfloat **dxi,**dxi2;\nfloat **dzj,**dzj2;\n*/\n//  double Kb,Ks,Kf,a,D;\nfloat tt, x, y, z, xoleft, xoright, yoleft, yoright, zoleft, zoright, d0=0.0f;\nfloat v0, y5, y6 , y7 , y8, y9 , y10, y11, y12, y13, rho_tempx, rho_tempy, rho_tempz, muxz, muxy, muyz, lamda2u_temp, lamda_temp;\n//  double D11_tempx,D12_tempx,D22_tempx,R_temp,D11_tempz,D12_tempz,D22_tempz;\nfloat xx1, xy1, xz1, yx2 , yy2, yz2 , zx3, zy3, zz3 , best_dt;\nint ix, iy, iz, it;\n\n\nvs = space3d(NZ, NY, NX);\nvp = space3d(NZ, NY, NX);\nrho = space3d(NZ, NY, NX);\nvf = space3d(NZ, NY, NX);\nrhof = space3d(NZ, NY, NX);\n//Q=space2d(NZ,NX);\n//R=space2d(NZ,NX);\npor = space3d(NZ, NY, NX);\n//D11=space2d(NZ,NX);D12=space2d(NZ,NX);D22=space2d(NZ,NX);\n//  b=space2d(NZ,NX);\nmu = space3d(NZ, NY, NX);\nlamda = space3d(NZ, NY, NX);\nlamda2u = space3d(NZ, NY, NX);\ncreate_model(vp, vs, rho, vf, rhof, lamda, lamda2u, mu, por, NZ, NY, NX);\n\nchar vp_name[] = \"vp_ext.dat\";\nfloat ***vs_ext;\nfloat ***vp_ext;\nfloat ***rho_ext;\nfloat ***mu_ext;\nfloat ***lamda_ext;\nfloat ***lamda2u_ext;\n// float ***Q_ext;\n//  float ***R_ext;\nfloat ***vf_ext;\nfloat ***rhof_ext;\nfloat ***por_ext;\nfloat ***dxi;\nfloat ***dxi2;\nfloat ***dyj;\nfloat ***dyj2;\nfloat ***dzk;\nfloat ***dzk2;\n//float ***b_ext;\n//  float **D11_ext,**D12_ext,**D22_ext;\n\n\n//数据扩充，加上吸收边界\nvs_ext = extmodel(vs, NZ, NY, NX, NP);\nvp_ext = extmodel(vp, NZ, NY, NX, NP);\nrho_ext = extmodel(rho, NZ, NY, NX, NP);\nmu_ext = extmodel(mu, NZ, NY, NX, NP);\nlamda_ext = extmodel(lamda, NZ, NY, NX, NP);\nlamda2u_ext = extmodel(lamda2u, NZ, NY, NX, NP);\nvf_ext = extmodel(vf, NZ, NY, NX, NP);\nrhof_ext = extmodel(rhof, NZ, NY, NX, NP);\n//Q_ext=extmodel(Q,NZ,NX,NP);\n//R_ext=extmodel(R,NZ,NX,NP);\npor_ext = extmodel(por, NZ, NY, NX, NP);\n//  b_ext=extmodel(b,NZ,NX,NP);\n//  D11_ext=extmodel(D11,NZ,NX,NP);D12_ext=extmodel(D12,NZ,NX,NP);\n//  D22_ext=extmodel(D22,NZ,NX,NP);\n\nwfile3d(vp_name, vp_ext, NZ_ext, NY_ext, NX_ext);  //建立文件\n\n                                                   //申请空间\n\nvx = space3d(NZ_ext, NY_ext, NX_ext);\n\n\nvy = space3d(NZ_ext, NY_ext, NX_ext);\nvxx= space3d(NT/200,NZ_ext/2,  NX_ext/2);\n\nvz = space3d(NZ_ext, NY_ext, NX_ext);\n/*Vx_x=space2d(NZ_ext,NX_ext);\nVx_z=space2d(NZ_ext,NX_ext);\nVx=space2d(NZ_ext,NX_ext);\nVz_x=space2d(NZ_ext,NX_ext);\nVz_z=space2d(NZ_ext,NX_ext);\nVz=space2d(NZ_ext,NX_ext);*/\ntxx = space3d(NZ_ext, NY_ext, NX_ext);\n\n\ntyy = space3d(NZ_ext, NY_ext, NX_ext);\n\ntzz = space3d(NZ_ext, NY_ext, NX_ext);\n\ntxy = space3d(NZ_ext, NY_ext, NX_ext);\n\ntxz = space3d(NZ_ext, NY_ext, NX_ext);\n\ntyz = space3d(NZ_ext, NY_ext, NX_ext);\nsource= space3d(NZ_ext, NY_ext, NX_ext);\n\n/*  tzz_x=space2d(NZ_ext,NX_ext);\ntzz_z=space2d(NZ_ext,NX_ext);\ntxz_x=space2d(NZ_ext,NX_ext);\ntxz_z=space2d(NZ_ext,NX_ext);\nssx=space2d(NZ_ext,NX_ext);\nssz=space2d(NZ_ext,NX_ext);*/\n\ndxi = space3d(NZ_ext, NY_ext, NX_ext);\ndxi2 = space3d(NZ_ext, NY_ext, NX_ext);\ndyj = space3d(NZ_ext, NY_ext, NX_ext);\ndyj2 = space3d(NZ_ext, NY_ext, NX_ext);\ndzk = space3d(NZ_ext, NY_ext, NX_ext);\ndzk2 = space3d(NZ_ext, NY_ext, NX_ext);\nchar dxi_name[] = \"dxi.dat\";\nchar dzk_name[] = \"dzk.dat\";\n//(xoleft,xoright)模型的物理边界—不算PML\nxoleft = DP;\nxoright = (NX_ext-1)*H - DP;\nyoleft = DP;\nyoright = (NY_ext-1 )*H - DP;\nzoleft = DP;\nzoright = (NZ_ext-1 )*H - DP;\n\n\n//用于对vx_x[iz][ix]等的求解，加入吸收边界，使数值衰减\nfor (iz = 0; iz < NZ_ext; iz++)\n{\n    for (iy = 0; iy < NY_ext; iy++)\n    {\n        for (ix = 0; ix < NX_ext; ix++)\n        {\n            x = ix*H; y = iy*H; z = iz*H;\n            if ((x>=0&&x < xoleft)&&(y>=0&&y= 0 && z= xoright && x < NX_ext*H) && (y >= 0 && y= 0 && z= 0 && x= 0 && y < yoleft) && (z >= 0 && z= 0 && x= yoright && y < NY_ext*H) &&(z >= 0 && z= 0 && x= 0 && y= 0 && z < zoleft) )\n                {\n                    v0 = 1500.0f;\n                    d0 = 3.0*v0*log(1.0 / RC) / (2.0*DP);\n                    //d0=3.0*v0*(8.0/15.0-3.0/100.0*NP+NP*NP/1500.0)/H; \n                    dzk[iz][iy][ix] = d0*pow(((zoleft - z) / DP), 2);\n                    dzk2[iz][iy][ix] = d0*pow(((zoleft - z - 0.5*H) / DP), 2);\n                }\n                if ((x >= 0 && x= 0 && y= zoright && z < NZ_ext*H) )\n                {\n                    v0 = 1500.0f;\n                    d0 = 3.0*v0*log(1.0 / RC) / (2.0*DP);\n                    dzk[iz][iy][ix] = d0*pow(((z - zoright) / DP), 2);\n                    dzk2[iz][iy][ix] = d0*pow(((z + 0.5*H - zoright) / DP), 2);\n                }\n\n            }\n        }\n    }\n}\n\n\n/*for (iz = 0; iz= 0.9999*yoright)\n            {\n                v0 = 1500.0;\n                d0 = 3.0*v0*log(1.0 / RC) / (2.0*DP);\n                dyj[iz][iy][ix] = d0*pow(((y - yoright) / DP), 2);\n                dyj2[iz][iy][ix] = d0*pow(((y + 0.5*H - yoright) / DP), 2);\n            }\n            else\n            {\n                dyj[iz][iy][ix] = 0.; dyj2[iz][iy][ix] = 0.;\n            }\n        }\n    }\n}\n\nfor (iz = 0; iz= 0.9999*zoright)\n            {\n                v0 = 1500.0;\n                d0 = 3.0*v0*log(1.0 / RC) / (2.0*DP);\n                dzk[iz][iy][ix] = d0*pow(((z - zoright) / DP), 2);\n                dzk2[iz][iy][ix] = d0*pow(((z + 0.5*H - zoright) / DP), 2);\n            }\n            else\n            {\n                dzk[iz][iy][ix] = 0.; dzk2[iz][iy][ix] = 0.;\n            }\n        }\n    }\n}*/\nwfile3d(dxi_name, dxi, NZ_ext, NY_ext, NX_ext);\nwfile3d(dzk_name, dzk, NZ_ext, NY_ext, NX_ext);//建立文件\n                                                 //开始时间递推\nfloat a1 = 9.0 / 8.0;\nfloat a2 = -1.0 / 24.0;\n//pml初始参数\n\nfloat xFHalfTemp1;\nfloat xFHalfTemp2 , xFIntTemp1, xFIntTemp2, yFHalfTemp1, yFHalfTemp2, yFIntTemp1, yFIntTemp2, zFHalfTemp1, zFHalfTemp2, zFIntTemp1, zFIntTemp2 ;\nfloat ***pmlxSxx, ***pmlySxy, ***pmlzSxz, ***pmlxSxy, ***pmlySyy, ***pmlzSyz, ***pmlxSxz, ***pmlySyz, ***pmlzSzz;\npmlxSxx= space3d(NZ_ext, NY_ext, NX_ext);\npmlySxy= space3d(NZ_ext, NY_ext, NX_ext);\npmlzSxz= space3d(NZ_ext, NY_ext, NX_ext);\npmlxSxy= space3d(NZ_ext, NY_ext, NX_ext);\npmlySyy= space3d(NZ_ext, NY_ext, NX_ext);\npmlzSyz= space3d(NZ_ext, NY_ext, NX_ext);\npmlxSxz= space3d(NZ_ext, NY_ext, NX_ext);\npmlySyz= space3d(NZ_ext, NY_ext, NX_ext);\npmlzSzz= space3d(NZ_ext, NY_ext, NX_ext);\nfloat ***pmlxVx, ***pmlyVy, ***pmlzVz, ***pmlzVx, ***pmlxVz, ***pmlyVx, ***pmlxVy, ***pmlzVy, ***pmlyVz;\npmlxVx = space3d(NZ_ext, NY_ext, NX_ext);\npmlyVy = space3d(NZ_ext, NY_ext, NX_ext);\npmlzVz = space3d(NZ_ext, NY_ext, NX_ext);\npmlzVx = space3d(NZ_ext, NY_ext, NX_ext);\npmlxVz = space3d(NZ_ext, NY_ext, NX_ext);\npmlyVx = space3d(NZ_ext, NY_ext, NX_ext);\npmlxVy = space3d(NZ_ext, NY_ext, NX_ext);\npmlzVy = space3d(NZ_ext, NY_ext, NX_ext);\npmlyVz = space3d(NZ_ext, NY_ext, NX_ext);\nfloat ***DxSxxPre, ***DySxyPre, ***DzSxzPre, ***DxSxyPre, ***DySyyPre, ***DzSyzPre, ***DxSxzPre, ***DySyzPre, ***DzSzzPre;\nDxSxxPre = space3d(NZ_ext, NY_ext, NX_ext);\nDySxyPre = space3d(NZ_ext, NY_ext, NX_ext);\nDzSxzPre = space3d(NZ_ext, NY_ext, NX_ext);\nDxSxyPre = space3d(NZ_ext, NY_ext, NX_ext);\nDySyyPre = space3d(NZ_ext, NY_ext, NX_ext);\nDzSyzPre = space3d(NZ_ext, NY_ext, NX_ext);\nDxSxzPre = space3d(NZ_ext, NY_ext, NX_ext);\nDySyzPre = space3d(NZ_ext, NY_ext, NX_ext);\nDzSzzPre = space3d(NZ_ext, NY_ext, NX_ext);\nfloat ***DxVxPre, ***DyVyPre, ***DzVzPre, ***DzVxPre, ***DxVzPre, ***DyVxPre, ***DxVyPre, ***DzVyPre, ***DyVzPre;\nDxVxPre = space3d(NZ_ext, NY_ext, NX_ext);\nDyVyPre = space3d(NZ_ext, NY_ext, NX_ext);\nDzVzPre = space3d(NZ_ext, NY_ext, NX_ext);\nDzVxPre = space3d(NZ_ext, NY_ext, NX_ext);\nDxVzPre = space3d(NZ_ext, NY_ext, NX_ext);\nDyVxPre = space3d(NZ_ext, NY_ext, NX_ext);\nDxVyPre = space3d(NZ_ext, NY_ext, NX_ext);\nDzVyPre = space3d(NZ_ext, NY_ext, NX_ext);\nDyVzPre = space3d(NZ_ext, NY_ext, NX_ext);\n\n//这里的代码应该修正一下，应该从数据里面提取最大最小速度\n\n//控制网格频散\nif (Vsmin / (F0*H) < 15)\n    printf(\"1\\n\");\n\n//检查稳定性条件\nbest_dt = 6.0*H / (7.0*sqrt(2.0)*Vpmax);\nif (DT >= best_dt)\n    printf(\"2 %f\\n\", best_dt);\n\nclock_t start, finish;\nstart = clock();\n\nsis_x = space2d(NZ, NT);\nsis_y = space2d(NZ, NT);\nsis_z = space2d(NZ, NT);\n    omp_set_num_threads(16);\n#pragma omp parallel private(ix, iy,iz,rho_tempx,rho_tempy,rho_tempz,xx1 , xy1, xz1, yx2, yy2 , yz2, zx3,zy3, zz3,xFHalfTemp1,yFHalfTemp1,zFHalfTemp1,xFIntTemp1,yFIntTemp1,zFIntTemp1,xFHalfTemp2,yFHalfTemp2,zFHalfTemp2,xFIntTemp2,yFIntTemp2,zFIntTemp2,muxz,muyz,muxy,lamda2u_temp,lamda_temp,y5,y6,y7,y8,y9,y10,y11,y12,y13) \n{//$OMP PARALLEL DEFAULT(SHARED) PRIVATE(x, y, z, &\n    //$OMP                                  VxSource, VySource, VzSource, SxxSource, SyySource, SzzSource, SxySource, SxzSource, SyzSource, &\n    //$OMP                                  DenOReciprocal, DenXReciprocal, DenYReciprocal, DenZReciprocal, LambdaO, MuO, MuXY, MuXZ, MuYZ, &\n    //$OMP                                  DxSxx, DySxy, DzSxz, DxSxy, DySyy, DzSyz, DxSxz, DySyz, DzSzz, &\n    //$OMP                                  xAttFactorHalfTemp1, xAttFactorIntTemp1, yAttFactorHalfTemp1, yAttFactorIntTemp1, zAttFactorHalfTemp1, zAttFactorIntTemp1, &\n    //$OMP                                  xAttFactorHalfTemp2, xAttFactorIntTemp2, yAttFactorHalfTemp2, yAttFactorIntTemp2, zAttFactorHalfTemp2, zAttFactorIntTemp2, &\n    //$OMP                                  BoolTemp1, BoolTemp2, BoolTemp3, BoolTemp4, BoolTemp5, BoolTemp6, &\n    //$OMP                                  DxVx, DyVy, DzVz, DxVz, DzVx, DxVy, DyVx, DyVz, DzVy)\n\n\n    for (it = 0; it < NT; it++)\n    {\n        tt = it*DT;\n        if (it % 100 == 0) printf(\"it=%d\\n\", it);\n\n        //计算质点速度v\n       #pragma omp for\n        for (iz = 2; iz < NZ_ext - 2; iz++)\n            for (iy = 2; iy < NY_ext - 2; iy++)\n                for (ix = 2; ix < NX_ext - 2; ix++)\n                {\n                    rho_tempx = 2 / (rho_ext[iz][iy][ix] + rho_ext[iz][iy][ix + 1]);\n                    rho_tempy = 2 / (rho_ext[iz][iy][ix] + rho_ext[iz][iy + 1][ix]);\n                    rho_tempz = 2 / (rho_ext[iz][iy][ix] + rho_ext[iz + 1][iy][ix]);\n\n                   //4阶\n                    /*xx1 = ((a1*(txx[iz][iy][ix + 1] - txx[iz][iy][ix]) +\n                        a2*(txx[iz][iy][ix + 2] - txx[iz][iy][ix - 1]))) / H;\n                    xy1 = ((a1*(txy[iz][iy][ix] - txy[iz][iy - 1][ix]) +\n                        a2*(txy[iz][iy + 1][ix] - txy[iz][iy - 2][ix]))) / H;\n                    xz1 = ((a1*(txz[iz][iy][ix] - txz[iz - 1][iy][ix]) +\n                        a2*(txz[iz + 1][iy][ix] - txz[iz - 2][iy][ix]))) / H;\n\n                    yx2 = ((a1*(txy[iz][iy][ix] - txy[iz][iy][ix - 1]) +\n                        a2*(txy[iz][iy][ix + 1] - txy[iz][iy][ix - 2]))) / H;\n                    yy2 = ((a1*(tyy[iz][iy + 1][ix] - tyy[iz][iy][ix]) +\n                        a2*(tyy[iz][iy + 2][ix] - tyy[iz][iy - 1][ix]))) / H;\n                    yz2 = ((a1*(tyz[iz][iy][ix] - tyz[iz - 1][iy][ix]) +\n                        a2*(tyz[iz + 1][iy][ix] - tyz[iz - 2][iy][ix]))) / H;\n\n                    zx3 = ((a1*(txz[iz][iy][ix] - txz[iz][iy][ix - 1]) +\n                        a2*(txz[iz][iy][ix + 1] - txz[iz][iy][ix - 2]))) / H;\n                    zy3 = ((a1*(tyz[iz][iy][ix] - tyz[iz][iy - 1][ix]) +\n                        a2*(tyz[iz][iy + 1][ix] - tyz[iz][iy - 2][ix]))) / H;\n                    zz3 = ((a1*(tzz[iz + 1][iy][ix] - tzz[iz][iy][ix]) +\n                        a2*(tzz[iz + 2][iy][ix] - tzz[iz - 1][iy][ix]))) / H;*/\n                    //二阶\n                    xx1 = (txx[iz][iy][ix + 1] - txx[iz][iy][ix]) / H;\n                    xy1 = (txy[iz][iy][ix] - txy[iz][iy - 1][ix]) / H;\n                    xz1 = (txz[iz][iy][ix] - txz[iz - 1][iy][ix]) / H;\n                    yx2 = (txy[iz][iy][ix] - txy[iz][iy][ix - 1]) / H;\n                    yy2 = (tyy[iz][iy + 1][ix] - tyy[iz][iy][ix]) / H;\n                    yz2 = (tyz[iz][iy][ix] - tyz[iz - 1][iy][ix]) / H;\n                    zx3 = (txz[iz][iy][ix] - txz[iz][iy][ix - 1]) / H;\n                    zy3 = (tyz[iz][iy][ix] - tyz[iz][iy - 1][ix]) / H;\n                    zz3 = (tzz[iz + 1][iy][ix] - tzz[iz][iy][ix]) / H;\n\n                    /*x = ix*H; y = iy*H; z = iz*H;\n                    if(((x >= 0 && x < xoleft) && (y >= 0 && y= 0 && z= xoright && x < NX_ext*H) && (y >= 0 && y= 0 && z= 0 && x= 0 && y < yoleft) && (z >= 0 && z= 0 && x= yoright && y < NY_ext*H) && (z >= 0 && z= 0 && x= 0 && y= 0 && z < zoleft))\n\n                        ||((x >= 0 && x= 0 && y= zoright && z < NZ_ext*H)))*/\n                                            if((ix=(NX_ext-1-NP))||(iy=(NY_ext-1-NP))||(iz=(NZ_ext-1-NP)))\n                    {\n                        xFHalfTemp1 = exp(-(dxi2[iz][iy][ix])*DT); xFIntTemp1 = exp(-(dxi[iz][iy][ix])*DT);\n                        yFHalfTemp1 = exp(-(dyj2[iz][iy][ix])*DT); yFIntTemp1 = exp(-(dyj[iz][iy][ix])*DT);\n                        zFHalfTemp1 = exp(-(dzk2[iz][iy][ix])*DT); zFIntTemp1 = exp(-(dzk[iz][iy][ix])*DT);\n                        xFHalfTemp2 = -DT*dxi2[iz][iy][ix] * 0.5; xFIntTemp2 = -DT*dxi[iz][iy][ix] * 0.5;\n                        yFHalfTemp2 = -DT*dyj2[iz][iy][ix] * 0.5; yFIntTemp2 = -DT*dyj[iz][iy][ix] * 0.5;\n                        zFHalfTemp2 = -DT*dzk2[iz][iy][ix] * 0.5; zFIntTemp2 = -DT*dzk[iz][iy][ix] * 0.5;\n                        pmlxSxx[iz][iy][ix] = xFHalfTemp1*pmlxSxx[iz][iy][ix] + xFHalfTemp2*(xFHalfTemp1*DxSxxPre[iz][iy][ix] + xx1);\n                        pmlySxy[iz][iy][ix] = yFIntTemp1*pmlySxy[iz][iy][ix] + yFIntTemp2*(yFIntTemp1*DySxyPre[iz][iy][ix] + xy1);\n                        pmlzSxz[iz][iy][ix] = zFIntTemp1*pmlzSxz[iz][iy][ix] + zFIntTemp2*(zFIntTemp1*DzSxzPre[iz][iy][ix] + xz1);\n                        DxSxxPre[iz][iy][ix] = xx1; DySxyPre[iz][iy][ix] = xy1; DzSxzPre[iz][iy][ix] = xz1;\n                        xx1 = xx1 + pmlxSxx[iz][iy][ix];\n                        xy1 = xy1 + pmlySxy[iz][iy][ix];\n                        xz1 = xz1 + pmlzSxz[iz][iy][ix];\n\n                        pmlxSxy[iz][iy][ix] = xFIntTemp1*pmlxSxy[iz][iy][ix] + xFIntTemp2*(xFIntTemp1*DxSxyPre[iz][iy][ix] + yx2);\n                        pmlySyy[iz][iy][ix] = yFHalfTemp1*pmlySyy[iz][iy][ix] + yFHalfTemp2*(yFHalfTemp1*DySyyPre[iz][iy][ix] + yy2);\n                        pmlzSyz[iz][iy][ix] = zFIntTemp1*pmlzSyz[iz][iy][ix] + zFIntTemp2*(zFIntTemp1*DzSyzPre[iz][iy][ix] + yz2);\n                        DxSxyPre[iz][iy][ix] = yx2; DySyyPre[iz][iy][ix] = yy2; DzSyzPre[iz][iy][ix] = yz2;\n                        yx2 = yx2 + pmlxSxy[iz][iy][ix];\n                        yy2 = yy2 + pmlySyy[iz][iy][ix];\n                        yz2 = yz2 + pmlzSyz[iz][iy][ix];\n\n                        pmlxSxz[iz][iy][ix] = xFIntTemp1*pmlxSxz[iz][iy][ix] + xFIntTemp2*(xFIntTemp1*DxSxzPre[iz][iy][ix] + zx3);\n                        pmlySyz[iz][iy][ix] = yFIntTemp1*pmlySyz[iz][iy][ix] + yFIntTemp2*(yFIntTemp1*DySyzPre[iz][iy][ix] + zy3);\n                        pmlzSzz[iz][iy][ix] = zFHalfTemp1*pmlzSzz[iz][iy][ix] + zFHalfTemp2*(zFHalfTemp1*DzSzzPre[iz][iy][ix] + zz3);\n                        DxSxzPre[iz][iy][ix] = zx3; DySyzPre[iz][iy][ix] = zy3; DzSzzPre[iz][iy][ix] = zz3;\n                        zx3 = zx3 + pmlxSxz[iz][iy][ix];\n                        zy3 = zy3 + pmlySyz[iz][iy][ix];\n                        zz3 = zz3 + pmlzSzz[iz][iy][ix];\n                    }\n\n                    vx[iz][iy][ix] = vx[iz][iy][ix] + (DT*rho_tempx)*(xx1 + xy1 + xz1);\n                    vy[iz][iy][ix] = vy[iz][iy][ix] + (DT*rho_tempy)*(yx2 + yy2 + yz2);\n                    vz[iz][iy][ix] = vz[iz][iy][ix] + (DT*rho_tempz)*(zx3 + zy3 + zz3);\n\n\n\n                }\n\n           #pragma omp barrier\n\n        //计算应力\n         #pragma omp for\n        for (iz = 2; iz < NZ_ext - 2; iz++)\n            for (iy = 2; iy < NY_ext - 2; iy++)\n                for (ix = 2; ix < NX_ext - 2; ix++)\n                {\n\n                    if (mu_ext[iz][iy][ix] == 0 || mu_ext[iz][iy][ix + 1] == 0 || mu_ext[iz + 1][iy][ix] == 0 || mu_ext[iz + 1][iy][ix + 1] == 0)\n                        muxz = 0.0;\n                    else\n                        muxz = 1 / (0.25*(1 / mu_ext[iz][iy][ix] + 1 / mu_ext[iz][iy][ix + 1] + 1 / mu_ext[iz + 1][iy][ix] + 1 / mu_ext[iz + 1][iy][ix + 1]));\n                    if (mu_ext[iz][iy][ix] == 0 || mu_ext[iz][iy + 1][ix] == 0 || mu_ext[iz + 1][iy][ix] == 0 || mu_ext[iz + 1][iy + 1][ix] == 0)\n                        muyz = 0.0;\n                    else\n                        muyz = 1 / (0.25*(1 / mu_ext[iz][iy][ix] + 1 / mu_ext[iz][iy + 1][ix] + 1 / mu_ext[iz + 1][iy][ix] + 1 / mu_ext[iz + 1][iy + 1][ix]));\n                    if (mu_ext[iz][iy][ix] == 0 || mu_ext[iz][iy][ix + 1] == 0 || mu_ext[iz][iy + 1][ix] == 0 || mu_ext[iz][iy + 1][ix + 1] == 0)\n                        muxy = 0.0;\n                    else\n                        muxy = 1 / (0.25*(1 / mu_ext[iz][iy][ix] + 1 / mu_ext[iz][iy][ix + 1] + 1 / mu_ext[iz][iy + 1][ix] + 1 / mu_ext[iz][iy + 1][ix + 1]));\n\n\n                    lamda2u_temp = lamda2u_ext[iz][iy][ix];\n                    lamda_temp = lamda_ext[iz][iy][ix];\n                    //  muxz=0.0;\n                    if ( i==0&&ix == sx&&iy == sy&&iz == sz)\n                    {\n                        source[iz][iy][ix] = (1 - 2 * PI*PI*F0*F0*(tt - T0)*(tt - T0))*exp(-PI*PI*F0*F0*(tt - T0)*(tt - T0));\n                    }\n                    else if(i==1&&(ix == (sx+1)&&iy == sy&&iz == sz))\n                    {\n                        source[iz][iy][ix] = -(1 - 2 * PI*PI*F0*F0*(tt - T0)*(tt - T0))*exp(-PI*PI*F0*F0*(tt - T0)*(tt - T0));\n                    }\n                    else if (i == 1 && (ix == (sx - 1) && iy == sy&&iz == sz))\n                    {\n                        source[iz][iy][ix] = (1 - 2 * PI*PI*F0*F0*(tt - T0)*(tt - T0))*exp(-PI*PI*F0*F0*(tt - T0)*(tt - T0));\n                    }\n\n                    //  Qz=Q_ext[iz][ix];R_temp=R_ext[iz][ix];lamda2u_temp=lamda2u_ext[iz][ix];lamda_temp=lamda_ext[iz][ix];\n\n                    /*y5 = (a1*(vx[iz][iy][ix] - vx[iz][iy][ix - 1]) +\n                        a2*(vx[iz][iy][ix + 1] - vx[iz][iy][ix - 2])) / H;\n\n                    y6 = (a1*(vy[iz][iy][ix] - vy[iz][iy - 1][ix]) +\n                        a2*(vy[iz][iy + 1][ix] - vy[iz][iy - 2][ix])) / H;\n\n                    y7 = (a1*(vz[iz][iy][ix] - vz[iz - 1][iy][ix]) +\n                        a2*(vz[iz + 1][iy][ix] - vz[iz - 2][iy][ix])) / H;\n\n                    y8 = ((a1*(vx[iz][iy + 1][ix] - vx[iz][iy][ix]) +\n                        a2*(vx[iz][iy + 2][ix] - vx[iz][iy - 1][ix]))) / H;\n\n                    y9 = ((a1*(vy[iz][iy][ix + 1] - vy[iz][iy][ix]) +\n                        a2*(vy[iz][iy][ix + 2] - vy[iz][iy][ix - 1]))) / H;\n\n                    y10 = ((a1*(vx[iz + 1][iy][ix] - vx[iz][iy][ix]) +\n                        a2*(vx[iz + 2][iy][ix] - vx[iz - 1][iy][ix]))) / H;\n\n                    y11 = ((a1*(vz[iz][iy][ix + 1] - vz[iz][iy][ix]) +\n                        a2*(vz[iz][iy][ix + 2] - vz[iz][iy][ix - 1]))) / H;\n\n                    y12 = ((a1*(vy[iz + 1][iy][ix] - vy[iz][iy][ix]) +\n                        a2*(vy[iz + 2][iy][ix] - vy[iz - 1][iy][ix]))) / H;\n\n                    y13 = ((a1*(vz[iz][iy + 1][ix] - vz[iz][iy][ix]) +\n                        a2*(vz[iz][iy + 2][ix] - vz[iz][iy - 1][ix]))) / H;*/\n                    y5 = (vx[iz][iy][ix] - vx[iz][iy][ix - 1]) / H;\n                    y6 = (vy[iz][iy][ix] - vy[iz][iy - 1][ix]) / H;\n                    y7 = (vz[iz][iy][ix] - vz[iz - 1][iy][ix]) / H;\n                    y8 = (vx[iz][iy + 1][ix] - vx[iz][iy][ix]) / H;\n                    y9 = (vy[iz][iy][ix + 1] - vy[iz][iy][ix]) / H;\n                    y10 = (vx[iz + 1][iy][ix] - vx[iz][iy][ix]) / H;\n                    y11 = (vz[iz][iy][ix + 1] - vz[iz][iy][ix]) / H;\n                    y12 = (vy[iz + 1][iy][ix] - vy[iz][iy][ix]) / H;\n                    y13 = (vz[iz][iy + 1][ix] - vz[iz][iy][ix]) / H;\n                    //x = ix*H; y = iy*H; z = iz*H;\n                    /*if (((x >= 0 && x < xoleft) && (y >= 0 && y= 0 && z= xoright && x < NX_ext*H) && (y >= 0 && y= 0 && z= 0 && x= 0 && y < yoleft) && (z >= 0 && z= 0 && x= yoright && y < NY_ext*H) && (z >= 0 && z= 0 && x= 0 && y= 0 && z < zoleft))\n\n                        || ((x >= 0 && x= 0 && y= zoright && z < NZ_ext*H)))*/\n                                             if((ix=(NX_ext-1-NP))||(iy=(NY_ext-1-NP))||(iz=(NZ_ext-1-NP)))\n                    {\n                        xFHalfTemp1 = exp(-(dxi2[iz][iy][ix])*DT); xFIntTemp1 = exp(-(dxi[iz][iy][ix])*DT);\n                        yFHalfTemp1 = exp(-(dyj2[iz][iy][ix])*DT); yFIntTemp1 = exp(-(dyj[iz][iy][ix])*DT);\n                        zFHalfTemp1 = exp(-(dzk2[iz][iy][ix])*DT); zFIntTemp1 = exp(-(dzk[iz][iy][ix])*DT);\n                        xFHalfTemp2 = -DT*dxi2[iz][iy][ix] * 0.5; xFIntTemp2 = -DT*dxi[iz][iy][ix] * 0.5;\n                        yFHalfTemp2 = -DT*dyj2[iz][iy][ix] * 0.5; yFIntTemp2 = -DT*dyj[iz][iy][ix] * 0.5;\n                        zFHalfTemp2 = -DT*dzk2[iz][iy][ix] * 0.5; zFIntTemp2 = -DT*dzk[iz][iy][ix] * 0.5;\n                        pmlxVx[iz][iy][ix] = xFIntTemp1*pmlxVx[iz][iy][ix] + xFIntTemp2*(xFIntTemp1*DxVxPre[iz][iy][ix] + y5);\n                        pmlyVy[iz][iy][ix] = yFIntTemp1*pmlyVy[iz][iy][ix] + yFIntTemp2*(yFIntTemp1*DyVyPre[iz][iy][ix] + y6);\n                        pmlzVz[iz][iy][ix] = zFIntTemp1*pmlzVz[iz][iy][ix] + zFIntTemp2*(zFIntTemp1*DzVzPre[iz][iy][ix] + y7);\n                        DxVxPre[iz][iy][ix] = y5; DyVyPre[iz][iy][ix] = y6; DzVzPre[iz][iy][ix] = y7;\n                        y5 = y5 + pmlxVx[iz][iy][ix];\n                        y6 = y6 + pmlyVy[iz][iy][ix];\n                        y7 = y7 + pmlzVz[iz][iy][ix];\n\n                        pmlzVx[iz][iy][ix] = zFHalfTemp1*pmlzVx[iz][iy][ix] + zFHalfTemp2*(zFHalfTemp1*DzVxPre[iz][iy][ix] + y10);\n                        pmlxVz[iz][iy][ix] = xFHalfTemp1*pmlxVz[iz][iy][ix] + xFHalfTemp2*(xFHalfTemp1*DxVzPre[iz][iy][ix] + y11);\n                        DzVxPre[iz][iy][ix] = y10; DxVzPre[iz][iy][ix] = y11;\n                        y10 = y10 + pmlzVx[iz][iy][ix];\n                        y11 = y11 + pmlxVz[iz][iy][ix];\n\n                        pmlyVx[iz][iy][ix] = yFHalfTemp1*pmlyVx[iz][iy][ix] + yFHalfTemp2*(yFHalfTemp1*DyVxPre[iz][iy][ix] + y8);\n                        pmlxVy[iz][iy][ix] = xFHalfTemp1*pmlxVy[iz][iy][ix] + xFHalfTemp2*(xFHalfTemp1*DxVyPre[iz][iy][ix] + y9);\n                        DyVxPre[iz][iy][ix] = y8; DxVyPre[iz][iy][ix] = y9;\n                        y8 = y8 + pmlyVx[iz][iy][ix];\n                        y9 = y9 + pmlxVy[iz][iy][ix];\n\n                        pmlzVy[iz][iy][ix] = zFHalfTemp1*pmlzVy[iz][iy][ix] + zFHalfTemp2*(zFHalfTemp1*DzVyPre[iz][iy][ix] + y12);\n                        pmlyVz[iz][iy][ix] = yFHalfTemp1*pmlyVz[iz][iy][ix] + yFHalfTemp2*(yFHalfTemp1*DyVzPre[iz][iy][ix] + y13);\n                        DzVyPre[iz][iy][ix] = y12; DyVzPre[iz][iy][ix] = y13;\n                        y12 = y12 + pmlzVy[iz][iy][ix];\n                        y13 = y13 + pmlyVz[iz][iy][ix];\n                    }\n\n                    txx[iz][iy][ix] = txx[iz][iy][ix] + (lamda2u_temp*DT)*y5 + (lamda_temp*DT)*y6 + (lamda_temp*DT)*y7 + source[iz][iy][ix];\n                    tyy[iz][iy][ix] = tyy[iz][iy][ix] + (lamda_temp*DT)*y5 + (lamda2u_temp*DT)*y6 + (lamda_temp*DT)*y7 + source[iz][iy][ix];\n                    tzz[iz][iy][ix] = tzz[iz][iy][ix] + (lamda_temp*DT)*y5 + (lamda_temp*DT)*y6 + (lamda2u_temp*DT)*y7 + source[iz][iy][ix];\n                    txz[iz][iy][ix] = txz[iz][iy][ix] + (muxz*DT)*(y10 + y11);\n                    txy[iz][iy][ix] = txy[iz][iy][ix] + (muxy*DT)*(y8 + y9);\n                    tyz[iz][iy][ix] = tyz[iz][iy][ix] + (muyz*DT)*(y12 + y13);\n\n                    //加震源—单极子\n                    if (i == 0)\n                    {\n                        tzz[sz][sy][sx] =(1 -2 * PI*PI*F0*F0*(tt - T0)*(tt - T0))*exp(-PI*PI*F0*F0*(tt - T0)*(tt - T0)); //对正应力加爆炸源\n                        tyy[sz][sy][sx] = (1-2 * PI*PI*F0*F0*(tt - T0)*(tt - T0))*exp(-PI*PI*F0*F0*(tt - T0)*(tt - T0));\n                        txx[sz][sy][sx] =(1 -2 * PI*PI*F0*F0*(tt - T0)*(tt - T0))*exp(-PI*PI*F0*F0*(tt - T0)*(tt - T0));\n                    }\n                    //加震源—偶极子 \n\n                    if (i == 1)\n                    {\n                        tzz[sz][sy][sx + 1] = tzz[sz][sy][sx + 1] - (1 - 2 * PI*PI*F0*F0*(tt - T0)*(tt - T0))*exp(-PI*PI*F0*F0*(tt - T0)*(tt - T0));; //对正应力加爆炸源\n                        tyy[sz][sy][sx + 1] = tyy[sz][sy][sx + 1] - (1 - 2 * PI*PI*F0*F0*(tt - T0)*(tt - T0))*exp(-PI*PI*F0*F0*(tt - T0)*(tt - T0));;\n                        txx[sz][sy][sx + 1] = txx[sz][sy][sx + 1] - (1 - 2 * PI*PI*F0*F0*(tt - T0)*(tt - T0))*exp(-PI*PI*F0*F0*(tt - T0)*(tt - T0));;\n                        tzz[sz][sy][sx - 1] = tzz[sz][sy][sx - 1] + (1 - 2 * PI*PI*F0*F0*(tt - T0)*(tt - T0))*exp(-PI*PI*F0*F0*(tt - T0)*(tt - T0));; //对正应力加爆炸源\n                        tyy[sz][sy][sx - 1] = tyy[sz][sy][sx - 1] + (1 - 2 * PI*PI*F0*F0*(tt - T0)*(tt - T0))*exp(-PI*PI*F0*F0*(tt - T0)*(tt - T0));;\n                        txx[sz][sy][sx - 1] = txx[sz][sy][sx - 1] + (1 - 2 * PI*PI*F0*F0*(tt - T0)*(tt - T0))*exp(-PI*PI*F0*F0*(tt - T0)*(tt - T0));;\n                    }\n\n                }\n\n       #pragma omp barrier\n       #pragma omp master\n                {\n\n\n                    //  tzz_z[sz][sx]=tzz_z[sz][sx]-2*PI*PI*F0*F0*(tt-T0)*exp(-PI*PI*F0*F0*(tt-T0)*(tt-T0));\n                    //sis_y[1][it] = tzz[sz][sy][sx];\n\n                    //记录地震记录\n                    if (i == 0)\n                    {\n                        for (iz = NP; iz < NP + NZ; iz++)\n                        {\n                            sis_z[iz - NP][it] = tzz[iz][sy][sx];\n\n                            sis_x[iz - NP][it] = txx[iz][sy][sx];\n                            //sis_z[iz-NP][it]=vz_x[iz][NX_ext-iz]+vz_z[iz][NX_ext-iz]; \n                            //  sis_x[iz-NP][it]=vx_x[iz][NX_ext-iz]+vx_z[iz][NX_ext-iz];\n                        }\n                    }\n                    if (i == 1)\n                    {\n                        for (iz = NP; iz < NP + NZ; iz++)\n                        {\n                            sis_z[iz - NP][it] = tzz[iz][sy][sx];\n\n                            sis_x[iz - NP][it] = txx[iz][sy][sx+1];\n                            //sis_z[iz-NP][it]=vz_x[iz][NX_ext-iz]+vz_z[iz][NX_ext-iz]; \n                            //  sis_x[iz-NP][it]=vx_x[iz][NX_ext-iz]+vx_z[iz][NX_ext-iz];\n                        }\n                    }\n                    if (it % 200 == 0)\n                    {\n                        int n = it / 200;\n                        for (iz = 2; iz < NZ_ext - 2; iz = iz + 2)\n                        {\n                            int a = iz / 2;\n                            for (ix = 2; ix < NX_ext - 2; ix = ix + 2)\n                            {\n                                int b = ix / 2;\n\n                                vxx[n][a][b] = txx[iz][sy][ix];\n                            }\n                        }\n                    }\n                }\n   #pragma omp barrier\n    }\n}\nfinish = clock();\nprintf(\"%f seconds\\n\", (double)(finish - start) / CLOCKS_PER_SEC);\n\n//输出波场快照\nchar vzname[] = \"vz.dat\";\nchar vxname[] = \"vx.dat\";\nchar vxxname[] = \"vxx.dat\";\nchar vyname[] = \"vy.dat\";\nchar txx_xname[] = \"txx_x.dat\";\nchar pmlxSxxname[] = \"pmlxSxx.dat\";\nchar DxSxxPrename[] = \"DxSxxPre.dat\";\n/*  for (iz=0; iz=(nr+14-ix))//45度倾斜井孔\n\n//if (iz>= 50)\n\n                //if(ix>=50)\n\n                {\n\n                    vp[iz][iy][ix] = 1500.0f;\n\n                    vs[iz][iy][ix] = 0.0f;\n\n                    vf[iz][iy][ix] = 1500.0f;\n\n                    rho[iz][iy][ix] = 1000.0f;\n\n                    rhof[iz][iy][ix] = 1000.0f;\n\n                    por[iz][iy][ix] = 1.0f;\n\n                    //  b[iz][ix]=0.0;\n\n\n\n                mu[iz][iy][ix] = rho[iz][iy][ix] * vs[iz][iy][ix] * vs[iz][iy][ix];\n                lamda2u[iz][iy][ix] = rho[iz][iy][ix] * vp[iz][iy][ix] * vp[iz][iy][ix];\n                lamda[iz][iy][ix] = lamda2u[iz][iy][ix] - 2 * mu[iz][iy][ix];\n                /*  Kb1=rho[iz][ix]*(1-por[iz][ix])*(vp[iz][ix]*vp[iz][ix]-vs[iz][ix]*vs[iz][ix]*4.0/3.0);//骨架压缩模量，声波测井原理与应用，P39\n                Ks1=rho[iz][ix]*(vp[iz][ix]*vp[iz][ix]-vs[iz][ix]*vs[iz][ix]*4.0/3.0);//岩石固态颗粒的体积模量\n                Kf1=rhof[iz][ix]*vf[iz][ix]*vf[iz][ix];//孔隙流体的体积压缩模量\n                a1=1-Kb1/Ks1;\n                D1=a1-por[iz][ix]+por[iz][ix]*Ks1/Kf1;\n                Q[iz][ix]=(a1-por[iz][ix])*por[iz][ix]*Ks1/D1;\n                R[iz][ix]=por[iz][ix]*por[iz][ix]*Ks1/D1;\n\n                tao=0.5*(1+1/por[iz][ix]);//孔隙弯曲度\n                rou11=(1-por[iz][ix])*rho[iz][ix]-(1-tao)*por[iz][ix]*rhof[iz][ix];\n                rou12=(1-tao)*por[iz][ix]*rhof[iz][ix];\n                rou22=tao*por[iz][ix]*rhof[iz][ix];\n\n                D11[iz][ix]=rou11/(rou11*rou22-rou12*rou12);\n                D12[iz][ix]=rou12/(rou11*rou22-rou12*rou12);\n                D22[iz][ix]=rou22/(rou11*rou22-rou12*rou12);*/\n            }\n            else\n            {\n\n                vp[iz][iy][ix] = 4000.0f;   //纵波\n                vs[iz][iy][ix] = 2340.0f;   //横波\n                rho[iz][iy][ix] = 2500.0f; //密度\n                vf[iz][iy][ix] = 1500.0f;\n                rhof[iz][iy][ix] = 1000.0f;\n                por[iz][iy][ix] = 0.2f;\n                //b[iz][ix]=0.2*pow(10,-3);// b=eta*por*por/perm*pow(10,-3);\n\n                mu[iz][iy][ix] = rho[iz][iy][ix] * vs[iz][iy][ix] * vs[iz][iy][ix];\n                lamda2u[iz][iy][ix] = rho[iz][iy][ix] * vp[iz][iy][ix] * vp[iz][iy][ix];\n                lamda[iz][iy][ix] = lamda2u[iz][iy][ix] - 2.0*mu[iz][iy][ix];\n\n            }\n\n        }\n    }\n\n}\n\n\n\n\n}\n\n\n\n/*\n\n//将模型扩边,用于PML\n\n//具体的操作过程是将实际模型参数放置在扩边后的数据中央，四周的数据用\n\n//最外缘的数据填充\n\nfloat **extmodel(float **inqit_model,int nz,int nx,int np)\n\n{\n\nfloat **p;\n\nint i,j;\n\nint nx2=nx+2*np;\n\nint nz2=nz+2*np;\n\n\n\np=space2d(nz2,nx2);\n\nfor (i=np; i<nz+np; i++)\n\nfor (j=0; j<np; j++)\n\np[i][j]=init_model[i-np][0];\n\nfor (i=np; i<nz+np; i++)\n\nfor (j=nx+np; j<nx2; j++)\n\np[i][j]=init_model[i-np][nx-1];\n\nfor (i=nz; i<nz2; i++)\n\nfor (j=np; j<np+nx; j++)\n\np[i][j]=init_model[nz-1][j-np];\n\nfor(i=0; i<np; i++)\n\nfor(j=np; j<np+nx; j++)\n\np[i][j]=init_model[0][j-np];\n\nfor(i=0; i<np; i++)\n\nfor(j=0; j<np; j++)\n\np[i][j]=init_model[0][0];\n\nfor(i=0; i<np; i++)\n\nfor(j=nx+np; j<nx2; j++)\n\np[i][j]=init_model[0][nx-1];\n\nfor (i=nz+np; i<nz2; i++)\n\nfor (j=0; j<np; j++)\n\np[i][j]=init_model[nz-1][0];\n\nfor (i=nz+np; i<nz2; i++)\n\nfor (j=nx+np; j<nx2; j++)\n\np[i][j]=init_model[nz-1][nx-1];\n\nfor (i=np; i<nz+np; i++)\n\nfor (j=np; j<nx+np; j++)\n\np[i][j]=init_model[i-np][j-np];\n\n\n\nreturn p;\n\n}*/\n\n\n\n//将模型扩边,用于PML\n\n//具体的操作过程是将实际模型参数放置在扩边后的数据中央，四周的数据用\n\n//最外缘的数据填充_3D\n\nfloat \n**extmodel(float *\n*init_model, int nz, int ny, int nx, int np)\n\n{\n\n    float ***p;\n\n    int i, j, k;\n\n    int nx2 = nx + 2 * np;\n\n    int ny2 = ny + 2 * np;\n\n    int nz2 = nz + 2 * np;\n\n\n\np = space3d(nz2, ny2, nx2);\n\n\nfor (i = 0; i<np; i++)\n    for (k = 0; k<np; k++)\n        for (j = 0; j<np; j++)\n            p[i][k][j] = init_model[0][0][0];\nfor (i = 0; i<np; i++)\n    for (k = np; k<np + ny; k++)\n        for (j = 0; j<np; j++)\n            p[i][k][j] = init_model[0][k - np][0];\nfor (i = 0; i<np; i++)\n    for (k = np + ny; k<ny2; k++)\n        for (j = 0; j<np; j++)\n            p[i][k][j] = init_model[0][ny - 1][0];\n\nfor (i = np; i<nz + np; i++)\n    for (k = 0; k<np; k++)\n        for (j = 0; j<np; j++)\n            p[i][k][j] = init_model[i - np][0][0];\nfor (i = np; i<nz + np; i++)\n    for (k = np; k<np + ny; k++)\n        for (j = 0; j<np; j++)\n            p[i][k][j] = init_model[i - np][k - np][0];\nfor (i = np; i<nz + np; i++)\n    for (k = ny + np; k<ny2; k++)\n        for (j = 0; j<np; j++)\n            p[i][k][j] = init_model[i - np][ny - 1][0];\n\nfor (i = nz + np; i<nz2; i++)\n    for (k = 0; k<np; k++)\n        for (j = 0; j<np; j++)\n            p[i][k][j] = init_model[nz - 1][0][0];\nfor (i = nz + np; i<nz2; i++)\n    for (k = np; k<np + ny; k++)\n        for (j = 0; j<np; j++)\n            p[i][k][j] = init_model[nz - 1][k - np][0];\nfor (i = nz + np; i<nz2; i++)\n    for (k = ny + np; k<ny2; k++)\n        for (j = 0; j<np; j++)\n            p[i][k][j] = init_model[nz - 1][ny - 1][0];\n\n\n\nfor (i = 0; i<np; i++)\n    for (k = 0; k<np; k++)\n        for (j = np; j<np + nx; j++)\n            p[i][k][j] = init_model[0][0][j - np];\nfor (i = 0; i<np; i++)\n    for (k = np; k<np + ny; k++)\n        for (j = np; j<np + nx; j++)\n            p[i][k][j] = init_model[0][k - np][j - np];\nfor (i = 0; i<np; i++)\n    for (k = ny + np; k<ny2; k++)\n        for (j = np; j<np + nx; j++)\n            p[i][k][j] = init_model[0][ny - 1][j - np];\n\nfor (i = np; i<nz + np; i++)\n    for (k = 0; k<np; k++)\n        for (j = np; j<np + nx; j++)\n            p[i][k][j] = init_model[i - np][0][j - np];\nfor (i = np; i<nz + np; i++)\n    for (k = np; k<np + ny; k++)\n        for (j = np; j<np + nx; j++)\n            p[i][k][j] = init_model[i - np][k - np][j - np];\nfor (i = np; i<nz + np; i++)\n    for (k = ny + np; k<ny2; k++)\n        for (j = np; j<np + nx; j++)\n            p[i][k][j] = init_model[i - np][ny - 1][j - np];\n\nfor (i = nz + np; i<nz2; i++)\n    for (k = 0; k<np; k++)\n        for (j = np; j<np + nx; j++)\n            p[i][k][j] = init_model[nz - 1][0][j - np];\nfor (i = nz + np; i<nz2; i++)\n    for (k = np; k<np + ny; k++)\n        for (j = np; j<np + nx; j++)\n            p[i][k][j] = init_model[nz - 1][k - np][j - np];\nfor (i = nz + np; i<nz2; i++)\n    for (k = ny + np; k<ny2; k++)\n        for (j = np; j<np + nx; j++)\n            p[i][k][j] = init_model[nz - 1][ny - 1][j - np];\n\n\nfor (i = 0; i<np; i++)\n    for (k = 0; k<np; k++)\n        for (j = np + nx; j<nx2; j++)\n            p[i][k][j] = init_model[0][0][nx - 1];\nfor (i = 0; i<np; i++)\n    for (k = np; k<np + ny; k++)\n        for (j = np + nx; j<nx2; j++)\n            p[i][k][j] = init_model[0][k - np][nx - 1];\nfor (i = 0; i<np; i++)\n    for (k = ny + np; k<ny2; k++)\n        for (j = np + nx; j<nx2; j++)\n            p[i][k][j] = init_model[0][ny - 1][nx - 1];\n\nfor (i = np; i<nz + np; i++)\n    for (k = 0; k<np; k++)\n        for (j = np + nx; j<nx2; j++)\n            p[i][k][j] = init_model[i - np][0][nx - 1];\nfor (i = np; i<nz + np; i++)\n    for (k = np; k<np + ny; k++)\n        for (j = np + nx; j<nx2; j++)\n            p[i][k][j] = init_model[i - np][k - np][nx - 1];\nfor (i = np; i<nz + np; i++)\n    for (k = ny + np; k<ny2; k++)\n        for (j = np + nx; j<nx2; j++)\n            p[i][k][j] = init_model[i - np][ny - 1][nx - 1];\n\nfor (i = nz + np; i<nz2; i++)\n    for (k = 0; k<np; k++)\n        for (j = np + nx; j<nx2; j++)\n            p[i][k][j] = init_model[nz - 1][0][nx - 1];\nfor (i = nz + np; i<nz2; i++)\n    for (k = np; k<np + ny; k++)\n        for (j = np + nx; j<nx2; j++)\n            p[i][k][j] = init_model[nz - 1][k - np][nx - 1];\nfor (i = nz + np; i<nz2; i++)\n    for (k = ny + np; k<ny2; k++)\n        for (j = np + nx; j<nx2; j++)\n            p[i][k][j] = init_model[nz - 1][ny - 1][nx - 1];\n\n\nreturn p;\n\n\n\n\n}", "Tag": "算法分析"}
{"Answer": "你注意看 10后面有个点", "Konwledge_Point": "应对NP完全问题", "Question": "python spyder 画手绘风出现 invalid syntax 问题\npython spyder 画手绘风出现 invalid syntax 问题\n\n\nfrom PIL import Image\nimport numpy as np\n\n\na = np.asarray(Image.open(\"D:/data/ipython/LiuYifei.jpg\").convert('L').astype('float')\n\n\ndepth = 10.#虚拟深度值\ngrad = np.gradient(a)#去图像灰度的梯度值\ngrad_x, grad_y = grad #分别取横纵图像的梯度值\ngrad_x = grad_x\ndepth/100.\ngrad_y = grad_y\ndepth/100.\nA = np.sqrt(grad_x\n2 + grad_y\n2 + 1.)\nuni_x = grad_x/A\nuni_y = grad_y/A\nuni_z = 1./A\n\n\nvec_el = np.pi/2.2 #光源的俯视角度，弧度值\nvec_az = np.pi/4. #光源的方位角度，弧度值\ndx = np.cos(vec_el)*np.cos(vec_az) #光源对X 轴我的影响\ndy = np.cos(vec_el)*np.sin(vec_az) #光源对Y 轴的影响\ndz = np.sin(vec_el) #光源对z 轴的影响\n\n\nb = 255*(dx\nuni_x + dy\nuni_y + dz*uni_z) #光源归一化\nb = b.clip(0,255) \n\n\nim = Image.fromarray(b.astype('uint8')) #重构图像\nim.save(\"D:\\data\\ipython\\lyfHD.jpg\")\n\n", "Tag": "算法分析"}
{"Answer": "你都没有输出打印，怎么会有值显现嘛", "Konwledge_Point": "应对NP完全问题", "Question": "pycharm的问题\nimport numpy as np\ndef AND(x1,x2):\n    x=np.array([x1,x2])\n    w=np.array([0.5,0.5])\n    b=-0.7\n    tmp=np.sum(w*x)+b\n    if tmp<=0:\n        return 0\n    else:\n        return 1\nAND(1,1)\n\n\n\n\n为什么点了运行没有显示结果", "Tag": "算法分析"}
{"Answer": "出错在第62行，重点检查flash_lat,flash_lon数组的值，是不是符合函数参数要求，在concatenate时，如果横向合并的话要加参数axis=1,另外再检查hexbin函数参数使用是否有问题。\nmatplotlib.pyplot.hexbin — Matplotlib 3.4.3 documentation\n\n\n\n\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hexbin.html\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python中出现“ ValueError: zero-size array to reduction operation maximum which has no identity”错误\n以下是我编写的程序，最终想要绘出闪电的图\n\n\n尝试运行后报错\n\n\n请问这个问题应该怎么解决呢？以及我编写的程序中是不是还有其他错误啊？谢谢啦", "Tag": "算法分析"}
{"Answer": "【简单叙述】你装的是python3.9，我也是原来装的python3.9，在import pandas时，就没有用，都出现你这个问题。\n【解决办法】卸载python3.9，装回python3.7，就不再出现这个问题。希望能帮到你!\nTraceback (most recent call last):\n  File \"E:\\pythonlearn\\learn\\learn.py\", line 6, in <module>\n    import pandas as pd\n  File \"D:\\python3.9.0a4(64-bit)\\lib\\site-packages\\pandas\\__init__.py\", line 16, in <module>\n    raise ImportError(\nImportError: Unable to import required dependencies:\nnumpy: DLL load failed while importing mtrand: 找不到指定的程序。\n", "Konwledge_Point": "应对NP完全问题", "Question": "pandas\\__init__.py报错\n这是我想爬某乎的程序\n\n\n\n \n\n\n\n\n\nimport requests\nimport json\nimport time\nimport re\nimport datetime\nimport pandas as pd\n\n\ndef get_data(url):\n    '''\n    功能：访问 url 的网页，获取网页内容并返回\n    参数：\n        url ：目标网页的 url\n    返回：目标网页的 html 内容\n    '''\n    headers = {\n        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36',\n    }\n\n    try:\n        r = requests.get(url, headers=headers)\n        r.raise_for_status()\n        return r.text\n\n    except requests.HTTPError as e:\n        print(e)\n        print(\"HTTPError\")\n    except requests.RequestException as e:\n        print(e)\n    except:\n        print(\"Unknown Error !\")\n\n\ndef parse_data(html):\n    '''\n    功能：提取 html 页面信息中的关键信息，并整合一个数组并返回\n    参数：html 根据 url 获取到的网页内容\n    返回：存储有 html 中提取出的关键信息的数组\n    '''\n    json_data = json.loads(html)['data']\n    comments = []\n\n    try:\n        for item in json_data:\n            comment = []\n            comment.append(item['author']['name'])  # 姓名\n            comment.append(item['author']['gender'])  # 性别\n            # comment.append(item['author']['url'])     # 个人主页\n            comment.append(item['voteup_count'])  # 点赞数\n            comment.append(item['comment_count'])  # 评论数\n            # comment.append(item['url'])               # 回答链接\n            comments.append(comment)\n\n        return comments\n\n    except Exception as e:\n        print(comment)\n        print(e)\n\n\ndef save_data(comments):\n    '''\n    功能：将comments中的信息输出到文件中/或数据库中。\n    参数：comments 将要保存的数据\n    '''\n    filename = 'Data/comments.csv'\n\n    dataframe = pd.DataFrame(comments)\n\n    dataframe.to_csv(filename, mode='a', index=False, sep=',', header=False)\n    # dataframe.to_csv(filename, mode='a', index=False, sep=',', header=['name','gender','user_url','voteup','cmt_count','url'])\n\n\ndef main():\n    url = 'https://www.zhihu.com/api/v4/questions/440710739/answers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cattachment%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Cis_labeled%2Cpaid_info%2Cpaid_info_content%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_recognized%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics%3Bdata%5B%2A%5D.settings.table_of_content.enabled&limit=3&offset=3&platform=desktop&sort_by=default'\n\n    # get total cmts number\n    html = get_data(url)\n    totals = json.loads(html)['paging']['totals']\n\n    print(totals)\n    print('---' * 10)\n\n    page = 0\n\n    while (page < totals):\n        url = 'https://www.zhihu.com/api/v4/questions/440710739/answers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cattachment%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Cis_labeled%2Cpaid_info%2Cpaid_info_content%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_recognized%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics%3Bdata%5B%2A%5D.settings.table_of_content.enabled&limit=3&offset=' + str(\n            page) + '&platform=desktop&sort_by=default'\n\n        html = get_data(url)\n        comments = parse_data(html)\n        save_data(comments)\n\n        print(page)\n        page += 3\n\n\nif __name__ == '__main__':\n    main()\n    print(\"完成！！\")\n\n\n\n\n\n#以下是我的报错\n\n\n\n\nTraceback (most recent call last):\n\n  File \"E:\\pythonlearn\\learn\\learn.py\", line 6, in \n\n    import pandas as pd\n\n  File \"D:\\python3.9.0a4(64-bit)\\lib\\site-packages\\pandas\\__init__.py\", line 16, in \n\n    raise ImportError(\n\nImportError: Unable to import required dependencies:\n\nnumpy: DLL load failed while importing mtrand: 找不到指定的程序。\n\n\n\n \n\n\n\n下面是__init__py\n\n\n\n\n\n# flake8: noqa\n\n__docformat__ = \"restructuredtext\"\n\n# Let users know if they're missing any of our hard dependencies\nhard_dependencies = (\"numpy\", \"pytz\", \"dateutil\")\nmissing_dependencies = []\n\nfor dependency in hard_dependencies:\n    try:\n        __import__(dependency)\n    except ImportError as e:\n        missing_dependencies.append(f\"{dependency}: {e}\")\n\nif missing_dependencies:\n    raise ImportError(\n        \"Unable to import required dependencies:\\n\" + \"\\n\".join(missing_dependencies)\n    )\ndel hard_dependencies, dependency, missing_dependencies\n\n# numpy compat\nfrom pandas.compat.numpy import (\n    np_version_under1p17 as _np_version_under1p17,\n    np_version_under1p18 as _np_version_under1p18,\n    is_numpy_dev as _is_numpy_dev,\n)\n\ntry:\n    from pandas._libs import hashtable as _hashtable, lib as _lib, tslib as _tslib\nexcept ImportError as e:  # pragma: no cover\n    # hack but overkill to use re\n    module = str(e).replace(\"cannot import name \", \"\")\n    raise ImportError(\n        f\"C extension: {module} not built. If you want to import \"\n        \"pandas from the source directory, you may need to run \"\n        \"'python setup.py build_ext --force' to build the C extensions first.\"\n    ) from e\n\nfrom pandas._config import (\n    get_option,\n    set_option,\n    reset_option,\n    describe_option,\n    option_context,\n    options,\n)\n\n# let init-time option registration happen\nimport pandas.core.config_init\n\nfrom pandas.core.api import (\n    # dtype\n    Int8Dtype,\n    Int16Dtype,\n    Int32Dtype,\n    Int64Dtype,\n    UInt8Dtype,\n    UInt16Dtype,\n    UInt32Dtype,\n    UInt64Dtype,\n    Float32Dtype,\n    Float64Dtype,\n    CategoricalDtype,\n    PeriodDtype,\n    IntervalDtype,\n    DatetimeTZDtype,\n    StringDtype,\n    BooleanDtype,\n    # missing\n    NA,\n    isna,\n    isnull,\n    notna,\n    notnull,\n    # indexes\n    Index,\n    CategoricalIndex,\n    Int64Index,\n    UInt64Index,\n    RangeIndex,\n    Float64Index,\n    MultiIndex,\n    IntervalIndex,\n    TimedeltaIndex,\n    DatetimeIndex,\n    PeriodIndex,\n    IndexSlice,\n    # tseries\n    NaT,\n    Period,\n    period_range,\n    Timedelta,\n    timedelta_range,\n    Timestamp,\n    date_range,\n    bdate_range,\n    Interval,\n    interval_range,\n    DateOffset,\n    # conversion\n    to_numeric,\n    to_datetime,\n    to_timedelta,\n    # misc\n    Flags,\n    Grouper,\n    factorize,\n    unique,\n    value_counts,\n    NamedAgg,\n    array,\n    Categorical,\n    set_eng_float_format,\n    Series,\n    DataFrame,\n)\n\nfrom pandas.core.arrays.sparse import SparseDtype\n\nfrom pandas.tseries.api import infer_freq\nfrom pandas.tseries import offsets\n\nfrom pandas.core.computation.api import eval\n\nfrom pandas.core.reshape.api import (\n    concat,\n    lreshape,\n    melt,\n    wide_to_long,\n    merge,\n    merge_asof,\n    merge_ordered,\n    crosstab,\n    pivot,\n    pivot_table,\n    get_dummies,\n    cut,\n    qcut,\n)\n\nimport pandas.api\nfrom pandas.util._print_versions import show_versions\n\nfrom pandas.io.api import (\n    # excel\n    ExcelFile,\n    ExcelWriter,\n    read_excel,\n    # parsers\n    read_csv,\n    read_fwf,\n    read_table,\n    # pickle\n    read_pickle,\n    to_pickle,\n    # pytables\n    HDFStore,\n    read_hdf,\n    # sql\n    read_sql,\n    read_sql_query,\n    read_sql_table,\n    # misc\n    read_clipboard,\n    read_parquet,\n    read_orc,\n    read_feather,\n    read_gbq,\n    read_html,\n    read_json,\n    read_stata,\n    read_sas,\n    read_spss,\n)\n\nfrom pandas.io.json import _json_normalize as json_normalize\n\nfrom pandas.util._tester import test\nimport pandas.testing\nimport pandas.arrays\n\n# use the closest tagged version if possible\nfrom ._version import get_versions\n\nv = get_versions()\n__version__ = v.get(\"closest-tag\", v[\"version\"])\n__git_version__ = v.get(\"full-revisionid\")\ndel get_versions, v\n\n\n# GH 27101\ndef __getattr__(name):\n    import warnings\n\n    if name == \"datetime\":\n        warnings.warn(\n            \"The pandas.datetime class is deprecated \"\n            \"and will be removed from pandas in a future version. \"\n            \"Import from datetime module instead.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n        from datetime import datetime as dt\n\n        return dt\n\n    elif name == \"np\":\n\n        warnings.warn(\n            \"The pandas.np module is deprecated \"\n            \"and will be removed from pandas in a future version. \"\n            \"Import numpy directly instead\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        import numpy as np\n\n        return np\n\n    elif name in {\"SparseSeries\", \"SparseDataFrame\"}:\n        warnings.warn(\n            f\"The {name} class is removed from pandas. Accessing it from \"\n            \"the top-level namespace will also be removed in the next version\",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n        return type(name, (), {})\n\n    elif name == \"SparseArray\":\n\n        warnings.warn(\n            \"The pandas.SparseArray class is deprecated \"\n            \"and will be removed from pandas in a future version. \"\n            \"Use pandas.arrays.SparseArray instead.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        from pandas.core.arrays.sparse import SparseArray as _SparseArray\n\n        return _SparseArray\n\n    raise AttributeError(f\"module 'pandas' has no attribute '{name}'\")\n\n\n# module level doc-string\n__doc__ = \"\"\"\npandas - a powerful data analysis and manipulation library for Python\n=====================================================================\n\n**pandas** is a Python package providing fast, flexible, and expressive data\nstructures designed to make working with \"relational\" or \"labeled\" data both\neasy and intuitive. It aims to be the fundamental high-level building block for\ndoing practical, **real world** data analysis in Python. Additionally, it has\nthe broader goal of becoming **the most powerful and flexible open source data\nanalysis / manipulation tool available in any language**. It is already well on\nits way toward this goal.\n\nMain Features\n-------------\nHere are just a few of the things that pandas does well:\n\n  - Easy handling of missing data in floating point as well as non-floating\n    point data.\n  - Size mutability: columns can be inserted and deleted from DataFrame and\n    higher dimensional objects\n  - Automatic and explicit data alignment: objects can be explicitly aligned\n    to a set of labels, or the user can simply ignore the labels and let\n    `Series`, `DataFrame`, etc. automatically align the data for you in\n    computations.\n  - Powerful, flexible group by functionality to perform split-apply-combine\n    operations on data sets, for both aggregating and transforming data.\n  - Make it easy to convert ragged, differently-indexed data in other Python\n    and NumPy data structures into DataFrame objects.\n  - Intelligent label-based slicing, fancy indexing, and subsetting of large\n    data sets.\n  - Intuitive merging and joining data sets.\n  - Flexible reshaping and pivoting of data sets.\n  - Hierarchical labeling of axes (possible to have multiple labels per tick).\n  - Robust IO tools for loading data from flat files (CSV and delimited),\n    Excel files, databases, and saving/loading data from the ultrafast HDF5\n    format.\n  - Time series-specific functionality: date range generation and frequency\n    conversion, moving window statistics, date shifting and lagging.\n\"\"\"\n", "Tag": "算法分析"}
{"Answer": "参数值过大了数值溢出，把规模缩小", "Konwledge_Point": "应对NP完全问题", "Question": "如何解决RuntimeWarning: overflow encountered in exp？\n显示错误提示如下：\nFlsf.py:92: RuntimeWarning: overflow encountered in exp\n  RP += (np.exp(M_PS[row_C, n]) - np.exp(-M_PS[row_C, n])) / (np.exp(M_PS[row_C, n]) + np.exp(-M_PS[row_C, n]))\nFlsf.py:92: RuntimeWarning: invalid value encountered in double_scalars\n  RP += (np.exp(M_PS[row_C, n]) - np.exp(-M_PS[row_C, n])) / (np.exp(M_PS[row_C, n]) + np.exp(-M_PS[row_C, n]))", "Tag": "算法分析"}
{"Answer": "把print(computeCost((x, y, theta))改成print(computeCost(x, y, theta)）", "Konwledge_Point": "应对NP完全问题", "Question": "python程序的报错小问题\n刚开始学做机器学习的时候遇到的问题\n\n\n问题相关代码 python\n\n\ndef computeCost(x, y, theta):#代价函数\n    \ninner\n=np.power(((x*theta.T)-y),2)\n    return np.sum(inner)/(2*len(x))\ndata.insert(0,\n'ones'\n,1)#data加一列1\n\n#print(data.head())\n\n\ncols\n=data.shape[1]#\ncols\n=data列数\n\nX\n=data.iloc[:,0:cols-1]#pandas iloc提取前两列\n\nY\n=data.iloc[:,cols-1:cols]#pandas iloc提取后1列\n\nx\n=np.matrix(X.values)\n\ny\n=np.matrix(Y.values)\n\ntheta\n=np.matrix(np.array([0,0]))\n\n#print(x.shape,theta.shape,y.shape)#看矩阵维度\n\n\nprint\n(computeCost((x, y, theta)))\n\n\n\n运行结果及报错内容\n\n\nTraceback (most recent \ncall\n \nlast\n):\n  File \n\"D:\\pythonProject\\ml\\work1.py\"\n, \nline\n \n24\n, in \n\n\n    \nprint\n(computeCost((\nx\n, \ny\n, theta)))\nTypeError: computeCost() missing \n2\n required positional arguments: \n'y'\n \nand\n \n'theta'\n\n\n\n\n\n\n函数computeCost缺少 2 个必需 位置参数\n我这里不是已经定义了y和theta了吗？\n想请教一下是哪里出了问题", "Tag": "算法分析"}
{"Answer": "你调用的库里是封装了函数的，肯定不能简单的计算时间复杂度为1，至于这个函数需要时间复杂度是多久，需要你思考其内部是否必须有for循环才能实现", "Konwledge_Point": "应对NP完全问题", "Question": "关于时间复杂度计算的问题\ngudinjuzhen_1 = \nnp\n.ones(((math.\nfactorial\n(\nnum\n-\n2\n)),\n1\n),dtype=\nnp\n.int32) \ngudinjuzhen_0 = \nnp\n.zeros(((math.\nfactorial\n(\nnum\n-\n2\n)),\nnum\n-\n2\n),dtype=\nnp\n.int32)\ngudinjuzhen = \nnp\n.column_stack((gudinjuzhen_1,gudinjuzhen_0))\n\n\n\n这样时间复杂度能算作3吗，我代码里面很多单步运算时间都不同的，我能把每个单步都算作1去计算时间复杂度吗", "Tag": "算法分析"}
{"Answer": "\npd.merge(a.fillna(b), b.fillna(a), on=['药品名','商品号','条形码'])\n ", "Konwledge_Point": "应对NP完全问题", "Question": "pandas的merge函数多键合并的问题\n\n\na = pd.DataFrame({'药品名':['感康','感冒灵','皮炎平'], '商品号':[np.nan,'0023','0045'],'条形码':['69000000001',np.nan,'69000000003'], '规格':['12片','8袋','15克']})\nb = pd.DataFrame({'药品名':['感康','感冒灵','皮炎平'], '商品号':['0012','0023',np.nan,],'条形码':['69000000001','69000000002','69000000003'], '零售价':[10, 9, 7.5]})\n\n\n\n\n\n\n我想通过‘商品号’和‘条形码’这两列，合并这两个表格\n\n但这两列，只要其中一列相等即可合并，我要的结果如下：\n\n\n\n\n\n\n如何用最少的代码，实现这个结果？", "Tag": "算法分析"}
{"Answer": "暂时没有解决方法 。。。。。。。。。。。。。。。。。。。。。。。。。。。。。", "Konwledge_Point": "应对NP完全问题", "Question": "跪求大神帮我检查一下BP算法计算异或问题程序的错误\n感觉是权值更新部分出错了 \n\n            运行出来都是0.5左右  不知道为啥\n\n\n\nimport math\nimport numpy as np\nimport random as rd\ndef sigmoid(x):\n    f = 1/(1+np.exp(-1*x))\n    return f\n\ndef sigmoid_(x):\n\n    f = [[sigmoid(x[i][j])*(1-sigmoid(x[i][j]))  for j in range(len(x[i]))] for i in range(len(x))]\n    return f\n\ndef differentail_matrix(x):\n    f = np.diag(x[0])\n    return f\n\ninput_X = np.array([[0,0],[1,0],[0,1],[1,1]])\n#print(input_X.shape)\ninput_Y = np.array([[0],[1],[1],[0]])\n#print(input_Y.shape)\n\nW1 = np.random.rand(2,2) #later input,former output\nb1 = np.zeros([2,1])\n#W1 = np.array([[2,2],[-1,-1]])\n#b1 = np.array([[-1],[1.5]])\nW2 = np.random.rand(1,2)\nb2 = np.random.rand(1,1)\n# W2 = np.array([1,1])\n# W2.shape = 1,2\n# b2 = np.array([-1.0])\n#print(W1,b1)\n\nalpha = 0.05 #learn rate\nfor k in range(10000):\n\n    r = rd.sample([0,1,2,3], 1)           #随机抽取\n\n\n\n    X = np.array(input_X[r])\n    X.shape = 2,1                        #transpose\n    Y = np.array(input_Y[r])\n\n    out1 = sigmoid(np.dot(W1,X) + b1)\n    pred_y = sigmoid(np.dot(W2,out1) + b2)\n    #print('predy:',pred_y)\n    err = Y-pred_y\n    #print('err:',err)\n\n\n    #back propagation\n\n    s2 = -2*sigmoid(pred_y)*(1-sigmoid(pred_y))*err         #计算敏感度\n\n    temp = sigmoid_(out1)\n\n    temp = np.array(temp)\n    temp.shape = 1, 2\n    temp2 = np.array(differentail_matrix(temp))\n    s1 = np.dot(temp2, W2.T)*s2\n\n    W2 = W2-alpha*s2*np.transpose(out1)               #权值更新\n    b2 = b2 - alpha*s2\n\n    W1 = W1 - alpha*np.dot(s1,np.transpose(X))\n    b1 = b1 - alpha*s1\n\n\n    #print('第:',i,'次迭代','\\n','权值1',W1,'\\n',b1)\n    #print()\n\n\n    if k%500 == 0:\n        out1 = sigmoid(np.dot(W1,np.transpose(input_X)) + b1)\n        pred_y = sigmoid(np.dot(W2,out1) + b2)\n        print(pred_y)\n", "Tag": "算法分析"}
{"Answer": "```\r\nint N,i,j,r,a[N][N],b[N][N],t; //这样写是错的，a和b的数组分配内存错了\r\nscanf(\"%d\",&N);\r\n```\r\n\r\n```\r\n#include \"stdio.h\"\r\n#include \"stdlib.h\"\r\nint main ()\r\n{\r\n\tint N,i,j,r,a[100][100],b[100][100],t;\r\n\tscanf(\"%d\",&N);\r\n\tif(N>2&&N<130)\r\n\t{\r\n\t\tfor(i=0;i<N;i++)\r\n\t\t\tfor(j=0;j<N;j++)\r\n\t\t\t\tscanf(\"%d\",&a[i][j]);\r\n\t\tfor(i=0;i<=N;i++)\r\n\t\t\tfor(j=0;j<=N;j++)\r\n\t\t\t\tb[i][j]=a[j][i];\r\n\t\tfor(i=0;i<N;i++)\r\n\t\t{\r\n\t\t\tfor(j=0;j<N;j++)\r\n\t\t\t{\r\n\t\t\t\tif(j<N-1)\r\n\t\t\t\t\tprintf(\"%d \",b[i][j]);\r\n\t\t\t\telse\r\n\t\t\t\t\tprintf(\"%d \",b[i][j]);\r\n\t\t\t}\r\n\t\t\tprintf(\"\\n\");\r\n\t\t}\r\n\t}\r\n\treturn 0;\r\n}\r\n```", "Konwledge_Point": "应对NP完全问题", "Question": "请问为什么我的矩阵输出全为0？\n把矩阵 A 的行换成相应的列，得到的新矩阵称为 A 的转置矩阵。现在给定任意的一个 n 阶方阵 S , 求出其转置矩阵.\n\n\n\n输入描述第一行给定一个正整数 n (2 < n < 100). 接下来有 n 行，每行 n 个整数，代表方阵中第 i 行第 j 列的数.\n\n\n\n输出描述\n\n输出该方阵的转置矩阵.\n\n\n\n注意数字与数字之间用空格分开，每行最后一个数字后面仅跟换行符.\n\n\n\n样例输入\n\n3\n\n1 2 3\n\n4 5 6\n\n7 8 9\n\n样例输出\n\n1 4 7\n\n2 5 8\n\n3 6 9\n\n#include\n\n#define N130\n\nint main ()\n\n{\n\nint N,i,j,r,a[N][N],b[N][N],t;\n\nscanf(\"%d\",&N);\n\nif(N>2&&N<130)\n\n{\n\nfor(i=0;i<N;i++)\n\nfor(j=0;j<N;j++)\n\nscanf(\"%d\",&a[i][j]);\n\nfor(i=0;i<=N;i++)\n\nfor(j=0;j<=N;j++)\n\nb[i][j]=a[j][i];\n\nfor(i=0;i<N;i++)\n\n{\n\n    for(j=0;j<N;j++)\n\n    {\n\nif(j<N-1) \n\n    printf(\"%d \",b[i][j]);\n\n    else\n\n    printf(\"%d \",b[i][j]);\n\n}\n\n    printf(\"\\n\");\n\n}\n\n}\n\nreturn 0;\n\n}", "Tag": "算法分析"}
{"Answer": "按照你这个方位地址来看，这个index.xhtml文件应该直接位于你的webContent目录下才能这样访问。404说明这个文件路径不是直接在这个目录下的。\r\n检查下你的文件路径，不介意的话贴出来看看。", "Konwledge_Point": "应对NP完全问题", "Question": "这个错误怎么解决，谢谢！\n", "Tag": "算法分析"}
{"Answer": "最后一句data.loc[data['质量等级'=='优',:]]写错了，'质量等级'=='优'不能作为索引。要使用布尔索引，这样改写一下：\nprint(data.loc[data['质量等级']=='优',:])\n\n如对你有帮助，请点击下采纳。", "Konwledge_Point": "应对NP完全问题", "Question": "关于数据集的抽样问题\n问题遇到的现象和发生背景\n\n\n导入excel北京空气质量数据，然后用numpy抽样\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport numpy as np\nimport pandas as pd\ndata=pd.read_excel(r'C:\\Users\\18464\\Desktop\\北京市空气质量数据.xlsx')\nnp.random.seed(123)\nsampler=np.random.randint(0,len(data),10)\nprint(sampler)\nsample=np.random.permutation(len(data))[:10]\nprint(sample)\ndata.take(sampler)\ndata.loc[data['质量等级'=='优',:]]\n\n\n运行结果及报错内容\n\n\nTypeError: '(False, slice(None, None, None))' is an invalid key\n\n\n我的解答思路和尝试过的方法\n\n\n不是很懂为什么会出错\n\n\n我想要达到的结果\n\n\n得到随机抽样的结果", "Tag": "算法分析"}
{"Answer": "\na = [21, 2.5 , -23.2, 8.4, -4, 3, -1.03]\n\n#遍历列表\nfor i in range(len(a)):\n    if a[i]<0:  #发现负数元素就取反\n        a[i] = -a[i]\nprint(a)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python怎么将转换列表里的负数,转换为正数\na = [21, 2.5 , -23.2, 8.4, -4, 3,  -1.03] ", "Tag": "算法分析"}
{"Answer": "Union 是 Python 的类型提示库中的一个类型，用来表示可能是多种类型之一。在这里，Union[Tuple[np.ndarray, np.ndarray], Tuple[float, float]] 表示返回值可能是一个包含两个 numpy 数组的元组，或者是一个包含两个浮点数的元组。talib.MAX(self.high, n) 和 talib.MIN(self.low, n) 是调用的 TA-Lib 库中的函数，这些函数会返回一个 numpy 数组，所以可以直接赋值给 np.ndarray 类型的变量。", "Konwledge_Point": "应对NP完全问题", "Question": "关于python中的一些语法问题\ndef donchian(\n        self, n: int, array: bool = False\n    ) -> \nUnion\n[\n        \nTuple\n[np.ndarray, np.ndarray],\n        \nTuple\n[float, float]\n    ]:\n        \n\"\"\"\n        Donchian Channel.\n        \"\"\"\n\n        up: np.ndarray = talib.MAX(self.high, n)\n        down: np.ndarray = talib.MIN(self.low, n)\n\n        \nif\n array:\n            \nreturn\n up, down\n        \nreturn\n up[-\n1\n], down[-\n1\n]\n\n\n\n在vn.py看到的一段代码，关于donchian的定义，想请教\n1.对于donchian后的-> Union， Union不是一个类吗，在这里代表什么呢？\n2.对于up: np.ndarray = talib.MAX(self.high, n)，为什么np.ndarray作为已定义的类，可以被talib.MAX/MIN赋值？\n谢谢！", "Tag": "算法分析"}
{"Answer": "import pandas as pd\nimport numpy as np\ndata={\n'AUD/USD': [1.72,1.74,np.nan,168,1.75,1.71,],\n'EUR/AUD': [0.62,0.63,0.59,0.64,np.nan,0.61,],}\nindex = ['2022-10-1','2022-10-2','2022-10-3','2032-10-4','2022-10-7','2022-10-8']\ndf =pd.DataFrame(data,index)\n\nsel_q1=['2022-10-1','2022-10-7']\nq1=df.loc[sel_q1]\nprint(q1)\n\nprint('\\n\\n')\n\n(start_q2,stop_q2)=(0,2)\nq2=df.iloc[start_q2:stop_q2]\nprint(q2)\n\nprint('\\n\\n')\n\n(start_q3,stop_q3)=(0,2)\nq3=df[start_q3:stop_q3]\nprint(q3)\n\nprint('\\n\\n')\n\nrow_sel_q4=['2022-10-1','2022-10-2']\ncol_sel_q4=['AUD/USD']\nq4=df.loc[row_sel_q4,col_sel_q4]\nprint(q4)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "根据提供的DataFrame df回的下列问题\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "https://www.jb51.net/article/152595.htm", "Konwledge_Point": "应对NP完全问题", "Question": "python通过初等行变换求逆矩阵的问题\n从线代角度来看感觉没什么问题，但是测试的时候发现对4阶以下的矩阵能够正确求得其逆矩阵，但是高于5阶就只能偶尔正确，不知道出现了什么问题", "Tag": "算法分析"}
{"Answer": "MiniGoogLeNet的代码有点问题，参考下面这个。\n\n# import the necessary packages\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras import backend as K\n \nclass MiniGoogLeNet:\n    @staticmethod\n    def conv_module(x, K, kX, kY, stride, chanDim, padding=\"same\"):\n        # define a CONV => BN => RELU pattern\n        x = Conv2D(K, (kX, kY), strides=stride, padding=padding)(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = Activation(\"relu\")(x)\n        # return the block\n        return x\n \n    @staticmethod\n    def inception_module(x, numK1x1, numK3x3, chanDim):\n        # define two CONV modules, then concatenate across the\n        # channel dimension\n        conv_1x1 = MiniGoogLeNet.conv_module(x, numK1x1, 1, 1, (1, 1), chanDim)\n        conv_3x3 = MiniGoogLeNet.conv_module(x, numK3x3, 3, 3, (1, 1), chanDim)\n        x = concatenate([conv_1x1, conv_3x3], axis=chanDim)\n        # return the block\n        return x\n \n    @staticmethod\n    def downsample_module(x, K, chanDim):\n        # define the CONV module and POOL, then concatenate\n        # across the channel dimensions\n        conv_3x3 = MiniGoogLeNet.conv_module(x, K, 3, 3, (2, 2), chanDim, padding=\"valid\")\n        pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n        x = concatenate([conv_3x3, pool], axis=chanDim)\n        # return the block\n        return x\n \n    @staticmethod\n    def build(width, height, depth, classes):\n        # initialize the input shape to be \"channels last\" and the\n        # channels dimension itself\n        inputShape = (height, width, depth)\n        chanDim = -1\n \n        # if we are using \"channels first\", update the input shape\n        # and channels dimension\n        if K.image_data_format() == \"channels_first\":\n            inputShape = (depth, height, width)\n            chanDim = 1\n        # define the model input and first CONV module\n        inputs = Input(shape=inputShape)\n        x = MiniGoogLeNet.conv_module(inputs, 96, 3, 3, (1, 1), chanDim)\n        # two Inception modules followed by a downsample module\n        x = MiniGoogLeNet.inception_module(x, 32, 32, chanDim)\n        x = MiniGoogLeNet.inception_module(x, 32, 48, chanDim)\n        x = MiniGoogLeNet.downsample_module(x, 80, chanDim)\n \n        # four Inception modules followed by a downsample module\n        x = MiniGoogLeNet.inception_module(x, 112, 48, chanDim)\n        x = MiniGoogLeNet.inception_module(x, 96, 64, chanDim)\n        x = MiniGoogLeNet.inception_module(x, 80, 80, chanDim)\n        x = MiniGoogLeNet.inception_module(x, 48, 96, chanDim)\n        x = MiniGoogLeNet.downsample_module(x, 96, chanDim)\n \n        # two Inception modules followed by global POOL and dropout\n        x = MiniGoogLeNet.inception_module(x, 176, 160, chanDim)\n        x = MiniGoogLeNet.inception_module(x, 176, 160, chanDim)\n        x = AveragePooling2D((7, 7))(x)\n        x = Dropout(0.5)(x)\n \n        # softmax classifier\n        x = Flatten()(x)\n        x = Dense(classes)(x)\n        x = Activation(\"softmax\")(x)\n \n        # create the model\n        model = Model(inputs, x, name=\"googlenet\")\n \n        # return the constructed network architecture\n        return model\n", "Konwledge_Point": "应对NP完全问题", "Question": "python使用model.compile方法的时候，遇到AttributeError: 'NoneType' object has no attribute 'compile'这个问题\n在跑《deep learning for computer vision with python》第二本第11章的googlenet程序时候，遇到使用model.compile方法出现bug。'NoneType' object has no attribute 'compile'\n\n\n\n\nimport\n matplotlib\n\nmatplotlib.use(\n\"Agg\"\n)\n\n\nfrom\n sklearn.preprocessing \nimport\n LabelBinarizer\n\nfrom\n pyimagesearch.nn.conv \nimport\n MiniGoogLeNet\n\nfrom\n pyimagesearch.callbacks \nimport\n TrainingMonitor\n\nfrom\n keras.preprocessing.image \nimport\n ImageDataGenerator\n\nfrom\n keras.callbacks \nimport\n LearningRateScheduler\n\nfrom\n keras.optimizers \nimport\n SGD\n\nfrom\n keras.datasets \nimport\n cifar10\n\nimport\n numpy \nas\n np\n\nimport\n argparse\n\nimport\n os\n\nNUM_EPOCHS = \n70\n\nINIT_LR = \n5e-3\n\n\n\n\ndef\n \npoly_decay\n(\nepoch\n):\n    maxEpochs = NUM_EPOCHS\n    baseLR = INIT_LR\n    power = \n1.0\n\n\n    alpha = baseLR * (\n1\n - (epoch / \nfloat\n(maxEpochs))) ** power\n\n    \nreturn\n alpha\n\n\nap = argparse.ArgumentParser()\nap.add_argument(\n\"-m\"\n, \n\"--model\"\n, required=\nTrue\n,\n                \nhelp\n=\n\"path to output model\"\n)\nap.add_argument(\n\"-o\"\n, \n\"--output\"\n, required=\nTrue\n,\n                \nhelp\n=\n\"path to output directory (logs, plots, etc.)\"\n)\nargs = \nvars\n(ap.parse_args())\n\n\nprint\n(\n\"[INFO] loading CIFAR-10 data...\"\n)\n((trainX, trainY), (testX, testY)) = cifar10.load_data()\ntrainX = trainX.astype(\n\"float\"\n)\ntestX = testX.astype(\n\"float\"\n)\n\nmean = np.mean(trainX, axis=\n0\n)\ntrainX -= mean\ntestX -= mean\n\nlb = LabelBinarizer()\ntrainY = lb.fit_transform(trainY)\ntestY = lb.transform(testY)\n\naug = ImageDataGenerator(width_shift_range=\n0.1\n, height_shift_range=\n0.1\n,\n                         horizontal_flip=\nTrue\n,\n                         fill_mode=\n\"nearest\"\n)\n\nfigPath = os.path.sep.join([args[\n\"output\"\n], \n\"{}.png\"\n.\nformat\n(os.getpid())])\njsonPath = os.path.sep.join([args[\n\"output\"\n], \n\"{}.json\"\n.\nformat\n(os.getpid())])\ncallbacks = [TrainingMonitor(figPath, jsonPath=jsonPath),\n             LearningRateScheduler(poly_decay)]\n\n\nprint\n(\n\"[INFO] compiling model...\"\n)\nopt = SGD(lr=INIT_LR, momentum=\n0.9\n)\nmodel = MiniGoogLeNet.build(width=\n32\n, height=\n32\n, depth=\n3\n, classes=\n10\n)\nmodel.\ncompile\n(loss=\n\"categorical_crossentropy\"\n, optimizer=opt,\n              metrics=[\n\"accuracy\"\n])\n\n\nprint\n(\n\"[INFO] training network...\"\n)\nmodel.fit_generator(aug.flow(trainX, trainY, batch_size=\n64\n),\n                    validation_data=(testX, testY), steps_per_epoch=\nlen\n(trainX) // \n64\n,\n                    epochs=NUM_EPOCHS, callbacks=callbacks, verbose=\n1\n)\n\nprint\n(\n\"[INFO] serializing network...\"\n)\nmodel.save(args[\n\"model\"\n])\n\n\n\n\n在terminal端输入 python googlenet_cifar10.py 指令的时候 出现'NoneType' object has no attribute 'compile'的bug", "Tag": "算法分析"}
{"Answer": "把plt.bar  中的left 换成x就可以了", "Konwledge_Point": "应对NP完全问题", "Question": "关于python matplotlib的问题\nimport matplotlib.pyplot as plt\nimport numpy as np\nbar_width = 0.4\nplt.bar(left = np.arange(len(school)),height = group_ms,label = 'MS', color = 'steelblue',width = bar_width)\nplt.bar(left = np.arange(len(school))+bar_width,height = group_gp,label = 'GP', color = 'indianred',width = bar_width)\nplt.xticks(np.arange(3)+0.2,school)\nplt.legend()\nplt.show()\n\n\n\n\n错误代码是：\n\n\n\nTypeError                                 Traceback (most recent call last)\n in \n      2 import numpy as np\n      3 bar_width = 0.4\n----> 4 plt.bar(left = np.arange(len(school)),height = group_ms,label = 'MS', color = 'steelblue',width = bar_width)\n      5 plt.bar(left = np.arange(len(school))+bar_width,height = group_gp,label = 'GP', color = 'indianred',width = bar_width)\n      6 plt.xticks(np.arange(3)+0.2,school)\n\nTypeError: bar() missing 1 required positional argument: 'x'\n\n\n\n\n\n数据是\n\n\n\n需要将两个学校的数据进行对比", "Tag": "算法分析"}
{"Answer": "内存错误，一般是运行时内存占用超过了内存配置", "Konwledge_Point": "应对NP完全问题", "Question": "Python编译出现MemoryError问题，救\n问题遇到的现象和发生背景\n\n\n用Jupyter学习纽约出租车运行情况分析建模时，在聚类的时候，运行下列代码出现MemoryError报错。\n\n\n问题相关代码\n\n\nkmeans = KMeans(n_clusters=\n15\n, random_state=\n2\n, n_init = \n10\n)\n.fit\n(loc_df)\nloc_df\n[\n'label'\n]\n = kmeans\n.labels_\n\n\nloc_df = loc_df\n.sample\n(\n200000\n)\nplt\n.figure\n(figsize = (\n10\n,\n10\n))\n\nfor\n \nlabel\n \nin\n loc_df\n.label\n.unique\n():\n    plt\n.plot\n(loc_df\n.longitude\n[loc_df.label == label]\n,loc_df\n.latitude\n[loc_df.label == label]\n,\n'.'\n,alpha = \n0.3\n, markersize = \n0.3\n)\n    \nplt\n.title\n(\n'NewYork Clusters'\n)\nplt\n.show\n()\n\n\n\n\n运行结果及报错内容\n\n\n---------------------------------------------------------------------------\nMemoryError                               Traceback (most recent call last)\n \nin\n ()\n----> 1 kmeans = KMeans(\nn_clusters\n=15, \nrandom_state\n=2, n_init = 10).fit(loc_df)\n      2 loc_df[\n'label'\n] = kmeans.labels_\n      3 \n      4 loc_df = loc_df.sample(200000)\n      5 plt.figure(figsize = (10,10))\n\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py \nin\n fit(self, X, y)\n    894                 \ntol\n=self.tol, \nrandom_state\n=random_state, \ncopy_x\n=self.copy_x,\n    895                 \nn_jobs\n=self.n_jobs, \nalgorithm\n=self.algorithm,\n--> 896                 \nreturn_n_iter\n=\nTrue\n)\n    897         return self\n    898 \n\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py \nin\n k_means(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\n    344                 X, n_clusters, \nmax_iter\n=max_iter, \ninit\n=init, \nverbose\n=verbose,\n    345                 \nprecompute_distances\n=precompute_distances, \ntol\n=tol,\n--> 346                 \nx_squared_norms\n=x_squared_norms, \nrandom_state\n=random_state)\n    347             # determine \nif\n these results are the best so far\n    348             \nif\n best_inertia is None \nor\n inertia < best_inertia:\n\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py \nin\n _kmeans_single_elkan(X, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\n    398         \nprint\n(\n'Initialization complete'\n)\n    399     centers, labels, n_iter = k_means_elkan(X, n_clusters, centers, \ntol\n=tol,\n--> 400                                             \nmax_iter\n=max_iter, \nverbose\n=verbose)\n    401     inertia = np.sum((X - centers[labels]) ** 2, \ndtype\n=np.float64)\n    402     return labels, inertia, centers, n_iter\n\nsklearn\\cluster\\_k_means_elkan.pyx \nin\n sklearn.cluster._k_means_elkan.k_means_elkan()\n\nMemoryError: \n\n\n\n\n请问这个怎么解决吖\n\n\n读取文件用的是\n\n\ndf\n = pd.read_csv(\n'yellow_tripdata_2012-01.csv'\n)\n", "Tag": "算法分析"}
{"Answer": "这个应该是线程回归，m是回归系数。", "Konwledge_Point": "应对NP完全问题", "Question": "谁知道这串代码求的是什么东西啊？\nm=(((np.mean(x)\nnp.mean(y))-np.mean(x\ny))/((np.mean(x)\nnp.mean(x))-np.mean(x\nx)))", "Tag": "算法分析"}
{"Answer": "不知道你说的是不是这个\nfrom numpy import *\n\nprint(pi)\n# print(sin())\n# print(cos())\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python从库里导入后如何定义\nimport numpy as np\nprint(np.pi)\na=np.array([[0，pi]，[pi/2，-pi/4])\nnp.sin\nnp.cos", "Tag": "算法分析"}
{"Answer": "安装numpy包，在命令行执行：\n\npip install numpy", "Konwledge_Point": "应对NP完全问题", "Question": "python代码问题\n运行import numpy as np\n\n\n\n显示ModuleNotFoundError: No module named 'numpy'", "Tag": "算法分析"}
{"Answer": "1、你下载的文件是否能被通用的解压缩工具打开并解压， 比如winrar2、用普通的解压缩文件，做一个测试zip文件，来测试你的代码。\n有的时候， 文件后缀是zip 不是真的zip格式。 ", "Konwledge_Point": "应对NP完全问题", "Question": "python使用zipfile模块，解压已有的zip文件时，报错File is not a zip file\n问题遇到的现象和发生背景\n\n\n就是在接API的时候，需要将数据以zip格式下载保存到本地，然后再解压，解析上传到数据库，但是在用zipfile，按照网上的方法，传入路径、传入文件名，最终都会报错：zipfile.BadZipFile: File is not a zip file\n\n\n问题相关代码，请勿粘贴截图\n\n\ndef unzip_file(zip_file,target_dir):\n    with zipfile.ZipFile(zip_file, \"r\") as zfile:\n        for file in zfile.namelist():\n            zfile.extract(file, target_dir)\n\n\n运行结果及报错内容\n\n\nTraceback (most recent call last):\n  File \"/Users/micra/PycharmProjects/pythonProject2/搜狗API.py\", line 92, in \n    run()\n  File \"/Users/micra/PycharmProjects/pythonProject2/搜狗API.py\", line 87, in run\n    unzip_file(\"demo1.zip\", \"demo\")\n  File \"/Users/micra/PycharmProjects/pythonProject2/搜狗API.py\", line 65, in unzip_file\n    z =  zipfile.ZipFile(zip_file, \"r\")\n  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/zipfile.py\", line 1269, in \ninit\n    self._RealGetContents()\n  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/zipfile.py\", line 1336, in _RealGetContents\n    raise BadZipFile(\"File is not a zip file\")\nzipfile.BadZipFile: File is not a zip file\n\n\n进程已结束，退出代码为 1\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "直接原因是 imdb_w2v[word] 这里的 word 是 Word2Vec 类型对象，不能用来做 imdb_w2v 的切片脚标，检查下 imdb_w2v 的定义。", "Konwledge_Point": "应对NP完全问题", "Question": "第五十行大家看看为啥报错啊，弄了好久了\n\n\n\n\n\nTraceback (most recent call last):\n  File \"E:/python/spider/svm&lstm/svm/train_svm.py\", line 130, in \n    get_train_vecs(x_train, x_test)  # 计算词向量并保存为train_vecs.npy,test_vecs.npy\n  File \"E:/python/spider/svm&lstm/svm/train_svm.py\", line 70, in get_train_vecs\n    train_vecs = np.concatenate([buildWordVector(z, n_dim, imdb_w2v) for z in x_train])\n  File \"E:/python/spider/svm&lstm/svm/train_svm.py\", line 70, in \n    train_vecs = np.concatenate([buildWordVector(z, n_dim, imdb_w2v) for z in x_train])\n  File \"E:/python/spider/svm&lstm/svm/train_svm.py\", line 49, in buildWordVector\n    t = imdb_w2v[word].reshape((1, size))\nTypeError: 'Word2Vec' object is not subscriptable", "Tag": "算法分析"}
{"Answer": "\n\nimport pandas as pd\nimport numpy as np\ndtr=pd.date_range(start='2016-5-20',end='2020-9-7',freq='Y')\ndf=pd.DataFrame({},index=dtr.year)\ndf['营业额']= np.random.randint(200,1000, size=(len(dtr)))\nmax_year = df['营业额'].idxmax()\nprint(\"每年营业额：\")\nprint(df)\nprint(\"营业额最大的年份：{}\".format(max_year))\n", "Konwledge_Point": "应对NP完全问题", "Question": "关于数据框的问题如何解决\n部分代码，这个问题怎么实现图片上的结果\n\n\n\n\nimport\n pandas \nas\n pd\n\nimport\n numpy \nas\n np\ndtr=pd.date_range(start=\n'2016-5-20'\n,end=\n'2020-9-7'\n,freq=\n'5d'\n)\ndf=pd.DataFrame({},\nindex\n=dtr)\ndf[\n'营业额'\n]= np.random.randint(\n200\n,\n1000\n, size=(len(dtr)))\n\n", "Tag": "算法分析"}
{"Answer": "astype() 对数据类型进行转换https://blog.csdn.net/weixin_42036641/article/details/86064700\n\nnp.random.random()函数https://blog.csdn.net/qq_40108803/article/details/107523991\n\nnp.random.randint()函数https://blog.csdn.net/qq_40643699/article/details/107986176", "Konwledge_Point": "应对NP完全问题", "Question": "paddle，dataset问题\n请问  image = np.random.random([784]).astype('float32')\n        label = np.random.randint(0, 9, (1, )).astype('int64')什么意思呢？", "Tag": "算法分析"}
{"Answer": "少了两个右括号\n\n修改如下：\ns = abs(np.sin((ang1-ang2)/180*np.pi)*l*h)+abs(np.cos((ang1-ang2)/180*np.pi)*w*h)\n", "Konwledge_Point": "应对NP完全问题", "Question": "plt.figure() 报错SyntaxError: invalid syntax\n问题遇到的现象和发生背景\n\n\njupyter notebook\n\n\n问题相关代码，请勿粘贴截图\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nplt\n.rcParams\n[\n'font.sans-serif'\n]\n=\n[\n'SimHei'\n]\n\nplt\n.rcParams\n[\n'axes.unicode_minus'\n]\n=False\n\n\nl = np\n.linspace\n(\n1\n,\n15\n,\n100\n)\nh = \n10\n\nw = \n10\n\nang1 = \n30\n\nang2 = \n45\n\n\n\ns = abs(np\n.sin\n((ang1-ang2)/\n180\n*np.pi)*l*h+abs(np\n.cos\n((ang1-ang2)/\n180\n*np.pi)*w*h\n\n\nplt\n.figure\n()\nplt\n.xlabel\n(\n\"长度\"\n)\nplt\n.ylabel\n(\n\"面积\"\n)\nplt\n.title\n(\n\"面积\"\n)\nplt\n.ylim\n(\n0\n,\n20\n)\nplt\n.xlim\n(\n0\n,\n20\n)\nplt\n.plot\n(l,s)\nplt\n.show\n()\n\n\n\n\n运行结果及报错内容\n\n\n  \nFile\n \n\"\"\n, \nline\n 16\n    plt.figure()\n    ^\nSyntaxError: invalid \nsyntax\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果\n\n\n这个该怎么解决", "Tag": "算法分析"}
{"Answer": "在代码中，np.floor(5*np.random.random((2, 4))) 表示生成一个 $2\\times4$ 的随机数组并向下取整，其中的 5 是一个系数，用于控制随机数的范围。类似地，np.ceil(6*np.random.random((4, 2))) 表示生成一个 $4\\times2$ 的随机数组并向上取整，其中的 6 是一个系数。\n具体来说，np.random.random() 函数会生成一个形状为给定参数的随机浮点数数组，数值的范围在 [0, 1) 之间。通过将生成的随机数组乘以系数，可以控制随机数的范围和精度。np.floor() 和 np.ceil() 函数可以将数组中的每个元素向下或向上取整，以保证数据的整数性。\n在这个例子中， np.floor(5*np.random.random((2, 4))) 和 np.ceil(6*np.random.random((4, 2))) 主要用于生成一些随机数据，方便演示矩阵合并和分割操作。在实际应用中，这些数字需要根据具体情况进行调整。\n总之，这里的 5 和 6 只是一些系数，用于控制随机数的范围和精度，并无特殊意义。", "Konwledge_Point": "应对NP完全问题", "Question": "numpy库中矩阵的合并与分割\n\nimport numpy as np\n\na\n = np\n.arange\n(\n16\n)\n.reshape\n(\n4\n,\n4\n)  #生成\n4\n行\n4\n列的数组\n\nb\n = np\n.floor\n(\n5\n*np\n.random\n.random\n((\n2\n, \n4\n)))\nc = np\n.ceil\n(\n6\n*np\n.random\n.random\n((\n4\n, \n2\n)))\nd = np\n.vstack\n(\n[a, b]\n)  #上下合并矩阵\ne = np\n.hstack\n(\n[a, c]\n)  #左右合并矩阵\n\n\n\nb = np.floor(5\nnp.random.random((2, 4)))\nc = np.ceil(6\nnp.random.random((4, 2)))中的5\n和6\n是什么意思或作用", "Tag": "算法分析"}
{"Answer": "import numpy as np\ndata=np.random.randn(1000)  #生成数据\nhist1,edges1=np.histogram(data) #是一个生成直方图的函数\nprint(hist1)\nprint(edges1)\nimport matplotlib.pyplot as plt\n\nplt.figure() #创建展示的结构\nplt.bar(edges1[:-1],hist1,width=edges1[1:]-edges1[:-1])\n#格式  plt.bar(x, height, width=0.8, bottom=None,color)\n# x → 为一个标量序列，确定x轴刻度数目\n# height → 确定y轴的刻度\n# width → 单个直方图的宽度\n# bottom → 设置y边界坐标轴起点\n# color → 设置直方图颜色（只给出一个值表示全部使用该颜色，若赋值颜色列表则会逐一染色，若给出颜色列表数目少于直方图数目则会循环利用）\n\nplt.show() #画图\n", "Konwledge_Point": "应对NP完全问题", "Question": "python的直方图问题\n创建具有1000随机数的人工数据集，并画出其条形图\n\n\n书上给出的代码\nimport numpy as np\ndata=np.random.randn(1000)\nimport matplotlib.pyplot as plt\nplt.figure()\nhist1,edges1=np.histogram(datal)\nplt.bar(edges1[:-1],hist1,width=edges1[1:]-edgesq[:-1]\n\n\n但是我不是很理解，也不知道顺序，望解答", "Tag": "算法分析"}
{"Answer": "axis表示折叠第几维度你是三维列表，所以axis=0就是折叠0号维度。reshape(3,4,2)，或者方括号从外向内数，依次为0,1,2号维度。所以折叠0号维度，就是变成4行2列的二维列表，所有数据在0号维度上求和，比如0+8+16=24,1+9+17=27同样地，还可以折叠1号或2号维度，变成3行2列，或3行4列的二维列表，所有数据在被折叠的维度上求和：\n>>> c.sum(axis=0)\narray([[24, 27],\n       [30, 33],\n       [36, 39],\n       [42, 45]])\n>>> \n>>> c.sum(axis=1)\narray([[12, 16],\n       [44, 48],\n       [76, 80]])\n>>> \n>>> c.sum(axis=2)\narray([[ 1,  5,  9, 13],\n       [17, 21, 25, 29],\n       [33, 37, 41, 45]])\n", "Konwledge_Point": "应对NP完全问题", "Question": "numpy库中axis的有关问题\nimport numpy as np\nc= np.arange(24).reshape(3,4,2)\n它的c.sum(axis=0)的结果为：\n是如何计算出来的？", "Tag": "算法分析"}
{"Answer": "c=a 相当于将符号c指向符号a的内容，所以本质上c和a是相同的对于d=b[:]，numpy对于切片拷贝的处理是不对相同数据进行复制，以确保处理大量数据时节省空间\nnumpy中数组元素的切片复制_小灰笔记-CSDN博客\n代码1：#!/usr/bin/pythonimport numpy as nparr1 = np.arange(10)print(arr1)slice_data = arr1[3:5]print(slice_data)slice_data[0] = 123print(slice_data)print(arr1)类似的代\n\n\n\nhttps://blog.csdn.net/grey_csdn/article/details/69053476\n\n由此得出来的结论是：切片后的变量与原始的数据共享同一数据存储。而这种特点是专门针对大数据的处理而定制的。有帮助望采纳", "Konwledge_Point": "应对NP完全问题", "Question": "python numpy的向量赋值的问题\nimport numpy as np\na=np\n.array\n(\n[1,2,3]\n)\nb=np\n.array\n(\n[4,5,7]\n)\n\nc=\na\n\nd=\nb\n[:]\n\n\nc\n[-1]\n=\n100\n\nd\n[-1]\n=\n100\n\n\n\nprint\n(a)\n\n\nprint\n(id(c)\n,id(a))\n\nprint\n(b)\n\n\nprint\n(id(d)\n,id(b))\n\n\n\n\n输出是：\n\n\n\n\n为什么d和c的id已经不一样了，给一个赋值另外一个还是会变化", "Tag": "算法分析"}
{"Answer": "np.where(pd.isnull(a), b, a) 就是把a里面是空的用b对应的值替换，不然还是用a的值", "Konwledge_Point": "应对NP完全问题", "Question": "python where函数问题\nimport pandas as pd\n\nfrom pandas import Series, DataFrame\n\nimport numpy as np\n\n\n\na = pd.Series([np.nan, 2.5, 0.0, 3.5, 4.5, np.nan],\n\n              index = ['f', 'e', 'd', 'c', 'b', 'a'])\n\nb = pd.Series([0, np.nan, 2., np.nan, np.nan, 5.],\n\n              index = ['a', 'b', 'c', 'd', 'e', 'f'])\n\n\n\na\n\nOut[11]:\n\nf    NaN\n\ne    2.5\n\nd    0.0\n\nc    3.5\n\nb    4.5\n\na    NaN\n\ndtype: float64\n\n\n\nb\n\nOut[12]:\n\na    0.0\n\nb    NaN\n\nc    2.0\n\nd    NaN\n\ne    NaN\n\nf    5.0\n\ndtype: float64\n\n\n\nnp.where(pd.isnull(a), b, a)\n\nOut[13]: array([0. , 2.5, 0. , 3.5, 4.5, 5. ])\n\n\n\nnp.where(pd.isnull(b), b, a)\n\nOut[14]: array([nan, nan,  0., nan, nan, nan])\n\n\n\n\n\n\n谁帮忙解释一下np.where(pd.isnull(a), b, a) 和 np.where(pd.isnull(b), b, a)  是什么意思啊？谢谢。", "Tag": "算法分析"}
{"Answer": "t.reshape(3,2)", "Konwledge_Point": "应对NP完全问题", "Question": "python3矩阵问题\nimport numpy as np \nlist=\n[[1,2]\n,\n[3,4]\n,\n[5,6]\n] \nt=np\n.array\n(list)\n\nprint\n(t)\n\nt=np\n.arange\n(\n24\n)\n.reshape\n(\n4\n,\n6\n)\n\nprint\n(t)\n\n\nprint\n(t.ndim)\n\n\nprint\n(t.size)\n\n\nprint\n(t.shape)\n\nt.shape=(\n3\n,\n2\n)\n\nprint\n(t)\n\n\n\n\nt.shape=(3,2)这里报错了，为什么改变矩阵失败了？应该怎么改变？", "Tag": "算法分析"}
{"Answer": "或者 x' = x/ max(fabs(x)) ； \r\n或者 x' = 2 * ( x -  ( max(x) + min(x) ) * 0.5 )  / ( max(x) - min(x))， 但是变号，归一化到[-1, 1]；\r\n或者以均值为0计算sigma, x' = x / sigma, 归一化到 N~【0, sigma】分布", "Konwledge_Point": "应对NP完全问题", "Question": "含负数数据如何标准化\n如何对既有正数又有负数的数据标准化？且标准化后数据的正负号不变。", "Tag": "算法分析"}
{"Answer": "d=d.replace('-',0)", "Konwledge_Point": "应对NP完全问题", "Question": "关于#数据清洗#的问题，如何解决？\n\n\nimport\n numpy \nas\n np\n\nimport\n pandas \nas\n pd\n\n#1利用read_csv导入9healthy_lifestyle_city_2021.csv\n\nd=pd.read_csv(\nr'C:\\Users\\ASUS\\Desktop\\毕设代码\\fangjia2019.csv'\n,sep=\n','\n)\npd.set_option(\n'display.max_columns'\n, \nNone\n)\n#显示所有列\n\nd.info()\n#快速浏览数据,查看所有列数据类型以及每列中非空值的数量\n\n\nprint\n(d)\n#打印数据\n\n\n#2用均值填充缺失值NaN\n\nd.fillna(d.mean(),inplace=\nTrue\n)\n#要不要在源数据填充\n\n\nprint\n(d.isnull().\nany\n())\n#统计data里每一列是否有空值,出现空值就删除\n\n\n#3保存至新数据表 数据清洗后数据.csv\n\nd.to_csv(\n\"fangjia2019_清洗.csv\"\n,sep=\n','\n,encoding=\n'utf_8_sig'\n)\n\n\n\n怎么可以把横杠去除，我只会删除空值", "Tag": "算法分析"}
{"Answer": "参照网上的这个解决方案：http://zhidao.baidu.com/link?url=bLoKrFv9OfTzkqeQVzK5JsMOYXJWxzBN3U_Ca9KUGj1hxK1CMwirBkuG8_MbvyP0qIZRAjntyWeZxsffU_SRq-8iwzrprxBCNBoDHEV4cEy", "Konwledge_Point": "应对NP完全问题", "Question": "flash air NativeProcess 相关问题\n先附上源码\n\nimport flash.desktop.NativeApplication;\n\nimport flash.desktop.NativeProcess;\n\nimport flash.desktop.NativeProcessStartupInfo;\n\nimport flash.filesystem.File;\n\nimport flash.events.MouseEvent;\n\n\n\nvar process:NativeProcess = new NativeProcess;\n\nNativeApplication.nativeApplication.autoExit = true;\n\n\n\nvar file:File = new File;\n\nfile = file.resolvePath(\"C:/Windows/System32/cmd.exe\");\n\n\n\nvar processArg:Vector. = new Vector.();\n\nprocessArg.push(\"/c\");\n\nprocessArg.push(\"start C:/Windows/System32/osk.exe\");\n\n\n\nvar npsi:NativeProcessStartupInfo = new NativeProcessStartupInfo();\n\nnpsi.executable = file;\n\nnpsi.arguments = processArg;\n\n\n\nstop();\n\n\n\nstage.addEventListener(MouseEvent.CLICK, click_handler );\n\n\n\nfunction click_handler( e : MouseEvent ) : void\n\n{\n\n    if( process.running )\n\n    {\n\n        return;\n\n    }\n\n\n\nprocess.start(npsi);\n\n\n\n\n}\n\n\n\n这段源码在flash编译器里调试运行的时候没有任何问题，点击舞台可以弹出系统软键盘，但是很奇怪的是我发不成exe安装包后，点击安装，然后运行程序，点击应用就调不出软键盘了；   Air 3.2，配置文件中 desktop extendedDesktop已添加", "Tag": "算法分析"}
{"Answer": "shape是数组的属性，不能对其赋值——即便可以，对于24个元素的数组，怎么可以变成3行2列呢？想要改变数组的shape，请使用resize函数。resize和reshape的区别在于，前者改变了数组的shape但没有返回值，后者不会改变数组的shape但返回一个新的视图。\nimport numpy as np\nt = np.arange(12)\nt.shape\n(12,)\na = t.reshape(3,4)\na.shape\n(3, 4)\nt.shape\n(12,)\nt.resize(3,4)\nt.shape\n(3, 4)\n", "Konwledge_Point": "应对NP完全问题", "Question": "python3关于矩阵的问题\nimport numpy as np \n\n\nlist=\n[[1,2]\n,\n[3,4]\n,\n[5,6]\n] \nt=np\n.array\n(list)\n\nprint\n(t)\n\nt=np\n.arange\n(\n24\n)\n.reshape\n(\n4\n,\n6\n)\n\nprint\n(t)\n\n\nprint\n(t.ndim)\n\n\nprint\n(t.size)\n\n\nprint\n(t.shape)\n\nt.shape=(\n3\n,\n2\n)\n\nprint\n(t)\n\n\n\n\n第九行改变矩阵的代码为什么显示错误？正确的方法是什么？", "Tag": "算法分析"}
{"Answer": "model.add(LSTM(50, input_shape=(train_x1.shape[1], train_x1.shape[2])))\r\n->\r\nmodel.add(LSTM(50, input_shape=(train_x1.shape[0],, train_x1.shape[1], train_x1.shape[2])))\r\n\r\n或者还有一个办法就是不用input_shape，而是指定下timesteps=1\r\n\r\n具体看下keras的文档。", "Konwledge_Point": "应对NP完全问题", "Question": "LSTM输入数据格式问题\n输入样本train_x1 标签train_y1 \n\n样本与标签都是（20000，10）\n\n然后我reshape变成三维（20000，1，10）报错 求该如何修改格式\n\n输入十个数值，输出十个数值。\n\n\n\ntrain_x1 = np.reshape(train_x1, (train_x1.shape[0],1，train_x1.shape[1]))\n\ntrain_y1 = np.reshape(train_y1, (train_y1.shape[0],1，train_y1.shape[1]))\n\n\n\nmodel = Sequential()\n\n\n\nmodel.add(LSTM(50, input_shape=(train_x1.shape[1], train_x1.shape[2])))\n\n\n\nmodel.add(Dense(10))\n\n\n\nmodel.compile(loss='mse', optimizer='adam')\n\n\n\nmodel.fit(train_x1, train_y1, nb_epoch = 300, batch_size = 10)\n\n\n\nmodel.save_weights('LSTM.model')", "Tag": "算法分析"}
{"Answer": "你在 ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.get_cmap('rainbow')) 前面加上一句\nax=plt.axes(projection='3d')\n\n\n或者是加\nax = fig.gca(projection='3d')\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的问题：我想问一下 为什么我的电脑不会显示出3d图形\n我想问一下 为什么我的电脑不会显示出3d图形，只有空白\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = Axes3D(fig)\n# \nX\n, \nY\n value\n\nX\n = np.arange(\n-4\n, \n4\n, \n0\n.25\n)\n\nY\n = np.arange(\n-4\n, \n4\n, \n0\n.25\n)\n\nX\n, \nY\n = np.meshgrid(\nX\n, \nY\n)\n\nR\n = np.sqrt(\nX\n ** \n2\n + \nY\n ** \n2\n)\n# height value\n\nZ\n = np.sin(\nR\n)\n\nax.plot_surface(\nX\n, \nY\n, \nZ\n, rstride=\n1\n, cstride=\n1\n, cmap=plt.get_cmap(\n'rainbow'\n))\nax.contourf(\nX\n, \nY\n, \nZ\n, zdir=\n'z'\n, offset=\n-2\n, cmap=plt.get_cmap(\n'rainbow'\n))\n#zdir 等高线沿z轴 offset 将等高线图放到沿该轴距离为\n-2\n的面上\nax.set_zlim(\n-2\n, \n2\n)\n\nplt.show()\n\n\n", "Tag": "算法分析"}
{"Answer": "这个并不是错误，指示剂警告。告诉你这个API有变化，以后建议用新的API", "Konwledge_Point": "应对NP完全问题", "Question": "PYTHON使用statsmodels库失败\n#代码\n\nimport numpy\n\nfrom statsmodels.tsa.stattools import adfuller\n\nadfuller(numpy.random.rand(100))\n\n\n\n提示的错误为\n\nWarning (from warnings module):\n\n  File \"C:\\Python27\\lib\\site-packages\\statsmodels\\compat\\pandas.py\", line 56\n\n    from pandas.core import datetools\n\nFutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.", "Tag": "算法分析"}
{"Answer": "使用az的plot_trace方法：import arviz as az...with basic_model:    az.plot_trace(trace)    display(az.summary(trace, round_to=2))参考文档https://docs.pymc.io/en/v3/pymc-examples/examples/getting_started.html参考如下可完整运行代码：\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc3 as pm\nfrom IPython.display import display\nprint(f\"Running on PyMC3 v{pm.__version__}\")\nRANDOM_SEED = 8927\nrng = np.random.default_rng(RANDOM_SEED)\naz.style.use(\"arviz-darkgrid\")\n# True parameter values\nalpha, sigma = 1, 1\nbeta = [1, 2.5]\n\n# Size of dataset\nsize = 100\n\n# Predictor variable\nX1 = np.random.randn(size)\nX2 = np.random.randn(size) * 0.2\n\n# Simulate outcome variable\nY = alpha + beta[0] * X1 + beta[1] * X2 + rng.normal(size=size) * sigma\nbasic_model = pm.Model()\n\nwith basic_model:\n\n    # Priors for unknown model parameters\n    alpha = pm.Normal(\"alpha\", mu=0, sigma=10)\n    beta = pm.Normal(\"beta\", mu=0, sigma=10, shape=2)\n    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n\n    # Expected value of outcome\n    mu = alpha + beta[0] * X1 + beta[1] * X2\n\n    # Likelihood (sampling distribution) of observations\n    Y_obs = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=Y)\n# By default,BFGS,method=\"powell\"\nmap_estimate = pm.find_MAP(model=basic_model, method=\"powell\")\nprint(map_estimate)\nwith basic_model:\n    # draw 500 posterior samples\n    trace = pm.sample(500, return_inferencedata=False)\nwith basic_model:\n    # instantiate sampler\n    step = pm.Slice()\n    trace = pm.sample(5000, step=step, return_inferencedata=False)\nwith basic_model:\n    az.plot_trace(trace)\n    display(az.summary(trace, round_to=2))\nplt.show()\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "如何使用pymc3时，pm.traceplot(trace)出现问题？(语言-python)\n问题遇到的现象和发生背景\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport numpy as np\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy import optimize\nimport sys\n\n\ndef main_1():\n    # 设置随机数种子\n    np.random.seed(123)\n\n\nalpha\n = \n1\n\n\nsigma\n = \n1\n\n\nbeta\n = [\n1\n, \n2.5\n]\n\n\nN\n = \n100\n\n\n\nX1\n = np.random.randn(N)\n\nX2\n = np.random.randn(N)\n\n\nY\n = alpha + beta[\n0\n] * X1 + beta[\n1\n] * X2 + np.random.randn(N) * sigma\n\n\n\n\nbasic_model\n = pm.Model()\n\nwith\n basic_model:\n    \nalpha\n = pm.Normal('alpha', \nmu=0,\n \nsd=10)\n\n    \nbeta\n = pm.Normal('beta', \nmu=0,\n \nsd=10,\n \nshape=2)\n\n    \nsigma\n = pm.HalfNormal('sigma', \nsd=1)\n\n\n    \nmu\n = alpha + beta[\n0\n] * X1 + beta[\n1\n] * X2\n\n    \nY_obs\n = pm.Normal('Y_obs', \nmu=mu,\n \nsd=sigma,\n \nobserved=Y)\n\n\n\nwith\n basic_model:\n    \n# 用MAP获得初始点\n\n    \nstart\n = pm.find_MAP(\nmethod='BFGS')\n\n\n    \n# 实例化采样器\n\n    \nstep\n = pm.Slice(\nvars=[sigma])\n\n\n    \n# 对后验分布进行5000次采样\n\n    \ntrace\n = pm.sample(\n5000\n, \nstep=step,\n \nstart=start)\n\n\npm.traceplot(trace)\n\n\n\nif \nname\n == '\nmain\n':\n    sys.exit(main_1())\n\n\n运行结果及报错内容\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "numpy没有repeats的属性，你是不是想用repeat属性呀？\nimport numpy as np\narr71 = np.array([0.065, 0.148, 0.217, -0.126, 0.089])\narr72 = np.array([0.15, 0.30, 0.22, 0.23, 0.10])\narr73 = np.dot(arr71, arr72)\narr74 = np.repeat(arr73,2)\nprint(arr74)\n", "Konwledge_Point": "应对NP完全问题", "Question": "Python 矩阵的repeat问题\n要用Repeat ()替代Numpy.dot()，我写的代码报错没看懂\n我写的是：\n\n\nimport\n numpy as np\n\narr71\n = np.array([\n0\n.\n065\n, \n0\n.\n148\n, \n0\n.\n217\n, -\n0\n.\n126\n, \n0\n.\n089\n])\n\narr72\n = np.array([\n0\n.\n15\n, \n0\n.\n30\n, \n0\n.\n22\n, \n0\n.\n23\n, \n0\n.\n10\n])\n\narr73\n = np.dot(arr71, arr72)\n\narr74\n = np.repeats(arr73)\n\nprint\n(arr74)\n\n\n\n但是arr74出现报错，显示的是这个：\n\n\n---------------------------------------------------------------------------\n\nAttributeError                            Traceback (most recent \ncall\n last)\n~\\AppData\\\nLocal\n\\\nTemp\n/ipykernel_150896/\n859011211.\npy \nin\n \n      \n1\n # d. \nWrite\n your code here\n\n----> 2 arr74 = np.repeats(arr73)\n\n      \n3\n print(arr74)\n\nE:\\python\\lib\\site-packages\\numpy\\__init__.py \nin\n __getattr__(attr)\n    \n301\n                 \nreturn\n Tester\n    \n302\n \n\n--> 303             raise AttributeError(\"module {!r} has no attribute \"\n\n    \n304\n                                  \"{!r}\".format(__name__, attr))\n    \n305\n \n\nAttributeError: module \n'numpy'\n has \nno\n \nattribute\n \n'repeats'\n\n\n\n\n\n请问怎么解决", "Tag": "算法分析"}
{"Answer": "from PIL import ImageImage的首字母要大写", "Konwledge_Point": "应对NP完全问题", "Question": "真不知道这种问题该怎么改\n今天编代码 ，就很突然的出了问题 (Python)\n代码如下\n\n\nimport\n  numpy \nas\n np\n\nfrom\n \nPIL\n \nimport\n image\n......\n\nim\n = np.dstack((r,g,b))\n\n\n\n这还不是完整的代码\n在这里的底下还有一个 \n\n\nimage\n.\nshow\n()\n\n\n\n不知道怎么办，加了一个image.show()运行出错\n\n\nImporterror : cannot \nimport\n \nname\n \n'image'\n  \nfrom\n \n'PIL'\n\n\n\n\n看代码之后发现\n有了这行代码'image'不属于模块！\n没有这行代码'image'属于模块！\n真的求解决办法", "Tag": "算法分析"}
{"Answer": "“Devil组”引证GPT后的撰写：\n用Matplotlib中的子图（subplot）功能。在这种情况下，可以创建一个具有单个图形的子图网格，然后将所有数据绘制在同一个网格中。可以使用该subplot()函数定义子图网格，以及其他Matplotlib函数来绘制图形。\n下面是示例代码，其中假设数据存储在三个Numpy数组strike，dip和rake中，每个数组包含多个值，希望将它们绘制在同一张图上。\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 创建一个具有单个图形的子图网格\nfig, ax = plt.subplots()\n\n# 调用函数以获取数据\nn_1 = np.zeros([np.size(strike),3])\nu_1 = np.zeros([np.size(strike),3])\n# ... 其他变量的计算\n\n# 绘制数据\nax.scatter(x_values, y_values, label='data1')\nax.scatter(x_values, y_values, label='data2')\nax.scatter(x_values, y_values, label='data3')\n\n# 添加标签和图例\nax.set_xlabel('x label')\nax.set_ylabel('y label')\nax.legend()\n\n# 显示图形\nplt.show()\n\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python如何将所有数据画在一张图上\n这是定义的函数（这个函数是没问题的，一个软件包里的，但是我需要用他画图）\n\n\ndef\n plot_P_T_axes(strike,dip,rake):\n    \n    \nimport\n matplotlib.pyplot as plt\n    \nimport\n numpy as np\n    \n    \nn_1\n = np.zeros([np.size(strike),\n3\n])\n    \nu_1\n = np.zeros([np.size(strike),\n3\n])\n    \n    \nn_1\n[:,\n0\n] = -np.sin(dip*np.pi/\n180\n)*np.sin(strike*np.pi/\n180\n)\n    \nn_1\n[:,\n1\n] =  np.sin(dip*np.pi/\n180\n)*np.cos(strike*np.pi/\n180\n)\n    \nn_1\n[:,\n2\n] = -np.cos(dip*np.pi/\n180\n)\n    \n    \nu_1\n[:,\n0\n] =  np.cos(rake*np.pi/\n180\n)*np.cos(strike*np.pi/\n180\n) + np.cos(dip*np.pi/\n180\n)*np.sin(rake*np.pi/\n180\n)*np.sin(strike*np.pi/\n180\n)\n    \nu_1\n[:,\n1\n] =  np.cos(rake*np.pi/\n180\n)*np.sin(strike*np.pi/\n180\n) - np.cos(dip*np.pi/\n180\n)*np.sin(rake*np.pi/\n180\n)*np.cos(strike*np.pi/\n180\n)\n    \nu_1\n[:,\n2\n] = -np.sin(rake*np.pi/\n180\n)*np.sin(dip*np.pi/\n180\n)\n    \n    \nN\n = np.size(strike)\n    \n#--------------------------------------------------------------------------\n\n    \n# lower hemisphere equal-area projection\n\n    \n#--------------------------------------------------------------------------\n\n    \nprojection\n = -\n1\n  \n\n    \n    \n#--------------------------------------------------------------------------\n\n    \n# P/T axes\n\n    \n#--------------------------------------------------------------------------\n\n    \nP_osa\n = np.zeros([N,\n3\n])\n    \nT_osa\n = np.zeros([N,\n3\n])\n    \n    \nP_azimuth\n = np.zeros(N)\n    \nT_azimuth\n = np.zeros(N)\n    \n    \nP_theta\n = np.zeros(N)\n    \nT_theta\n = np.zeros(N)\n    \n    \nP_x\n = np.zeros(N)\n    \nP_y\n = np.zeros(N)\n    \nT_x\n = np.zeros(N)\n    \nT_y\n = np.zeros(N)\n    \n    \nfor\n i in range(N):\n        \nP_osa\n[i,:] = (n_1[i,:]-u_1[i,:])/np.linalg.norm(n_1[i,:]-u_1[i,:],\n2\n)\n        \nT_osa\n[i,:] = (n_1[i,:]+u_1[i,:])/np.linalg.norm(n_1[i,:]+u_1[i,:],\n2\n)\n        \n        \nif\n (P_osa[i,\n2\n]>\n0\n): P_osa[i,\n0\n]=-P_osa[i,\n0\n]; P_osa[i,\n1\n]=-P_osa[i,\n1\n]; P_osa[i,\n2\n]=-P_osa[i,\n2\n]; \n        \nif\n (T_osa[i,\n2\n]>\n0\n): T_osa[i,\n0\n]=-T_osa[i,\n0\n]; T_osa[i,\n1\n]=-T_osa[i,\n1\n]; T_osa[i,\n2\n]=-T_osa[i,\n2\n]; \n    \n        \nfi\n = np.arctan(np.abs(P_osa[i,\n0\n]/P_osa[i,\n1\n]))*\n180\n/np.pi\n    \n        \nif\n (P_osa[i,\n0\n]>\n0\n and P_osa[i,\n1\n]>\n0\n): P_azimuth[i] = fi;       # \n1\n. kvadrant\n        \nif\n (P_osa[i,\n0\n]>\n0\n and P_osa[i,\n1\n]<\n0\n): P_azimuth[i] = \n180\n-fi;   # \n2\n. kvadrant\n        \nif\n (P_osa[i,\n0\n]<\n0\n and P_osa[i,\n1\n]<\n0\n): P_azimuth[i] = fi+\n180\n;   # \n3\n. kvadrant\n        \nif\n (P_osa[i,\n0\n]<\n0\n and P_osa[i,\n1\n]>\n0\n): P_azimuth[i] = \n360\n-fi;   # \n4\n. kvadrant\n    \n        \nP_theta\n[i] = np.arccos(np.abs(P_osa[i,\n2\n]))*\n180\n/np.pi\n    \n        \nfi\n = np.arctan(np.abs(T_osa[i,\n0\n]/T_osa[i,\n1\n]))*\n180\n/np.pi\n        \n        \nif\n (T_osa[i,\n0\n]>\n0\n and T_osa[i,\n1\n]>\n0\n): T_azimuth[i] = fi;       # \n1\n. kvadrant\n        \nif\n (T_osa[i,\n0\n]>\n0\n and T_osa[i,\n1\n]<\n0\n): T_azimuth[i] = \n180\n-fi;   # \n2\n. kvadrant\n        \nif\n (T_osa[i,\n0\n]<\n0\n and T_osa[i,\n1\n]<\n0\n): T_azimuth[i] = fi+\n180\n;   # \n3\n. kvadrant\n        \nif\n (T_osa[i,\n0\n]<\n0\n and T_osa[i,\n1\n]>\n0\n): T_azimuth[i] = \n360\n-fi;   # \n4\n. kvadrant\n            \n        \nT_theta\n[i] = np.arccos(np.abs(T_osa[i,\n2\n]))*\n180\n/np.pi\n            \n        \nP_x\n[i] = np.sqrt(\n2\n.)*projection*np.sin(P_theta[i]*np.pi/\n360\n)*np.sin(P_azimuth[i]*np.pi/\n180\n)\n        \nP_y\n[i] = np.sqrt(\n2\n.)*projection*np.sin(P_theta[i]*np.pi/\n360\n)*np.cos(P_azimuth[i]*np.pi/\n180\n)\n    \n        \nT_x\n[i] = np.sqrt(\n2\n.)*projection*np.sin(T_theta[i]*np.pi/\n360\n)*np.sin(T_azimuth[i]*np.pi/\n180\n)\n        \nT_y\n[i] = np.sqrt(\n2\n.)*projection*np.sin(T_theta[i]*np.pi/\n360\n)*np.cos(T_azimuth[i]*np.pi/\n180\n)\n    \n    \n    \nplt\n.plot(P_y,P_x,'ro', markeredgecolor='r', markerfacecolor='none', markersize=\n8\n, markeredgewidth=\n1\n.\n5\n)\n    \nplt\n.plot(T_y,T_x,'b+', markersize=\n8\n, markeredgewidth=\n1\n.\n5\n)\n\n\n\n\n但是只使用这个函数画出来是直角坐标：\n\n\n\n\n然后在函数定义中加入下面代码\n\n\n    \n#--------------------------------------------------------------------------\n\n    \n# boundary circle and the centre of the circle\n\n    \n#--------------------------------------------------------------------------\n\n    \nfi\n = np.arange(\n0\n,\n360\n, \n0\n.\n1\n)\n    \nplt\n.plot(np.cos(fi*np.pi/\n180\n.),np.sin(fi*np.pi/\n180\n.),'k-', linewidth = \n2\n.\n0\n)\n    \nplt\n.plot(\n0\n,\n0\n,'k+', markersize = \n10\n);\n\n\n\n\n图片变成了这样\n\n\n再在函数定义中加入代码：\n\n\n    \n#--------------------------------------------------------------------------\n\n    \n# plotting the stress directions in the focal sphere\n\n    \n#--------------------------------------------------------------------------\n\n    \nfig\n, ax = plt.subplots()\n    \nax\n.set_title('Principal stress and P/T axes',fontsize = \n16\n)\n    \n    \nax\n.axis('equal')\n    \nax\n.axis([-\n1\n.\n05\n,  \n1\n.\n70\n, -\n1\n.\n05\n, \n1\n.\n05\n])\n    \nax\n.axis('\noff\n')\n    \n#ax.axis()\n\n\n\n\n\n\n变成了一堆这样的图片，但是这个图片的样子是我要的，只是变成一组数据一张图了，要怎么让所有数据都在一张图上呢\n\n\n调用函数的代码如下：\n\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n plot_stress \nas\n plots\n\nfrom\n openpyxl \nimport\n load_workbook\nwb = load_workbook(\nr'satsi_output.xlsx'\n) \n# 获取已存在的工作簿\n\nws = wb[\n'DC_50'\n]\n# 获取工作表\n\n\nfor\n i \nin\n \nrange\n(\n2\n,\n30\n):\n    strike = ws.cell(row=i, column=\n39\n).value\n    dip = ws.cell(row=i, column=\n40\n).value\n    rake = ws.cell(row=i, column=\n41\n).value\n    plots.plot_P_T_axes(strike, dip, rake)\nplt.show()\n\n", "Tag": "算法分析"}
{"Answer": "因为这是在循环内部创建的变量，作用域仅限于本轮循环，把它放在  i=0.8 的前面，while 循环外面试试。", "Konwledge_Point": "应对NP完全问题", "Question": "python创建创建数组的问题\ni = 0.8\n\nwhile i <2:\n\n    temp= []\n\n    d = np.random.choice([3,i],size = 10, replace=True,p =[0.5,0.5])\n\n    temp.append(d)\n\n    i+= 0.1\n\n\n\n我想要把每次循环创建的数组纵向合并，为什么这个temp只有一个数组出来， 求解答！", "Tag": "算法分析"}
{"Answer": "![图片说明](https://img-ask.csdn.net/upload/201905/09/1557370643_321074.png)", "Konwledge_Point": "应对NP完全问题", "Question": "小白问题--在python中for循环结束位置在哪里呢？\n不确定python中for循环每次的结束位置。和C语言不一样，python的for循环没有括号（）之类的约束，怎么看他的边界在哪里吖？是看他代码开头的空格是否与for对齐吗？\n\n\n\nsize = 100\ntheta0Vals = np.linspace(-10, 10,  size)  \n# 前两个参数分别是数列的开头与结尾。第三个参数，表示数列的元素个数\ntheta1Vals = np.linspace(-1, 4, size)   \nJVals = np.zeros((size, size))\nfor i in range(size):\n    for j in range(size):\n        col = np.array([ [theta0Vals[i]], [theta1Vals[j]] ]).reshape(-1,1) \n                #不知道z的shape属性，想让z变成只有一列，行数管，\n                #通过`z.reshape(-1,1)，Numpy自动计算出有16行\n        JVals[i,j] = compute_cost(X, y, col)\n\ntheta0Vals, theta1Vals = np.meshgrid(theta0Vals,  theta1Vals)  \n# 产生一个以向量x为行，向量y为列的矩阵，\n#X、Y必定是行数、列数相等的，且X、Y的行数都等\n# 于输入参数y中元素的总个数，X、Y的列数都等于输入参数x中元素总个数；形成网格\nJVals = JVals.T\nprint (JVals.shape, JVals[0, 0], JVals[1, 1] ) # test correct\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.plot_surface(theta0Vals, theta1Vals, JVals)     # 绘制一个三维曲面\nax.set_xlabel(r'$\\theta_0$')\nax.set_ylabel(r'$\\theta_1$')\nax.set_zlabel(r'$J(\\theta)$')\nplt.show()\n\n", "Tag": "算法分析"}
{"Answer": "可以将aggfunc参数写成函数的列表形式， aggfunc=['count',np.mean,np.sum],参考https://www.cnblogs.com/Yanjy-OnlyOne/p/11195621.html", "Konwledge_Point": "应对NP完全问题", "Question": "pandas pivot_table aggfunc问题\n请教大佬们一个问题pandas pivot_table aggfunc(df,values=［value］index=［u\"test\"］,columns=［item］,aggfunc=\"count\")\n我想计算value的平均数，可是代入np.mean始终不可以，但是count记数的始终可以很奇怪", "Tag": "算法分析"}
{"Answer": "参考GPT和自己的思路，您可以尝试使用torch.where()函数，而不是用np.where()函数，这可以使您在GPU上并行化处理，从而提高您的计算速度。具体来说，您可以使用torch.where()函数来创建一个bool类型的掩码张量，该张量的形状与输入张量相同，但是只有掩码中的值为True时，才会计算损失。这样就不必将掩码扁平化并使用for循环逐个读取坐标，而是可以使用张量的广播机制，使每个掩码元素与相应的预测值和目标值相匹配，从而并行计算损失。\n下面是使用torch.where()函数修改您的smooth_l1函数的示例：\ndef smooth_l1(y_pred, y, mask, beta=1):\n    loss_item = torch.abs(y_pred - y)\n    loss_item = torch.where(loss_item < beta, 0.5 * torch.pow(loss_item, 2) / beta, loss_item - 0.5 * beta)\n    loss = torch.sum(loss_item * mask) / torch.sum(mask)\n    return loss\n\n在这个版本的smooth_l1函数中，我们首先计算了预测值与目标值之间的差异，然后使用torch.where()函数来计算smooth L1损失。最后，我们将掩码应用于损失，并在返回之前求出平均值。", "Konwledge_Point": "应对NP完全问题", "Question": "用自写的loss计算很慢的问题\n背景：一个图像预测任务，图像并不是指常见的jpg等格式的图片，其实就是网格内每个像素点有数值的那种类似于热点图的图像，如下图\n\n\n\n\n问题：如上图所示，一张图里其实有很大一片区域是0值。但搭的网络输入输出是整图，维度为（32，224，224）。最近意识到计算loss不可以将整图放入计算，只想要关注body内有值的区域，所以就用上自写的loss function，出现了\n计算极其缓慢\n的现象。\n——\ntrain预测过程：x为输入，y为GT，y_pred为预测结果，b是与y维度相同的、只有0或1的mask，b==1就是body内区域。我的做法是把b展平成一维，再用np.where读取b==1的坐标到BODY，同时y，y_pred也展平成一维。放入自写的smooth_L1计算loss\n\n\n        x = np\n.array\n(inputList)\n        y = np\n.array\n(labelList)\n        \nb\n = np\n.array\n(bodyList)\n        \nb\n = \nb\n.flatten\n()\n        BODY = np\n.where\n(\nb\n == \n1\n)\n        BODY = np\n.array\n(BODY)\n        BODY = BODY\n.flatten\n()\n        x = np\n.float32\n(x)\n        y = np\n.float32\n(y)\n        x = torch\n.tensor\n(x)\n        y = torch\n.tensor\n(y)\n        BODY = torch\n.tensor\n(BODY)\n        x = x\n.to\n(device)\n        y = y\n.to\n(device)\n        BODY = BODY\n.to\n(device)\n        optimizer\n.zero_grad\n()\n        y_pred = model(x)\n        y_pred = y_pred\n.view\n(-\n1\n)\n        y = y\n.view\n(-\n1\n)\n        train_loss = smooth_l1(y_pred, y, BODY)\n        print(f\n\"第 {t + 1}/{epoch} epoch, 第 {i + 1}/{batch_train} batch, train_loss: {train_loss.item()}\"\n)\n        train_loss\n.backward\n()\n        optimizer\n.step\n()\n\n\n\n自写的loss：就是一个smoothL1 Loss，但是我只想计算body内区域。所以用了一个for循环读取mask里（也就是BODY里）存好的坐标。\n\n\n# 定义损失函数\n\ndef \nsmooth_item\n(x, \nbeta\n):\n    \nif\n x < \nbeta\n:\n        loss_item = (\n0.5\n*torch.square(x))/\nbeta\n\n    \nelse\n:\n        loss_item = x\n-0.5\n*\nbeta\n\n    \nreturn\n loss_item\ndef \nsmooth_l1\n(y_pred, y, mask, \nbeta\n=\n1\n):\n    n = \nlen\n(mask)\n    loss = \n0\n\n    \nfor\n i in mask:\n        loss += smooth_item(torch.\nabs\n(y_pred[i]-y[i]), \nbeta\n)\n    return loss/n\n\n\n\n\ndebug发现这样子计算loss好慢好慢，有什么方法优化吗？要说用GPU加速，但这样子一个个坐标读取再累加也不是一个并行过程呀", "Tag": "算法分析"}
{"Answer": "贴个代码更能准确定位问题！\n没有传入self。类对象调用类内部函数时，会自动传入self参数，而代码中没有给self留位置，所以就会报错了。", "Konwledge_Point": "应对NP完全问题", "Question": "使用预训练权重，运行predict.py出现问题怎么解决啊\n发生异常: TypeError\narray\n() takes 1 positional argument but 2 were given\n  File \"G:\\maskrcnn\\faster-rcnn-pytorch-master\\frcnn.py\", line 118, in detect_image\n    photo = np.transpose(np.array(image,dtype = np.float32)/255, (2, 0, 1))\n  File \"G:\\maskrcnn\\faster-rcnn-pytorch-master\\predict.py\", line 53, in \n    r_image = frcnn.detect_image(image)", "Tag": "算法分析"}
{"Answer": "这段代码是用来找到a中最小的无重复元素。\n>>> import numpy as np\n>>> a=np.array([1,1,5,2,3,3,4,4])\n>>> b=np.unique(a,return_counts=True)\n>>> b\n(array([1, 2, 3, 4, 5]), array([2, 1, 2, 2, 1], dtype=int64))\n>>> type(b[0]),type(b[1])\n(<class 'numpy.ndarray'>, <class 'numpy.ndarray'>)\n>>> list(b[1]).index(1)\n1\n>>> b[0][list(b[1]).index(1)]\n2\n>>>\n\n首先，np.unique用来去掉a中所有的重复元素，并从小到大排序，也就是b[0]。同时return_counts参数设为True，则会统计b[0]中每个元素重复了几次。然后，list(b[1]).index(1)把b[1]转换成list列表，用list的index方法找出b[1]中第一个值为1的元素的下标1，这里值1表示没有重复，下标1对应b[0][1]，也就是2。", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的问题，如何解决？(语言-python)\nimport\n numpy as np\n\na\n=np.array([\n1\n,\n1\n,\n2\n,\n3\n,\n3\n,\n4\n,\n4\n])\n\nb\n=np.unique(a,return_counts=True)\n\nprint\n(b[\n0\n][list(b[\n1\n]).index(\n1\n)])\n\n\n\n想问一下代码中print()里面的是怎么计算的。\nb[0][list(b[1]).index(1)]结果为什么是2，运算步骤是啥。", "Tag": "算法分析"}
{"Answer": "没有查到对应的files方法，不知你要的是不是tofile()方法\n", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的问题：data.files\n#建立模型\n\n\nimport\n numpy \nas\n np\n\nimport\n tensorflow \nas\n tf\n\n#读取数据\n\ndata = np.load(\n'mnist.npz'\n)\ndata.files\ntrain_images, train_labels, test_images, test_labels = data[\n'x_train'\n], data[\n'y_train'\n], data[\n'x_test'\n],data[\n'y_test'\n]\n\nprint\n(train_images.shape)\n\nprint\n(train_labels.shape)\n\nprint\n(test_images.shape)\n\nprint\n(test_labels.shape)\n\n#交叉熵\n\ntarget_y = np.array([\n1\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n])\npredicted_y1 = np.array([\n0.4\n, \n0.5\n, \n0.1\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n])\npredicted_y2 = np.array([\n0.1\n, \n0.2\n, \n0.7\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n])\n-np.\nsum\n( target_y * np.log(predicted_y1+\n0.0000001\n))\n-np.\nsum\n( target_y * np.log(predicted_y2+\n0.0000001\n))\n\n#搭建网络结构\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Flatten(input_shape=(\n28\n,\n28\n)))\nmodel.add(tf.keras.layers.Dense(\n128\n, activation = \n'relu'\n))\nmodel.add(tf.keras.layers.Dense(\n10\n, activation = \n'softmax'\n))\n\n#编译模型\n\nmodel.\ncompile\n(optimizer = \n'adam'\n, loss = \n'sparse_categorical_crossentropy'\n, metrics = [\n'accuracy'\n])\n\n#训练\n\nmodel.fit(train_images, train_labels, verbose = \n1\n, epochs = \n20\n, validation_data = (test_images,test_labels))\n\n#模型保存\n\nmodel.save(\n'model_mnist.h5'\n)\n\n#用模型进行预测\n\n\nimport\n tensorflow \nas\n tf\n\nimport\n matplotlib.pyplot \nas\n plt\nmodel = tf.keras.models.load_model(\n'model_mnist.h5'\n)\nmodel.summary()\n\nfor\n i \nin\n \nrange\n(\n30\n):\n    image = plt.imread(\n'testimages/'\n + \nstr\n(i)+ \n'.jpg'\n)\n    image_new = image.reshape([\n1\n, \n28\n, \n28\n])\n    result = model.predict(image_new)[\n0\n].argmax()\n    \nprint\n(\n'The'\n, i + \n1\n, \n'th picture shows:'\n, result)\n\n\n\n\n\n\n\n这是为什么啊？怎么改啊？谢谢！", "Tag": "算法分析"}
{"Answer": "a = np.array，这不是都写对了吗b = np.arary，这么明显的拼写错误，错误信息都给你打印出来了，你是对字母有多不敏感", "Konwledge_Point": "应对NP完全问题", "Question": "Python代码运行错误\na = np.array([[80, 86],\n[82, 80],\n[85, 78],\n[90, 90],\n[86, 82],\n[82, 90],\n[78, 80],\n[92, 94]])\nb = np.arary([0.7],[0.3])\nnp.matmul(a, b)\nnp.dot(a,b)\n这段代码跑出来报错如下，请问是哪里出了问题：\n\n\nAttributeError                            Traceback (most recent call last)\n in ()\n      7 [78, 80],\n      8 [92, 94]])\n 9 b = np.arary([0.7],[0.3])\n     10 np.matmul(a, b)\n     11 np.dot(a,b)\n\n\nAttributeError: module 'numpy' has no attribute 'arary'", "Tag": "算法分析"}
{"Answer": "说实话没理解 ", "Konwledge_Point": "应对NP完全问题", "Question": "python for 循环columns问题\n我的数据是一个excel ，下表附我筛选目标字段数据，下图是我数据中要筛选的字段， 图为筛选条件\n\n\n\n\n\n\n问题相关代码，请勿粘贴截图 \n\ncabg\n=ss_name[\n'ASD'\n].tolist()\n\nprint\n(cabg)\n\nwhile\n np.nan \nin\n cabg:\n    cabg.\nremove\n(np.nan)\ngm = data[data[\n'SSJCZMC1'\n].str.replace(\n\".\"\n,\n\"\"\n).str.contains(\n'|'\n.join(cabg))]\ngm.\ncolumns\n=name.columns\ngm.to_excel(\n\"ASD.xlsx\"\n, \nindex\n=\nFalse\n)\n\n\n\n我的解答思路和尝试过的方法\n\n\nfor\n i \nin\n data.columns:\n    \nss\n=dbz_ss[i].tolist()\n    \nwhile\n np.nan \nin\n ss:\n        ss.\nremove\n(np.nan)\n    gm = data[data[\n'SSJCZMC1'\n].str.replace(\n\".\"\n,\n\"\"\n).str.contains(\n'|'\n.join(ss))]\n    gm.\ncolumns\n=name.columns\n    gm.to_excel(ss.xlsx, \nindex\n=\nFalse\n)\n\n\n\n\n运行结果及报错内容\n\n\n\n\n我想要达到的结果\n\n\n按照筛选条件得到各个名字的excel 表\n\n\n| SSJCZMC1 |\n| ------ | ------ |\n\n\nSSJCZMC1\n颈内静脉穿刺中心静脉置管术\n腹腔镜左半结肠切除术\n颈内静脉穿刺中心静脉置管术\n腹腔镜左半结肠切除术\n腹腔镜下胃病损切除术\n股静脉穿刺置管术\n经导管颅内动脉瘤栓塞术\n后入路玻璃体切割术\n纤维支气管镜检查伴肺泡灌洗术\n上颌骨病损切除术\n胃-十二指肠镜检查\n胸腔镜下肺叶切除术\n主动脉瓣机械瓣膜置换术\n全髋关节置换", "Tag": "算法分析"}
{"Answer": "已解决，标签图没有读进去，数据集的路径出错，一定要检查好图片路径！", "Konwledge_Point": "应对NP完全问题", "Question": "TypeError: '>=' not supported between instances of 'NoneType' and 'float'\n问题遇到的现象和发生背景\n\n\n在服务器用python3.7跑代码时报错\n\n\n问题相关代码，请勿粘贴截图\n\n\n\n\ndef\n \nown_loader\n(\nid\n, root\n):  \n# 这里只导入了原始数据的，用的就是这个\n\n    img = cv2.imread(os.path.join(root, \n'{}_sat.png'\n).\nformat\n(\nid\n))\n    mask = cv2.imread(os.path.join(root + \n'{}_mask.png'\n).\nformat\n(\nid\n), cv2.IMREAD_GRAYSCALE)\n    mask = np.expand_dims(mask, axis=\n2\n)\n    \n# img = np.array(img, np.float32).transpose(2,0,1)/255.0 * 3.2 - 1.6\n\n    \n# mask = np.array(mask, np.float32).transpose(2,0,1)/255.0\n\n    mask[mask >= \n0.5\n] = \n1\n\n    mask[mask <= \n0.5\n] = \n0\n\n    img = np.array(img, np.float32).transpose(\n2\n, \n0\n, \n1\n)\n    mask = np.array(mask, np.float32).transpose(\n2\n, \n0\n, \n1\n)\n    \nreturn\n img, mask\n\n\nclass\n \nImageFolder\n(data.Dataset):\n\n    \ndef\n \n__init__\n(\nself, trainlist, root\n):\n        self.ids = trainlist\n        \n# self.loader = default_loader  #原始的\n\n        self.loader = own_loader  \n# 用了自己的数据导入\n\n        self.root = root\n        \n# self.trans = transforms.Compose([transforms.ToTensor()])\n\n        \n# self.trans = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))])\n\n\n    \ndef\n \n__getitem__\n(\nself, index\n):\n        \nid\n = \nlist\n(self.ids)[index]\n        img, mask = self.loader(\nid\n, self.root)\n        \n# img = np.transpose(img, (1,2,0))\n\n        \n# img = self.trans(img)\n\n        img = torch.Tensor(img)\n        mask = torch.Tensor(mask)\n        \nreturn\n img, mask\n\n    \ndef\n \n__len__\n(\nself\n):\n        \nreturn\n \nlen\n(\nlist\n(self.ids))\n\n\n\n\n运行结果及报错内容\n\n\nline 132, in own_loader\n    mask[mask >= 0.5] = 1\nTypeError: '>=' not supported between instances of 'NoneType' and 'float'\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "浮点数的一个普遍问题是它们并不能精确的表示十进制数。 并且，即使是最简单的数学运算也会产生小的误差，比如：\n\n>>> a = 4.2\n\n>>> b = 2.1\n\n>>> a + b\n\n6.300000000000001\n\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python中numpy有关问题，在for中用arange函数得到意外结果\npython初学者，写的一段代码中用到了for i in np.arange(2.0, 2.4, 0.1)，发现i的值与预想的有偏差，试验后发现如下现象，求问为何会这样，如何解决？\n代码如下：\n\n\na\n = np\n.arange\n(\n2.0\n, \n2.4\n, \n0.1\n)\n\nb\n = np\n.array\n(\n[2.0, 2.1, 2.2, 2.3]\n)\n\nprint\n(a)\n\n\nprint\n(b)\n\n\nprint\n(a.tolist()\n)\n\nprint\n(b.tolist()\n)\n\n\n\n结果如下：", "Tag": "算法分析"}
{"Answer": "只有aa.data[0]列表本是是内存地址不会变，变得是内部元素的引用，修改其中某一项元素的值，或者添加几个元素，不会改变其本身的地址，只会改变其内部元素的地址引用，但是如果对其进行重新赋值操作时，就会给列表重新赋予一个地址，来覆盖之前的地址这时列表地址会发生改变。其余的变量只是保存了一个引用，并不是指值本身，变量的每一次初始化，都开辟了一个新的空间，将新内容的地址赋值给变量", "Konwledge_Point": "应对NP完全问题", "Question": "关于ndarray的问题\nimport numpy as np\naa=np\n.array\n(\n[1,2]\n)\naa\n.data\n\nOut\n[278]\n: \n\nid\n(aa[\n0\n])\n\nOut\n[279]\n: \n1919105783888\n\n\nid\n(aa[\n0\n])\n\nOut\n[280]\n: \n1919105781968\n\n\nid\n(aa.data[\n0\n])\n\nOut\n[281]\n: \n140709382463232\n\n\nid\n(aa.data[\n0\n])\n\nOut\n[282]\n: \n140709382463232\n\naa.__array_interface__\n[\n'data'\n]\n[0]\n\nOut\n[283]\n: \n1919049863168\n\n\n\n\n我有3个问题：\n\n\naa.data的值 0x000001BED39961C0是数据所在内存地址吗？\n为什么两次输出id(aa[0])的结果不一样，而同样两次输出id(aa.data[0])结果却是相同的？\nid(aa.data[0])和aa.\narray_interface\n['data'][0]的值为什么不一样？", "Tag": "算法分析"}
{"Answer": "你确定传进去的theta和X都是DataFrame类型吗？这个错误说明两个数据里面至少有一个不是DataFrame类型（可能是ndarray），而且形状不同，不能转换。", "Konwledge_Point": "应对NP完全问题", "Question": "python矩阵计算问题\n#在计算损失函数时，总是报错\n#已有：X 为（97, 2)的dataframe\n#y为（97，1）的dataframe\n#theta为（1，2）的dataframe\n\n\n#代码如下：\n\n\ndef\n \nCompute_cost\n(\ntheta, X, y\n):\n    total_cost = (X*theta.T-y)**\n2\n\n    m = \nlen\n(y)\n    J = total_cost/(\n2\n*m)\n    \nreturn\n J\n    \n\n\n\n\nprint(\nCompute_cost(\ntheta\n,X,\ny\n)\n)\n\n\n\n\n#运行结果及报错内容\nUnable to coerce to DataFrame, shape must be (97, 2): given (2, 1)\n\n\n#猜测可能是dataframe的type问题，用np.mat()转换theta 后问题依然存在", "Tag": "算法分析"}
{"Answer": "pip install --upgrade matplotlib升级或者降级matplotlib", "Konwledge_Point": "应对NP完全问题", "Question": "jupyter notebook中的matplotlib导入问题\n问题遇到的现象和发生背景\n\n\n在pycharm中可以正常导入matplotlib库，但是在jupyter中总显示导入错误，想问一下这个到底是咋回事，怎么解决\n\n\n用代码块功能插入代码，请勿粘贴截图\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n\n运行结果及报错内容\n\n\nModuleNotFoundError: No module named 'matplotlib.artist'", "Tag": "算法分析"}
{"Answer": "没问题啊：\nimport numpy as np\n\na = []\nfor i in range(281):\n    a.append(np.array([[[0]*50]*50]*50))\n\na = np.array(a)\n\nprint(a.shape)\n\nprint(a[0].shape)\n\n输出：(281, 50, 50, 50)(50, 50, 50)", "Konwledge_Point": "应对NP完全问题", "Question": "使用numpy.array把列表转换成数组的问题，求解答\n#问题代码如下\na = []\nfor i in range(281):\n        a.append(一个(50 * 50 * 50)的numpy数组)\na = np.array(a)\n#经过这个代码之后，a应该是一个(281 * 50 * 50 * 50)的numpy数组，但实际是一个(281, )的数组\n#当我把循环改成了28之后，得到的a是一个(28 * 50 * 50 * 50)的数组，就没有问题了\n#之前一直这么用的，不知道为什么281就出了问题，我实在不能理解，求佬们解答", "Tag": "算法分析"}
{"Answer": "这个不就是正确的吗?你可以看看这篇博客:Python numpy.take用法及代码示例 - 纯净天空", "Konwledge_Point": "应对NP完全问题", "Question": "关于#numpy#的问题，如何解决？(语言-python)\nimport numpy as np\nx = np.array([[11, 12, 13, 14, 15],\n              [16, 17, 18, 19, 20],\n              [21, 22, 23, 24, 25],\n              [26, 27, 28, 29, 30],\n              [31, 32, 33, 34, 35]])\nr = [0, 1, 2]\nc = [2, 3, 4]\ny = np.take(x, [r, c])\nprint(y)\n\n\n[[11 12 13]\n\n\n[13 14 15]]\n\n\n结果为什么是这个。", "Tag": "算法分析"}
{"Answer": "你好，这里的tight_layout参数不需要指定对象，只需要设置pad大小就可以。通过查看matplotlib文档，https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.tight_layout.html找到对应函数：matplotlib.pyplot.tight_layout(*, pad=1.08, h_pad=None, w_pad=None, rect=None)根据函数定义，只需要对应关键字参数即可。因此，你这里可以直接写：plt.tight_layout(rect=(0, 0, 1, 0.95))也可以plt.tight_layout(pad=2, w_pad=0.5, h_pad=1.0)此外，还可能会碰到无法显示中文的问题，下载simhei字体，按照如下博客操作即可解决。https://blog.csdn.net/u012744245/article/details/119735461最后结果：\n", "Konwledge_Point": "应对NP完全问题", "Question": "Python问题：Tight _ layout ( )接受0个位置参数，但给出了1个位置参数(和1个仅关键字的参数)\n程序运行之后出现这样的错误：\nplt.tight_layout()使用\n该怎么解决！\n\n\n\nTraceback (most recent \ncall\n \nlast\n):\n  File \n\"D:\\Program Files\\Python310\\lib\\code.py\"\n, \nline\n \n90\n, in runcode\n    exec(code, self.locals)\n  File \n\"\"\n, \nline\n \n1\n, in \n\n\n  File \n\"D:\\PyCharm2022\\PyCharm 2022.1.3\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\"\n, \nline\n \n198\n, in runfile\n    pydev_imports.execfile(filename, global_vars, local_vars)  # \nexecute\n the script\n  File \n\"D:\\PyCharm2022\\PyCharm 2022.1.3\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\"\n, \nline\n \n18\n, in execfile\n    exec(compile(contents+\n\"\\n\"\n, \nfile\n, \n'exec'\n), \nglob\n, \nloc\n)\n  File \n\"E:\\PyCharm\\Python project\\demo\\5.3..py\"\n, \nline\n \n110\n, in \n\n\n    plt.tight_layout(\n1\n, rect=(\n0\n, \n0\n, \n1\n, \n0.95\n))\nTypeError: tight_layout() takes \n0\n positional arguments but \n1\n positional \nargument\n (\nand\n \n1\n keyword-\nonly\n \nargument\n) were given\n\n\n\n代码如下：\n\n\n\nimport numpy as np\n\nfrom\n sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n\nfrom\n sklearn.preprocessing import PolynomialFeatures\n\nfrom\n sklearn.pipeline import Pipeline\n\nfrom\n sklearn.exceptions import ConvergenceWarning\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport warnings\n\n# import seaborn\n\n\n\ndef xss(y, y_hat):\n    y = y.ravel()\n    y_hat = y_hat.ravel()\n    # Version 1\n    tss = ((y - np.average(y)) ** 2).sum()\n    rss = ((y_hat - y) ** 2).sum()\n    ess = ((y_hat - np.average(y)) ** 2).sum()\n    r2 = 1 - rss / tss\n    # \nprint\n \n'RSS:'\n, rss, \n'\\t ESS:'\n, ess\n    # \nprint\n \n'TSS:'\n, tss, \n'RSS + ESS = '\n, rss + ess\n    tss_list.append(tss)\n    rss_list.append(rss)\n    ess_list.append(ess)\n    ess_rss_list.append(rss + ess)\n    # Version 2\n    # tss = np.var(y)\n    # rss = np.average((y_hat - y) ** 2)\n    # r2 = 1 - rss / tss\n    corr_coef = np.corrcoef(y, y_hat)[0, 1]\n    return r2, corr_coef\n\n\n\nif\n __name__ == \n\"__main__\"\n:\n    warnings.filterwarnings(\naction\n=\n'ignore'\n, \ncategory\n=ConvergenceWarning)\n    np.random.seed(0)\n    np.set_printoptions(\nlinewidth\n=300, \nsuppress\n=\nTrue\n)\n    N = 9\n    x = np.linspace(0, 6, N) + np.random.randn(N)\n    x = np.sort(x)\n    y = x*\n*2\n - 4*x - 3 + np.random.randn(N)\n    x.shape = -1, 1\n    y.shape = -1, 1\n\n    models = [Pipeline([\n        (\n'poly'\n, PolynomialFeatures()),\n        (\n'linear'\n, LinearRegression(\nfit_intercept\n=\nFalse\n))]),\n        Pipeline([\n            (\n'poly'\n, PolynomialFeatures()),\n            (\n'linear'\n, RidgeCV(\nalphas\n=np.logspace(-3, 2, 10), \nfit_intercept\n=\nFalse\n))]),\n        Pipeline([\n            (\n'poly'\n, PolynomialFeatures()),\n            (\n'linear'\n, LassoCV(\nalphas\n=np.logspace(-3, 2, 10), \nfit_intercept\n=\nFalse\n))]),\n        Pipeline([\n            (\n'poly'\n, PolynomialFeatures()),\n            (\n'linear'\n, ElasticNetCV(\nalphas\n=np.logspace(-3, 2, 10), l1_ratio=[.1, .5, .7, .9, .95, .99, 1],\n                                    \nfit_intercept\n=\nFalse\n))])\n    ]\n    mpl.rcParams[\n'font.sans-serif'\n] = [\n'simHei'\n]\n    mpl.rcParams[\n'axes.unicode_minus'\n] = \nFalse\n\n\n    plt.figure(figsize=(15, 10), \nfacecolor\n=\n'w'\n)\n    d_pool = np.arange(1, N, 1)  # 阶\n    m = d_pool.size\n    clrs = []  # 颜色\n    \nfor\n c \nin\n np.linspace(16711680, 255, m, \ndtype\n=int):\n        clrs.append(\n'#%06x'\n % c)\n    line_width = np.linspace(5, 2, m) * 0.7\n    titles = \n'线性回归'\n, \n'Ridge回归'\n, \n'LASSO'\n, \n'ElasticNet'\n\n    tss_list = []\n    rss_list = []\n    ess_list = []\n    ess_rss_list = []\n    \nfor\n t \nin\n range(4):\n        model = models[t]\n        plt.subplot(2, 2, t+1)\n        plt.plot(x, y, \n'ro'\n, \nmarkersize\n=7, \nzorder\n=N, \nmec\n=\n'k'\n)\n        \nfor\n i, d \nin\n enumerate(d_pool):\n            model.set_params(\npoly__degree\n=d)\n            model.fit(x, y.ravel())\n            lin = model.get_params(\n'linear'\n)[\n'linear'\n]\n            output = \n'%s：%d阶，系数为：'\n % (titles[t], d)\n            \nif\n hasattr(lin, \n'alpha_'\n):\n                idx = output.\nfind\n(\n'系数'\n)\n                output = output[:idx] + (\n'alpha=%.6f，'\n % lin.alpha_) + output[idx:]\n            \nif\n hasattr(lin, \n'l1_ratio_'\n):   # 根据交叉验证结果，从输入l1_ratio(list)中选择的最优l1_ratio_(float)\n                idx = output.\nfind\n(\n'系数'\n)\n                output = output[:idx] + (\n'l1_ratio=%.6f，'\n % lin.l1_ratio_) + output[idx:]\n            \nprint\n(output, lin.coef_.ravel())\n            x_hat = np.linspace(x.min(), x.max(), \nnum\n=100)\n            x_hat.shape = -1, 1\n            y_hat = model.predict(x_hat)\n            s = model.score(x, y)\n            r2, corr_coef = xss(y, model.predict(x))\n            # \nprint\n \n'R2和相关系数：'\n, r2, corr_coef\n            # \nprint\n \n'R2：'\n, s, \n'\\n'\n\n            z = N - 1 \nif\n (d == 2) \nelse\n 0\n            label = \n'%d阶，$R^2$=%.3f'\n % (d, s)\n            \nif\n hasattr(lin, \n'l1_ratio_'\n):\n                label += \n'，L1 ratio=%.2f'\n % lin.l1_ratio_\n            plt.plot(x_hat, y_hat, \ncolor\n=clrs[i], \nlw\n=line_width[i], \nalpha\n=0.75, \nlabel\n=label, \nzorder\n=z)\n        plt.legend(\nloc\n=\n'upper left'\n)\n        plt.grid(\nTrue\n)\n        plt.title(titles[t], \nfontsize\n=18)\n        plt.xlabel(\n'X'\n, \nfontsize\n=16)\n        plt.ylabel(\n'Y'\n, \nfontsize\n=16)\n    plt.tight_layout(1, rect=(0, 0, 1, 0.95))\n    plt.suptitle(\n'多项式曲线拟合比较'\n, \nfontsize\n=22)\n    plt.show()\n\n    y_max = max(max(tss_list), max(ess_rss_list)) * 1.05\n    plt.figure(figsize=(9, 7), \nfacecolor\n=\n'w'\n)\n    t = np.arange(len(tss_list))\n    plt.plot(t, tss_list, \n'ro-'\n, \nlw\n=2, \nlabel\n=\n'TSS(Total Sum of Squares)'\n, \nmec\n=\n'k'\n)\n    plt.plot(t, ess_list, \n'mo-'\n, \nlw\n=1, \nlabel\n=\n'ESS(Explained Sum of Squares)'\n, \nmec\n=\n'k'\n)\n    plt.plot(t, rss_list, \n'bo-'\n, \nlw\n=1, \nlabel\n=\n'RSS(Residual Sum of Squares)'\n, \nmec\n=\n'k'\n)\n    plt.plot(t, ess_rss_list, \n'go-'\n, \nlw\n=2, \nlabel\n=\n'ESS+RSS'\n, \nmec\n=\n'k'\n)\n    plt.ylim((0, y_max))\n    plt.legend(\nloc\n=\n'center right'\n)\n    plt.xlabel(\n'实验：线性回归/Ridge/LASSO/Elastic Net'\n, \nfontsize\n=15)\n    plt.ylabel(\n'XSS值'\n, \nfontsize\n=15)\n    plt.title(\n'总平方和TSS=？'\n, \nfontsize\n=18)\n    plt.grid(\nTrue\n)\n    plt.show()\n\n", "Tag": "算法分析"}
{"Answer": "你看下这篇博客吧, 应该有用👉 ：Pandas中Apply函数加速百倍的技巧", "Konwledge_Point": "应对NP完全问题", "Question": "pandas中apply函数的参数问题\n\n\nimport\n pandas \nas\n pd\n\nimport\n numpy \nas\n np\n\nmatrix = [\n    [\n1\n,\n2\n,\n3\n],\n    [\n4\n,\n5\n,\n6\n],\n    [\n7\n,\n8\n,\n9\n]\n]\n\ndf = pd.DataFrame(matrix, \ncolumns\n=list(\n'xyz'\n), \nindex\n=list(\n'abc'\n))\ndf.apply(np.square)\n\ndf.apply(lambda x : np.square(x) \nif\n x.name == \n'a'\n \nelse\n x, axis=\n1\n)\n\n\n\n就是关于最后一行代码的，对于Name属性我了解到的是会返回元素的列标签，但是运行上面会把第一行全部平方，其它不变，加上这个axis=1就是表示获取name属性后，不再返回列标签而是行标签名？？", "Tag": "算法分析"}
{"Answer": "要建一个别名字典，然后apply 替换", "Konwledge_Point": "应对NP完全问题", "Question": "dataframe中，怎样实现old列按照new列名字重新命名，然后把结果放在第三列\n以 new 列中的名字为标准，把 old 列中的名字全部替换，然后放在第三列，两列中虽然名字不一样，但其实是同一个物质，如图中标示颜色所示，\n\n\n\n\n代码如下：\n\n\nimport\n  pandas  \nas\n  pd\n\nimport\n  numpy  \nas\n  np\ndf = pd.DataFrame({\n\n'old'\n: [\n'醋酸铵'\n, \n'氨水（5mol/L氨溶液）'\n, \n'甘油'\n, \n'盐酸萘乙二胺'\n, \n'氨水'\n, \n'硫酸铜'\n, \n'钙紫红素'\n, \n'冰醋酸'\n,np.\nnan\n, np.\nnan\n, np.\nnan\n, np.\nnan\n, np.\nnan\n, np.\nnan\n, np.\nnan\n, np.\nnan\n, np.\nnan\n, np.\nnan\n, np.\nnan\n, np.\nnan\n, np.\nnan\n,np.\nnan\n], \n\n'new'\n: [\n'乙酸铵'\n, \n'十二水合硫酸铝钾'\n, \n'硫酸铵'\n, \n'百里香酚蓝（溴酚蓝）'\n, \n'钙紫红素'\n, \n'间甲酚紫'\n, \n'硫酸钙(无水)'\n,\n'氢氧化氨(氨水)'\n, \n'磷酸氢二钾'\n, \n'盐酸萘乙二胺'\n, \n'乙二胺四乙酸二钠盐，二水'\n, \n'冰醋酸（冰乙酸）'\n,\n'抗坏血酸（维生素C）'\n, \n'氨水（5mol/L氨溶液）'\n, \n'无水硫酸镁'\n, \n'丙三醇(甘油)'\n, \n'四水合，酒石酸钾钠'\n,\n'邻苯二甲酸氢钾'\n, \n'磷酸二氢钾'\n, \n'亚铁氰化钾'\n, \n'硫酸钾'\n, \n'五水合硫酸铜'\n]})\n", "Tag": "算法分析"}
{"Answer": "别人的好像和你的不太一样\nhttps://blog.csdn.net/weixin_43669978/article/details/120914852", "Konwledge_Point": "应对NP完全问题", "Question": "python使用model.compile方法的时候，遇到AttributeError: 'NoneType' object has no attribute 'compile'这个问题\n在跑《deep learning for computer vision with python》第一本第15章的mvggnet程序时候，遇到使用model.compile方法出现bug。'NoneType' object has no attribute 'compile'\n\n\nimport matplotlib\n\nmatplotlib.use(\n\"Agg\"\n)\n\n\nfrom\n sklearn.preprocessing import LabelBinarizer\n\nfrom\n sklearn.metrics import classification_report\n\nfrom\n pyimagesearch.nn.conv import minivggnet\n\nfrom\n keras.optimizers import SGD\n\nfrom\n keras.datasets import cifar10\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\n\nap = argparse.ArgumentParser()\nap.add_argument(\n\"-o\"\n, \n\"--output\"\n, \nrequired\n=\nTrue\n,\n                \nhelp\n=\n\"path to the output loss/accuracy plot\"\n)\nargs = vars(ap.parse_args())\n\n\nprint\n(\n\"[INFO] loading CIFAR-10 data...\"\n)\n((trainX, trainY), (testX, testY)) = cifar10.load_data()\ntrainX = trainX.astype(\n\"float\"\n) / 255.0\ntestX = testX.astype(\n\"float\"\n) / 255.0\n\nlb = LabelBinarizer()\ntrainY = lb.fit_transform(trainY)\ntestY = lb.transform(testY)\n\nlabelNames = [\n\"airplane\"\n, \n\"automobile\"\n, \n\"bird\"\n, \n\"cat\"\n, \n\"deer\"\n,\n              \n\"dog\"\n, \n\"frog\"\n, \n\"horse\"\n, \n\"ship\"\n, \n\"truck\"\n]\n\n\nprint\n(\n\"[INFO] compiling model...\"\n)\nopt = SGD(\nlr\n=0.01, \ndecay\n=0.01 / 40, \nmomentum\n=0.9, \nnesterov\n=\nTrue\n)\nmodel = minivggnet.build(\nwidth\n=32, \nheight\n=32, \ndepth\n=3, \nclasses\n=10)\nmodel.compile(\nloss\n=\n'categorical_crossentropy'\n, \noptimizer\n=opt,\n              metrics=[\n\"accuracy\"\n])\n\n\nprint\n(\n\"[INFO] training network...\"\n)\nH = model.fit(trainX, trainY, validation_data=(testX, testY), \nbatch_size\n=64,\n              \nepydoc\n=40, \nverbose\n=1)\n\n\nprint\n(\n\"[INFO] evaluating network...\"\n)\npredictions = model.predict(testX, \nbatch_size\n=64)\n\nprint\n(classification_report(testY.argmax(\naxis\n=1),\n                            predictions.argmax(\naxis\n=1),\n                            \ntarget_names\n=labelNames))\n\nplt.style.use(\n\"ggplot\"\n)\nplt.figure()\nplt.plot(np.arange(0, 40), H.history[\n\"loss\"\n], \nlabel\n=\n\"train_loss\"\n)\nplt.plot(np.arange(0, 40), H.history[\n\"val_loss\"\n], \nlabel\n=\n\"val_loss\"\n)\nplt.plot(np.arange(0, 40), H.history[\n\"accuracy\"\n], \nlabel\n=\n\"train_accuracy\"\n)\nplt.plot(np.arange(0, 40), H.history[\n\"val_accuracy\"\n], \nlabel\n=\n\"val_accuracy\"\n)\nplt.title(\n\"Training Loss and Accuracy on CIFAR-10\"\n)\nplt.xlabel(\n\"Epoch #\"\n)\nplt.ylabel(\n\"Loss/Accuracy\"\n)\nplt.legend()\nplt.savefig(args[\n\"output\"\n])\n\n\n\n\nAttributeError: 'NoneType' object has no attribute 'compile'\n\n\n有无帅哥美女帮忙解决一下问题哭了\n\n\n我想要达到的结果\n\n\n\n\n\n\n\n\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "你的数据中有非数值型的字符串型数据，你检查一下新使用的数据", "Konwledge_Point": "应对NP完全问题", "Question": "python聚类问题\n问题遇到的现象和发生背景\n\n\n部分数据集\nd=[[1994.0, 9.6], [1957.0, 9.5], [1997.0, 9.5], [1994.0, 9.4], [1993.0, 9.4], [2012.0, 9.4], [1993.0, 9.4], [1997.0, 9.4], [2013.0, 9.4], [1994.0, 9.4], [2003.0, 9.3], [2016.0, 9.3], [2009.0, 9.3], [2009.0, 9.3], [2008.0, 9.3], [2008.0, 9.3], [1957.0, 9.3], [2008.0, 9.3], [2001.0, 9.2], [2009.0, 9.2], [1931.0, 9.2], [1961.0, 9.2], [2010.0, 9.2], [2004.0, 9.2], [1998.0, 9.2]]\n\n\nimport\n random\n\nimport\n pandas as pd\n\nimport\n numpy as np\n\nimport\n matplotlib.pyplot as plt\n\n\n# 计算欧拉距离\n\ndef calcDis(dataSet, centroids, k):\n    \nclalist=[]\n\n    for data \nin\n dataSet:\n        \ndiff\n = np.tile(data, (k, \n1\n)) - centroids  \n#相减   (np.tile(a,(2,1))就是把a先沿x轴复制1倍，即没有复制，仍然是 [0,1,2]。 再把结果沿y方向复制2倍得到array([[0,1,2],[0,1,2]]))\n\n        \nsquaredDiff\n = diff ** \n2\n     \n#平方\n\n        \nsquaredDist\n = np.sum(squaredDiff, \naxis=1)\n   \n#和  (axis=1表示行)\n\n        \ndistance\n = squaredDist ** \n0.5\n  \n#开根号\n\n        clalist.append(distance) \n    \nclalist\n = np.array(clalist)  \n#返回一个每个点到质点的距离len(dateSet)*k的数组\n\n    return clalist\n\n\n# 计算质心\n\ndef classify(dataSet, centroids, k):\n    \n# 计算样本到质心的距离\n\n    \nclalist\n = calcDis(dataSet, centroids, k)\n    \n# 分组并计算新的质心\n\n    \nminDistIndices\n = np.argmin(clalist, \naxis=1)\n    \n#axis=1 表示求出每行的最小值的下标\n\n    \nnewCentroids\n = pd.DataFrame(dataSet).groupby(minDistIndices).mean() \n#DataFramte(dataSet)对DataSet分组，groupby(min)按照min进行统计分类，mean()对分类结果求均值\n\n    \nnewCentroids\n = newCentroids.values\n \n    \n# 计算变化量\n\n    \nchanged\n = newCentroids - centroids\n \n    return changed, newCentroids\n\n\n# 使用k-means分类\n\ndef kmeans(dataSet, k):\n    \n# 随机取质心\n\n    \ncentroids\n = random.sample(dataSet, k)\n    \n    \n# 更新质心 直到变化量全为0\n\n    changed, \nnewCentroids\n = classify(dataSet, centroids, k)\n    while np.any(changed != \n0\n):\n        changed, \nnewCentroids\n = classify(dataSet, newCentroids, k)\n \n    \ncentroids\n = sorted(newCentroids.tolist())   \n#tolist()将矩阵转换成列表 sorted()排序\n\n \n    \n# 根据质心计算每个集群\n\n    \ncluster\n = []\n    \nclalist\n = calcDis(dataSet, centroids, k) \n#调用欧拉距离\n\n    \nminDistIndices\n = np.argmin(clalist, \naxis=1)\n  \n    for i \nin\n range(k):\n        cluster.append([])\n    for i, j \nin\n enumerate(minDistIndices):   \n#enymerate()可同时遍历索引和遍历元素\n\n        cluster[j].append(dataSet[i])\n        \n    return centroids, cluster\n \n\n# 创建数据集\n\ndef createDataSet():\n   \n    return d\n    \n\nif\n \n__name__=='__main__':\n \n    \ndataset\n = createDataSet()\n    \n    centroids, \ncluster\n = kmeans(dataset, \n3\n)\n    print('质心为：%s' % centroids)\n    print('集群为：%s' % cluster)\n    for i \nin\n range(len(dataset)):\n        \nlabel_pred\n = estimator.labels_  \n# 获取聚类标签\n\n        \n# 绘制k-means结果\n\n        \nx0\n = X[\nlabel_pred\n == \n0\n]\n        \nx1\n = X[\nlabel_pred\n == \n1\n]\n        \nx2\n = X[\nlabel_pred\n == \n2\n]\n        plt.scatter(x0[:, \n0\n], x0[:, \n1\n], \nc=\"deeppink\",\n \nmarker='o',\n \nlabel='label0')\n\n        plt.scatter(x1[:, \n0\n], x1[:, \n1\n], \nc=\"green\",\n \nmarker='*',\n \nlabel='label1')\n\n        plt.scatter(x2[:, \n0\n], x2[:, \n1\n], \nc=\"blue\",\n \nmarker='+',\n \nlabel='label2')\n\n      \n        for j \nin\n range(len(centroids)):\n            plt.scatter(centroids[j][\n0\n],centroids[j][\n1\n],\nmarker='x',color='red',s=70,label='质心')\n\n        \n    plt.show()\n\n\n\n\n\n运行结果及详细报错内容\n\n\n想知道错在哪里，怎么修改", "Tag": "算法分析"}
{"Answer": "检查一下传入的tokenizer参数，导致抛出None值无encode_plus属性错误。", "Konwledge_Point": "应对NP完全问题", "Question": "如何解决XLnet分类存在的问题？\n使用XLnet进行文本分类，出现了下面的问题：\n\n\ndef\n \nget_inputs\n(\ncontent, tokenizer, max_len=\n120\n):\n    \n\"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n\n    inps = [tokenizer.encode_plus(t, max_length=maxlen, pad_to_max_length=\nFalse\n, add_special_tokens=\nTrue\n) \nfor\n t \nin\n content]\n    inp_tok = np.array([a[\n'input_ids'\n] \nfor\n a \nin\n inps])\n    ids = np.array([a[\n'attention_mask'\n] \nfor\n a \nin\n inps])\n    segments = np.array([a[\n'token_type_ids'\n] \nfor\n a \nin\n inps])\n    \nreturn\n inps, inp_tok, ids, segments\n\n\ndef\n \nwarmup\n(\nepoch, lr\n):\n    \n\"\"\"Used for increasing the learning rate slowly, this tends to achieve better convergence.\n    However, as we are finetuning for few epoch it's not crucial.\n    \"\"\"\n\n    \nreturn\n \nmax\n(lr +\n1e-6\n, \n2e-5\n)\n\n\ndef\n \nplot_metrics\n(\npred, true_labels\n):\n    \n\"\"\"Plots a ROC curve with the accuracy and the AUC\"\"\"\n\n    acc = accuracy_score(true_labels, np.array(pred.flatten() >= \n.5\n, dtype=\n'int'\n))\n    fpr, tpr, thresholds = roc_curve(true_labels, pred)\n    auc = roc_auc_score(true_labels, pred)\n\n    fig, ax = plt.subplots(\n1\n, figsize=(\n8\n,\n8\n))\n    ax.plot(fpr, tpr, color=\n'red'\n)\n    ax.plot([\n0\n,\n1\n], [\n0\n,\n1\n], color=\n'black'\n, linestyle=\n'--'\n)\n    ax.set_title(\nf\"AUC: \n{auc}\n\\nACC: \n{acc}\n\"\n);\n    \nreturn\n fig\n\ninps, inp_tok, ids, segments = get_inputs(x_train_text, xlnet_tokenizer)\n\n\n\nAttributeError                            Traceback (most recent call last)\n/tmp/ipykernel_18279/2457827602.py \nin\n \n----> 1 inps, inp_tok, ids, segments = get_inputs(x_train_text, xlnet_tokenizer)\n\n/tmp/ipykernel_18279/3374919276.py \nin\n get_inputs(content, tokenizer, max_len)\n      1 def get_inputs(content, tokenizer, \nmax_len\n=120):\n      2     \n\"\"\n\" Gets tensors from text using the tokenizer provided\"\n\"\"\n\n----> 3     inps = [tokenizer.encode_plus(t, \nmax_length\n=max_len, \npad_to_max_length\n=\nFalse\n, \nadd_special_tokens\n=\nTrue\n) \nfor\n t \nin\n content]\n      4     inp_tok = np.array([a[\n'input_ids'\n] \nfor\n a \nin\n inps])\n      5     ids = np.array([a[\n'attention_mask'\n] \nfor\n a \nin\n inps])\n\n/tmp/ipykernel_18279/3374919276.py \nin\n (.0)\n      1 def get_inputs(content, tokenizer, \nmax_len\n=120):\n      2     \n\"\"\n\" Gets tensors from text using the tokenizer provided\"\n\"\"\n\n----> 3     inps = [tokenizer.encode_plus(t, \nmax_length\n=max_len, \npad_to_max_length\n=\nFalse\n, \nadd_special_tokens\n=\nTrue\n) \nfor\n t \nin\n content]\n      4     inp_tok = np.array([a[\n'input_ids'\n] \nfor\n a \nin\n inps])\n      5     ids = np.array([a[\n'attention_mask'\n] \nfor\n a \nin\n inps])\n\nAttributeError: \n'NoneType'\n object has \nno\n attribute \n'encode_plus'\n\n\n", "Tag": "算法分析"}
{"Answer": "首先\r\n1 2 3 4\r\n5 6 7 8\r\n按照列得到序列\r\n1 5 2 6 3 7 4 8\r\n然后按照f reshape成4 2\r\n1 5 2 6 \r\n3 7 4 8\r\n上下对齐得到\r\n1 3\r\n5 7\r\n2 4\r\n6 8\r\n深入的解释\r\nhttps://stackoverflow.com/questions/45973722/how-does-numpy-reshape-with-order-f-work", "Konwledge_Point": "应对NP完全问题", "Question": "关于Python中的reshape,order相关问题\nimport numpy as np\n\nA = np.array([[1,2,3,4],[5,6,7,8]])\n\nA.reshape((4,2),order=\"F\")\n\n为什么显示的是\n\n\n\nOut[5]:\n\narray([[1, 3],\n\n       [5, 7],\n\n       [2, 4],\n\n       [6, 8]])\n\n\n\n        我知道order=\"F\"代表列优先，但是我实在搞不明白最后的结果顺序是经过怎样的排序后得出的\n", "Tag": "算法分析"}
{"Answer": "这么改就行了：\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\n'''让下面的都在一个框框里面'''\nax = Axes3D(fig,auto_add_to_figure=False)\nfig.add_axes(ax)\n'''绘制3D空间（坐标轴）'''\nX = np.arange(-4, 4, 0.25)\nY = np.arange(-4, 4, 0.25)\nX, Y = np.meshgrid(X, Y)\n'''把x,y绘制对应到底面的面上去'''\nR = np.sqrt(X ** 2 + Y ** 2 + np.exp(np.pi))\nZ = np.tanh(R)\nax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.get_cmap('rainbow'))\n'''绘制3D cmap又一种方法'''\nax.contourf(X, Y, Z, zdir='z', offset=0.99987, cmap='summer')\n'''顺便在某平面画个等高线 zdir是决定从哪个方向压下去'''\nplt.show()\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "matplotlib 3D绘图一个警告 请问怎么解决？\n代码内容\n\n\n\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n numpy \nas\n np\n\nfrom\n mpl_toolkits.mplot3d \nimport\n Axes3D\n\n\nfig = plt.figure()\n\n'''让下面的都在一个框框里面'''\n\nax = Axes3D(fig)\n\n\n'''绘制3D空间（坐标轴）'''\n\nX = np.arange(-\n4\n,\n4\n,\n0.25\n)\nY = np.arange(-\n4\n,\n4\n,\n0.25\n)\nX,Y = np.meshgrid(X,Y)\n\n'''把x,y绘制对应到底面的面上去'''\n\nR = np.sqrt(X**\n2\n + Y**\n2\n + np.exp(np.pi))\n\nZ = np.tanh(R)\n\nax.plot_surface(X,Y,Z,rstride=\n1\n,cstride=\n1\n,cmap=plt.get_cmap(\n'rainbow'\n))\n\n'''绘制3D cmap又一种方法'''\n\nax.contourf(X,Y,Z,zdir=\n'z'\n,offset=\n0.99987\n,cmap=\n'summer'\n)\n\n'''顺便在某平面画个等高线 zdir是决定从哪个方向压下去'''\n\n\nplt.show()\n\n\n\n图顺利出来，警告内容：\n\n\n\n\nMatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6.  This is consistent with other Axes classes.\n  ax = Axes3D(fig)\n\n\n\n\n如何解决警告的内容？", "Tag": "算法分析"}
{"Answer": "我改写了一下，效果一样速度应该可以快100倍吧\n\n    def __init_params(self, grayFrame):\n        for k in range(self.__defaultNbSamples):\n            c = np.random.randint(-1,2,size=(self.__Height,self.__Width))\n            self.__samples[k] = np.maximum(grayFrame + c, 0)\n", "Konwledge_Point": "应对NP完全问题", "Question": "Python运行vibe算法过慢\n现在在做毕设，有一部分要用到前景目标提取，我打算用VIBE算法来做，但是同样的算法，Python运行时初始化都要20-50s，我朋友用matlab基本就是1s内最多1s多一点就能完成初始化，虽然Python运行慢，但是也不至于慢这么多吧，而且理论上vibe算法的运行速度应该是比较快的，大佬们看一下是不是写的代码的问题。。。\n\n\n\ndef initial_background(I_gray, N):\n    t1 = cv2.getTickCount()\n    I_pad = np.pad(I_gray, 1, 'symmetric')#对称填充\n    height = I_pad.shape[0]\n    width = I_pad.shape[1]\n    samples = np.zeros((height, width, N))\n    t2 = cv2.getTickCount()\n    time = (t2 - t1) * 1000 / cv2.getTickFrequency()\n    print(time)\n    for i in range(1, height - 1):\n        for j in range(1, width - 1):\n            for n in range(N):\n                x, y = 0, 0\n                while (x == 0 and y == 0):\n                    x = np.random.randint(-1, 1)\n                    y = np.random.randint(-1, 1)\n                ri = i + x\n                rj = j + y\n                samples[i, j, n] = I_pad[ri, rj]\n    t3 = cv2.getTickCount()\n    time2 = (t3 - t1) * 1000 / cv2.getTickFrequency()\n    print(time2)\n    samples = samples[1:height - 1, 1:width - 1]\n    return samples\n\n\n\n\n上面是Python的初始化部分代码，大佬们帮忙看看这部分代码有哪些可以优化的。matlab代码我看不懂，也不知道该发哪个，第一次在CSDN上问问题，不知道会不会有人回答。。。", "Tag": "算法分析"}
{"Answer": "data为二维数组，axis=0求每一列的标准差，输出为一行；axis=1求每一行的标准差。输出为一列。", "Konwledge_Point": "应对NP完全问题", "Question": "numpy 很基础的问题\ndata = [[1, 2], [3, 4]]\nstd1 = np.std(data, axis=0)\nprint(\"std1:\", std1)\n\n\ndata = [[1, 2], [4, 3]]\nstd2 = np.std(data, axis=0)\nprint(\"std2:\", std2)\n\n\n结果：\nstd1: [1. 1.]\nstd2: [1.5 0.5]\n\n\n为什么这两段代码运行结果不一样？", "Tag": "算法分析"}
{"Answer": "可以看一下我的博客http://t.csdn.cn/2TE4K", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的问题：问题遇到的现象和发生背景\n问题遇到的现象和发生背景\n\n\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n\n\n\n\n我想要达\n\n\n```python\nimport numpy as np\nimport cv2\ncap=cv2.VideoCapture(0)\n\n\nwhile(True):\n    ret,frame=cap.read()\n    frame=cv2.flip(frame,1)\n    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n    cv2.imshow('frame',frame)\n    cv2.imshow('gray',gray)\n    if cv2.waitKey(1)&0xFF==ord('q'):\n        break\n\n\ncap.release()\ncv2.destroyAllWindows()\n\n\n```到的结果", "Tag": "算法分析"}
{"Answer": " 设置import matplotlib as mplmpl.use('module://backend_interagg') \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# 保留到磁盘，需要设置 mpl.use('Agg') 同时注释掉plt.show()\n# 设置 mpl.use('module://backend_interagg') plt.show才能显示\nimport matplotlib as mpl\nprint(mpl.get_backend())\n# mpl.use('Agg') # 保留图片到png pdf等需要设置，并且注释掉plt.show()\nmpl.use('module://backend_interagg') # plt.show()需要设置\n\nx = np.random.uniform(10, 40, 30)\ny = np.random.uniform(100, 200, 30)\nz = np.random.uniform(10, 20, 30)\n\nfig = plt.figure()\nax3d = Axes3D(fig)\nax3d.scatter(x, y, z, c=\"b\", marker=\"*\")\nplt.show()\n# plt.savefig('plot_axe3D.png')\n", "Konwledge_Point": "应对NP完全问题", "Question": "Pycharm中Axes3D库调用不显示图片\n问题遇到的现象和发生背景\n\n\n最近在研究一个遗传算法的代码，结果能跑通，但是配套的图片没有出来，原链接有一个3D的二元函数图片。\n后来发现，问题应该是出在from mpl_toolkits.mplot3d import Axes3D\n可能是这个没有被正确调用，然后测试了一下代码，也没有图片。\n\n\n用代码块功能插入代码，请勿粘贴截图\n\n\nimport numpy as np\nimport matplotlib\n.pyplot\n as plt\nfrom mpl_toolkits\n.mplot3d\n import Axes3D\n\n\nx = np\n.random\n.uniform\n(\n10\n,\n40\n,\n30\n)\ny = np\n.random\n.uniform\n(\n100\n,\n200\n,\n30\n)\nz = np\n.random\n.uniform\n(\n10\n,\n20\n,\n30\n)\n\n\nfig = plt\n.figure\n()\nax3d = Axes3D(fig)\nax3d\n.scatter\n(x,y,z,c=\n\"b\"\n,marker=\n\"*\"\n)\n\nplt\n.show\n()\n\n\n\n\n运行结果及报错内容\n\n\n没有生成图片，按理来说会生成3D图片\n\n\n我的解答思路和尝试过的方法\n\n\n我不知道 Axes3D 这个库是包含在matplotlib 这个库里面的还是单独的一个库，matplotlib 和numpy 库都有正确安装，也没有搜索到Axes3D这个库，不知道问题出现在哪里。\n\n\n我想要达到的结果\n\n\n刚学python，请问有知道的朋友，这个问题如何解决，感谢！", "Tag": "算法分析"}
{"Answer": "你好，需要知道的是python的下标从0开始的\ndef ThomasAGiven(N):\n    import numpy as np\n    Num = N + 2\n    h = 1/(Num-1)\n \n    x0 = np.ones(Num)\n    x1 = np.ones(Num-1)\n    A = 4*np.eye(Num)-(np.diag(-2*x0)+np.diag(x1,1)+np.diag(x1,-1))/(h*h)\n    A[0,:] = 0\n    A[Num-1,:] = 0\n    A[0,0] = 1\n    A[Num-1,Num-1] = 1\n    b = np.zeros((Num,1))\n    b[0] = 0\n    b[Num-1] = 10\n    x = np.linspace(0,1,Num)\n    ui = np.linalg.inv(A)*b\n    AA = (-10/((np.exp(-2)-np.exp(2))))*np.exp(2*x)+(10/((np.exp(-2)-np.exp(2))))*np.exp(-2*x)\n    ua = AA\n    single_err = np.diag(ui-ua)\n    sqr_err = single_err**2\n    err = np.sqrt(np.sum(sqr_err))\n    return err\nimport numpy as np\nimport matplotlib.pyplot as plt \nN = np.array([10,20,40,80])\nerr = np.zeros(4)\nfor i in range(len(N)):\n    err[i] = ThomasAGiven(N[i])\nplt.plot(np.log(N),np.log(err))\nplt.show()\np = np.polyfit(np.log(N),np.log(err),1)%线性拟合\nprint('斜率是', p[0])\n\n\n结果\n\n斜率是 0.5244323436897312", "Konwledge_Point": "应对NP完全问题", "Question": "Matlab转成python码 一直在报错 请问能帮我改一下吗？\n问题遇到的现象和发生背景\n\n\n原要求是用托马斯算法写一个公式\n4u - d2u/dx2 = 0,\nboundary是 u(0) =0, u(1) = 10\n要求算N = 10 20 40 80分别的analytical solution (ua)  和 central difference solution (ui)\n然后求两个解的error\nerr=sqrt(sum(diag(ui-ua).^2));\n\n\n问题相关代码，请勿粘贴截图\n\n\n这是我原先的matlab码 \n\n\nN = \n10\n; %10, 20, 40,  80\n\nNum = N+2\n;\n\nh = \n1.0\n/(\nNum-1\n)\n;\n\nx0=ones(\nNum\n,\n1\n)\n;\n\nx1=ones(\nNum-1\n,\n1\n)\n;\n\nA=4*eye(\nNum\n)-(\ndiag\n(\n-2*x0\n)+diag(\nx1\n,\n1\n)+diag(\nx1\n,\n-1\n))/(\nh*h\n)\n;\n\nA(\n1\n,:)=0\n;\n\nA(\nNum\n,:)=0\n;\n\nA(\n1\n,\n1\n)=1\n;\n\nA(\nNum\n,Num)=1\n;\n\nb=zeros(\nNum\n,\n1\n)\n;\n\nb(\n1\n)=0\n;\n\nb(\nNum\n)=10\n;\n\nx=linspace(\n0\n,\n1\n,Num)\n;\n\nui=inv(\nA\n)*b\n;\n\nAA=(\n-10/\n((\nexp\n(\n-2\n)-exp(\n2\n))))*exp(\n2\n*x)+(10/((exp(-2)-exp(2))))*exp(\n-2*x\n)\n;\n\nua=AA\nplot(\nx\n,ui,x,ua,'Marker','*')\nsingle_err=diag(\nui-ua\n)\n;\n\nsqr_err=single_err.^\n2\n;\n\nerr=sqrt(\nsum\n(\nsqr_err\n))\n;\n\nerr\n;\n\n%now I'll try to convert everything to horizontal format and export ua, ui,\n%and x onto the same sheet. %\nsheet = [x\n;ui';ua];\n\n\n\n\n最后可以对每一个N（10，20，40，80）画出一个ua ui的线图，同时给出对应的error值。我想把这个码改写成python形式。 \n\n\n运行结果及报错内容\n\n\nValueError                                Traceback (most recent call last)\n in \n----> 1 ThomasAGiven(10)\n\n\n in ThomasAGiven(N)\n      7     x0 = np.ones((Num,1))\n      8     x1 = np.ones((Num-1,1))\n----> 9     A = 4\nnp.eye(Num)-(np.diag(-2\nx0)+np.diag(x1,1)+np.diag(x1,-1))/(h*h)\n     10     A[1,:] = 0\n     11     A[Num,:] = 0\n\n\nValueError: operands could not be broadcast together with shapes (12,12) (0,) \n\n\n我的解答思路和尝试过的方法\n\n\n以下是我当前改的，但一直说我operand有问题，向量/矩阵格式不对\n\n\ndef\n ThomasAGiven(N):\n    \nimport\n numpy as np\n    \nimport\n matplotlib.pyplot as plt \n    \nNum\n = N + \n2\n\n    \nh\n = \n1\n/(Num-\n1\n)\n\n    \nx0\n = np.ones((Num,\n1\n))\n    \nx1\n = np.ones((Num-\n1\n,\n1\n))\n    \nA\n = \n4\n*np.eye(Num)-(np.diag(-\n2\n*x0)+np.diag(x1,\n1\n)+np.diag(x1,-\n1\n))/(h*h)\n    \nA\n[\n1\n,:] = \n0\n\n    \nA\n[Num,:] = \n0\n\n    \nA\n[\n1\n,\n1\n] = \n1\n\n    \nA\n[Num,Num] = \n1\n\n    \nb\n = np.zeros[Num,\n1\n]\n    \nb\n[\n1\n] = \n0\n\n    \nb\n[Num] = \n10\n\n    \nx\n = np.linspace(\n0\n,\n1\n,Num)\n    \nui\n = np.linalg.inv(A)*b\n\n    \nAA\n = (-\n10\n/((np.exp(-\n2\n)-np.exp(\n2\n))))*np.exp(\n2\n*x)+(\n10\n/((np.exp(-\n2\n)-np.exp(\n2\n))))*np.exp(-\n2\n*x)\n    \nua\n = AA\n    \nsingle_err\n = np.diag(ui-ua)\n    \nsqr_err\n = single_err**\n2\n\n    \nerr\n = np.sqrt(np.sum(sqr_err))\n    \nreturn\n err\n\n\n\n我想要达到的结果\n\n\n改成python码，同时把每一个N对应的error值写入注明斜率的loglog图。\n（log每一个N值，同时log每一个对应的error值，然后插入图表，这个线状图应该用matplotlib的哪一个图呢？） ", "Tag": "算法分析"}
{"Answer": "将int型转化为float形,方便与后面的numpy公式进行运算可以简单理解为数字1和2", "Konwledge_Point": "应对NP完全问题", "Question": "python书写数学公式中的疑惑\n下为两个数学公式 非常疑惑1.与2.代表的含义\n\n\ndef f1(x,t): \n    \nreturn\n \n1\n./\nnp\n.\ncosh\n(x+\n3\n)*\nnp\n.\nexp\n(\n2.\n3j*t)\n\ndef f2(x,t):\n    \nreturn\n \n2\n./\nnp\n.\ncosh\n(x)*\nnp\n.\ntanh\n(x)*\nnp\n.\nexp\n(\n2.\n8j*t)\n", "Tag": "算法分析"}
{"Answer": "实际上，Python自带的列表list，也是这样倒序的。在list或numpy的array后面，有三种情况（以下x为list类型或array类型）：\nx[a]表示x中第a项（以0开始），如x[0]表示x中的第一项，x[5]表示x中的第六项，以此类推。特殊用法：x[-1]表示x中的最后一项，以此类推。x[a:b]表示将x中从a开始到b这个部分（不包括b），即由x[a],x[a+1],x[a+2],...,x[b]这个部分组成的列表复制一份。特殊用法：x[:b]表示从x开头一直到b这个部分（不包括b），同理，x[a:]表示x的a一直到结尾这个部分，而x[:]则是将x列表完全复制一份。注意！不能直接将x赋值给其他变量！否则，其中一个列表发生变化，另一个列表也会跟着发生相同的变化，只能使用x[:]的方法将副本赋值给另一个变量。x[a:b:k]表示与2的用法几乎相同的作用，但是它是从a开始，每隔k个元素地选取。如设x为[0,1,2,3,4,5]，则x[::2]表示从开头开始，到结尾，选特定的值[0,2,4]（到5之后因为没有值了就忽略）。特殊用法：即问题所述，a[::-1]，表示从最后开始，到最前面，因为每个值是列表，而里层列表还是不变的顺序，只改变外层列表的顺序。因此为[[3 4][2 3][1 2]]。这也是翻转列表的一种常用方式。", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的问题：如何理解numpy倒序\n给出下列代码：\n\n\nimport numpy as np\n\na\n = np\n.array\n(\n[[1, 2]\n,\n[2, 3]\n,\n[3, 4]\n])\n\na\n = \na\n[::-1]\n\n\nprint\n(a)\n\n\n\n\n输出：\n\n\n[[3 4]\n [2 3]\n [1 2]]\n\n\n\n\na = a[::-1]中第一个：如何理解？感谢！", "Tag": "算法分析"}
{"Answer": "K-Means算法从随机初始化簇质心开始。每次运行KMeans时，此选择都会有所不同，可能会产生不同的结果。为了得到可再现的结果，可以在KMeans中使用random_state参数，这将修复簇中心线的初始选择：\nmodel = KMeans(n_clusters=number_of_clusters, \n               init='k-means++', \n               max_iter=100, \n               n_init=100, \n               random_state=123)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的问题，如何解决？\npython在跑k均值聚类时候的时候，每次运行结果都不一样，求帮忙瞅瞅是哪出问题了？\n\n\nimport numpy as np\n\nfrom\n sklearn.datasets import load_iris\niris = load_iris()\nx,y =iris.data,iris.target\n\n#x=np.array([[1,1,1,1],[10,10,10,10],[20,20,20,20],[5,5,5,5],[3,3,3,3],[6,6,6,6],[25,25,25,25]])\n\ncentroids_a=[]\nlist1=[]\n\n#聚类中心\n\ndef init_random_centroids(k,x):\n    \ndata1\n=x.shape[0]\n    suiji_a =np.random.choice(data1,k,\nreplace\n=\nFalse\n)\n    \nfor\n i \nin\n suiji_a:\n        xlist = x[i]\n        centroids_a.append(xlist)\n        centroids =np.array(centroids_a)\n    return centroids\n\n\n\n#计算距离\n\ndef euclidean_distance(one_sample, x):\n    #\nprint\n(\n\"中心点\"\n,x)\n    #\nprint\n(\n\"样本\"\n,one_sample)\n    #\nprint\n(\n\"单个样本\"\n,one_sample)\n    #\nprint\n(\n\"中心\"\n,x)\n    distances= np.sum((x-one_sample)*\n*2\n)\n    #\nprint\n(\n\"距离\"\n,distances)\n    return distances\n\n\n\n#返回离样本最近的中心索引\n\ndef _closest_centroid(sample, centroids):\n    k = centroids.shape[0]\n    #\nprint\n(\n\"k\"\n,k)\n    #\nprint\n(\n\"聚类中心\"\n,centroids)\n    list_distance =[]\n    \nfor\n i \nin\n range(k):\n        \na\n=euclidean_distance(one_sample=sample, \nx\n=centroids[i])\n        list_distance.append(a)\n        # \nprint\n(\n\"list\"\n,list_distance)\n        \ndistance_list\n=np.array(list_distance)\n        id =np.argmin(distance_list)\n    #\nprint\n(\n\"单个样本与聚类中心的距离：\"\n,distance_list)\n    #\nprint\n(\n\"距离最近的中心索引\"\n,id)\n    return id\n\n\n#将所有样本进行归类，归类规则就是将该样本归类到与其最近的中心\n\ndef create_clusters(k,centroids, x):\n    clusters = [[] \nfor\n _ \nin\n range(k)]\n    #\nprint\n(\n\"cen\"\n,centroids)\n    \nfor\n sample \nin\n x:\n        \nys\n=_closest_centroid(sample, centroids)\n        #\nprint\n(ys)\n        clusters[ys].append(sample)\n    #\nprint\n(\n\"0\"\n,clusters[0])\n    #\nprint\n(\n\"1\"\n,clusters[1])\n    #\nprint\n(\n\"2\"\n,clusters[2])\n    return clusters\n\n\n#中心点更新\n\ndef update_centroids(k,clusters):\n    #\nprint\n(k)\n    # \nprint\n(\n\"类型0\"\n,clusters[0])\n    # \nprint\n(\n\"类型1\"\n,clusters[1])\n    # \nprint\n(\n\"类型2\"\n,clusters[2])\n    \nfor\n i \nin\n range(k):\n        centroid = np.mean(clusters[i], \naxis\n=0)\n        #\nprint\n(\n\"平均聚点\"\n,centroid)\n        centroids[i] = centroid\n    \nprint\n(\n\"新聚类中心\"\n,centroids)\n    return centroids\n\n\n\n# 将所有样本进行归类，其所在的类别的索引就是其类别标签\n\ndef get_cluster_labels(clusters, x):\n    #\nprint\n(clusters)\n    y_pred = []\n    \nfor\n sample \nin\n x:\n        \nys\n=_closest_centroid(sample=sample, \ncentroids\n=centroids)\n        y_pred.append(ys)\n    \nprint\n(y_pred)\n    return y_pred\n\n\n\n\n#随机选取k个聚类中心\n\ncentroids = init_random_centroids(3, x)\n\nfor\n number \nin\n range(20000000):\n    #样本归类\n    cluster = create_clusters(\nk\n=3,centroids=centroids,x=x)\n    former_centroids = centroids\n    #更新新的聚类中心\n    clusters = update_centroids(\nk\n=3,clusters=cluster)\n    diff = centroids - former_centroids\n    \nif\n diff.any() <0.000001:\n        get_cluster_labels(\nclusters\n=cluster,x=x)\n        break\n\n", "Tag": "算法分析"}
{"Answer": "\n#include<iostream>\nusing namespace std;\n\nstruct Student\n{\n    long num; //学号\n    char name[20]; //姓名\n    char sex; //性别\n    float score; //成绩\n};\n\nstruct SLink\n{\n    struct Student s;\n    struct SLink* next;\n} *np;\n\nvoid createLink()\n{\n    np = nullptr;\n}\n\nvoid insertData(struct Student s) \n{\n    if (np == nullptr){\n        np = (struct SLink*)malloc(sizeof(struct SLink));\n        np->s = s;\n        np->next = nullptr;\n        return;\n    }\n    SLink* pPrevNode = nullptr;\n    SLink* pNode = np;\n    while (pNode) {\n        if (s.num <= pNode->s.num) {\n            if (pPrevNode == nullptr) {\n                np = (struct SLink*)malloc(sizeof(struct SLink));\n                np->s = s;\n                np->next = pNode;\n            }\n            else {\n                pPrevNode->next = (struct SLink*)malloc(sizeof(struct SLink));\n                pPrevNode->next->s = s;\n                pPrevNode->next->next = pNode;\n            }\n            break;\n        }\n        if (pNode->next == nullptr){\n            pNode->next = (struct SLink*)malloc(sizeof(struct SLink));\n            pNode->next->s = s;\n            pNode->next->next = nullptr;\n            break;\n        }\n        pPrevNode = pNode;\n        pNode = pNode->next;\n    }\n}\n\nvoid deleteData(long num)\n{\n    SLink* pPrevNode = nullptr;\n    SLink* pNode = np;\n    while (pNode){\n        if (num == pNode->s.num){\n            if (pPrevNode == nullptr){\n                np = pNode->next;\n            }\n            else{\n                pPrevNode->next = pNode->next;\n            }\n            //SLink* pNext = pNode->next;\n            free(pNode);\n            break;\n           // pNode = pNext;\n        }\n        else{\n            pPrevNode = pNode;\n            pNode = pNode->next;\n        }\n    }\n}\n\nvoid printStudent(const Student& stud)\n{\n    cout << stud.num << \"\\t\" << stud.name << \"\\t\" << (stud.sex ? \"male\" : \"female\") << \"\\t\" << stud.score << endl;\n}\n\nvoid printLink() \n{\n    cout << \"num\\t\" << \"name\\t\" << \"sex\\t\" << \"score\" << endl;\n    SLink* pNode = np;\n    while (pNode) {\n        printStudent(pNode->s);\n        pNode = pNode->next;\n    }\n}\n\nint main(void)\n{\n    createLink();\n\n    struct Student a {0, \"a\", true,  100};\n    struct Student b {10, \"b\", true,  90};\n    struct Student c {2, \"c\", true,  80};\n    struct Student d {3, \"d\", true,  100};\n    struct Student e {43, \"e\", true,  99};\n    struct Student f {5, \"f\", true,  60};\n\n    insertData(a);\n    insertData(b);\n    insertData(c);\n    insertData(d);\n    insertData(e);\n    insertData(f);\n\n    printLink();\n    deleteData(10);\n    printLink();\n\n    int i;\n    cin >> i;\n\n    return 0;\n}\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "建立某链表的有关问题\n‎建立一个链表，链表节点用于存储如下结构体：\n\n\n‍\n‎        struct Student\n\n\n‍\n‎ { long num;            //学号\n\n\n‍\n‎ char name[20];    //姓名\n\n\n‍\n‎ char sex;              //性别\n\n\n‍\n‎ float score;          //成绩\n\n\n‍\n‎ };\n\n\n‍\n‎要求：使用函数和指针进行合理的编程，\n\n\n‍\n‎    建立链表\n\n\n‍\n         struct SLink\n\n\n‍\n         {     struct Student s;\n\n\n‍\n               struct SLink *next;\n\n\n‍\n         } *np;\n\n\n‍\n‎    ，并设计如下函数：\n\n\n‍\n‎    1）createLink()生成一个空链表；\n\n\n‍\n‎    2）insertData(struct Student s)按学生学号由小到大的顺序，将s插入到链表合适的位置；\n\n\n‍\n‎      提示：使用np=(struct SLink *) malloc(sizeof(struct SLink)开辟内存以建立新节点。\n\n\n‍\n‎    3）deleteData(long num)删除学号为num的节点。\n\n\n‍\n‎      提示：使用free(np)释放被删除节点的内存; \n\n\n‍\n‎    4）printLink()顺序打印输出链表各节点的内容。\n\n\n‍", "Tag": "算法分析"}
{"Answer": "extract_features在哪里调用呗，路径就是在调用的时候输入的，例如Matcher.match(image_path,5),这里的imgpath就是图像的路径。或者你可以像我这样在调用里面print一下路径，就知道每次调用的图像在哪里\ndef extract_features(image_path, vector_size=32):\n    image = imread(image_path, mode=\"RGB\")\n    print(image_path)\n", "Konwledge_Point": "应对NP完全问题", "Question": "没有添加路径 不知道问题在哪 也没有报错 可否讲下代码\n没有报错 不知道哪里有问题 程序是github上找的\nimport cv2\nimport numpy as np\nimport pickle as pk\nimport random\nimport os\nimport matplotlib.pyplot as plt\nfrom imageio import imread\n\n\ndef extract_features(image_path, vector_size=32):\n    image = imread(image_path, mode=\"RGB\")\n    try:\n        alg = cv2.SIFT_create()\n        kps = alg.detect(image)\n        kps = sorted(kps, key=lambda x: -x.response)[:vector_size]\n        kps, dsc = alg.compute(image, kps)\n        dsc = dsc.flatten()\n        needed_size = (vector_size * 64)\n        if dsc.size < needed_size:\n            dsc = np.concatenate([dsc, np.zeros(needed_size - dsc.size)])\n    except cv2.error as e:\n        print('Error: ', e)\n        return None\n\n\nreturn\n dsc\n\n\n\ndef batch_extractor(images_path, pickled_db_path):\n    files = [os.path.join(images_path, p) for p in sorted(os.listdir(images_path))]\n\n\nresult\n = {}\n\nfor\n f \nin\n files:\n    print('Extracting features \nfrom\n image %s' % f)\n    \nname\n = f.split('/')[\n-1\n].lower()\n    \nresult\n[\nname\n] = extract_features(f)\n\n\n# saving all our feature vectors in pickled file\n\n\nwith\n open(pickled_db_path, 'w') \nas\n fp:\n    pickle.dump(\nresult\n, fp)\n\n\n\nclass Matcher(object):\n\n\ndef\n \n__init__\n(\nself, pickled_db_path=\n\"features.pck\"\n):\n    \nwith\n \nopen\n(pickled_db_path) \nas\n fp:\n        self.data = pickle.load(fp)\n    self.names = []\n    self.matrix = []\n    \nfor\n k, v \nin\n self.data.iteritems():\n        self.names.append(k)\n        self.matrix.append(v)\n    self.matrix = np.array(self.matrix)\n    self.names = np.array(self.names)\n\n\ndef\n \ncos_cdist\n(\nself, vector\n):\n    \n# getting cosine distance between search image and images database\n\n    v = vector.reshape(\n1\n, -\n1\n)\n    \nreturn\n scipy.spatial.distance.cdist(self.matrix, v, \n'cosine'\n).reshape(-\n1\n)\n\n\ndef\n \nmatch\n(\nself, image_path, topn=\n5\n):\n    features = extract_features(image_path)\n    img_distances = self.cos_cdist(features)\n    \n# getting top 5 records\n\n    nearest_ids = np.argsort(img_distances)[:topn].tolist()\n    nearest_img_paths = self.names[nearest_ids].tolis\n", "Tag": "算法分析"}
{"Answer": "int是什么鬼？改成__init__，不然你都没有初始化model，导致你的model就是空的", "Konwledge_Point": "应对NP完全问题", "Question": "实现pytorch时出现空参数问题\n\n\nimport\n torch\n\nimport\n torch.nn \nas\n nn\n\nimport\n numpy \nas\n np\n\n\nx_train\n = [i for i \nin\n range(\n10\n)]\n\nx_train\n = np.array(x_train, dtype=np.float32)\n\nx_train\n.reshape(-\n1\n, \n1\n)\n\ny_train\n = [\n2\n * i + \n1\n for i \nin\n range(\n10\n)]\n\ny_train\n = np.array(y_train, dtype=np.float32)\n\ny_train\n.reshape(-\n1\n, \n1\n)\n\n\n\n\nclass\n \nMyModule\n(\nnn\n.\nModule\n):\n    def __int__(\nself\n, \ninput_dim\n, \noutput_dim\n):\n        super(\nMyModule\n, \nself\n).__int__()\n        self.linear = nn.\nLinear\n(1, 1)\n\n    def forward(\nself\n, \nx\n):\n        return self.linear(\nx\n)\n\n\n# 定义网络结果，损失函数，优化器\nif torch.cuda.is_available():\n    model = \nMyModule\n().cuda()\nelse:\n    model = \nMyModule\n()\nlea = 0.0001\noptimizer = torch.optim.\nSGD\n(\nmodel\n.\nparameters\n(), lr=lea)\ncriterion = nn.\nMSELoss\n()\nepochs = 10\nfor epoch in range(\nepochs\n):\n    optimizer.zero_grad()\n    if torch.cuda.is_available():\n        inputs = torch.from_numpy(\nx_train\n).cuda()\n        labels = torch.from_numpy(\ny_train\n).cuda()\n    else:\n        inputs = torch.from_numpy(\nx_train\n)\n        labels = torch.from_numpy(\ny_train\n)\n    outs = model(\nx_train\n)\n    loss = criterion(\nlabels\n, \ny_train\n)\n    loss.backward()\n    optimizer.step()\n    print('\nepoch\n {}\\\ntloss\n {}'.\nformat\n(\nepoch\n, \nloss\n))\n\n\n\n\n\n\npytorch版本   1.12.1+cu116\n代码如上，优化器那行报错了，如下。\nValueError: optimizer got an empty parameter list", "Tag": "算法分析"}
{"Answer": "本身jpg就没有alpha通道，换句话说jpg没有透明度，麻烦采纳一下哟，谢谢๑•́₃•̀๑", "Konwledge_Point": "应对NP完全问题", "Question": "用matplotlib画图保存出来后，为什么png格式坐标轴及标题的背景区域是透明的\n用matplotlib画图保存，以png格式输出的图片，坐标轴背景为透明，改成jpg后背景就是白色的了，这是为什么呢\n\n\nimport numpy as np\nimport matplotlib\n.pyplot\n as plt\n\nnp\n.random\n.seed\n(\n19680801\n)\nN=\n50\n\nx=np\n.random\n.rand\n(N)\ny=np\n.random\n.rand\n(N)\ncolors=np\n.random\n.rand\n(N)\narea=(\n30\n*np\n.random\n.rand\n(N))**\n2\n\nplt\n.title\n(\n'random'\n)\nplt\n.scatter\n(x,y,s=area,c=colors,alpha=\n0.5\n)\n\nplt\n.savefig\n(\n'E:\\jupyter notebook learning./test5.jpg'\n, dpi=\n600\n)\nplt\n.show\n()\n\n\n\n\n这样的图片：\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(19680801)\n\nN\n=50\n\nx\n=np.random.rand(N)\n\ny\n=np.random.rand(N)\n\ncolors\n=np.random.rand(N)\narea=(30*np.random.rand(N))*\n*2\n\nplt.title(\n'random'\n)\nplt.scatter(x,y,\ns\n=area,c=colors,alpha=0.5)\n\nplt.savefig(\nfname\n=\n'C:\\\\bubble\\\\HH.png'\n, \ndpi\n=600)\nplt.show() \n\n\n\n这样的图片：", "Tag": "算法分析"}
{"Answer": "scanf_s(\"%f\", &a)  \n\n改成\nscanf_s(\"%lf\", &a)\n\ndouble是双精度类型要用 %lf 输入  不能用 %f 输入 , 输出时用%f和%lf都可以\n#include<stdio.h>\n\nint main(void)\n{\n    double a;\n\n    printf(\"Enter a floating-point value:\");\n    scanf_s(\"%lf\", &a); // %f 改成 %lf\n    printf(\"\\n fixed-point notation: %lf\", a);  // %f 改成 %lf\n    printf(\"\\n exponential notation: %le\", a);  // %e 改成 %le\n    printf(\"\\np notation: %a\", a);\n\n    return 0;\n}\n\n\n\n\n如有帮助，请点击我的回答下方的【采纳该答案】按钮帮忙采纳下，谢谢!\n", "Konwledge_Point": "应对NP完全问题", "Question": "数据类型用float无问题，double则结果错误，如何解决？\n问题遇到的现象和发生背景\n\n\n如果a数据类型用double不能正确显示结果，如果用float则无问题\n编译器VS2022\n\n\n问题相关代码，请勿粘贴截图\n\n\n#\ninclude\n\n\n\n\nint\n \nmain\n(\nvoid\n)\n\n\n{\n    \ndouble\n a;\n\n    \nprintf\n(\n\"Enter a floating-point value:\"\n);\n    \nscanf_s\n(\n\"%f\"\n, &a);\n    \nprintf\n(\n\"\\n fixed-point notation: %f\"\n, a);\n    \nprintf\n(\n\"\\n exponential notation: %e\"\n, a);\n    \nprintf\n(\n\"\\np notation: %a\"\n, a);\n\n    \nreturn\n \n0\n;\n}\n\n\n\n运行结果及报错内容\n\n\n如图为数据类型设为double时结果\n\n\n\n\n如图为数据类型为float时的结果\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "目前来看难度较大，socrecardpy只能应用线性模型，你可以试试xgb回归与sgd回归，必须带有.coef_指令的模型", "Konwledge_Point": "应对NP完全问题", "Question": "scorecardpy的模型改写问题\n我用了scorecardpy包来做一个评分卡模型，现在问题是，我想将它自带的逻辑回归预测模型替换为我自己的autogluon模型，但是报错了，代码是这样的\n\n\nimport pandas as pd\nimport numpy as np\nimport scorecardpy as sc\n\n\ndat=pd\n.read_csv\n(\n\"dat.csv\"\n,index_col=\n0\n)\ndat\n\ndt_s = sc\n.var_filter\n(dat, y=\n\"isDefault\"\n)\n\ndt_s\n.info\n()\n\ntrain, test = sc\n.split_df\n(dt_s,\n'isDefault'\n,ratio=\n0.8\n)\n.values\n()\nbins = sc\n.woebin\n(dt_s, y=\n'isDefault'\n,method=\n\"chimerge\"\n)\nsc\n.woebin_plot\n(bins)\ntrain_woe = sc\n.woebin_ply\n(train, bins)\ntest_woe = sc\n.woebin_ply\n(test, bins)\ny_train = train_woe\n.loc\n[:,\n'isDefault'\n]\n\nX_train = train_woe\n.loc\n[:,train_woe.columns != \n'isDefault'\n]\n\ny_test = test_woe\n.loc\n[:,\n'isDefault'\n]\n\nX_test = test_woe\n.loc\n[:,train_woe.columns != \n'isDefault'\n]\n\nmport autogluon\nfrom autogluon\n.tabular\n import TabularDataset,TabularPredictor  \nimport pandas as pd\nimport numpy as np\nlabel=\n'isDefault'\n\ntrain_data=TabularDataset(train_woe )\nmetric = \n'roc_auc'\n\ntime_limit=\n60\n\npredictor=TabularPredictor(label=\nlabel\n,eval_metric=metric)\n.fit\n(train_data,presets=\n'best_quality'\n,time_limit=time_limit,auto_stack=True)\ntest_data=TabularDataset(test_woe)\npredictor\n.predict_proba\n(train_data)\ntrain_proba=predictor\n.predict_proba\n(train_data)\ntrain_proba=train_proba\n.values\n[:,1]\n\ntrain_proba = np\n.array\n(train_proba)\n.flatten\n() \ntrain_proba\npredictor\n.predict_proba\n(test_data)\nauto_proba=predictor\n.predict_proba\n(test_data)\nauto_proba=auto_proba\n.values\n[:,1]\n\nauto_proba = np\n.array\n(auto_proba)\n.flatten\n() \nauto_proba\nimport toad\nfrom toad\n.metrics\n import KS, AUC\n\n\n\n\nprint\n(\n'Training error'\n)\n\n\nprint\n(\n'KS:'\n, KS(train_proba,y_train)\n)\n\nprint\n(\n'AUC:'\n, AUC(train_proba,y_train)\n)\n\n\n\n\n\nprint\n(\n'\\nTest error'\n)\n\n\nprint\n(\n'KS:'\n, KS(auto_proba,y_test)\n)\n\nprint\n(\n'AUC:'\n, AUC(auto_proba,y_test)\n)\n\ncard = sc\n.scorecard\n(bins,predictor, X_train.\ncolumns\n)\n\n\n\n问题出在这个最后一句，我将我的模型predictor替换了scorecard包自带的lr，就显示报错\n'TabularPredictor' object has no attribute 'coef_'\n这个怎么解决？\n谢谢各位了", "Tag": "算法分析"}
{"Answer": "哦不好意思大家！我解决了，是在声明model时没有传入相关的参数，把第五行改成我定义的\n    model = FCN.get_fcn8s_model(input_shape=(256, 256, 3), class_no=2)\n\n就好啦", "Konwledge_Point": "应对NP完全问题", "Question": "关于Keras模型导出遇到Model expects 0 top-level weight(s). Received 1 saved top-level weight(s)的问题！\n问题遇到的现象和发生背景\n\n\nKeras训练模型结束后想要直接使用保存好的weights来预测模型，结果遇到了标题所示的错误\n\n\n问题相关代码\n\n\nif\n __name__ == \n\"__main__\"\n:\n    \n# Use VOC 2012 Dataset\n\n    \nhorse_path\n = 'membrane'\n    \nbatch_size\n = \n2\n\n    \nmodel\n = FCN.get_fcn8s_model()\n    \nprint\n('======== Start Test ===========')\n    \nmodel\n.load_weights('fcn32s.h5')\n    \n# 取val图片，测试一下效果\n\n    \nval_gen2\n = horse_test_gen.get_horse_generator(horse_path, batch_size=\n1\n, input_hw=(\n256\n, \n256\n, \n3\n),\n                                                  \nmask_hw\n=(\n256\n, \n256\n, \n2\n))\n    \ni\n = \n0\n\n    \nfor\n val_images in val_gen2:\n        \nimg_np\n = val_images[\n0\n]\n        \nimg_np\n = (img_np + \n1\n.) * \n128\n\n        \nim0\n = Image.fromarray(np.uint8(img_np))\n        \nim0\n.save('output/{}_img.jpg'.format(i))\n\n        \nres\n = model.predict(val_images)[\n0\n]\n        \npred_label\n = res.argmax(axis=\n2\n)\n        \npred_label\n[pred_label == \n1\n] = \n255\n\n        \nim1\n = Image.fromarray(np.uint8(pred_label))\n        \nim1\n.save('output/{}_pred.png'.format(i))\n\n        \ni\n += \n1\n\n        \nif\n i == \n3\n:\n            \nprint\n('End test')\n            \nsys\n.exit(\n0\n)\n\n\n\n运行结果及报错内容\n\n\n======== \nStart\n Test ===========\nTraceback (most recent \ncall\n last):\n  File \"E:/PycharmProjects/CloudInn/image_segmentation-master/main.py\", \nline\n \n42\n, \nin\n \n    model.load_weights(\n'fcn32s.h5'\n)\n  File \"E:\\PycharmProjects\\CloudInn\\venv\\lib\\site-packages\\keras\\engine\\training_v1.py\", \nline\n \n214\n, \nin\n load_weights\n    \nreturn\n super(Model, self).load_weights(filepath, by_name, skip_mismatch)\n  File \"E:\\PycharmProjects\\CloudInn\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", \nline\n \n67\n, \nin\n error_handler\n    \nraise\n e.with_traceback(filtered_tb) \nfrom\n \nNone\n\n  File \"E:\\PycharmProjects\\CloudInn\\venv\\lib\\site-packages\\keras\\saving\\hdf5_format.py\", \nline\n \n748\n, \nin\n load_weights_from_hdf5_group\n    \nraise\n ValueError(\nValueError: Weight count mismatch \nfor\n top-\nlevel\n weights \nwhen\n loading weights \nfrom\n file. Model expects \n0\n top-\nlevel\n weight(s). Received \n1\n saved top-\nlevel\n weight(s)\n\n\n\n自己去网上搜索也没有找到个所以然，为什么会说“模型要求0个顶级权重。收到1个已保存的顶级权重。”，这个要怎么解决？\n错误报在“model.load_weights('fcn32s.h5')”这里", "Tag": "算法分析"}
{"Answer": "对，你在wb=xlwt.Workbook()ws=wb.add_sheet('明细')style = xlwt.XFStyle()font = xlwt.Font()font.name = '微软雅黑' # 设置字体font.height = 20*8 #设置字号为10号style.font = fontws.write(0, 10, 10,style)wb.save('D:/python/数据.xlsx')这里设置了格式但你df1=pd.read_excel('D:/python/df1.xlsx')df1.to_excel('D:/python/数据.xlsx')你读取了一个文件，然后保存，新保存的将以前的覆盖掉了", "Konwledge_Point": "应对NP完全问题", "Question": "pandas to excel 修改单元格格式问题\n各位巨佬好，我现在遇到一个问题。就是用pandas数据处理做个一个dataframe，现在想导入到excel里，并且设置下单元格的格式，字体8号和微软雅黑等，但是用了下面的代码后，格式并没有修改，\n代码如下：\nimport warnings\nwarnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')\nimport pandas as pd\nimport numpy as np\nimport xlwt\n\n\nwb=xlwt.Workbook()\nws=wb.add_sheet('明细')\nstyle = xlwt.XFStyle()\nfont = xlwt.Font()\nfont.name = '微软雅黑' # 设置字体\nfont.height = 20*8 #设置字号为10号\nstyle.font = font\nws.write(0, 10, 10,style)\nwb.save('D:/python/数据.xlsx')\n\n\ndf1=pd.read_excel('D:/python/df1.xlsx')\ndf1.to_excel('D:/python/数据.xlsx')\n\n\n麻烦各位看下，我这个代码有什么问题吗，为啥最后单元格的格式并没有修改，谢谢各位了", "Tag": "算法分析"}
{"Answer": "路径而且建议不要文件夹带有空格", "Konwledge_Point": "应对NP完全问题", "Question": "为什么会报错啊，代码应该没有问题吧\n代码：\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimg=np.array(Image.open(\"C:\\Users\\lenovo\\Pictures\\Saved Pictures\\u=2404042546,574440023&fm=26&fmt=auto.webp\").convert(\"L\"))\nplt.imshow(img,plt.cm.gray)\nprint(img)\nrow=(img.shape[0])\nprint(row)", "Tag": "算法分析"}
{"Answer": "使用where函数进行条件判断，代码这样写：\nimport numpy as np\na = np.array([[1,2,3],[4,5,6],[7,8,9]])\nb = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([[1,0,6],[8,0,0],[0,2,5]])\nd=np.where(c==0,a+b,a)\nprint(d)\n\n\n运行结果：\n[[ 1  4  3]\n [ 4 10 12]\n [14  8  9]]\n\n\n如有帮助 ，请点采纳 。", "Konwledge_Point": "应对NP完全问题", "Question": "关于python numpy ndarray 遍历太慢的问题\n假设有三个矩阵，如下\n\n\na\n = np.array([[\n1\n,\n2\n,\n3\n],[\n4\n,\n5\n,\n6\n],[\n7\n,\n8\n,\n9\n]])\n\nb\n = np.array([[\n1\n,\n2\n,\n3\n],[\n4\n,\n5\n,\n6\n],[\n7\n,\n8\n,\n9\n]])\n\nc\n = np.array([[\n1\n,\n0\n,\n6\n],[\n8\n,\n0\n,\n0\n],[\n0\n,\n2\n,\n5\n]])\n\n\n\n想通过判断c矩阵相应位置的值，来决定a相应位置的值是否与b相应位置的值相加，直接想到的做法是遍历，如下：\n\n\nfor\n \ni\n \nin\n range(\n3\n):\n    \nfor\n j \nin\n range(\n3\n):\n        \nif\n c\n[i]\n[j]\n==\n0\n:\n            \na\n[i]\n[j]\n = \na\n[i]\n[j]\n + \nb\n[i]\n[j]\n\n\n\n\n但当a,b,c都很大时运算相当慢，有没有这种运算的矢量写法来提升效率？不用for循环", "Tag": "算法分析"}
{"Answer": "python中按数字切片的时候, 是包含前面不包含后面. 类似于数学中的  [1:4)", "Konwledge_Point": "应对NP完全问题", "Question": "关于numpy切片的问题的疑惑\n我在学习numpy切片的时候碰到一个疑惑\n\n\n\n\n\nimport numpy as np\na = np.array([[11, 12, 13, 14, 15],\n              [16, 17, 18, 19, 20],\n              [21, 22, 23, 24, 25],\n              [26, 27, 28 ,29, 30],\n              [31, 32, 33, 34, 35]])\nprint(a[0, 1:4])\n\n\n\n结果为什么是\n\n\n\n\n\n[12 13 14]\n\n\n\n而不是\n\n\n\n\n\n[12 13 14 15]\n\n\n\n ", "Tag": "算法分析"}
{"Answer": "http://blog.csdn.net/kevinelstri/article/details/52937236", "Konwledge_Point": "应对NP完全问题", "Question": "python分组绘制箱线图的问题 \n初学者。我现在有这样一张类似以下形式的excel表：\n\n编号 性别 工资\n\n1 男 5000\n\n2 女 4000\n\n3 男 8000\n\n4 男 7000\n\n5 女 1000\n\n6 男 6000\n\n7 女 3000\n\n我已经用pandas读入excel表了，用groupby对性别分组，现在想要用matplotlib在一张图上分别绘制男和女的工资的箱线图，但是好像出现了问题，不知道应该怎么解决.\n\nPS：这里原始表格包含中文\n\n我编的代码如下：\n\n\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\n\n\ndata = pd.read_excel('XXX.xlsx')\n\ndata1 = data.groupby('性别')\n\nplt.boxplot(data1['工资]) ", "Tag": "算法分析"}
{"Answer": "使用groupby方法。\n\nimport numpy as np\nimport pandas as pd\n \ndf = pd.DataFrame()\nn = 200\ndf['category'] = np.random.choice(('A', 'B'), n)\ndf['data1'] = np.random.randint(1, 100, len(df))\ndf['data2'] = np.random.randint(1, 100, len(df))\nprint(df)\n\nrowIndex = pd.Series()\nfor name, group in df.groupby('category'):\n\theading = group[group['data1']<group['data2'].max()].sort_values(by='data2', ascending=False).head(4)\n\trowIndex = pd.concat([rowIndex, pd.Series(heading.index)])\nout = df.loc[rowIndex]\nprint(out)\n\n\n    category  data1  data2\n71         A     23     99\n115        A     58     96\n167        A     87     95\n174        A     12     95\n119        B     12     99\n135        B     12     98\n117        B     90     98\n88         B     19     97\n ", "Konwledge_Point": "应对NP完全问题", "Question": "dataframe 分类排序问题2\n我之前问过类似的问题，见链接： \nhttps://ask.csdn.net/questions/7409644\n\n\n\n现在碰到更复杂的问题，我需要对dataframe按照类分类，每类按照该类data2的值最大的时候的data1值过滤该类，然后排序，请问该如何优化？\n\n\n\n我把整数改成浮点数，更精确一点 ，代码如下：\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame()\nn = 200\ndf['category'] = np.random.choice(('A', 'B'), n)\ndf['data1'] = np.random.rand(len(df))*100\ndf['data2'] = np.random.rand(len(df))*100\n\na = df[df['category'] == 'A']\nc = a[a['data2'] == a.data2.max()].data1.max()\na = a[a['data1'] <= c]\na = a.sort_values(by='data2', ascending=False).head(4)\n\nb = df[df['category'] == 'B']\nc = b[b['data2'] == b.data2.max()].data1.max()\nb = b[b['data1'] <= c]\nb = b.sort_values(by='data2', ascending=False).head(4)\n\ndf = pd.concat([a, b]).sort_values(by=['category', 'data1'], ascending=[True, False]).reset_index(drop=True)\nprint(df)\n\n\n\n结果为：\n\n\n\n\n\n  category      data1      data2\n0        A  77.453241  98.628388\n1        A  54.786469  97.470081\n2        A  19.618200  96.261181\n3        A   9.031004  97.067451\n4        B  50.751809  99.219009\n5        B  47.546003  96.488705\n6        B  32.735357  98.565826\n7        B  14.092039  95.359450", "Tag": "算法分析"}
{"Answer": "\ndata[' '] data是一个字典,data['**']代表取data字典中的一个数据,如data['captions'].shape[0]表示取data['captions']的数据,看样子取出来的数据是个矩阵列表,shape[0]是取这个矩阵列表一维列表的长度\n\n\n有帮助请采纳,有问题继续交流,你的采纳是对我回答的最大的肯定和动力\n", "Konwledge_Point": "应对NP完全问题", "Question": "学习中遇到的问题，关于tensorflow中的问题。\nn_examples = data['captions'].shape[0]\nn_iters_per_epoch = int(np.ceil(float(n_examples) / batch_size))\ncaptions = data['captions']\n这个data[' ']整体是什么意思啊？求解答一下。", "Tag": "算法分析"}
{"Answer": "\nimport  numpy as np\nnp.random.seed(0)  # 随机数种子为0\ndata = np.random.randn(7,4)    #随机生成一个a*b维的标准正态分布数组，浮点型\nprint(data)\n\n点击采纳该回答哦 谢谢", "Konwledge_Point": "应对NP完全问题", "Question": "关于python中用numpy求正态分布问题\n题目从name名单['Bob','Joe','Will','Bob','will','Joe','Joe' ]抽调出对应'Joe'的数据data\n\n\n其中data用np.random生成一组7*4符合标准正态分布的数据。（设置随机种子为0）\n\n\n要实现用np.random生成一组7\n4符合标准正态分布的数据\n题目对应截图：\n其中，我对于要求生成一组7\n4的数据却生成的是3*4的数据的地方不理解，请求指点", "Tag": "算法分析"}
{"Answer": "fread(&pname, sizeof(pname), 1, np);你这个结构最后有个next指针，与文本的行信息不一致啊建议修改：char buf[200];fgets(buf,200,np);sscanf(buf,\"%s %d %s %s %s %d %d %d %s %s\",pname.name,&pname->age,pname.sex,pname.tele,.....);", "Konwledge_Point": "应对NP完全问题", "Question": "从文件里读取数据调试发现字符串中的字符无效\n//我定义的链表\ntypedef struct stuffsystem\n{\n    char name[name1];\n    int age;\n    char sex[sex1];\n    char tele[tele1];\n    char sectionname[name1];\n    int latawork;//迟到早退的数量\n    int workextra;//加班的数量\n    int achieve;//个人创收\n    char addr[addr1];\n    char educ[educ1];\n    struct stuffsystem* next;\n}linklist;\n\n\n//相关函数\nint judgename(char name[name1])\n{\n    int input = 0;\n    FILE* np;\n    linklist pname;\n    if ((np = fopen(\"stuff.txt\", \"r\")) == NULL)\n    {\n        printf(\"警告！员工信息数据库丢失，请尽快找回\\n\");\n        exit(0);\n    }\n    fread(&pname, sizeof(pname), 1, np);\n    while (!feof(np))\n    {\n        if (!strcmp(pname.name, name))\n        {\n            input = 0;\n            fclose(np);\n            return input;\n        }\n        fread(&pname, sizeof(pname), 1, np);\n    }\n    fclose(np);\n    return input;\n}\n\n\n//调试截图\n\n\n\n\n//被读取的文件\n\n\n\n\n想知道问题出在哪里呀", "Tag": "算法分析"}
{"Answer": "参考GPT和自己的思路：你可以使用scipy.spatial.distance.cdist函数来计算两个array之间的距离，并使用argsort函数和切片来找到表1每个点与表2中距离最近的40个点。以下是一个示例代码，它使用分块计算来加快速度：\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\n# 假设points1和points2是两个array，每一行代表一个点的坐标\npoints1 = np.random.rand(100000, 3)\npoints2 = np.random.rand(200000, 3)\n\n# 将points2划分成多个块，每个块包含1000个点\nblock_size = 1000\nblocks2 = [points2[i:i+block_size] for i in range(0, len(points2), block_size)]\n\n# 计算每个点在points1中与所有块中的点的距离，并找到每个点距离最近的40个点的索引\nk = 40\ndistances = []\nfor block2 in blocks2:\n    dist = cdist(points1, block2)\n    indices = np.argpartition(dist, k-1)[:, :k]\n    distances.append((dist, indices))\ndistances = np.concatenate(distances, axis=1)\nnearest_indices = np.argsort(distances, axis=1)[:, :k]\n\n# 最近的40个点在points2中的坐标\nnearest_points2 = np.concatenate([blocks2[i][nearest_indices[:, i]] for i in range(len(blocks2))])\n\n# 计算表1每个点与最近的40个点的距离\ndistances = np.linalg.norm(points1[:, np.newaxis] - nearest_points2, axis=2)\n\n\n\n这个代码首先将points2划分成多个块，并分别计算每个点在points1中与所有块中的点的距离，然后找到每个点距离最近的40个点的索引。最后，它计算最近的40个点在points2中的坐标，并计算表1每个点与最近的40个点的距离。", "Konwledge_Point": "应对NP完全问题", "Question": "关于#Python#的问题，如何解决？\n问题遇到的现象和发生背景\n\n\n想利用Python计算两个array之间的距离\npoints1是表1的array，points2是表2的array\n\n\npoints1\n \n=\n dfA.values\n\npoints2\n \n=\n dfe1.values\n\n\n\n我的解答思路和尝试过的方法\n\n\n使用过分块计算和np.sqrt计算，但是不支持float类型的计算，出错了\n\n\n我想要达到的结果\n\n\n下面两个表每一列代表坐标x,y,z，想计算表1和表2两两点的距离并筛选出表1每个点与表2中距离最近的40个点\n注：每个表基本上都有几十万行，所以计算速度是个问题，而且每一列的数据类型都是float，最好能够使用分块计算\n表1\n\n\n表2", "Tag": "算法分析"}
{"Answer": "按这个形式写， 应该能达到你的要求\r\n```\r\ncheck_flag = 'A'\r\nfor i in range(数据总量)\r\n    if check_flag == 'A':\r\n\t\t    if 满足条件A:\r\n\t\t\t\t    check_flag = 'B'\r\n\t\t\t\t\t\tprint('FIND A index', i )\r\n\t\t\t\t    #你要做的操作\r\n    else:\r\n\t\t    if 满足条件B:\r\n\t\t\t\t    check_flag = 'A'\r\n\t\t\t\t\t\tprint('FIND B index', i )\r\n\t\t\t\t    #你要做的操作\t\t    \r\n\r\n\r\n\r\n```", "Konwledge_Point": "应对NP完全问题", "Question": "关于pandas,python,循环问题？\n希望实现这样一种效果：\n\n从第一行开始，找到满足if条件A的某行（比如index是3），然后从index是3这行向下，找到满足if条件B的某行比如index是25，接着从index是25这行这行向下，找到满足if条件A的某行，如此循环下去。\n\n\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(20200324)\nnum = np.random.rand(400).round(2)\ndata = np.array(num).reshape(100, 4)\ndf = pd.DataFrame(data, columns=list('ABCD'))\n\ntemp_i = 0\nfor i in range(temp_i, df.shape[0]):\n    if df.at[i, 'A'] > df.at[i, 'D']:\n        df.at[i, 'Signal'] = 'num_up'\n        for i2 in range(i+1, df.shape[0]):\n            if df.at[i2, 'A'] < df.at[i2, 'D'] * 0.5:\n                df.at[i2, 'Signal_2'] = 'num_down'\n                temp_i = i2\n                break\n        break\n\n\nprint df\n\n\n\n\n目前只能写出一次的循环，不知道是否有办法可以把现在的代码循环下去，目前我只能用野办法就是把，循环一次的复制了很多很多次，想求问各位高手这种循环怎么去写呢？\n\n\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(20200324)\nnum = np.random.rand(400).round(2)\ndata = np.array(num).reshape(100, 4)\ndf = pd.DataFrame(data, columns=list('ABCD'))\n\ntemp_i = 0\nfor i in range(temp_i, df.shape[0]):\n    if df.at[i, 'A'] > df.at[i, 'D']:\n        df.at[i, 'Signal'] = 'num_up'\n        for i2 in range(i+1, df.shape[0]):\n            if df.at[i2, 'A'] < df.at[i2, 'D'] * 0.5:\n                df.at[i2, 'Signal_2'] = 'num_down'\n                temp_i = i2\n                break\n        break\n\nfor i in range(temp_i, df.shape[0]):\n    if df.at[i, 'A'] > df.at[i, 'D']:\n        df.at[i, 'Signal'] = 'num_up'\n        for i2 in range(i+1, df.shape[0]):\n            if df.at[i2, 'A'] < df.at[i2, 'D'] * 0.5:\n                df.at[i2, 'Signal_2'] = 'num_down'\n                temp_i = i2\n                break\n        break\n\nfor i in range(temp_i, df.shape[0]):\n    if df.at[i, 'A'] > df.at[i, 'D']:\n        df.at[i, 'Signal'] = 'num_up'\n        for i2 in range(i+1, df.shape[0]):\n            if df.at[i2, 'A'] < df.at[i2, 'D'] * 0.5:\n                df.at[i2, 'Signal_2'] = 'num_down'\n                temp_i = i2\n                break\n        break\n\n\n\nprint df\n", "Tag": "算法分析"}
{"Answer": "axis=-1，其实也就等于axis=2。因为这是个三维矩阵，所以axis可能的取值为0,1,2，所以最后一个就是2。你可以自己试试看两个取值结果是否相同。望采纳哦！！！", "Konwledge_Point": "应对NP完全问题", "Question": "numpy基础问题数组拼接\ntimes=np.concatenate((time0,time1),-1)\n这个是数组拼接，那么-1是按行还是列拼接啊？我没有查到", "Tag": "算法分析"}
{"Answer": "该回答引用gpt：这是一个使用numpy库中的where函数的语句，其中X是一个二维的numpy数组，Y是一个一维的numpy数组，w是一个浮点数，ch_i和c是两个整数。\n这句语句的作用是：如果X数组的第i列的元素等于ch_i，且Y数组的元素等于c，那么将w赋值给新的数组的相应位置，否则赋值为0。其中，新的数组是通过where函数生成的。\n具体来说，where函数的用法是：np.where(condition, x, y)，其中condition是一个bool型的numpy数组，x和y是两个相同形状的numpy数组，如果condition中对应的元素为True，则新的数组对应位置的元素为x中对应位置的元素，否则为y中对应位置的元素。\n在这句语句中，condition的值是(X[:,i] == ch_i) * (Y == c)，即X数组的第i列的元素等于ch_i且Y数组的元素等于c的位置为True，其余位置为False。因此，新的数组中只有这些True对应的位置的元素才会被赋值为w，其余位置的元素为0。", "Konwledge_Point": "应对NP完全问题", "Question": "python 统计学习方法 分类问题\nnp.where((X[:,i] == ch_i) * (Y == c), w, 0.0)这句是什么意思呀？\n\n", "Tag": "算法分析"}
{"Answer": "因为linspace方法的第三个参数表示要生成的个数，应该是一个整数，而你的是小数。所以报错。建议使用：np.arange(0,3.6,0.7)望采纳下。谢谢", "Konwledge_Point": "应对NP完全问题", "Question": "关于#numpy#的问题，如何解决？\nnumpy模块\n在3.5公里的路段每隔0.7公里设置一路桩，创建路桩的位置数组。\n\n\nimport numpy \nas\n np\narr3=np.linspace(\n0\n,\n3.5\n,\n0.7\n)\n\nprint\n(arr3)\n\nTypeError                                 Traceback (most recent \ncall\n \nlast\n)\n~\\AppData\\Local\\Temp\\ipykernel_58568\\\n2770250342\n.\npy\n in \n\n\n----> \n1\n arr3=np.linspace(\n0\n,\n3.5\n,\n0.7\n)\n      \n2\n \nprint\n(arr3)\n\n<__array_function__ internals> in linspace(*\nargs\n, **kwargs)\n\nD:\\Anoconda3\\lib\\site-packages\\numpy\\core\\function_base.\npy\n in linspace(start, \nstop\n, num, endpoint, retstep, dtype, axis)\n    \n118\n \n    \n119\n     \n\"\"\n\"\n\n--> \n120\n     num = operator.\nindex\n(num)\n    \n121\n     \nif\n num < \n0\n:\n    \n122\n         raise ValueError(\n\"Number of samples, %s, must be non-negative.\"\n % num)\n\nTypeError: \n'float'\n object cannot \nbe\n interpreted \nas\n \nan\n integer\n", "Tag": "算法分析"}
{"Answer": "text是个str呀，你不要给它赋值一个float", "Konwledge_Point": "应对NP完全问题", "Question": "txt数据输出给docx出现问题\n我把txt文件数据输入Word表格中出现这种情况，还会出现名称可以写入表格，但是数据写入不了，请问该怎么改？\n这是部分数据\n205c    -4248.7840194\n127n    11.095004310167\n23a    11.2456067097867\n\n\nimport numpy as \nnp\n\nimport docx\nfrom docx import Document\nfile = 's.energy.txt'\nf = open(file)\nlines = f.readlines()\nlist1 = []\nlist2 = []\n\nline_data = []\n\nfor\n line \nin\n lines:\n    a = line.\nsplit\n()\n    line_data.\nappend\n(a)\ndata = \nnp\n.\narray\n(line_data)\nneed_data = data[\n0\n:\n14\n,\n1\n]\nneed_name = data[\n0\n:\n14\n,\n0\n]\nf.\nclose\n()\n\nfor\n j \nin\n need_name:\n    list1.\nappend\n(j)\n\nfor\n i \nin\n need_data:\n\n    list2.\nappend\n(i)\n\n\nfile_name\n = 'ca.docx'\ndoc = docx.Document(\nfile_name\n)\ntable = doc.add_table(rows=len(need_name)*\n3\n, cols=\n4\n)\ncells = table.rows[\n0\n].cells\n\n\n\n\n\n\nfor\n \ncol\n \nin\n \nrange\n(\n4\n):\n    \nfor\n \nrow\n \nin\n \nrange\n(len(need_name) * \n3\n):\n        \nfor\n x \nin\n list1:\n            \nfor\n y \nin\n list2:\n                \nif\n (\nrow\n+\n1\n)\n%\n3\n==\n0\n:\n                    a=\nrow\n+\n1\n\n                    table.cell(\nrow\n,\ncol\n).text = \nnp\n.\nround\n(\nfloat\n(y),\n2\n)#这行出现问题，其他正常运行，有时候不报错但是文件里没有数据\n                    \nif\n \nrow\n==a-\n1\n:\n                        table.cell(\nrow\n,\ncol\n).text = x\ndoc.\nsave\n('\n1\n'+\nfile_name\n)\n\n\n\n\n```", "Tag": "算法分析"}
{"Answer": "clf = self.estimator.fit(np.array(x_train),np.array(y_train))\n\n这个返回的是个History类型的对象而History里面没有predict这个方法你是不是搞错了", "Konwledge_Point": "应对NP完全问题", "Question": "AttributeError: 'History' object has no attribute 'predict'           做集成时碰到这个问题怎么办呢？\n问题遇到的现象和发生背景\n\n\n做集成算法时出现了下列错误，求帮忙\n\n\n问题相关代码，请勿粘贴截图\n\n\ndef \nTrainPredict(\nself\n,\nx_train\n,\ny_train\n,\nx_test\n)\n:          #训练基础模型，并返回模型预测结果\n    clf = self.estimator.fit(np.\narray\n(x_train),np.\narray\n(y_train))\n    result = clf.predict(x_test)\n    return result\n\ndef \nBagging_clf(\nself\n, \nx_train\n, \nx_test\n, \ny_train\n, \ny_test\n, \nsample_type\n=\n\"RepetitionRandomSampling\"\n)\n:\n    print(\n\"self.Bagging single_basemodel\"\n)\n    result = \n()\n\n\n    \nif\n sample_type\n == \n\"RepetitionRandomSampling\"\n:\n        print(\n\"选择的采样方法：\"\n, sample_type)\n        sample_function = self.RepetitionRandomSampling\n    elif sample_type\n == \n\"UnderSampling\"\n:\n        print(\n\"选择的采样方法：\"\n, sample_type)\n        sample_function = self.UnderSampling\n        print(\n\"采样率\"\n, self.rate)\n    elif sample_type\n == \n\"IF_SubSample\"\n:\n        print(\n\"选择的采样方法：\"\n, sample_type)\n        sample_function = self.IF_SubSample\n        print(\n\"采样率\"\n, (\n1.0\n - self.rate))\n    print(sample\n_function(\ntrain\n, \nlen\n(\ntrain\n)\n))\n    \nfor\n i \nin\n range(self.n_estimators):\n        sample = sample\n_function(\ntrain\n, \nlen\n(\ntrain\n)\n)  # 构建数据集\n        x_train = np.\narray\n(sample)\n[:, :, \n0\n:-\n1\n]\n\n        y_train = np.\narray\n(sample)\n[:, :, -\n1\n]\n\n        \nlist\n(result).append(self.\nTrainPredict(\nx_train\n, \ny_train\n, \nx_test\n)\n)  # 训练模型 返回每个模型的输出\n    print(np.\narray\n(result))\n    score = self.\nVoting(\nresult\n)\n\n    recall, precision = self.\nMetrics(\nscore\n, \ny_test\n)\n\n    return recall, precision\n\nrecall_self,precision_self = clf_self.\nBagging_clf(\nx_train\n, \nx_test\n, \ny_train\n, \ny_test\n)\n\nprint(\n\"recall:\"\n,\n'\\n'\n,recall_self)\nprint(\n\"precision\"\n,\n'\\n'\n,precision_self)\n\n\n\n运行结果及报错内容\n\n\nTraceback (most recent call last):\n  File \"D:\\pycharm\\practice\\待改进(smote与模仿github中的自写bagging).py\", line 236, in \n    recall_self,precision_self = clf_self.Bagging_clf(x_train, x_test, y_train, y_test)\n  File \"D:\\pycharm\\practice\\待改进(smote与模仿github中的自写bagging).py\", line 170, in Bagging_clf\n    list(result).append(self.TrainPredict(x_train, y_train, x_test))  # 训练模型 返回每个模型的输出\n  File \"D:\\pycharm\\practice\\待改进(smote与模仿github中的自写bagging).py\", line 133, in TrainPredict\n    result = clf.predict(x_test)\nAttributeError: 'History' object has no attribute 'predict'\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "这是numpy自动导入了numbers模块导致的, 而这个numbers模块可能是你自己写的, 文件是numbers.py, 而且在sys.path目录下, 会被numpy自动检测到。先在python脚本的目录下找numbers.py, 改成其他名字。如果找不到, 运行这行代码: \nimport sys\nprint(sys.path)\n\n然后把各个sys.path下的numbers.py改成其他名字, 即可。", "Konwledge_Point": "应对NP完全问题", "Question": "numpy相关的运行问题\n刚开始学习，现在学习numpy模块相关的内容，根据网上公开课程学习，\n\n\n用pip install numpy，显示Requirement already satisfied: numpy in c:\\users\\lsl\\appdata\\roaming\\python\\python38\\site-packages (1.23.0)，这不是已经安装成功的意思吗？\n\n\n在pycharm 写了如下几行简单的代码：\n\n\nimport numpy as np\narr1 = np.array([11,22,33,22,33,44,33,44,55])\nprint(arr1) \n\n\n运行结果：\n\n\n1\n\n\n1000000\n\n\n500000500000\n\nTraceback (most recent call last):\n  省略\nAttributeError: \nmodule\n \n'numbers'\n has no attribute \n'Integral'\n\n\n######到底哪里出错了???\n\n", "Tag": "算法分析"}
{"Answer": "W要大写，是destroyAllWindows，不是destroyAllwindows", "Konwledge_Point": "应对NP完全问题", "Question": "python-opencvan使用问题\n问题遇到的现象和发生背景\n\n\n用的python3.9,anaconda3,opencv4.6.0.66版本，opencv-contrib-python和opencv都安装成功，代码可以跑但是报错\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nimport numpy as np\nimg = cv.imread(\"1.jpg\")\nprint(img)\ncv.imshow(\"img\", img)\ncv.waitKey(10000)\ncv.destroyAllwindows()\nprint(img.shape)\n\n\n运行结果及报错内容\n\n\n在 '\ninit\n.py' 中找不到引用 'imread'\n在 '\ninit\n.py' 中找不到引用 'imshow'\n在 '\ninit\n.py' 中找不到引用 'waitKey'\n在 '\ninit\n.py' 中找不到引用 'destroyAllwindows'\n运行结果\nAttributeError: module 'cv2' has no attribute 'destroyAllwindows'\n\n\n我的解答思路和尝试过的方法\n\n\n有反复重新安装过但是没有用\n\n\n我想要达到的结果\n\n\n不报错", "Tag": "算法分析"}
{"Answer": "img_shift = np.float32([（0，1，0）,（1,0，h_move）])\n", "Konwledge_Point": "应对NP完全问题", "Question": "使用opencv2的warpAffine使用问题\n在使用opencv2的warpAffine函数对照片进行平移处理的时候宽高颠倒\n\n\ndef Image_traslation(img):\n    h_move = \nrandom\n.randint(\n0\n, \n200\n)\n    rows, cols = img.shape[:\n2\n]\n    \nprint\n(rows, cols)\n    img_shift = np.float32(\n[[0, 1, 0], [1, 0, h_move]]\n)\n    img_new = cv2.warpAffine(img, img_shift, (cols, rows))\n    \nreturn\n img_new\n\n\n\n请问该如何解决这个问题", "Tag": "算法分析"}
{"Answer": "可使用set_xlim来设置：ax.set_xlim(0, df.shape[0])", "Konwledge_Point": "应对NP完全问题", "Question": "python 绘图的问题：去掉两边的空白部分。\n问题遇到的现象和发生背景\n\n\n\n\n问题相关代码，请勿粘贴截图\n\n\n蓝色部分，我不需要，请问如何去掉。让数据显示的紧凑点。\n简单来说：有多余的空白部分。我想让数据与纵轴相交，而不是保持一定的距离。\n代码如下：\n\n\n    plt.style.use(\n'dark_background'\n)\n    \ndf\n=日线接口(开始日期='20210505')\n    \nfig\n=plt.figure(figsize=(12,7))\n    \nax\n=fig.add_subplot(111)\n    显示中文()\n    #开始绘制折线图\n    # 绘制K线金融图()\n    ax.plot(df[\n'High'\n].values,\nlabel\n=\n'最高价'\n)\n    plt.legend(\nloc\n=\n'best'\n)\n    #设置日期\n    ax.set_xticks(np.arange(0,df.shape[0],15))\n    plt.show()\n\n", "Tag": "算法分析"}
{"Answer": "dd=ts.get_k_data('002466',start='2010-01-01',end='2021-07-18')['close']dd.sort_index(inplace=True)可以看出dd不是list类型 list没有.sort_index", "Konwledge_Point": "应对NP完全问题", "Question": "关于python的2个小问题\n\n问题\n1\n：\nimport tushare as ts\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.simplefilter(\n\"ignore\"\n)\nfrom matplotlib import pyplot as plt\n\nimport seaborn\n%matplotlib inline\ndd=ts.get_k_data(\n'002466'\n,start=\n'2010-01-01'\n,end=\n'2021-07-18'\n)[\n'close'\n]\ndd.sort_index(inplace=True)\ndd_returns=dd.pct_change()\ndd_returns.dropna(inplace=True)\nd_mu_1=dd_returns.mean()\nd_sigma_1=dd_returns.std()\n\n# VaR_1=d_mu_1-2.33*d_sigma_1\n\nds_0=dd[-\n1\n]\n\n为什么最后一行位置函数使用-\n1\n不行\n![img](https:\n//img\n-mid.csdnimg.cn\n/release/\nstatic\n/image/mi\nd\n/ask/\n438498095626177\n.png)\n\n改成\n1\n又可以\n![img](https:\n//img\n-mid.csdnimg.cn\n/release/\nstatic\n/image/mi\nd\n/ask/\n445589095626179\n.png)\n\n问题\n2\n：\ndef ZQ(ds_0,d_mu_1,d_sigma_1,T,n):\n    delta_1=T/n\n    sim_price=[ds_0]\n    \nfor\n i \nin\n range(n):\n        start_price=sim_price[i]\n        epsilon=np.random.normal()\n        end_price=start_price+start_price*(d_mu_1*delta_1+d_sigma_1*epsilon*np.sqrt(delta_1))                                  \n        end_price = max(\n0\n,end_price)                               \n        sim_price.append(end_price)   \n                                             \n    return sim_price\n\nzq1=ZQ(ds_0,d_mu_1,d_sigma_1,\n1000\n,\n1000\n)\n\n# zq1=np.array(zq1)\n\nzq1=pd.Series(zq1)\nzq2=[ZQ(ds_0,d_mu_1,d_sigma_1,\n1000\n,\n1000\n) \nfor\n iu \nin\n range(\n100\n)]\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\nimport matplotlib.cm as cmx\nimport numpy as np\nimport matplotlib\nimport seaborn as sns\n\n# 设置图像风格\n\nsns.set_style(\n'whitegrid'\n)\n\n# 设置绘图参数\n\n\n# for ii in range(len(zq2)):\n\nfig = plt.figure(figsize=(\n9\n,\n9\n))\nax = fig.add_subplot(\n111\n)\nlines = []\n\nfor\n ii \nin\n range(len(zq2)):\n    line = zq2[ii]\n    retLine, = ax.plot(line,\n                       color=\n'cornflowerblue'\n,\n                       alpha=\n0.8\n,\n                       linewidth=\n0.6\n)\n\nplt.xlabel(\n'stock_price'\n, fontsize=\n'large'\n)\nplt.ylabel(\n'steps'\n, fontsize=\n'large'\n)\nplt.title(\n'Monte Carlo Simulation'\n, fontsize=\n'large'\n)\nplt.show()        \nplt.plot(zq1)\n\n为什么在定义zq2时引用zq1和不引用zq1画出的效果不同？\nzq2=[zq1 \nfor\n iu \nin\n range(\n100\n)]\n![img](https:\n//img\n-mid.csdnimg.cn\n/release/\nstatic\n/image/mi\nd\n/ask/\n818303195626188\n.png)\n\nzq2=[ZQ(ds_0,d_mu_1,d_sigma_1,\n1000\n,\n1000\n) \nfor\n iu \nin\n range(\n100\n)]\n![img](https:\n//img\n-mid.csdnimg.cn\n/release/\nstatic\n/image/mi\nd\n/ask/\n853014195626184\n.png)\n\n\n\n", "Tag": "算法分析"}
{"Answer": "原因： markers生成的是numpy数组，不是List，需要先转换为list才能使用里面的元素做为index", "Konwledge_Point": "应对NP完全问题", "Question": "python3.8.8运行遇到问题\n代码：\n\n\n\n\n\nfor cnt, x in enumerate(data):\n    # getting the winner\n    w = som.winner(x)\n    # place a marker on the winning position for the sample xx\n    wx, wy = som.convert_map_to_euclidean(w) \n    wy = wy * np.sqrt(3) / 2\n    plt.plot(wx, wy, \n             markers[t[cnt]-1], \n             markerfacecolor='None',\n             markeredgecolor=colors[t[cnt]-1], \n             markersize=12, \n             markeredgewidth=2)\n\nxrange = np.arange(weights.shape[0])\nyrange = np.arange(weights.shape[1])\nplt.xticks(xrange-.5, xrange)\nplt.yticks(yrange * np.sqrt(3) / 2, yrange)\n\n\n\n在markers[t[cnt]-1]这一行报错，不知道该如何解决。\n\n\n\n\n\nTraceback (most recent call last):\n  File \"naqu.py\", line 67, in \n    markers[t[cnt]-1],\nTypeError: list indices must be integers or slices, not numpy.float64\n\n\n\n\n\n\n\n\n\n ", "Tag": "算法分析"}
{"Answer": "a要是numpy.uint8类型，还有一件事，你用这个矩阵的，看不出效果的..........", "Konwledge_Point": "应对NP完全问题", "Question": "在用opencv中resize函数时遇到的问题。\nimport cv2\nimport numpy as np\n\na = np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])\norign = a.copy()\n\ncv_r = cv2.resize(a, None,fx = 0.5,fy = 0.5,interpolation=cv2.INTER_LINEAR)\n\n\n\n\n为了测试下resize的作用，自己敲了上述代码，会出现下面的错误：\n\n\n\nerror: OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:3863: error: (-215:Assertion failed) func != 0 in function 'cv::hal::resize'\n\n\n\n\n望高手解答", "Tag": "算法分析"}
{"Answer": "目测是一种切片操作的简写，‘...’是指出指定之外的所有纬度的所有索引，如[....,1]是指选取最后一维的索引为1，其他纬度 不加限定，\r\n在你的实例中I[...,1]和I[:,:,1]等价", "Konwledge_Point": "应对NP完全问题", "Question": "请问[...,0],[...,1]是什么意思？\nimport numpy as np\n\nI=np.random.randint(0,2,(16,16,3)).astype(np.ubyte)\n\nF=I[...,0]*(256*256)+I[...,1]*256+I[...,2]\n\nn=len(np.unique(F))\n\nprint(n)\n\n\n\n第3行的代码没看懂，向大佬们请教一下", "Tag": "算法分析"}
{"Answer": "尝试方法2：Matplotlib中文乱码的两种详细解决方案，https://www.jb51.net/article/255051.htm%EF%BC%8C%E7%BB%93%E5%90%88%E6%96%B9%E6%B3%951%E5%92%8C2%EF%BC%8C%E7%94%A8%E6%96%B9%E6%B3%952%E6%B5%8B%E8%AF%95%E6%97%B6%E6%9C%89%E6%98%BE%E7%A4%BA%E9%94%99%E8%AF%AF%EF%BC%9A下午打开Spyder ，matplotlib库无法正常使用，提示:FileNotFoundError: [Errno 2] No such file or directory: 'D:\\programFiles\\ANACONDA\\lib\\site-packages\\matplotlib\\mpl-data\\matplotlibrc'。所以配置文件不要乱改啊，改的话也要备份啊，所以参考：anaconda卸载重装matplotlib，进行matplotlib的重装。已解决：中文无法正常显示，是因为书写错误，不是 fontpropertise  是fontproperties 啊！！！", "Konwledge_Point": "应对NP完全问题", "Question": "用fontpropertise中文乱码\nspyder中用pyploth画图，图像显示不出来；用fontpropertise中文乱码，小方框。\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\na=np.arange(0.0,5.0,0.02)\n\n\nplt.xlabel('横轴：时间', fontproperties='SimHei',fontsize=20)\nplt.ylabel('纵轴：振幅', fontpropertise='SimSum',fontsize=20)\nplt.plot(a, np.cos(2\nnp.pi\na),'r--')\nplt.show() \n\n\n\n\n是不是我电脑的问题啊，因为昨天可以显示中文，今天运行代码就开始出问题乱码；也尝试用这个方法：matplotlib中文乱码的两种解决方案，但是按照步骤做到最后一步的时候，又发生错误：\n\n\n为什么昨天运行好好地，今天就中文乱码，图像也显示不出来，难道是因为我reset Spyder吗？", "Tag": "算法分析"}
{"Answer": "你只要保证seed值是固定的，那概率肯定是相同的呀", "Konwledge_Point": "应对NP完全问题", "Question": "python怎么生成相同概率的随机数\n\n\n\n\nnp\n.\nrandom\n.seed(\n0\n)\nx1 = \nnp\n.\nrandom\n.randint(\n10\n,size=\n100\n)\nx1\n", "Tag": "算法分析"}
{"Answer": "可以看XY的值，XY的范围都是-1到1，每次跨度为0.2，所以XY都是10个元素的数组\n\n嵌套for循环就是输出一个10*10个二维数组,因为输出的索引都是i，而且X Y是相同的，所以输出的值一直是一样的", "Konwledge_Point": "应对NP完全问题", "Question": "python中for套for的用法。\nimport numpy as np\nX = np.arange(-1.0, 1.0, 0.2)\nY = np.arange(-1.0, 1.0, 0.2)\n\n\nfor i in range(10):\n    for j in range(10):\n        s = np.array([X[i], Y[i]])\n        print(s)\n\n\n对这个循环的输出该怎么解释？", "Tag": "算法分析"}
{"Answer": "文件发我？", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的问题：import matplotlib.pyplot as plt\n\n \n\nimport\n matplotlib\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n numpy \nas\n np\n \n\ndef\n \nmain\n():\n    matplotlib.rcParams[\n'font.family'\n] = \n'SimHei'\n\n    stuScore = np.loadtxt(\n'student_score.csv'\n, delimiter=\n','\n)  \n# 读入成绩文件,返回数组\n\n    sumEach = np.\nsum\n(stuScore[:, \n1\n:], axis=\n1\n)  \n# 返回每个学生3门课程总分\n\n    avgEach = np.average(stuScore[:, \n1\n:], axis=\n0\n)  \n# 返回每个学生每门课程平均分\n\n    \n# 取出各科成绩\n\n    mathScore = stuScore[:, \n1\n]\n    engScore = stuScore[:, \n2\n]\n    pythonScore = stuScore[:, \n3\n]\n    \n# Performanceanalysis(avgEach, stuScore, sumEach)\n\n    \nwhile\n \nTrue\n:\n        \nprint\n(\n\"\"\"成绩分析与可视化系统  \n 1: 基本信息显示      \n 2: 成绩分析          \n 3: 可视化         \n 4: 退出系统\"\"\"\n)\n        operation = \ninput\n(\n\"请输入你的操作\"\n)\n        \nif\n operation.isdigit():\n            operation = \nint\n(operation)\n            \nif\n operation == \n1\n:\n                \nprint\n(\n\" 学号  高数  英语  python\"\n)\n                \nfor\n i \nin\n stuScore:\n                    \nprint\n(\nf\"\n{\nint\n(i[\n0\n])}\n \n{i[\n1\n]}\n \n{i[\n2\n]}\n \n{i[\n3\n]}\n\"\n)\n            \nelif\n operation == \n2\n:\n                Performanceanalysis(avgEach, stuScore, sumEach)\n            \nelif\n operation == \n3\n:\n                \n# name= input(\"请输入课程名\")\n\n                \n# if name=='xxx':\n\n                \n# 由于不清楚你的课程名是啥,你这里自己填 if elif else结构就可以\n\n                Highnumberhistogram(mathScore)\n                Englishhistogram(engScore)\n                Scorehistogram(pythonScore)\n            \nelif\n operation == \n4\n:\n                \nimport\n sys\n                sys.exit(\n0\n)\n            \nelse\n:\n                \nprint\n(\n\"输入错误,请重新输入\"\n)\n \n\ndef\n \nPerformanceanalysis\n(\navgEach, stuScore, sumEach\n):\n    \n# 返回最高分和最低分\n\n    maxMath = np.\nmax\n(stuScore[:, \n1\n])\n    maxEng = np.\nmax\n(stuScore[:, \n2\n])\n    maxPython = np.\nmax\n(stuScore[:, \n3\n])\n    minMath = np.\nmax\n(stuScore[:, \n1\n])\n    minEng = np.\nmax\n(stuScore[:, \n2\n])\n    minPython = np.\nmax\n(stuScore[:, \n3\n])\n    \nprint\n(\n\"个人总分情况是：\"\n)\n    \nprint\n(sumEach)\n    \nprint\n(\n\"个人平均分情况是：\"\n)\n    \nprint\n(avgEach)\n    \nprint\n(\n\"班级每门课程最高分：\"\n)\n    \nprint\n(maxMath, maxEng, maxPython)\n    \nprint\n(\n\"班级每门课程最低分：\"\n)\n    \nprint\n(minMath, minEng, minPython)\n \n\ndef\n \nHighnumberhistogram\n(\nmathScore\n):\n    \n# 绘制高数直方图\n\n    plt.suptitle(\n\"成绩分布直方图\"\n)\n    plt.subplot(\n3\n, \n1\n, \n1\n)\n    plt.hist(mathScore, bins=\n10\n, \nrange\n=(\n0\n, \n100\n), color=\n'red'\n)  \n# 0-100分,分成10段\n\n    plt.xlabel(\n\"高数成绩分数段\"\n)  \n# 设置x轴标签\n\n    plt.ylabel(\n\"人数\"\n)  \n# 设置y轴标签\n\n    plt.xlim(\n0\n, \n100\n)  \n# 设置x轴区间\n\n    plt.xticks([\n0\n, \n10\n, \n20\n, \n30\n, \n40\n, \n50\n, \n60\n, \n70\n, \n80\n, \n90\n, \n100\n])  \n# 设置x轴刻度\n\n    plt.yticks([\n0\n, \n10\n, \n20\n, \n30\n, \n40\n, \n50\n, \n60\n, \n70\n, \n80\n, \n90\n, \n100\n])  \n# 设置y轴刻度\n\n    \n# plt.grid()\n\n    plt.show()\n \n\ndef\n \nEnglishhistogram\n(\nengScore\n):\n    \n# 绘制英语直方图\n\n    plt.subplot(\n3\n, \n1\n, \n2\n)\n    plt.hist(engScore, bins=\n10\n, \nrange\n=(\n0\n, \n100\n), color=\n'blue'\n)  \n# 0-100分,分成10段\n\n    plt.xlabel(\n\"英语成绩分数段\"\n)  \n# 设置x轴标签\n\n    plt.ylabel(\n\"人数\"\n)  \n# 设置y轴标签\n\n    plt.xlim(\n0\n, \n10\n)  \n# 设置x轴区间\n\n    plt.xticks([\n0\n, \n10\n, \n20\n, \n30\n, \n40\n, \n50\n, \n60\n, \n70\n, \n80\n, \n90\n, \n100\n])  \n# 设置x轴刻度\n\n    plt.yticks([\n0\n, \n10\n, \n20\n, \n30\n, \n40\n, \n50\n, \n60\n, \n70\n, \n80\n, \n90\n, \n100\n])  \n# 设置y轴刻度\n\n    \n# plt.grid()\n\n    plt.show()\n \n\ndef\n \nScorehistogram\n(\npythonScore\n):\n    \n# 绘制python直方图\n\n    plt.suptitle(\n\"成绩分布直方图\"\n)\n    plt.subplot(\n3\n, \n1\n, \n3\n)\n    plt.hist(pythonScore, bins=\n10\n, \nrange\n=(\n0\n, \n100\n), color=\n'green'\n)  \n# 0-100分,分成10段\n\n    plt.xlabel(\n\"Python成绩分数段\"\n)  \n# 设置x轴标签\n\n    plt.ylabel(\n\"人数\"\n)  \n# 设置y轴标签\n\n    plt.xlim(\n0\n, \n100\n)  \n# 设置x轴区间\n\n    plt.xticks([\n0\n, \n10\n, \n20\n, \n30\n, \n40\n, \n50\n, \n60\n, \n70\n, \n80\n, \n90\n, \n100\n])  \n# 设置x轴刻度\n\n    plt.yticks([\n0\n, \n10\n, \n20\n, \n30\n, \n40\n, \n50\n, \n60\n, \n70\n, \n80\n, \n90\n, \n100\n])  \n# 设置y轴刻度\n\n    \n# plt.grid()\n\n    plt.show()\n \n\nif\n __name__ == \n'__main__'\n:\n    main()\n\n\n\n\n\n\n在这个python成绩后面画一个python成绩的饼图", "Tag": "算法分析"}
{"Answer": "\nimport pandas as pd\nimport numpy as np\nimport math\ndf=pd.DataFrame({'a':[1,np.nan],\n'b':['x',np.nan]})\n\ns1=set(df['a'])\ns2=set(df['b'])\ns1 = set([x for x in s1 if not math.isnan(x)])\nprint(s1)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python集合空值删除\n如何把python集合中的空值删除\n\n\nimport pandas as pd\nimport numpy as np\ndf=pd.DataFrame({'a':[1,np.nan],\n                 'b':['x',np.nan]})\n\n\ns1=set(df['a'])\ns2=set(df['b'])\n\n\ns1.remove(np.nan)\ns2.remove(np.nan)\n\n\nprint(s1,s2)\n\n\n集合s1 的空值跟整数一列,运行remove(np.nan) 报错,但是s2可以正常删除\n\n\n我已经试过几种方法,如remove(None)等,都无法解决删除s1中的空值\n\n\n我想要达到的结果: 不提前用df.fillna()的方法, 直接删除s1中的空值", "Tag": "算法分析"}
{"Answer": "open()和with open() 语句都是打开文件。需要的参数都是文件路径你应该将 path = 'C:\\Users\\Administrator\\Desktop\\实训\\data\\anhui.txt'", "Konwledge_Point": "应对NP完全问题", "Question": "TypeError: expected str, bytes or os.PathLike object, not TextIOWrapper\n读取txt文件时报错\n\n\nimport\n csv\n\nimport\n io\n\nimport\n numpy \nas\n np\n\nfrom\n numpy.matlib \nimport\n repmat\n\npath\n=\nopen\n(\n'C:\\\\Users\\\\Administrator\\\\Desktop\\\\实训\\\\data\\\\anhui.txt'\n,\n'r'\n,encoding=\n'utf-8'\n)\n\n#a=np.loadtxt('C:\\\\Users\\\\Administrator\\\\Desktop\\\\实训\\\\data\\\\anhui.txt',encoding='utf-8')\n\n\n#b=np.reshape(a,(17,10))   \n\n\nwith\n \nopen\n(\npath\n,\n'r'\n,encoding=\n'utf-8'\n)\nas\n f1:\n    data=f1.\nread\n()\nprint(data)\n", "Tag": "算法分析"}
{"Answer": "out = x.copy这一句copy后面少了小括号，应该是out = x.copy()", "Konwledge_Point": "应对NP完全问题", "Question": "使用Python建立ReLU层时遇到的问题\n在学习Python建立ReLU层时，看到书上的一串代码，大意是设置一个mask作为判断输入的numpy.array中的值是否大于0的标志，若大于0则mask为False，小于等于0则mask为True，并将所有mask为True的输出值置0。代码如下：\n\n\nimport numpy \nas\n np\n\n\n\nclass\n \nReLU\n:\n\n    \ndef\n \n__init__\n(\nself\n):\n        \nself\n.mask = None\n\n    \ndef\n \nforward\n(\nself\n,  x):\n        \nself\n.mask = (x <= \n0\n)\n        \nout\n = x.copy\n        \nout\n[\nself\n.mask] = \n0\n\n        \nreturn\n \nout\n\n\n    \ndef\n \nbackward\n(\nself\n, dout):\n        dout[\nself\n.mask] = \n0\n\n        \nreturn\n dout\n\nre = ReLU()\nx = np.array([[-\n1\n, \n1\n], [\n3\n, -\n2\n]])\noutput = re.forward(x)\nprint(output)\n\n\n\n但是运行之后的报错信息如下：\n\n\nTraceback (most recent call last):\n  File \n\"路径\"\n, line \n20\n, \nin\n \n    output = re.forward(x)\n  File \n\"路径\"\n, line \n11\n, \nin\n forward\n    out[self.mask] = \n0\n\nTypeError: \n'builtin_function_or_method'\n object does not support item assignment\n\nProcess finished with \nexit\n code \n1\n\n\n\n\n\n请教一下如何解决问题", "Tag": "算法分析"}
{"Answer": "\nThe usual approach here is to write a predictive parser. For you, this could mean using regular expressions to match either a noun, verb or predicate and then deciding what production to use. You are correct that parsing a grammar requires the computational power of a push-down automata (ie more than what a regular expression alone can achieve). Simulating a push-down automaton is one approach and is what parser generators like yacc/bison often do. For a small grammar like that though, you can use the call stack implicitly. \n", "Konwledge_Point": "应对NP完全问题", "Question": "如何使用PHP preg_match_all实现简单的CFG解析器？\n\n\n\nI am using preg_match_all to make a simple parser. Note that since it will parse only few sentences, the performance does not matter. Would it be possible to make a parser which parse through below Context free grammer?\n\n\n\nS -> NP VP\nPP -> P NP\nNP -> 'the' N | N PP | 'the' N PP\nVP -> V NP | V PP | V NP PP\nN -> 'cat'\nN -> 'dog'\nN -> 'rug'\nV -> 'chased'\nV -> 'sat'\nP -> 'in'\nP -> 'on'\n\n\n\n\nThe problem here that I couldn't resolve was loop. \n\n\n\nFor example, do you see loop where there can be PP -> NP -> PP and so on?\n\n\n\nIs there anything in PHP that works like Push-down automata that can solve this problem?\n\n\n\nExample input: 'the cat chased the dog'\n\n\n\nExample output:\n\n\n\n(S (NP the (N cat)) (VP (V chased) (NP the (N dog))))\n\n\n\nExample input: 'the cat chased the dog on the rug'\n\n\n\nExample output(s):\n\n\n\n(S\n  (NP the (N cat))\n  (VP (V chased) (NP the (N dog) (PP (P on) (NP the (N rug))))))\n\n\n\n(S\n  (NP the (N cat))\n  (VP (V chased) (NP the (N dog)) (PP (P on) (NP the (N rug)))))\n\n    ", "Tag": "算法分析"}
{"Answer": "我仔细思考了一下终于解决啦！我的数据为sigmoid函数映射到0和1之间分布的点数，我想把他以大于0.5和小于0.5分别预测为1和0，输出的结果为0 1分布，但是新版tensorflow没有predict_classes属性，所以我做了如下操作\ny_train_predict = mlp.predict(X_train)\na = np.ones(630)\nb = a/2\nc = np.insert(y_train_predict,0,b,axis=1)\ny_train_predict = np.argmax(c,axis=1)\ny_train_predict = y_train_predict.reshape(630,1)\nprint(y_train_predict)\n\n生成一列0.5加入到第0列，再用np.argmax按行索引判断最大值的位置，如果0.5是最大的就返回0，如果0.5是最小的就返回1再转化为二维数组格式，输出结果为\n\n和之前的predict_class结果基本一样", "Konwledge_Point": "应对NP完全问题", "Question": "关于新版tensorflow 'Sequential' object has no attribute 'predict_classes'的问题\n在Tensorflow2.6之前的版本中拥有predict_class属性\n在结果预测时可以自动将结果变成0 1分布\n\n\ny_train_predict = mlp\n.predict_classes\n(X_train)\n\nprint\n(y_train_predict)\n\n\n\n\n\n输出结果为\n\n\n\n\n而新版本的经上网查询只能用下面代码替代\n\n\ny_train_predict = mlp\n.predict\n(X_train)\ny_train_predict = np\n.argmax\n(y_train_predict, axis=\n1\n)\n\nprint\n(y_train_predict)\n\n\n\n\n\n而打印出的预测结果确实这样\n\n\n\n\n结果变成一维数据并且预测全为0，有什么办法可以解决？", "Tag": "算法分析"}
{"Answer": "27749492/300无法整除啊，也就是reshape到300列，你的行数不对，最后一行少几个参数，所以会报错。[:, 300]是切片，相当于将列号为300列的这一列切分出来。\n", "Konwledge_Point": "应对NP完全问题", "Question": "出现cannot reshape array of size 27749792 into shape (300,1)\n\n\n# 转numpy数组,打乱顺序\n\n    \ndataSet\n = np.array(dataSet).reshape(-\n1\n, \n300\n)\n    \nlableSet\n = np.array(lableSet).reshape(-\n1\n, \n1\n)\n    \ntrain_ds\n = np.hstack((dataSet, lableSet))\n    \nnp\n.random.shuffle(train_ds)\n\n    \n# 数据集及其标签集\n\n    \nX\n = train_ds.reshape(-\n1\n, \n300\n, \n1\n)\n    \nY\n = train_ds\n\n\n\n这样就会出这个报错\n然后我把改成这样，就能运行\n\n\n\n    \nX\n = train_ds[:, \n300\n].reshape(-\n1\n, \n300\n, \n1\n)\n    \nY\n = train_ds[:, \n300\n]\n\n\n\n\n想问问这个[:, 300]是什么意思？有没有人来解答一下", "Tag": "算法分析"}
{"Answer": "最后的return语句要拿到for循环外面，不然循环一次就结束了", "Konwledge_Point": "应对NP完全问题", "Question": "python中for循环range（）的问题\n在python中遇到一共for循环和range的问题\n为啥结果d=0呀？不应该是0 1 2 吗？哪里出问题了？\n\n\ndef euclidean_distance(one_sample, x):\n    \nk\n=x.shape[0]\n    \nprint\n(k)\n    \nprint\n(\n\"中心点\"\n,x)\n    \nprint\n(\n\"样本\"\n,one_sample)\n    \nfor\n d \nin\n range(k):\n        \nprint\n(\n\"d\"\n,d)\n        \nprint\n(\n\"单个样本\"\n,one_sample)\n        \nprint\n(\n\"中心\"\n,x[d])\n        distances= np.sum((x[d]-one_sample)*\n*2\n)\n        \nprint\n(\n\"距离\"\n,distances)\n        return distances\n\n运行结果是：\n3\n中心点 [[4.8 3.1 1.6 0.2]\n [6.7 3.  5.  1.7]\n [5.1 3.5 1.4 0.2]]\n样本 [5.9 3.  5.1 1.8]\nd 0\n单个样本 [5.9 3.  5.1 1.8]\n中心 [4.8 3.1 1.6 0.2]\n距离 16.029999999999998\n", "Tag": "算法分析"}
{"Answer": "print(type(arrl).arrl)\n应该是：\nprint(type(arrl),arrl)\n点号改成逗号，希望采纳", "Konwledge_Point": "应对NP完全问题", "Question": "根据报错提示修改了变量\n友友们这是出了什么问题，根据报错提示修改了变量，可是仍然报错，怎么修改，忘提示\n\n\n\n\nimport\n numpy \nas\n np\n\nlst\n=[\n11\n,\n22\n,\n33\n]\n\nprint\n(\ntype\n(lst),lst)\n\n\narrl\n = np.array(lst)\n\nprint\n(\ntype\n(arrl).arrl)\n\n\n\n", "Tag": "算法分析"}
{"Answer": "res = requests.get(url)\nprint(res.text)#打印内容看下，应该是缺少参数或者不正确导致返回了错误信息，没有result属性。根据返回的错误看下api需要什么参数\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "利用百度Api将经纬度转地址时，出现“ KeyError: 'result' ”怎么解决\n代码如下：\n\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport json\nlat1 = np\n.array\n(df2\n[\n'经度'\n]\n)\nlng1 = np\n.array\n(df2\n[\n'纬度'\n]\n)\n\naddress1 = \n[]\n\n\nfor\n \ni\n \nin\n range(len(lat1)):\n    AK = \n'tBB0SG06vj3MWDjHj7X1shMxXVgvZOCm'\n\n    url = \n'http://api.map.baidu.com/reverse_geocoding/v3/?ak={}&output=json&coordtype=wgs84ll&location={},{}'\n.format\n(AK,lat1\n[i]\n,lng1\n[i]\n)\n    res = requests\n.get\n(url)\n    address_ = json\n.loads\n(res.text)\n[\n'result'\n]\n[\n'formatted_address'\n]\n\n    address1\n.append\n(address_)\n\n", "Tag": "算法分析"}
{"Answer": "", "Konwledge_Point": "应对NP完全问题", "Question": "关于#arr数组#的问题，如何解决？\narr1=np.array([[1,2,3],[4,5,6],[7,8,9]])\narr2=np.array([[True, False, False ],[False, True, False],[False,False, True]])\nprint(arr1)\nprint(arr2)\n运行 arr1[arr2]的结果是?\nA、 array([1, 4, 9])\nB、 array([1, 4, 8])\nC、 array([1, 5, 9])\nD、 array([2, 4, 9])", "Tag": "算法分析"}
{"Answer": "因为你原来有10000个元素，你reshape只有6000\r\n![图片说明](https://img-ask.csdn.net/upload/201909/07/1567823128_375982.jpg)", "Konwledge_Point": "应对NP完全问题", "Question": "如何解决 cannot reshape array of size 10000 into shape (1000,6)\n我把excel数据（纯数字）导入datamatrix，但是在np.array(x).reshape((len(x),6)) 的时候报错了，有哪位大佬可以解释一下吗\n\n\n\nimport numpy as np\nimport urllib.request\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport pandas as pd\nimport xlrd\nfrom sklearn import preprocessing\ndef excel_to_matrix(path):\n    table = xlrd.open_workbook(path).sheets()[0]  # 获取第一个sheet表\n    row = table.nrows  # 行数\n    col = table.ncols  # 列数\n    datamatrix = np.zeros((row, col))\n    for x in range(col):\n        cols = np.matrix(table.col_values(x))\n\n        datamatrix[:, x] = cols\n    return datamatrix\n\n\ndatafile = u'C:\\\\Users\\\\asus\\\\PycharmProjects\\\\2\\\\venv\\\\Lib\\\\附件2：数据.xls'\ndatamatrix=excel_to_matrix(datafile)\ndata=pd.DataFrame(datamatrix)\n\ny=data[10]\ndata=data.drop(10,1)\nx=data\n\nfrom sklearn import preprocessing\nx_MinMax=preprocessing.MinMaxScaler()\ny_MinMax=preprocessing.MinMaxScaler()\n\ny.as_matrix(y)\ny=np.array(y).reshape((len(y),1))\nx=np.array(x).reshape((len(x),6))\nx=x_MinMax.fit_transform(x)\ny=y_MinMax.fit_transform(y)\nx.mean(axis=0)\n\nimport random\nfrom sklearn.cross_validation import train_test_split\nnp.random.seed(2016)\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n\nfrom sknn.mlp import Regressor,Layer #预测模型\nw_train=x_train[:,0]\nw_train[y_train==0]=1\nw_train[y_train==1]=1.10\nfit3=Regressor(layers=[Layer('Tanh',units=45),Layer('Tanh',units=18),\n                       Layer('Tanh',units=18),\n                       Layer('softmax')],\n               learning_rate=0.02,\n               random_state=2016,\n               valid_size=0.25,\n               dropout_rate=0.2,\n               learning_momentum=0.30,\n               batch_size=35,\n               n_iter=10\n               )\nfit3.fit(x_train,y_train,w_train)\n\nfrom sklearn.metrics import confusion_matrix\npredict3_train=fit3.predict(x_train)\nscore3=fit3.score(x_train,y_train)\nconfu3=confusion_matrix(y_train,predict3_train)\nprint(confu3)\nscore_text3=fit3.score(x_test,y_test)\nprint(score_text3)\npredict3_test=fit3.predict(x_test)\nconfu3_test=confusion_matrix(y_test,predict3_test)\nprint(confu3_test)\n\n", "Tag": "算法分析"}
{"Answer": "https://www.cnblogs.com/pinard/p/11114748.html", "Konwledge_Point": "应对NP完全问题", "Question": "Python中XGBoost库Param使用问题。\n在学习使用xgboost的过程中，编写param传入xgboost.cv等函数过程中jupyter一直提示Parameters: { xxx } might not be used.想请问出现的原因以及解决办法，代码应该是没问题的。\n\n\n\nfrom xgboost import XGBRegressor as XGBR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.linear_model import LinearRegression as LinearR\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import KFold, cross_val_score as CVS, train_test_split as TTS\nfrom sklearn.metrics import mean_squared_error as MSE\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom time import time\nimport datetime\nimport xgboost as xgb\nfrom sklearn.datasets import load_breast_cancer\ndata2 = load_breast_cancer()\nx2 = data2.data\ny2 = data2.target\ndfull2 = xgb.DMatrix(x2,y2)\nparam1 = {'silent':True,'obj':'binary:logistic',\"gamma\":0,\"nfold\":5}\nparam2 = {'silent':True,'obj':'binary:logistic',\"gamma\":2,\"nfold\":5}\nnum_round = 100\ntime0 = time()\ncvresult1 = xgb.cv(params=param1,dtrain=dfull2, num_boost_round=num_round,metrics=(\"error\"))\nprint(datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))\ntime0 = time()\ncvresult2 = xgb.cv(params=param2,dtrain=dfull2, num_boost_round=num_round,metrics=(\"error\"))\nprint(datetime.datetime.fromtimestamp(time()-time0).strftime(\"%M:%S:%f\"))\nplt.figure(figsize=(20,5))\nplt.grid()\nplt.plot(range(1,101),cvresult1.iloc[:,0],c=\"red\",label=\"train,gamma=0\")\nplt.plot(range(1,101),cvresult1.iloc[:,2],c=\"orange\",label=\"test,gamma=0\")\nplt.plot(range(1,101),cvresult2.iloc[:,0],c=\"green\",label=\"train,gamma=2\")\nplt.plot(range(1,101),cvresult2.iloc[:,2],c=\"blue\",label=\"test,gamma=2\")\nplt.legend()\nplt.show()\n\n\n\n\n结果显示如下\n\n\n\nWARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \nParameters: { nfold, obj, silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[20:18:40] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \nParameters: { nfold, obj, silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[20:18:40] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \nParameters: { nfold, obj, silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n00:00:104294\n[20:18:41] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \nParameters: { nfold, obj, silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[20:18:41] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \nParameters: { nfold, obj, silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n[20:18:41] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \nParameters: { nfold, obj, silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n", "Tag": "算法分析"}
{"Answer": "data = torch.cat(data, 0)  这一行后data是个tensor对象了，后面再for循环时候不是list 也就不能append，可以等for结束后再执行这个拼接", "Konwledge_Point": "应对NP完全问题", "Question": "关于张量存储到list中报错：AttributeError: 'Tensor' object has no attribute 'append'\n代码部分\n\n\n\n        data = []\n        \nfor\n n \nin\n \nrange\n(\n3\n, \n11\n, \n2\n):\n            \nfor\n i \nin\n \nrange\n(\n0\n, n, \n2\n):\n                \nimage\n = Image.open(img_path_list[i]).\nconvert\n('RGB')\n                data.\nappend\n(torch.unsqueeze(self.transforms(\nimage\n), \ndim\n=\n0\n))\n            \nfor\n i \nin\n \nrange\n(\n1\n, n - \n1\n, \n2\n):\n                \nimage\n = Image.open(img_path_list[i]).\nconvert\n('\n1\n')\n                \nimage\n = \nnp\n.\narray\n(\nimage\n)\n                \nimage\n = \nnp\n.expand_dims(\nimage\n,axis=\n2\n)\n                \nimage\n = \nnp\n.concatenate((\nimage\n, \nimage\n, \nimage\n), axis=-\n1\n)\n                data.\nappend\n(torch.unsqueeze(self.transforms(\nimage\n), \ndim\n=\n0\n))\n            data = torch.cat(data, \n0\n)  # torch.cat是将两个张量（tensor）拼接在一起\n            \nlabel\n = Image.open(img_path_list[n])\n            \nlabel\n = torch.squeeze(self.transforms(\nlabel\n))\n        sample = {'data': data, '\nlabel\n': \nlabel\n}\n        \nreturn\n sample\n\n\n\n报错\n\n\nAttributeError: \n'Tensor'\n \nobject\n has \nno\n \nattribute\n \n'append'\n \n\n\n\n是哪里的问题？如何解决这个报错呢？", "Tag": "算法分析"}
{"Answer": "我费劲给你写了朋友，采纳给我不过分把？\n\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame([[1,np.nan,3],[4,np.nan,6],[7,8,9]])\n\nprint(df)\n\n# 转层矩阵，让NAN右移动，行遍历\nmatrix = df.values\nfor m in range(len(matrix)):\n    row = matrix[m]\n    for x in range(len(row)):\n        if np.isnan(row[x]):\n            if x == len(row)-1:\n                pass\n            else:\n                matrix[m] = np.concatenate([row[:x], row[x+1:] , row[x:x+1]],axis=0)\ndf_ = pd.DataFrame(matrix)\ndf_.columns = df.columns\nprint(df_)\n\n# 替换nan\ndf_ = df_.fillna('否')\nprint(df_)\n输出展示：\n\n请关注公众号「机器学习炼丹术」～", "Konwledge_Point": "应对NP完全问题", "Question": "dataframe 数据列错位，后一列替换前一列\n\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[1,np.nan,3],[4,np.nan,6],[7,8,9]])\nprint(df)\n\n\n\n\n\n\n\n期望的结果：\n\n\n\n3和6移到前一列，并用“否”替换之\n\n\n\n\n\n\n ", "Tag": "算法分析"}
{"Answer": "\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame({'收益率': ['4', 0.5, np.NAN, 0.36,0.45678, 1,'0.345','aaa']})\nprint(df)\n\ndef astype_percent(data):\n    if isinstance(data,float):\n        if np.isnan(data):\n            return data\n        return '%.2f%%' % (data * 100)\n    elif isinstance(data,int):\n        return '%.2f%%' % (data * 100)\n    elif isinstance(data,str):\n        try:\n            data = float(data)\n        except:\n            return np.NAN\n        else:\n            return '%.2f%%' % (float(data) * 100)\n\n\ndf['收益率'] = df['收益率'].apply(astype_percent)\n\nprint(df)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "【python, append】将一列数据改为百分比、并解决append替换前面数据的问题\n如何将一列混合数据改为百分比、并解决append替换前面数据问题\n\n\ndf为一个有多行多列的dataframe，其中一列df['收益率']包含多种类型(float、NAN、以及string)，希望把df['收益率']中的float转为百分比格式，其他数据不变。\n\n\n代码如下：\n\n\ndef\n \nastype_percent\n(\ndf\n):\n    t = []      \n    \nfor\n i \nin\n df:\n        use_dict = {}\n        \nif\n \ntype\n(i) == \nfloat\n:\n            \nif\n \nnot\n np.isnan(i):\n                i = \n'%.2f%%'\n % (i * \n100\n)\n        \nif\n \ntype\n(i) == \nstr\n:\n            use_dict = \nstr\n(i)        \n        t.append(use_dict)\n    \nreturn\n pd.Series(t)\n\ndf[\n'收益率'\n] = astype_percent(df[\n'收益率'\n])\n\n\n\n然后发现df['收益率'] 这列整体往前移动了一行，导致错位、最后一行缺值。\n查了如何解决append替换前面数据的问题，但没有起作用", "Tag": "算法分析"}
{"Answer": "import mathpi=math.pi这样后面就可以用这个pi了", "Konwledge_Point": "应对NP完全问题", "Question": "pi从库里导入后如何定义。(语言-python)\na=（[[0，pi]，[pi/2，-pi/4]）\nnp.cos\nnp.sin", "Tag": "算法分析"}
{"Answer": "代码有两个问题。\n首先，你在更新坐标点时，有时候会尝试将点移动到超出100*100的边界之外，这可能导致程序停止。你可以在移动之前添加一个条件判断，如果点移动到边界之外，就跳过当前的循环。\n其次，你在选择下一个步骤时，使用的是option数组，但是在处理边界情况时，你分别使用了option_r、option_l、option_u、option_d数组。这会导致在某些情况下无法正确处理点的移动方向。你可以在处理边界情况时，使用option数组来确保正确处理点的移动方向。", "Konwledge_Point": "应对NP完全问题", "Question": "我试图用上述代码模拟   在100*100 中  50*50的位置有一个激活点 \n之后的1000次 每次从边界随机选取一个位置放置一个点 点进行随机移动直到周围也有激活点从而使它也被激活\n\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\ntuples_1 = \n[(x, y) for x in range(1, 101) for y in range(1, 101)]\n\ncoordinate = {t: 0 for t in tuples_1}\ncoordinate\n[(50,50)]\n = 1\ntuples_2 = \n[(x, y) for x in (1,100) for y in range(1, 101)]\n\ntuples_3 = \n[(x, y) for x in range(1,101) for y in (1, 100)]\n\ntuples_4=tuples_2 + tuples_3\nboundary = {t: 0 for t in tuples_4}\ncore = 1\nwhile core <= 999:\n    item = random.sample(boundary.keys(), 1)\n    state = boundary.get(item\n[0]\n)\n    if state == 1:\n        continue\n    if state == 0:\n        item_list=list(item\n[0]\n)\n        x = item_list\n[0]\n\n        y = item_list\n[1]\n\n        print(x, y)\n        option = np.array(\n[\n[0, 1]\n, \n[1, 0]\n, \n[0, -1]\n, \n[-1, 0]\n]\n)\n        option_r = np.array(\n[\n[0, 1]\n, \n[0, -1]\n, \n[-1, 0]\n]\n)\n        option_l = np.array(\n[\n[0, 1]\n, \n[0, -1]\n, \n[1, 0]\n]\n)\n        option_u = np.array(\n[\n[1, 0]\n, \n[0, -1]\n, \n[-1, 0]\n]\n)\n        option_d = np.array(\n[\n[0, 1]\n, \n[1, 0]\n, \n[-1, 0]\n]\n)\n        flag = 0\n        while flag == 0:\n            if x == 1:\n                next_steps = np.random.randint(len(option_l))\n                x += np.array(option\n[next_steps]\n)\n[0]\n\n                y += np.array(option\n[next_steps]\n)\n[1]\n\n            elif x == 100:\n                next_steps = np.random.randint(len(option_r))\n                x += np.array(option\n[next_steps]\n)\n[0]\n\n                y += np.array(option\n[next_steps]\n)\n[1]\n\n            elif y == 1:\n                next_steps = np.random.randint(len(option_d))\n                x += np.array(option\n[next_steps]\n)\n[0]\n\n                y += np.array(option\n[next_steps]\n)\n[1]\n\n            elif y == 100:\n                next_steps = np.random.randint(len(option_u))\n                x += np.array(option\n[next_steps]\n)\n[0]\n\n                y += np.array(option\n[next_steps]\n)\n[1]\n\n            else:\n                next_steps = np.random.randint(len(option))\n                x += np.array(option\n[next_steps]\n)\n[0]\n\n                y += np.array(option\n[next_steps]\n)\n[1]\n\n            if x>100 or x<1 or y>100 or y<1:\n                continue\n            print(x, y)\n            right = coordinate.get((x+1,y))\n            right = int(0 if right \nis\n None else right)\n            left = coordinate.get((x-1,y))\n            left = int(0 if left \nis\n None else left)\n            up = coordinate.get((x,y+1))\n            up = int(0 if up \nis\n None else up)\n            down = coordinate.get((x,y-1))\n            down = int(0 if down \nis\n None else down)\n            if right + left + up + down >= 1:\n                flag = 1\n        print(x,y)\n        coordinate\n[(x, y)]\n = 1\n        boundary\n[(x, y)]\n = 1\n        core += 1\nx_list, y_list = \n[]\n, \n[]\n\nfor k, v in coordinate.items():\n    if v == 1:\n        x_list.append(k\n[0]\n)\n        y_list.append(k\n[1]\n)\nplt.scatter(x_list, y_list)\nplt.xlim(0, 100)\nplt.ylim(0, 100)\nplt.xticks()\nplt.yticks()\nplt.grid(True)\nplt.show()\n\n\n\n\n我试图用上述代码模拟   在100\n100 中  50\n50的位置有一个激活点\n之后的1000次 每次从边界随机选取一个位置放置一个点 点进行随机移动直到周围也有激活点从而使它也被激活\n不知道代码哪里出现了问题，每次到了某个点就会停止 我用的pycharm", "Tag": "算法分析"}
{"Answer": "好吧，换了python37可以跑，老版本需要手动引入。你的r作为全局变量，赋值只操作了一次，后续每次test送入的range1,anchors都是不会变动值的变量，全是固定值的变量，结果当然都一样。。。", "Konwledge_Point": "应对NP完全问题", "Question": "如何让一个函数循环执行100次生成不同的结果并存入列表？\n如图，我想让这个程序运行100次，并把100次定位的结果存到列表中以进行数据分析，请大神帮忙看看怎么弄！\n我当前的代码只能打印出一百个相同的结果，不知道为什么！\n\n\n\n\nimport\n numpy \nas\n np\n\nimport\n matplotlib.pyplot \nas\n plt\n\nfrom\n scipy.optimize \nimport\n minimize\n\nimport\n numpy \nas\n np\n\nfrom\n math \nimport\n *\n\nimport\n matplotlib.pyplot \nas\n plt\n\nfrom\n scipy.optimize \nimport\n minimize\n\nfrom\n numpy \nimport\n *\n\nfrom\n numpy.linalg \nimport\n inv, qr\n\nNoise = \n1\n        \n#噪声方差\n\na = random.random(\n3\n)\nr = np.zeros(\n3\n)\n\nfor\n i \nin\n \nrange\n(\n0\n,\n3\n):\n    r[i]=\n2\n*a[i]-\n1\n                                   \n#随机生成一个-1到1的数\n\n\n\ndef\n \nopt_location\n(\nrange1, anchors\n):               \n#range1是真实测得的toa\n\n\n    \ndef\n \ncon\n():\n        \n# 约束条件 分为eq 和ineq\n\n        \n# eq表示 函数结果等于0 ； ineq 表示 表达式大于等于0\n\n        x1min, x1max, x2min, x2max = -\n5\n, \n15\n, -\n5\n, \n15\n\n        cons = ({\n'type'\n: \n'ineq'\n, \n'fun'\n: \nlambda\n x: x[\n0\n] - x1min},              \n#x[0]-x1min >= 0\n\n                {\n'type'\n: \n'ineq'\n, \n'fun'\n: \nlambda\n x: -x[\n0\n] + x1max},\n                {\n'type'\n: \n'ineq'\n, \n'fun'\n: \nlambda\n x: x[\n1\n] - x2min},\n                {\n'type'\n: \n'ineq'\n, \n'fun'\n: \nlambda\n x: -x[\n1\n] + x2max})\n        \nreturn\n cons                \n# \n\n\n    \ndef\n \ncost\n(\npos\n):\n#***************\n\n        ref = np.sqrt(np.\nsum\n((anchors-pos.reshape(\n1\n,\n2\n))**\n2\n, axis=\n1\n))         \n#anchors为四个基站的位置，pos为要优化的位置\n\n        ref0 = ref[\n1\n:] - ref[\n0\n]\n        ref1 = ref[\n2\n:] - ref[\n1\n]\n        ref2 = ref[\n3\n:] - ref[\n2\n]                            \n#tdoa\n\n        Ri_0 = range1[\n1\n:] - range1[\n0\n] + \n3\n*Noise*r[\n0\n]\n        Ri_1 = range1[\n2\n:] - range1[\n1\n] + \n2\n*Noise*r[\n1\n]               \n        Ri_2 = range1[\n3\n:] - range1[\n2\n] + Noise*r[\n2\n]                       \n#真实测得的tdoa\n\n        \nreturn\n np.\nsum\n((Ri_0 - ref0)**\n2\n) + np.\nsum\n((Ri_1 - ref1)**\n2\n) + np.\nsum\n((Ri_2 - ref2)**\n2\n)    \n#目标函数的目的是要求所估计位置和各基站的tdoa\n\n                                                                                                  \n#和真实测得的tdoa相差最小\n\n\n    x0 = np.array([\n5\n, \n5\n])\n    res = minimize(cost, x0, constraints=con()).x\n    \nreturn\n res\n\ndef\n \ntest\n():\n    \n#pos = np.array([0,4])\n\n    z = opt_location(range1, anchors)\n    \nreturn\n z\n    \nanchors = np.array([[\n0\n,\n0\n],[\n0\n,\n10\n],[\n10\n,\n10\n],[\n10\n,\n0\n]])\n\n#range = np.array([7.07106781,7.07106781,7.07106781,7.07106781])\n\nrangetoa = np.array([\n0.00000002358654336\n , \n0.00000002358654336\n ,\n0.00000002358654336\n ,\n0.00000002358654336\n])  \n#移动台为5 5的toa\n\n\n#rangetoa = np.array([0.00000001886923469 , 0.00000002405364897,0.00000002830385204 ,0.00000002405364897])#移动台为4 4 的toa\n\nrangetoa = np.array([\n0.00000002540348465\n, \n0.00000001415192602\n,\n0.00000002540348465\n ,\n0.00000003302116071\n])\n#移动台为3 7 的toa\n\n\n#rangetoa = np.array([0.00000002028991179, 0.00000001375319997,0.00000003285225341 ,0.00000003608047346])  #移动台为1 6的toa\n\nrange1 = rangetoa * \n299792458\n\n\nposition = np.zeros((\n100\n,\n2\n),dtype = \nfloat\n)\n\nfor\n i \nin\n \nrange\n (\n0\n,\n100\n):\n    position[i] = test()\n    \n\nprint\n(position)\n", "Tag": "算法分析"}
{"Answer": "你参数名称写错了，有用记得采纳\ncamp=plt.cm.gray\n\n改成：\ncmap =plt.cm.gray\n", "Konwledge_Point": "应对NP完全问题", "Question": "关于pytorch的一个小问题。\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nmyim = Image.open(\"D:/Lenna.png\")\nmyimgray=np.array(myim.convert(\"L\"),dtype=np.float32)\nimh,imw=myimgray.shape\nmyimgray_t=torch.from_numpy(myimgray.reshape(1,1,imh,imw))\nmyimgray_t.shape\nkersize=5\nker=torch.ones(kersize,kersize,dtype=torch.float32)*-1\nker[2,2]=24\nker=ker.reshape((1,1,kersize,kersize))\nconv2d=nn.Conv2d(1,2,(kersize,kersize),bias=False)\nconv2d.weight.data[0]=ker\nimconv2dout=conv2d(myimgray_t)\nimconv2dout_im=imconv2dout.data.squeeze()\nprint(\"卷积后尺寸：\",imconv2dout_im.shape)\nplt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nplt.imshow(imconv2dout_im[0],camp=plt.cm.gray)\nplt.axis(\"off\")\nplt.subplot(1,2,2)\nplt.imshow(imconv2dout_im[1],camp=plt.cm.gray)\nplt.axis(\"off\")\nplt.show()\n\n\n请问为什么会出现以下报错？\nenvironment:vsc\n\n", "Tag": "算法分析"}
{"Answer": "您好，我是有问必答小助手，您的问题已经有小伙伴解答了，您看下是否解决，可以追评进行沟通哦~\n\n如果有您比较满意的答案 / 帮您提供解决思路的答案，可以点击【采纳】按钮，给回答的小伙伴一些鼓励哦～～\n\nps:问答VIP仅需29元，即可享受5次/月 有问必答服务，了解详情>>>https://vip.csdn.net/askvip?utm_source=1146287632", "Konwledge_Point": "应对NP完全问题", "Question": "纯小白，求教numpy一个矩阵计算的问题，挺急的，谢过了！\n已知有矩阵nu 和 Q,应该怎么写程序才能得到如图所示的矩阵 ek ？\n\n\n\n\n\n\n之前实在matlab上做的，现在想转到numpy上但不知道程序怎么写（就是个简单的循环但是不知道在numpy是什么格式）\n\n\n\n自己写的numpy报错：\n\n\n\nek=np.zeros([6,4]);\n\n\n\nq=np.array([2.588 2.964 4.029 1.4]);\n\n\n\nnu=np.array([[1,0,1,0],[1,1,2,0],[1,1,1,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]);\n\nQ=np.array([0.848,0.54,1.2,1.224,0.901,1.4]);\n\nQ_trans=np.transpose(Q);\n\n\n\n\nfor i in range(6): \n\nek[:,1]= np.multiply(nu[:,1]*Q_trans) \n\n\n\n附原来matlab上写的（运行顺利）\n\n\n\nek=zeros(6,4);\n\n\n\nq=[2.588 2.964 4.029 1.4];\n\n\n\nnu = [1 0 1 0 ; 1 1 2 0 ; 1 1 1 0  ; 0 1 0 0 ; 0 0 1 0 ; 0 0 0 1];\n\n\n\nfor i = 1:6\n\nek(i,:) = nu(i,:)*Q(i)./q;\n\nend\n\n\n\n如果可以的话，我能给各位发matlab的程序，然后大神帮我翻译成numpy的格式吗", "Tag": "算法分析"}
{"Answer": "第15行写错了 plt.slow() -> plt.show()", "Konwledge_Point": "应对NP完全问题", "Question": "这是咋回事，需要qt吗，我已经下载了呀\n问题遇到的现象和发生背景\n\n\n问题相关代码，请勿粘贴截图\n\n\n运行结果及报错内容\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果\n\n\nimport\n numpy as np\n\nimport\n mpl_toolkits.axisartist as ast\n\nfrom\n matplotlib import pyplot as plt\n\n\nx1\n=[\n2\n,\n5\n,\n3\n,\n1\n]\n\ny1\n=[\n4\n,\n7\n,\n3\n,\n4\n]\n\n\nx2\n=[\n1\n,\n2\n,\n3\n,\n4\n]\n\ny2\n=[\n2\n,\n3\n,\n4\n,\n5\n]\n\n\nx3\n=np.random.randint(\n0\n,\n5\n,\n8\n)\n\ny3\n=np.random.randint(\n0\n,\n5\n,\n8\n)\n\n\nplt\n.scatter(x1,y1,marker='o',color='red',s=\n40\n,label='面包')\n\nplt\n.slow()\n\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "你可以参考下这个问题的回答, 看看是否对你有帮助, 链接: https://ask.csdn.net/questions/7798826这篇博客你也可以参考下：粒子群算法(PSO)的Python实现（求解多元函数的极值）这篇博客也不错, 你可以看下粒子群算法(PSO)的Python实现（求解多元函数的极值）", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的问题：python PSO 粒子群算法 多元函数求极小值问题 运行结果和迭代次数每次都不一样\npython PSO 粒子群算法 多元函数求极小值问题 运行结果和迭代次数每次都不一样\n\n\n\n\n\n\n# -*- coding: utf-8 -*-\n\n\n\nimport\n math\n\nimport\n random\n\nimport\n numpy \nas\n np\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n pylab \nas\n mpl\nmpl.rcParams[\n'font.sans-serif'\n] = [\n'SimHei'\n]\n\n\n\nclass\n \nPSO\n:\n    \ndef\n \n__init__\n(\nself, dimension, time, size, low, up, v_low, v_high, a, b\n):\n        \n# 初始化\n\n        self.dimension = dimension  \n# 变量个数\n\n        self.time = time  \n# 迭代的代数\n\n        self.size = size  \n# 种群大小\n\n        self.bound = []  \n# 变量的约束范围\n\n        self.bound.append(low)\n        self.bound.append(up)\n        self.v_low = v_low\n        self.v_high = v_high\n        self.x = np.zeros((self.size, self.dimension))  \n# 所有粒子的位置\n\n        self.v = np.zeros((self.size, self.dimension))  \n# 所有粒子的速度\n\n        self.p_best = np.zeros((self.size, self.dimension))  \n# 每个粒子最优的位置\n\n        self.g_best = np.zeros((\n1\n, self.dimension))[\n0\n]  \n# 全局最优的位置\n\n        self.a = a\n        self.b = b\n\n        \n# 初始化第0代初始全局最优解\n\n        temp = \n1000000\n\n        \nfor\n i \nin\n \nrange\n(self.size):\n            \nfor\n j \nin\n \nrange\n(self.dimension):\n                self.x[i][j] = random.uniform(self.bound[\n0\n][j], self.bound[\n1\n][j])\n                self.v[i][j] = random.uniform(self.v_low, self.v_high)\n            self.p_best[i] = self.x[i]  \n# 储存最优的个体\n\n            fit = self.fitness(self.p_best[i])\n            \n# 做出修改\n\n            \nif\n fit < temp:\n                self.g_best = self.p_best[i]\n                temp = fit\n\n    \ndef\n \nfitness\n(\nself, x\n):\n        \n\"\"\"\n        个体适应值计算\n        \"\"\"\n\n        x1 = x[\n0\n]\n        x2 = x[\n1\n]\n        x3 = x[\n2\n]\n        x4 = x[\n3\n]\n        \nfor\n i \nin\n \nrange\n(\n11\n):\n            y = (a[i] - (x1 * (\n1\n + b[i] * x2))/(\n1\n + b[i] * x3 + x4 * b[i] * b[i]))**\n2\n\n        \n# print(y)\n\n        \nreturn\n y\n\n    \ndef\n \nupdate\n(\nself, size\n):\n        c1 = \n1.5\n  \n# 学习因子\n\n        c2 = \n1.5\n\n        w = \n0.8\n  \n# 自身权重因子\n\n        \nfor\n i \nin\n \nrange\n(size):\n            \n# 更新速度(核心公式)\n\n            self.v[i] = w * self.v[i] + c1 * random.uniform(\n0\n, \n1\n) * (\n                    self.p_best[i] - self.x[i]) + c2 * random.uniform(\n0\n, \n1\n) * (self.g_best - self.x[i])\n            \n# 速度限制\n\n            \nfor\n j \nin\n \nrange\n(self.dimension):\n                \nif\n self.v[i][j] < self.v_low:\n                    self.v[i][j] = self.v_low\n                \nif\n self.v[i][j] > self.v_high:\n                    self.v[i][j] = self.v_high\n\n            \n# 更新位置\n\n            self.x[i] = self.x[i] + self.v[i]\n            \n# 位置限制\n\n            \nfor\n j \nin\n \nrange\n(self.dimension):\n                \nif\n self.x[i][j] < self.bound[\n0\n][j]:\n                    self.x[i][j] = self.bound[\n0\n][j]\n                \nif\n self.x[i][j] > self.bound[\n1\n][j]:\n                    self.x[i][j] = self.bound[\n1\n][j]\n            \n# 更新p_best和g_best\n\n            \nif\n self.fitness(self.x[i]) < self.fitness(self.p_best[i]):\n                self.p_best[i] = self.x[i]\n            \nif\n self.fitness(self.x[i]) < self.fitness(self.g_best):\n                self.g_best = self.x[i]\n\n    \ndef\n \npso\n(\nself\n):\n        best = []\n        self.final_best = np.array([\n1\n, \n2\n, \n3\n, \n4\n])\n        \nfor\n gen \nin\n \nrange\n(self.time):\n            self.update(self.size)\n            \nif\n self.fitness(self.g_best) < self.fitness(self.final_best):\n                self.final_best = self.g_best.copy()\n            \nprint\n(\n'当前最佳位置：{}'\n.\nformat\n(self.final_best))\n            temp = self.fitness(self.final_best)\n            \nprint\n(\n'当前的最佳适应度：{}'\n.\nformat\n(temp))\n            best.append(temp)\n        t = [i \nfor\n i \nin\n \nrange\n(self.time)]\n        plt.figure()\n        plt.plot(t, best, color=\n'red'\n, marker=\n'.'\n, ms=\n15\n)\n        plt.rcParams[\n'axes.unicode_minus'\n] = \nFalse\n\n        plt.margins(\n0\n)\n        plt.xlabel(\nu\"迭代次数\"\n)  \n# X轴标签\n\n        plt.ylabel(\nu\"适应度\"\n)  \n# Y轴标签\n\n        plt.title(\nu\"迭代过程\"\n)  \n# 标题\n\n        plt.show()\n\n\n\nif\n __name__ == \n'__main__'\n:\n    time = \n100\n\n    size = \n100\n\n    dimension = \n4\n\n    v_low = -\n0.5\n\n    v_high = \n0.5\n\n    low = [-\n5\n, -\n5\n, -\n5\n, -\n5\n]\n    up = [\n5\n, \n5\n, \n5\n, \n5\n]\n    a = [\n0.1957\n, \n0.1947\n, \n0.1735\n, \n0.16\n, \n0.0844\n, \n0.0627\n, \n0.0456\n, \n0.0342\n, \n0.0323\n, \n0.0235\n, \n0.0246\n]\n    b = [\n0.25\n, \n0.5\n, \n1\n, \n2\n, \n4\n, \n6\n, \n8\n, \n10\n, \n12\n, \n14\n, \n16\n]\n    pso = PSO(dimension, time, size, low, up, v_low, v_high, a, b)\n    pso.pso()\n\n\n\n\n\n\n\n\n\n\n是代码哪里错误了吗，还是粒子群算法结果就是这样的，希望能给出错误地方或者解决这个问题的代码。", "Tag": "算法分析"}
{"Answer": "创建的数组1为：\n [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n创建的数组2为：\n [[1. 1. 1.]\n [1. 1. 1.]\n [1. 1. 1.]]\n叠放后的数组为：\n [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 1. 1.]\n [1. 1. 1.]\n [1. 1. 1.]]\n", "Konwledge_Point": "应对NP完全问题", "Question": "数据分析 答案是什么 求解\n请把三行的输出结果写出来\narr1 = np.eye(3)\narr2 = np. ones(3,3)\nprint('创建的数组1为：＼n',arr1)\nprint('创建的数组2为：＼n',np.ones(3,3))\nprint('叠放后的数组为：＼n',np.vstack((arr1,arr2)))", "Tag": "算法分析"}
{"Answer": "代码中数据导入是通过读取xslx表格数据为数据框，然后取数据框中第一列后面所有列的数组赋值给X,把第一列数组赋值给y。因X是一个numpy数组，不是dataframe,没有to_excel方法，所以报错。numpy数组的保存方法，参考一下这里https://blog.csdn.net/leilei7407/article/details/107511187", "Konwledge_Point": "应对NP完全问题", "Question": "想问一下决策树模型中如何导入做出的数据\n#模型输出y\n\nChurn_y=np.array(Churn).reshape(-1,1)\n\n\n\n#模型输入x\n\nSeniorCitizen_x=np.array(SeniorCitizen).reshape(-1,1)\n\nContract_x=Contract_dummies.values\n\nInternetService_x=InternetService_dummies.values\n\nPaymentMethod_x=np.array(PaymentMethod).reshape(-1,1)\n\nOnlineSecurity_x=np.array(OnlineSecurity).reshape(-1,1)\n\nOnlineBackup_x=np.array(OnlineBackup).reshape(-1,1)\n\nDeviceProtection_x=np.array(DeviceProtection).reshape(-1,1)\n\nTechSupport_x=np.array(TechSupport).reshape(-1,1)\n\ntenure_x=tenure_dummies.values\n\nMonthlyCharges_x=MonthlyCharges_dummies.values\n\n\n\nX=np.concatenate([SeniorCitizen_x,Contract_x,InternetService_x,PaymentMethod_x,OnlineSecurity_x,\\ OnlineBackup_x,DeviceProtection_x,TechSupport_x,tenure_x,MonthlyCharges_x],axis=1)\n\n#数据导入\n\nimport pandas as pd\n\nimport numpy as np\n\ndata_frame=pd.read_excel(\"Save_X.xlsx\")\n\nX=np.array(data_frame.values[:,1:])\n\ndata_frame2=pd.read_excel(\"Save_y.xlsx\")\n\ny=np.array(data_frame2.values[:,1])\n\n#模型训练\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\ntree = DecisionTreeClassifier(max_depth=6,random_state=0)\n\ntree.fit(x_train,y_train)\n\nprint(\"training set score:{:.3f}\".format(tree.score(x_train, y_train)))\n\nprint(\"test set score:{:.3f}\".format(tree.score(x_test, y_test)))\n\nprint(\"Feature importances : \\n{}\".format(tree.feature_importances_))\n\n \n\n\n\n这里的data_frame=pd.read_excel(\"Save_X.xlsx\")\n\n\n\ndata_frame2=pd.read_excel(\"Save_y.xlsx\")是怎么导入进去的\n\n\n\n使用X.to_excel的时候会出现问题：\n\n\n\nAttributeError: 'numpy.ndarray' object has no attribute 'to_excel'", "Tag": "算法分析"}
{"Answer": "你现在画的图应该纵坐标是[0,30,60,90,120,150,180,150,120,90,60,30]，你的目标是要在后面加上W和E？", "Konwledge_Point": "应对NP完全问题", "Question": "python坐标轴设置\npython\n怎么设置y轴为指定经纬度？就像这幅图一样。这是代码\n\n\n\n\n我用的python版本为3.9\n\n\nimport os\nos.environ[\n'PROJ_LIB'\n]=\n'D:\\\\Anaconda\\\\Library\\\\share'\n\nos.chdir(\n\"D:\\硕士研究生\"\n)\nos.getcwd()\n\nfrom\n netCDF4 import Dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\n\n\n#def colormap():\n\n\n#    import matplotlib.colors as colors\n\n\n#    cdict=[(1,1,1),(0,1,1),(0,157/255,1),(0,0,1),(9/255,130/255,175/255),(0,1,0),(8/255,175/255,20/255),(1,214/255,0),(1,152/255,0),(1,0,0),(221/255,0,27/255),(188/255,0,54/255),(121/255,0,109/255),(121/255,51/255,160/255),(195/255,163/255,212/255),]\n\n\n#    return (colors.ListedColormap(cdict,'indexed'))\n\n\n\ndata_file\n=\n'201412-100.nc'\n\n\nnc_file\n=Dataset(data_file)\n\n#print(nc_file)\n\n\nvar_names\n=nc_file.variables.keys()\n\n#print(var_names)\n\n\nall_vars\n=nc_file.variables.items()\n\n#print(all_vars)\n\n\n#print(nc_file.variables['u'])\n\n\nlev\n=nc_file.variables[\n't'\n]\n\n#print(lev)\n\n\nlev1\n=nc_file.variables[\n'latitude'\n]\n\n#print(lev1)\n\n\n\nU\n=nc_file.variables[\n\"u\"\n][:,:,:,:]\n\nT\n=nc_file.variables[\n\"t\"\n][:,:,:,:]\n\n\nshape_U\n=np.shape(U)\n\nprint\n(shape_U)\n\nlat_U2hPa\n=np.zeros((31,91),dtype='f')\n\nlat_U2hPa100_130\n=np.zeros((31,91),dtype='f')\n\nlon_U2hPa\n=np.zeros((31,360),dtype='f')\n\nlon_U2hPa35_70\n=np.zeros((31,360),dtype='f')\n\nlon_U2hPareal\n=np.zeros((360,31),dtype='f')\n\nlon_U2hPa35_70real\n=np.zeros((360,31),dtype='f')\n\nsum\n=np.zeros(31,dtype='f')\n\nsum1\n=np.zeros(31,dtype='f')\n\nlat_T2hPa\n=np.zeros((31,91),dtype='d')\n\nlat_T2hPa100_130\n=np.zeros((31,91),dtype='d')\n\nlon_T2hPa\n=np.zeros((31,360),dtype='d')\n\nlon_T2hPa35_70\n=np.zeros((31,360),dtype='d')\n\nlon_T2hPareal\n=np.zeros((360,31),dtype='f')\n\nlon_T2hPa35_70real\n=np.zeros((360,31),dtype='f')\n\n\nfor\n i \nin\n np.arange(shape_U[0]):\n    \nfor\n j \nin\n np.arange(shape_U[2]):\n        \nfor\n k \nin\n np.arange(shape_U[3]):\n            lat_U2hPa[i,j]+=U[i,1,j,k]\n            lat_T2hPa[i,j]+=T[i,1,j,k]\n        lat_U2hPa[i,j]=lat_U2hPa[i,j]/shape_U[3]\n        lat_T2hPa[i,j]=lat_T2hPa[i,j]/shape_U[3]\n\nfor\n i \nin\n np.arange(shape_U[0]):\n    \nfor\n j \nin\n np.arange(shape_U[2]):\n        \nfor\n k \nin\n np.arange(280,310):\n            lat_U2hPa100_130[i,j]+=U[i,1,j,k]\n            lat_T2hPa100_130[i,j]+=T[i,1,j,k]\n        lat_U2hPa100_130[i,j]=lat_U2hPa100_130[i,j]/30\n        lat_T2hPa100_130[i,j]=lat_T2hPa100_130[i,j]/30\n\n\nlat_U2hPa\n=lat_U2hPa100_130-lat_U2hPa\n\nlat_T2hPa\n=lat_T2hPa100_130-lat_T2hPa\n\nlat_U2hPa\n=lat_U2hPa.transpose(1,0)\n\nlat_T2hPa\n=lat_T2hPa.transpose(1,0)\n\n\nfor\n i \nin\n np.arange(shape_U[0]):\n    \nfor\n j \nin\n np.arange(shape_U[3]):\n        \nfor\n k \nin\n np.arange(20,55):\n            lon_U2hPa35_70[i,j]+=U[i,1,k,j]\n            lon_T2hPa35_70[i,j]+=T[i,1,k,j]\n        lon_U2hPa35_70[i,j]=lon_U2hPa35_70[i,j]/35\n        lon_T2hPa35_70[i,j]=lon_T2hPa35_70[i,j]/35\n\nfor\n i \nin\n np.arange(shape_U[0]):\n    \nfor\n j \nin\n np.arange(shape_U[3]):\n        sum[i]+=lon_U2hPa35_70[i,j]\n        sum1[i]+=lon_T2hPa35_70[i,j]\n    sum[i]=sum[i]/shape_U[3]\n    sum1[i]=sum1[i]/shape_U[3]\n    \nfor\n j \nin\n np.arange(shape_U[3]):\n        lon_U2hPa[i,j]=lon_U2hPa35_70[i,j]-sum[i]\n        lon_T2hPa[i,j]=lon_T2hPa35_70[i,j]-sum1[i]\n        \n\nlon_U2hPa\n=lon_U2hPa.transpose(1,0)\n\nlon_T2hPa\n=lon_T2hPa.transpose(1,0)\n\n\nfor\n i \nin\n np.arange(0,31):\n    \nfor\n j \nin\n np.arange(0,360):\n        \nif\n j <180:\n            lon_U2hPareal[j,i]=lon_U2hPa[j+180,i]\n            lon_T2hPareal[j,i]=lon_T2hPa[j+180,i]\n        lon_U2hPareal[j,i]=lon_U2hPa[j-180,i]\n        lon_T2hPareal[j,i]=lon_T2hPa[j-180,i]\n\n\nprint\n(np.shape(lon_U2hPa))\n\nprint\n(np.shape(lon_T2hPareal))\n\n\n#绘图\n\n\n#print(np.shape(lat_U))\n\n\nx\n=np.arange(1,32)\n\n#print(np.shape(Y))\n\n\ny\n=nc_file.variables[\n\"latitude\"\n][:]\n\ny1\n=nc_file.variables[\n\"longitude\"\n][:]\n\nprint\n(np.shape(y1))\n\n\n#print(np.shape(X))\n\nX,\nY\n=np.meshgrid(x,y)\nX1,\nY1\n=np.meshgrid(x,y1)\nfg,\naxes\n=plt.subplots(2,1)\nfg.set_size_inches(5,8)\n\nlvl\n=np.arange(-45,55,5)\n\nlv2\n=np.arange(-20,30,5)\n\nim1\n=axes[0].contourf(X,Y,lat_U2hPa,\nlevels\n=lvl,cmap='jet',extend='both')\n\nim2\n=axes[0].contour(X,Y,lat_T2hPa,\nlevels\n=lv2,colors='k')\naxes[0].set_xlabel(\n'date'\n,\nfontsize\n=10)\naxes[0].clabel(im2,\nfontsize\n=8)\naxes[0].yaxis.set_major_formatter(mticker.FormatStrFormatter(\n'%.0f°N'\n))\n\n\nlv3\n=np.arange(-20,30,5)\n\nim3\n=axes[1].contourf(X1,Y1,lon_U2hPareal,\nlevels\n=lvl,cmap='jet',extend='both')\n\nim4\n=axes[1].contour(X1,Y1,lon_T2hPareal,\nlevels\n=lv3,colors='k')\naxes[1].clabel(im4,\nfontsize\n=8)\naxes[1].set_xlabel(\n'date'\n,\nfontsize\n=10)\n\nc1\n=np.arange(181)\n\nc2\n=np.arange(180)\n\nc3\n=c2[::-1]\n\npallels\n=np.concatenate((c1,c3),axis=0)\ncolum_y=[0,30,60,90,120,150,180,150,120,90,60,30]\naxes[1].set_yticks(pallels,colum_y)\naxes[1].yaxis.set_major_formatter(mticker.FormatStrFormatter(\n'%.0f°E'\n))\n\nfg.text(0.15,0.5,\n'Dec'\n,\nfontsize\n=10,horizontalalignment='right')\nfg.text(0.15,0.08,\n'Dec'\n,\nfontsize\n=10,horizontalalignment='right')\n\ncbar\n=fg.colorbar(im1,ax=axes)\n\n", "Tag": "算法分析"}
{"Answer": "img_src 是 3通道，label 是 单通道，根本就不能复制\nimg_src = np.zeros((960, 575, 3), np.uint8)\nlabel = np.zeros((160, 50), np.uint8)\n\nlabel = img_src[6:20, 861:938]\n", "Konwledge_Point": "应对NP完全问题", "Question": "关于#OPENCV# #OCR#设定区域无效的问题\n哪位能帮忙解决下，我用OPENCV 和paddleOCR尝试写一个 识别游戏画面的文字，我设定的区域是屏幕右上角的一小块，label = img_src[6:20, 861:938]，但是运行中貌似这个命令没有用，每次OCR都识别的是全图的所有文字，哪位能帮忙看下是哪里出错了，谢谢啦 \n\n\nimport\n cv2\n\nimport\n numpy \nas\n np\n\nfrom\n PIL \nimport\n ImageGrab\n\nfrom\n win32 \nimport\n win32gui\n\nimport\n mss\n\nfrom\n paddleocr \nimport\n PaddleOCR, draw_ocr\n\nimport\n threading\n\nfrom\n PIL \nimport\n ImageGrab, Image\n\n\nglobal\n label\n\nglobal\n img_src\n\nglobal\n cut\n\nglobal\n bboxes\n\n\nimg_src = np.zeros((\n960\n, \n575\n, \n3\n), np.uint8)\nlabel = np.zeros((\n160\n, \n50\n), np.uint8)\nbboxes = np.array([])\ncut = \nFalse\n\n\n\n\ndef\n \ngetScreenshot\n():\n    hwnd = win32gui.FindWindow(\n\"LDPlayerMainFrame\"\n, \n\"雷电模拟器\"\n)\n    x0, y0, x1, y1 = win32gui.GetWindowRect(hwnd)\n    mtop, mbot = \n30\n, \n1\n\n    \n# print(x0, y0, x1, y1)\n\n    monitor = {\n\"left\"\n: x0, \n\"top\"\n: y0, \n\"width\"\n: x1-x0, \n\"height\"\n: y1-y0}\n    img_src = np.array(mss.mss().grab(monitor))\n    img_src = img_src[:, :, :\n3\n]\n    img_src = img_src[mtop:-mbot]\n    \nreturn\n img_src, [x0, y0, x1, y1, mtop, mbot]\n\n\n# ocr-----------------------------------------\n\nocr = PaddleOCR(use_angle_cls=\nFalse\n, lang=\n\"ch\"\n, show_log=\nFalse\n)\n\n\ndef\n \ngetMonitor\n():\n    \nglobal\n img_src, label, cut\n    \nwhile\n \nTrue\n:\n        img_src, _ = getScreenshot()\n        label = img_src[\n6\n:\n20\n, \n861\n:\n938\n]\n        \n\ndef\n \ngetOcrText\n(\nimg\n):\n    img = img_src.copy()\n    img, _ = getScreenshot()\n    \nprint\n(\n\"img::::\"\n + \nstr\n(img))\n    result = ocr.ocr(img, cls=\nFalse\n)\n    \nreturn\n result\n\n\n\ndef\n \ngetLabelExist\n(\nimg,name\n):\n    result = getOcrText(img)\n    \nprint\n(result)\n    \nfor\n re \nin\n result:\n        text = re[\n1\n][\n0\n]\n        \nif\n name == text:\n            \nreturn\n \nTrue\n\n    \nreturn\n \nFalse\n\n\n\ndef\n \ncheckLabel\n():\n    \nglobal\n label\n    \nglobal\n cut\n   \n    \nwhile\n \nTrue\n:\n        cut = getLabelExist(label, \n'一层'\n)\n        \nif\n cut:\n            \nprint\n(\n\"找到\"\n)\n         \n        \nelse\n:\n            \nprint\n(\n\"未找到\"\n)\n       \n\n\n\n\n\nif\n __name__ == \n'__main__'\n:\n    t1 = threading.Thread(target=getMonitor,args=(),daemon=\nTrue\n)\n    t1.start()\n    t2 = threading.Thread(target=checkLabel,args=(),daemon=\nTrue\n)\n    t2.start()\n   \n\n\nwhile\n \nTrue\n:\n        img = img_src.copy()\n        img, _ = getScreenshot()\n        \n# 按比例缩小-------------------------------------------\n\n        x, y = img.shape[\n0\n:\n2\n]\n        imgs = cv2.resize(img, (\n0\n, \n0\n), fx=\n0.5\n, fy=\n0.5\n, interpolation=cv2.INTER_NEAREST)\n        \n# ------------------------------------------------\n\n        \n# bboxes = getDetection(img)\n\n        \n# img = drawBBox(img.copy(),bboxes)\n\n        cv2.imshow(\n\"1234\"\n, imgs)\n        cv2.imshow(\n\"\"\n,label)\n        \nif\n cv2.waitKey(\n1\n) & \n0xFF\n == \n27\n:\n            cv2.destroyAllWindows()\n            \nbreak\n\n\n", "Tag": "算法分析"}
{"Answer": "因为train_y[i]一直都是0", "Konwledge_Point": "应对NP完全问题", "Question": "使用matplotlib画图，为什么我的颜色不变？\n问题遇到的现象和发生背景\n\n\n使用matplotlib画图，为什么我的颜色不变？\n\n\n用代码块功能插入代码，请勿粘贴截图\n\n\n%matplotlib inline\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\n\nnp.random.seed(314)\ndata_size1=100\nx1=np.random.randn(data_size,2)+np.array([4,4])\ny1=[0 for _ in range(data_size)]\ndata_size2=100\nx2=np.random.randn(data_size,2)+np.array([10,10])\ny2=[0 for _ in range(data_size)]\n\n\nx=np.concatenate((x1,x2),axis=0)\ny=np.concatenate((y1,y2),axis=0)\n\n\ndata_all_size=data_size1+data_size2\nshuff_index=np.random.permutation(data_all_size)\nx=x[shuff_index]\ny=y[shuff_index]\n\n\ntrain_size=int(data_all_size*0.7)\ntrain_x=x[:train_size]\ntrain_y=y[:train_size]\ntest_x=x[train_size:]\ntest_y=y[train_size:]\n\n\nfor i in range(train_size):\n    if train_y[i]==0:\n        plt.scatter(train_x[i,0],train_x[i,1],s=38,c='g',marker='*')\n    else:\n        plt.scatter(train_x[i,0],train_x[i,1],s=38,c='r',marker='.')\nmpl.rcParams['font.family'] = 'SimHei'\nplt.title(\"训练数据\")\nplt.savefig(\"fig-res-knn-traindata.pdf\")\nplt.show()\n\n\n运行结果及报错内容\n\n", "Tag": "算法分析"}
{"Answer": ">>> a = np.arange(1, 10)\n>>> np.where((3<=a)&(a<=9), -a, a)\narray([ 1,  2, -3, -4, -5, -6, -7, -8, -9])\n", "Konwledge_Point": "应对NP完全问题", "Question": "numpy 按条件翻转 无法正确实现\n# 创建一个包含1-10中所有整数的向量，并对3-8之间的元素取相反数，即输出[1,2,-3,-4,-5,-6,-7,-8,9]\n\nimport numpy \nas\n np\n\n\na\n = np.arange(\n1\n, \n10\n)\n\na\n = np.where(\n3\n <= \na\n <= \n9\n, -\na\n, \na\n)\nprint(\na\n)\n\n\n\n要求如注释,报错如下\n写成\na = np.where((3 <= a), -a, a)\n就不会报错,但与题目要求不符\n想知道如何使用np.where完成如上题目", "Tag": "算法分析"}
{"Answer": "你是在串口中断中输入信息？你试试在命令结尾加\\n\\r", "Konwledge_Point": "应对NP完全问题", "Question": "关于继电器232控制协议编辑问题\nAMX的NP-Rel8继电器的232协议命令，在最后必须敲一下回车键，请问用哪个指令能代替它。现在我手上的这个中控不支持敲回车命令。十万火急，希望大神多多照顾。", "Tag": "算法分析"}
{"Answer": "#include \r\nint clength(int i, int count)\r\n{\r\n    if(i == 1) {\r\n        count++;\r\n        return count;\r\n    }\r\n    if(i & 1) {\r\n        count++;\r\n        return clength(3 * i + 1, count);\r\n    } else {\r\n        count++;\r\n        return clength(i >> 1, count);\r\n    }\r\n}\r\nint main()\r\n{\r\n    int i, j, s, max;\r\n    while(scanf(\"%d %d\", &i, &j) != EOF) {\r\n        max = 0;\r\n        for(int k = i; k <= j; k++) {\r\n            s = clength(k, 0);\r\n            if(s > max) {\r\n                max = s;\r\n            }\r\n        }\r\n        printf(\"%d %d %d\\n\", i, j, max);\r\n    }\r\n\r\n    return 0;\r\n}", "Konwledge_Point": "应对NP完全问题", "Question": "The 3n + 1 problem           \nProblem Description\n\nProblems in Computer Science are often classified as belonging to a certain class of problems (e.g., NP, Unsolvable, Recursive). In this problem you will be analyzing a property of an algorithm whose classification is not known for all possible inputs.\n\n\n\nConsider the following algorithm: \n\n\n\n1.      input n\n\n2.      print n\n\n3.      if n = 1 then STOP\n\n4.           if n is odd then n <- 3n + 1\n\n5.           else n <- n / 2\n\n6.      GOTO 2\n\n\n\n\nGiven the input 22, the following sequence of numbers will be printed 22 11 34 17 52 26 13 40 20 10 5 16 8 4 2 1 \n\n\n\nIt is conjectured that the algorithm above will terminate (when a 1 is printed) for any integral input value. Despite the simplicity of the algorithm, it is unknown whether this conjecture is true. It has been verified, however, for all integers n such that 0 < n < 1,000,000 (and, in fact, for many more numbers than this.) \n\n\n\nGiven an input n, it is possible to determine the number of numbers printed (including the 1). For a given n this is called the cycle-length of n. In the example above, the cycle length of 22 is 16. \n\n\n\nFor any two numbers i and j you are to determine the maximum cycle length over all numbers between i and j. \n\n\n\nInput\n\nThe input will consist of a series of pairs of integers i and j, one pair of integers per line. All integers will be less than 1,000,000 and greater than 0. \n\n\n\nYou should process all pairs of integers and for each pair determine the maximum cycle length over all integers between and including i and j. \n\n\n\nYou can assume that no opperation overflows a 32-bit integer.\n\n\n\nOutput\n\nFor each pair of input integers i and j you should output i, j, and the maximum cycle length for integers between and including i and j. These three numbers should be separated by at least one space with all three numbers on one line and with one line of output for each line of input. The integers i and j must appear in the output in the same order in which they appeared in the input and should be followed by the maximum cycle length (on the same line). \n\n\n\nSample Input\n\n1 10\n\n100 200\n\n201 210\n\n900 1000\n\n\n\nSample Output\n\n1 10 20\n\n100 200 125\n\n201 210 89\n\n900 1000 174", "Tag": "算法分析"}
{"Answer": "需要转化，拟合函数需要是线性的时候才可以使用最小二乘法", "Konwledge_Point": "应对NP完全问题", "Question": "y=a*exp(b*x)这个式子怎么在python中作为条件编写（使用最小二乘法拟合的参数估计程序）\n问题遇到的现象和发生背景\n\n\ny=a\nexp(b\nx)这个式子怎么在python中作为条件编写，这是一个使用最小二乘法拟合的参数估计程序。\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport numpy as np\nimport matplotlib\n.pylab\n as plt\nimport statsmodels\n.api\n as sm\nimport math\n\nexp=np\n.loadtxt\n(\n'5.dat'\n,delimiter=\n','\n)\n\nn=len(exp\n[:,0]\n)\nave_x=np\n.average\n(exp\n[:,0]\n)\nave_y=np\n.average\n(exp\n[:,1]\n)\n\nSxx=np\n.sum\n((exp\n[:,0]\n-ave_x)*(exp\n[:,0]\n-ave_x))/n\nSyy=np\n.sum\n((exp\n[:,1]\n-ave_y)*(exp\n[:,1]\n-ave_y))/n\nSxy=np\n.sum\n((exp\n[:,0]\n-ave_x)*(exp\n[:,1]\n-ave_y))/n\n\nS=np\n.cov\n(exp,rowvar=False, bias=True)\n\n\nprint\n(Sxx,S[\n0\n,\n0\n])\n\n\nprint\n(Syy,S[\n1\n,\n1\n])\n\n\nprint\n(Sxy,S[\n0\n,\n1\n])\n\n\nb=Sxy/Sxx\na=ave_y-b*ave_x\n\nx=np\n.sort\n(exp\n[:,0]\n)\ny=a*exp(b*x) #这里出错了\n\nplt\n.plot\n(exp\n[:,0]\n,exp\n[:,1]\n,linestyle=\n'None'\n,marker=\n'x'\n)\nplt\n.plot\n(x,y,\ncolor\n=\n'r'\n)\nplt\n.show\n()\n\nres_exp=Syy*n\nres_model=np\n.sum\n((y-ave_y)*(y-ave_y))\nR2=res_model/res_exp\n\nsigma_y=np\n.sqrt\n(np\n.sum\n((exp\n[:,1]\n-a-b*exp\n[:,0]\n)*(exp\n[:,1]\n-a-b*exp\n[:,0]\n))/(n-\n2\n))\nsigma_b=sigma_y/np\n.sqrt\n(n*Sxx)\nsigma_a=sigma_y*np\n.sqrt\n((ave_x*ave_x+Sxx)/(n*Sxx))\n\n\nprint\n(sigma_a,sigma_b)\n\n\nx_add_const=sm\n.add_constant\n(exp\n[:,0]\n)\nresult=sm\n.OLS\n(exp\n[:,1]\n,x_add_const)\n.fit\n()\n\n\nprint\n(result.summary()\n)\n\n\nprint\n(\n'Parameters: '\n,result.params)\n\n\nprint\n(\n'Standard error: '\n,result.bse)\n\n\nprint\n(\n'Coefficient of determination '\n,result.rsquared)\n\n\ny=result\n.params\n[0]\n+result\n.params\n[1]\n*x\nplt\n.plot\n(exp\n[:,0]\n,exp\n[:,1]\n,linestyle=\n'None'\n,marker=\n'x'\n)\nplt\n.plot\n(x,y,\ncolor\n=\n'r'\n)\nplt\n.show\n()\n\n\n\n\n运行结果及报错内容\n\n\nTraceback (most recent call last):\n  File \"E:/python practice/555/5-1.py\", line 26, in \n    y=a\nexp(b\nx)\nTypeError: 'numpy.ndarray' object is not callable\n\n\n我的解答思路和尝试过的方法\n\n\n是否要化成这样来写程序？\nln(𝑦) = ln(𝑎exp 𝑏𝑥 )\nln 𝑦 = ln 𝑎 + ln exp 𝑏𝑥 = ln 𝑎 + 𝑏𝑥\n𝑦́ = 𝑎́ + 𝑏𝑥\n𝑎 ± 𝜎6 = exp 𝑎G± 𝜎6́ = exp 𝑎́ ± exp 𝑎́ 𝜎6\n\n\n我想要达到的结果\n\n\n我想要在y=aexp(bx)的条件下完成a、b误差的计算。", "Tag": "算法分析"}
{"Answer": "import matplotlib.pyplot as plt\nimport numpy as np\nfig=plt.figure()\naxes3d= fig.add_subplot(111, projection='3d')\nzs = [1, 5, 10, 15, 20]\nfor z in zs:\n    x = np.arange(0, 10)\n    y = np.random.randint(0, 30, size=10)\n    axes3d.bar(x, y,zs=z, zdir='x', color=['r', 'green', 'yellow', 'c'])\n    axes3d.set_xlabel('X Label')\n    axes3d.set_ylabel('Y Label')\n    axes3d.set_zlabel('Z Label')\nplt.show()\n", "Konwledge_Point": "应对NP完全问题", "Question": "Spyder画3D柱形图只显示figure size 432*288 with 0Axe,却没有图片s\n代码\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D\nimport numpy as np\nfig=plt.figure()\naxes3d=Axes3D(fig)\nzs = [1, 5, 10, 15, 20]\nfor z in zs:\n    x = np.arange(0, 10)\n    y = np.random.randint(0, 30, size=10)\n    axes3d.bar(x, y,zs=z, zdir='x', color=['r', 'green', 'yellow', 'c'])\n结果\n", "Tag": "算法分析"}
{"Answer": "可以", "Konwledge_Point": "应对NP完全问题", "Question": "Python的if   else问题\n\n        \nif\n username == \n\"\"\n or password == \n\"\"\n:\n            wx\n.MessageBox\n(\n'用户名和密码不能为空！'\n)\n        \nelse\n:\n            \nfor\n \ni\n \nin\n np:\n                \nif\n (\ni\n[1]\n == username) and (\ni\n[2]\n == password):\n                    login_sign = \n1\n\n                    break\n\n\n\n可以这样在else后接入for吗", "Tag": "算法分析"}
{"Answer": "其实是同样的原理，zip会把可迭代对象对应位置的元素组合成一个元组，你这里的zip里面第二个参数虽然是二维的，但也可以看作是一维数组的数组，每个元素是一个一维数组，这样子应该就好理解了吧。zip把第一个可迭代对象的值和第二个可迭代对象对应位置的一维数组组合成元组，结果是一个元组列表，再传递给dict，生成一个字典，字典的key为元组第一个元素，其value为相应元组的第二个元素\n", "Konwledge_Point": "应对NP完全问题", "Question": "pandas 中zip的使用\n最后一行代码啥意思，zip里面，users_ratings.index不是就一维吗，后面np.random.rand(len(users_ratings),10).astype(np.float32) 应该是二维，调用zip发生了啥，然后外面再套一层就是把这个转换为字典是吧？主要是调用zip时发生了啥，我以前用zip函数都是同样的维度\n\n\ndtype=[(\n'userId'\n,np.\nint32\n),(\n'movieId'\n,np.\nint32\n),(\n'rating'\n,np.\nfloat32\n)]\ndataset=pd.read_csv(DATA_PATH,usecols=\nrange\n(\n3\n),dtype=dtype) # 读取csv文件\nusers_ratings=dataset.groupby(\n'userId'\n).agg([list])\nP = dict(zip(users_ratings.index,np.random.rand(\nlen\n(users_ratings),\n10\n).astype(np.\nfloat32\n)\n        ))\n\n\n\nprint(users_ratings)，对于users_ratings输出如下\n\n\n\n\ncsv文件数据如下\n\n", "Tag": "算法分析"}
{"Answer": "你只是定义了函数,但是没有使用在下面添加\ncontent_l_numpy()\ncontent_l_math()\n\n两行调用即可有帮助望采纳~", "Konwledge_Point": "应对NP完全问题", "Question": "计算0到10之间数的正弦值没有输出\n\n\n\nimport \ntime\n\nimport numpy as \nnp\n\nimport math\ndef content_l_numpy():\n    \nfor\n i \nin\n \nnp\n.logspace(\n0\n,\n8\n,\n9\n):\n        t1 = \ntime\n.\ntime\n()\n        \nfor\n j \nin\n \nnp\n.linspace(\n0\n,\n10\n,int(i)):\n             \nprint\n(\nnp\n.\nsin\n(j))\n             \nprint\n(\n\"取点数10^%d耗时→%s\"\n%\n(math.\nlog\n(i,\n10\n),\ntime\n.\ntime\n()-t1))\n          \n            \ndef content_l_math():\n    \nfor\n i \nin\n \nnp\n.logspace(\n0\n,\n8\n,\n9\n):\n        t2 = \ntime\n.\ntime\n()\n        x = \nnp\n.arange(\n0\n,\n10\n,\n10\n/i)\n        math.\nsin\n(x)\n        \nprint\n(\n\"取点数10^%d耗时→%s\"\n%\n(math.\nlog\n(i,\n10\n),\ntime\n.\ntime\n()-t2))\n", "Tag": "算法分析"}
{"Answer": "path = '16.png'\r\ndef compute(path):\r\n    \r\n    image_Rmean = []\r\n    image_Gmean = []\r\n    image_Bmean = []\r\n    img = cv2.imread(path, 1)\r\n    image_Bmean.append(np.mean(img[:,:,0]))\r\n    image_Gmean.append(np.mean(img[:,:,1]))\r\n    image_Rmean.append(np.mean(img[:,:,2]))\r\n    R_mean = np.mean(image_Rmean)\r\n    G_mean = np.mean(image_Gmean)\r\n    B_mean = np.mean(image_Bmean)\r\n    return R_mean, G_mean, B_mean\r\n \r\nif __name__ == '__main__':\r\n    R, G, B= compute(path)\r\n    Y = 0.299*R+0.587*G+0.114*B\r\n    print(\"目标图片亮度为\"+str(round(Y,2)))", "Konwledge_Point": "应对NP完全问题", "Question": "Python实现计算图像RGB均值,怎么只读一张图片？\n网上用python实现计算图像RGB均值都是批量的，而我只想得到一张图片的rgb均值（即整张图片三个通道的三个均值）如下的代码是一位大神的代码，对于刚学python的小白不会修改，求大神帮助。总之就是将下面代码从批量变为只分析一张图片的。问题应该简单，原谅我的无知。这是源码地址：\nhttps://blog.csdn.net/yql_617540298/article/details/83617512\n\n\n\n-*- coding: utf-8 -*-\n\n\n\n\"\"\"\n\nCreated on Thu Nov  1 10:43:29 2018\n\n@author: Administrator\n\n\"\"\"\n\n\n\nimport os\n\nimport cv2\n\nimport numpy as np\n\n\n\npath = 'C:/Users/Administrator/Desktop/rgb'\n\ndef compute(path):\n\n    file_names = os.listdir(path)\n\n    per_image_Rmean = []\n\n    per_image_Gmean = []\n\n    per_image_Bmean = []\n\n    for file_name in file_names:\n\n        img = cv2.imread(os.path.join(path, file_name), 1)\n\n        per_image_Bmean.append(np.mean(img[:,:,0]))\n\n        per_image_Gmean.append(np.mean(img[:,:,1]))\n\n        per_image_Rmean.append(np.mean(img[:,:,2]))\n\n    R_mean = np.mean(per_image_Rmean)\n\n    G_mean = np.mean(per_image_Gmean)\n\n    B_mean = np.mean(per_image_Bmean)\n\n    return R_mean, G_mean, B_mean\n\n\n\nif \nname\n == '__main__':\n\n    R, G, B= compute(path)\n\n    print(R, G ,B)", "Tag": "算法分析"}
{"Answer": "https://www.zhihu.com/question/27022786", "Konwledge_Point": "应对NP完全问题", "Question": "关于SimpleCV和opencv的问题\n环境为mac下的py2.7，在使用simplecv的时候出现提示：ImportError: Cannot load OpenCV library which is required by SimpleCV\n\njupyter的提示为：\n\n“\n\n/Users/sapphire/_Work space/颈动脉/图像分类代码/featuresHOG.py in ()\n\n      5 import numpy as np\n\n      6 import cv2  # opencv 2\n\n----> 7 from SimpleCV import *\n\n      8 import skimage\n\n      9 \n\n\n\n/Users/sapphire/anaconda/envs/python27/lib/python2.7/site-packages/SimpleCV/__init__.py in ()\n\n      1 \nversion\n = '1.3.0'\n\n      2 \n\n----> 3 from SimpleCV.base import *\n\n      4 from SimpleCV.Camera import *\n\n      5 from SimpleCV.Color import *\n\n\n\n/Users/sapphire/anaconda/envs/python27/lib/python2.7/site-packages/SimpleCV/base.py in ()\n\n     57         import cv\n\n     58     except ImportError:\n\n---> 59         raise ImportError(\"Cannot load OpenCV library which is required by SimpleCV\")\n\n     60 \n\n     61 #optional libraries\n\n\n\nImportError: Cannot load OpenCV library which is required by SimpleCV\n\n”\n\n请问该如何解决这个问题？", "Tag": "算法分析"}
{"Answer": "从你输出的数据来看，你的all_tag不是一个纯粹的np数据，里面的一些元素是list对象，所以才导致不能用1-all_tag。所以，你只能是通过两个for循环来实现 self.tags['empty'] 的计算，或者可以尝试一下下面的代码： self.tags['empty'] = [1-np.array(v) for k, v in self.tags.items() if len(v) > 0 ]", "Konwledge_Point": "应对NP完全问题", "Question": "TypeError: unsupported operand type(s) for -: 'int' and 'list'\npython 已转换为np.ndarray类型，仍然报错TypeError: unsupported operand type(s) for -: 'int' and 'list'\n\n\n源代码如下\n\n\n        all_tag = np\n.array\n(\n[v for k, v in self.tags.items() if len(v) > 0 ]\n)\n        self\n.tags\n[\n'empty'\n]\n = np\n.all\n(\n1\n - all_tag, axis=\n1\n)\n.astype\n(np.int32)\n.tolist\n()\n\n\n\n报错如下\n\n\nTraceback (most recent \ncall\n \nlast\n):\n  File \n\"../../tools/eval.py\"\n, \nline\n \n36\n, in \n\n\n    dataset = VOTDataset(\nargs\n.dataset, root)\n  File \n\"/home/cao/桌面/Code/SiamMask/utils/pysot/datasets/vot.py\"\n, \nline\n \n127\n, in __init__\n    meta_data[video][\n'height'\n])\n  File \n\"/home/cao/桌面/Code/SiamMask/utils/pysot/datasets/vot.py\"\n, \nline\n \n52\n, in __init__\n    self.\ntags\n[\n'empty'\n] = np.\nall\n(\n1\n - all_tag, axis=\n1\n).astype(np.int32).tolist()\nTypeError: unsupported operand \ntype\n(s) \nfor\n -: \n'int'\n \nand\n \n'list'\n\n\n\n\n明明已经转换为ndarray类型了为什么报错仍然是int和list类型错误，np的广播机制为什么没有见效？", "Tag": "算法分析"}
{"Answer": "N/2会被算成小数，你看看用的是不是python3.7。只有2.7会按整数算，后面的都是按浮点数算，应该写成N//2，这样是取整的意思。", "Konwledge_Point": "应对NP完全问题", "Question": "sorted 函数bug，有没有大神帮忙指点下这是什么错误\nimport numpy as np\n\n\n\nc = np.loadtxt('data.csv', delimiter = ',', usecols = (6, ), unpack = True)\n\nprint (\"median =\", np.median(c))\n\nsorted = np.msort(c)\n\nprint (\"sorted =\", sorted)\n\n\n\nN = len(c)\n\nprint(\"middle =\", sorted[(N - 1)/2])\n\nprint(\"average middle =\", (sorted[N / 2] + sorted[(N - 1) / 2]) / 2)\n\n\n\nprint(\"variance =\", np.var(c))\n\nprint(\"variance from definition =\", np.mean((c - c.mean()) ** 2))\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "赋值的0.6改为1，把最后一个else改为 if a > b:，条件清晰明了，有用记得采纳\nimport numpy as np\n\n# Evaluator1 - Repeatability\n# Deviation\nDevice1_deviation = 0.06 - 0.05\nDevice2_deviation = 0.06 - 0.05\nDevice3_deviation = 0.3 - 0.05\nNumberOfCandidateDevices = 3\nDeviceEvaluatorList = []\n\nDeviceEvaluatorList.append(Device1_deviation)\nDeviceEvaluatorList.append(Device2_deviation)\nDeviceEvaluatorList.append(Device3_deviation)\nB = np.empty((NumberOfCandidateDevices, NumberOfCandidateDevices))\nfor i in range(1, NumberOfCandidateDevices + 1):\n    for j in range(1, NumberOfCandidateDevices + 1):\n        a = DeviceEvaluatorList[i - 1]\n        b = DeviceEvaluatorList[j - 1]\n        if a < b:\n            B[i - 1, j - 1] = 1\n        if a == b:\n            B[i - 1, j - 1] = 0.5\n        if a > b:\n            B[i - 1, j - 1] = 0\nprint(B)\n", "Konwledge_Point": "应对NP完全问题", "Question": "python 矩阵运算出问题\nNumberOfCandidateDevices = 3\npython矩阵运算出问题。 按照代码逻辑， 应该会得到这样的矩阵\n\n\n\n\n#Evaluator1 - Repeatability\n\n\n#Deviation\n\n\nDevice1_deviation\n = \n0\n.\n06\n - \n0\n.\n05\n\n\nDevice2_deviation\n = \n0\n.\n06\n - \n0\n.\n05\n\n\nDevice3_deviation\n = \n0\n.\n3\n - \n0\n.\n05\n\n\n\nDeviceEvaluatorList\n =\n []\n\n\nDeviceEvaluatorList\n.append(Device1_deviation)\n\nDeviceEvaluatorList\n.append(Device2_deviation)\n\nDeviceEvaluatorList\n.append(Device3_deviation)\n\nB\n =np.empty((NumberOfCandidateDevices,NumberOfCandidateDevices))\n\nfor\n i in range(\n1\n, NumberOfCandidateDevices+\n1\n):\n    \nfor\n j in range(\n1\n, NumberOfCandidateDevices+\n1\n):\n        \na\n = DeviceEvaluatorList[i-\n1\n]\n        \nb\n = DeviceEvaluatorList[j-\n1\n]\n        \nif\n a < b:\n            \nB\n[i-\n1\n,j-\n1\n] = \n0\n.\n6\n\n        \nif\n a == b:\n            \nB\n[i-\n1\n,j-\n1\n] = \n0\n.\n5\n\n        \nelse\n:\n            \nB\n[i-\n1\n,j-\n1\n] = \n0\n\n\nB\n\n\n\n\n但是实际运算出的矩阵如下。即最右边前两行应该为1，但是算出来却为0. 可否解释下原因，并修正下代码\n\n\n\n\n除此之外想把生成 的B 转换成矩阵，如何操作？", "Tag": "算法分析"}
{"Answer": "调用文件时使用绝对路径，引号前加r，比如文件test.txt在D盘test文件夹，调用的时候用r\"D:\\test\\test.txt\"", "Konwledge_Point": "应对NP完全问题", "Question": "文件明明存在在路径中，为什么还会报错？\n遇到报错：tensorflow.python.framework.errors_impl.NotFoundError: E:\\pcrnet-master\\utils\\pc_distance\\tf_nndistance_so.so not found\n\n\n具体报错如下：\n\n\nConnected to pydev debugger (build \n201.8743\n.20\n)\n\nD\n:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.\npy\n:\n516\n: \nFutureWarning\n: Passing (\ntype\n, \n1\n) \nor\n \n'1type'\n as a synonym of \ntype\n is deprecated; \nin\n a future version of numpy, it will be understood as (\ntype\n, (\n1\n,)) / \n'(1,)type'\n.\n  _np_qint8 = np.dtype([(\n\"qint8\"\n, np.int8, \n1\n)])\n\nD\n:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.\npy\n:\n517\n: \nFutureWarning\n: Passing (\ntype\n, \n1\n) \nor\n \n'1type'\n as a synonym of \ntype\n is deprecated; \nin\n a future version of numpy, it will be understood as (\ntype\n, (\n1\n,)) / \n'(1,)type'\n.\n  _np_quint8 = np.dtype([(\n\"quint8\"\n, np.uint8, \n1\n)])\n\nD\n:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.\npy\n:\n518\n: \nFutureWarning\n: Passing (\ntype\n, \n1\n) \nor\n \n'1type'\n as a synonym of \ntype\n is deprecated; \nin\n a future version of numpy, it will be understood as (\ntype\n, (\n1\n,)) / \n'(1,)type'\n.\n  _np_qint16 = np.dtype([(\n\"qint16\"\n, np.int16, \n1\n)])\n\nD\n:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.\npy\n:\n519\n: \nFutureWarning\n: Passing (\ntype\n, \n1\n) \nor\n \n'1type'\n as a synonym of \ntype\n is deprecated; \nin\n a future version of numpy, it will be understood as (\ntype\n, (\n1\n,)) / \n'(1,)type'\n.\n  _np_quint16 = np.dtype([(\n\"quint16\"\n, np.uint16, \n1\n)])\n\nD\n:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.\npy\n:\n520\n: \nFutureWarning\n: Passing (\ntype\n, \n1\n) \nor\n \n'1type'\n as a synonym of \ntype\n is deprecated; \nin\n a future version of numpy, it will be understood as (\ntype\n, (\n1\n,)) / \n'(1,)type'\n.\n  _np_qint32 = np.dtype([(\n\"qint32\"\n, np.int32, \n1\n)])\n\nD\n:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.\npy\n:\n525\n: \nFutureWarning\n: Passing (\ntype\n, \n1\n) \nor\n \n'1type'\n as a synonym of \ntype\n is deprecated; \nin\n a future version of numpy, it will be understood as (\ntype\n, (\n1\n,)) / \n'(1,)type'\n.\n  np_resource = np.dtype([(\n\"resource\"\n, np.ubyte, \n1\n)])\n\nD\n:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.\npy\n:\n541\n: \nFutureWarning\n: Passing (\ntype\n, \n1\n) \nor\n \n'1type'\n as a synonym of \ntype\n is deprecated; \nin\n a future version of numpy, it will be understood as (\ntype\n, (\n1\n,)) / \n'(1,)type'\n.\n  _np_qint8 = np.dtype([(\n\"qint8\"\n, np.int8, \n1\n)])\n\nD\n:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.\npy\n:\n542\n: \nFutureWarning\n: Passing (\ntype\n, \n1\n) \nor\n \n'1type'\n as a synonym of \ntype\n is deprecated; \nin\n a future version of numpy, it will be understood as (\ntype\n, (\n1\n,)) / \n'(1,)type'\n.\n  _np_quint8 = np.dtype([(\n\"quint8\"\n, np.uint8, \n1\n)])\n\nD\n:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.\npy\n:\n543\n: \nFutureWarning\n: Passing (\ntype\n, \n1\n) \nor\n \n'1type'\n as a synonym of \ntype\n is deprecated; \nin\n a future version of numpy, it will be understood as (\ntype\n, (\n1\n,)) / \n'(1,)type'\n.\n  _np_qint16 = np.dtype([(\n\"qint16\"\n, np.int16, \n1\n)])\n\nD\n:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.\npy\n:\n544\n: \nFutureWarning\n: Passing (\ntype\n, \n1\n) \nor\n \n'1type'\n as a synonym of \ntype\n is deprecated; \nin\n a future version of numpy, it will be understood as (\ntype\n, (\n1\n,)) / \n'(1,)type'\n.\n  _np_quint16 = np.dtype([(\n\"quint16\"\n, np.uint16, \n1\n)])\n\nD\n:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.\npy\n:\n545\n: \nFutureWarning\n: Passing (\ntype\n, \n1\n) \nor\n \n'1type'\n as a synonym of \ntype\n is deprecated; \nin\n a future version of numpy, it will be understood as (\ntype\n, (\n1\n,)) / \n'(1,)type'\n.\n  _np_qint32 = np.dtype([(\n\"qint32\"\n, np.int32, \n1\n)])\n\nD\n:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.\npy\n:\n550\n: \nFutureWarning\n: Passing (\ntype\n, \n1\n) \nor\n \n'1type'\n as a synonym of \ntype\n is deprecated; \nin\n a future version of numpy, it will be understood as (\ntype\n, (\n1\n,)) / \n'(1,)type'\n.\n  np_resource = np.dtype([(\n\"resource\"\n, np.ubyte, \n1\n)])\nTraceback (most recent call last):\n  File \n\"\"\n, line \n983\n, \nin\n _find_and_load\n  File \n\"\"\n, line \n967\n, \nin\n _find_and_load_unlocked\n  File \n\"\"\n, line \n677\n, \nin\n _load_unlocked\n  File \n\"\"\n, line \n728\n, \nin\n exec_module\n  File \n\"\"\n, line \n219\n, \nin\n _call_with_frames_removed\n  File \n\"E:\\点云配准\\神经网络\\PCRNet\\pcrnet-master\\models\\pcr_model.py\"\n, line \n12\n, \nin\n <\nmodule\n>\n    \nimport\n tf_util_loss\n  File \n\"E:\\pcrnet-master\\utils\\tf_util_loss.py\"\n, line \n2\n, \nin\n <\nmodule\n>\n    \nfrom\n pc_distance \nimport\n tf_nndistance, tf_approxmatch\n  File \n\"E:\\pcrnet-master\\utils\\pc_distance\\tf_nndistance.py\"\n, line \n5\n, \nin\n <\nmodule\n>\n    nn_distance_module=tf.load_op_library(\nos\n.path.join(BASE_DIR, \n'tf_nndistance_so.so'\n))\n  File \n\"D:\\anaconda\\envs\\p37\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\"\n, line \n61\n, \nin\n load_op_library\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\ntensorflow.python.framework.errors_impl.\nNotFoundError\n: \nE\n:\\点云配准\\神经网络\\PCRNet\\pcrnet-master\\utils\\pc_distance\\tf_nndistance_so.so \nnot\n found\n\nProcess finished with exit code \n1\n\n\n\n", "Tag": "算法分析"}
{"Answer": "我复现了你的问题，是matplotlib版本问题，你的版本太高了。我测试的另一个版本3.3.4可以显示结果。\n3.3.4可以显示。3.5.0不能显示", "Konwledge_Point": "应对NP完全问题", "Question": "机器学习方法自动分类遇到plt显示空白图的问题\n在完成题目时遇到这个问题，代码跟答案给出的一模一样，但就是画不出来图，而且刻度还有重叠\n\n\n这些小数的刻度不知道从哪里冒出来的\n整段代码如下：\n\n\nimport\n numpy as np\n\nimport\n matplotlib\n\nimport\n matplotlib.pyplot as plt\n\nfrom\n sklearn.linear_model import LogisticRegression\n\nfrom\n sklearn.model_selection import train_test_split\n\nfrom\n sklearn.datasets import make_classification\n\nX\n,y=make_classification(n_samples=\n100\n,n_features=\n2\n,n_redundant=\n0\n,random_state=\n42\n)\n\ntrain_X\n,test_X,train_y,test_y=train_test_split(X,y,random_state=\n42\n)\n\nmodel\n=LogisticRegression()\n\nmodel\n.fit(train_X,train_y)\n\npred_y\n=model.predict(test_X)\n\nprint\n(model.score(test_X,test_y))\n\n\nplt\n.scatter(X[:,\n0\n],X[:,\n1\n],c=y,marker='.',cmap=matplotlib.cm.get_cmap(name='bwr'),alpha=\n0\n.\n7\n)\n\nXi\n=np.linspace(-\n10\n,\n10\n)\n\nY\n=-model.coef_[\n0\n][\n0\n]/model.coef_[\n0\n][\n1\n]* Xi-model.intercept_/model.coef_[\n0\n][\n1\n]\n\nplt\n.plot(Xi,Y)\n\nprint\n(model.coef_)\n\nprint\n(model.intercept_)\n\nprint\n(model.coef_[\n0\n][\n0\n])\n\nprint\n(model.coef_[\n0\n][\n1\n])\n\n\nplt\n.xlim(min(X[:,\n0\n])-\n0\n.\n5\n,max(X[:,\n0\n])+\n0\n.\n5\n)\n\nplt\n.ylim(min(X[:,\n1\n])-\n0\n.\n5\n,max(X[:,\n1\n])+\n0\n.\n5\n)\n\nplt\n.axes().set_aspect(\n\"equal\"\n,\n\"datalim\"\n)\n\nplt\n.title(\n\"classification data using LogisticRegression\"\n)\n\nplt\n.xlabel('x-axis')\n\nplt\n.ylabel('y-axis')\n\nplt\n.show()\n\n\n\n\n问题大概如上，已经被困扰很久了，希望能有人帮忙找出问题所在，感激不尽！！", "Tag": "算法分析"}
{"Answer": "复制的路径中存在不可见字符，将路径删除后手动输入一下试试，看下可否消除非法字符读取到文件。", "Konwledge_Point": "应对NP完全问题", "Question": "python图像处理疑问\n这段代码是我学习numpy库的时候直接抄的代码，但是粘贴到anaconda的notebook后系统报错，想问一下为什么\n这个是错误\nOSError                                   Traceback (most recent call last)\n in \n      2 import numpy as np\n      3\n----> 4 a = np.asarray(Image.open('‪D:/desktop/hqh.JPG').convert('L')).astype('float')\n      5\n      6 depth = 10. # (0-100)\n\n\nD:\\Anaconda\\lib\\site-packages\\PIL\\Image.py in open(fp, mode, formats)\n   2910\n   2911     if filename:\n-> 2912         fp = builtins.open(filename, \"rb\")\n   2913         exclusive_fp = True\n   2914 \n\n\nOSError: [Errno 22] Invalid argument: '\\u202aD:/desktop/hqh.JPG'\n\n\n代码如下\n\n\nfrom PIL \nimport\n Image\n\nimport\n numpy as np\n\na\n = np.asarray(Image.open('‪D:/desktop/hqh.JPG').convert('L')).astype('float')\n\ndepth\n = \n10\n. \n# (0-100)\n\n\ngrad\n = np.gradient(a)\n#取图像灰度的梯度值\n\ngrad_x, \ngrad_y\n = grad \n#分别取横纵图像梯度值\n\n\ngrad_x\n = grad_x*depth/\n100\n.\n\ngrad_y\n = grad_y*depth/\n100\n.\n\nA\n = np.sqrt(grad_x**\n2\n + grad_y**\n2\n + \n1\n.)\n\nuni_x\n = grad_x/A\n\nuni_y\n = grad_y/A\n\nuni_z\n = \n1\n./A\n\n\nvec_el\n = np.pi/\n2.2\n \n# 光源的俯视角度，弧度值\n\n\nvec_az\n = np.pi/\n4\n. \n# 光源的方位角度，弧度值\n\n\ndx\n = np.cos(vec_el)*np.cos(vec_az) \n#光源对x 轴的影响\n\n\ndy\n = np.cos(vec_el)*np.sin(vec_az) \n#光源对y 轴的影响\n\n\ndz\n = np.sin(vec_el) \n#光源对z 轴的影响\n\n\n\nb\n = \n255\n*(dx*uni_x + dy*uni_y + dz*uni_z) \n#光源归一化\n\n\nb\n = b.clip(\n0\n,\n255\n)\n\n\nim\n = Image.fromarray(b.astype('uint8')) \n#重构图像\n\nim.save('‪D:/desktop/hqhHD.JPG')\n\n", "Tag": "算法分析"}
{"Answer": "rand是随机生成值在0-1之间的函数；randn是随机生成均值为0，方差为1的正态分布上的数值。看这个图体会下区别：\n\n\n有帮助的话，请点采纳该答案~\n", "Konwledge_Point": "应对NP完全问题", "Question": "python中pandas库生成二维数组的使用\n\ndates = pd.date_range(\nstart\n=\n'20191101'\n, \nend\n=\n'20191124'\n, \nfreq\n=\n'D'\n)              \na1 = pd.DataFrame(np.random.randn(24, 4), \nindex\n=dates, \ncolumns\n=list('ABCD'))   \na2 = pd.DataFrame(np.random.rand(24, 4))                                       \n\n\n\n(np.random.randn(24, 4)和(np.random.rand(24, 4))  中的randn与rand有什么区别", "Tag": "算法分析"}
{"Answer": "转换后的data前面的1,2,3事行号吗?\noutdata=\"\"\nfor i in list(data):\n    # 转换为字符串\n    outdata+=\",\".join([str(s) for s in i])\n    # 末尾添加逗号+换行\n    outdata+=',\\n'\n", "Konwledge_Point": "应对NP完全问题", "Question": "python使用pandas库读取数据格式转换问题\n问题遇到的现象和发生背景\n\n\n问题相关代码，请勿粘贴截图\n\n\n\n#从excel中读取数据并转换为矩阵\ndatA=pd.read_excel(r'C:\\Users\\\n49175\n\\Desktop\\jzgb.xlsx')\ndata=np.array(datA,dtype=np.int64)\nprint(data)\n#上述代码读取数据在python中的结果如下所示\n#[[\n41703072\n       \n85\n       \n83\n ...       \n92\n       \n91\n       \n79\n]\n [\n41703100\n       \n78\n       \n83\n ...       \n87\n       \n75\n       \n84\n]\n [\n41707033\n       \n91\n       \n79\n ...       \n88\n       \n83\n       \n77\n]\n ...\n [\n41724096\n       \n83\n       \n73\n ...       \n61\n       \n93\n       \n78\n]\n [\n41724098\n       \n88\n       \n83\n ...       \n70\n       \n64\n       \n78\n]\n [\n41724099\n       \n87\n       \n73\n ...       \n65\n       \n75\n       \n64\n]]\n\n#如何在python中把上面代码读取出来的excel数据转换成下面这种数值格式\n\n\n#数据集：\ndata = \"\"\"\n\n1,88,89,85\n,\n79,98,95,82\n,\n45,95,65,59\n,\n88,49,67,85\n,\n75,72,73,71\n,\n88,89,89,87\n,\n85,84,86,86\n,\n85,85,85,85\n,\n85,85,85,85\n,\n84,81,84,82\n,\n\n2,71,72,73\n,\n74,75,76,84\n,\n89,95,91,70\n,\n64,52,85,14\n,\n16,74,95,84\n,\n89,45,65,85\n,\n75,95,44,61\n,\n60,84,59,65\n,\n87,91,99,98\n,\n54,67,88,72\n,\n\n3,89,85,65\n,\n44,85,75,86\n,\n67,95,84,67\n,\n95,75,78,78\n,\n76,84,81,89\n,\n85,88,95,96\n,\n94,73,85,78\n,\n85,86,84,89\n,\n92,87,86,88\n,\n81,91,66,84\n,\"\"\"\n\n\n\n运行结果及报错内容\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "你的函数没有返回值，没有return ,res 就是None", "Konwledge_Point": "应对NP完全问题", "Question": "Flask接口处理post时出现问题\nFlask接口post请求之后另一边只能收到None\n\n\n代码能够正确运行，并且使用在接口代码中加入print能够正确输出内容，但是使用测试接口的代码时只能收到None，而且在最后会出现一个\"POST / HTTP/1.1\" 500 -的信息\n\n\n\n\n这是接口的代码：\n\nimport\n json\n\nimport\n cv2\n\nfrom\n PIL \nimport\n Image\n\nimport\n numpy \nas\n np\n\nfrom\n modelscope.pipelines \nimport\n pipeline\n\nfrom\n modelscope.utils.constant \nimport\n Tasks\n\nfrom\n modelscope.outputs \nimport\n OutputKeys\n\nfrom\n flask \nimport\n request\n\nfrom\n flask \nimport\n jsonify\n\nfrom\n flask \nimport\n Flask,make_response\n\nimport\n base64\n\nface_recognition = pipeline(Tasks.face_recognition, model=\n'damo/cv_ir101_facerecognition_cfglint'\n)\n\n\n# def face_recognition(image1,image2):\n\n\n#     emb1 = face_recognition(image1)[OutputKeys.IMG_EMBEDDING]\n\n\n#     emb2 = face_recognition(image2)[OutputKeys.IMG_EMBEDDING]\n\n\n#     sim = np.dot(emb1[0], emb2[0])\n\n\n#     sim=face_recognition(image1, image2)\n\n\n#     return jsonify(sim)\n\n\n#     return (f'Face cosine similarity={sim:.3f}, get_img1:{image1}  get_img2:{image2}')\n\n\n\napp = Flask(__name__)\napp.config[\n\"JSON_AS_ASCII\"\n] = \nFalse\n\n\ndef\n \nmaking_response\n(\njson_data\n):\n    response = make_response(jsonify(json_data))\n    response.headers[\n'Content-Type'\n] = \n'application/json;charset=UTF-8'\n\n    \nreturn\n response\n\n\n@app.route(\n\"/\"\n,methods=[\n\"POST\"\n]\n)\n\n\ndef\n \nfirst_post\n():\n    my_json = {\n\"msg\"\n: \nNone\n,\n               \n\"sim\"\n: \nNone\n\n                }\n    data = request.get_data()\n    \nif\n \nnot\n data:\n        my_json[\n\"msg\"\n] = \n\"No data obtained!\"\n\n    \ntry\n:\n        data=json.loads(data)\n        image1 = base64.b64decode(data[\n\"image1\"\n].encode())\n        image2 = base64.b64decode(data[\n\"image2\"\n].encode())\n        \n# image1_base64 = data[\"image1\"].encode()\n\n        \n# image2_base64 = data[\"image2\"].encode()\n\n        image1 = cv2.imdecode(np.frombuffer(image1, np.uint8), cv2.IMREAD_ANYCOLOR) \n#cv2.IMREAD_UNCHANGED\n\n        image2 = cv2.imdecode(np.frombuffer(image2, np.uint8), cv2.IMREAD_ANYCOLOR)\n\n        emb1 = face_recognition(image1)[OutputKeys.IMG_EMBEDDING]\n        emb2 = face_recognition(image2)[OutputKeys.IMG_EMBEDDING]\n        sim = np.dot(emb1[\n0\n], emb2[\n0\n])\n        \nprint\n(sim)\n        my_json[\n\"msg\"\n] = \n\"successful!\"\n\n        my_json[\n\"sim\"\n] = sim\n\n        \n# face_recognition(image1,image2)\n\n    \nexcept\n Exception \nas\n e:\n        \nprint\n(e)\n        my_json[\n\"msg\"\n] = \n\"出错了，请检查是否正确访问!\"\n\n        response = making_response(my_json)\n        \nreturn\n response\n\n    response = make_response(jsonify(my_json))\n    response.headers[\n'Content-Type'\n] = \n'application/json;charset=UTF-8'\n\n    \nreturn\n response\n\n\nif\n __name__ == \n\"__main__\"\n:\n    app.run(host=\n'0.0.0.0'\n, port=\n12000\n, debug=\nTrue\n, use_reloader=\nFalse\n)\n\n这是我测试用的代码：\n\nimport\n os\n\nimport\n time\n\nimport\n json\n\nimport\n base64\n\nimport\n random\n\nimport\n requests\n\n\nIMG_FORMATS = [\n'bmp'\n, \n'jpg'\n, \n'jpeg'\n, \n'png'\n, \n'tif'\n, \n'tiff'\n, \n'dng'\n, \n'webp'\n, \n'mpo'\n]\n\n\n\n# 文件目录遍历，返回[fileP, fileN]\n\n\ndef\n \nget_filepaths\n(\npath\n):\n    pathlists = []\n    \nfor\n root, dirs, files \nin\n os.walk(path):\n        \nfor\n file \nin\n files:\n            pathlists.append([os.path.join(root, file), file])\n    \nreturn\n pathlists\n\n\n\nclass\n \napi_test\n:\n    \ndef\n \n__init__\n(\nself\n):\n        self.url = \n\"http://0.0.0.0:12000/\"\n  \n# nginx url\n\n\n\n    @staticmethod\n\n    \ndef\n \nread_img_base64\n(\np\n):\n        \nwith\n \nopen\n(p, \n'rb'\n) \nas\n f:\n            imgString = base64.b64encode(f.read())\n            base64_data = imgString.decode()\n        \nreturn\n base64_data\n\n    \ndef\n \nsend_post\n(\nself, img_path1,img_path2\n):\n        base64_data1 = self.read_img_base64(img_path1)\n        base64_data2 = self.read_img_base64(img_path2)\n        \n#image_name = str(img_path)\n\n        data = {\n            \n\"image1\"\n: base64_data1,\n            \n\"image2\"\n: base64_data2\n        }\n\n        session = requests.session()\n        start_time = time.time()\n        response = session.post(self.url, json.dumps(data))  \n# Json格式请求\n\n        end_time = time.time()\n        run_time = end_time - start_time\n        \n# logger.info('接口调用时间为：%s' % run_time)\n\n        \n# logger.info(response.status_code)\n\n        \n# logger.info(response.text)\n\n        session.close()\n        response.close()\n\n\n\nif\n __name__ == \n\"__main__\"\n:\n    \n# 开始单元测试\n\n    api = api_test()\n\n\n    \n# 单张图片测试\n\n    img_path1 = \n'./样本.jpg'\n\n    img_path2 = \n'./样本1.jpg'\n\n    res = api.send_post(img_path1,img_path2)\n    \nprint\n(res)\n\n\n###### \n\nPress CTRL+C to quit\n/opt/conda/lib/python3\n.7\n/site-packages/mmdet/core/anchor/anchor_generator.py:\n333\n: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` \n  warnings.warn(\n'``grid_anchors`` would be deprecated soon. '\n\n/opt/conda/lib/python3\n.7\n/site-packages/mmdet/core/anchor/anchor_generator.py:\n370\n: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` \n  \n'``single_level_grid_anchors`` would be deprecated soon. '\n\n\n127.0\n.0\n.1\n - - [\n29\n/Dec/\n2022\n \n11\n:\n20\n:\n27\n] \n\"POST / HTTP/1.1\"\n \n500\n -\n这是\nprint\n输出的结果：-\n0.11088603\n\n这是测试代码得到的结果：\nNone\n\n\n\n\n\n一头雾水。不知道是哪出了问题\n\n\n我想要测试代码访问时 能把输出的结果也就是代码中的sim这个值返回给访问的那一端\n\n\nPS：\n这个模型是魔塔社区的模型 我用来练习使用的，如果各位需要测试使用的话可以直接去\nhttps://www.modelscope.cn/models/damo/cv_ir101_facerecognition_cfglint/summary\n 打开使用测试", "Tag": "算法分析"}
{"Answer": "不是停不下来，而是你代码运行时长太长了。\n直到第一个函数运行完，第二个函数报错，已经跑了我一个多小时了", "Konwledge_Point": "应对NP完全问题", "Question": "按要求编写程序，运行结果停不下来\n\n\n\n\nimport \ntime\n\nimport numpy as np\nimport math\ndef content_l_numpy():\n    for i in np.\nlogspace\n(\n0\n,\n8\n,\n9\n):\n        t1 = time.\ntime\n()\n        for j in np.\nlinspace\n(\n0\n,\n10\n,\nint\n(i)):\n             \nprint\n(np.\nsin\n(j))\n             \nprint\n(\n\"取点数10^%d耗时→%s\"\n%(math.\nlog\n(i,\n10\n),time.\ntime\n()-t1))\n         \n def \ncontent_l_math\n():\n    for i in np.\nlogspace\n(\n0\n,\n8\n,\n9\n):\n        t2 = time.\ntime\n()\n        x = np.\narange\n(\n0\n,\n10\n,\n10\n/i)\n        math.\nsin\n(x)\n        \nprint\n(\n\"取点数10^%d耗时→%s\"\n%(math.\nlog\n(i,\n10\n),time.\ntime\n()-t2))\n\ncontent_l_numpy\n()\n\ncontent_l_math\n()\n", "Tag": "算法分析"}
{"Answer": "你好！一\n[[ 7, 10, 13],\n[21, 24, 27]]\n\n二\n[[ 2,  2,  4,  7],\n       [11, 11,  5,  3]]\n\n三\n[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n", "Konwledge_Point": "应对NP完全问题", "Question": "这些题目的结果是多少\n第一题：y = np.arange(35).reshape(5,7)\n\n\ny[1:5:2,::3]\n第二题：a=np.arange(12)\n\n\nj=np.array([[2,2,4,7],[-1,-1,5,3]])\n\n\na[j]\n第三题：y = np.arange(35).reshape(5,7)\n\n\nc=y>20\n\n\ny[c]", "Tag": "算法分析"}
{"Answer": "前后括号数量没对上", "Konwledge_Point": "应对NP完全问题", "Question": "关于#Python#绘制多条折线图的问题，如何解决？\n图片点开看，不糊的！\nn=input(\"请输入要查询的省份数：\")\nn=int(n)\nchnprovince=np.arange(n)\nc=np.arange(n)\nchnprovince=list(map(lambda x:str(x),chnprovince))\n\n\nchnprovince=input(\"请输入省份的拼音全称：\").split(\",\")\nc=(\n    Line(\n        init_opts=opts.InitOpts(bg_color='#282853',\n                                 width='1280px',\n                                 height='800px',\n                                 page_title='疫情数据',\n                                 theme=ThemeType.WONDERLAND,\n #WONDERLAND\n    )\n    )\n    .add_xaxis(xaxis_data=pd.to_datetime(y['Date']).dt.strftime('%Y-%m-%d').drop_duplicates().to_list(),\n              )\n    i=0\n    while(i<n):\n       .add_yaxis(chnprovince[i],y_axis=y['Confirmed'].loc[y['Province_State']==chnprovince[0]].to_list())\n       .set_global_opts(title_opts=opts.TitleOpts(title=chnprovince[i]+\"数据\"),xaxis_opts=opts.AxisOpts(name_rotate=90, axislabel_opts={\"rotate\":30}))\n       i=i+1\n\n\n)   \n\n\nc.render_notebook()\n\n\n总是出现报错，在想是不是缩进的问题？", "Tag": "算法分析"}
{"Answer": "试试这个\n\ntemp  = df.groupby('category')['data'].nlargest(4).reset_index()\ntemp.drop('level_1',axis=1)\n ", "Konwledge_Point": "应对NP完全问题", "Question": "dataframe分类排序优化问题\n我想取dataframe 'category'列两个字母中，另外一列'data'的最大4个，并按照一定的规则排序，代码如下，请问有什么更简单的方法么？\n\n\n\n\n\nimport numpy as np\nfrom time import time\nimport pandas as pd\n\ndf = pd.DataFrame()\nn = 200\ndf['category'] = np.random.choice(('A', 'B'), n)\ndf['data'] = np.random.randint(1, 10000, len(df))\na = df[df['category'] == 'A'].sort_values(by='data', ascending=False).head(4)\nb = df[df['category'] == 'B'].sort_values(by='data', ascending=False).head(4)\ndf = pd.concat([a, b]).sort_values(by=['category','data'],ascending=[True,False]).reset_index(drop=True)\nprint(df)\n\n\n\n\n结果如下\n\n\n\n\n\n  category  data\n0        A  9889\n1        A  9879\n2        A  9873\n3        A  9822\n4        B  9909\n5        B  9855\n6        B  9775\n7        B  9689", "Tag": "算法分析"}
{"Answer": "我理解的是，应该在写入之前，也就是DataFrame 之前就把数据的“元”去掉\n因为收入的键的值是个list,所以可以变量list之后，将“元”替换为空，replace()方法", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的问题：df = pd.DataFrame({'姓名':['张强','李小强','牛新','牛增强','张强','张强发'],\npython数据框正则表达式怎么去除元\n\n\nimport pandas as pd\n \nimport numpy as np\n \npd\n.set_option\n(\n'display.unicode.east_asian_width'\n, True)\n \ndf = pd\n.DataFrame\n({\n'姓名'\n:\n[\n'张强'\n,\n'李小强'\n,\n'牛新'\n,\n'牛增强'\n,\n'张强'\n,\n'张强发'\n]\n,\n \n                   \n'收入'\n:\n[\n'500元'\n,\n'530元'\n,\n'470元'\n,\n'600元'\n,np.nan,\n'560元'\n]\n})\ndf=df\n.dropna\n(axis=\n0\n)\n\nprint\n(df)\n\n\n", "Tag": "算法分析"}
{"Answer": "问题在后面的 model(data[0])，data 和 x 不匹配，设置错误.\n可以打印和追踪矩阵的形状，是开始设置时就不匹配，还是中间被修改了，在哪里发生修改的。", "Konwledge_Point": "应对NP完全问题", "Question": "关于#神经网络#的问题：RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x64 and 128x64)\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (64x64 and 128x64)\n近期在调试一个四层的BP神经网络，报出了这样的错误：\n\n\n\n\n大概知道问题出在这里：\n\n\n    def forward(self, x):\n        x = F.\nrelu\n(self.\nhidden_1\n(x))\n        x = self.\ndropout\n(self.\nbn1\n(x))\n        x = F.\nrelu\n(self.\nhidden_2\n(x))\n        x = self.\ndropout\n(self.\nbn2\n(x))\n        x = F.\nrelu\n(self.\nhidden_3\n(x))\n        x = self.\ndropout\n(self.\nbn3\n(x))\n        x = F.\nrelu\n(self.\nhidden_4\n(x))%问题所在\n        x = self.\ndropout\n(self.\nbn4\n(x))\n        x = self.\nout\n(x)\n        return x\n\n\n\n目前已经尝试过添加\n\n\nx\n = torch.flatten(x,\n1\n)\n\n\n\n不能解决/(ㄒoㄒ)/~~，感觉也不是池化层输出形状的问题叭，因为打印在def forward(self, x)中print(x.shape)输出是([64,64])，唉，神经网络小菜请教如何解决🙇‍\n下面贴出相关代码：\n\n\nclass\n \nClassifier(\ntorch\n.\nnn\n.Module)\n:\n    def \n__init__(\nself\n, \nn_feature\n, \nn_hidden\n, \nn_output\n, \ndropout\n=0.5)\n:\n        super(Classifier, self).\n__init__()\n\n        self.dropout = torch.nn.\nDropout(\ndropout\n)\n\n\n        self.hidden_1 = torch.nn.\nLinear(\nn_feature\n, \nn_hidden\n)\n\n        self.bn1 = torch.nn.\nBatchNorm1d(\nn_hidden\n)\n\n\n        self.hidden_2 = torch.nn.\nLinear(\nn_hidden\n, \nn_hidden\n/\n/\n4)\n\n        self.bn2 = torch.nn.\nBatchNorm1d(\nn_hidden\n/\n/\n4)\n\n\n        self.hidden_3 = torch.nn.\nLinear(\nn_hidden\n/\n/\n4, \nn_hidden\n/\n/\n8)\n\n        self.bn3 = torch.nn.\nBatchNorm1d(\nn_hidden\n/\n/\n8)\n\n        #三层卷积与四层卷积\n        self.hidden_4 = torch.nn.\nLinear(\nn_hidden\n \n/\n/\n 4, \nn_hidden\n \n/\n/\n 8)\n\n        self.bn4 = torch.nn.\nBatchNorm1d(\nn_hidden\n \n/\n/\n 8)\n\n\n        self.out = torch.nn.\nLinear(\nn_hidden\n/\n/\n8, \nn_output\n)\n\n\n    def forward(self, x):\n        x = \nF\n.\nrelu(self.hidden\n_1(\nx\n)\n)\n        x = self.dropout(self.bn1(x))\n        x = \nF\n.\nrelu(self.hidden\n_2(\nx\n)\n)\n        x = self.dropout(self.bn2(x))\n        x = \nF\n.\nrelu(self.hidden\n_3(\nx\n)\n)\n        x = self.dropout(self.bn3(x))\n        x = \nF\n.\nrelu(self.hidden\n_4(\nx\n)\n)\n        x = self.dropout(self.bn4(x))\n        x = self.out(x)\n        return x\n\n\n\n \nfor\n \ni\n, data \nin\n enumerate(train_loader):\n            optimizer\n.zero_grad\n() \n            train_pred = model(data\n[0]\n)\n            batch_loss = loss(train_pred, data\n[1]\n) \n\n            batch_loss\n.backward\n()  \n            optimizer\n.step\n()  \n\n            train_acc += np\n.sum\n(np\n.argmax\n(train_pred\n.cpu\n()\n.data\n.numpy\n(), axis=\n1\n) == data\n[1]\n.numpy\n())\n            train_loss += batch_loss\n.item\n()\n", "Tag": "算法分析"}
{"Answer": "python是弱类型的，2-0.0001会是个小数而不会自动给你取整而numpy.array是有类型的，你传入的是整数，那么它的类型就是整型，你传入的是小数，它的类型就是浮点型你像整型数组里传入小数，会进行强制转换", "Konwledge_Point": "应对NP完全问题", "Question": "2减去一个微小值，得到了1\n问题描述\n\n\n神奇，这里 2 - 0.0001 居然得到的结果是 1 ？而且我不明确这种问题会在什么情况下发生，我感到很害怕。\nx_grad = numerical_gradient_no_batch(function_1, x)\n 这里如果我传入的x是小数，则是程序是符合预期的。若 x 为整数，就会发生上述问题。\n\n\nx为整数的运行结果\n\n\n\n\nx为小数的运行结果\n\n\n\n\n环境\n\n\npython3.8，jupyter notebook，win10操作系统\n\n\n相关代码\n\n\nimport\n numpy \nas\n np\n\nimport\n matplotlib.pylab \nas\n plt\n\nfrom\n mpl_toolkits.mplot3d \nimport\n Axes3D\n\n\ndef\n \nfunction_1\n(\nx\n):\n    \nif\n x.ndim == \n1\n:\n        \nreturn\n np.\nsum\n(x**\n2\n)\n    \nreturn\n np.\nsum\n(x**\n2\n, axis=\n1\n)\n\n\n# 数值法求函数 f 在某点 x（一个向量） 处的梯度\n\n\ndef\n \nnumerical_gradient_no_batch\n(\nf, x\n):\n    h = \n1e-4\n\n    grad = np.zeros_like(x)\n    \nfor\n idx \nin\n \nrange\n(x.size):\n        tmp_x = x[idx]\n        x[idx] = tmp_x + h\n        fh1 = f(x)\n        \nprint\n(tmp_x, h, tmp_x + h, x[idx], fh1)\n    \n        x[idx] = tmp_x - h\n        fh2 = f(x)\n        \nprint\n(tmp_x, h, tmp_x - h, x[idx], fh2)\n        \n        grad[idx] = (fh1 - fh2) / (h *  \n2\n)\n        x[idx] = tmp_x\n        \nprint\n(grad)\n    \nreturn\n grad\n\nx = np.array([\n2\n])\nx_grad = numerical_gradient_no_batch(function_1, x)\n", "Tag": "算法分析"}
{"Answer": "\ndf['a','b','c']=df['a','b','c'].astype('int64')\n", "Konwledge_Point": "应对NP完全问题", "Question": "pandas转换object为int失败了,有人能解答吗\n用pandas创建了一个dataframe，第1,2,3列是数字，第4列是字符串。dataframe自动将所有列都转换成了object属性。\n\n\nimport numpy as np\nimport pandas as pd\nmytype =\n[\n'A'\n,\n'B'\n,\n'C'\n]\n\ndata1 = np\n.random\n.randint\n(\n0\n,\n10\n,(\n20\n,\n3\n))\ndata2 = np\n.random\n.choice\n(mytype,(\n20\n,\n1\n))\ndata = np\n.hstack\n((data1,data2))\ndf = pd\n.DataFrame\n(data,\ncolumns\n=[\n'a'\n,\n'b'\n,\n'c'\n,\n'type'\n])\n\n\n\n\ndtype: object\na       object\nb       object\nc       object\ntype    object\ndtype: object\n\n\n我想让前三列保持int类型，该怎么操作？\n查阅了相关文档，使用的infer_object可以转换，但是我这里并没有实现成功，不知道为什么。\n\n\n\n\nimport numpy as np\nimport pandas as pd\nmytype =\n[\n'A'\n,\n'B'\n,\n'C'\n]\n\ndata1 = np\n.random\n.randint\n(\n0\n,\n10\n,(\n20\n,\n3\n))\ndata2 = np\n.random\n.choice\n(mytype,(\n20\n,\n1\n))\ndata = np\n.hstack\n((data1,data2))\ndf = pd\n.DataFrame\n(data,\ncolumns\n=[\n'a'\n,\n'b'\n,\n'c'\n,\n'type'\n])\n\nprint\n(df.dtypes)\n\ndf\n.infer_objects\n()\n\nprint\n(df.dtypes)\n\n\nprint\n(df)\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "已经告诉你了，第三行，前面%是什么意思呢？并且前面带了空格，肯定会有缩进问题啊", "Konwledge_Point": "应对NP完全问题", "Question": "出现了缩进问题，可是改了后依然报错\nimport matplotlib\n.pyplot\n as plt\nimport numpy as np\n %matplotlib inline\ndef quadratic(\nvar\n):\n    return \n2\n*\npow\n(var,\n2\n)\nx=np.\narrange\n(\n0\n,.\n5\n,.\n1\n)\nplt.\nplot\n(x,\nquadratic\n(x))\npli.\nplot\n([\n1\n,\n4\n],[\nqoadratic\n(\n1\n),\nquadratic\n(\n4\n)],linewidth=\n2.0\n)\npli.\nplot\n([\n1\n,\n4\n],[\nqoadratic\n(\n1\n),\nquadratic\n(\n1\n)],linewidth=\n3.0\n,label=\n\"Change in x\"\n)\npli.\nplot\n([\n4\n,\n4\n],[\nqoadratic\n(\n1\n),\nquadratic\n(\n4\n)],linewidth=\n3.0\n,label=\n\"Change in y\"\n)\nplt.\nplot\n(x,\n10\n*x-\n8\n)\nplt.\nplot\n()\n\n\n\n\n\n\n  \nFile\n \n\"C:\\Users\\zhaoxin_pc\\.spyder-py3\\temp.py\"\n, line \n9\n\n    %matplotlib \ninline\n\n    ^\nIndentationError: unexpected indent\n", "Tag": "算法分析"}
{"Answer": "唉，终究是解决了，求均值的时候X[y_pred==i].mean()没加axis=0顺便把新实现的代码贴一下吧\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nclass Kmeans:\n    def __init__(self, k, init='pp-soft', max_iter=300, thresh=1e-5):\n        self.k = k\n        self.thresh = thresh\n        self.max_iter = max_iter\n        self.init = init\n\n    def random_centroid_init(self,X):\n        # 随机选取K个样本作为聚类中心\n        return X[np.random.choice(X.shape[0], size=self.k)]\n    \n    def max_centroid_init(self,X):\n        centroids = []\n        centroids.append(X[np.random.choice(X.shape[0])])\n        for i in range(self.k-1):\n            index = np.argmax([np.min(self.dist(x)) for x in X])\n            centroids.append(X[index])\n        return np.array(centroids)\n\n    def soft_centroid_init(self,X):\n        centroids = []\n        centroids.append(X[np.random.choice(X.shape[0])])\n        for i in range(self.k-1):\n            D = [np.min(self.dist(x)) for x in X]\n            number = np.random.choice(int(np.sum(D)))\n            for i,d in enumerate(D):\n                number -= d\n                if number<0:\n                    centroids.append(X[i])\n                    break\n        return np.array(centroids)\n\n    def dist(self, x):\n        return [np.linalg.norm(x - c) for c in self.centroids]\n\n    def fit_predict(self, X):\n        # 初始化聚类中心\n        if self.init == 'random':\n            self.centroids = self.random_centroid_init(X)\n        elif self.init == 'pp-max':\n            self.centroids = self.max_centroid_init(X)\n        else:\n            self.centroids = self.soft_centroid_init(X)\n        for _ in range(self.max_iter):\n            # 涂色\n            y_pred = np.array([np.argmin(self.dist(x)) for x in X])\n            \n            # 计算新的聚类中心\n            new_centroids = self.centroids.copy()\n            for i in range(self.k):\n                new_centroids[i] = np.mean(X[y_pred==i],axis=0)\n            \n            # 如果聚类中心位置基本没有变化，那么终止\n            if np.max(np.abs(new_centroids - self.centroids)) < self.thresh:\n                break\n            \n            # 否则更新聚类中心，重复上述步骤\n            self.centroids = new_centroids\n        return y_pred\n    \nX, y = make_blobs(n_samples=1000, n_features=2, centers=3)\n\nmodel = Kmeans(3)\ny_pred = model.fit_predict(X)\n\nplt.figure()\nplt.subplot(121)\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.subplot(122)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.show()\n", "Konwledge_Point": "应对NP完全问题", "Question": "kmeans++聚类聚成这样合理吗\nkmeans++聚类聚成这样合理吗\n聚类的算法代码如下\n\n\nclass\n \nKmeans\n:\n    \ndef\n \n__init__\n(\nself, k, threshold=\n1e-5\n):\n        self.k = k\n        self.threshold = threshold\n    \n    \ndef\n \ncentroid_init\n(\nself,X\n):\n        centroids = []\n        centroids.append(X[np.random.choice(X.shape[\n0\n])])\n        \nfor\n i \nin\n \nrange\n(self.k-\n1\n):\n            D = []\n            \nfor\n x \nin\n X:\n                D.append(np.\nmin\n([np.linalg.norm(x - c) \nfor\n c \nin\n centroids]))\n            centroids.append(X[np.argmax(D)])\n        \nreturn\n np.array(centroids)\n                              \n    \ndef\n \ntrain\n(\nself, X\n):\n        \n# 初始化聚类中心\n\n        self.centroids = self.centroid_init(X)\n        y_pred = np.zeros(shape=(X.shape[\n0\n],))\n        \nwhile\n \nTrue\n:\n            \n# 涂色\n\n            \nfor\n i, x \nin\n \nenumerate\n(X):\n                y_pred[i] = self.predict(x)\n            \n            \n# 计算新的聚类中心\n\n            new_centroids = self.centroids.copy()\n            \nfor\n i \nin\n \nrange\n(self.k):\n                new_centroids[i] = X[y_pred==i].mean()\n            \n            \n# 如果聚类中心位置基本没有变化，那么终止\n\n            \nif\n np.\nmax\n(np.\nabs\n(new_centroids - self.centroids)) < self.threshold:\n                \nbreak\n\n            \n            \n# 否则更新聚类中心，重复上述步骤\n\n            self.centroids = new_centroids\n        \nreturn\n y_pred\n\n    \ndef\n \npredict\n(\nself, x\n):\n        dis = []\n        \n# 计算每个样本与中心的距离\n\n        \nfor\n c \nin\n self.centroids:\n            dis.append(np.linalg.norm(x - c))\n        \n# 将样本索引添加到距离最小的中心对应的分类中\n\n        \nreturn\n np.argmin(dis)\n\n\n\n下图左边是原数据分布，右边是上面的算法生成的聚类分布", "Tag": "算法分析"}
{"Answer": "tensorflow 版本 这个应该是要使用2.x的", "Konwledge_Point": "应对NP完全问题", "Question": "树莓派调用tensorflow时出现no module named compat.v1\n\n\n\n\nimport\n numpy as np\n\nimport\n os\n\nimport\n sys\n\nimport\n tarfile\n\nimport\n tensorflow as tf\n\nimport\n cv2\n\nimport\n time\nfrom collections \nimport\n defaultdict\n\nsys.path.append(\n\"../..\"\n)\n\nfrom object_detection.utils \nimport\n label_map_util\nfrom object_detection.utils \nimport\n visualization_utils as vis_util\n\n\n\nMODEL_NAME\n = 'ssd_mobilenet_v1_coco_2018_01_28'\n\n\nPATH_TO_CKPT\n = MODEL_NAME + '/frozen_inference_graph.pb'\n\n\nPATH_TO_LABELS\n = os.path.join('/home/pi/models/research/object_detection/data', 'mscoco_label_map.pbtxt')\n\n\nmodel_path\n = \n\"/home/pi/models/research/object_detection/models/ssd_mobilenet_v1_coco_2018_01_28/model.ckpt\"\n\n\n\nstart\n = time.clock()\n\nNUM_CLASSES\n = \n90\n\n\n\nend=\n time.clock()\nprint('load the model' ,(end -start))\n\ndetection_graph\n = tf.Graph()\n\nwith\n detection_graph.as_default():\n    \nod_graph_def\n = tf.GraphDef()\n\nwith\n tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n    \nserialized_graph\n = fid.read()\n    od_graph_def.ParseFromString(serialized_graph)\n    tf.import_graph_def(od_graph_def, \nname='')\n\n\n\nlabel_map\n = label_map_util.load_labelmap(PATH_TO_LABELS)\n\n\ncategories\n = label_map_util.convert_label_map_to_categories(label_map, \nmax_num_classes=NUM_CLASSES,\n \nuse_display_name=True)\n\n\ncategory_index\n = label_map_util.create_category_index(categories)\n\n\ncap\n = cv2.VideoCapture(\n0\n)\n\nwith\n detection_graph.as_default():\n    \nwith\n tf.Session(\ngraph=detection_graph)\n as sess:\n        \nwriter\n = tf.summary.FileWriter(\n\"logs/\"\n, sess.graph)\n        sess.run(tf.global_variables_initializer())\n\n        \nloader\n = tf.train.import_meta_graph(model_path + '.meta')\n        loader.restore(sess, model_path)\n        while(\n1\n):\n            \nstart\n = time.clock()\n            ret, \nframe\n = cap.read()\n            \nif\n cv2.waitKey(\n1\n) & \n0\nxFF\n == ord('q'):\n                break\n            \nimage_np\n =frame\n\n            \nimage_np_expanded\n = np.expand_dims(image_np, \naxis=0)\n\n            \nimage_tensor\n = detection_graph.get_tensor_by_name('image_tensor:\n0\n')\n            \nboxes\n = detection_graph.get_tensor_by_name('detection_boxes:\n0\n')\n            \nscores\n = detection_graph.get_tensor_by_name('detection_scores:\n0\n')\n            \nclasses\n = detection_graph.get_tensor_by_name('detection_classes:\n0\n')\n            \nnum_detections\n = detection_graph.get_tensor_by_name('num_detections:\n0\n')\n\n            (boxes, scores, classes, num_detections) = sess.run(\n                [boxes, scores, classes, num_detections],\n                \nfeed_dict={image_tensor:\n image_np_expanded})\n\n            vis_util.visualize_boxes_and_labels_on_image_array(\n                image_np, np.squeeze(boxes),\n                np.squeeze(classes).astype(np.int32),\n                np.squeeze(scores),\n                category_index,\n                \nuse_normalized_coordinates=True,\n\n                \nline_thickness=6)\n\n            \nend\n = time.clock()\n\n            print('One frame detect take time:' ,end - start)\n\n            cv2.imshow(\n\"capture\"\n, image_np)\n            print('after cv2 show')\n            cv2.waitKey(\n1\n)\ncap.release()\ncv2.destroyAllWindows()\n\n", "Tag": "算法分析"}
{"Answer": "这个需要建立字典封装起来", "Konwledge_Point": "应对NP完全问题", "Question": "如何将训练好的BP神经网络模型保存并可以在其他py文件中直接调用？\n下面是我写的BP神经网络代码，问问如何将训练好的神经网络模型保存并可以在其他py文件中直接调用？请老师给出详细的代码，谢谢\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import radviz\n\n'''\n    构建一个具有1个隐藏层的神经网络，隐层的大小为10\n    输入层为4个特征，输出层为3个分类\n    (1,0,0)为第一类，(0,1,0)为第二类，(0,0,1)为第三类\n'''\n\n\n# 1.初始化参数\ndef initialize_parameters(n_x, n_h, n_y):\n    np.random.seed(2)\n\n    # 权重和偏置矩阵\n    w1 = np.random.randn(n_h, n_x) * 0.01\n    b1 = np.zeros(shape=(n_h, 1))\n    w2 = np.random.randn(n_y, n_h) * 0.01\n    b2 = np.zeros(shape=(n_y, 1))\n\n    # 通过字典存储参数\n    parameters = {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2}\n\n    return parameters\n\n\n# 2.前向传播\ndef forward_propagation(X, parameters):\n    w1 = parameters['w1']\n    b1 = parameters['b1']\n    w2 = parameters['w2']\n    b2 = parameters['b2']\n\n    # 通过前向传播来计算a2\n    z1 = np.dot(w1, X) + b1  # 这个地方需注意矩阵加法：虽然(w1*X)和b1的维度不同，但可以相加\n    a1 = np.tanh(z1)  # 使用tanh作为第一层的激活函数\n    z2 = np.dot(w2, a1) + b2\n    a2 = 1 / (1 + np.exp(-z2))  # 使用sigmoid作为第二层的激活函数\n\n    # 通过字典存储参数\n    cache = {'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}\n\n    return a2, cache\n\n\n# 3.计算代价函数\ndef compute_cost(a2, Y, parameters):\n    m = Y.shape[1]  # Y的列数即为总的样本数\n\n    # 采用交叉熵（cross-entropy）作为代价函数\n    logprobs = np.multiply(np.log(a2), Y) + np.multiply((1 - Y), np.log(1 - a2))\n    cost = - np.sum(logprobs) / m\n\n    return cost\n\n\n# 4.反向传播（计算代价函数的导数）\ndef backward_propagation(parameters, cache, X, Y):\n    m = Y.shape[1]\n\n    w2 = parameters['w2']\n\n    a1 = cache['a1']\n    a2 = cache['a2']\n\n    # 反向传播，计算dw1、db1、dw2、db2\n    dz2 = a2 - Y\n    dw2 = (1 / m) * np.dot(dz2, a1.T)\n    db2 = (1 / m) * np.sum(dz2, axis=1, keepdims=True)\n    dz1 = np.multiply(np.dot(w2.T, dz2), 1 - np.power(a1, 2))\n    dw1 = (1 / m) * np.dot(dz1, X.T)\n    db1 = (1 / m) * np.sum(dz1, axis=1, keepdims=True)\n\n    grads = {'dw1': dw1, 'db1': db1, 'dw2': dw2, 'db2': db2}\n\n    return grads\n\n\n# 5.更新参数\ndef update_parameters(parameters, grads, learning_rate=0.06):\n    w1 = parameters['w1']\n    b1 = parameters['b1']\n    w2 = parameters['w2']\n    b2 = parameters['b2']\n\n    dw1 = grads['dw1']\n    db1 = grads['db1']\n    dw2 = grads['dw2']\n    db2 = grads['db2']\n\n    # 更新参数\n    w1 = w1 - dw1 * learning_rate\n    b1 = b1 - db1 * learning_rate\n    w2 = w2 - dw2 * learning_rate\n    b2 = b2 - db2 * learning_rate\n\n    parameters = {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2}\n\n    return parameters\n\n\n# 6.模型评估\ndef predict(parameters, x_test, y_test):\n    w1 = parameters['w1']\n    b1 = parameters['b1']\n    w2 = parameters['w2']\n    b2 = parameters['b2']\n\n    z1 = np.dot(w1, x_test) + b1\n    a1 = np.tanh(z1)\n    z2 = np.dot(w2, a1) + b2\n    a2 = 1 / (1 + np.exp(-z2))\n\n    # 结果的维度\n    n_rows = y_test.shape[0]\n    n_cols = y_test.shape[1]\n\n    # 预测值结果存储\n    output = np.empty(shape=(n_rows, n_cols), dtype=int)\n\n    # for i in range(n_rows):\n    #     for j in range(n_cols):\n    #         if a2[i][j] > 0.5:\n    #             output[i][j] = 1\n    #         else:\n    #             output[i][j] = 0\n\n    for i in range(n_cols):\n        # 将每条测试数据的预测结果（概率）存为一个行向量\n        temp = np.zeros(shape=n_rows)\n        for j in range(n_rows):\n            temp[j] = a2[j][i]\n\n        # 将每条结果（概率）从小到大排序，并获得相应下标\n        sorted_dist = np.argsort(temp)\n        length = len(sorted_dist)\n\n        # 将概率最大的置为1，其它置为0\n        for k in range(length):\n            if k == sorted_dist[length - 1]:\n                output[k][i] = 1\n            else:\n                output[k][i] = 0\n\n    print('预测结果：')\n    print(output)\n    print('真实结果：')\n    print(y_test)\n\n    count = 0\n    for k in range(0, n_cols):\n        if output[0][k] == y_test[0][k] and output[1][k] == y_test[1][k] and output[2][k] == y_test[2][k]:\n            count = count + 1\n        else:\n            print(k)\n\n    acc = count / int(y_test.shape[1]) * 100\n    print('准确率：%.2f%%' % acc)\n\n    return output\n\n\n# 建立神经网络\ndef nn_model(X, Y, n_h, n_input, n_output, num_iterations=10000, print_cost=False):\n    np.random.seed(3)\n\n    n_x = n_input  # 输入层节点数\n    n_y = n_output  # 输出层节点数\n\n    # 1.初始化参数\n    parameters = initialize_parameters(n_x, n_h, n_y)\n\n    # 梯度下降循环\n    for i in range(0, num_iterations):\n        # 2.前向传播\n        a2, cache = forward_propagation(X, parameters)\n        # 3.计算代价函数\n        cost = compute_cost(a2, Y, parameters)\n        # 4.反向传播\n        grads = backward_propagation(parameters, cache, X, Y)\n        # 5.更新参数\n        parameters = update_parameters(parameters, grads)\n\n        # 每1000次迭代，输出一次代价函数\n        if print_cost and i % 1000 == 0:\n            print('迭代第%i次，代价函数为：%f' % (i, cost))\n\n    return parameters\n\n\n\n# 结果可视化\n# 特征有4个维度，类别有1个维度，一共5个维度，故采用了RadViz图\n# def result_visualization(x_test, y_test, result):\n#     cols = y_test.shape[1]\n#     y = []\n#     pre = []\n#\n#     # 反转换类别的独热编码\n#     for i in range(cols):\n#         if y_test[0][i] == 0 and y_test[1][i] == 0 and y_test[2][i] == 1:\n#             y.append('setosa')\n#         elif y_test[0][i] == 0 and y_test[1][i] == 1 and y_test[2][i] == 0:\n#             y.append('versicolor')\n#         elif y_test[0][i] == 1 and y_test[1][i] == 0 and y_test[2][i] == 0:\n#             y.append('virginica')\n#\n#     for j in range(cols):\n#         if result[0][j] == 0 and result[1][j] == 0 and result[2][j] == 1:\n#             pre.append('setosa')\n#         elif result[0][j] == 0 and result[1][j] == 1 and result[2][j] == 0:\n#             pre.append('versicolor')\n#         elif result[0][j] == 1 and result[1][j] == 0 and result[2][j] == 0:\n#             pre.append('virginica')\n#         else:\n#             pre.append('unknown')\n#\n#     # 将特征和类别矩阵拼接起来\n#     real = np.column_stack((x_test.T, y))\n#     prediction = np.column_stack((x_test.T, pre))\n#\n#     # 转换成DataFrame类型，并添加columns\n#     df_real = pd.DataFrame(real, index=None,\n#                            columns=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Species'])\n#     df_prediction = pd.DataFrame(prediction, index=None,\n#                                  columns=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Species'])\n#\n#     # 将特征列转换为float类型，否则radviz会报错\n#     df_real[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']] = df_real[\n#         ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']].astype(float)\n#     df_prediction[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']] = df_prediction[\n#         ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']].astype(float)\n#\n#     # 绘图\n#     plt.figure('真实分类')\n#     radviz(df_real, 'Species', color=['blue', 'green', 'red', 'yellow'])\n#     plt.figure('预测分类')\n#     radviz(df_prediction, 'Species', color=['blue', 'green', 'red', 'yellow'])\n#     plt.show()\n\n\nif __name__ == \"__main__\":\n    # 读取数据\n    data_set = pd.read_csv('C:/Users/29291/Desktop/sjwl.csv', header=None)\n\n    # # 第1种取数据方法：\n    # X = data_set.iloc[0:671, 0:269].values  # 前四列是特征，T表示转置\n    # Y = data_set.iloc[672:674, 0:269].values  # 后三列是标签\n\n    # 第2种取数据方法：\n    # X = data_set.ix[0:671, 0:269].values\n    # Y = data_set.ix[672:674, 0:269].values\n\n    # 第3种取数据方法：\n    X = data_set.loc[0:671, 0:269].values\n    x_mean = np.mean(X, axis=0)\n    x_std = np.std(X, axis=0)\n    X = (X - x_mean) / x_std\n    Y = data_set.loc[672:674, 0:269].values\n    # X = data_set.loc[1:670, 1:270].values\n    # Y = data_set.loc[672:674, 0:269].values\n    # 第4种取数据方法：\n    # X = data_set[data_set.columns[0:671, 0:269].values.T\n    # Y = data_set[data_set.columns[672:674, 0:269]].values.T\n    Y = Y.astype('uint8')\n\n    # 开始训练\n    start_time = datetime.datetime.now()\n    # 输入4个节点，隐层10个节点，输出3个节点，迭代10000次\n    parameters = nn_model(X, Y, n_h=30, n_input=672, n_output=3, num_iterations=10000, print_cost=True)\n    end_time = datetime.datetime.now()\n    print(\"用时：\" + str((end_time - start_time).seconds) + 's' + str(\n        round((end_time - start_time).microseconds / 1000)) + 'ms')\n    # 对模型进行测试\n    data_test = pd.read_csv('C:/Users/29291/Desktop/sjwl.csv', header=None)\n    # x_test = data_test.iloc[0:671, 270:299].values\n    # y_test = data_test.iloc[672:674, 270:299].values\n    x_test =data_set.loc[0:671, 270:299].values\n    x_mean = np.mean(x_test , axis=0)\n    x_std = np.std(x_test , axis=0)\n    x_test  = (x_test  - x_mean) / x_std\n    y_test =data_set.loc[672:674, 270:299].values\n    y_test = y_test.astype('uint8')\n    result = predict(parameters, x_test, y_test)", "Tag": "算法分析"}
{"Answer": "comment表示注释，解析时不把它当成真正的数据，也就是把那行忽略掉\r\n\r\napi里面的解释：\r\n```\r\n comments : str, optional\r\n        The character used to indicate the start of a comment.\r\n        All the characters occurring on a line after a comment are discarded\r\n```\r\n\r\n如果data:开头的是数据，就该把comments设为None（如有注释行，就该是注释行标记符）\r\n要么在读取文本前把前面的data处理掉，要么在得到数组后把第一个元素(data:编程了nan，表示非数字)去掉", "Konwledge_Point": "应对NP完全问题", "Question": "关于numpy中的genfromtxt方法的参数设定问题？\n请教一下，现在我想把一个utf-8编码的txt文件导入为一个numpy数组，txt文件内容如下：\n\n\n\n导入数据如下：\n\ndata:1,10,100,1000,10000,100000,1000000,10000000\n\ndata:1,2,4,8,16,32,64,128\n\n请找出数据的递增规则\n\n\n\n我写的代码如下：\n\n\n\na=np.genfromtxt('data.txt',delimiter=',',comments='data:',skip_header=1,skip_footer=1,encoding='utf-8')\na\n\n\n\n\n输出结果是：array([], dtype=float64) 看上去应该是一个空数组，我尝试把中间两条数据记录前面的data：去掉即可正常读取数据，推测是comments参数设置的问题，可否指导一下应该怎么修改啊？", "Tag": "算法分析"}
{"Answer": "因为确实没有rank函数你可能是想https://jingyan.baidu.com/article/0eb457e51003cb42f1a90586.html", "Konwledge_Point": "应对NP完全问题", "Question": "python一直出现错误提示\n代码：\n\n\nnp\n.\nrank\n(myArray)\n\n\n\n\n错误提示：\n\n\nAttributeError                            Traceback (most recent \ncall\n last)\n \nin\n \n      \n1\n \nimport\n numpy \nas\n np\n\n----> 2 np.rank(myArray)\n\n\nD:\\anaconda\\lib\\site-packages\\numpy\\__init__.py \nin\n __getattr__(attr)\n    \n301\n                 \nreturn\n Tester\n    \n302\n \n\n--> 303             raise AttributeError(\"module {!r} has no attribute \"\n\n    \n304\n                                  \"{!r}\".format(__name__, attr))\n    \n305\n \n\n\n\n\nAttributeError: module 'numpy' has no attribute 'rank'\n想知道这个问题怎么解决呢？", "Tag": "算法分析"}
{"Answer": "是不是你的训练数据太少了？", "Konwledge_Point": "应对NP完全问题", "Question": "写了一个人脸识别，准确度一直有问题，是不是我的训练思路出问题了？\n每个人一个文件夹，每个文件夹里面有3张人脸照片\n\n\n\n\nimport\n cv2\n\nimport\n numpy \nas\n np\n\nfrom\n cv2 \nimport\n face\n\nimport\n os\n\nimport\n pymysql\n\ndef\n \nface_xunlian\n():\n    \n    \n#提取字典中的key值并转化为列表\n\n    keyvalues = \nlist\n(\nmap\n(\nint\n,result.keys()))\n\n    \n#样本图像列表\n\n    photos =\nlist\n()\n    \n#标签列表\n\n    lables =\nlist\n()\n    \n#定位文件夹\n\n    wenjianjia = os.listdir(\n\"D:/python/opencv/shuju\"\n)\n\n    \nprint\n(wenjianjia)\n    num=\n1\n\n    \n#遍历文件夹\n\n    \nfor\n ids \nin\n wenjianjia:\n        \n#定位文件夹中的图片\n\n        tupin_id=os.listdir(\n\"D:/python/opencv/shuju/\"\n+ids)\n        \n#遍历文件夹中的图片\n\n        \nfor\n tupin_local \nin\n tupin_id:\n            photos.append(cv2.imread(\n\"D:/python/opencv/shuju/\"\n+ids+\n\"/\"\n+tupin_local,\n0\n))\n            \nprint\n(ids)\n            \nprint\n(\n\"D:/python/opencv/shuju/\"\n+ids+\n\"/\"\n+tupin_local)\n            \nprint\n(cv2.imread(\n\"D:/python/opencv/shuju/\"\n+ids+\n\"/\"\n+tupin_local,\n0\n))\n            \nif\n num<\nlen\n(ids)+\n1\n:\n                \nfor\n kk \nin\n keyvalues:\n                    num+\n1\n\n                    s=kk,kk,kk\n                    \nfor\n numnber \nin\n s:\n                        \nprint\n(numnber)\n                        lables.append(numnber)\n                        num+=\n1\n\n\n\n\n\n    \n#加载识别器（LBPH）\n\n    recognizer=cv2.face.LBPHFaceRecognizer_create()\n    \n#识别器训练\n\n    recognizer.train(photos,np.array(lables))\n    \n#训练结果录入\n\n    recognizer.write(\n'./trainer_face.yml'\n)\n\n\n\n\nface_xunlian()\n\n", "Tag": "算法分析"}
{"Answer": "float() argument must be a string or a number报错的意思float(A)，要求A是一个字符串数字或者数字，但是你的是空的，你在哪里用float转换数值的地方出问题了", "Konwledge_Point": "应对NP完全问题", "Question": "TypeError: float() argument must be a string or a number, not '_NoValueType'\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n随机获取训练图片\n\n\ndataiter\n = iter(trainloader)\nimages, \nlabels\n = dataiter.next()\n\n\n\n显示图片\n\n\nimshow\n(torchvision.utils.make_grid(images)\n)\n\n\n\n打印图片标签\n\n\npr\nint\n(\n' '\n.joi\nn\n(\n'%5s'\n % classes[labels[j]] for j \nin\n \nrange\n(4)))\n", "Tag": "算法分析"}
{"Answer": "这个错误提示表明，在计算距离的时候使用了字符串类型的数据，而距离计算一般是针对数值型数据的。你需要检查你的数据，看看是否有些特征是字符串类型的，如果有，你需要进行相应的处理，将其转换为数值类型，比如使用独热编码等方式。答案来自 我点评开发社区 https://www.wodianping.com/ 你也可以考虑在 KNN 模型中使用其他距离度量方式，比如 Jaccard 相似度、余弦相似度等，这些度量方式可以适用于一些非数值型数据。", "Konwledge_Point": "应对NP完全问题", "Question": "关于#机器学习#的问题，如何解决？(语言-python)\nTypeError                                 Traceback (most recent \ncall\n \nlast\n)\n~\\AppData\\Local\\Temp\\ipykernel_2024\\\n4008345267\n.\npy\n in \n\n\n      \n9\n knn = KNN(\nk\n=\n2\n)\n     \n10\n knn.fit(train_X,train_y)\n---> \n11\n result = knn.predict(test_X)\n     \n12\n \ndisplay\n(result)\n\n~\\AppData\\Local\\Temp\\ipykernel_2024\\\n1590028057\n.\npy\n in predict(self, \nX\n)\n     \n11\n         #每次取一行样本当作一个点\n     \n12\n         \nfor\n \nx\n in \nX\n:\n---> \n13\n             dis=np.\nsqrt\n(np.sum((\nx\n - self.\nX\n) ** \n2\n,axis=\n1\n))\n     \n14\n             \nindex\n=dis.argsort()\n     \n15\n             \nindex\n=\nindex\n[:self.\nk\n]\n\nTypeError: unsupported operand \ntype\n(s) \nfor\n -: \n'str'\n \nand\n \n'str'\n\n\n", "Tag": "算法分析"}
{"Answer": "对于单个频率的波形，取频谱的峰值,np.argmax()可以取到下标值，根据freq=下标*N/fs,获得频率值， 对于含有多个频率的波形，我是做一个区分，判断频谱中\r\n振幅大于某个数，就为信号，小于某个数就为噪声。这样不准确，最好用信噪比判断，然后同样获取下标，获得频率值，相应还能得到幅度。", "Konwledge_Point": "应对NP完全问题", "Question": "python：对一个波形做傅里叶变换，能得到整个频谱，怎么提取其中的频率分量呢？\nimport wave\n\nimport struct\n\nimport numpy as np\n\n\n\nif \nname\n == '__main__':\n\n    data_size = 40000\n\n    fname = \"test.wav\"\n\n    frate = 11025.0\n\n    wav_file = wave.open(fname, 'r')\n\n    data = wav_file.readframes(data_size)\n\n    wav_file.close()\n\n    data = struct.unpack('{n}h'.format(n=data_size), data)\n\n    data = np.array(data)\n\n\n\nw = np.fft.fft(data)\nfreqs = np.fft.fftfreq(len(w))\nprint(freqs.min(), freqs.max())\n# (-0.5, 0.499975)\n\n# Find the peak in the coefficients\nidx = np.argmax(np.abs(w))\nfreq = freqs[idx]\nfreq_in_hertz = abs(freq * frate)\nprint(freq_in_hertz)\n# 439.8975\n\n\n\n\n这个是对于单个data_size,frate已知的情况，真实情况是未知，且有多个频率的信号。求解。。。", "Tag": "算法分析"}
{"Answer": "？什么意思，128754个元素要怎么转成 1 x 1118的格式啊？", "Konwledge_Point": "应对NP完全问题", "Question": "麻烦大家帮我debug一个reshape array的问题\ndef plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2) # Truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).\n        lsa.fit(np.array(test_data).reshape(1,1118))\n        lsa_scores = lsa.transform(np.array(test_data).reshape(1,1118))\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        print ('colormapper=',color_mapper)\n        #print ('colorColumn=',color_column)\n        colors = ['blue','green','red']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            red_patch = mpatches.Patch(color='red', label='Negative')\n            blue_patch = mpatches.Patch(color='blue', label='Neutral')\n            green_patch = mpatches.Patch(color='green', label='Positive')\n            plt.legend(handles=[red_patch, green_patch, blue_patch], prop={'size': 30})\n\n\n```fig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train, y_train)\nplt.show()\n\n\n\n\n\nValueError                                Traceback (most recent call last)\n in \n     22 \n     23 fig = plt.figure(figsize=(5, 5))\n---> 24 plot_LSA(X_train, y_train)\n     25 plt.show()\n     26 \n\n in plot_LSA(test_data, test_labels, savepath, plot)\n      7 def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n      8         lsa = TruncatedSVD(n_components=2) # Truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).\n----> 9         lsa.fit(np.array(test_data).reshape(1,1118))\n     10         lsa_scores = lsa.transform(np.array(test_data).reshape(1,1118))\n     11         color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n\nValueError: cannot reshape array of size 128764 into shape (1,1118)\n\n\n\n\n\n之前出现的问题是python是1D array,没有办法显示2D array,然后我按照网上的指导解决了，但是以上错误不知道怎么修改，麻烦大家给我一点建议。谢谢！\n", "Tag": "算法分析"}
{"Answer": "保存地址里\\U被转义了呗，可改为\nplt.savefig(r'C:\\Users\\30575\\Desktop\\daima-tu', dpi=500, bbox_inches='tight')\n# or\nplt.savefig('C:\\\\Users\\\\30575\\\\Desktop\\\\daima-tu', dpi=500, bbox_inches='tight')\n", "Konwledge_Point": "应对NP完全问题", "Question": "plt.savefig()使用时报错\n问题遇到的现象和发生背景\n\n\nplt.savefig()使用时报错\n\n\n问题相关代码，请勿粘贴截图\n\n\n[](\n\n\nplt.plot(np.array(plt_beta), np.array(plt_k))\nplt.xlabel(r\n'$\\eta$'\n,\nfontsize\n=12)\nplt.ylabel(\n'Iterations'\n,\nfontsize\n=12)\nplt.savefig(\n'C:\\Users\\30575\\Desktop\\daima-tu'\n, \ndpi\n=500, \nbbox_inches\n=\n'tight'\n) #保存高清图片\nplt.show()\n\n\n\n```)\n\n\n###### 运行结果及报错内容 \n\n请问为什么会报错：(unicode error) \n'unicodeescape'\n codec can\n't decode bytes in position\n2-3: truncated \\UXXXXXXXX escape\n\n\n###### 我的解答思路和尝试过的方法 \n\n###### 我想要达到的结果\n\n\n", "Tag": "算法分析"}
{"Answer": "直接改一改就能用了，YOLOV5本身就是opencv格式读写的，opencv本身就是numpy格式的数据，只不过输入读取的不是而已，你可以直接将读取接口换成numpy就可以了具体可以修改这里面的东西，copy一份，然后将里面的imread读取换成直接复赋值numpy矩阵就可以了，但是后面的一些像自适应缩放啥的还是要的，或者你可以都不要这些，直接自己参考这个数据加载的方式自己实现一个，只要能保证返回的结果里面的值格式一致即可", "Konwledge_Point": "应对NP完全问题", "Question": "yolov5传入numpy矩阵视频流\n问题遇到的现象和发生背景\n\n\n最近接手一个项目，用yolov5和工业相机实现目标检测。\n目前已经用python调用海康工业相机得到numpy数组类型的视频。\n但是想要将这种格式的视频传入yolov5的detect.py就出现了问题。\nyolov5目前支持的格式为：\n1.mp4、avi等视频格式的文件；\n2.RTSP、HTTP等视频流；\n3.电脑自带相机以及其他免驱动相机；\n\n\n所以，yolov5不支持传入numpy数组格式的视频\n\n\ntemp\n = np.asarray(data_buf)  #\ntemp\n是工业相机得到的numpy视频流\n\ntemp\n = \ntemp\n.reshape((img_h, img_w, img_c))\n\ntemp\n = cv2.cvtColor(\ntemp\n, cv2.COLOR_BGR2RGB)\ncv2.namedWindow(\"temp\", cv2.WINDOW_NORMAL)\ncv2.imshow(\n'temp'\n,\ntemp\n)\n\n\n\n\n我尝试将numpy数组转换成图片\n\n\nimport\n numpy as np\n\nfrom\n PIL import Image\n\n\n# 将numpy数组转换成图片\n\n\ndef\n array2img(arr):\n    \n# 将numpy数组转换为PIL图片\n\n    \nimg\n = Image.fromarray(np.uint8(arr))\n    \n# 显示图片\n\n    \nimg\n.show()\n\n\n# 创建一个3x3的numpy数组\n\n\narr\n = np.array([[\n1\n,\n2\n,\n3\n],[\n4\n,\n5\n,\n6\n],[\n7\n,\n8\n,\n9\n]], dtype=np.uint8)\n\n# 调用函数将numpy数组转换成图片\n\n\narray2img\n(arr)\n\n\n\n\n但是这种方法会保存图片，\n对于实施检测来说，就会保存大量图片\n\n\n实现目标：\n将numpy视频流能传入yolov5", "Tag": "算法分析"}
{"Answer": "这句话改成常见写法是：\r\nfor sp in range(3)：\r\n    Q[s, a] += T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp]))\r\n\r\n举个例子：\r\nt = [i for i in range(3)]\r\n相当于：\r\nt = []\r\nfor i in range(3):\r\n    t.append(i)\r\n这是一种非常方便的写法。", "Konwledge_Point": "应对NP完全问题", "Question": "一个在python句子里面的for循环\n新手学python，我遇到一个python在句子里面，我不能理解这个for循环是什么意思。\n\n如果改成常见的那种for循环应该怎么写。\n\n我看不懂的代码是这一句\n\n\n\n Q[s, a] = np.sum([T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp]))    for sp in range(3)])\n\n\n\n\n这是完整的代码，运行的环境是jupyter，python版本是3.6.5\n\n\n\n import numpy as np\nnan = np.nan\nT = np.array([[[0.7,0.3,0.0],[1.0,0.0,0.0],[0.8,0.2,0.0]],\n              [[0.0,1.0,0.0],[nan,nan,nan],[0.0,0.0,1.0]],\n              [[nan,nan,nan],[0.8,0.1,0.1],[nan,nan,nan]]\n             ])\nR = np.array([[[10.,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],\n              [[10.,0.0,0.0],[nan,nan,nan],[0.0,0.0,-50.0]],\n              [[nan,nan,nan],[40.0,0.0,0.0],[nan,nan,nan]]\n             ])\npossible_actions = [[0,1,2],[0,2],[1]]\n\nQ = np.full((3, 3), -np.inf)  # -inf 对应着不可能的动作 \nfor state, actions in enumerate(possible_actions):    \n    Q[state, actions] = 0.0  # 对所有可能的动作初始化为0.0\nlearning_rate = 0.01 \ndiscount_rate = 0.95 \nn_iterations = 100\nfor iteration in range(n_iterations):   \n    Q_prev = Q.copy()    \n    for s in range(3):        \n        for a in possible_actions[s]:            \n            Q[s, a] = np.sum([T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp]))  \n            for sp in range(3)])\n", "Tag": "算法分析"}
{"Answer": "我感觉你写得不对，以下代码供你参考\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as  plt\nfrom sklearn.metrics import multilabel_confusion_matrix\nfrom sklearn.metrics import classification_report,roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_blobs\n\nfrom sklearn import svm\n\n#使用最初的X和y，样本不均衡的这个模型\nclass_1 = 500 #类别1有500个样本\nclass_2 = 50 #类别2只有50个\ncenters = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心\nclusters_std = [1.5, 0.5] #设定两个类别的方差，通常来说，样本量比较大的类别会更加松散\nX, y = make_blobs(n_samples=[class_1, class_2],\n                  centers=centers,\n                  cluster_std=clusters_std,\n                  random_state=0, shuffle=False)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"rainbow\",s=10)\n\n\nclf_proba = RandomForestClassifier(n_estimators = 8).fit(X,y)\ny_predict = clf_proba.predict(X)\nypredict = clf_proba.predict_proba(X)\n\nfrom sklearn.metrics import roc_curve\nFPR, recall, thresholds = roc_curve(y,ypredict[:,1], pos_label=1)\n\n\nfrom sklearn.metrics import roc_auc_score as AUC\narea = AUC(y,ypredict[:,1])\nprint(area)\n\nfrom sklearn.metrics import confusion_matrix\ncm= confusion_matrix(y, y_predict)\nprint(cm)\n\nplt.figure()\nplt.plot(FPR, recall, color='red',\n         label='ROC curve (area = %0.4f)' % area)\nplt.plot([0, 1], [0, 1], color='black', linestyle='--')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('Recall')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "ROC曲线画出来混乱线\n问题遇到的现象和发生背景\n\n\n\n\n今天在做建模分析的时候，用到随机森林处理多分类问题，最后画出的ROC曲线很奇怪，想知道这是什么原因造成的。\n\n\n\n\n问题相关代码\n\n\nimport numpy as np\n\nfrom\n sklearn.metrics import multilabel_confusion_matrix\n\nfrom\n sklearn.metrics import classification_report,roc_auc_score\n\npred_y_quant = rf.predict_proba(Xtest)\ncon = multilabel_confusion_matrix(Ytest,rf.predict(Xtest),labels=[1.0,2.0,3.0,4.0,5.0])\n\nprint\n(classification_report(Ytest,rf.predict(Xtest),labels=[1.0,2.0,3.0,4.0,5.0]))\n\n# macro avg       0.52      0.40      0.42      4646 weighted avg       0.71      0.72      0.70      4646\n\n\n#AUC\n\nAUC = roc_auc_score(np.array(Ytest),np.array(pred_y_quant),\nmulti_class\n=\n'ovo'\n)\n\nprint\n(\n'AUC:'\n,auc)\npred_y_score = []\n\nfor\n i \nin\n pred_y_quant:\n    pred_y_score.append(max(i))\n\n#ROC\n\nplt.figure()\nlw = 2\nplt.plot(np.array(Ytest),np.array(pred_y_score),\ncolor\n=\n'darkorange'\n,\n         \nlw\n=lw,label = \n'randomforest'\n)\nplt.plot(\ncolor\n=\n'navy'\n, \nlw\n=lw, \nlinestyle\n=\n'--'\n)\nplt.xlim([0.0, 5.0])\n\n#plt.ylim([0.0, 1.05])\n\nplt.xlabel(\n'False Positive Rate'\n)\nplt.ylabel(\n'True Positive Rate'\n)\nplt.title(\n'Receiver operating characteristic example'\n)\nplt.legend(\nloc\n=\n\"lower right\"\n)\nplt.show()\n\n\n\n运行结果及报错内容\n\n\n\n\n我想要达到的结果\n\n\n\n\n画出正确的ROC曲线\n\n", "Tag": "算法分析"}
{"Answer": "发布/订阅sensor_msgs::Image或sensor_msgs::CompressedImage", "Konwledge_Point": "应对NP完全问题", "Question": "ROS编写服务问题：图像处理相关\n这是我写的图像处理python代码，基于opencv。现在想把它封装成一个ros服务。请问各位该如何去改呢？\n以下是源码。\n\n\n \nimport cv2\nimport numpy \nas\n np\nimport os\n \n \ndef imgdetect(image):\n    blue_lower = np.\narray\n(\n[\n100\n, \n50\n, \n50\n]\n)\n    blue_upper = np.\narray\n(\n[\n124\n, \n255\n, \n255\n]\n)\n    hsv = cv2.cvt\nColor(\nimage\n, \ncv2\n.COLOR_BGR2HSV)\n\n    mask = cv2.\nin\nRange(\nhsv\n, \nlowerb\n=\nblue_lower\n, \nupperb\n=\nblue_upper\n)\n\n    Canny = cv2.\nCanny(\nmask\n, 9, 9)\n\n    circle = cv2.\nHoughCircles(Canny, \ncv2\n.HOUGH_GRADIENT, 1, 100, \nparam1\n=100, \nparam2\n=30, \nminRadius\n=100, \nmaxRadius\n=200)\n\n    return circle\n \n \nmyList = os.listdir('ImagesQuery')\n \ncap = cv2.\nVideoCapture(0)\n\n \n\nwhile\n True:\n    success, image = cap.read\n()\n\n    imgOriginal = image.copy\n()\n\n    cir = imgdetect(image)\n    \nif\n not cir is None:\n        cir = np.uint16(np.around(cir))\n        max_r, max_i = \n0\n, \n0\n\n        \nfor\n i \nin\n range(len(cir\n[:, :, \n2\n]\n[\n0\n]\n)):\n            \nif\n cir\n[:, :, \n2\n]\n[\n0\n]\n[\ni\n]\n > \n50\n \nand\n cir\n[:, :, \n2\n]\n[\n0\n]\n[\ni\n]\n > max_r:\n                max_i = i\n                max_r = cir\n[:, :, \n2\n]\n[\n0\n]\n[\ni\n]\n\n        x, y, r = cir\n[:, :, :]\n[\n0\n]\n[\nmax_i\n]\n\n        \nif\n y > r \nand\n x > r:\n            square = imgOriginal\n[\ny\n - \nr\n:\ny\n + \nr\n, \nx\n - \nr\n:\nx\n + \nr\n]\n\n            img_gray = cv2.cvt\nColor(\nsquare\n, \ncv2\n.COLOR_BGR2GRAY)\n\n            cv2.imshow('img_gray', img_gray)\n            \nfor\n i \nin\n range(\n4\n):\n                template = cv2.imread('ImagesQuery/' + myList\n[\ni\n]\n)\n                template = cv2.cvt\nColor(\ntemplate\n, \ncv2\n.COLOR_BGR2GRAY)\n\n                res = cv2.\nmatch\nTemplate(\nimg_gray\n, \ntemplate\n, \ncv2\n.TM_CCORR_NORMED)\n\n                (_, score, _, _) = cv2.min\nMaxLoc(\nres\n)\n\n                threshold = \n0.8\n\n                h, w = template.shape\n[:\n2\n]\n\n                \nif\n score > \n0.93\n:\n                    cv2.rectangle(imgOriginal, (x - r - \n5\n, y - r - \n5\n), (x + r + \n5\n, y + r + \n5\n), (\n0\n, \n255\n, \n0\n), \n2\n)\n                    \nif\n i\n == \n0\n:\n                        cv2.put\nText(\nimgOriginal\n, 'Back', (\nx\n, \ny\n + \nr\n + 10)\n, cv2.FONT_HERSHEY_DUPLEX, \n1\n, (\n255\n, \n0\n, \n0\n), \n2\n)\n                    \nif\n i\n == \n1\n:\n                        cv2.put\nText(\nimgOriginal\n, 'Forward', (\nx\n, \ny\n + \nr\n + 10)\n, cv2.FONT_HERSHEY_DUPLEX, \n1\n, (\n255\n, \n0\n, \n0\n), \n2\n)\n                    \nif\n i\n == \n2\n:\n                        cv2.put\nText(\nimgOriginal\n, 'Left', (\nx\n, \ny\n + \nr\n + 10)\n, cv2.FONT_HERSHEY_DUPLEX, \n1\n, (\n255\n, \n0\n, \n0\n), \n2\n)\n                    \nif\n i\n == \n3\n:\n                        cv2.put\nText(\nimgOriginal\n, 'Right', (\nx\n, \ny\n + \nr\n + 10)\n, cv2.FONT_HERSHEY_DUPLEX, \n1\n, (\n255\n, \n0\n, \n0\n), \n2\n)\n    cv2.imshow('img', imgOriginal)\n    cv2.wait\nKey(1)\n\n \n\n", "Tag": "算法分析"}
{"Answer": "lst=[]\nfor j in contours:\n    # 显示图片\n    lst.append(temp)\nboth = np.hstack(lst)\ncv2.imshow('line', both)\ncv2.waitKey()\n", "Konwledge_Point": "应对NP完全问题", "Question": "想用for循环遍历图中的两个轮廓，但是只能显示一半\n问题遇到的现象和发生背景\n\n\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport\n cv2\n\nimport\n numpy as np\n\n\nimagepath\n = 'E:/\n149\n_v4.png'\n\n# imagepath = ''\n\n\nimg\n = cv2.imread(imagepath, \n1\n)\n\ngray\n = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# 二值化\n\nret, \nthresh\n = cv2.threshold(gray, \n127\n, \n255\n, cv2.THRESH_BINARY)\n\n# 图片轮廓\n\nimage, contours, \nhierarchy\n = cv2.findContours(thresh, \n2\n, \n1\n)\n\nprint(len(contours))\nfor i \nin\n range(len(contours)):\n    \nif\n len(contours[i]) < \n10\n:        \n# 删除图中可能存在的独立小凸包\n\n        contours.remove(contours[i])\n\nfor j \nin\n contours:\n    \ncnt\n = j\n\n    \n# 寻找凸包并绘制凸包（轮廓）\n\n    \nhull\n = cv2.convexHull(cnt)\n    \n# hull = np.asarray(hull)\n\n\n    \nlength\n = len(hull)-\n1\n\n    \n# print(length)\n\n    \ndis_list\n = []\n    for k \nin\n range(length):\n        \ndis\n = np.linalg.norm(hull[k] - hull[k + \n1\n])\n        \nif\n dis < \n100\n:\n            dis_list.append(hull[k])\n            length \n-=\n \n1\n\n        \n# new_hull = np.delete(hull, k + 1, axis=0)\n\n        \n# print(dis)\n\n\n        \n# dis_list.append(dis)\n\n    \ndis_list\n = np.asarray(dis_list)\n\n    \ntemp\n = np.zeros(img.shape, np.uint8)        \n# 创建一个黑色的背景图\n\n    cv2.polylines(temp, [dis_list], True, (\n255\n, \n255\n, \n2\n), \n2\n)\n    \n# cv2.polylines(temp, [s_dis_list], True, (255, 5, 255), 2)\n\n    \n# cv2.polylines(img, [hull], True, (0, 255, 0), 2)\n\n    \n# 显示图片\n\n    cv2.imshow('line', temp)\n    cv2.waitKey()\n\n\n\n运行结果及报错内容\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果\n\n\n如何能让两个轮廓在一起显示", "Tag": "算法分析"}
{"Answer": "这个是生成网格点矩阵的。\nhttps://blog.csdn.net/lllxxq141592654/article/details/81532855", "Konwledge_Point": "应对NP完全问题", "Question": "matplotlib三位绘图的中meshgrid()函数的使用\nimport matplotlib\n.pyplot\n as plt\nimport numpy as np\n\nplt\n.rcParams\n[\n'font.sans-serif'\n]\n = \n[\n'Simhei'\n]\n\nplt\n.rcParams\n[\n'axes.unicode_minus'\n]\n = False\n\nax = plt\n.axes\n(projection=\n'3d'\n)\n\nx = np\n.linspace\n(-\n10\n, \n10\n, \n500\n)\ny = np\n.linspace\n(-\n10\n, \n10\n, \n300\n)\nX, Y = np\n.meshgrid\n(x, y)\nz = (\n1\n/\n4\n)*X**\n2\n-(\n1\n/\n2\n)*Y**\n2\n\n\nax\n.plot_surface\n(X, Y, z, cmap=\n'cool'\n)\nplt\n.axis\n(\n'off'\n)\n\nplt\n.show\n()\n\n\n\n\n如图 制作三维表面图像时为什么需要进行\n\n\n\n\n\n\nX\n, \nY\n = np.meshgrid(x, y)\n\n\n\n\n的操作", "Tag": "算法分析"}
{"Answer": "导包的代码我省略了，里面主要修改的是数据需要归一化、权重系统weight_init_std=0.01修改为了1、学习率降低(修改为0.05)。如果想做的更好一些，可以增加dropout\n# 显示图形\ndef img_show(img):\n    pil_img = Image.fromarray(np.uint8(img))\n    pil_img.show()\n \n \n# 2层神经网络的类\nclass TwoLayerNet:\n    def __init__(self, input_size, hidden_size, output_size, weight_init_std=1):\n        self.params = {}\n        self.params['w1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['w2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n        self.params['b2'] = np.zeros(output_size)\n \n    def predict(self, x):  # x=输入\n        w1, w2 = self.params['w1'], self.params['w2']#权重\n        b1, b2 = self.params['b1'], self.params['b2']#偏移\n        a1 = np.dot(x, w1) + b1\n        z1 = sigmoid(a1)#第一层输出\n        a2 = np.dot(z1, w2) + b2\n        z2 = softmax(a2)#第二层输出\n        return z2\n \n    def loss(self, x, t):  # x=输入，t=监督数据\n        y = self.predict(x)\n        return cross_entropy_error(y, t)\n \n    def accuracy(self, x, t):\n        y = self.predict(x)\n        y = np.argmax(y, axis=1)\n        t = np.argmax(t, axis=1)\n        accuracy = np.sum(y == t) / float(x.shape[0])\n        return accuracy\n \n    def numerical_gradient(self, x, t):\n        loss_w = lambda w: self.loss(x, t)#损失函数\n        grads = {\n            'w1': numerical_gradient(loss_w, self.params['w1']),\n            'b1': numerical_gradient(loss_w, self.params['b1']),\n            'w2': numerical_gradient(loss_w, self.params['w2']),\n            'b2': numerical_gradient(loss_w, self.params['b2'])\n        }\n        return grads\n \n \n# 梯度函数\ndef numerical_gradient(f, x):\n    h = 1e-4\n    grad = np.zeros_like(x)\n    for idx in range(x.shape[0]):\n        tmp_val = x[idx]\n        x[idx] = tmp_val + h\n        fxh1 = f(x)\n        x[idx] = tmp_val - h\n        fxh2 = f(x)\n        grad[idx] = (fxh1 - fxh2) / (2 * h)#求梯度\n        x[idx] = tmp_val#还原x\n    return grad\n \n \n# 误差函数cross entropy error\ndef cross_entropy_error(y, t):\n    delta = 1e-7\n    return -np.sum(t * np.log(y + delta))\n \n \n# softmax函数\ndef softmax(a):\n    c = np.max(a)\n    exp_a = np.exp(a - c)  # 防止溢出\n    sum_exp_a = np.sum(exp_a)\n    y = exp_a / sum_exp_a\n    return y\n \n \n# sigmoid函数\ndef sigmoid(a):\n    out = a.copy()\n    sel = ((a > 100) & (a < -100))\n    out = 1 / (1 + np.exp(-a))#sigmoid计算\n    out[sel] = 1 / (1 + np.exp(-100))#防止指数爆炸\n    return out\n \n \n# 验证集转one-hot\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\ny_train = keras.utils.to_categorical(y_train)\ny_test = keras.utils.to_categorical(y_test)\n \n# 数据改为60000*784浮点格式\nx_train = x_train.reshape(x_train.shape[0], 784).astype('float')\nx_train = x_train / 255.0\n \ntrain_loss_list = []\n \n# 参数初始化\niters_num = 100  # 循环次数\ntrain_size = x_train.shape[0]  # 总数据量\nbatch_size = 32  # 每次取出的数据量\nlearning_rate = 0.05  # 学习率\nnetwork = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)  # 创建对象\n \nfor i in range(iters_num):\n    # 获取mini_batch\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    y_batch = y_train[batch_mask]\n \n    # 计算梯度\n    grad = network.numerical_gradient(x_batch, y_batch)\n    # 更新参数\n    for key in ('w1', 'b1', 'w2', 'b2'):\n        network.params[key] = network.params[key] - learning_rate * grad[key]\n    # 损失量\n    loss = network.loss(x_batch, y_batch)\n    train_loss_list.append(loss)\n \n# 损失量图像\nx = np.arange(0, iters_num / 10, 0.1)\ny = np.array(train_loss_list)\nplt.plot(x, y)\nplt.show()\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "Mnist两层神经网络梯度一直为零\n  最近开始学习机器学习，在编写简单的二层神经网络的过程中，发现损失函数一直居高不下，然后看了一下损失函数的梯度，发现梯度一直都是零。\n  我在途中试着改变损失函数，将cross entropy error函数换成mean_squared_error函数；又试着改变激活函数:sigmoid函数，Relu函数，softmax函数都试着用过了；然后又试着改变训练次数，mini_batch的大小，学习率，可惜都无济于事，我希望能够找到问题所在。以下是我的完整代码。希望有大佬能指点一下迷津。\n\n\nfrom\n keras.datasets \nimport\n mnist\n\nimport\n keras\n\nimport\n numpy \nas\n np\n\nfrom\n PIL \nimport\n Image\n\nimport\n matplotlib.pylab \nas\n plt\n\n\n\n# 显示图形\n\n\ndef\n \nimg_show\n(\nimg\n):\n    pil_img = Image.fromarray(np.uint8(img))\n    pil_img.show()\n\n\n\n# 2层神经网络的类\n\n\nclass\n \nTwoLayerNet\n:\n    \ndef\n \n__init__\n(\nself, input_size, hidden_size, output_size, weight_init_std=\n0.01\n):\n        self.params = {}\n        self.params[\n'w1'\n] = weight_init_std * np.random.randn(input_size, hidden_size)\n        self.params[\n'b1'\n] = np.zeros(hidden_size)\n        self.params[\n'w2'\n] = weight_init_std * np.random.randn(hidden_size, output_size)\n        self.params[\n'b2'\n] = np.zeros(output_size)\n\n    \ndef\n \npredict\n(\nself, x\n):  \n# x=输入\n\n        w1, w2 = self.params[\n'w1'\n], self.params[\n'w2'\n]\n#权重\n\n        b1, b2 = self.params[\n'b1'\n], self.params[\n'b2'\n]\n#偏移\n\n        a1 = np.dot(x, w1) + b1\n        z1 = sigmoid(a1)\n#第一层输出\n\n        a2 = np.dot(z1, w2) + b2\n        z2 = softmax(a2)\n#第二层输出\n\n        \nreturn\n z2\n\n    \ndef\n \nloss\n(\nself, x, t\n):  \n# x=输入，t=监督数据\n\n        y = self.predict(x)\n        \nreturn\n cross_entropy_error(y, t)\n\n    \ndef\n \naccuracy\n(\nself, x, t\n):\n        y = self.predict(x)\n        y = np.argmax(y, axis=\n1\n)\n        t = np.argmax(t, axis=\n1\n)\n        accuracy = np.\nsum\n(y == t) / \nfloat\n(x.shape[\n0\n])\n        \nreturn\n accuracy\n\n    \ndef\n \nnumerical_gradient\n(\nself, x, t\n):\n        loss_w = \nlambda\n w: self.loss(x, t)\n#损失函数\n\n        grads = {\n            \n'w1'\n: numerical_gradient(loss_w, self.params[\n'w1'\n]),\n            \n'b1'\n: numerical_gradient(loss_w, self.params[\n'b1'\n]),\n            \n'w2'\n: numerical_gradient(loss_w, self.params[\n'w2'\n]),\n            \n'b2'\n: numerical_gradient(loss_w, self.params[\n'b2'\n])\n        }\n        \nreturn\n grads\n\n\n\n# 梯度函数\n\n\ndef\n \nnumerical_gradient\n(\nf, x\n):\n    h = \n1e-4\n\n    grad = np.zeros_like(x)\n    \nfor\n idx \nin\n \nrange\n(x.shape[\n0\n]):\n        tmp_val = x[idx]\n        x[idx] = tmp_val + h\n        fxh1 = f(x)\n        x[idx] = tmp_val - h\n        fxh2 = f(x)\n        grad[idx] = (fxh1 - fxh2) / (\n2\n * h)\n#求梯度\n\n        x[idx] = tmp_val\n#还原x\n\n    \nreturn\n grad\n\n\n\n# 误差函数cross entropy error\n\n\ndef\n \ncross_entropy_error\n(\ny, t\n):\n    delta = \n1e-7\n\n    \nreturn\n -np.\nsum\n(t * np.log(y + delta))\n\n\n\n# softmax函数\n\n\ndef\n \nsoftmax\n(\na\n):\n    c = np.\nmax\n(a)\n    exp_a = np.exp(a - c)  \n# 防止溢出\n\n    sum_exp_a = np.\nsum\n(exp_a)\n    y = exp_a / sum_exp_a\n    \nreturn\n y\n\n\n\n# sigmoid函数\n\n\ndef\n \nsigmoid\n(\na\n):\n    out = a.copy()\n    sel = ((a > \n100\n) & (a < -\n100\n))\n    out = \n1\n / (\n1\n + np.exp(-a))\n#sigmoid计算\n\n    out[sel] = \n1\n / (\n1\n + np.exp(-\n100\n))\n#防止指数爆炸\n\n    \nreturn\n out\n\n\n\n# 验证集转one-hot\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\ny_train = keras.utils.to_categorical(y_train)\ny_test = keras.utils.to_categorical(y_test)\n\n\n# 数据改为60000*784浮点格式\n\nx_train = x_train.reshape(x_train.shape[\n0\n], \n784\n).astype(\n'float'\n)\n\ntrain_loss_list = []\n\n\n# 参数初始化\n\niters_num = \n100\n  \n# 循环次数\n\ntrain_size = x_train.shape[\n0\n]  \n# 总数据量\n\nbatch_size = \n32\n  \n# 每次取出的数据量\n\nlearning_rate = \n0.1\n  \n# 学习率\n\nnetwork = TwoLayerNet(input_size=\n784\n, hidden_size=\n100\n, output_size=\n10\n)  \n# 创建对象\n\n\n\nfor\n i \nin\n \nrange\n(iters_num):\n    \n# 获取mini_batch\n\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    y_batch = y_train[batch_mask]\n\n    \n# 计算梯度\n\n    grad = network.numerical_gradient(x_batch, y_batch)\n    \n# 更新参数\n\n    \nfor\n key \nin\n (\n'w1'\n, \n'b1'\n, \n'w2'\n, \n'b2'\n):\n        network.params[key] = network.params[key] - learning_rate * grad[key]\n    \n# 损失量\n\n    loss = network.loss(x_batch, y_batch)\n    train_loss_list.append(loss)\n\n\n# 损失量图像\n\nx = np.arange(\n0\n, iters_num / \n10\n, \n0.1\n)\ny = np.array(train_loss_list)\nplt.plot(x, y)\nplt.show()\n\n", "Tag": "算法分析"}
{"Answer": "你这个没用输出啊", "Konwledge_Point": "应对NP完全问题", "Question": "关于正态性检验的问题，如何解决？\n正态性检验\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import shapiro\nfrom pandas import Series, DataFrame\ndata=pd.read_excel('D:/trial/data.xlsx')\nshapiro(data.ALT)\n代码如上，运行后无报错，但也无任何结果显示，请问错在那里？感谢", "Tag": "算法分析"}
{"Answer": "相当于一个没有名字的变量，举几个例子：\n>>> a = [1,2]\n>>> _, b = a\n>>> b\n2\n>>> #相当于 b=a[1]\n>>> a = [1,2,3,4]\n>>> _, *b = a\n>>> b\n[2, 3, 4]\n>>> _,_, *b = a\n>>> b\n[3, 4]\n>>> _,_,_,b = a\n>>> b\n4\n", "Konwledge_Point": "应对NP完全问题", "Question": "函数里面下划线逗号   _,    有什么作用啊？\n问题遇到的现象和发生背景\n\n\n问题相关代码，请勿粘贴截图\n\n\ndef\n buy_lower(stock):\n    \n#设置一个一行两列的可视化图表\n\n    \n_\n, axs=plt.subplots(nrows=\n1\n,ncols=\n2\n,figsize=(\n16\n,\n5\n))\n\n    \n#绘制前450天的股票走势图，np.cumsum():序列连续求和\n\n    \naxs\n[\n0\n].plot(np.arange(\n0\n,days-keep_days),\n               \nstock_day_train\n[stock].cumsum())\n\n\n\n\n\n运行结果及报错内容\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果\n\n\n没有加下划线逗号   _,   就出现上面的报错，加了就能正常运行打印出来图，所以下划线逗号到底是代表什么意思呢", "Tag": "算法分析"}
{"Answer": "所谓信息熵描述了信息源各可能事件发生的不确定性。试想若某人仅在一家药店购药，则他的行为是确定的；而若在多家药店购药，则存在一定的不确定性，需要通过博主上面给出的公式计算。以上就是对这道题的理解。\n解题思路：首先，判断每一个buyer（BID00i）是否在一家药店购药，也就是是判断每一列的非零值是否不唯一；第二，若buyer仅在一家药店购药，则范围信息熵值为0；第三，若buyer在多家药店购药，则按上述公式计算。给一个简单的编程思路：首先循环遍历每列，把非零值找出来，代入np.log2(a) * a * (-1)计算，然后将本列所有计算好的值求sum，即为该列对应的buyer的信息熵值。", "Konwledge_Point": "应对NP完全问题", "Question": "python/pandas 计算 数值 信息熵值\n想计算信息熵值，结果 output的是 nan\n问题准备：\n图为sheet_new dataframe 格式\n\n\n举例：\n若某甲仅在一家药店留有购药记录，其信息熵值为0，因其p=1。\n若某甲在不同药店的购药金额为[10,30,60]，其信息熵值为\n'''-(0.1 * np.log2(0.1) + 0.3 * np.log2(0.3) + 0.6 * np.log2(0.6))'''，即1.295461844238322。\n\n\n思路：\n\n\n#想计算每一个 Buyer 的信息熵值\n\n\n#想定义 信息熵值\n\ndef infor(sheet_new):\n    \na\n = sheet_new.values / \n100\n\n    \nreturn\n \nsum\n(np.\nlog2\n(\na\n) * \na\n * (\n-1\n))\n\n\n\n#尝试计算BID0001 的信息熵值\n\n\nprint\n(infor(sheet_new.BID0001))\n\n#输出结果为   nan 并有警告 但没报错\n\n\n\n\n可能我的思路有误\n希望大家帮忙解答\n谢谢！", "Tag": "算法分析"}
{"Answer": "zip(list1,list2)是返回一个迭代对象, 每次迭代时会同时读取list1,list2两个列表中的各个元素打包成一个元组,元组中有两个元素分别是list1,list2两个列表中相同下标的元素,比如：list1 = [\"key1\",\"key2\",\"key3\"]list2 = [\"value1\",\"value2\",\"value3\"]print(list( zip(list1,list2)))结果为[('key1', 'value1'), ('key2', 'value2'), ('key3', 'value3')]\n\nd = {n:v for n,v in zip(list1,list2)}是字典生成式,遍历zip(list1,list2)返回的迭代对象，每次遍历将之前打包成的元组中的两个元素拆开分别赋值给n和v，n:v是用n做为字典的键，用v做为字典的值，生成字典。d的结果为{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}\n ", "Konwledge_Point": "应对NP完全问题", "Question": "python在机器学习中的循环的语法问题\nprint(\"Sample counts per class:\\n{}\".format(\n     {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))\n\n\n中的  n:v for n,v in zip() 这句语法怎么理解  \n\n\n尤其是其中的 n:v 这里", "Tag": "算法分析"}
{"Answer": "你题目的解答代码如下：\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\nnp.random.seed(42)\nimport warnings\nwarnings.filterwarnings('ignore')\n\nx = 2*np.random.uniform(-3, 3, size=100)\nX = x.reshape(100, 1)\ny = 0.5*X**2+X+np.random.randn(100,1)\n\nplt.scatter(x, y)\nplt.axis([-3,3,-10,10])\n\nX_poly = np.hstack([X, X**2])\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, y)\ny_predict = lin_reg.predict(X_poly)\n\n# 由于x是乱的，所以应该进行排序\nplt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')\nplt.show()\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "补全代码生成一个多项式回归曲线 并对代码进行简单的注释\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\nnp.random.seed(42)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nx = 2\nnp.random.uniform(-3, 3, size=100)\nX = x.reshape(100, 1)\ny = 0.5\nX**2+X+np.random.randn(100,1)\n\n\nplt.scatter(x, y)\nplt.axis([-3,3,-10,10])\nplt.show()\n\n\nX_poly = np.hstack([X, X**2])\n\n\n补全代码生成一个多项式回归曲线 并对上述各行代码进行简单的注释", "Tag": "算法分析"}
{"Answer": "看出问题了吗？", "Konwledge_Point": "应对NP完全问题", "Question": "python 大小写字母变量会影响迭代吗\n问题遇到的现象和发生背景\n\n\n第一段代码其中变量x,y1,y2都是大写 能正常运行 没有报错\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nn=\n12\n\n\nX\n=np.arange(n)\n\nY\n1\n=(\n1\n-\nX\n/float(n))*np.random.uniform(\n0\n.5\n,\n1\n.0\n,n)\n\nY\n2\n=(\n1\n-\nX\n/float(n))*np.random.uniform(\n0\n.5\n,\n1\n.0\n,n)\nplt.bar(\nX\n,+\nY\n1\n,facecolor=\n'#9966ff'\n,edgecolor=\n'white'\n)\nplt.bar(\nX\n,-\nY\n2\n,facecolor=\n'#ff9966'\n,edgecolor=\n'white'\n)\nplt.xlim(\n-.5\n,n)\nplt.ylim(\n-1.25\n,\n1\n.25\n)\nfor x,y in zip(\nX\n,\nY\n1\n):\n        plt.text(x,y+\n0\n.05\n,\n'%.2f'\n%y,ha=\n'center'\n,va=\n'bottom'\n)\nfor x,y in zip(\nX\n,\nY\n2\n):\n        plt.text(x,-y\n-0.05\n,\n'%.2f'\n%y,ha=\n'center'\n,va=\n'top'\n)\nprint(\nY\n1\n)\nprint(\nY\n2\n)\nplt.show()\n\n\n\n\n但是如果把x，y1，y2改成小写 就会报错代码如下\n\n\nimport\n matplotlib.pyplot as plt\n\nimport\n numpy as np\n\nn\n=\n12\n\n\nx\n=np.arange(n)\n\ny1\n=(\n1\n-x/float(n))*np.random.uniform(\n0\n.\n5\n,\n1\n.\n0\n,n)\n\ny2\n=(\n1\n-x/float(n))*np.random.uniform(\n0\n.\n5\n,\n1\n.\n0\n,n)\n\nplt\n.bar(x,+y1,facecolor='#\n9966\nff',edgecolor='white')\n\nplt\n.bar(x,-y2,facecolor='#ff9966',edgecolor='white')\n\nplt\n.xlim(-.\n5\n,n)\n\nplt\n.ylim(-\n1\n.\n25\n,\n1\n.\n25\n)\n\nfor\n x,y in zip(x,y1):\n        \nplt\n.text(x,y+\n0\n.\n05\n,'%.\n2\nf'%y,ha='center',va='bottom')\n\nfor\n x,y in zip(x,y2):\n        \nplt\n.text(x,-y-\n0\n.\n05\n,'%.\n2\nf'%y,ha='center',va='top')\n\n\nplt\n.show()\n\n\n\n\n报错如下\n\n\n\n    for \nx\n,\ny in zip(\nx\n,\ny\n2\n):\nTypeError: zip argument \n#1\n must support iteration\n", "Tag": "算法分析"}
{"Answer": "\nThis should work for you:\n$np = $_POST[\"np\"]; // `sky` for example\n$a = \"inc/\" . $np . \".php\";\n$new = fopen($a, \"w\") or die(\"Unable to open file!\");\n$str='$np = \\'' . $np . \"';\";\nfwrite($new, $str);\nfclose($new);\n\nThe problem was that you are using \" to define the string $str. Therefore, the content of $np was printed instead of the string '$np'.\n", "Konwledge_Point": "应对NP完全问题", "Question": "如何在新的php文件中编写php代码\n\n\n\n$np = $_POST[\"np\"]; // `sky` for example\n$a = \"inc/\" . $np . \".php\";\n$new = fopen($a, \"w\") or die(\"Unable to open file!\");\n$str=\"$np='\" . $np . \"';\";\nfwrite($new, $str);\nfclose($new);\n\n\n\n\nI need to create a new \ninc/.sky.php\n file and write the following inside it:\n\n\n\n$np = 'sky';\n\n\n\n\nBut what I get is:\n\n\n\nsky='sky';\n\n\n\n\nAny idea?\n\n    ", "Tag": "算法分析"}
{"Answer": "下有代码，可直接复制使用。如有帮助，敬请采纳，你的采纳是我前进的动力，O(∩_∩)O谢谢！！！！！！！！路过的朋友也可以点个赞~(≧▽≦)/~\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(r'C:\\Users\\Administrator\\Desktop\\1.csv')\nprint(df, type(df))\nx = df.iloc[:3]['A']\nprint(x, type(x))\n\nx = ''.join(x.tolist()).replace('][', ',').replace(']', '').replace('[', '')\nx_np = np.fromstring(x, dtype=float, sep=',').reshape(-1, 2)\n\nprint(x_np + 1)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "numpy.core._exceptions._UFuncNoLoopError: ufunc 'add' did not contain a loop with signature\n想要给以下表格的每个数字都加上1\n\n\n代码如下\n\n\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport math\ndf = pd.read_csv(r'C:\\Users\\Administrator\\Desktop\\1.csv')\nprint(df, type(df))\nx = df.iloc[:3]['A']\nprint(x, type(x))\nprint(np.array(list(x))+1)\n\n\n出现以下错误提示，尝试用各种方法将字符转为数字，但均未成功，该如何解决？", "Tag": "算法分析"}
{"Answer": "array_P=[np.random.randint(0,2,10)]\r\narray_A=[np.random.randint(0,2,10)]\r\n->\r\narray_P=np.random.randint(0,2,10).tolist()\r\narray_A=np.random.randint(0,2,10).tolist()\r\n加上方括号把array变成数组的数组了", "Konwledge_Point": "应对NP完全问题", "Question": "python随机产生0和1，得出F1-score的问题，求大神帮忙\nimport numpy as np\n\narray_P=[np.random.randint(0,2,10)]\n\narray_A=[np.random.randint(0,2,10)]\n\n代码如下\n\nprint(array_P)\n\nprint(array_A)\n\nTP=0\n\nFP=0\n\nFN=0\n\nfor i in range(10):\n\n    if array_P[i]==1 and array_A[i]==1:\n\n        TP+=1\n\n    elif array_P[i]==1 and array_A[i]==1:\n\n        FP+=1\n\n    elif array_P[i]==0 and array_A[i]==1:\n\n        FN+=1\n\nprint(TP)\n\nprint(FP)\n\nprint(FN)\n\nP=TP/(TP+FP)\n\nR=TP/(TP+FN)\n\nfl_score = (2*R*P)/(R+P)\n\nprint('name:', fl_score)\n\n报错\n\nrunfile('D:/python/files/未命名0.py')\n\n[array([1, 0, 1, 1, 0, 0, 0, 0, 1, 0])]\n\n[array([1, 1, 0, 1, 1, 0, 1, 0, 0, 1])]\n\nTraceback (most recent call last):\n\n\n\nFile \"D:\\python\\files\\未命名0.py\", line 30, in \n\n    if array_P[i]==1 and array_A[i]==1:\n\n\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "Tag": "算法分析"}
{"Answer": "new BusinessCerts()对象,并传递这三个参数\nif __name__ == \"__main__\":\n  img=''\n  ocr_recognition = OCRRecognition()\n  dbnet_det = DbNetInfer()\n  certs = BusinessCerts(img,dbnet_det,ocr_recognition)\n", "Konwledge_Point": "应对NP完全问题", "Question": "OCR的学习，在探索代码的时候遇到问题\n在学习OCR识别时，我拿到一串代码，但是看不出来要怎么让它跑起来拿到结果（只是一个类，原本是通过接口访问的代码）\n\n\nimport\n re\n\n\nimport\n numpy \nas\n np\n\nfrom\n PIL \nimport\n Image\n\n\nfrom\n utils \nimport\n order_point, crop_image, calc_distance\n\n\n\nclass\n \nBusinessCerts\n:\n\n    \ndef\n \n__init__\n(\nself, img=\n''\n, dbnet_det=\n''\n, ocr_recognition=\n''\n):\n        self.img = img\n        self.dbnet_det = dbnet_det\n        self.ocr_recognition = ocr_recognition\n        self.det_result = []\n        self.box_result = []\n        self.left_uppers = []\n        self.line_space = -\n1\n\n        self.line_height = -\n1\n\n\n        self.box_detect()\n        self.ocr_recognize()\n\n        self.left_uppers = np.array([[point[\n'pts'\n][\n0\n], point[\n'pts'\n][\n1\n]] \nfor\n point \nin\n self.box_result])\n        self.left_lowers = np.array([[point[\n'pts'\n][\n6\n], point[\n'pts'\n][\n7\n]] \nfor\n point \nin\n self.box_result])\n        self.results = {\n'名称'\n: \n''\n, \n'统一社会信用代码'\n: \n''\n, \n'住所'\n: \n''\n}\n\n        self.ocr_pipline()\n\n    \n# DBNet文字检测\n\n    \ndef\n \nbox_detect\n(\nself\n):\n        det_result = self.dbnet_det.predict(self.img)\n        det_result = det_result.reshape(-\n1\n, \n8\n)\n        self.det_result = det_result[det_result[:, \n1\n].argsort()]\n        \nreturn\n det_result[det_result[:, \n1\n].argsort()]\n\n    \n# 识别文字\n\n    \ndef\n \nocr_recognize\n(\nself\n):\n        box_result = []\n\n        \nfor\n i \nin\n \nrange\n(self.det_result.shape[\n0\n]):\n            box_dict = {}\n            pts = order_point(self.det_result[i])\n            image_crop = crop_image(self.img, pts)\n            image_crop = Image.fromarray(image_crop)\n            result = self.ocr_recognition.rec(image_crop)\n            pts = pts.reshape(-\n1\n).astype(\nint\n)\n            box_dict[\n'pts'\n] = pts\n            box_dict[\n'text'\n] = result\n            box_result.append(box_dict)\n\n        self.box_result = box_result\n\n    \n# 根据标签索引，通过计算距离确定对应值\n\n    \ndef\n \nget_item_value\n(\nself, key_idx\n):\n        curr_pts = self.box_result[key_idx][\n'pts'\n]\n        curr_right_upper = np.array([curr_pts[\n2\n], curr_pts[\n3\n]])\n\n        dists = calc_distance(curr_right_upper, self.left_uppers)\n        dists[key_idx] = \nfloat\n(\n'inf'\n)\n\n        value_idx = np.argmin(dists)\n        \nreturn\n value_idx, self.box_result[value_idx][\n'text'\n]\n\n    \n# 获取名称对应Box索引\n\n    \ndef\n \nget_name_index\n(\nself\n):\n        index = -\n1\n\n        \nfor\n i, temp \nin\n \nenumerate\n(self.box_result):\n            \nif\n \n'名称'\n == temp[\n'text'\n] \nor\n \n'称'\n == temp[\n'text'\n]:\n                index = i\n        \nreturn\n index\n\n    \ndef\n \nget_credit_code_index\n(\nself\n):\n        index = -\n1\n\n        \nfor\n i, temp \nin\n \nenumerate\n(self.box_result):\n            \nif\n \n'统一社会信用代码'\n == temp[\n'text'\n] \nor\n \n'信用代码'\n == temp[\n'text'\n] \nor\n \n'代码'\n == temp[\n'text'\n]:\n                index = i\n        \nreturn\n index\n\n    \ndef\n \nget_address_index\n(\nself\n):\n        index = -\n1\n\n        \nfor\n i, temp \nin\n \nenumerate\n(self.box_result):\n            \nif\n \n'住所'\n == temp[\n'text'\n] \nor\n \n'经营场所'\n == temp[\n'text'\n]:\n                index = i\n        \nreturn\n index\n\n    \ndef\n \nocr_pipline\n(\nself\n):\n        address_index = self.get_address_index()\n        \nif\n address_index != -\n1\n:\n            address_value_idx, address_result = self.get_item_value(address_index)\n\n            address_value_pts = self.box_result[address_value_idx][\n'pts'\n]\n            self.line_height = address_value_pts[\n7\n] - address_value_pts[\n1\n]\n\n            address_value_left_lower = np.array([address_value_pts[\n6\n], address_value_pts[\n7\n]])\n            \n# 计算两行之间的距离，住所Box左下角点和其他Box左上角点的距离\n\n            line_space_dists = calc_distance(address_value_left_lower, self.left_uppers)\n            line_space_dists[address_value_idx] = \nfloat\n(\n'inf'\n)\n\n            \n# 计算行距\n\n            left_lowers = [[point[\n'pts'\n][\n6\n], point[\n'pts'\n][\n7\n]] \nfor\n point \nin\n self.box_result]\n            left_upper = np.array([address_value_pts[\n0\n], address_value_pts[\n1\n]])\n            line_height_dists = calc_distance(left_upper, left_lowers)\n            line_height_dists[address_value_idx] = \nfloat\n(\n'inf'\n)\n            self.line_space = \nmin\n(line_height_dists)\n\n            \n# 如果最小行距小于正常行距的 1/2 ，认为有换行\n\n            \nif\n \nmin\n(line_space_dists) < (self.line_space / \n2\n):\n                min_idx = np.argmin(line_space_dists)\n                next_line = self.box_result[min_idx][\n'text'\n]\n                address_result += next_line\n\n            self.results[\n'住所'\n] = address_result\n\n        name_index = self.get_name_index()\n\n        \nif\n name_index != -\n1\n:\n            name_value_idx, name_result = self.get_item_value(name_index)\n\n            name_value_pts = self.box_result[name_value_idx][\n'pts'\n]\n            name_value_left_lower = np.array([name_value_pts[\n6\n], name_value_pts[\n7\n]])\n\n            \n# 计算两行之间的距离，名称Box左下角点和其他Box左上角点的距离\n\n            line_space_dists = calc_distance(name_value_left_lower, self.left_uppers)\n            line_space_dists[name_value_idx] = \nfloat\n(\n'inf'\n)\n\n            \n# 如果最小行距小于正常行距的 1/2 ，认为有换行\n\n            \nif\n \nmin\n(line_space_dists) < (self.line_space / \n2\n):\n                min_idx = np.argmin(line_space_dists)\n                next_line = self.box_result[min_idx][\n'text'\n]\n                address_result += next_line\n\n            self.results[\n'名称'\n] = name_result\n        credit_code_index = self.get_credit_code_index()\n        \nif\n credit_code_index != -\n1\n:\n            _, credit_code_result = self.get_item_value(credit_code_index)\n            re_result = re.findall(\nr'[0-9A-Z]{15,}'\n, credit_code_result)\n            \nif\n re_result:\n                self.results[\n'统一社会信用代码'\n] = re_result[\n0\n]\n            \nelse\n:\n                \nfor\n box \nin\n self.box_result:\n                    re_result = re.findall(\nr'[0-9A-Z]{15,}'\n, box[\n'text'\n])\n                    \nif\n re_result:\n                        self.results[\n'统一社会信用代码'\n] = re_result[\n0\n]\n                        \nbreak\n\n\nif\n __name__ == \n\"__main__\"\n:\n    \n\n\n\n我的IF后要怎么写才能单独把这串代码运行起来？\n\n\n我想到达成的目的就是把这个类给它实例化，然后运行拿到结果", "Tag": "算法分析"}
{"Answer": "你的代码中，使用QImage对象替换了QPixmap对象来显示图片，由于QImage对象是只读的，因此您无法在QImage对象上执行操作。你需要将QImage对象转换为QPixmap对象，然后再将其设置为标签的图像。这是修改以后的代码\nfrom PyQt5 import QtCore, QtGui, QtWidgets\nclass Ui_Form(object):\n    def setupUi(self, Form):\n        Form.setObjectName(\"Form\")\n        Form.resize(400, 300)\n        self.label = QtWidgets.QLabel(Form)\n        self.label.setGeometry(QtCore.QRect(60, 90, 261, 151))\n        self.label.setStyleSheet(\"background-color:rgb(85, 170, 255)\")\n        self.label.setObjectName(\"label\")\n        self.pushButton = QtWidgets.QPushButton(Form)\n        self.pushButton.setGeometry(QtCore.QRect(50, 30, 75, 23))\n        self.pushButton.setObjectName(\"pushButton\")\n\n        self.retranslateUi(Form)\n        QtCore.QMetaObject.connectSlotsByName(Form)\n\n    def retranslateUi(self, Form):\n        _translate = QtCore.QCoreApplication.translate\n        Form.setWindowTitle(_translate(\"Form\", \"Form\"))\n        self.label.setText(_translate(\"Form\", \"Label\"))\n        self.pushButton.setText(_translate(\"Form\", \"PushButton\"))\n\nimport sys\n\nfrom PIL import Image, ImageQt\nfrom PyQt5 import QtGui, QtCore, QtWidgets\nfrom PyQt5.QtWidgets import QMainWindow\nimport numpy as np\n\nfrom testLabel import Ui_Form\nimport cv2\n\nclass MyMainForm(QMainWindow, Ui_Form):\n    def __init__(self):\n        super().__init__()\n        self.setupUi(self)\n        self.pushButton.clicked.connect(self.openPicture)\n\n\n    def openPicture(self):\n\n        # pix = Image.open('result.jpg')\n        # pix = ImageQt.toqpixmap(pix)\n        # img = cv2.imread('result.jpg')\n        # pix = QtGui.QPixmap('result.jpg')\n        # self.label.setScaledContents(True)\n        img = np.random.randint(0, 255, [10, 60, 3], np.uint8)\n        img = img.astype(\"uint8\")\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = QtGui.QImage(img, img.shape[1], img.shape[0], img.shape[1] * 3, QtGui.QImage.Format_RGB888)\n\n        pixmap = QtGui.QPixmap.fromImage(img)\n        self.label.setPixmap(pixmap)\n\n\nif __name__ == '__main__':\n    QtCore.QCoreApplication.setAttribute(QtCore.Qt.AA_EnableHighDpiScaling)\n    app = QtWidgets.QApplication(sys.argv)\n    my = MyMainForm()\n    my.show()\n    sys.exit(app.exec_())\n\n如果能解决问题，望采纳", "Konwledge_Point": "应对NP完全问题", "Question": "python， Pyqt5的图像显示问题\n我想使用python的pyqt5编写一个QLabel显示图片，直接读取图片为QPixmap后用setPixmap显示图片的时候正常，在使用PIL.image读取图片为PIL.Image类型后再用ImageQt.toqpixmap转换为QPixmap后就无法显示图片了，程序跑了直接退出也不报错，debug也是直接退出。这个有什么解决方案吗。ps:不直接使用Qpixmap是因为某个项目里面用到PIL.Image进行运算，运算后需要转换为Qpixmap进行显示\n补充，我自己在找解决办法的时候发现，我将PIL.Image类型转化为ndarray后直接用matplotlib进行显示是可以的，但是转换为Pixmap又不行了。后面我尝试随机生成一个RGB数据数组转换为Pixmap发现也无法显示。由于程序直接退出不报错，不知道问题所在，下面是我的测试代码。\n\n\nfrom PyQt5 import QtCore, QtGui, QtWidgets\n\nclass\n \nUi_Form(\nobject\n)\n:\n    def setup\nUi(\nself\n, Form)\n:\n        \nForm\n.\nset\nObjectName(\n\"Form\"\n)\n\n        \nForm\n.\nresize(\n400\n, \n300\n)\n        self.label = QtWidgets.\nQLabel(Form)\n\n        self.label.set\nGeometry(QtCore.QRect(60, 90, 261, 151)\n)\n        self.label.set\nStyleSheet(\n\"background-color:rgb(85, 170, 255)\"\n)\n\n        self.label.set\nObjectName(\n\"label\"\n)\n\n        self.pushButton = QtWidgets.\nQPushButton(Form)\n\n        self.pushButton.set\nGeometry(QtCore.QRect(50, 30, 75, 23)\n)\n        self.pushButton.set\nObjectName(\n\"pushButton\"\n)\n\n\n        self.retranslate\nUi(Form)\n\n        \nQtCore\n.\nQMetaObject\n.\nconnect\nSlotsByName(Form)\n\n\n    def retranslate\nUi(\nself\n, Form)\n:\n        _translate = \nQtCore\n.\nQCoreApplication\n.\ntranslate\n        \nForm\n.\nset\nWindowTitle(\n_translate\n(\n\"Form\"\n, \n\"Form\"\n)\n)\n        self.label.set\nText(\n_translate\n(\n\"Form\"\n, \n\"Label\"\n)\n)\n        self.pushButton.set\nText(\n_translate\n(\n\"Form\"\n, \n\"PushButton\"\n)\n)\n\n\n\n\nimport sys\n\nfrom PIL import Image, ImageQt\nfrom PyQt5 import QtGui, QtCore, QtWidgets\nfrom PyQt5.QtWidgets import QMainWindow\nimport numpy \nas\n np\n\nfrom testLabel import Ui_Form\nimport cv2\n\n\nclass\n \nMyMainForm(QMainWindow, Ui_Form)\n:\n    def \n__init__(\nself\n)\n:\n        super\n()\n.\n__init__()\n\n        self.setup\nUi(\nself\n)\n\n        self.pushButton.clicked.connect(self.openPicture)\n\n\n    def \nopen\nPicture(\nself\n)\n:\n\n        # pix = \nImage\n.\nopen\n('result.jpg')\n        # pix = \nImageQt\n.\ntoqpixmap(pix)\n        # img = cv2.imread('result.jpg')\n        # pix = QtGui.\nQPixmap('\nresult\n.\njpg\n')\n\n        # self.label.set\nScaledContents(True)\n\n        img = np.random.randint(\n0\n, \n255\n, \n[\n10\n, \n60\n, \n3\n]\n, np.uint8)\n        img = img.astype(\n\"uint8\"\n)\n        img = cv2.cvt\nColor(\nimg\n, \ncv2\n.COLOR_BGR2RGB)\n\n        img = QtGui.\nQImage(\nimg\n, \nimg\n.\nshape\n[1], \nimg\n.\nshape\n[0], \nimg\n.\nshape\n[1] \n*\n 3, QtGui.QImage.Format_RGB888)\n\n\n\n        self.label.set\nPixmap(\nimg\n)\n\n\n\n\nif\n __name__\n == \n'__main__':\n    \nQtCore\n.\nQCoreApplication\n.\nset\nAttribute(QtCore.Qt.AA_EnableHighDpiScaling)\n\n    app = QtWidgets.\nQApplication(\nsys\n.\nargv\n)\n\n    my = \nMyMainForm()\n\n    my.show\n()\n\n    sys.exit(app.exec\n_()\n)\n", "Tag": "算法分析"}
{"Answer": "试了下 可以啊点击到每个蓝色点才会展示新的figure\n\n如有帮助，请采纳哦~", "Konwledge_Point": "应对NP完全问题", "Question": "matplotlib通过event事件使图表可交互失败\n源代码\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n# Fixing random state for reproducibility\n\nnp.random.seed(19680801)\n\nX = np.random.rand(100, 1000)\nxs = np.mean(X, \naxis\n=1)\nys = np.std(X, \naxis\n=1)\n\nfig, ax = plt.subplots()\nax.set_title(\n'click on point to plot time series'\n)\nline, = ax.plot(xs, ys, \n'o'\n, \npicker\n=\nTrue\n, \npickradius\n=5)\n\n\ndef onpick(event):\n\n    \nif\n event.artist != line:\n        return \nTrue\n\n\n    N = len(event.ind)\n    \nif\n \nnot\n N:\n        return \nTrue\n\n\n    figi, axs = plt.subplots(N, \nsqueeze\n=\nFalse\n)\n    \nfor\n ax, dataind \nin\n zip(axs.flat, event.ind):\n        ax.plot(X[dataind])\n        ax.text(.05, .9, \n'mu=%1.3f\\nsigma=%1.3f'\n % (xs[dataind], ys[dataind]),\n                \ntransform\n=ax.transAxes, \nva\n=\n'top'\n)\n        ax.set_ylim(-0.5, 1.5)\n    figi.show()\n    return \nTrue\n\n\nfig.canvas.mpl_connect(\n'pick_event'\n, onpick)\n\nplt.show()\n\n\n\n\n生成的图表\n\n\n鼠标点击并没有产生交互\n请问问题出在了哪里", "Tag": "算法分析"}
{"Answer": "我大概猜到了，因为精度很小的时候，在靠近0的地方类似于无穷小，然后1/无穷小就得到了无穷大。\n然后接着y轴为了显示出无穷大的数据，就被无限地拉伸，然后图像看起来就是错误的了。\ny轴的刻度值验证了我的这一猜想。\n\n应该可以用设置xy等比例或者设置值域的方法来解决。", "Konwledge_Point": "应对NP完全问题", "Question": "Matplotlib为什么无法绘制高精度的反比例函数？\n我用Python的matplotlib库绘制了反比例函数，当x步长很小时，得出的图像是错误的，请问这是为什么？\n\n\n\n代码：\n\n\n\n步长为0.001（错误图像）\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(-10, 10, 0.001)\ny = 1/x\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n步长为0.5（正确图像）：\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(-10, 10, 0.5)\ny = 1/x\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n \n\n", "Tag": "算法分析"}
{"Answer": "```\r\neval('bay_' + str(x_1[1]))[np.where(eval('bay_' + str(x_1[1]))[:, 0]==x_1[0])]=x_1\r\n```", "Konwledge_Point": "应对NP完全问题", "Question": "python（numpy）中，如何根据条件来修改某个矩阵中的值？\n假设有一变量 x_1，x_1的值为[1:10]中的某一个。还有bay_1,bay_2,...bay_10.共10个矩阵。\n\n\n\n我想让x_1=1时，对bay_1矩阵进行操作（更新数据），x_1=2时，对bay_2矩阵进行操作，其余同理。\n\n\n\n下面代码是其中的一部分，以此举例。（python3.7）\n\n\n\n    import numpy as np\n    bay_7 = [[121, 7, 1, 1, 1], [122, 7, 2, 1, 1], [123, 7, 3, 1, 1], [124, 7, 4, 1, 1], [125, 7, 5, 1, 1], [126, 7, 1, 2, 0], [127, 7, 2, 2, 1],\n         [128, 7, 3, 2, 1], [129, 7, 4, 2, 1], [130, 7, 5, 2, 1], [131, 7, 1, 3, 0], [132, 7, 2, 3, 1], [133, 7, 3, 3, 1], [134, 7, 4, 3, 1],\n         [135, 7, 5, 3, 1], [136, 7, 1, 4, 0], [137, 7, 2, 4, 0], [138, 7, 3, 4, 1], [139, 7, 4, 4, 0], [140, 7, 5, 4, 0]]\n    bay_7 = np.array(bay_7)\n    x_1 = [133, 7, 3, 3, 0]\n\n\n\n\n如果我用下面这行代码来对bay_7进行修改，倒是可行，\n\n\n\n    bay_7[np.where(bay_7[:, 0]==x_1[0])]=x_1\n\n\n\n\n其他的矩阵也可以用if，elif语句逐个进行判断，但是我想用eval这类的函数对它进行操作，就是为了简化代码。然后我自己写的eval代码如下：\n\n\n\n    eval('bay_'+str(x_1[1])+'[np.where(bay_'+str(x_1[1])+'[:,0]==x_1[0])] = x_1')\n\n\n\n\n然后他就提示我如下错误：\n\n\n\neval('bay_'+str(x_1[1])+'[np.where(bay_'+str(x_1[1])+'[:,0]==x_1[0])] = x_1')\n\n  File \"\", line 1\n\n    bay_7[np.where(bay_7[:,0]==x_1[0])] = x_1\n\n                                        ^\n\nSyntaxError: invalid syntax\n\n请问，这种问题要怎么修改才可行？", "Tag": "算法分析"}
{"Answer": "第4行temp_col = a[:,i]并没有新申请内存，temp_col和a[:,i]指向同一存储地址，因此对temp_col的修改，其实就是对a的修改；", "Konwledge_Point": "应对NP完全问题", "Question": "python值的传到\nimport numpy as np\ndef fill(\na\n):\n    for i in \nrange\n(a.shape[\n1\n]):\n        temp_col = a[:,i]\n        nan_num = np.\ncount_nonzero\n(np.\nisnan\n(temp_col))\n        if nan_num != \n0\n:\n            temp_not_nan_col = temp_col[temp_col == temp_col]\n            temp_col[np.\nisnan\n(temp_col)] = temp_not_nan_col.\nmean\n()\n            a[:,i] = temp_col\n    return a\n\nif __name__ == \n\"__main__\"\n:\n    a = np.\narange\n(\n24\n).\nreshape\n(\n4\n, \n6\n).\nastype\n(float)\n    a[\n1\n, \n2\n:] = np.nan\n    \nprint\n(a)\n    a = \nfill\n(a)\n    \nprint\n(a)\n\n\n\n上面是修改nan位置的值，请问第9行代码去了为什么a的值也改变了，没有第9行（a[:,i] = temp_col），都没对a做修改，但最后a的值也变了，很疑惑", "Tag": "算法分析"}
{"Answer": "首先，手写识别的关键是特征描述，如果这一步没有做好，用什么方法，怎么调参，也不会有好的结果。将图像像素值直接作为输入向量，原则上是不适当的。推荐实现方法如下：（1）首先，样本均匀，标准化，归一化，这些必要的准备工作就不说了，（2）特征提取，或者说特征向量构造，将字符图像转换为特征向量作为模型的输入，（3）KNN，可以选择不同的K值，2～5之间有些影响，5 以上没必要。关于特征构造，推荐两种方法：1，HOG，方向梯度直方图2，小波特征，例如Haar我查了一下以前的程序，检验集识别准确率大约 80～90%。给出一段 HOG 特征描述符的构造例程，这类似于SIFT的特征描述符，效果不错。\nimport cv2 as cv\n\n    # (2) 构造 HOG 描述符\n    # HOGDescriptor\n    winSize = (20, 20)\n    blockSize = (10, 10)\n    blockStride = (5, 5)\n    cellSize = (5, 5)\n    nbins = 8\n    derivAperture = 1\n    winSigma = -1.\n    histogramNormType = 0\n    L2HysThreshold = 0.2\n    gammaCorrection = 1\n    nlevels = 16\n    signedGradients = True\n    hog = cv.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins,\n                           derivAperture, winSigma, histogramNormType,\n                           L2HysThreshold, gammaCorrection, nlevels)\n    p = (1+(20-10)//5)*(1+(20-10)//5)*(10//5)*(10//5)*8  # 特征描述符长度，288\n\n参考结果：\nRecognition of handwritten digits by KNN-HOG\nk=2, correct=938, accuracy=93.80%\nk=3, correct=939, accuracy=93.90%\nk=4, correct=940, accuracy=94.00%\nk=5, correct=938, accuracy=93.80%\n", "Konwledge_Point": "应对NP完全问题", "Question": "提升knn算法的准确率\n不使用 sklearn写的knn算法，识别mnist数据集，准确率只有百分之六十， 如何进一步提高识别的准确率\n已经尝试过使用不同的k值和对图片进行归一化处理\n\n\n\n\ndef load_mnist():\n    \nX_train\n = np.fromfile('mnist_data/train-images-idx3-ubyte', \ndtype=np.uint8,\n \noffset=16)\n\n    \nX_train\n = X_train.reshape(int(\n6\ne4), \n28\n, \n28\n)\n    \nX_test\n = np.fromfile('mnist_data/t10k-images-idx3-ubyte', \ndtype=np.uint8,\n \noffset=16)\n\n    \nX_test\n = X_test.reshape(int(\n1\ne4), \n28\n, \n28\n)\n    \ny_train\n = np.fromfile('mnist_data/train-labels-idx1-ubyte', \ndtype=np.uint8,\n \noffset=8)\n\n    \ny_train\n = y_train.reshape(int(\n6\ne4))\n    \ny_test\n = np.fromfile('mnist_data/t10k-labels-idx1-ubyte', \ndtype=np.uint8,\n \noffset=8)\n\n    \ny_test\n = y_test.reshape(int(\n1\ne4))\n\n\nclass Knn(object):\n\n    def __init__(self, \nk=3):\n\n        self.\nk\n = k\n\n    def fit(self, X, y):\n        self.\nX\n = X\n        self.\ny\n = y\n\n    def predict(self, X):\n        \ndataset\n = self.X\n        \nlabels\n = self.y\n        \nk\n = self.k\n        \npredict_labels\n = []\n        \nX\n = np.reshape(X, (X.shape[\n0\n], -\n1\n))\n        \ndataset\n = np.reshape(dataset, (dataset.shape[\n0\n], -\n1\n))\n\n        \nscalar\n = MaxAbsScaler()\n        scalar.fit(dataset)\n        \ndataset\n = scalar.transform(dataset)\n        \nX\n = scalar.transform(X)\n\n        print(dataset[\n0\n])\n\n        \ndataset_size\n = dataset.shape[\n0\n]\n        for i \nin\n tqdm(range(X.shape[\n0\n])):\n            \ndiff_mat\n = np.tile(X[i], (dataset_size, \n1\n)) - dataset\n            \nsq_diff_mat\n = diff_mat ** \n2\n\n            \nsq_distances\n = sq_diff_mat.sum(\naxis=1)\n\n            \ndistances\n = sq_distances ** \n0.5\n\n            \nsorted_dist_indicies\n = distances.argsort()\n            \nclass_count\n = {}\n            for j \nin\n range(k):\n                \nvote_label\n = labels[sorted_dist_indicies[i]]\n                class_count[vote_label] = class_count.get(vote_label, \n0\n) + \n1\n\n            \nsorted_class_count\n = sorted(class_count.items(), \nkey=operator.itemgetter(1),\n \nreverse=True)\n\n            predict_labels.append(sorted_class_count[\n0\n][\n0\n])\n        \npredict_labels\n = np.array(predict_labels)\n        return predict_labels\n\n\n", "Tag": "算法分析"}
{"Answer": "改成np.power()试试：\nimport numpy as np\na=np.array([1,2,3,4])\nb=np.array([2])\nprint(np.power(a,b))\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python进行层次聚类时数据读取出现问题\n问题遇到的现象和发生背景\n\n\n从excel中导入数据进行层次聚类分析，导入数据类型不对，无法进行计算\n\n\n问题相关代码，请勿粘贴截图\n\n\n#描述: 基于组平均的AGNES算法，支持多维数组，距离用欧式距离\n\n\nimport\n numpy \nas\n np\n\nimport\n pandas \nas\n pd\n\nfrom\n scipy.cluster.hierarchy \nimport\n dendrogram,linkage\n\nfrom\n scipy.spatial.distance \nimport\n squareform\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n math\n\nimport\n pylab \nas\n pl\n\n#从excel中读取数据并转换为矩阵\n\ndatA=pd.read_excel(\nr'C:\\Users\\49175\\Desktop\\jzgb.xlsx'\n)\ndata=np.array(datA,dtype=np.int64)\n\n#数据处理 dataset是样本的列表\n\na = np.array_split(datA,\n78\n,axis=\n0\n)\ndataset = [(a[i], a[i+\n1\n]) \nfor\n i \nin\n \nrange\n(\n1\n, \nlen\n(a)-\n1\n, \n3\n)]\n\n#计算欧几里得距离,a,b分别为两个元组\n\n\ndef\n \ndist\n(\na, b\n):\n    t = \n0\n\n    n=\n44\n\n    \nfor\n i \nin\n \nrange\n(n):\n        t = t + math.\npow\n(a[i]-b[i], \n2\n)\n    \nreturn\n math.sqrt(t)\n\n#dist_min\n\n\ndef\n \ndist_min\n(\nCi, Cj\n):\n    \nreturn\n \nmin\n(dist(i, j) \nfor\n i \nin\n Ci \nfor\n j \nin\n Cj)\n\n#dist_max\n\n\ndef\n \ndist_max\n(\nCi, Cj\n):\n    \nreturn\n \nmax\n(dist(i, j) \nfor\n i \nin\n Ci \nfor\n j \nin\n Cj)\n\n#dist_avg\n\n\ndef\n \ndist_avg\n(\nCi, Cj\n):\n    \nreturn\n \nsum\n(dist(i, j) \nfor\n i \nin\n Ci \nfor\n j \nin\n Cj)/(\nlen\n(Ci)*\nlen\n(Cj))\n\n#找到距离最小的下标\n\n\ndef\n \nfind_Min\n(\nM\n):\n    \nmin\n = \n1000\n\n    x = \n0\n; y = \n0\n\n    \nfor\n i \nin\n \nrange\n(\nlen\n(M)):\n        \nfor\n j \nin\n \nrange\n(\nlen\n(M[i])):\n            \nif\n i != j \nand\n M[i][j] < \nmin\n:\n                \nmin\n = M[i][j];x = i; y = j\n    \nreturn\n (x, y, \nmin\n)\n\n#算法模型：\n\n\ndef\n \nAGNES\n(\ndataset, dist, k\n):\n    \n#初始化C和M\n\n    C = [];M = []\n    \nfor\n i \nin\n dataset:\n        Ci = []\n        Ci.append(i)\n        C.append(Ci)\n    \nfor\n i \nin\n C:\n        Mi = []\n        \nfor\n j \nin\n C:\n            Mi.append(dist(i, j))\n        M.append(Mi)\n    q = \nlen\n(dataset)\n    \n#合并更新\n\n    \nwhile\n q > k:\n        x, y, \nmin\n = find_Min(M)\n        C[x].extend(C[y])\n        C.remove(C[y])\n        M = []\n        \nfor\n i \nin\n C:\n            Mi = []\n            \nfor\n j \nin\n C:\n                Mi.append(dist(i, j))\n            M.append(Mi)\n        q -= \n1\n\n    \nreturn\n C\n\n#画图\n\n\ndef\n \ndraw\n(\nC\n):\n    colValue = [\n'r'\n, \n'y'\n, \n'g'\n, \n'b'\n, \n'c'\n, \n'k'\n, \n'm'\n]\n    \nfor\n i \nin\n \nrange\n(\nlen\n(C)):\n        coo_X = []    \n#x坐标列表\n\n        coo_Y = []    \n#y坐标列表\n\n        \nfor\n j \nin\n \nrange\n(\nlen\n(C[i])):\n            coo_X.append(C[i][j][\n0\n])\n            coo_Y.append(C[i][j][\n1\n])\n        pl.scatter(coo_X, coo_Y, marker=\n'x'\n, color=colValue[i%\nlen\n(colValue)], label=i)\n\n    pl.legend(loc=\n'upper right'\n)\n    pl.show()\nC = AGNES(dataset, dist_avg, \n3\n)\ndraw(C)\n\n\n\n\n运行结果及报错内容\n\n\nTraceback (most recent call last):\n  \nFile\n \n\"C:/各种东西练习/1/4.py\"\n, \nline\n 92, \nin\n \n    C = AGNES(dataset, dist_avg, 3)\n  \nFile\n \n\"C:/各种东西练习/1/4.py\"\n, \nline\n 62, \nin\n AGNES\n    Mi.\nappend\n(dist(i, j))\n  \nFile\n \n\"C:/各种东西练习/1/4.py\"\n, \nline\n 39, \nin\n dist_avg\n    \nreturn\n \nsum\n(dist(i, j) \nfor\n i \nin\n \nCi\n \nfor\n j \nin\n Cj)/(len(\nCi\n)*len(Cj))\n  \nFile\n \n\"C:/各种东西练习/1/4.py\"\n, \nline\n 39, \nin\n \n    \nreturn\n \nsum\n(dist(i, j) \nfor\n i \nin\n \nCi\n \nfor\n j \nin\n Cj)/(len(\nCi\n)*len(Cj))\n  \nFile\n \n\"C:/各种东西练习/1/4.py\"\n, \nline\n 28, \nin\n dist\n    t = t + math.pow(a[i]-b[i], 2)\nTypeError: must be real number, not DataFrame\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果\n\n\n能帮我看一看代码中是否有其他问题\n并实现层次聚类Agnes算法\n能够从excel表格中读取数据，并以每一行为单位，进行层次聚类\n数据样式如图\n\n", "Tag": "算法分析"}
{"Answer": "布尔数组colindex用于索引时，等效于元素为True的索引下标所组成的数组，即np.array([0, 1, 4])", "Konwledge_Point": "应对NP完全问题", "Question": "numpy花式索引，不写newaxis的结果，搞不明白怎么计算的\ntwodim=np.arange(20).reshape(4,5)\nrowindex=np.array([0,2,1])\ncolindex=np.array([1,1,0,0,1],dtype=bool)\ntwoarr=twodim[rowindex,colindex]\nprint(twoarr)\n\n\n结果为：\n[ 0 11  9]\n\n\n搞不明白为什么是这样的结果。", "Tag": "算法分析"}
{"Answer": "你 from skimage.io import imread 是直接引入了imread函数\n要\nimg = imread(file)\n这样调用", "Konwledge_Point": "应对NP完全问题", "Question": "在使用函数时显示NameError: name 'skimage' is not defined\n\n\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import svm, metrics, datasets\nfrom sklearn.utils import Bunch\nfrom sklearn.model_selection import  train_test_split\nfrom skimage.io import imread# abc\nfrom skimage.transform import resize\n\n\ndef load_image_files(container_path, dimension=(64, 64)):  \n    image_dir = Path(container_path)\n    folders = [directory for directory in image_dir.iterdir() if directory.is_dir()]\n    categories = [fo.name for fo in folders]\n\n    descr = \"A image classification dataset\"\n    images = []\n    flat_data = []\n    target = []\n    for i, direc in enumerate(folders):\n        for file in direc.iterdir():\n            img = skimage.io.imread(file)\n            img_resized = resize(img, dimension, anti_aliasing=True, mode='reflect')\n            flat_data.append(img_resized.flatten()) \n            images.append(img_resized)\n            target.append(i)\n    flat_data = np.array(flat_data)\n    target = np.array(target)\n    images = np.array(images)\n\n    return Bunch(data=flat_data,\n                 target=target,\n                 target_names=categories,\n                 images=images,\n                 DESCR=descr)\n\nimage_dataset = load_image_files(\"G:\\images\")\n", "Tag": "算法分析"}
{"Answer": "import numpy as np\n\na = np.arange(64).reshape(8, 8)\nprint(a)\n\nb = [1,3,6]\na_del = np.delete(a, b, 0) # 删除a的第1，3，6\nprint(a_del)\n\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python3矩阵删除的问题\n有一个矩阵a，和一个列表b，列表b里面记录了矩阵a的几个行数，怎么把矩阵a的列表b的所有行数删除\n\n\n#实例\n\n\na\n=np.zeros((\n25\n,\n10\n))#一个\n25\n行\n10\n列的矩阵\n\nb\n=[\n1\n,\n3\n,\n6\n,\n8\n,\n10\n,\n14\n,\n19\n,\n23\n]#行数\n\n#删除a的第1，3，6，8，10，14，19，23行后结果是多少怎么求\n\n", "Tag": "算法分析"}
{"Answer": "这个原因是对于圆形的物体使用正方形的核去做膨胀和腐蚀就会变的像四边形，只要修改kernel变成圆形就好了；\r\n\r\n```\r\nimport cv2 as cv\r\n    import numpy as np\r\n    import math\r\n\r\n    pie = cv.imread('0000.png')\r\n    kernel = np.ones((7, 7), np.uint8)\r\n    kernel_re = []\r\n    rows, cols = kernel.shape\r\n    for i in range(rows):\r\n        result = [0 if math.sqrt((i-3)**2+(j-3)**2) > 3 else 1 for j in range(cols)]\r\n        kernel_re.append(result)\r\n    kernel_re = np.array(kernel_re, np.uint8)\r\n    print(kernel_re, kernel)\r\n    print(type(kernel_re), type(kernel))\r\n    print(kernel_re.shape, kernel.shape)\r\n    dilate = cv.dilate(pie, kernel, iterations=10)\r\n    erosion = cv.erode(pie, kernel_re, iterations=10)\r\n    res = np.hstack((pie, dilate, erosion))\r\n    cv.imshow('res', res)\r\n    cv.waitKey(0)\r\n    cv.destroyAllWindows()\r\n```\r\n![图片说明](https://img-ask.csdn.net/upload/202009/21/1600667194_310167.png)", "Konwledge_Point": "应对NP完全问题", "Question": "关于opencv（）中的腐蚀操作，我操作过后会让原来的圆变形，怎么让圆腐蚀后不变形依旧保持圆形？\n代码：\n\nimport cv2 as cv\n\nimport numpy as np\n\npie=cv.imread('d:/pic/pie.png')\n\nkernel=np.ones((7,7),np.uint8)\n\ndilate=cv.dilate(pie,kernel,iterations=10)\n\nerosion=cv.erode(pie,kernel,iterations=10)\n\nres=np.hstack((pie,dilate,erosion))\n\ncv.imshow('res',res)\n\ncv.waitKey(0)\n\ncv.destroyAllWindows()\n\n\n\n结果：", "Tag": "算法分析"}
{"Answer": "\nimport numpy as np\nimport pandas as pd\ndf = pd.read_excel(r'C:\\Users\\jackey\\Desktop\\CSDN/test.xlsx')\ndf = df.pivot_table(index='产品',columns='月份',values=['销量','金额'],aggfunc=np.sum)\ndf=df.swaplevel(0, 1, axis=1)\ndf=df.sort_values(by='月份',axis=1)\nprint(df)\n\n月份    1月        2月        3月        4月        5月      金额  销量    金额  销量    金额  销量    金额  销量    金额  销量产品A    100   1   200   2   300   3   400   4   500   5B   1300  13  1400  14  1500  15  1600  16  1700  17", "Konwledge_Point": "应对NP完全问题", "Question": "pandas pivot table的columns和values上下顺序\n问题遇到的现象和发生背景\n\n\n\n\n\n\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport numpy as np\nimport pandas as pd\ndf = pd.read_excel('111.xlsx', sheet_name='test')\ndf = df.pivot_table(index='产品',columns='月份',values=['销量','金额'],aggfunc=np.sum)\nprint(df)\n\n\n运行结果及报错内容\n\n\n希望把pivot table 的效果做成月份在上,不知道怎么填写参数\n\n\n我的解答思路和尝试过的方法\n\n\n反复测试了除了几种方法,都不成功\n\n\n我想要达到的结果\n\n\n希望把pivot table 的效果做成月份在上,不知道怎么填写参数", "Tag": "算法分析"}
{"Answer": "运行挺正常的呀\n你看看控制台有没有其他的报错信息", "Konwledge_Point": "应对NP完全问题", "Question": "tkinter继承问题\n在写第二个analyse类是总是写到了它的父类上就是主界面上，直接把home类中的菜单覆盖了，可能是继承出现了问题，我把all_datas类的内容放到analyse下却是正常的，\n另外，time.sleep(4)  win_about.destroy()到时无法自动退出。\n\n\nclass First_Page(object):\n    def __init__(self, master):\n        self.root = master\n        self.root.geometry(\n\"%dx%d\"\n % (700, 600))\n        self.root.title(\n'复盘'\n)  # 背景颜色\n        self.createPage()\n    def createPage(self, \ncur_dirt\n=None):\n        self.page = tkinter.Frame(self.root, \nwidth\n=600, \nheight\n=480, )\n        img1 = Image.open(\n\"股票图片.png\"\n)\n        img1 = img1.resize((700, 600)) \n        photo1 = ImageTk.PhotoImage(img1)  \n        label1 = tk.Label(self.page, \nimage\n=photo1, \nborderwidth\n=0)\n        label1.img = photo1 \n        label1.grid(\nrow\n=0, \ncolumn\n=1, \nsticky\n=\n\"n\"\n)  \n        tkinter.Label(self.page, \ntext\n=\n'股票数据分析'\n, font=(\n'宋体'\n, 28)).place(\nx\n=240, \ny\n=60)\n        tkinter.Label(self.page, \ntext\n=\n'请先爬取数据，等待5秒后再进入软件'\n, font=(\n'宋体'\n, 15)).place(\nx\n=200, \ny\n=200)\n        tkinter.Button(self.page, \ntext\n=\n'爬取今日市场数据'\n, \ncommand\n=scrapy,width=15, \nheight\n=3,).place(x=200, \ny\n=300)\n        tkinter.Button(self.page, \ntext\n=\n'进入软件'\n, \nwidth\n=15, \nheight\n=3, \ncommand\n=self.success_tip).place(x=400, \ny\n=300)\n        self.page.pack()\n    def success_tip(self):\n        \nif\n os.path.isfile(\n\"今日股票趋势.csv\"\n):\n            self.page.destroy()\n            Home(self.root)\n        \nelse\n:\n            win_about = tk.Tk()\n            win_about.geometry(\n\"400x200\"\n)\n            win_about.title(\n\"提示\"\n)\n            tk.Label(win_about, \ntext\n=\n'请先爬取数据，等待5秒后再进入软件'\n, font=(\n'宋体'\n, 15)).place()\n            win_about.mainloop()\n            time.sleep(4)\n            win_about.destroy()\nclass Home(object):\n    def __init__(self, master: tkinter.Tk):\n        self.root = master\n        self.root.geometry(\n\"%dx%d\"\n % (1000,800))\n        self.root.title(\n'主界面'\n)\n        self.creat_page()\n    def creat_page(self):\n        # 市场数据页面\n        self.all_datas_frame = all_datas(self.root)\n        #分析页面\n        self.analyse_frame = analyse(self.root)\n        # 关于页面\n        self.about_frame = tkinter.Frame(self.root)\n        mesg = \n\"xxx\"\n\n        tkinter.Label(self.about_frame, \ntext\n=mesg, font=(\n'宋体'\n, 10)).pack()\n        menubar = tkinter.Menu(self.root)\n        menubar.add_command(\nlabel\n=\n'市场数据'\n, \ncommand\n=self.show_all_datas)\n        menubar.add_command(\nlabel\n=\n'个股数据'\n,command=self.show_analyse)\n        menubar.add_command(\nlabel\n=\n'修改'\n)\n        menubar.add_command(\nlabel\n=\n'关于'\n, \ncommand\n=self.show_about)\n        self.root[\n'menu'\n] = menubar\n    def show_about(self):\n        self.about_frame.pack()\n        self.all_datas_frame.pack_forget()\n        self.analyse_frame.pack_forget()\n    def show_analyse(self):\n        self.analyse_frame.pack()\n        self.all_datas_frame.pack_forget()\n        self.about_frame.pack_forget()\n    def show_all_datas(self):\n        self.all_datas_frame.pack()\n        self.about_frame.pack_forget()\n        self.analyse_frame.pack_forget()\nclass all_datas(tkinter.Frame):\n    def __init__(self, root):\n        super().__init__(root)\n        self.table_vawe = tkinter.Frame()\nclass analyse(tkinter.Frame):\n    def __init__(self,root):\n        super().__init__(root)\n        self.plot = tkinter.Frame()\n        self.name = tkinter.Frame()\n        self.name.pack()\n        self.plot.pack(\nside\n=tkinter.TOP, \nfill\n=tkinter.BOTH, \nexpand\n=1)\n        self.code = tkinter.StringVar()\n        self.create_matplotlib()\n        self.createWidget(self.figure)\n        self.master.mainloop()\n    def createWidget(self, figure):\n        tkinter.Label(self.name, \ntext\n=\n'请输入股票代码（请带后缀）'\n, \nwidth\n=30).grid(row=0, \ncolumn\n=0)\n        tkinter.Entry(self.name, \ntextvariable\n=self.code, \nwidth\n=30, \nbd\n=5).grid(row=0, \ncolumn\n=1)\n        self.canvas = FigureCanvasTkAgg(figure, self.plot)\n        self.canvas.draw()\n        self.canvas.get_tk_widget().pack(\nside\n=tkinter.TOP, \nfill\n=tkinter.BOTH, \nexpand\n=1)\n        toolbar = NavigationToolbar2Tk(self.canvas, self.plot)\n        toolbar.update()\n        self.canvas._tkcanvas.pack(\nside\n=tkinter.TOP, \nfill\n=tkinter.BOTH, \nexpand\n=1)\n        self.button = tkinter.Entry(self, )\n        self.button.pack(\nside\n=tkinter.BOTTOM)\n    def create_matplotlib(self):\n        mpl.rcParams[\n'font.sans-serif'\n] = [\n'SimHei'\n] \n        mpl.rcParams[\n'axes.unicode_minus'\n] = \nFalse\n \n        self.figure = plt.figure(\nnum\n=2, figsize=(7, 4), \ndpi\n=80, \nfacecolor\n=\n\"gold\"\n, \nedgecolor\n=\n'green'\n, \nframeon\n=\nTrue\n)\n        fig1 = plt.subplot(1, 1, 1) \n        x = np.arange(-2 * np.pi, 2 * np.pi, 0.1)\n        y1 = np.sin(x)\n        y2 = np.cos(x)\n        line1 = fig1.plot(x, y1, \ncolor\n=\n'red'\n, \nlinewidth\n=2, \nlabel\n=\n'y=sin(x)'\n, \nlinestyle\n=\n'--'\n) \n        line2 = fig1.plot(x, y2, \ncolor\n=\n'green'\n, \nlabel\n=\n'y=cos(x)'\n)\n        plt.setp(line2, \nlinewidth\n=1, \nlinestyle\n=\n'-'\n, \nalpha\n=0.7)  \n        fig1.set_title(\n\"数学曲线图\"\n, \nloc\n=\n'center'\n, \npad\n=20, \nfontsize\n=\n'xx-large'\n, \ncolor\n=\n'red'\n)\n        fig1.legend([\n'正弦'\n, \n'余弦'\n], \nloc\n=\n'lower right'\n, \nfacecolor\n=\n'orange'\n, \nframeon\n=\nTrue\n, \nshadow\n=\nTrue\n, \nframealpha\n=0.7)\n        # ,\nfontsize\n=\n'xx-large'\n\n        fig1.set_xlabel(\n'(x)横坐标'\n)  \n        fig1.set_ylabel(\n\"(y)纵坐标\"\n)\n        fig1.set_yticks([-1, -1 / 2, 0, 1 / 2, 1]) \n        fig1.grid(\nwhich\n=\n'major'\n, \naxis\n=\n'x'\n, \ncolor\n=\n'gray'\n, \nlinestyle\n=\n'-'\n, \nlinewidth\n=0.5, \nalpha\n=0.2)  \n\nif\n __name__ == \n'__main__'\n:\n    root = tkinter.Tk()\n    First_Page(root)\n    root.mainloop()\n\n", "Tag": "算法分析"}
{"Answer": "数值溢出啊，你的结果值太大了就会这样，但是按理来说你的x和y才等于2页不会啊，除非你的矩阵里面本身就有很大的数值", "Konwledge_Point": "应对NP完全问题", "Question": "使用numpy时遇到运行时错误\n两个运行时警告了，而且在我完整的代码里没有相关计算结果\noverflow encountered in multiply\ninvalid value encountered in add\n指向下面这条语句，一开始定义的x=2，y=2，然后其它3个变量都是numpy.ndarray类型\npredicRate= np.dot(user_vector, x * movie_lfm_vector + y * movie_bert_vector)\n然后改成下面语句就没有这两个警告了\nerr = np.float32(realRate - np.dot(user_vector, x * movie_lfm_vector + y * movie_bert_vector))\n不知道是不是因为加了np.float32的原因", "Tag": "算法分析"}
{"Answer": "send_post方法没有return ，导致打印的结果为None", "Konwledge_Point": "应对NP完全问题", "Question": "Flask接口处理post请求时出现问题\nFlask接口post请求之后另一边只能收到None\n\n\n代码能够正确运行，并且使用在接口代码中加入print能够正确输出内容，但是使用测试接口的代码时只能收到None\n\n\n而且在最后会出现一个\"POST / HTTP/1.1\" 500 -的信息\n\n\n\n\n这是接口的代码：\n\nimport\n json\n\nimport\n cv2\n\nfrom\n PIL \nimport\n Image\n\nimport\n numpy \nas\n np\n\nfrom\n modelscope.pipelines \nimport\n pipeline\n\nfrom\n modelscope.utils.constant \nimport\n Tasks\n\nfrom\n modelscope.outputs \nimport\n OutputKeys\n\nfrom\n flask \nimport\n request\n\nfrom\n flask \nimport\n jsonify\n\nfrom\n flask \nimport\n Flask,make_response\n\nimport\n base64\n\nface_recognition = pipeline(Tasks.face_recognition, model=\n'damo/cv_ir101_facerecognition_cfglint'\n)\n\n\n# def face_recognition(image1,image2):\n\n\n#     emb1 = face_recognition(image1)[OutputKeys.IMG_EMBEDDING]\n\n\n#     emb2 = face_recognition(image2)[OutputKeys.IMG_EMBEDDING]\n\n\n#     sim = np.dot(emb1[0], emb2[0])\n\n\n#     sim=face_recognition(image1, image2)\n\n\n#     return jsonify(sim)\n\n\n#     return (f'Face cosine similarity={sim:.3f}, get_img1:{image1}  get_img2:{image2}')\n\n\n\napp = Flask(__name__)\napp.config[\n\"JSON_AS_ASCII\"\n] = \nFalse\n\n\ndef\n \nmaking_response\n(\njson_data\n):\n    response = make_response(jsonify(json_data))\n    response.headers[\n'Content-Type'\n] = \n'application/json;charset=UTF-8'\n\n    \nreturn\n response\n\n\n@app.route(\n\"/\"\n,methods=[\n\"POST\"\n]\n)\n\n\ndef\n \nfirst_post\n():\n    my_json = {\n\"msg\"\n: \nNone\n,\n               \n\"sim\"\n: \nNone\n\n                }\n    data = request.get_data()\n    \nif\n \nnot\n data:\n        my_json[\n\"msg\"\n] = \n\"No data obtained!\"\n\n    \ntry\n:\n        data=json.loads(data)\n        image1 = base64.b64decode(data[\n\"image1\"\n].encode())\n        image2 = base64.b64decode(data[\n\"image2\"\n].encode())\n        \n# image1_base64 = data[\"image1\"].encode()\n\n        \n# image2_base64 = data[\"image2\"].encode()\n\n        image1 = cv2.imdecode(np.frombuffer(image1, np.uint8), cv2.IMREAD_ANYCOLOR) \n#cv2.IMREAD_UNCHANGED\n\n        image2 = cv2.imdecode(np.frombuffer(image2, np.uint8), cv2.IMREAD_ANYCOLOR)\n\n        emb1 = face_recognition(image1)[OutputKeys.IMG_EMBEDDING]\n        emb2 = face_recognition(image2)[OutputKeys.IMG_EMBEDDING]\n        sim = np.dot(emb1[\n0\n], emb2[\n0\n])\n        \nprint\n(sim)\n        my_json[\n\"msg\"\n] = \n\"successful!\"\n\n        my_json[\n\"sim\"\n] = sim\n\n        \n# face_recognition(image1,image2)\n\n    \nexcept\n Exception \nas\n e:\n        \nprint\n(e)\n        my_json[\n\"msg\"\n] = \n\"出错了，请检查是否正确访问!\"\n\n        response = making_response(my_json)\n        \nreturn\n response\n\n    response = make_response(jsonify(my_json))\n    response.headers[\n'Content-Type'\n] = \n'application/json;charset=UTF-8'\n\n    \nreturn\n response\n\n\nif\n __name__ == \n\"__main__\"\n:\n    app.run(host=\n'0.0.0.0'\n, port=\n12000\n, debug=\nTrue\n, use_reloader=\nFalse\n)\n\n这是我测试用的代码：\n\nimport\n os\n\nimport\n time\n\nimport\n json\n\nimport\n base64\n\nimport\n random\n\nimport\n requests\n\n\nIMG_FORMATS = [\n'bmp'\n, \n'jpg'\n, \n'jpeg'\n, \n'png'\n, \n'tif'\n, \n'tiff'\n, \n'dng'\n, \n'webp'\n, \n'mpo'\n]\n\n\n\n# 文件目录遍历，返回[fileP, fileN]\n\n\ndef\n \nget_filepaths\n(\npath\n):\n    pathlists = []\n    \nfor\n root, dirs, files \nin\n os.walk(path):\n        \nfor\n file \nin\n files:\n            pathlists.append([os.path.join(root, file), file])\n    \nreturn\n pathlists\n\n\n\nclass\n \napi_test\n:\n    \ndef\n \n__init__\n(\nself\n):\n        self.url = \n\"http://0.0.0.0:12000/\"\n  \n# nginx url\n\n\n\n    @staticmethod\n\n    \ndef\n \nread_img_base64\n(\np\n):\n        \nwith\n \nopen\n(p, \n'rb'\n) \nas\n f:\n            imgString = base64.b64encode(f.read())\n            base64_data = imgString.decode()\n        \nreturn\n base64_data\n\n    \ndef\n \nsend_post\n(\nself, img_path1,img_path2\n):\n        base64_data1 = self.read_img_base64(img_path1)\n        base64_data2 = self.read_img_base64(img_path2)\n        \n#image_name = str(img_path)\n\n        data = {\n            \n\"image1\"\n: base64_data1,\n            \n\"image2\"\n: base64_data2\n        }\n\n        session = requests.session()\n        start_time = time.time()\n        response = session.post(self.url, json.dumps(data))  \n# Json格式请求\n\n        end_time = time.time()\n        run_time = end_time - start_time\n        \n# logger.info('接口调用时间为：%s' % run_time)\n\n        \n# logger.info(response.status_code)\n\n        \n# logger.info(response.text)\n\n        session.close()\n        response.close()\n\n\n\nif\n __name__ == \n\"__main__\"\n:\n    \n# 开始单元测试\n\n    api = api_test()\n\n\n    \n# 单张图片测试\n\n    img_path1 = \n'./样本.jpg'\n\n    img_path2 = \n'./样本1.jpg'\n\n    res = api.send_post(img_path1,img_path2)\n    \nprint\n(res)\n\n\n###### \n\nPress CTRL+C to quit\n/opt/conda/lib/python3\n.7\n/site-packages/mmdet/core/anchor/anchor_generator.py:\n333\n: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` \n  warnings.warn(\n'``grid_anchors`` would be deprecated soon. '\n\n/opt/conda/lib/python3\n.7\n/site-packages/mmdet/core/anchor/anchor_generator.py:\n370\n: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` \n  \n'``single_level_grid_anchors`` would be deprecated soon. '\n\n\n127.0\n.0\n.1\n - - [\n29\n/Dec/\n2022\n \n11\n:\n20\n:\n27\n] \n\"POST / HTTP/1.1\"\n \n500\n -\n这是\nprint\n输出的结果：-\n0.11088603\n\n这是测试代码得到的结果：\nNone\n\n\n\n\n\nPress CTRL+C to quit\n\n\n/opt/conda/lib/python3.7/site-packages/mmdet/core/anchor/anchor_generator.py:333: UserWarning: \ngrid_anchors\n would be deprecated soon. Please use \ngrid_priors\n  warnings.warn('\ngrid_anchors\n would be deprecated soon. '\n/opt/conda/lib/python3.7/site-packages/mmdet/core/anchor/anchor_generator.py:370: UserWarning: \nsingle_level_grid_anchors\n would be deprecated soon. Please use \nsingle_level_grid_priors\n  '\nsingle_level_grid_anchors\n would be deprecated soon. '\n127.0.0.1 - - [29/Dec/2022 11:20:27] \"POST / HTTP/1.1\" 500 -\n这是print输出的结果：-0.11088603\n这是测试代码得到的结果：None\n\n\n一头雾水。不知道是哪出了问题\n\n\n我想要测试代码访问时 能把输出的结果也就是代码中的sim这个值返回给访问的那一端\n\n\nPS：这个模型是魔塔社区的模型 我用来练习使用的，如果各位需要测试使用的话可以直接去\nhttps://www.modelscope.cn/models/damo/cv_ir101_facerecognition_cfglint/summary\n 打开使用测试", "Tag": "算法分析"}
{"Answer": "这样？\n\nimport re\nimport os\nimport numpy as np\nfiles = []\nx=[]\ny=[]\ni=0\nj=0\ndir_file_path = 'xukka'\nfor file in os.listdir(dir_file_path):\n    if re.findall('.*?txt',file):\n        files.append(file)\nfor name in files:\n    a = np.loadtxt(dir_file_path+'/'+name)\n    x.append(a[:,0])\n    y.append(a[:,1])\n\nfor i in range(len(x)): #这里让x变成x1，x2,x3\n    if i==0:\n        x1=x[i]\n    elif i==1:\n        x2=x[i]\n    else:\n        x3=x[i]\n        \nprint(x1)\nprint(x2)\nprint(x3)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的问题：我希望可以通过循环让x变成x1=[],x2=[],x3=[]比如说第一次循环是x1\n我希望可以通过循环让x变成x1=[],x2=[],x3=[]\n比如说第一次循环是x1，第二次变成x2\n\n\n\n```python\nimport re\nimport os\nimport numpy as np\n\nfiles = \n[]\n\nx=\n[]\n\ny=\n[]\n\n\ni=\n0\n\nj=\n0\n\ndir_file_path = \n'xukka'\n\n\nfor\n file \nin\n os\n.listdir\n(dir_file_path):\n    \nif\n re\n.findall\n(\n'.*?txt'\n,file):\n        files\n.append\n(file)\n\nfor\n name \nin\n files:\n    \na\n = np\n.loadtxt\n(name)\n    x\n.append\n(\na\n[:,0]\n)\n    y\n.append\n(\na\n[:,1]\n)\n\nprint\n(x)\n\n\nfor\n \ni\n \nin\n len(x): #这里让x变成x1，x2,x3\n\n\nprint\n(x)\n\n\n\n\n\n```", "Tag": "算法分析"}
{"Answer": "对 a 进行遍历每个元素的值翻倍", "Konwledge_Point": "应对NP完全问题", "Question": "这个numpy里for循环的代码什么意思\n为什么这样可以修改数组a\n\n\n\nimport numpy as np\n\na\n = np\n.arange\n(\n0\n,\n60\n,\n5\n)\n\na\n = \na\n.reshape\n(\n3\n,\n4\n) \nprint (\n\"原数组是:\"\n,a)\n\nfor\n x \nin\n np\n.nditer\n(\na\n, op_flags=\n[\n'readwrite'\n]\n):\n    x\n[...]\n=\n2\n*x\nprint (\n'修改后的数组是：'\n,a)\n", "Tag": "算法分析"}
{"Answer": "你这个结果直接滤波下不就是了吗。。。来上几次闭操作，或者用findcontours把面积区域小的直接去掉就是了。如果能保证绿色区域最大，那直接找最大的区域就是了。\n还有一个方法就是将低的那个绿色阈值改大点，像你这种的绿色这么亮，阈值设定高点试试。\n其实你这个图片不建议使用hsv，直接bgr通道就能做，提取g通道，直接二值化，阈值设定高点，比如200左右，直接就出来了", "Konwledge_Point": "应对NP完全问题", "Question": "OpenCV提取HSV颜色范围问题\n\n\nsrc = cv.imread(\"D://1.jpg\")\nhsv = cv.cvtColor(src, cv.COLOR_BGR2HSV)\nlower_hsv = np.array([35, 43, 46])\nupper_hsv = np.array([77, 255, 255])\nmask = cv.inRange(hsv, lowerb=lower_hsv, upperb=upper_hsv)\ncv.imshow(\"video\", src)\ncv.imshow(\"mask\", mask)\n\n\n\n代码如上，提取的是绿色的，那个HSV范围是网上的表格里的。想提取绿色的球，可结果也太夸张了。\n\n\n\n\n\n\n结果\n\n\n\n\n\n\n即使没有进行滤波，这结果好像也有点夸张，我看其它地方的颜色也不怎么绿啊。进行滤波后一样有很多干扰，提取不出绿色的球。能帮我看看怎么提取出绿色的球吗？", "Tag": "算法分析"}
{"Answer": "以下答案由GPT-3.5大模型与博主波罗歌共同编写：根据您提供的代码，可以发现训练出来的 LSTM 模型在预测时的表现并不理想。在分析时需要注意以下几点：\n数据归一化\n由于 LSTM 模型的特点，输入数据需要进行归一化。在您的代码中已经使用了 sklearn 的 MinMaxScaler 进行数据归一化，这是正确的。\n数据集生成方式\n在您的代码中，数据集是通过将连续的若干行数据合并为一个训练/测试数据，这种方式虽然能够将序列信息传给模型，但是容易造成信息丢失。而在实际物流预测的应用场景中，时间序列的填充方式可能不太合适。因此，建议您使用一些其他的方法，例如滑动窗口法等，来生成数据集。\n模型参数的选择\n您的代码中选择了 2 层 LSTM，hidden state 的维度，hidden_dim 是 32，学习率 lr 是 0.01。这些参数的选择可能不太合适。建议您通过一定的试验，调整模型参数，比如可以尝试调整 LSTM 层数、隐藏节点数等参数，从而得到更好的模型。此外，建议您在模型训练过程中使用交叉验证等技术，对模型进行评估和调优。\n模型训练过程中的问题\n在您的代码中，训练过程中使用了 y_train_lstm 进行训练和误差计算，但是未调用 model.eval() 方法以使用 y_test_lstm 进行测试。此外，代码中使用了 torch.nn.MSELoss() 作为损失函数，但是可以尝试使用其他的损失函数来进行训练，比如说 Huber Loss，MAE 等，从而提高模型的鲁棒性。\n综上所述，建议您参考一些 LSTM 预测模型的开源项目，尝试使用其他的数据集，并调整模型参数，从而得到更好的预测结果。以下是一个 LSTM 预测模型的示例代码，仅供参考：\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\n\nclass LSTMModel(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n        self.fc = torch.nn.Linear(in_features=hidden_size, out_features=output_size)\n\n    def forward(self, x):\n        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        _, (h, _) = self.lstm(x, (h_0, c_0))\n        out = self.fc(h[-1, :, :])\n        return out\n\n\nclass Trainer:\n    def __init__(self, model, loss_fn, optimizer):\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n\n    def train(self, x, y):\n        self.model.train()\n        self.optimizer.zero_grad()\n        out = self.model(x)\n        loss = self.loss_fn(out.view(-1), y.view(-1))\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def validate(self, x, y):\n        self.model.eval()\n        with torch.no_grad():\n            out = self.model(x)\n            loss = self.loss_fn(out.view(-1), y.view(-1))\n        return loss.item()\n\n    def predict(self, x):\n        self.model.eval()\n        with torch.no_grad():\n            out = self.model(x)\n        return out.view(-1)\n\n    def save(self, epoch, model_path):\n        torch.save({\n            'epoch': epoch,\n            'state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n        }, model_path)\n\n    def load(self, model_path):\n        checkpoint = torch.load(model_path)\n        self.model.load_state_dict(checkpoint['state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        return epoch\n\n\ndef preprocess_data(data, train_ratio):\n    \"\"\"\n    :param data: pandas.DataFrame, raw data with column 'T-A11'\n    :param train_ratio: float, ratio of training set to validation set\n    :return: tuple, (train_data, val_data), each of which contain numpy.ndarray of shape (num_data, num_features)\n    \"\"\"\n    scaler = MinMaxScaler()\n    data_norm = scaler.fit_transform(data)\n\n    train_size = int(len(data_norm) * train_ratio)\n    train_data = data_norm[:train_size, :]\n    val_data = data_norm[train_size:, :]\n    return train_data, val_data\n\n\ndef create_dataset(data, lookback):\n    \"\"\"\n    :param data: numpy.ndarray of shape (num_data, num_features)\n    :param lookback: int, number of past time steps to use for next time step prediction\n    :return: tuple, (x, y) numpy.ndarray of shapes (num_samples, lookback, num_features) and (num_samples, 1)\n    \"\"\"\n    x, y = [], []\n    for i in range(lookback, len(data)):\n        x_ = data[i-lookback:i, :]\n        y_ = data[i, 0]  # predict only first column (T-A11)\n        x.append(x_)\n        y.append(y_)\n    return np.array(x), np.array(y).reshape(-1, 1)\n\n\ndef train_model(train_data, val_data, lookback, hidden_size, num_layers, num_epochs, lr):\n    train_x, train_y = create_dataset(train_data, lookback)\n    val_x, val_y = create_dataset(val_data, lookback)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    model = LSTMModel(input_size=train_x.shape[-1], hidden_size=hidden_size, num_layers=num_layers,\n                      output_size=train_y.shape[-1]).to(device)\n    loss_fn = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n    trainer = Trainer(model, loss_fn, optimizer)\n\n    train_losses, val_losses = [], []\n\n如果我的回答解决了您的问题，请采纳！", "Konwledge_Point": "应对NP完全问题", "Question": "关于#LSTM#的问题，如何解决？\n毕业论文，物流方向的，想做一个LSTM的预测，在b站上找了个教程，训练出的结果绘图符合预期，但是需要预测的目标数据范围从22.2-23.6变成了0-120。\n因为之前从未接触过python和机器学习，所以就把全部代码放出来：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n这里得出的RMSE也不符合预期，找正常数据来说跨度不可能这么大\n\n\n在这里的结果中，y轴数据范围变成了0-120，但是图形符合预期\n\n\n\n\n在最后的汇总图里面，y轴数据仍然和原来的不一样，图形仍符合预期\n训练过程和代码如上，使用的是jupyter，求知道怎么让训练结果的y轴和实际数据相同，且RMSE符合预期，谢谢各位。\njupyter上复制下来的代码在这下面：\n\n\n\n\n# In[1]:\n\n\n\nimport numpy as np\nimport pandas as pd\n\n\n\n# In[2]:\n\n\n\nfilepath = \n'C:/Users/本子怪/Desktop/论文/实验数据/分析用数据/DATA10.01.csv'\n\ndata = pd.read_csv(filepath)\ndata = data.sort_values(\n'Time'\n)\ndata.head()\n\n\n\n# In[3]:\n\n\n\ndata.shape\n\n\n\n# In[4]:\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns. set_style(\n\"darkgrid\"\n)\nplt. figure(figsize = (21, 9))\nplt. plot(data[[\n'T-A11'\n]])\nplt. xticks (range(0,data.shape[0],150), data[\n'Time'\n].loc[::150],\nrotation\n=45)\nplt. title(\n\"ee Stock Price\"\n,\nfontsize\n=18, \nfontweight\n=\n'bold'\n)\nplt. xlabel(\n'Time'\n,\nfontsize\n=18)\nplt. ylabel(\n'T-A11'\n,\nfontsize\n=18)\nplt. show()\n\n\n\n# In[5]:\n\n\n\nprice = data[[\n'T-A11'\n,\n'Twr'\n,\n'PreCool'\n,\n'Tpre'\n,\n'Cmen'\n,\n'Aopen'\n]]\nprice\n\n\n\n# In[6]:\n\n\n\n\nfrom\n sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(-1,1))\nprice[\n'T-A11'\n] = scaler.fit_transform(price[\n'T-A11'\n].values.reshape(-1,1))\n\n\n\n# In[7]:\n\n\n\nprice[\n'Twr'\n] = scaler.fit_transform(price[\n'Twr'\n].values.reshape(-1,1))\n\n\n\n# In[8]:\n\n\n\nprice[\n'PreCool'\n] = scaler.fit_transform(price[\n'PreCool'\n].values.reshape(-1,1))\n\n\n\n# In[9]:\n\n\n\nprice[\n'Tpre'\n] = scaler.fit_transform(price[\n'Tpre'\n].values.reshape(-1,1))\n\n\n\n# In[10]:\n\n\n\nprice[\n'Cmen'\n] = scaler.fit_transform(price[\n'Cmen'\n].values.reshape(-1,1))\n\n\n\n# In[11]:\n\n\n\nprice[\n'Aopen'\n] = scaler.fit_transform(price[\n'Aopen'\n].values.reshape(-1,1))\nprice\n\n\n\n# In[12]:\n\n\n\ndef split_data(stock, lookback):\n    data_raw = stock.to_numpy() \n    data = []\n    \n\n# you can free play (seqlength)\n\n    \nfor\n index \nin\n range(len(data_raw) - lookback):\n        data.append(data_raw[index: index + lookback])\n\n    data = np.array(data);\n    test_set_size = int(np.round(0.2 * data.shape[0])) \n    train_set_size = data.shape[0] - (test_set_size)\n\n    x_train = data[:train_set_size,:-1,:]\n    y_train = data[:train_set_size,-1,0:1]\n\n    x_test = data[train_set_size:,:-1,:] \n    y_test = data[train_set_size:,-1,0:1]\n\n    return [x_train, y_train, x_test, y_test]\n\n\n\n# In[13]:\n\n\n\nlookback = 20\nx_train, y_train, x_test, y_test = split_data(price, lookback) \n\nprint\n(\n'x_train.shape = '\n,x_train.shape) \n\nprint\n(\n'y_train.shape = '\n,y_train.shape) \n\nprint\n(\n'x_test.shape = '\n,x_test.shape) \n\nprint\n(\n'y_test.shape = '\n,y_test.shape)\n\n\n\n# In[14]:\n\n\n\nimport torch\nimport torch.nn as nn\n\nx_train = torch.from_numpy (x_train).type(torch.Tensor) \nx_test = torch.from_numpy(x_test).type(torch.Tensor)\ny_train_lstm = torch.from_numpy(y_train).type(torch.Tensor) \ny_test_lstm = torch.from_numpy(y_test).type(torch.Tensor) \ny_train_gru = torch.from_numpy(y_train).type(torch.Tensor) \ny_test_gru = torch.from_numpy(y_test).type(torch.Tensor)\n\n\n\n# In[15]:\n\n\n\ninput_dim = 6\nhidden_dim = 32\nnum_layers = 2\noutput_dim = 1\nnum_epochs = 100\n\n\n\n# In[16]:\n\n\n\nclass LSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super(LSTM, self). __init__() \n        self.hidden_dim = hidden_dim \n        self.num_layers = num_layers\n\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, \nbatch_first\n=\nTrue\n) \n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_() \n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_() \n        out, (hn,cn)= self.lstm(x, (h0.detach(), c0.detach()))\n        out = self.fc(out[:,-1, :]) \n        return out\n\n\n\n# In[17]:\n\n\n\nmodel = LSTM(\ninput_dim\n=input_dim, hidden_dim = hidden_dim, num_layers = num_layers, output_dim = output_dim) \ncriterion = torch.nn.MSELoss()\noptimiser = torch.optim.Adam(model.parameters(),\nlr\n=0.01)\n\n\n\n# In[18]:\n\n\n\nimport time\n\nhist = np.zeros(num_epochs) \nstart_time = time.time() \nlstm = []\n\n\nfor\n t \nin\n range(num_epochs):\n    y_train_pred = model(x_train)\n\n    loss = criterion(y_train_pred, y_train_lstm) \n    \nprint\n(\n\"Epoch \"\n, t,\n\"MSE:\"\n, loss.item()) \n    hist[t] = loss.item()\n\n    optimiser.zero_grad() \n    loss.backward()\n    optimiser.\nstep\n()\n\ntraining_time = time.time()-start_time\n\nprint\n(\n\"Training time: {}\"\n.format(training_time))\n\n\n\n# In[19]:\n\n\n\npredict = pd.DataFrame(scaler.inverse_transform(y_train_pred.detach().numpy())) \noriginal = pd.DataFrame(scaler.inverse_transform(y_train_lstm.detach().numpy()))\n\n\n\n# In[20]:\n\n\n\nimport seaborn as sns\nsns.set_style(\n\"darkgrid\"\n)\n\nfig = plt.figure()\nfig.subplots_adjust(\nhspace\n=0.2, \nwspace\n=0.2)\n\nplt.subplot(1,2,1)\nax = sns.lineplot(x = original.index, y = original[0], \nlabel\n=\n\"Data\"\n, \ncolor\n=\n'royalblue'\n)\nax = sns.lineplot(x = predict.index, y = predict[0], \nlabel\n=\n\"Training Prediction (LSTM)\"\n, \ncolor\n=\n'tomato'\n)\nax.set_title(\n'Stock price'\n, size = 14, \nfontweight\n=\n'bold'\n) \nax.set_xlabel(\n\"Time\"\n,size = 14)\nax.set_ylabel(\n\"T-A11\"\n,size = 14) \nax.set_xticklabels(\n''\n, \nsize\n=10)\n\n#手动更改标签以及刻度\n\n\n#ax.set_yticklabels([22.2,22.4,22.6,22.8,23.0,23.2,23.4,23.6])\n\n\nplt.subplot(1,2,2)\nax = sns.lineplot(\ndata\n=hist, \ncolor\n=\n'royalblue'\n) \nax.set_xlabel(\n\"Epoch\"\n, size = 14) \nax.set_ylabel(\n\"Loss\"\n, size =14)\nax.set_title(\n\"Training Loss\"\n, size = 14, fontweight= \n'bold'\n) \nfig.set_figheight(6) \nfig.set_figwidth(16)\n\n\n\n# In[21]:\n\n\n\nimport math, time\n\nfrom\n sklearn.metrics import mean_squared_error\n\n\n# make predictions\n\ny_test_pred = model(x_test)\n\n\n# invert predictions\n\ny_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy()) \ny_train = scaler.inverse_transform(y_train_lstm.detach().numpy())\ny_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy()) \ny_test = scaler.inverse_transform(y_test_lstm.detach().numpy())\n\n\n# calculate root mean squared error\n\ntrainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0])) \n\nprint\n(\n'Train Score: %.2f RMSE'\n % (trainScore))\ntestScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0])) \n\nprint\n(\n'Test Score: %.2f RMSE'\n % (testScore)) \nlstm.append(trainScore) \nlstm.append(testScore)\nlstm.append(training_time)\n\n\n\n# In[22]:\n\n\n\nnew_price = price[[\n'T-A11'\n]]\nnew_price\n\n\n\n# In[23]:\n\n\n\ntrainPredictPlot = np.empty_like(new_price)\ntrainPredictPlot[:,0] = np.nan\ntrainPredictPlot[lookback\n:len\n(y_train_pred)+lookback, :] = y_train_pred\n\n\n\n# In[24]:\n\n\n\ntestPredictPlot = np.empty_like(new_price)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(y_train_pred)+lookback-1\n:len\n(price)-1, :] = y_test_pred\n\n\n\n# In[25]:\n\n\n\noriginal = scaler.inverse_transform(price[\n'T-A11'\n].values.reshape(-1,1))\n\npredictions = np.append(trainPredictPlot,testPredictPlot, \naxis\n=1) \npredictions = np.append(predictions, original,\naxis\n=1) \nresult = pd.DataFrame(predictions)\n\n\n\n# In[26]:\n\n\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(go.Scatter(\nx\n=result.index, \ny\n=result[0],\n                    \nmode\n=\n'lines'\n,\n                    \nname\n=\n'Train prediction'\n)))\nfig.add_trace(go.Scatter(\nx\n=result.index, \ny\n=result[1],\n                    \nmode\n=\n'lines'\n,\n                    \nname\n=\n'Test prediction'\n))\nfig.add_trace(go.Scatter(go.Scatter(\nx\n=result.index, \ny\n=result[2],\n                   \nmode\n=\n'lines'\n,\n                   \nname\n=\n'Actual Value'\n)))\nfig.update_layout(\n    \nxaxis\n=dict(\n        \nshowline\n=\nTrue\n, \n        \nshowgrid\n=\nTrue\n,\n        \nshowticklabels\n=\nFalse\n, \n        \nlinecolor\n=\n'white'\n, \n        \nlinewidth\n=2\n    ),\n    \nyaxis\n=dict(\n        \ntitle_text\n=\n'当前温度 (T-A11)'\n, \n        \ntitlefont\n=dict(\n            \nfamily\n=\n'Rockwell'\n, \n            \nsize\n=12,\n            \ncolor\n=\n'white'\n,\n        ),\n        \nshowline\n=\nTrue\n, \n        \nshowgrid\n=\nTrue\n,\n        \nshowticklabels\n=\nTrue\n, \n        \nlinecolor\n=\n'white'\n, \n        \nlinewidth\n=2,\n        \nticks\n=\n'outside'\n,\n        \ntickfont\n=dict(\n            \nfamily\n=\n'Rockwell'\n, \n            \nsize\n=12,\n            \ncolor\n=\n'white'\n,\n        ),\n    ),\n    \nshowlegend\n=\nTrue\n,\n    template =\n'plotly_dark'\n\n\n)\n\n\n\nannotations = []\nannotations.append(dict(\nxref\n=\n'paper'\n, \nyref\n=\n'paper'\n, \nx\n=0.0, \ny\n=1.05,\n                              \nxanchor\n=\n'left'\n, \nyanchor\n=\n'bottom'\n, \n                              \ntext\n=\n'Results (LSTM)'\n,\n                              \nfont\n=dict(family='Rockwell',\n                                        \nsize\n=26,\n                                        \ncolor\n=\n'white'\n),\n                              \nshowarrow\n=\nFalse\n))\nfig.update_layout(\nannotations\n=annotations)\n\nfig.show()\n\n", "Tag": "算法分析"}
{"Answer": "矩阵形状错误了", "Konwledge_Point": "应对NP完全问题", "Question": "学习OpenCV时遇到如下问题：error: (-5:Bad argument) Wrong shapes for given matrices. 怎么解决？\n问题遇到的现象和发生背景\n\n\n刚刚学习openCV，打算调用笔记本电脑上的摄像头来简单实现人脸识别，输出身份。但在进行识别时报错，一时无法解决，前来请教。传入识别器的图片和训练用图片都一样是640*480大小，3通道。将图像转化为灰度图像后程序可以运行，但彩色该怎么办？\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport cv2\nimport numpy as np\nfrom cv2 import VideoCapture\n\nphotos=list()\nlables=list()\nphotos\n.append\n(cv2\n.imread\n(\n\"a1.jpg\"\n))\nphotos\n.append\n(cv2\n.imread\n(\n\"a2.jpg\"\n))\nphotos\n.append\n(cv2\n.imread\n(\n\"a3.jpg\"\n))\nlables\n.append\n(\n0\n)\nlables\n.append\n(\n0\n)\nlables\n.append\n(\n0\n)\n\nphotos\n.append\n(cv2\n.imread\n(\n\"lb1.jpg\"\n))\nphotos\n.append\n(cv2\n.imread\n(\n\"b2.jpg\"\n))\nphotos\n.append\n(cv2\n.imread\n(\n\"b3.jpg\"\n))\nlables\n.append\n(\n1\n)\nlables\n.append\n(\n1\n)\nlables\n.append\n(\n1\n)\n\nnames={\n\"0\"\n:\n\"a\"\n,\n\"1\"\n:\n\"b\"\n}\nrecognizer=cv2\n.face\n.EigenFaceRecognizer_create\n()\nrecognizer\n.train\n(photos,np\n.array\n(lables))\n\ncapture=VideoCapture(\n0\n)\nwhile capture\n.isOpened\n():\n    retval,image=capture\n.read\n()\n    faceCascade = cv2\n.CascadeClassifier\n(\n\"haarcascade_frontalface_default.xml\"\n)\n    faces = faceCascade\n.detectMultiScale\n(image, \n1.15\n)\n    \nfor\n (x, y, w, h) \nin\n faces:\n\n        cv2\n.rectangle\n(image, (x, y), (x + w, y + h), (\n0\n, \n0\n, \n255\n), \n5\n)\n        #image=cv2\n.putText\n(image,names\n[str(lable)]\n,(x,y),cv2\n.FONT_HERSHEY_DUPLEX\n,\n2\n,(\n0\n,\n255\n,\n0\n),\n5\n)\n    cv2\n.imshow\n(\n\"Video\"\n,image)\n    key=cv2\n.waitKey\n(\n1\n)\n    \nif\n key==\n32\n:\n        lable, confidence = recognizer\n.predict\n(image)\n        print(names\n[str(lable)]\n)\n    \nif\n key==\n27\n:\n        print(image.shape)\n        break\n\ncapture\n.release\n()\ncv2\n.destroyAllWindows\n()\n\n\n\n运行结果及报错内容\n\n\ncv2.error: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\lda.cpp:183: error: (-5:Bad argument) Wrong shapes for given matrices. Was size(src) = (1,921600), size(W) = (307200,6). in function 'cv::LDA::subspaceProject'", "Tag": "算法分析"}
{"Answer": "按列试试  axis=0", "Konwledge_Point": "应对NP完全问题", "Question": "如何使用python对excel做归一化处理并保存？\n使用python对excel做归一化处理并保存\n\n\ndf=pd.read_excel('data/testdata_2.xlsx')\nmax_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\ndf1=df.apply(max_min_scaler, axis=1)\ndf1.to_excel('data/testdata_2n.xlsx',index=False)\n\n\n运行结果及报错内容\n\n\n没报错就是按行的 我不要\n\n\n我的解答思路和尝试过的方法\n\n\n这个代码是按行做归一化我需要按列做归一化\n\n\n我想要达到的结果\n\n\n按列做归一化", "Tag": "算法分析"}
{"Answer": "创建的时候加个 dtype = int  看看", "Konwledge_Point": "应对NP完全问题", "Question": "numpy 中空白向量数据类型异常\n背景问题说明：**python  3中引用numpy  ，生成一个数组，但最终数组为啥不是整数型，而是如图这种类型，辛苦帮忙看下，感激；\n*\n问题相关代码:**\nimport  numpy as np\ndata=np.empty(10)\ndata[4]=1\nprint(\"对应的数组为 \",data)\n\n\n**运行结果及报错内容 :**没有报错，就是最后出来的值格式不对 ，不是整数型，而是一个很复杂的计数\n\n\n\n\n**我的解答思路和尝试过的方法 **： 未尝试过其他方案，第一次练习，没有思路\n\n\n** 我想要达到的结果**：变成整数型", "Tag": "算法分析"}
{"Answer": "在第11行代码后加一行就行了。\nax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9,ax10,ax11,ax12 = ax.flatten()\nax9.set_axis_off()\n\n是这样的效果吗？\n", "Konwledge_Point": "应对NP完全问题", "Question": "matplotlib多图状态下雷达图是否可去掉矩形框\nmatplotlib雷达图与其他图形共存时，如何设置可将雷达图外的矩形框隐藏或去除（左下图）\n\n\n\n\n\n```python\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\n\n\n#图形设置\n\nfig,ax = plt.subplots(\nnrows\n=3,ncols=4,figsize=(20,10),dpi=80)\nax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9,ax10,ax11,ax12 = ax.flatten()\n\n\n#中文设置\n\nplt.rcParams[\n'font.sans-serif'\n] = [\n'SimHei'\n]  # 用于显示中文\nplt.rcParams[\n'axes.unicode_minus'\n] = \nFalse\n  # 用于解决保存图像是负号‘-’显示为方框的问题\n\n\n\n#设置随机数据源\n\na = []\nb = []\nc = []\n\nfor\n i \nin\n a,b,c:\n    \nfor\n j \nin\n random.choices(range(1,6),\nk\n=5):\n        i.append(j)\n        \nif\n len(c) == 5:\n            break\n\n#极轴刻度\n\nlabels = [\n'var1'\n,\n'var2'\n,\n'var3'\n,\n'var4'\n,\n'var5'\n]\n\n# labels.append(labels[0])                    #要么这里闭合，要么下面闭合\n\n\n#轴径\n\nN = len(a)\n\n#设置雷达图的角度\n\nangles = np.linspace(0,2*np.pi,N,\nendpoint\n=\nFalse\n)\n\n#封闭雷达图\n\na = np.concatenate((a,[a[0]]))\nb = np.concatenate((b,[b[0]]))\nc = np.concatenate((c,[c[0]]))\nangles = np.concatenate((angles,[angles[0]]))\nlabels = np.concatenate((labels,[labels[0]]))           #要么这里闭合，要么上面闭合\n\n#设置为极坐标格式\n\nax9 = fig.add_subplot(349,\npolar\n=\nTrue\n)\n\n#绘制折线图\n\nax9.plot(angles,a,\n'b.'\n,\nlw\n=1,label='A',alpha=0.8)\nax9.plot(angles,b,\n'r.'\n,\nlw\n=1,label='B',alpha=0.8)\nax9.plot(angles,c,\n'g.'\n,\nlw\n=1,label='C',alpha=0.8)\n\n#填充颜色\n\nax9.fill(angles,a,\n'b'\n,\nalpha\n=0.5)\nax9.fill(angles,b,\n'r'\n,\nalpha\n=0.5)\nax9.fill(angles,c,\n'g'\n,\nalpha\n=0.5)\n\n#添加特征标签\n\nax9.set_thetagrids(angles\n*180\n/np.pi,labels)\n\n# ax9.set_xticks(angles*180/np.pi,labels)\n\n\n# ax9.set_xticks(angles*180/np.pi)\n\n\n# ax9.set_xticks(angles[:-1],labels)\n\n\n#设置极轴范围\n\nax9.set_ylim(0,6)\n\n#设置极轴方向\n\nax9.set_theta_zero_location(\n'N'\n)\n\n#设置网格线\n\nax9.grid(\nTrue\n)\n\n#设置图例\n\nax9.legend(\nloc\n=3)\n\n#设置标题\n\nax9.set_title(\n'雷达图'\n)\n\nplt.show()\n\n\n\n```", "Tag": "算法分析"}
{"Answer": "你看下这篇博客吧, 应该有用👉 ：pytorch+cnn+lstm+词向量", "Konwledge_Point": "应对NP完全问题", "Question": "pytorch搭建的cnn-lstm的Tensor问题\n\nimport torch\n\nfrom\n torch import nn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom\n sklearn.preprocessing import MinMaxScaler\n\nfrom\n torch.nn import MaxPool2d, Conv2d, Dropout, ReLU\n\nfrom\n torch.utils.data import DataLoader, Dataset\n\n\n#准备数据集\n\n\ndf\n=pd.read_csv(\"train.csv\",parse_dates=[\n\"Date\"\n],index_col=[0])\n\nprint\n(df.shape)\n\ntrain_data_size\n=round(len(df)*0.8)\n\ntest_data_size\n=round(len(df)*0.2)\n\nprint\n(\n\"训练数据集的长度为：{}\"\n.format(train_data_size))\n\nprint\n(\n\"测试数据集的长度为：{}\"\n.format(test_data_size))\n\n\n# df[['Open']].plot()\n\n\n# plt.ylabel(\"stock price\")\n\n\n# plt.xlabel(\"times\")\n\n\n# plt.show()\n\n\nsel_col = [\n'Open'\n, \n'High'\n, \n'Low'\n, \n'Close'\n]\n\ndf\n=df[sel_col]\n\n\ndf_close_max\n=df[\n'Close'\n].max()\n\ndf_close_min\n=df[\n'Close'\n].min()\n\nprint\n(\n\"最高价=\"\n, df_close_max)\n\nprint\n(\n\"最低价=\"\n, df_close_min)\n\nprint\n(\n\"波动值=\"\n, df_close_max-df_close_min)\n\nprint\n(\n\"上涨率=\"\n, (df_close_max-df_close_min)/df_close_min)\n\nprint\n(\n\"下跌率=\"\n, (df_close_max-df_close_min)/df_close_max)\n\n\ndf\n=df.apply(lambda x:(x-min(x))/(max(x)-min(x)))\n\nprint\n(df)\n\n\ntotal_len\n=df.shape[0]\n\nprint\n(\n\"df.shape=\"\n,df.shape)\n\nprint\n(\n\"df_len=\"\n, total_len)\n\n\nsequence\n=10\nx=[]\ny=[]\n\n\nfor\n i \nin\n range(total_len-sequence):\n\n    x.append(np.array(df.iloc[i:(i+sequence),].values,\ndtype\n=np.float32))\n    y.append(np.array(df.iloc[(i+sequence),1],\ndtype\n=np.float32))\n\nprint\n(\n\"train data  of item  0: \\n\"\n, x[0])\n\nprint\n(\n\"train label of item  0: \\n\"\n, y[0])\n\n\nprint\n(\n\"\\n序列化后的数据形状：\"\n)\nX = np.array(x)\nY = np.array(y)\nY = np.expand_dims(Y, 1)\n\nprint\n(\n\"X.shape =\"\n,X.shape)\n\nprint\n(\n\"Y.shape =\"\n,Y.shape)\n\ntrain_x = X[:int(0.7 * total_len)]\ntrain_y = Y[:int(0.7 * total_len)]\n\n\n\n# 数据集前70%后的数据（30%）作为验证集\n\nvalid_x = X[int(0.7 * total_len):]\nvalid_y = Y[int(0.7 * total_len):]\n\n\nprint\n(\n\"训练集x的形状是：\"\n,train_x.shape)\n\nprint\n(\n\"测试集y的形状是：\"\n,train_y.shape)\n\nprint\n(\n\"测试集x的形状是：\"\n,valid_x.shape)\n\nprint\n(\n\"测试集y的形状是：\"\n,valid_y.shape)\n\n\nclass Mydataset(Dataset):\n\n    def __init__(self, x, y, \ntransform\n=None):\n        self.x = x\n        self.y = y\n\n    def __getitem__(self, index):\n        x1 = self.x[index]\n        y1 = self.y[index]\n        return x1, y1\n\n    def __len__(self):\n        return len(self.x)\n\ndataset_train = Mydataset(train_x, train_y)\ndataset_valid = Mydataset(valid_x, valid_y)\n\n\ntrain_dataloader\n=DataLoader(dataset_train,batch_size=64)\n\nvalid_dataloader\n=DataLoader(dataset_valid,batch_size=64)\n\n# print(train_dataloader)\n\n\n# print(valid_dataloader)\n\nclass cnn_lstm(nn.Module):\n    def __init__(self,window_size,feature_number):\n        super(cnn_lstm, self).__init__()\n        self.\nwindow_size\n=window_size\n        self.\nfeature_number\n=feature_number\n        self.conv1 = Conv2d(\nin_channels\n=1, \nout_channels\n=64, \nkernel_size\n=3, \nstride\n=1, \npadding\n=2)\n        self.relu1 = ReLU()\n        self.maxpooling1 = MaxPool2d(2, \nstride\n=1, \npadding\n=\n\"same\"\n)\n        self.dropout1 = Dropout(0.3)\n        self.lstm1 = nn.LSTM(\ninput_size\n=64 * feature_number, \nhidden_size\n=128, \nnum_layers\n=1, \nbatch_first\n=\nTrue\n)\n        self.lstm2 = nn.LSTM(\ninput_size\n=128, \nhidden_size\n=64, \nnum_layers\n=1, \nbatch_first\n=\nTrue\n)\n        self.fc = nn.Linear(\nin_features\n=64, \nout_features\n=32)\n        self.relu2 = nn.ReLU()\n        self.head = nn.Linear(\nin_features\n=32, \nout_features\n=1)\n\n    def forward(self, x):\n\n            # x = x.reshape([x.shape[0], 1, self.window_size, self.feature_number])\n            x = x.transpose(-1, -2)\n            x = self.conv1(x)\n            x = self.relu1(x)\n            x = self.pool(x)\n            x = self.dropout(x)\n\n            # x = x.reshape([x.shape[0], self.window_size, -1])\n            x = x.transpose(-1, -2)  #\n            x, (h, c) = self.lstm1(x)\n            x, (h, c) = self.lstm2(x)\n            x = x[:, -1, :]  # 最后一个LSTM只要窗口中最后一个特征的输出\n            x = self.fc(x)\n            x = self.relu2(x)\n            x = self.head(x)\n\n            return x\n\n\n#创建网络模型\n\n\ncnn_lstm\n=cnn_lstm(window_size=10,feature_number=4)\n\n\n#定义损失函数\n\n\nloss_fn\n=nn.MSELoss(size_average=True)\n\n\n#定义优化器\n\n\nlearning_rate\n=0.01\n\nopitmizer\n=torch.optim.Adam(cnn_lstm.parameters(),learning_rate)\n\n\n#设置训练网络参数\n\n\ntotal_train_step\n=0\n\ntotal_valid_step\n=0\n\n\n#训练论数\n\n\nepoch\n=10\n\n\nfor\n i \nin\n range(epoch):\n    \nprint\n(\n\"______第{}轮训练开始________\"\n.format((i + 1)))\n    \ny_train_pred\n=cnn_lstm(train_x)\n    \nloss\n=loss_fn(train_x,train_y)\n\n    #优化器优化模型\n    opitmizer.zero_gard()\n    loss.backward()\n    opitmizer.\nstep\n()\n\n    total_train_step = total_train_step + 1\n    \nif\n total_train_step % 100 == 0:\n        \nprint\n(\n\"训练次数：{}，loss:{}\"\n.format(total_train_step, loss.item()))\n\n\n\n\n请问在这个数据集划分的部分，在哪里可以添加 将数据类型转化为totensor的格式", "Tag": "算法分析"}
{"Answer": "我这边把你的代码测试了一下，结果是可以正常显示的。根据你的图片展示，个人分析是包没导入（因为numpy都是红波浪线）。检查下你的python解释器是否是你包含PIL的那一个。\n", "Konwledge_Point": "应对NP完全问题", "Question": "python中PIL库的导入和下载\n问题遇到的现象和发生背景\n\n\n在python3.10版本下，安装PIL库，在pycharm里面不能用，在IDLE里面能用，怎么回事\n\n\n用代码块功能插入代码，请勿粘贴截图\n\n\nfrom\n PIL \nimport\n Image\n\n#\n\n\nimport\n numpy \nas\n np\n\n#\n\nim = np.array(Image.\nopen\n(\nr\"C:\\Users\\伊迪斯\\OneDrive\\桌面\\core\\222.jpg\"\n))\n\n#\n\n\nprint\n(im.shape,im.dtype)\n\n\n\n\n\n运行结果及报错内容\n\n\n在pycharm里面是这样的\n\n\n\n\n在IDLE里面正常\n\n", "Tag": "算法分析"}
{"Answer": "p指针用于访问结构体内的x变量np中的next指针是用于指向下一个node的因为np本身就是一个指针,所以直接把np的地址值赋值给np->next即可", "Konwledge_Point": "应对NP完全问题", "Question": "第三个空为什么不是取p指针的地址\n答案是np为什么最后一条汇编指令是把p指针的地址送给⭐next对吧，然后p指针指向x的地址。如果想指向np的话我觉得应该要把p指针本身送给next。", "Tag": "算法分析"}
{"Answer": "从你所问一系列问题，感觉你基本一窍不通，程序运行不了，应该先看 readme.md / readme.txt 文档，搞清楚环境\r\n然后下载作者提供的原始数据集，先跑通程序，然后再让你的数据集的格式、维度和它的一致，然后再修改。", "Konwledge_Point": "应对NP完全问题", "Question": "如何解决Layer type `softmax` is not implemented\n第一个问题：我在编译的时候发现\nsoftmax\n is not implemented的问题，不知道怎么编写softmax和编写好之后把softmax文件放在哪\n\n第二个问题：如果我把Layer('softmax')删了，编译的时候会显示Mismatch between dataset size and units in output layer.我不知道哪个维数出问题了，大佬可以帮忙解答吗\n\n\n\nimport numpy as np\nimport urllib.request\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport pandas as pd\nimport xlrd\nfrom sklearn import preprocessing\ndef excel_to_matrix(path):\n    table = xlrd.open_workbook(path).sheets()[0]  # 获取第一个sheet表\n    row = table.nrows  # 行数\n    col = table.ncols  # 列数\n    datamatrix = np.zeros((row, col))\n    for x in range(col):\n        cols = np.matrix(table.col_values(x))\n\n        datamatrix[:, x] = cols\n    return datamatrix\n\n\ndatafile = u'C:\\\\Users\\\\asus\\\\PycharmProjects\\\\2\\\\venv\\\\Lib\\\\附件2：数据.xls'\ndatamatrix=excel_to_matrix(datafile)\ndata=pd.DataFrame(datamatrix)\n\ny=data[10]\ndata=data.drop(10,1)\nx=data\n# print(y.shape)\nfrom sklearn import preprocessing\nx_MinMax=preprocessing.MinMaxScaler()\ny_MinMax=preprocessing.MinMaxScaler()\n\ny.as_matrix(y)\ny=np.array(y).reshape((len(y),1))\nx=np.array(x).reshape((len(x),10))\nx=x_MinMax.fit_transform(x)\ny=y_MinMax.fit_transform(y)\nx.mean(axis=0)\n\nimport random\nfrom sklearn.cross_validation import train_test_split\nnp.random.seed(2016)\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n\nfrom sknn.mlp import Regressor,Layer #预测模型\n\nfit3=Regressor(layers=[Layer('Tanh',units=45),Layer('Tanh',units=18),\n\n                       Layer('softmax')],\n               learning_rate=0.02,\n               random_state=2016,\n               valid_size=0.25,\n               dropout_rate=0.2,\n               learning_momentum=0.30,\n               batch_size=35,\n               n_iter=10\n               )\nfit3.fit(x_train,y_train)\n\nfrom sklearn.metrics import confusion_matrix\npredict3_train=fit3.predict(x_train)\nscore3=fit3.score(x_train,y_train)\nconfu3=confusion_matrix(y_train,predict3_train)\nprint(confu3)\nscore_text3=fit3.score(x_test,y_test)\nprint(score_text3)\npredict3_test=fit3.predict(x_test)\nconfu3_test=confusion_matrix(y_test,predict3_test)\nprint(confu3_test)\n", "Tag": "算法分析"}
{"Answer": "把OpenCV 降级成3.4.3.18 就可以了，在终端输入pip install  opencv-python==3.4.3.18", "Konwledge_Point": "应对NP完全问题", "Question": "python调用cv2.findContours时报错：ValueError: not enough values to unpack (expected 3, got 2)\n完整代码如下：\n\n\n\nimport cv2\nimport numpy as np\n\nimg = np.zeros((200, 200), dtype=np.uint8)\nimg[50:150, 50:150] = 255\n\nret, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\nimage, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\ncolor = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\nimg = cv2.drawContours(color, contours, -1, (0,255,0), 2)\n\ncv2.imshow(\"contours\", color)\ncv2.waitKey()\ncv2.destroyAllWindows()\n\n\n\n\n但是cv2.findContours报如下错误：\n\nValueError: not enough values to unpack (expected 3, got 2)\n\n\n\npython版本为3.6，opencv为4.0.0", "Tag": "算法分析"}
{"Answer": "第一个问题：        根据soft voting和hard voting的定义来看，soft是将所有模型预测样本为某一类别的概率的平均值作为标准，概率最高的对应的类型为最终的预测结果；而hard是根据少数服从多数来定最终结果。       如果某一子模型在这个问题上表现不好，那使用soft就会将这个子模型的结果也考虑进去，这就会大大影响整个集成模型的效果。此时如果使用hard，那就会直接忽略掉这个效果不好的子模型，从而使整个集成模型的效果变好。\n第二个问题：        在这个问题中，楼主可能对模型存在一定误解，认为模型越多就越好，其实不然。举个简单的例子，当你的数据集较少时，你的决策树子树个数增加，这可能导致不同子树使用到的数据是相似甚至是相似的，这种情况下，增加子树就没有任何意义，甚至会出现过拟合现象。\n    对于模型而言，没有绝对的谁好谁坏，需要针对不同的数据集，不同的特征来选取合适的模型，这样才能得到比较好的效果。\n", "Konwledge_Point": "应对NP完全问题", "Question": "sklearn集成学习 关于 算法准确率的两个问题\n在学习sklearn中的集成学习中遇到了两个问题：\n\n\n集成学习中soft voting的准确率低于hard voting。\n代码如下：\n\n\nimport\n numpy \nas\n np\n\nimport\n matplotlib.pyplot \nas\n plt\n\nfrom\n sklearn \nimport\n datasets\nx,y = datasets.make_moons(n_samples = \n500\n,noise = \n0.3\n, random_state = \n42\n)\n\nfrom\n sklearn.model_selection \nimport\n train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = \n42\n)\n\nfrom\n sklearn.ensemble \nimport\n VotingClassifier\nvoting_clf = VotingClassifier(estimators = [\n    (\n'log_clf'\n,LogisticRegression()),\n    (\n'svm_clf'\n,SVC()),\n    (\n'dt_clf'\n,DecisionTreeClassifier())\n],voting = \n'hard'\n)\nvoting_clf.fit(x_train,y_train)\nvoting_clf.score(x_test,y_test)\nvoting_clf2 = VotingClassifier(estimators = [\n    (\n'log_clf'\n,LogisticRegression()),\n    (\n'svm_clf'\n,SVC(probability = \nTrue\n)), #修改SVC参数\n    (\n'dt_clf'\n,DecisionTreeClassifier(random_state = \n666\n))],voting = \n'soft'\n)\nvoting_clf2.fit(x_train,y_train)\nvoting_clf2.score(x_test,y_test)# soft 与hard 的结果都是\n0.904\n 很奇怪\n\n\n\n\n集成学习中采用决策树的数量增多，准确率并没有提高。\n代码如下：\nimport\n numpy as np\n\nimport\n matplotlib.pyplot as plt\nfrom sklearn \nimport\n datasets\nx,\ny\n = datasets.make_moons(\nn_samples\n = \n500\n,\nnoise\n = \n0.3\n, \nrandom_state\n = \n42\n)\nfrom sklearn.model_selection \nimport\n train_test_split\nx_train,x_test,y_train,\ny_test\n = train_test_split(x,y,\nrandom_state\n = \n42\n)\n\nbagging_clf\n = BaggingClassifier(DecisionTreeClassifier(),\n                            \nn_estimators\n =\n500\n, \nmax_samples\n = \n100\n,\nbootstrap\n = True)\n\n# n_estimator 多少个子模型 max_samples看多少样本 bootstrap是否放回\n\n%%time\nbagging_clf.fit(x_train,y_train)\n# 500个决策树\n\nbagging_clf.score(x_test,y_test)\n\nsingle_dec_tree\n = DecisionTreeClassifier()\nsingle_dec_tree.fit(x_train,y_train)\n# 1个决策树\n\nsingle_dec_tree.score(x_test,y_test)\n\nbagging_clf5000\n = BaggingClassifier(DecisionTreeClassifier(),\n                                \nn_estimators\n = \n5000\n,\nmax_samples\n = \n100\n,\nbootstrap\n = True)\n\n# 5000个决策树\n\n%%time\nbagging_clf5000.fit(x_train,y_train)\nbagging_clf5000.score(x_test,y_test)\n\n# 单个决策树的准确率为0.88，500个是0.928，5000个是0.912\n\n\n\n\n\n\n这两个问题类似，因为从算法的原理上讲，soft的结果至少不会比hard的差，为啥结果会这样呢？\n第二个问题中，子模型的数目增多，一定会使得整体的准确率提高，但是为啥决策树的数目增多，准确率不升反降？\n难道存在一些子模型的准确率低于平均准确率吗？很奇怪", "Tag": "算法分析"}
{"Answer": "（argmax只能返回第一个最大值）继续循环迭代下去.\n\nfirst = np.argmax(counts)\n\nmaxCount = counts.count(counts[first]) # 先数一下最大值有几个，大于1个就迭代\ndef findC(i, li):\n    if maxCount > 1:\n        nextMax = np.argmax(counts[first + 1: ])\n        print(first, nextMax)\n        findC(nextMax, counts)\n    else:\n        return\nfindC(first, maxCount)\n ", "Konwledge_Point": "应对NP完全问题", "Question": "numpy数组求众数，对于有多个众数的情况应该如何处理呢？\nimport numpy as np\n\n\n\nfrom scipy import stats\n\n\n\ndata=np.array([23,28,34,17,69,28,28,80,36,97,58,58,58])\n\n\n\n方法一：\n\n\n\ncounts=np.bincount(data)\n\n\n\nnp.argmax(counts)\n\n\n\n方法二：\n\n\n\nstats.mode(data)[0][0] #使用stats函数中的mode方法可以获取众数\n\n\n\n这两种方法都只能求出28为众数，实际上58也是众数，这个要怎么才能获取出来呢？", "Tag": "算法分析"}
{"Answer": "你看下这篇博客吧, 应该有用👉 ：python math库函数", "Konwledge_Point": "应对NP完全问题", "Question": "Python的numpy和math库的调用\n为什么用了numpy库的函数 却还要从math库里调用sqrt sin cos这三个函数 不能直接用np.sin np.cos这样调用出来吗 不是很懂第一行调用math库的函数的作用", "Tag": "算法分析"}
{"Answer": "np.where最简单：\n\nimport pandas as pd\nimport numpy as np\n\n\ndef main():\n    condition = {\"condition\": np.random.randn(200)}\n    df = pd.DataFrame(condition)\n    df['data1'] = np.random.randint(1, 100, len(df))\n    df['data2'] = np.random.randint(1, 100, len(df))\n    df['result'] = np.where(df['condition'] > 0, df['data1'], df['data2'])\n    print(df.head(10))\n    return\n\n\nmain()\n\n ", "Konwledge_Point": "应对NP完全问题", "Question": "我如何根据某列的条件来赋值？\n我在学习dataframe的时候碰到一个基本的问题，\n\n\n\n如果data列>0，我需要把a列的相同行的值赋给data2列，如果data2列<0，我需要把b列的相同行的值赋给data2，我尝试了好几种办法，都没成功\n\n\n\n代码如下(python 3.8)：\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\"data\": np.random.randn(200)}\ndf = pd.DataFrame(data)\ndf['a']=np.random.randint(1,100, len(df))\ndf['b']=np.random.randint(1,100, len(df))\ndf['data2']=0\n\n\n\n上面的df和我的数据类似，我想到一个方法，感觉很粗糙，有没有更优的方法？\n\n\n\n\n\ndf['data2'].loc[df['data']>0]=df[df['data']>0]['a']\ndf['data2'].loc[df['data']<0]=df[df['data']<0]['b']", "Tag": "算法分析"}
{"Answer": "你的数据中有非数值型的字符串类型数据，你检查一下新使用的数据", "Konwledge_Point": "应对NP完全问题", "Question": "python聚类问题\n问题遇到的现象和发生背景\n\n\n部分数据集\nd=[[1994.0, 9.6], [1957.0, 9.5], [1997.0, 9.5], [1994.0, 9.4], [1993.0, 9.4], [2012.0, 9.4], [1993.0, 9.4], [1997.0, 9.4], [2013.0, 9.4], [1994.0, 9.4], [2003.0, 9.3], [2016.0, 9.3], [2009.0, 9.3], [2009.0, 9.3], [2008.0, 9.3], [2008.0, 9.3], [1957.0, 9.3], [2008.0, 9.3], [2001.0, 9.2], [2009.0, 9.2], [1931.0, 9.2], [1961.0, 9.2], [2010.0, 9.2], [2004.0, 9.2], [1998.0, 9.2], [2009.0, 9.2], [1972.0, 9.2], [1939.0, 9.2], [2001.0, 9.2], [2015.0, 9.2], [1946.0, 9.2], [2011.0, 9.2], [2011.0, 7.0], [2010.0, 9.2], [1997.0, 9.2], [2010.0, 9.2], [2009.0, 9.2], [2013.0, 9.2], [1982.0, 9.2], [1960.0, 9.2], [2006.0, 9.2], [2012.0, 7.4], [2010.0, 8.5], [2008.0, 9.2], [2006.0, 9.2], [1988.0, 9.2], [1993.0, 8.0], [2013.0, 9.1], [2002.0, 9.1], [2016.0, 5.6], [2013.0, 9.1], [2011.0, 9.1], [2013.0, 8.6], [1995.0, 9.1], [1996.0, 5.7], [1995.0, 9.1], [1984.0, 9.0]]\n\n\n用代码块功能插入代码，请勿粘贴截图。 不用代码块回答率下降 50%\n\n\n# encoding:utf-8\n\nimport matplotlib.pyplot \nas\n plt\nimport \nrandom\n\nimport numpy \nas\n np\nimport math\nimport matplotlib\n\n\n\n# 计算两个点之间的欧式距离，参数为两个元组\n\ndef dist(t1, t2):\n    dis = math.\nsqrt\n((np.power((t1[\n0\n] - t2[\n0\n]), \n2\n) + np.power((t1[\n1\n] - t2[\n1\n]), \n2\n)))\n    \nreturn\n dis\n\n\n# DBSCAN算法，参数为数据集，Eps为指定半径参数，MinPts为制定邻域密度阈值\n\ndef dbscan(Data, Eps, MinPts):\n    \nnum\n = \nlen\n(Data)  \n# 点的个数\n\n    unvisited = [i \nfor\n i \nin\n range(\nnum\n)]  \n# 没有访问到的点的列表\n\n    visited = []  \n# 已经访问的点的列表\n\n    C = [\n-1\n \nfor\n i \nin\n range(\nnum\n)]  \n# C为输出结果，默认是一个长度为num的值全为-1的列表        \n\n    k = \n-1\n  \n# 用k来标记不同的簇，k = -1表示噪声点\n\n    \nwhile\n \nlen\n(unvisited) > \n0\n:        \n        p = \nrandom\n.choice(unvisited)  \n# 随机选择一个unvisited对象\n\n        unvisited.remove(p)\n        visited.append(p)        \n        N = []  \n# N为p的epsilon邻域中的对象的集合\n\n        \nfor\n i \nin\n range(\nnum\n):\n            \nif\n (dist(Data[i], Data[p]) <= Eps):  \n# and (i!=p):\n\n                N.append(i)\n        \n# 如果p的epsilon邻域中的对象数大于指定阈值，说明p是一个核心对象\n\n        \nif\n \nlen\n(N) >= MinPts:\n            k = k + \n1\n\n            C[p] = k\n            \n# 对于p的epsilon邻域中的每个对象pi\n\n            \nfor\n \npi\n \nin\n N:\n                \nif\n \npi\n \nin\n unvisited:\n                    unvisited.remove(\npi\n)\n                    visited.append(\npi\n)\n                    \n# 找到pi的邻域中的核心对象，将这些对象放入N中\n\n                    \n# M是位于pi的邻域中的点的列表\n\n                    M = []\n                    \nfor\n j \nin\n range(\nnum\n):\n                        \nif\n (dist(Data[j], Data[\npi\n]) <= Eps):  \n                            M.append(j)\n                    \nif\n \nlen\n(M) >= MinPts:\n                        \nfor\n t \nin\n M:\n                            \nif\n t \nnot\n \nin\n N:\n                                N.append(t)\n                \n# 若pi不属于任何簇，C[pi] == -1说明C中第pi个值没有改动\n\n                \nif\n C[\npi\n] == \n-1\n:\n                    C[\npi\n] = k\n        \n# 如果p的epsilon邻域中的对象数小于指定阈值，说明p是一个噪声点\n\n        \nelse\n:\n            C[p] = \n-1\n\n    \nreturn\n C\n\n\nif\n __name__ == \n'__main__'\n:\n    \n# 数据集二：788个点\n\n\n    dataSet =d\n    C = dbscan(dataSet,\n0.3\n,\n4\n)\n    x, y = [], []\n    \nfor\n data \nin\n dataSet:\n        x.append(data[\n0\n])\n        y.append(data[\n1\n])\n    plt.figure(figsize=(\n8\n, \n6\n), dpi=\n70\n)\n    plt.scatter(x, y, c=C, marker=\n'o'\n)\n    plt.show()\n\n\n\n\n\n运行结果及详细报错内容\n\n\n\n\n我的解答思路和尝试过的方法，不写自己思路的，回答率下降 60%\n\n\n我用过这个代码，可以运行，但换了一个文件中的数据，就有错误", "Tag": "算法分析"}
{"Answer": "检查输入数据是否正确：确保您的输入数据已经被正确地预处理和标准化，使其能够适应网络的要求。还要确保您的输入数据与您的问题域相匹配。\n检查网络结构：确认您的神经网络结构是否正确并满足您的问题要求。特别是在使用DDPG算法时，Actor网络通常采用全连接网络或卷积神经网络。您可以尝试增加或减少网络的深度和宽度，以看看是否有任何改善。\n检查超参数：确保您的优化器和学习率等超参数已正确设置，尝试使用其他优化器和学习率值，观察是否会对网络性能产生影响。\n检查目标函数和奖励函数：请确保您的目标函数和奖励函数与您的问题域相匹配，同时考虑使用不同的目标函数和奖励函数来比较其性能。\n调试代码：检查您的代码是否存在错误，可能有语法错误或者实现有误。\n如果以上方法都没有解决问题，您可能需要深入分析问题所在，比如可视化激活层的输出和权重，来查找问题的根源。\n至于您提供的Actor神经网络的模型，我发现它使用了ReLU激活函数，这是一个常用的激活函数，但在某些情况下可能会导致梯度消失的问题。您可以尝试使用其他激活函数，例如LeakyReLU或ELU，看看是否有任何改善。此外，您的输出层使用了tanh激活函数，这意味着输出值将始终在-1和1之间。", "Konwledge_Point": "应对NP完全问题", "Question": "神经网络输出多维向量的值都一样是什么问题呀\n最近在用DDPG算法解决无人机的轨迹优化问题 遇到了个问题——神经网络不管输入是什么 输出都一模一样 不变\n结果一直是（0.5 0.5 0.5 0.5）（输入数据维度是46 输出是4）\n研究了好多天了 减少神经元数量，改变优化器、损失函数种类 ，降低输入维度 都试过啦 还是不行 实在是不知道问题出在哪里啦 有知道的可以帮忙解决一下嘛 谢谢大家啦！\n\n\n这是actor神经网络的模型：\n\n\nclass\n \nActorNetwork\n(\nobject\n):\n    \n\"\"\"\n    Implements actor network\n    \"\"\"\n\n    \ndef\n \n__init__\n(\nself,sess,state_dim,action_dim,lr,tau\n):\n        self.sess = sess\n        K.set_session(sess)\n        K.set_learning_phase(\n1\n)\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.lr = lr\n        self.tau = tau\n        self.mainModel, self.mainModel_weights, self.mainModel_state = self._build_model()\n        self.targetModel, self.targetModel_weights, _ = self._build_model()\n        self.action_gradient = tf.placeholder(tf.float32, [\nNone\n, self.action_dim])\n        self.params_grad = tf.gradients(self.mainModel.output, self.mainModel_weights, -self.action_gradient)\n        grads = \nzip\n(self.params_grad, self.mainModel_weights)\n        self.optimize = tf.train.AdamOptimizer(self.lr).apply_gradients(grads)\n        self.sess.run(tf.global_variables_initializer())\n \n    \ndef\n \n_build_model\n(\nself\n):\n        input_obs = Input(shape=(self.state_dim,))  \n# 输入层 返回一个维度为self.state_dim的张量\n\n        h = BatchNormalization()(input_obs)\n        h = Dense(\n400\n, kernel_initializer = \n'random_uniform'\n)(h)  \n# 全连接层 400个神经元（即该层的输出维度）\n\n        h = LeakyReLU(alpha=\n0.1\n)(h)\n        h = Dense(\n300\n, kernel_initializer = \n'random_uniform'\n)(h)  \n# 全连接层 输出维度300\n\n        h = LeakyReLU(alpha=\n0.1\n)(h)\n        h = Dense(self.action_dim, kernel_initializer = \n'random_uniform'\n)(h)  \n# 全连接层 输出维度self.action_dim\n\n        h = Activation(\n'tanh'\n)(h)  \n# softmax 改为 tanh\n\n        pred = Lambda(\nlambda\n h: (h+\n1\n)/\n2\n)(h)\n \n        \n# RelaxedOneHotCategorical() 函数的作用？？？ 网络最终输出结果的范围？？？\n\n        \n# pred = Lambda(lambda h: tf.contrib.distributions.RelaxedOneHotCategorical(0.5, probs=h).sample())(h)\n\n \n        \n# 给定输入张量和输出张量 生成一个函数型模型 这里包括一个输入层和3个全连接层\n\n        model = Model(inputs=input_obs, outputs=pred)\n \n        \n# 用于配置训练模型 优化器：Adam 损失函数：categorical_crossentropy\n\n        model.\ncompile\n(optimizer=\n'Adam'\n(), loss=\n'categorical_crossentropy'\n)\n \n        \nreturn\n model, model.trainable_weights, input_obs\n \n    \ndef\n \nact\n(\nself, state\n):\n        act = self.mainModel.predict(state)\n        \nreturn\n act\n\n\n\n\n这是使用该神经网络获取动作值的部分：\n\n\n\n```python\n        \nfor\n stp \nin\n range(int(args[\n'max_episode_len'\n])):\n            \na\n = []\n            \nfor\n i \nin\n range(env.num_UAVs):\n                actor = actors[i]\n                \na\n.append(actor.act(np.reshape(s[i], (\n-1\n, actor.state_dim))).reshape(actor.action_dim, ))  \n# 输入状态 输出动作\n\n                \n# reshape 在不改变数据内容的情况下，改变一个数组的格式\n\n                \n# (-1,actor.state_dim) 表示将智能体i的状态信息转化为列数为actor.state_dim的矩阵 行数自适应\n\n                \n# 输入到actor网络的输出结果（动作）再reshape为行数为actor.action_dim的矩阵\n\n \n            \nfor\n i \nin\n range(env.num_UAVs):\n                \n# 增加探索扰动, 输出限制在 [0, 1] 范围内\n\n                \na\n[i] = np.clip(np.\nrandom\n.\nnormal\n(\na\n[i], NOISE), \n0\n, \n1\n)\n \n            s2, r, done = env.step(\na\n)\n            replayMemory.\nadd\n(s, \na\n, r, done, s2)\n            s = s2\n\n\n\n\n```", "Tag": "算法分析"}
{"Answer": "代码在哪个路径下wb.save('data.xlsx')  就保存在哪个路径", "Konwledge_Point": "应对NP完全问题", "Question": "运行不报错但是也没有结果，能找找问题吗\n\n\nimport\n glob  # 引用glob\n\nimport\n numpy \nas\n np  # 引用numpy\n\nimport\n openpyxl\n\nfrom\n openpyxl \nimport\n load_workbook  # 引用openpyxl的load_workbook\n\nflist = glob.glob(\n'G:\\\\data2py\\\\*.txt'\n)  # 读取当前文件夹所有txt，并存入列表\nwb = load_workbook(\n'G:\\\\data2py\\\\data.xlsx'\n)  # 打开要保存数据的excel\nsheet = wb[\n'Sheet1'\n]  # 打开要保存数据的sheet\nj = \n1\n  # 序数，用来将从txt提取的数据存储（放）到excel的不同行\n\nfor filename \nin\n flist:  # 利用for循环逐个读取txt文件\n    array = np.loadtxt(filename, dtype=str, delimiter=\n'\\t'\n)  # 将当前读取的txt文件数据存储矩阵，定界符为‘\\t’\n    number_col = array.shape[\n1\n]  # 获取数据矩阵列数\n    for i \nin\n range(number_col):\n        sheet.cell(j, i+\n1\n).value = array[\n1\n][i]  # 将需要用的第一行数据存储在excel中,就是\n'sheet1'\n\n    j = j + \n1\n    #行叠加 ，开始第二行        #             #行不变，列要变化（i+\n1\n）\n\nwb.save(\n'data.xlsx'\n)  # 保存excel文件并退出\n\n\n", "Tag": "算法分析"}
{"Answer": "202-03-07 08:00   是2020", "Konwledge_Point": "应对NP完全问题", "Question": "python的numpy库，关于时间增量timedelta64\n\n\nimport numpy as np\n\na = np.datetime64('2020-03-08') - np.datetime64('2020-03-07')\nb = np.datetime64('2020-03-08') - np.datetime64('202-03-07 08:00')\nc = np.datetime64('2020-03-08') - np.datetime64('2020-03-07 23:00', 'D')\n\nprint(a, a.dtype)  # 1 days timedelta64[D]\nprint(b, b.dtype)  # 956178240 minutes timedelta64[m]\nprint(c, c.dtype)  # 1 days timedelta64[D]\n\n\n\n为什么，b输出的会是956178240分钟呢？\n\n\n\n明明2020-03-08 00:00与2020-03-07 08:00相差了16个小时，\n\n\n\n怎么得到的是956178240分钟呢？\n\n\n", "Tag": "算法分析"}
{"Answer": "\n\nfor i in range(1, 25 + 1):\n    print(i, end='\\t')\n    if i % 5 == 0:\n        print(\"\\n\")\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python如何创建如何创建1到25的数组？要是5X5形式\n如何创建1到25的数组？要是5X5形式，不是只一排\n要有规律1,2,3,4到25，不要用np.random.randn(),不要用np.random.randn()", "Tag": "算法分析"}
{"Answer": "可以先导入所需的库：\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n然后可以定义一个函数来实现序贯概率比检验 (SPRT) 算法。这个函数接受一组观察数据和两个阈值 $\\alpha$ 和 $\\beta$ 作为参数，并返回是否应该拒绝假设。\ndef sprt(data, alpha, beta):\n    n = len(data)\n    p_hat = data.mean()\n    z = p_hat / (1 - p_hat)\n    a = (1 - beta) / alpha\n    b = beta / (1 - alpha)\n    for i in range(n):\n        z *= data[i] / (1 - data[i])\n        if z < a:\n            return \"Reject H0\"\n        elif z > b:\n            return \"Reject H1\"\n    return \"Fail to reject\"\n\n然后可以使用蒙特卡罗方法来模拟数据生成过程。假设已经实现了一个函数来计算几何分布的概率密度函数（pdf）和累积分布函数（cdf），这里就不再赘述。\n接下来，可以定义一个函数来生成符合几何分布的观察数据。这个函数接受几何分布的唯一参数 $\\theta$ 和要生成的数据点数量 $n$ 作为参数，并返回一组符合几何分布的观察数据。\ndef generate_data(theta, n):\n    p = 1 - theta\n    data = np.random.geometric(p, n)\n    return data\n\n然后可以使用这个函数来生成大量的符合几何分布的观察数据，并使用 SPRT 算法对这些数据进行检验。可以记录每次检验的结果，并统计错误类型 I 和 II 的次数。\n# 设置参数\ntheta = 0.5\nalpha = 0.05\nbeta = 0.1\nn_trials = 10000\n\n# 记录错误类型 I 和 II 的次数\nerror_I = 0\nerror_II = 0\n\n# 运行多次模拟\nfor i in range(n_trials):\n    # 生成符合几何分布的观察数据\n    data = generate_data(theta, n)\n    # 进行序贯概率比检验\n    result = sprt(data, alpha, beta)\n    if result == \"Reject H0\":\n        error_I += 1\n    elif result == \"Reject H1\":\n        error_II += 1\n\n# 计算错误概率\nerror_I_prob = error_I / n_trials\nerror_II_prob = error_II / n_trials\n\n可以使用计算出的错误概率来估计条件预期样本大小。可以通过计算错误概率与假设接近度之间的依赖关系来估计条件预期样本大小。这里假设已经定义了一个函数 expected_sample_size 来计算条件预期样本大小。\n# 计算条件预期样本大小\nn_0 = expected_sample_size(error_I_prob, alpha)\nn_1 = expected_sample_size(error_II_prob, beta)\n\n望采纳。", "Konwledge_Point": "应对NP完全问题", "Question": "使用蒙特卡洛方法对几何分布进行序贯概率比检验 (SPRT)\n问题遇到的现象和发生背景\n\n\n在 2 个复合假设的情况下，用计算机实施广义顺序测试，\n使用蒙特卡洛方法估计其错误类型 I 和 II 的概率，以及条件预期样本量。\n\n\n实现：几何分布，$\\theta$ 是它唯一的参数。\n\n\n分析错误概率和“假设接近度”和“大小”的预期样本大小的依赖关系。\n将 $\\alpha$ 和 $\\beta$ 用于您选择的阈值计算。\n\n\n我的解答思路和尝试过的方法，不写自己思路的，回答率下降 60%\n\n\n我找到的方法代码如下，但是仅仅只是实现几何分布的蒙特卡洛方法，不知道该怎样实现使用蒙特卡洛方法对几何分布进行序贯概率比检验 (SPRT)。\n\n\nimport\n numpy as np\n\nimport\n matplotlib.pyplot as plt\n\n\nnp\n.random.seed(\n222\n)\n\n\n# 把计算得到的函数写成一个函数\n\n\ndef\n distribution_z(z, p, max_k=\n200\n):\n    \nimport\n math\n    \nj\n = int(math.floor(z))\n    \nA\n = \n0\n\n    \nfor\n m in range(\n1\n, j + \n1\n):\n        \nA\n += (\n1\n - p) ** (m - \n1\n)\n    \nA\n *= p\n\n    \nB\n = \n0\n\n    \nfor\n k in range(j + \n1\n, max_k + \n1\n):\n        \na\n = (\n1\n - p) ** (k - \n1\n)\n        \na\n /= k\n        \nB\n += a\n    \nB\n *= z * p\n\n    \nreturn\n A + B\n\n\n\ndef\n pdf_z(z, p, max_k=\n200\n):\n    \nimport\n math\n    \nj\n = int(math.floor(z))\n    \nB\n = \n0\n\n    \nfor\n k in range(j + \n1\n, max_k + \n1\n):\n        \na\n = (\n1\n - p) ** (k - \n1\n)\n        \na\n /= k\n        \nB\n += a\n    \nreturn\n B * p\n\n\n\np\n = \n0\n.\n1\n\n\n# 选取数据点，点越多越精确\n\n\ndataPoints\n = \n10000\n\n\n\nUnit\n = np.random.rand(dataPoints)\n\nGeom\n = np.random.geometric(p, dataPoints)\n\ndistri_of_Monte\n = Geom * Unit\n\n\n# 概率密度函数 PDF\n\n\nplt\n.hist(distri_of_Monte, bins=\n40\n, range=(\n0\n, \n40\n))\n\npoints_of_z\n = np.arange(\n0\n, \n41\n, \n0\n.\n01\n)\n\npdf_of_z\n = np.array([pdf_z(zi, p) for zi in points_of_z]) * dataPoints\n\nplt\n.plot(points_of_z, pdf_of_z)\n\n# print(pdf_of_z)\n\n\nplt\n.show()\n\n\nhist\n, bin_edges = np.histogram(distri_of_Monte, bins=\n40\n, range=(\n0\n, \n40\n))\n\n\n# 概率分布函数 CDF\n\n\nhist_list\n = np.cumsum(hist) / dataPoints\n\n\nplt\n.plot(bin_edges[\n1\n:], hist_list)\n\n\npoints_of_z\n = np.arange(\n1\n, \n41\n, \n0\n.\n1\n)\n\ndistri_of_z\n =\n [distribution_z(zi, p) for zi in points_of_z]\n\n\n\nplt\n.plot(points_of_z, distri_of_z)\n\n\nplt\n.show()\n\n\n\n\n\nimport\n sprt as sprt\n\nimport\n numpy as np\n\n\n# Null value\n\n\nh0\n = \n0.5\n\n\n# Alternative value\n\n\nh1\n = \n0.55\n\n\n# Type I error rate = 0.05\n\n\nalpha\n = \n0.05\n\n\n# Type II error rate = 0.2\n\n\nbeta\n = \n0.2\n\n\n# Values\n\n\nvalues\n = np.random.binomial(\n1\n, \n0.55\n, \n100\n)\n\ntest\n = sprt.SPRTBinomial(\nh0\n = h0, \nh1\n = h1, \nalpha\n = alpha, \nbeta\n = beta, \nvalues\n = values)\n\ntest.plot()\n\n\n# Plot the data and boundary but without fill the color\n\ntest.plot(\nfill\n = False)\n\n\n\n\n\n\n\n\n我想要达到的结果，如果你需要快速回答，请尝试 “付费悬赏”\n\n\n使用蒙特卡洛方法，对几何分布进行序贯概率比检验 (SPRT)，同时满足题目的要求。使用Python编写代码。", "Tag": "算法分析"}
{"Answer": "\nimport numpy as np\n \nX0 = np.arange(-1, 101, 1.0)  # 炮检距（中心点即目标点），-1-100\nX = np.zeros(90)  # 射线经过的水平距离\nag = np.zeros((5, 101))  # 找到适合水平距离的粗始入射角变量，5行101列适用于4层100个炮检距\nagi = np.arange(0, 90, 1.0)  # 设置入射角数组，1度为步长，0-89\na = np.zeros((5, 101))  # 反射点处的入射角，5行101列适用于4层100个炮检距\nVp = [0, 3700, 2400, 3000, 2000]  # 层状速度数组\nH = [0, 60, 20, 40, 20]  # 层厚度数组\n \ndef angle_function(m, n):  # 定义子函数，计算层数为n、炮检距为m的入射角\n    for j in range(90):\n        for i in range(m + 1):\n            P = np.sin(agi[j] * np.pi / 180) / Vp[1]\n            # print(P*Vp[i], 1 - (P * Vp[i]) ** 2)\n            X[j] += 2 * H[i] * P * Vp[i] / np.sqrt(1 - (P * Vp[i]) ** 2)\n        if X[j - 1] < X0[n] < X[j]:  # 适合水平距离时停止增加入射角角度\n            for l in range(50):  # 二分法精确化入射角\n                ag[m][n] = (agi[j - 1] + agi[j]) / 2.0\n                for k in range(m + 1):\n                    P = np.sin(ag[m][1] * np.pi / 180) / Vp[1]\n                    X[l] += 2 * H[k] * P * Vp[k] / np.sqrt(1 - (P * Vp[k]) ** 2)\n                if abs(X[l] - X0[n]) > 0.5:\n                    if X[l] > X0[n]:\n                        agi[j - 1] = agi[j - 1]\n                        agi[j] = ag[m][n]\n                    if X[l] < X0[n]:\n                        agi[j - 1] = ag[m][n]\n                        agi[j] = agi[j]\n                else:\n                    return ag\n            return ag\n        \n\n# 循环调用子函数\nfor i in range(100,92,-1):\n    ag = angle_function(4, i)\nprint(ag)\n把你的break换成return ag，这样有返回值，就可以在上一步的基础上进行继续迭代运算，这里调用\n\nfor i in range(100,92,-1):\n    ag = angle_function(4, i)\nprint(ag)\n等同于你的\n\nangle_function(4, 100)\nangle_function(4, 99)\nangle_function(4, 98)\nangle_function(4, 97)\nangle_function(4, 96)\nangle_function(4, 95)\nangle_function(4, 94)\nangle_function(4, 93)\n\nprint(ag)\n你可以换成你之前那个循环就可以实现了", "Konwledge_Point": "应对NP完全问题", "Question": "请问大佬们，Python自定义的函数不可以循环调用吗o(╥﹏╥)o\n\n\nimport numpy as np\n\nX0 = np.arange(-1, 101, 1.0)  # 炮检距（中心点即目标点），-1-100\nX = np.zeros(90)  # 射线经过的水平距离\nag = np.zeros((5, 101))  # 找到适合水平距离的粗始入射角变量，5行101列适用于4层100个炮检距\nagi = np.arange(0, 90, 1.0)  # 设置入射角数组，1度为步长，0-89\na = np.zeros((5, 101))  # 反射点处的入射角，5行101列适用于4层100个炮检距\nVp = [0, 3700, 2400, 3000, 2000]  # 层状速度数组\nH = [0, 60, 20, 40, 20]  # 层厚度数组\n\n\ndef angle_function(m, n):  # 定义子函数，计算层数为n、炮检距为m的入射角\n    for j in range(90):\n        for i in range(m + 1):\n            P = np.sin(agi[j] * np.pi / 180) / Vp[1]\n            # print(P*Vp[i], 1 - (P * Vp[i]) ** 2)\n            X[j] += 2 * H[i] * P * Vp[i] / np.sqrt(1 - (P * Vp[i]) ** 2)\n        if X[j - 1] < X0[n] < X[j]:  # 适合水平距离时停止增加入射角角度\n            for l in range(50):  # 二分法精确化入射角\n                ag[m][n] = (agi[j - 1] + agi[j]) / 2.0\n                for k in range(m + 1):\n                    P = np.sin(ag[m][1] * np.pi / 180) / Vp[1]\n                    X[l] += 2 * H[k] * P * Vp[k] / np.sqrt(1 - (P * Vp[k]) ** 2)\n                if abs(X[l] - X0[n]) > 0.5:\n                    if X[l] > X0[n]:\n                        agi[j - 1] = agi[j - 1]\n                        agi[j] = ag[m][n]\n                    if X[l] < X0[n]:\n                        agi[j - 1] = ag[m][n]\n                        agi[j] = agi[j]\n                else:\n                    break\n            break\n\n# 循环调用子函数\nfor p in range(5):\n    for q in range(101):\n        angle_function(p, q)\nprint(ag)\n\n\n\n如上，循环调用子函数，结果大部分为零\n\n\n\n\n\nimport numpy as np\n\nX0 = np.arange(-1, 101, 1.0)  # 炮检距（中心点即目标点），-1-100\nX = np.zeros(90)  # 射线经过的水平距离\nag = np.zeros((5, 101))  # 找到适合水平距离的粗始入射角变量，5行101列适用于4层100个炮检距\nagi = np.arange(0, 90, 1.0)  # 设置入射角数组，1度为步长，0-89\na = np.zeros((5, 101))  # 反射点处的入射角，5行101列适用于4层100个炮检距\nVp = [0, 3700, 2400, 3000, 2000]  # 层状速度数组\nH = [0, 60, 20, 40, 20]  # 层厚度数组\n\n\ndef angle_function(m, n):  # 定义子函数，计算层数为n、炮检距为m的入射角\n    for j in range(90):\n        for i in range(m + 1):\n            P = np.sin(agi[j] * np.pi / 180) / Vp[1]\n            # print(P*Vp[i], 1 - (P * Vp[i]) ** 2)\n            X[j] += 2 * H[i] * P * Vp[i] / np.sqrt(1 - (P * Vp[i]) ** 2)\n        if X[j - 1] < X0[n] < X[j]:  # 适合水平距离时停止增加入射角角度\n            for l in range(50):  # 二分法精确化入射角\n                ag[m][n] = (agi[j - 1] + agi[j]) / 2.0\n                for k in range(m + 1):\n                    P = np.sin(ag[m][1] * np.pi / 180) / Vp[1]\n                    X[l] += 2 * H[k] * P * Vp[k] / np.sqrt(1 - (P * Vp[k]) ** 2)\n                if abs(X[l] - X0[n]) > 0.5:\n                    if X[l] > X0[n]:\n                        agi[j - 1] = agi[j - 1]\n                        agi[j] = ag[m][n]\n                    if X[l] < X0[n]:\n                        agi[j - 1] = ag[m][n]\n                        agi[j] = agi[j]\n                else:\n                    break\n            break\n\n\nangle_function(4, 100)\nangle_function(4, 99)\nangle_function(4, 98)\nangle_function(4, 97)\nangle_function(4, 96)\nangle_function(4, 95)\nangle_function(4, 94)\nangle_function(4, 93)\n\nprint(ag)\n\n\n\n这样一行一行调用，就能正常计算出ag。难道真的要一行一行的调用吗o(╥﹏╥)oo(╥﹏╥)o", "Tag": "算法分析"}
{"Answer": "请看👉 ：推荐系统之LFM算法详解", "Konwledge_Point": "应对NP完全问题", "Question": "LFM（隐语义模型）算法的实现\n做推荐系统的，下面是部分数据集\n\n\ndtype=[(\n'userId'\n,np.int32),(\n'movieId'\n,np.int32),(\n'rating'\n,np.float32)]\ndataset=pd.read_csv(DATA_PATH,usecols=\nrange\n(\n3\n),dtype=dtype) \n# 读取csv文件\n\nusers_ratings=dataset.groupby(\n'userId'\n).agg([\nlist\n])\nitems_ratings=dataset.groupby(\n'movieId'\n).agg([\nlist\n])\n\n# User-LF  10 代表 隐含因子个数是10个\n\nP = \ndict\n(\nzip\n(users_ratings.index,np.random.rand(\nlen\n(users_ratings),\n10\n).astype(np.float32)\n        ))\n\n# Item-LF\n\nQ = \ndict\n(\nzip\n(items_ratings.index,np.random.rand(\nlen\n(items_ratings),\n10\n).astype(np.float32)\n        ))\n\n\n# 梯度下降优化损失函数\n\n\nfor\n i \nin\n \nrange\n(\n15\n):\n        \nprint\n(\n'*'\n * \n10\n, i)\n        \nfor\n uid, iid, real_rating \nin\n dataset.itertuples(index=\nFalse\n):\n                \n# 遍历 用户 物品的评分数据 通过用户的id 到用户矩阵中获取用户向量\n\n                v_puk = P[uid]\n                \n# 通过物品的uid 到物品矩阵里获取物品向量\n\n                v_qik = Q[iid]\n                \n# 计算损失\n\n                error = real_rating - np.dot(v_puk, v_qik)\n                \n# 0.02学习率 0.01正则化系数\n\n                v_puk += \n0.02\n * (error * v_qik - \n0.01\n * v_puk)\n                v_qik += \n0.02\n * (error * v_puk - \n0.01\n * v_qik)\n\n                P[uid] = v_puk\n                Q[iid] = v_qik\n\n\n\nv_puk += 0.02 * (error * v_qik - 0.01 * v_puk)  主要是这个向量的更新我不明白，为啥这么更新的", "Tag": "算法分析"}
{"Answer": "你好，我是有问必答小助手，非常抱歉，本次您提出的有问必答问题，技术专家团超时未为您做出解答 本次提问扣除的有问必答次数，已经为您补发到账户，我们后续会持续优化，扩大我们的服务范围，为您带来更好地服务。", "Konwledge_Point": "应对NP完全问题", "Question": "计算机视觉的卷积及Dr的代码，上课没有听懂，需要超级详细讲解\n问题遇到的现象和发生背景\n\n\nPractical task 1: Developing a correlation filter.\nDeveloping a correlation filter.\n\n\nIt is necessary to develop a function - a filter based on correlation. without using ready-made implementations and using only numpy library functions.\n\n\nIt is necessary to use the following types of kernels for filters with normalization:\n\n\n1 1 1 1 1       2) -1 -1 -1 -1 -1     3) -1 -1 -1 -1 -1\n\n\n 1 1 1 1 1            0   0  0   0  0         0   0  1   0  0\n\n\n 1 1 1 1 1             1  1   1   1  1         1  1   1  1   1\n\n\n\n\nFor test Image please use picture boat1_resize.jpg.\n\n\nPlese, Save your Result as the processed image and code to the report file and subscribe it to answer of this task\n\n\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport numpy as np\nimport cv2 as cv\nimg=cv\n.imread\n(\n\"D:/boat1_resize.jpg\"\n,\n0\n)\ncv\n.imshow\n(\n\"1\"\n,img)\ncv\n.waitKey\n(\n0\n)\nkernel1=np\n.array\n(\n[[1,1,1]\n,                   \n                   \n[0,0,0]\n,\n                   \n[-1,-1,-1]\n])\nimg=cv\n.copyMakeBorder\n(\nimg\n,\n1\n,\n1\n,\n2\n,\n2\n,cv.BORDER_REPLICATE)\nimg2=\nimg\n.copy\n()\n\nfor\n \ni\n \nin\n range(\n0\n,\n321\n,\n1\n):\n    \nfor\n j \nin\n range(\n0\n,\n492\n,\n1\n):\n        a=np\n.abs\n(np\n.sum\n(np\n.multiply\n(\nimg\n[i:i+3,j:j+3]\n,kernel1)))\n        img2\n[i+1,j+2]\n=\na\n\nimg=\nimg\n[1:323,2:495]\n\ncv\n.imshow\n(\n\"1\"\n,img2)\ncv\n.waitKey\n(\n0\n)\ncv\n.destroyAllWindows\n()\n\n\n\n\n运行结果及报错内容\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "# grab方法返回的就是Image对象 \n# 直接传入 grab方法返回值 我这里不会出错\n>>> from PIL import ImageGrab as ig,Image\n>>> import pytesseract\n>>> img = ig.grab(bbox=(0,0,100,100))\n>>> img.show()\n>>> content = pytesseract.image_to_string(img)\n>>> print(content)\n*Python 3.\nEile Edit.\n\n是不是 pytesseract 的安装和配置问题？参考", "Konwledge_Point": "应对NP完全问题", "Question": "Image.open（img）必须是本地文件吗？\n问题遇到的现象和发生背景\n\n\n想通过Image.grab（）函数抓取截图然后用pytesseract.image_to_string()识别图中内容，如果要将抓取的图片保存到本地再打开的话将大大降低效率。\n\n\n问题相关代码，请勿粘贴截图\n\n\nfrom\n PIL \nimport\n ImageGrab \nas\n ig,Image\n\nimport\n numpy \nas\n np\n\nimport\n pytesseract\n\nimg = ig.grab()\nimage = Image.\nopen\n(img)\ncontent = pytesseract.image_to_string(image)   \n# 识别图片\n\n\nprint\n(content)\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n尝试过直接使用image.grab()抓取的结果传入报错，转换成np.array(img)数组形式也报错\n\n\n我想要达到的结果\n\n\n如何不保存直接传入image对象进行识别？求解释一下原因", "Tag": "算法分析"}
{"Answer": "用进程池吧，还能控制并发量，多香\nfrom concurrent.futures import ProcessPoolExecutor, wait\n\n\ndef run1(num):\n    return num, num - 1\n\n\ndef main():\n    pool = ProcessPoolExecutor(max_workers=10)\n    process = []\n    for i in range(10):\n        p = pool.submit(run1, i)\n        process.append(p)\n    wait(process)\n    for p in process:\n        r1, r2 = p.result()\n        print(r1, r2)\n\n\nif __name__ == '__main__':\n    main()\n\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "如何获取进程的返回值\n问题遇到的现象和发生背景\n\n\n我想使用多进程来处理并行工作，但是不知道如何获取进程的返回值。\n我需要保存这些返回值用于后续的处理。\n\n\n问题相关代码，请勿粘贴截图\n\n\nfrom\n multiprocessing import Process\n\ndef run1(N):\n    trajlen1 = np.\nempty\n(shape=(\n0\n, \n1\n))\n    etaODAVG1 = np.\nempty\n(shape=(\n0\n, \n1\n))\n    for i in N:\n        trajOD, etaODavg = \ntraj_judge\n(link_start[i], link_start5, link_end[i], link_end5, eta2)\n        if \nlen\n(trajOD) == \n1\n:\n            etaODavg1 = simple_eta1[i]\n        else:\n            etaODavg1 = etaODavg\n        etaODAVG1=np.\nappend\n(etaODAVG1, [[etaODavg1]], axis=\n0\n)\n        trajlen1=np.\nappend\n(trajlen1, [[\nlen\n(trajOD)]], axis=\n0\n)\n        \nprint\n(\n'write:'\n, i, \n'trajlen:'\n,\nlen\n(trajOD),\n'etaODAVG'\n,etaODavg1,\n'simpale_eta:'\n, simple_eta1[i], \n'eta1:'\n, eta1[i])\n    return etaODAVG1,trajlen1\n\ndef \nmain\n():\n    p1=\nProcess\n(target=run1,args=(l2[\n0\n],))\n    p1.\nstart\n()\n    p1.\njoin\n()\n    \nif __name__ == \n'__main__'\n:\n    \nmain\n()\n\n\n\n\n我想要达到的结果\n\n\n我需要在main（）中获取并保存etaODAVG1,trajlen1这两个返回值。", "Tag": "算法分析"}
{"Answer": "可以使用pandas的apply函数来实现对整个DataFrame的操作。\n定义一个函数extract_numbers，用于提取字符串中的数字，然后使用apply函数将这个函数应用于DataFrame的每一行，最后将提取出的数字存到一个新的DataFrame中。实现如下：\nimport re\nimport numpy as np\nimport pandas as pd\n\ndef extract_numbers(s):\n    \"\"\"提取字符串中的数字\"\"\"\n    return list(map(float, re.findall(r'\\d+.\\d+', str(s))))\n\ndf0 = pd.DataFrame([['x=6.2', 'y=6.3', 'z=6.7'], ['x=7.2', 'x=8.3','x=9.5']])\ndf1 = df0.apply(extract_numbers, axis=1)\ndf1_np = np.array(df1.to_list())\n\nprint(df1_np)\n\n输出\n[[6.2 6.3 6.7]\n [7.2 8.3 9.5]]\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "使用pandas快速提取表中的数字\n有一个类似这样的pandas表：\n\n\ndf0 = pd.\nDataFrame\n([[\n'x=6.2'\n, \n'y=6.3'\n, \n'z=6.7'\n], [\n'x=7.2'\n, \n'x=8.3'\n,\n'x=9.5'\n]])\n\n\n\n希望获得其中的所有数字，并存到numpy中。目前所使用的方法如下：\n\n\ndf0_np = np\n.zeros\n(\n[df0.shape[0]\n, \n3\n])\n\nfor\n \ni\n \nin\n range(df0_np\n.shape\n[0]\n):\n    df0_np\n[i, :]\n = df0\n.iloc\n[i, :]\n.str\n.extract\n(r\n'(\\d+.\\d+)'\n)\n.transpose\n()\n\nprint\n(df0_np)\n\n\n\n\n得到结果如下：\n\n\n[[6.2 6.3 6.7]\n [7.2 8.3 9.5]]\n\n\n\n\n由于str.extract()方法只能应用于series，不知道有什么更快捷的方法能够一次性应用于所有的dataframe，因此采用循环的方法解决。希望能够直接应用于dataframe对象获得最终的numpy数组，感谢。", "Tag": "算法分析"}
{"Answer": "那试试这样:\n\nimport pandas as pd\nimport numpy as np\n\nA = np.arange(10,100,10)\nB = np.arange(0.1,1,0.1)\nC = np.arange(2,16,2)\n\ni_re = 10\nj_re = 0.1\nk_re = 2\nv_re = -9.8\n\nfor i in A:\n    for j in B:\n        for k in C:\n            V = j*k-i\n            if V>v_re:\n                i_re,j_re,k_re,v_re = i,j,k,V\n                \nprint(i_re,j_re,k_re,v_re)    \n ", "Konwledge_Point": "应对NP完全问题", "Question": "如何找到for循环中的参数\n\nA = np.arange(10,100,10)\nB = np.arange(0.1,1,0.1)\nC = np.arange(2,16,2)\nfor i in A:\n    for j in B:\n        for k in C:\n            V = j*k-i\n如何找到V的最大值很简单但如何找到得到最大值时的ijk分别是多少 求大神告知", "Tag": "算法分析"}
{"Answer": "你的data是想要表示什么内容呢？w是一个小数，year是一个列表，这两者不能相乘，只能通过列表推导式把列表的每个元素和w相乘，再生成一个列表：data=[wi for i in year]但我觉得逻辑上好像应该把列表放在循环里，把每个wyear的值保存起来，所以先描述一下你的需求\ndata2=[]\nfor i in range(10):\n    # 代码省略\n    data2.append(w*year[i])\n", "Konwledge_Point": "应对NP完全问题", "Question": "python代码问题\n\n\nimport\n matplotlib.pyplot as plt\n\nimport\n numpy as np\n\n\nplt\n.title(\n\"year-olders Function\"\n,fontsize=\n12\n)\n\nplt\n.xlabel(\n\"year\"\n)\n\nplt\n.ylabel(\n\"olders\"\n)\n\n\nyear\n=[\n2000\n,\n2001\n,\n2002\n,\n2003\n,\n2004\n,\n2005\n,\n2006\n,\n2007\n,\n2008\n,\n2009\n]\n\nolder\n=[\n88274022\n,\n90615170\n,\n93770690\n,\n96920250\n,\n98790880\n,\n100682120\n,\n103843920\n,\n107024490\n,\n110225660\n,\n113432500\n]\n\nplt\n.scatter(year,older)\n\nw\n=\n0\n.\n5\n\n\nfor\n i in range(\n10\n):\n    \nx\n=year[i] #x=\n1\n\n    \ny\n=older[i] #y=\n0\n.\n89\n\n    \ndata\n=w*x #y_pre=\n0\n.\n5\n*\n1\n=\n0\n.\n5\n\n    \ne\n=y-data #e=\n0\n.\n39\n\n    \nalpha\n=\n0\n.\n05\n\n    \nw\n=w+alpha*e*x #w=\n0\n.\n5\n+\n0\n.\n05\n*\n0\n.\n39\n*\n1\n=\n0\n.\n5195\n\n\ndata\n=w*year\n\nplt\n.plot(year,data)\n\nplt\n.show()\n\n\n\n这段代码是哪里出错了呢，”data=w*year“这一行一直在报TypeError: can't multiply sequence by non-int of type 'float'错误，请赐教。", "Tag": "算法分析"}
{"Answer": "该回答引用GPTᴼᴾᴱᴺᴬᴵ\n从您提供的代码看，图片已经被读入，reshape 成为 (n_samples, n_features) 的二维数组 Z，再转化为一维数组 whw_df。但是在这里，一维数组 whw_df 中的每个元素又被包装成一个列表，这一步看上去没有必要。因此，可以直接把 whw_df 的元素改成三个维度，即 whw_df = whw_df.reshape(-1, 1, 3)。\n此外，聚类算法在图像分割中通常使用 RGB 颜色空间，而不是将每个像素的 R、G、B 三个分量分开考虑。因此，将 Z 按行展平为一维数组后，应该将其转化为一个二维数组，即 Z = img.reshape((-1, 3))。\n修改后的代码如下：\nimg = cv2.imread('/content/drive/MyDrive/no-fog vs fog/8_8_1.jpg')\nZ = img.reshape((-1, 3))\nwhw_df = np.float32(Z).reshape(-1, 1, 3)\n\n# 利用SSE选择k\nwhw_SSE = []  # 存放每次结果的误差平方和\nfor i in range(2, 8):  # 尝试要聚成的类数\n    whw_estimator = KMeans(n_clusters=i)  # 构造聚类器\n    whw_estimator.fit(whw_df)  # 存入数组（数据的列名标签）\n    whw_SSE.append(whw_estimator.inertia_)\n\nX = range(2, 8)  # 跟k值要一样\nfig = plt.figure(figsize=(8,6))\nplt.xlabel('聚类数目')\nplt.ylabel('SSE')\nplt.plot(X, whw_SSE,'o-')\nplt.title(\"K值在肘关节处最优\")\nplt.show()  # 画出图\n\n希望这能解决您的问题。", "Konwledge_Point": "应对NP完全问题", "Question": "想用肘关节法看图片最优聚类  输入维度有问题怎么解决？\n\n\nimg=cv2.imread('/content/drive/MyDrive/no-fog\n vs fog/\n8\n_8_1.jpg')\n\nZ\n = img.reshape((-\n1\n, \n3\n))\n\nZ\n = np.array(Z).reshape(\n1\n, -\n1\n)\n\nwhw_df\n = np.float32(Z)\n\nwhw_df\n = [[i] for i \nin\n whw_df]\n\n##KMeans进行选取聚类数目\n\n\n#分别利用手肘法和轮廓系数对聚类的数目进行探索：\n\n\n#手肘法\n\n\n# 利用SSE选择k\n\n\nwhw_SSE\n = []  \n# 存放每次结果的误差平方和\n\nfor i \nin\n range(\n2\n, \n8\n):  \n# 尝试要聚成的类数\n\n    \nwhw_estimator\n = KMeans(\nn_clusters=i)\n  \n# 构造聚类器\n\n    whw_estimator.fit(whw_df) \n#存入数组（数据的列名标签）\n\n    whw_SSE.append(whw_estimator.inertia_)\n\nX\n = range(\n2\n, \n8\n)  \n# 跟k值要一样\n\n\nfig\n = plt.figure(\nfigsize=(8,6))\n\nplt.xlabel('聚类数目')\nplt.ylabel('SSE')\nplt.plot(X, whw_SSE,'o-' )\nplt.title(\n\"K值在肘关节处最优\"\n)\nplt.show()  \n# 画出图\n\n", "Tag": "算法分析"}
{"Answer": "模型的定义没有看到，另外你epochs=10是不是太小了，还没有效果。", "Konwledge_Point": "应对NP完全问题", "Question": "基于keras，使用imagedatagenerator.flow函数读入数据，训练集ACC极低\n在做字符识别的神经网络，数据集是用序号标好名称的图片，标签取图片的文件名。想用Imagedatagenrator\n\n函数和flow函数，增加样本的泛化性，然后生成数据传入网络，可是这样acc=1/类别数，基本为零。请问哪里出了问题\n\n\n\ndatagen = ImageDataGenerator(\n       width_shift_range=0.1,\n       height_shift_range=0.1\n       )\ndef read_train_image(self, name):\n       myimg = Image.open(name).convert('RGB')\n       return np.array(myimg)\n\ndef train(self):\n       #训练集\n       train_img_list = []\n       train_label_list = []\n       #测试集\n       test_img_list = []\n       test_label_list = []\n       for file in os.listdir('train'):\n           files_img_in_array = self.read_train_image(name='train/' + file)\n           train_img_list.append(files_img_in_array)  # Image list add up\n           train_label_list.append(int(file.split('_')[0]))  # lable list addup\n       for file in os.listdir('test'):\n            files_img_in_array = self.read_train_image(name='test/' + file)\n            test_img_list.append(files_img_in_array)  # Image list add up\n            test_label_list.append(int(file.split('_')[0]))  # lable list addup\n\n        train_img_list = np.array(train_img_list)\n        train_label_list = np.array(train_label_list)\n        test_img_list = np.array(train_img_list)\n        test_label_list = np.array(train_label_list)\n        train_label_list = np_utils.to_categorical(train_label_list, 5788)\n        test_label_list = np_utils.to_categorical(test_label_list, 5788)\n        train_img_list = train_img_list.astype('float32')\n        test_img_list = test_img_list.astype('float32')\n        test_img_list /= 255.0\n        train_img_list /= 255.0\n\n\n\n\n这是图片数据的处理，图片和标签都存到list里。下面是用fit_genrator训练\n\n\n\nmodel.fit_generator(\n            self.datagen.flow(x=train_img_list, y=train_label_list, batch_size=2),\n            samples_per_epoch=len(train_img_list),\n            epochs=10,\n            validation_data=(test_img_list,test_label_list),\n            )\n", "Tag": "算法分析"}
{"Answer": "http://t.csdn.cn/zj6sG", "Konwledge_Point": "应对NP完全问题", "Question": "出现cannot reshape array of size 27749792 into shape (300,1)\n\n \n# 转numpy数组,打乱顺序\n\n    \ndataSet\n = np.array(dataSet).reshape(-\n1\n, \n300\n)\n    \nlableSet\n = np.array(lableSet).reshape(-\n1\n, \n1\n)\n    \ntrain_ds\n = np.hstack((dataSet, lableSet))\n    \nnp\n.random.shuffle(train_ds)\n\n    \n# 数据集及其标签集\n\n    \nX\n = train_ds.reshape(-\n1\n, \n300\n, \n1\n)\n    \nY\n = train_ds\n\n\n\n为什么这样就会出现\n\n\n把\nX = train_ds.reshape(-1, 300, 1)\nY = train_ds\n改成\nX = train_ds[:, 300].reshape(-1, 300, 1)\nY = train_ds[:, 300]\n就能运行出来了呢？那个[:, 300]是什么意思，看不懂", "Tag": "算法分析"}
{"Answer": "samples_generator模块在新版本scikit-learn中已被移除。samples_generator模块中相应的类/函数直接从sklearn.datasets中导入即可。\nfrom sklearn.datasets import make_circles", "Konwledge_Point": "应对NP完全问题", "Question": "关于PyCharm编译器中的报错问题\n\n\nimport\n numpy \nas\n np\n\nimport\n matplotlib.pyplot \nas\n plt\n\nfrom\n sklearn.svm \nimport\n SVC\n\nfrom\n sklearn.datasets.samples_generator \nimport\n make_circles\n\nX\n, y=make_circles(\n100\n,factor=.\n1\n,noise=.\n1\n)\n\nplt\n.scatter(\nX\n[:,\n0\n],\nX\n[:,\n1\n],c=y,s=\n50\n,cmap='autumn')\n\n\n\n这一段代码中的from sklearn.datasets.samples_generator import make_circles运行出错。sklearn也安装了。错误报告是：ModuleNotFoundError: No module named 'sklearn.datasets.samples_generator'   编译器使用的是PyCharm。请问有人知道要怎么弄吗？", "Tag": "算法分析"}
{"Answer": "strip 不改变原来的字符串，你需要把返回值赋值给原来的字符串，或者直接输出strip的结果", "Konwledge_Point": "应对NP完全问题", "Question": "python输出结果有空格\n问题遇到的现象和发生背景\n\n\n最近在用python编写一个机器人的正解程序，通过读取数据进行正解计算，并将计算出的数据输出到文档\n\n\n用代码块功能插入代码，请勿粘贴截图\n\n\nimport\n numpy \nas\n np\n\nimport\n math\n\nimport\n csv\n\nwith\n \nopen\n(\"tjx.csv\", mode=\"r\", encoding=\"utf-8\") \nas\n f,\\\n        \nopen\n(\"tt.txt\", mode=\"w\", encoding=\"utf-8\") \nas\n f1:\n    \nfor\n \nrow\n \nin\n csv.reader(f, skipinitialspace=\nTrue\n):\n        c1 = \nfloat\n(\nrow\n[\n0\n])\n        c2 = \nfloat\n(\nrow\n[\n1\n])\n        c3 = \nfloat\n(\nrow\n[\n2\n])\n        c4 = \nfloat\n(\nrow\n[\n3\n])\n        c5 = \nfloat\n(\nrow\n[\n4\n])\n        c6 = \nfloat\n(\nrow\n[\n5\n])\n\n        T1 = np.mat([[round(math.cos(c1)), -round(math.sin(c1)), \n0\n, \n0\n],\n                     [round(math.sin(c1)), round(math.cos(c1)), \n0\n, \n0\n],\n                     [\n0\n, \n0\n, \n1\n, \n187\n],\n                     [\n0\n, \n0\n, \n0\n, \n1\n]])\n        T2 = np.mat([[round(math.cos(c2), \n4\n), \n0\n, round(math.sin(c2), \n4\n), \n0\n],\n                     [round(math.sin(c2), \n4\n), \n0\n, -round(math.cos(c2), \n4\n), \n0\n],\n                     [\n0\n, \n1\n, \n0\n, \n6\n],\n                     [\n0\n, \n0\n, \n0\n, \n1\n]])\n        T3 = np.mat([[round(math.cos(c3), \n4\n), -round(math.sin(c3), \n4\n), \n0\n, \n210\n*(round(math.cos(c3), \n4\n))],\n                     [round(math.sin(c3), \n4\n), round(math.cos(c3), \n4\n), \n0\n, \n210\n*(round(math.sin(c3), \n4\n))],\n                     [\n0\n, \n0\n, \n1\n, \n0\n],\n                     [\n0\n, \n0\n, \n0\n, \n1\n]])\n        T4 = np.mat([[round(math.cos(c4), \n4\n), \n0\n, -round(math.sin(c4), \n4\n), \n0\n],\n                     [round(math.sin(c4), \n4\n), \n0\n, round(math.cos(c4), \n4\n), \n0\n],\n                     [\n0\n, \n-1\n, \n0\n, \n210.5\n],\n                     [\n0\n, \n0\n, \n0\n, \n1\n]])\n        T5 = np.mat([[round(math.cos(c5), \n4\n), \n0\n, round(math.sin(c5), \n4\n), \n0\n],\n                     [round(math.sin(c5), \n4\n), \n0\n, -round(math.cos(c5), \n4\n), \n0\n],\n                     [\n0\n, \n1\n, \n0\n, \n0\n],\n                     [\n0\n, \n0\n, \n0\n, \n1\n]])\n        T6 = np.mat([[round(math.cos(c6), \n4\n), \n0\n, -round(math.sin(c6), \n4\n), \n0\n],\n                     [round(math.sin(c6), \n4\n), \n0\n, round(math.cos(c6), \n4\n), \n0\n],\n                     [\n0\n, \n-1\n, \n0\n, \n159.3\n],\n                     [\n0\n, \n0\n, \n0\n, \n1\n]])\n\n        T01 = np.dot(T1, T2)\n        T12 = np.dot(T01, T3)\n        T23 = np.dot(T12, T4)\n        T34 = np.dot(T23, T5)\n        T45 = np.dot(T34, T6)\n        \nfor\n i \nin\n range(len(T45)):\n            f1.\nwrite\n(str(T45[i]) + \n'\\n'\n)\n\n\n\n\n\n运行结果及报错内容\n\n\n程序能够正常运行，结果可以正常输出，但是运行结果前有空格。\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n使用 strip 但是未能清楚两端空格\n\n\n我想要达到的结果\n\n\n输出的结果没有空格存在\n\n", "Tag": "算法分析"}
{"Answer": "angles 这个ndarray 有7个元素，而radar_labels 只有6个，不匹配", "Konwledge_Point": "应对NP完全问题", "Question": "霍兰德人格分析代码运行\n\n\n#HollandPersonalityAnalysis.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams[\n'font.family'\n]=\n'SimHei'\n\nradar_labels = np.array([\n'研究型(I)'\n,\n'艺术型(A)'\n,\n'社会型(S)'\n,\\\n                         \n'企业型(E)'\n,\n'常规型(C)'\n,\n'现实型(R)'\n]) #雷达标签\nnAttr = 6\ndata = np.array([[0.40, 0.32, 0.35, 0.30, 0.30, 0.88],\n                 [0.85, 0.35, 0.30, 0.40, 0.40, 0.30],\n                 [0.43, 0.89, 0.30, 0.28, 0.22, 0.30],\n                 [0.30, 0.25, 0.48, 0.85, 0.45, 0.40],\n                 [0.20, 0.38, 0.87, 0.45, 0.32, 0.28],\n                 [0.34, 0.31, 0.38, 0.40, 0.92, 0.28]]) #数据值\ndata_labels = (\n'艺术家'\n, \n'实验员'\n, \n'工程师'\n, \n'推销员'\n, \n'社会工作者'\n,\n'记事员'\n)\nangles = np.linspace(0, 2*np.pi, nAttr, \nendpoint\n=\nFalse\n)\ndata = np.concatenate((data, [data[0]]))\nangles = np.concatenate((angles, [angles[0]]))\nfig = plt.figure(\nfacecolor\n=\n\"white\"\n)\nplt.subplot(111, \npolar\n=\nTrue\n)\nplt.plot(angles,data,\n'o-'\n, \nlinewidth\n=1, \nalpha\n=0.2)\nplt.fill(angles,data, \nalpha\n=0.25)\nplt.thetagrids(angles\n*180\n/np.pi, radar_labels,frac = 1.2)\nplt.figtext(0.52, 0.95, \n'霍兰德人格分析'\n, \nha\n=\n'center'\n, \nsize\n=20)\nlegend = plt.legend(data_labels, loc=(0.94, 0.80), \nlabelspacing\n=0.1)\nplt.setp(legend.get_texts(), \nfontsize\n=\n'large'\n)\nplt.grid(\nTrue\n)\nplt.savefig(\n'holland_radar.jpg'\n)\nplt.show()\n\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "\nimport logging  # 引入logging模块\nimport os.path\nimport time\n# 第一步，创建一个logger\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)  # Log等级总开关\n# 第二步，创建一个handler，用于写入日志文件\nrq = time.strftime('%Y%m%d%H%M', time.localtime(time.time()))\nlog_path = os.path.dirname(os.getcwd()) + '/Logs/'\nlog_name = log_path + rq + '.log'\nlogfile = log_name\nfh = logging.FileHandler(logfile, mode='w')\nfh.setLevel(logging.DEBUG)  # 输出到file的log等级的开关\n# 第三步，定义handler的输出格式\nformatter = logging.Formatter(\"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\")\nfh.setFormatter(formatter)\n# 第四步，将logger添加到handler里面\nlogger.addHandler(fh)\n# 日志\nlogger.debug('this is a logger debug message')\nlogger.info('this is a logger info message')\nlogger.warning('this is a logger warning message')\nlogger.error('this is a logger error message')\nlogger.critical('this is a logger critical message')\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "您好！请问您在之前遇到的下面的这个问题解决了么，我也遇到了这个问题\nimport logging\nimport logging.config, configparser, numpy \nas\n np, random, codecs\nfrom collections import OrderedDict\n#导入日志配置文件\nlogging.config.fileConfig(\n\"logging.conf\"\n)\n#创建日志对象\nloggerInfo = logging.getLogger(\n\"TimeInfoLogger\"\n)\nConsolelogger = logging.getLogger(\n\"ConsoleLogger\"\n)\n#导入配置文件\n\nconf\n = configparser.configparser()\n\nconf\n.\nread\n(\n\"setting.conf\"\n)\nTraceback (most recent \ncall\n \nlast\n):\n\nFile \n\"\"\n, \nline\n \n5\n, in \n\n\nlogging.config.fileConfig(\n\"logging.conf\"\n)\n\nFile \n\"C:\\Users\\Anaconda3\\lib\\logging\\config.py\"\n, \nline\n \n76\n, in fileConfig\nformatters = _create_formatters(\ncp\n)\n\nFile \n\"C:\\Users\\Anaconda3\\lib\\logging\\config.py\"\n, \nline\n \n109\n, in _create_formatters\nflist = \ncp\n[\n\"formatters\"\n][\n\"keys\"\n]\n\nFile \n\"C:\\Users\\Anaconda3\\lib\\configparser.py\"\n, \nline\n \n956\n, in __getitem__\nraise KeyError(key)\n\nKeyError: \n'formatters'\n\n\n\n", "Tag": "算法分析"}
{"Answer": "望采纳\na = [1,2,3,4]\nprint(a[::-1])\n\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "下面代码得到的结果怎么倒置\na 的 列表 怎么倒置呢？【0.08 0.11 ..nan】\n代码怎么写\n\n\n\n\nfrom\n talib import abstract\nimport numpy as np\nMACD = abstract.Function(\n'MACD'\n)\nDIFF,DEA,MACDHIST = MACD(df.close,\nfastperiod\n=12,slowperiod=26,signalperiod=9)\n\na\n=np.around(DIFF,decimals=2)\n\nb\n=np.around(DEA,decimals=2)\n\nprint\n(a[-1])\n\nprint\n(a)\n\n0.08\n[ nan  nan  nan \n..\n. 0.11 0.11 0.08]\n", "Tag": "算法分析"}
{"Answer": "看起来这个代码只做了二次多项式回归，并且在图像中绘制了回归曲线。如果你想更改degree，你需要更改以下代码：\npoly_reg=PolynomialFeatures(degree=2)   ##reset degree\n\n\n把degree的值设置为你想要的数值。\n然而，每次运行代码所得到的MSE值都不一样是正常的。因为训练数据和测试数据是随机选取的，如果每次运行代码都选取了不同的训练数据和测试数据，则每次的MSE值都会有所不同。", "Konwledge_Point": "应对NP完全问题", "Question": "多项式回归，matplob绘图\n机器学习，用的Polynomial Regression，在jupyter运行，思路是先从excel读数据，再用LabelEncoder重新排序并索引，最后进行regression\n问题：不管怎么改degree的数值，只能输出degree=2的图像，但是每次MSE都不一样\n\n\n\n\n\nimport numpy as np  \nimport sklearn.linear_model as lm    \nimport matplotlib.pyplot as plt \nimport pandas as pd\n\nfrom\n sklearn.model_selection import train_test_split\n\nfrom\n sklearn.preprocessing import PolynomialFeatures\n\nfrom\n sklearn import preprocessing\n\nfrom\n sklearn.preprocessing import LabelEncoder\n\nfrom\n sklearn.metrics import mean_squared_error\n\n\ndf\n=pd.read_excel('data.xlsx',sheet_name='T5',header=9,nrows=1)\ndf.isnull()\n\ndata\n=df.iloc[0]    #read 2022 \nto\n 1950 into data\n\nY\n=data[1:]\n\nle\n=LabelEncoder()\n\nX\n=le.fit_transform(df.columns[1:])\n\n\ndatasets_X\n=np.arange(1950,2014,step=1)\n\ndataset_length\n=len(datasets_X)\n\ndatasets_Y\n=data[2023-datasets_X]\n\ndatasets_Y\n=np.array(datasets_Y)\n\ndatasets_X\n=np.array(datasets_X).reshape([dataset_length,1])\n\ntest_X\n=np.arange(2014,2023,step=1)\n\ntest_length\n=len(test_X)\n\ntest_X\n=np.array(test_X).reshape([test_length,1])\n\n\npoly_reg\n=PolynomialFeatures(degree=2)   ##reset degree\n\nX_poly\n=poly_reg.fit_transform(datasets_X)\n\nlin_reg\n=lm.LinearRegression()\nlin_reg.fit(X_poly,datasets_Y)\n\ndata1\n=poly_reg.fit_transform(test_X)\n\npred\n=lin_reg.predict(data1)\nTEST=[5469724,5535002,5607283,5612253,5638676,5703569,5685807,5453566,5637022]\n\nmse\n=mean_squared_error(TEST, pred)\n\nplt.scatter(datasets_X,datasets_Y,\ns\n=10)\n\nmy_x_ticks\n=np.arange(1950,2023,10)\nplt.xticks(my_x_ticks)\n\nX\n=np.arange(1950,2023).reshape([-1,1])\nplt.plot(X,lin_reg.predict(poly_reg.fit_transform(X)),\ncolor\n=\n'black'\n)\nplt.xlabel(\n\"Year\"\n)\nplt.ylabel(\n\"Total Population(*\n$10\n^6)$\"\n)\nplt.title(\n\"Multiple Regression\"\n)\nplt.scatter(test_X,TEST,\ns\n=10)\nplt.show()\n\nprint\n(mse)\n", "Tag": "算法分析"}
{"Answer": "问题已解决， 将算法中求幂的过程有 ** 改为 math.pow(row,3)即可", "Konwledge_Point": "应对NP完全问题", "Question": "python numpy整型数据溢出\n在计算图像矩的过程中，发现数据溢出了，与调用openCV库中的函数算出的结果不同。\n\n目前使用的是numpy数组储存数据，数据类型设为int64，但是他的范围还是不够大，数值会在10的16 次方的数量级。应该怎么设置这个数组的类型呀？\n\n\n\n# 计算p+q阶图像矩\ndef img_m(img):\n    # 用于存放图像矩计算结果\n    result = np.zeros([4,4],dtype='float')\n    # 获取图像大小\n    height = img.shape[0]\n    width = img.shape[1]\n    # 遍历图像内的像素点，进行图像矩计算\n    for row in range(height):\n        for col in range(width):\n            result[0, 0] += img[row,col]\n            result[0, 1] += row * img[row, col]\n            result[1, 0] += col * img[row, col]\n            result[1, 1] += row * col * img[row, col]\n            result[0, 2] += row ** 2 * img[row, col]\n            result[2, 0] += col ** 2 * img[row, col]\n            result[0, 3] += row ** 3 * img[row, col]\n            result[3, 0] += col ** 3 * img[row, col]\n            result[2, 1] += row * col ** 2 * img[row, col]\n            result[1, 2] += row ** 2 * col * img[row, col]\n\n", "Tag": "算法分析"}
{"Answer": "使用nditer在数组上迭代可以将原始数组的单元格视为0维数组。对于非对象数组，这几乎等同于生成标量，因为0维数组的行为通常类似于标量，但这不适用于对象数组用flat遍历对象数组只会直接得到对象nditer是numpy中的一个方法，且默认是只读对象，而flat方法是数组对象的方法。", "Konwledge_Point": "应对NP完全问题", "Question": "numpy flat函数和nditer函数有啥区别\n都可以遍历每一个元素，那他们区别在哪\na=np.arange(5)\nfor i in np.nditer(a):\n    print(i)\nprint('分割线')\nfor i in a.flat:\n    print(i)", "Tag": "算法分析"}
{"Answer": "可以将高度值 z 转换成二维数组的形式，然后再使用 plt.contour 函数绘制等高图。\n比如，可以使用 numpy.meshgrid 函数将横坐标和纵坐标列表转换成网格坐标矩阵，然后将这两个网格坐标矩阵广播相乘得到一个矩形网格坐标矩阵，再将高度值列表转换成与网格坐标矩阵大小相同的矩阵，最后将矩阵传入 plt.contour 函数即可。\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 数据准备\nx = [100, 200, 300, 400, 500]  # 横坐标\ny = [300, 500, 100, 200, 500]  # 纵坐标\nz = [100, 120, 110, 150, 180]  # 高度值\n\n# 将横坐标和纵坐标列表转换成网格坐标矩阵\nxx, yy = np.meshgrid(x, y)\n\n# 将高度值列表转换成与网格坐标矩阵大小相同的矩阵\nzz = np.array(z).reshape(xx.shape)\n\n# 绘制等高图\nC = plt.contour(xx, yy, zz, 8, colors='black')\n\n# 添加标题\nplt.title(f\"测试\", fontsize=20, fontname=\"SimHei\")   # \"SimHei\"黑体字体可确保中文正常显示\n\n# 等值线间添加过渡色\nplt.contourf(xx, yy, zz, 8)\n\n# colorbar()可在右侧显示颜色值\nplt.colorbar()\n\n# clabel用于标记等高线\nplt.clabel(C,inline=1,fontsize=10)\n\nplt.show()\n\n仅供参考，望采纳，谢谢。", "Konwledge_Point": "应对NP完全问题", "Question": "关于python中matplotlib绘制等高图的问题\n用matplotlib的plt.contour(x,y,z,8,colors='black')绘制等高图，其高度值z得是二维数组，但我手里高度值只是一组列表，要怎样才能绘制出等高图呢？\n\n\n代码如下：\n\n\nimport\n numpy \nas\n np\n\nimport\n pandas \nas\n pd\n\nimport\n matplotlib.pyplot \nas\n plt\n\n\n#数据准备\n\nx=[\n100\n,\n200\n,\n300\n,\n400\n,\n500\n]  \n#横坐标\n\ny=[\n300\n,\n500\n,\n100\n,\n200\n,\n500\n]  \n#纵坐标\n\nz=[\n100\n,\n120\n,\n110\n,\n150\n,\n180\n]  \n#高度值，运行时这里出错，提示“Input z must be 2D, not 1D”，\n\n                         \n#问题是如何转换成二维数组？？？？？\n\n\n\n'''添加标题'''\n\nplt.title(\nf\"测试\"\n, fontsize=\n20\n, fontname=\n\"SimHei\"\n)   \n#\"SimHei\"黑体字体可确保中文正常显示\n\n\n\n'''contour()函数可生成三维结构表面的等值线图'''\n\nC = plt.contour(x,y,z,\n8\n,colors=\n'black'\n)\n\n\n'''cmap=plt.cm.hot为等值线添加过渡色'''\n\nplt.contour(x,y,z,\n8\n,cmap=plt.cm.hot)\n\n\n'''等值线间添加过渡色'''\n\nplt.contourf(x,y,z, \n8\n)\n\n\n'''colorbar()可在右侧显示颜色值'''\n\nplt.colorbar()\n\n\n'''clabel用于标记等高线'''\n\nplt.clabel(C,inline=\n1\n,fontsize=\n10\n)\n\nplt.show()\n\n", "Tag": "算法分析"}
{"Answer": "遗漏对象未予封闭参考https://blog.csdn.net/iprobobo/article/details/123215345", "Konwledge_Point": "应对NP完全问题", "Question": "这是霍兰德人格分析图的代码，但解析器显示我有错误，请哪位看一下我哪里错了？\n这是错误类型\n\n\n这是我的代码\n\n\nHollandRadarDraw\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n\n\nmatplotlib.rcParams['font.family'] = 'SimHei'\nradar_labels = np.array(['研究型(I)', '艺术型(A)', '社会型(S)',\n                         '企业型(E)', '常规型(C)', '现实型(R)'])\ndata = np.array([[0.40, 0.32, 0.35, 0.30, 0.30, 0.88],\n                 [0.85, 0.35, 0.30, 0.40, 0.40, 0.30],\n                 [0.43, 0.89, 0.30, 0.28, 0.22, 0.30],\n                 [0.30, 0.25, 0.48, 0.85, 0.45, 0.40],\n                 [0.20, 0.38, 0.87, 0.45, 0.32, 0.28],\n                 [0.34, 0.31, 0.38, 0.40, 0.92, 0.28]])  # 数据值\ndata_labels = ('艺术家', '实验员', '工程师', '推销员', '社会工作者', '记事员')\nangles = np.linspace(0, 2 * np.pi, 6, endpoint=False)\ndata = np.concatenate((data, [data[0]]))\nangles = np.concatenate((angles, [angles[0]]))\nfig = plt.figure(facecolor=\"white\")\nplt.subplot(111, polar=True)\nplt.plot(angles, data, 'o-', linewidth=1, alpha=0.2)\nplt.fill(angles, data, alpha=0.25)\nplt.thetagrids(angles * 180 / np.pi, radar_labels, frac=1.2)\nplt.figtext(0.52, 0.95, '霍兰德人格分析', ha='center', size=20)\nlegend = plt.legend(data_labels, loc=(0.94, 0.80), labelspacing=0.1)\nplt.setp(legend.get_texts(), fontsize='large')\nplt.grid(True)\nplt.savefig('holland_radar.jpg')\nplt.show()", "Tag": "算法分析"}
{"Answer": "超时错误，输入数据input_data的维度必须是三维，检查下数据", "Konwledge_Point": "应对NP完全问题", "Question": "遇到RuntimeError报错该怎么处理？\n遇到报错：\nline 57, in forward\n    input_data = input_data.permute(2, 1, 0)\nRuntimeError: number of dims don't match in permute\n\n\n报错部分代码：\n\n\n    \ndef\n \nforward\n(\nself, input_data\n):\n        \n# input_data:         Point Cloud having shape input_shape.\n\n        \n# output:            PointNet features (Batch x emb_dims)\n\n        \nif\n self.input_shape == \n\"bnc\"\n:\n            num_points = input_data.shape[\n1\n]\n            input_data = input_data.permute(\n0\n, \n2\n, \n1\n)          \n#报错地方\n\n        \nelse\n:\n            num_points = input_data.shape[\n2\n]\n        \nif\n input_data.shape[\n1\n] != \n3\n:\n            \nraise\n RuntimeError(\n\"shape of x must be of [Batch x 3 x NumInPoints]\"\n)\n\n        output = input_data\n        \nfor\n idx, layer \nin\n \nenumerate\n(self.layers):\n            output = layer(output)\n\n        \nreturn\n output\n\n\n\n\n查看了input_data的维数，得到torch.Size([1, 3]) 确实是三维的\n\n\n读取数据部分代码\n\n\n    \nif\n args.user_data:\n        source_path = os.path.join(\nr'E:\\bunny\\data\\bun000.ply'\n)  \n# The source point cloud is a rotated and offset defect\n\n        \n# source=s3.float()\n\n        source_data = o3d.io.read_point_cloud(source_path)\n        points1 = np.array(source_data.points)\n        idx1 = np.arange(points1.shape[\n0\n])\n        np.random.shuffle(idx1)\n        source = points1[idx1[:args.num_points]]\n\n        template_path = os.path.join(\nr'E:\\bunny\\data\\bun045.ply'\n)  \n# Template point cloud is complete\n\n        \n# template=s5.float()\n\n        template_data = o3d.io.read_point_cloud(template_path)\n        points2 = np.array(template_data.points)\n        idx2 = np.arange(points2.shape[\n0\n])\n        np.random.shuffle(idx2)\n        template = points2[idx2[:args.num_points]]\n        testset = UserData(template=template, source=source, tpcc=\nNone\n, igt=\nNone\n)\n\n\n\n\nsource和template都是三维的（如下所示）\n [ 0.0585     0.0808363  0.0858177]\n [ 0.0465     0.0650293  0.0904855]\n [-0.00725    0.126023   0.0352551]\n\n\nif\n __name__ == \n'__main__'\n:\n    # Test the code.\n    x = torch.rand((10,1024,3))\n\n    pn = PointNet(\nuse_bn\n=\nTrue\n)\n    y = pn(x)\n    \nprint\n(\n\"Network Architecture: \"\n)\n    \nprint\n(pn)\n    \nprint\n(\n\"Input Shape of PointNet: \"\n, x.shape, \n\"\\nOutput Shape of PointNet: \"\n, y.shape)\n\n", "Tag": "算法分析"}
{"Answer": "cv2.pointPolygonTest(contour, pt, measureDist)就是了啊，这个函数当measureDist设置为false时，返回 -1、0、1三个固定值。若返回值为+1，表示点在多边形内部，返回值为-1，表示在多边形外部，返回值为0，表示在多边形上。", "Konwledge_Point": "应对NP完全问题", "Question": "判断点是否在最大轮廓内的python函数是什么\ncnts, hierarchy  = cv2.findContours(edged, cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)#查找轮廓\narea = []\nfor k in range(len(cnts)):\n   area.append(cv2.contourArea(cnts[k]))#轮廓面积数列\nmax_id = np.argmax(np.array(area))#最大面积轮廓序号\ncnt = cnts[max_id]#最大轮廓\nx=20\ny=30\npoint = np.array([x,y], dtype=\"int\")\npoint = np.int0(point)\n        if (pointPolygonTest(cnt, point, false) == -1):#判断point这个点是否在轮廓外部python函数是什么\n            #在图像img上把point这个点置黑怎么写", "Tag": "算法分析"}
{"Answer": "L大写", "Konwledge_Point": "应对NP完全问题", "Question": "convert将RGBA转为L会报错\nconversion from RGB to l not supported\n我想将图片从RGBA转为L，但是会出现上述报错\n具体代码如下：\n\n\n\n```python\nimport pytesseract\nfrom PIL import Image\nimport numpy as np\n\nim = Image.open(\n\"D:\\captcha.png\"\n)\nim = im.convert(\n\"l\"\n)\nthreshold = 50\n\narray \n= np.array(im)\n\narray \n= np.where(array > threshold, 255, 0)\nim = Image.fromarray(array.astype(\n\"unit8\"\n))\n\n\n\n\n\n\n```", "Tag": "算法分析"}
{"Answer": "len(data) 和len(index)长度不一致，检查一下长度，大概率是index出错", "Konwledge_Point": "应对NP完全问题", "Question": "Length of values does not match length of index\n\n\n用上证50做蒙特卡洛模拟分析有效前沿\n\n\n出现了如下错误：\n\n\n---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent \ncall\n last)\nC:\\Users\\ADMINI~\n1\n\\AppData\\\nLocal\n\\\nTemp\n/ipykernel_8100/\n1740976838.\npy \nin\n \n\n----> 1 PORT_WEIGHTS,PORT_RETURNS,PORT_SIGMAS,PORT_SR = montecarlo_portfolios(SH50_CODE_LIST,PORT_NUM,START_DATE,END_DATE,RF)\n\n\nC:\\Users\\ADMINI~\n1\n\\AppData\\\nLocal\n\\\nTemp\n/ipykernel_8100/\n2484461405.\npy \nin\n montecarlo_portfolios(stock_list, number, start_date, end_date, risk_free, trade_days)\n     \n10\n         weights_i_array = np.random.random(len(stock_list))\n     \n11\n         weights_i_array /=np.sum(weights_i_array)\n\n---> 12         port_i_year_return,port_i_year_sigma,port_i_year_sr = get_return_risk(weights_i_array,his_prices_df,risk_free)\n\n     \n13\n         port_weights_list.append(weights_i_array)\n     \n14\n         port_returns_list.append(port_i_year_return)\n\nC:\\Users\\ADMINI~\n1\n\\AppData\\\nLocal\n\\\nTemp\n/ipykernel_8100/\n3548710514.\npy \nin\n get_return_risk(port_i_weights_array, his_prices_df, risk_free, trade_days)\n      \n1\n def get_return_risk(port_i_weights_array,his_prices_df,risk_free,trade_days=\n250\n):\n      \n2\n     his_day_returns = np.log(his_prices_df/his_prices_df.shift(\n-1\n))\n\n----> 3     port_i_year_return = np.sum(his_day_returns.mean()*trade_days*port_i_weights_array)\n\n      \n4\n     port_i_year_sigma = np.sqrt(np.dot(port_i_weights_array,np.dot(his_day_returns.cov()*trade_days,port_i_weights_array.T)))\n      \n5\n     port_i_year_sr=(port_i_year_return-risk_free)/port_i_year_sigma\n\nD:\\p\\lib\\site-packages\\pandas\\core\\ops\\common.py \nin\n new_method(self, other)\n     \n67\n         other = item_from_zerodim(other)\n     \n68\n \n\n---> 69         return method(self, other)\n\n     \n70\n \n     \n71\n     \nreturn\n new_method\n\nD:\\p\\lib\\site-packages\\pandas\\core\\arraylike.py \nin\n __mul__(self, other)\n    \n106\n     @unpack_zerodim_and_defer(\"__mul__\")\n    \n107\n     def __mul__(self, other):\n\n--> 108         return self._arith_method(other, operator.mul)\n\n    \n109\n \n    \n110\n     @unpack_zerodim_and_defer(\"__rmul__\")\n\nD:\\p\\lib\\site-packages\\pandas\\core\\series.py \nin\n _arith_method(self, other, op)\n   \n5526\n             result = ops.arithmetic_op(lvalues, rvalues, op)\n   \n5527\n \n-> \n5528\n         \nreturn\n self._construct_result(result, \nname\n=res_name)\n   \n5529\n \n   \n5530\n \n\nD:\\p\\lib\\site-packages\\pandas\\core\\series.py \nin\n _construct_result(self, result, \nname\n)\n   \n2943\n         # We \ndo\n \nnot\n pass dtype \nto\n ensure that the Series constructor\n   \n2944\n         #  does inference \nin\n the \ncase\n \nwhere\n `result` has \nobject\n-dtype.\n-> \n2945\n         \nout\n = self._constructor(result, \nindex\n=self.\nindex\n)\n   \n2946\n         \nout\n = \nout\n.__finalize__(self)\n   \n2947\n \n\nD:\\p\\lib\\site-packages\\pandas\\core\\series.py \nin\n __init__(self, data, \nindex\n, dtype, \nname\n, \ncopy\n, fastpath)\n    \n428\n                 \nindex\n = ibase.default_index(len(data))\n    \n429\n             elif is_list_like(data):\n\n--> 430                 com.require_length_match(data, index)\n\n    \n431\n \n    \n432\n             # \ncreate\n/\ncopy\n the manager\n\nD:\\p\\lib\\site-packages\\pandas\\core\\common.py \nin\n require_length_match(data, \nindex\n)\n    \n529\n     \"\"\"\n    530     if len(data) != len(index):\n--> 531         raise ValueError(\n    532             \"Length \nof\n \nvalues\n \"\n    533             f\"({len(data)}) \"\n\nValueError: Length of values (50) does not match length of index (1)\n", "Tag": "算法分析"}
{"Answer": "https://blog.csdn.net/hlx371240/article/details/40861667?utm_source=blogxgwz7", "Konwledge_Point": "应对NP完全问题", "Question": "感知机的实现（二分类）——模拟出来的直线斜率没问题，但是截距有问题\n问题：模拟出来的函数拟不上数据，恳请大佬告知哪儿出了问题！！！\n\n\n\n损失函数：L（w1,w2,b）=-∑（w1*x1+w2*x2+b）y\n\n                 w1的梯度：-∑y*x1\n\n                                 w2的梯度:    -∑y*x2\n\n                                  b的梯度:    -∑y\n\n                 学习率为 0.01\n\n                                 使用梯度下降法进行优化\n\n\n\nimport pandas\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef ganzhiji(data_x1,data_x2,data_y):\n    w_1 = 0   # 参数w1\n    w_2 = 0   # 参数w2   \n    b = 0     # 偏置\n    step = 0.01\n    threshold = 0.1\n    f_pre = -0.2\n    re_num = 0  # 循环次数\n    f_current = loss_function(w_1, w_2, b, data_x1, data_x2, data_y)\n    while abs(f_current - f_pre) > threshold and re_num <= 40000:\n        w_1, w_2, b = update(data_x1, data_x2, data_y, w_1, w_2, b, step)\n        f_pre = f_current\n        f_current = loss_function(w_1, w_2, b, data_x1, data_x2, data_y)\n        re_num = re_num + 1\n\n    # 训练完毕后计算精度\n    num = 0\n    for a in range(len(data_x1)):\n        if (w_1 * data_x1[a] + w_2 * data_x2[a] + b) * data_y[a] < 0:\n            num = num + 1\n    print(\"w1={},w2={},b={},精度为：{}\".format(w_1, w_2, b, 1 - num/len(data_x1)))\n    return w_1,w_2,b\n\ndef loss_function(w_1, w_2, b, data_x1, data_x2, data_y):  # 计算参数更新后损失函数的值\n   sum = 0\n   for a in range(len(data_x1)):\n       if (w_1 * data_x1[a] + w_2 * data_x2[a] + b) * data_y[a] < 0:\n           sum = sum - (w_1 * data_x1[a] + w_2 * data_x2[a] + b) * data_y[a]\n   return sum\n\ndef tidu(data_x1, data_x2, data_y, b):  # 参数的梯度\n    t_w1 = 0\n    t_w2 = 0\n    t_b = 0\n    for a in range(len(data_x1)):\n        t_w1 = t_w1 - data_y[a] * data_x1[a]\n        t_w2 = t_w2 - data_y[a] * data_x2[a]\n        t_b =t_b - data_y[a]\n    return t_w1, t_w2, t_b\n\ndef update(data_x1, data_x2, data_y, w_1, w_2, b, step):  # 将参数进行梯度下降\n    t_w1 , t_w2, t_b = tidu(data_x1, data_x2, data_y, b)\n    w_1 = w_1 - step * t_w1\n    w_2 =w_2 - step * t_w2\n    b = b - step * t_b\n    return w_1, w_2, b\n\nif __name__=='__main__':\n    data = pandas.read_csv(r'C:\\Users\\科德的帝国\\Desktop\\ML_data.csv',engine='python')\n    data_x1 = data['x1']\n    data_x2 = data['x2']\n    data_y = data['y']\n\n    data_x1 = [a /10 for a in data_x1]         # 将数据放缩\n    data_x2 = [a/10 for a in data_x2]\n    data_y = [a for a in data_y]\n    for a in range(len(data_y)):               # 将数据文件中的类别y为0的值改为-1，以符合符号函数的输出 \n       if data_y[a] == 0:\n          data_y[a] = -1\n\n    # 配置\n    plt.rcParams['font.sans-serif']=['SimHei'] # 显示中文\n    plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n\n    # 画散点图\n    for a in range(len(data_x1)):\n       if data_y[a] == 1:\n            plt.scatter(data_x1[a], data_x2[a],color='red')\n       else:\n            plt.scatter(data_x1[a], data_x2[a],color='blue')\n\n    # 画训练完毕的函数        \n    w_1,w_2,b=ganzhiji(data_x1,data_x2,data_y)\n    x= np.arange(0,10,0.1)\n    y = [-(w_1*i+b)/w_2 for i in x]\n    plt.plot(x,y)\n    plt.show()\n\n\n\n\n执行结果：", "Tag": "算法分析"}
{"Answer": "\nimport pandas as pd\n\ndf=pd.DataFrame()\ndf['输入']=input_number\ndf['target']=output_number\ndf['输出']=target_number\n直接这样就可以吧", "Konwledge_Point": "应对NP完全问题", "Question": "Python中pandas的DataFrame，给列赋值出现错误的问题\n我的题目是从一个txt中读取一些数据，经过某个函数，得到想要的结果：\n\n\n\n三组数据分别存放在了input_number、target_number、output_number三个list中，并且长度均为3\n\n\n\n\n\n\n但是创建dataframe并且赋值后，出现了如下错误：\n\n\n\n\n\nimport pandas as pd\n\ncontents=[\"输入\",\"target\",\"输出\"]\n\ndf=pd.DataFrame(columns=contents)\ndf.iloc[:,0]=input_number\ndf.iloc[:,1]=target_number\ndf.iloc[:,2]=output_number\n\n\n\n错误：\n\n\n\n\n\nValueError                                Traceback (most recent call last)\n in ()\n     23 df.iloc[:,0]=input_number\n     24 df.iloc[:,1]=target_number\n---> 25 df.iloc[:,2]=output_number\n\nD:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py in __setitem__(self, key, value)\n    192             key = com._apply_if_callable(key, self.obj)\n    193         indexer = self._get_setitem_indexer(key)\n--> 194         self._setitem_with_indexer(indexer, value)\n    195 \n    196     def _has_valid_type(self, k, axis):\n\nD:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py in _setitem_with_indexer(self, indexer, value)\n    581                     value = np.array(value, dtype=object)\n    582                     if len(labels) != value.shape[1]:\n--> 583                         raise ValueError('Must have equal len keys and value '\n    584                                          'when setting with an ndarray')\n    585 \n\nValueError: Must have equal len keys and value when setting with an ndarray\n\n\n\n然后我尝试了将最后一行代码放在第二位，或者第一位：\n\n\n\n\n\nimport pandas as pd\n\ncontents=[\"输入\",\"target\",\"输出\"]\n\ndf=pd.DataFrame(columns=contents)\ndf.iloc[:,2]=output_number\ndf.iloc[:,0]=input_number\ndf.iloc[:,1]=target_number\n\n\n\n\n\nimport pandas as pd\n\ncontents=[\"输入\",\"target\",\"输出\"]\n\ndf=pd.DataFrame(columns=contents)\ndf.iloc[:,0]=input_number\ndf.iloc[:,2]=output_number\ndf.iloc[:,1]=target_number\n\n\n\n都是可以执行成功的：\n\n\n\n\n\n\n请问这是什么原因呢？找了一下发现有类似的，都是迭代之类的问题，但是我这个好像不是啊....\n\n\n\n谢谢各位！！", "Tag": "算法分析"}
{"Answer": "已解决for j in range(k):            # np.mean(r,g,b,label)，属性和label都求个平均值            one_cluster = img_new[img_new[:, 3] == j] # 找到所有label为j的像素,其中img_new.shape = (269180,4)            if len(con_cluster) != 0:                cluster_center[j] = np.mean(one_cluster, axis=0) # 通过img_new[:, 3] == j找到所有label为j的行索引(?, 4)，\n加上这句if就行了，mean方法不允许传入空数组", "Konwledge_Point": "应对NP完全问题", "Question": "kmeans 图像切割。 py-opencv，报错\n\n\nimport\n numpy as np\n\nimport\n matplotlib.pyplot as plt\n \n\nimg\n = plt.imread('G:/Experiment/Machine Learning/boat.jpg')\n\nimg_row\n = img.shape[\n0\n]\n\nimg_col\n = img.shape[\n1\n]\n \ndef knn(img, iter, k):\n    \nimg\n = img.reshape(-\n1\n,\n3\n) \n# 使二维空间，变成一维空间，避免后面计算距离时使用双层循环, 这样每一行代表不同空间的像素\n\n    \nimg_new\n = np.column_stack((img, np.ones(img_row*img_col))) \n# 加一列\n\n \n    \n# (1) 随机选择k个像素作为初始聚类中心\n\n    \ncluster_orientation\n = np.random.choice(img_row*img_col, k, \nreplace=False)\n \n# 产生k索引坐标，即k个中心的位置\n\n    \ncluster_center\n = img_new[cluster_orientation, :] \n# shape =（5,4）根据索引坐标，找到对应的聚类中心的rgb像素值\n\n \n    \n# 迭代\n\n    \ndistance\n = [ [] for i \nin\n range(k)] \n# [ [], [], [], [], []]生成list,每个元素是一个列向量，该列向量保存的是所有像素距离中心j的距离\n\n    for i \nin\n range(iter):\n        \n# (2) 计算所有像素与聚类中心j的颜色距离\n\n        print(\n\"迭代次数：%d\"\n % i)\n        for j \nin\n range(k):\n            distance[j] = np.sqrt(np.sum(np.square(img_new - cluster_center[j]), \naxis=1))\n \n# data_new.shape = (269180,4)，一行的和\n\n \n        \n# (3) 在当前像素与k个中心的颜色距离中，找到最小那个中心，更新图像所有像素label\n\n        \n# np.array(distance).shape = (5, 269180) ，返回一列中最小值对应的索引,范围是 [0, 4], 代表不同的label\n\n        \norientation_min_dist\n = np.argmin(np.array(distance), \naxis=0)\n   \n# np.array(distance).shape = (5, 269180) 一列中最小值\n\n        img_new[:, \n3\n] = orientation_min_dist \n# shape = (269180, ), 将返回的索引列向量赋值给第4维，即保存label的第3列\n\n        \n# (4) 更新第j个聚类中心\n\n        for j \nin\n range(k):\n            \n# np.mean(r,g,b,label)，属性和label都求个平均值\n\n            \none_cluster\n = img_new[img_new[:, \n3\n] == j] \n# 找到所有label为j的像素,其中img_new.shape = (269180,4)\n\n            cluster_center[j] = np.mean(one_cluster, \naxis=0)\n \n# 通过img_new[:, 3] == j找到所有label为j的行索引(?, 4)，\n\n            \n# 求一列均值，这样mean_r ,mean_g_, mean_b, mean_label,一次循环得到(1,4)\n\n \n    return img_new\n \n\nif\n \n__name__\n == '__main__':\n    \nlabels_vector\n = knn(img, \n100\n, \n5\n)\n    \nlabels_img\n = labels_vector[:,\n3\n].reshape(img_row, img_col)\n    plt.imshow(labels_img)\n    plt.show()\n\n\n\n当我使用2880x1920的图片进行测试时，程序正常执行。\n当我使用1440x960的图片进行测试，程序在第一次迭代就报错，RuntimeWarning: Mean of empty slice.\n\n\n查了一下没找到解决方案，请问这个该怎么解决。", "Tag": "算法分析"}
{"Answer": "\nfrom scipy import optimize\n\ndef func(x, k, b):\n  return k * x + b\n\nk, b = optimize.curve_fit(func, height, weight)[0]\n\nh = input(\"身高\")\nw = k * h + b \nprint(\"体重为\", w)\n", "Konwledge_Point": "应对NP完全问题", "Question": "给定一个身高体重数据集，如何利用python计算数据集通过输入身高预测体重呢\nimport pandas as pd\nimport numpy as np\np=pd.read_excel('D:\\zip\\zip.xlsx',sheet_name='Sheet1')\nm=np.array(p)\nheight=m[:,0]\nweight=m[:,1]", "Tag": "算法分析"}
{"Answer": "调用df.plot方法时，如果不指定ax参数，每次都会生成一个新的图，所以你第二次不会线没了，是根本就没有数据换图，建议：\r\n用医用变量保存第一次的画图，然后对图进行设置修改：\r\ntmp=df.plt()\r\ntmp.set_xticklabels(list(range(2007,2019)))", "Konwledge_Point": "应对NP完全问题", "Question": "ply作图坐标轴刻度问题\n\n上面这个图标，我用df.plot(),显示的图如下：\n\n我想看到横坐标所有的年份，所以对于命令进行如下修改：\n\ndf1.plot(xticks = np.arange(2007, 2019, 1),xlim = (2007,2019))\n\n这两个参数使用都是看了说明加上去的，但是结果却变成了这样\n\n年份都有了，但是线没了，啥情况，望指点", "Tag": "算法分析"}
{"Answer": "\nimport cv2\nimport numpy as np\ncap = cv2.VideoCapture('./cheliang.MP4')\n\nwhile True:\n  ret, frame = cap.read()\n  if ret == True:\n  frame = cv.flip(frame,0)\n  cv2.imshow('video', frame)\n  key = cv2.waitKey(1)\n  \n  if key == 27:\n  break\n\ncap.release()\ncv2.destroyAllWindows()\n", "Konwledge_Point": "应对NP完全问题", "Question": "如何解决opencv打开视频上下颠倒\n问题遇到的现象和发生背景\n\n\nopencv打开视频是颠倒的\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport cv2\nimport numpy as np\ncap = cv2.VideoCapture('./cheliang.MP4')\n\n\nwhile True:\n    ret, frame = cap.read()\n    if ret == True:\n        cv2.imshow('video', frame)\n\n\nkey\n = cv2.waitKey(\n1\n)\n\nif\n \nkey\n == \n27\n:\n    \nbreak\n\n\n\n\ncap.release()\ncv2.destroyAllWindows()\n\n\n运行结果及报错内容\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n解决不了问题\n\n\n我想要达到的结果\n\n\n让视频不在颠倒", "Tag": "算法分析"}
{"Answer": "这个跟keras还是pytorch没有任何关系，这就是个one hot，无非就是把类别标签都转为one hot，和框架没有任何关系，比如你有[0,1,2,3]四个类，那么0会由[1,0,0,0]表示， 1由[0,1,0,0]表示，如果是只有[0,1]两个类，则0由[1,0]表示，1由[0,1]表示，不知道楼主懂了吗？并且由于该方法与框架无关，楼主可以选择这部分就用keras的这个函数，也不会影响什么", "Konwledge_Point": "应对NP完全问题", "Question": "tensorflow转pytorch实现\n关于这篇文章：语音分类任务（基于UrbanSound8K数据集）\n地址不让打\n\n\ny_train = np.array(keras.utils.to_categorical(y_train, 10))\ny_test = np.array(keras.utils.to_categorical(y_test, 10))\n这两句代码怎么用pytorch框架实现？\n我看其他文章写出来的不知道对不对？\nlabels=[]\nfor i in range(len(train)):\n    labels.append(train[\"classID\"].iloc[i])\ny_train = np.array(labels)\n\n\nlabels2=[]\nfor i in range(len(test)):\n    labels2.append(test[\"classID\"].iloc[i])\ny_test = np.array(labels2)", "Tag": "算法分析"}
{"Answer": "\nYou need to use the type attachment instead of media. This should work:\n  add_action( 'rest_api_init', 'np_register_extra_field' );\n  function np_register_extra_field() {\n      register_rest_field( 'attachment',\n          'extra_media_field',\n          array(\n              'get_callback'    => 'np_get_extra_field',\n              'update_callback' => null,\n              'schema'          => null,\n          )\n      );\n  }\n  function np_get_extra_field( $object, $field_name, $request ) {\n    return 'foobar';\n  }\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "如何修改wp json api媒体响应\n\n\n\nI'm trying to add an extra field to the wp json api reponse for the '/media' endpoint.\nFollowing the \ndoc\n, I have it working for '/posts' or '/pages', but I cannot figure out how to add a field for the '/media' endpoint.\n\n\n\nSo (for '/posts' or '/pages') this works :\n\n\n\nadd_action( 'rest_api_init', 'np_register_extra_field' );\nfunction np_register_extra_field() {\n    register_rest_field( 'post',\n    // register_rest_field( 'page', // this works too\n        'extra_media_field',\n        array(\n          'get_callback'    => 'np_get_extra_field',\n          'update_callback' => null,\n          'schema'          => null,\n        )\n    );\n}\nfunction np_get_extra_field( $object, $field_name, $request ) {\n    return 'foobar';\n}\n\n\n\n\nFor media, this does not work, so far I've tried like this :\n\n\n\n  add_action( 'rest_api_init', 'np_register_extra_field' );\n  function np_register_extra_field() {\n      register_rest_field( 'media',\n          'extra_media_field',\n          array(\n              'get_callback'    => 'np_get_extra_field',\n              'update_callback' => null,\n              'schema'          => null,\n          )\n      );\n  }\n  function np_get_extra_field( $object, $field_name, $request ) {\n    return 'foobar';\n  }\n\n\n\n\nI also tried 'hooking' into other filters (is that a correct way to say that ?)\n\n\n\nadd_action( 'rest_media_query', 'np_register_extra_field' );\nadd_action( 'rest_pre_insert_media', 'np_register_extra_field' );\nadd_action( 'rest_prepare_attachment', 'np_register_extra_field' );\n\n\n\n\nNone of those seems to do the trick.\n\n\n\nthe endgoal is to add the field 'srcset' to the media response\n\n\n\nUsing \n\n\n\nwp json api : Version 2.0-beta12\n\n\n\nwordrpess : version 4.4.2\n\n\n\nAny help would be appreciated.\n\n    ", "Tag": "算法分析"}
{"Answer": "\n使用numpy.array()函数创建三维数组：\nimport numpy as np\na = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nprint(a.shape) # (2, 2, 3)\nprint(a)\n", "Konwledge_Point": "应对NP完全问题", "Question": "关于#python#的问题，请各位专家解答！\n\n{X = np.array(\n    \n[\n[4,3,0, 2]\n, \n[5,2,2, 0]\n, \n[3,4,4.5, 0]\n, \n[9,6,4.5, 3]\n, \n[2,7,3, 4.5]\n, \n[4,2,2, 1]\n,\n     \n[-7,-3,-3, -1]\n, \n[5,2,4, 6]\n, \n[-3,-5,-4, 0]\n]\n)}\n\n\n\n这产生的应该是三维数据了吧？", "Tag": "算法分析"}
{"Answer": "你把11行的image打印出来看看，这里有问题", "Konwledge_Point": "应对NP完全问题", "Question": "TypeError: 'NoneType' object is not subscriptable这个错误如何解决呀\n\n\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\n\n\ndef conv(image, kernel, mode='same'):\n    if mode == 'fill':  # 选择是否进行边缘填充\n        h = kernel.shape[0] // 2  # 卷积核的列除以2取整\n        w = kernel.shape[1] // 2  # 卷积核的行除以2取整\n        # 在原始图像边缘进行填充，常数填充，填数值0，假设原始图像600\n600，卷积核大小5\n5，则填充后图像大小604*604\n        img = np.pad(image, ((h, h), (w, w), (0, 0)), 'constant')\n\n\n    # 进行卷积运算\nconv_b = \n_convolve(\nimage\n[:, :, 0], \nkernel\n)\n\nconv_g = \n_convolve(\nimage\n[:, :, 1], \nkernel\n)\n\nconv_r = \n_convolve(\nimage\n[:, :, 2], \nkernel\n)\n\nres = np.dstack(\n[\nconv_b\n, \nconv_g\n, \nconv_r\n]\n)\nreturn res\n\n\n\ndef _convolve(image, kernel):\n    h_kernel, w_kernel = kernel.shape  # 获取卷积核的长宽，也就是行数和列数\n\n\nh_image, w_image = image.\nshape \n \n# 获取欲处理图片的长宽\n\n\n\n# 计算卷积核中心点开始运动的点，因为图片边缘不能为空\n\nres_h = h_image - h_kernel + \n1\n\nres_w = w_image - w_kernel + \n1\n\n\n\n# 生成一个0矩阵，用于保存处理后的图片\n\nres = np.zeros((res_h, res_w), np.uint8)\n\nfor i in range(res_h):  \n# 行\n\n    for \nj \nin range(res_w):  \n# 列\n\n        \n# image处传入的是一个与卷积核一样大小矩阵，这个矩阵取自于欲处理图片的一部分\n\n        \n# 这个矩阵与卷核进行运算，用i与j来进行卷积核滑动\n\n        res[i, \nj] \n= \nnormal(image[i:i \n+ h_kernel, \nj:j \n+ w_kernel], kernel)\n\nreturn res\n\n\n\ndef normal(image, kernel):\n    # np.multiply()函数：数组和矩阵对应位置相乘，输出与相乘数组/矩阵的大小一致（点对点相乘）\n\n\nres\n = np.multiply(image, kernel).sum()  # 点对点相乘后进行累加\n\nif\n \nres\n > \n255\n:\n    \nreturn\n \n255\n\nelif \nres\n < \n0\n:\n    \nreturn\n \n0\n\n\nelse\n:\n    \nreturn\n \nres\n\n\n\n\nif \nname\n == '\nmain\n':\n    path = 'qie原图.jpg   '  # 原图像路径\n    image = cv2.imread(path)\n\n\n# kernel1 是一个\n3\nx3的边缘特征提取器，可以提取各个方向上的边缘\n# kernel2 是一个\n5\nx5的浮雕特征提取器。\n\nkernel1 = np.array([\n    [\n1\n, \n1\n, \n1\n],\n    [\n1\n, \n-7.5\n, \n1\n],\n    [\n1\n, \n1\n, \n1\n]\n])\nkernel2 = np.array([[\n-1\n, \n-1\n, \n-1\n, \n-1\n, \n0\n],\n                    [\n-1\n, \n-1\n, \n-1\n, \n0\n, \n1\n],\n                    [\n-1\n, \n-1\n, \n0\n, \n1\n, \n1\n],\n                    [\n-1\n, \n0\n, \n1\n, \n1\n, \n1\n],\n                    [\n0\n, \n1\n, \n1\n, \n1\n, \n1\n]])\nres = conv(image, kernel2, \n'same'\n)\nplt.axis(\n'off'\n)  # 画图不显示坐标轴\nplt.imshow(res)\nplt.savefig(\n'./out/filtered_picdoramon01.jpg'\n, dpi=\n600\n)\nplt.show()\n\n\n\n运行中出现了错误\n谢谢大家", "Tag": "算法分析"}
{"Answer": "缩进？你缩进不对的话就会导致你的str——encode是空的，这也就是报错里面提示你说图片为空不能imshow", "Konwledge_Point": "应对NP完全问题", "Question": "python+opencv实现文件的读取\n 想实现局域网下不同设备的视频流通信，决定采用socket实现，由于socket无法直接实现图片通讯，采用将编码的字符串写入文件再读取写出拿来模拟socket通信，在解码端代码一直报bug，请求指点\n\n\n\n\n编码段\n\n\nimport\n cv2\n\nimport\n win32gui \n\nfrom\n PIL \nimport\n ImageGrab\n\nimport\n numpy \nas\n np\n\nim = cv2.imread(\n'bus.jpg'\n)\nimg_encode = cv2.imencode(\n'.jpg'\n, im，[cv.IMWRITE_JPEG_QUALITY, \n50\n])[\n1\n]\ndata_encode = np.array(img_encode)\nimage_binary = data_encode.tobytes()  # transform bytes stream\n#send_data = \n\"send\"\n.encode(\n'utf-8'\n) + (len(image_binary)).to_bytes(\n4\n, byteorder=\n'little'\n) + image_binary\nsend_data=str(image_binary)\n\n\nwith\n open(\n'img_encode.txt'\n, \n'w'\n) \nas\n f:\n    f.write(send_data)\n    f.flush\n\n\n\n\n解码端\n\n\nimport\n cv2\n\nimport\n numpy \nas\n np\n\n\n\nwith\n \nopen\n(\n'img_encode.txt'\n, \n'r'\n) \nas\n f:\n    str_encode = f.read()\n\nstr_encode = \nbytes\n(str_encode,encoding=\n'utf-8'\n)\nimage_np = np.frombuffer(str_encode, np.uint8)  \n# transform image to np.uint8 format\n\nimage = cv2.imdecode(image_np, cv2.IMREAD_COLOR)  \n# decode image to the type of opencv\n\ncv2.imshow(\n\"img_decode\"\n,image)\ncv2.waitKey(\n0\n)\n\n\n\n\n\n\n报错内容\n\n\nTraceback (most recent \ncall\n \nlast\n):\n  File \n\"C:\\Users\\pc\\Desktop\\new\\decode.py\"\n, \nline\n \n11\n, in \n\n\n    cv2.imshow(\n\"img_decode\"\n,image)\ncv2.error: OpenCV(\n4.5\n.\n4\n) D:\\\na\n\\opencv-\npython\n\\opencv-\npython\n\\opencv\\modules\\imgproc\\src\\color.cpp:\n182\n: error: (-\n215\n:Assertion failed) !_src.\nempty\n() in \nfunction\n \n'cv::cvtColor'\n\n", "Tag": "算法分析"}
{"Answer": "因为第一个数不是1，只是因为print显示的小数点位数不够约成了1，你用int转类型直接截取的整数部分0。\nprint((M*I)[0,0])\n0.9999999999999986\n", "Konwledge_Point": "应对NP完全问题", "Question": "浮点数转整型的疑惑，单位矩阵\n问题遇到的现象和发生背景\n\n\n\n\nM\n=\nnp.mat(\n\"1 22 44 5;1 0 -3 6;-9 -3 8 1;1 3 4 9\"\n)\n\nI\n=\nnp.linalg.inv(M)\n\n\n\n问题相关代码，请勿粘贴截图\n\n\n运行结果及报错内容\n\n\n现在的疑惑是转成整型的时候，为什么结果是\n(M*I).astype(int)\nmatrix([[0, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 0, 0],\n        [0, 0, 0, 0]])\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果\n\n\n正确结果应该是\narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n这样才对啊，请问大家是哪里出了问题", "Tag": "算法分析"}
{"Answer": "关于该问题，我找了一篇非常好的博客，你可以看看是否有帮助，链接：初步理解hyperopt源码", "Konwledge_Point": "应对NP完全问题", "Question": "关于HyperOpt的分类使用问题\n我想用Hyperopt进行数据的分类，但是遇见一个错误。\n\n\nfrom\n hpsklearn import HyperoptEstimator\nimport pandas as pd\n\n# 加载训练数据\n\ntrain = pd.read_csv(\n'train.csv'\n, \nindex_col\n=\n'id'\n)\n\n# 加载验证数据\n\ntest = pd.read_csv(\n'test.csv'\n, \nindex_col\n=\n'id'\n)\n\nx_train\n=train.loc[:,\n\"fixed acidity\"\n:\n\"alcohol\"\n]\n\ny_train\n=train.loc[:,\n\"quality\"\n]\n\nx_test\n=test.loc[:,\n\"fixed acidity\"\n:\n\"alcohol\"\n]\n\ny_test\n=test.loc[:,\n\"quality\"\n]\n\nimport numpy as np\n\n# Load Data\n\n\n# ...\n\n\n\n# Create the estimator object\n\nestim = HyperoptEstimator()\n\n\n# Search the space of classifiers and preprocessing steps and their\n\n\n# respective hyperparameters in sklearn to fit a model to the data\n\n\nestim.fit(x_train,y_train)\n\n\n# Make a prediction using the optimized model\n\nprediction = estim.predict(y_train)\n\n\n# Report the accuracy of the classifier on a given set of data\n\nscore = estim.score(x_test, y_test)\n\n\n# Return instances of the classifier and preprocessing steps\n\nmodel = estim.best_model()\n\n\n\n问题出在(\n\n\n\nestim.fit(x_train,y_train)\n\n```)\n这串代码上\n\n\n显示的错误为\n\n'numpy.random.mtrand.RandomState'\n \nobject\n has \nno\n \nattribute\n \n'integers'\n\n不知道该怎么办，我搜索网上说是numpy版本不对，我尝试下载\n1.14\n.5\n版本numpy失败了，请问还有别的办法避免这个错误吗？数据集用的红酒数据集，或者大佬们有别的写代码方式用HyperOp方法跑出来吗？\n", "Tag": "算法分析"}
{"Answer": "你代码的第11行漏写了df，你直接给df1赋值了一个list，修改后的完整代码如下\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.rc(\"font\",family=\"SimHei\",size=\"15\")  #解决中文乱码问题\nexcelFile=r'E:\\计算机大作业\\nmicrobiology图表复现\\COPD_multiomics-main\\COPD_multiomics-main\\6-Figure scripts\\Fig E2 Source Data.xlsx'\ndf=pd.DataFrame(pd.read_excel(excelFile,sheet_name='Bacterial load'))\ndf1=df[['Group_Site','log10cpn']]\ndf2=df1.loc[df1['Group_Site']=='Guangzhou_C']\nprint(df2)\n", "Konwledge_Point": "应对NP完全问题", "Question": "关于#AttributeError#的问题，如何解决？\nAttributeError: 'list' object has no attribute 'loc'怎么解决\n\n\n%matplotlib inline\n\nimport\n pandas \nas\n pd\n\nimport\n numpy \nas\n np\n\nimport\n seaborn \nas\n sns\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n warnings\nwarnings.filterwarnings(\n'ignore'\n)\nplt.rc(\n\"font\"\n,family=\n\"SimHei\"\n,size=\n\"15\"\n)  \n#解决中文乱码问题\n\nexcelFile=\nr'E:\\计算机大作业\\nmicrobiology图表复现\\COPD_multiomics-main\\COPD_multiomics-main\\6-Figure scripts\\Fig E2 Source Data.xlsx'\n\ndf=pd.DataFrame(pd.read_excel(excelFile,sheet_name=\n'Bacterial load'\n))\ndf1=[[\n'Group_Site'\n,\n'log10cpn'\n]]\ndf2=df1.loc[df1[\n'Group_Site'\n]==\n'Guangzhou_C'\n]\n\nprint\n(df2)\n\n\n\n\n报错：AttributeError                            Traceback (most recent call last)\nCell In[12], line 12\n     10 df=pd.DataFrame(pd.read_excel(excelFile,sheet_name='Bacterial load'))\n     11 df1=[['Group_Site','log10cpn']]\n   -> 12 df2=df1.loc[df1['Group_Site']=='Guangzhou_C']\n     13 print(df2)\n\n\nAttributeError: 'list' object has no attribute 'loc'", "Tag": "算法分析"}
{"Answer": "numpy的T，表示转置矩阵，最简单的就是列向量变行向量", "Konwledge_Point": "应对NP完全问题", "Question": "请问这句代码中T是什么意思\n\nxx = 223.iloc[np.r_[3480:3660, 7140:7320, 10800:10980]].T.corr()\n\n\n\n解释一下iloc函数读np函数连接的行数据做相关性检验，不知道为什么会有个大写字母T", "Tag": "算法分析"}
{"Answer": "值错误:包含一个以上元素的数组的真值是不明确的，要使用a.any()或a.all()183行改成    if ((x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10) / 0.6 + (x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20) / 0.66 + (x21 + x22 + x23 + x24 + x25 + x26 + x27 + x28 + x29 + x30) / 0.72 <= 56400).all():", "Konwledge_Point": "应对NP完全问题", "Question": "python遗传算法\n\n\nimport\n numpy as np\n\n\nDNA_size\n = \n24\n\n\npops\n = \n100\n\n\njiaohuan\n = \n0\n.\n7\n\n\nbianyi\n = \n0\n.\n03\n\n\ngen\n = \n100\n\n\n# x_bound=[-3,3]\n\n\n# y_bound=[-3,3]\n\n\nx1_bound\n =\n [0, 3147]\n\n\nx2_bound\n =\n [0, 30977]\n\n\nx3_bound\n =\n [0, 1724]\n\n\nx4_bound\n =\n [0, 966]\n\n\nx5_bound\n =\n [0, 971]\n\n\nx6_bound\n =\n [0, 7661]\n\n\nx7_bound\n =\n [0, 9385]\n\n\nx8_bound\n =\n [0, 2521]\n\n\nx9_bound\n =\n [0, 699]\n\n\nx10_bound\n =\n [0, 36972]\n\n\nx11_bound\n =\n [0, 7885]\n\n\nx12_bound\n =\n [0, 10207]\n\n\nx13_bound\n =\n [0, 1181]\n\n\nx14_bound\n =\n [0, 9768]\n\n\nx15_bound\n =\n [0, 8181]\n\n\nx16_bound\n =\n [0, 1014]\n\n\nx17_bound\n =\n [0, 21293]\n\n\nx18_bound\n =\n [0, 2081]\n\n\nx19_bound\n =\n [0, 2816]\n\n\nx20_bound\n =\n [0, 21267]\n\n\nx21_bound\n =\n [0, 1788]\n\n\nx22_bound\n =\n [0, 736]\n\n\nx23_bound\n =\n [0, 922]\n\n\nx24_bound\n =\n [0, 595]\n\n\nx25_bound\n =\n [0, 15114]\n\n\nx26_bound\n =\n [0, 23695]\n\n\nx27_bound\n =\n [0, 5398]\n\n\nx28_bound\n =\n [0, 342]\n\n\nx29_bound\n =\n [0, 2005]\n\n\nx30_bound\n =\n [0, 381]\n\n\n\n\ndef\n fun(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24,\n        \nx25\n, x26, x27, x28, x29, x30):\n    \nreturn\n \n1\n.\n2\n * (x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10) + \n1\n.\n1\n * (\n            \nx11\n + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20) + (\n                   \nx21\n + x22 + x23 + x24 + x25 + x26 + x27 + x28 + x29 + x30)\n\n\n\ndef\n get_fitness(pop):\n    \nx1\n, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30 = trans(\n        \npop\n)\n    \npred\n = fun(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23,\n               \nx24\n, x25, x26, x27, x28, x29, x30)\n    \nreturn\n pred\n\n\n\ndef\n trans(pop):\n    \nx1_pop\n = pop[:, \n0\n:DNA_size]\n    \nx2_pop\n = pop[:, \n0\n:DNA_size]\n    \nx3_pop\n = pop[:, \n0\n:DNA_size]\n    \nx4_pop\n = pop[:, \n0\n:DNA_size]\n    \nx5_pop\n = pop[:, \n0\n:DNA_size]\n    \nx6_pop\n = pop[:, \n0\n:DNA_size]\n    \nx7_pop\n = pop[:, \n0\n:DNA_size]\n    \nx8_pop\n = pop[:, \n0\n:DNA_size]\n    \nx9_pop\n = pop[:, \n0\n:DNA_size]\n    \nx10_pop\n = pop[:, \n0\n:DNA_size]\n    \nx11_pop\n = pop[:, \n0\n:DNA_size]\n    \nx12_pop\n = pop[:, \n0\n:DNA_size]\n    \nx13_pop\n = pop[:, \n0\n:DNA_size]\n    \nx14_pop\n = pop[:, \n0\n:DNA_size]\n    \nx15_pop\n = pop[:, \n0\n:DNA_size]\n    \nx16_pop\n = pop[:, \n0\n:DNA_size]\n    \nx17_pop\n = pop[:, \n0\n:DNA_size]\n    \nx18_pop\n = pop[:, \n0\n:DNA_size]\n    \nx19_pop\n = pop[:, \n0\n:DNA_size]\n    \nx20_pop\n = pop[:, \n0\n:DNA_size]\n    \nx21_pop\n = pop[:, \n0\n:DNA_size]\n    \nx22_pop\n = pop[:, \n0\n:DNA_size]\n    \nx23_pop\n = pop[:, \n0\n:DNA_size]\n    \nx24_pop\n = pop[:, \n0\n:DNA_size]\n    \nx25_pop\n = pop[:, \n0\n:DNA_size]\n    \nx26_pop\n = pop[:, \n0\n:DNA_size]\n    \nx27_pop\n = pop[:, \n0\n:DNA_size]\n    \nx28_pop\n = pop[:, \n0\n:DNA_size]\n    \nx29_pop\n = pop[:, \n0\n:DNA_size]\n    \nx30_pop\n = pop[:, \n0\n:DNA_size]\n\n    \nx1\n = x1_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x1_bound[\n1\n] - x1_bound[\n0\n]) + x1_bound[\n        \n0\n]\n    \nx2\n = x2_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x2_bound[\n1\n] - x2_bound[\n0\n]) + x2_bound[\n        \n0\n]\n    \nx3\n = x3_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x3_bound[\n1\n] - x3_bound[\n0\n]) + x3_bound[\n        \n0\n]\n    \nx4\n = x4_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x4_bound[\n1\n] - x4_bound[\n0\n]) + x4_bound[\n        \n0\n]\n    \nx5\n = x5_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x5_bound[\n1\n] - x5_bound[\n0\n]) + x5_bound[\n        \n0\n]\n    \nx6\n = x6_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x6_bound[\n1\n] - x6_bound[\n0\n]) + x6_bound[\n        \n0\n]\n    \nx7\n = x7_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x7_bound[\n1\n] - x7_bound[\n0\n]) + x7_bound[\n        \n0\n]\n    \nx8\n = x8_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x8_bound[\n1\n] - x8_bound[\n0\n]) + x8_bound[\n        \n0\n]\n    \nx9\n = x9_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x9_bound[\n1\n] - x9_bound[\n0\n]) + x9_bound[\n        \n0\n]\n    \nx10\n = x10_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x10_bound[\n1\n] - x10_bound[\n0\n]) + \\\n          \nx10_bound\n[\n0\n]\n    \nx11\n = x11_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x11_bound[\n1\n] - x11_bound[\n0\n]) + \\\n          \nx11_bound\n[\n0\n]\n    \nx12\n = x12_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x12_bound[\n1\n] - x12_bound[\n0\n]) + \\\n          \nx12_bound\n[\n0\n]\n    \nx13\n = x13_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x13_bound[\n1\n] - x13_bound[\n0\n]) + \\\n          \nx13_bound\n[\n0\n]\n    \nx14\n = x14_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x14_bound[\n1\n] - x14_bound[\n0\n]) + \\\n          \nx14_bound\n[\n0\n]\n    \nx15\n = x15_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x15_bound[\n1\n] - x15_bound[\n0\n]) + \\\n          \nx15_bound\n[\n0\n]\n    \nx16\n = x16_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x16_bound[\n1\n] - x16_bound[\n0\n]) + \\\n          \nx16_bound\n[\n0\n]\n    \nx17\n = x17_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x17_bound[\n1\n] - x17_bound[\n0\n]) + \\\n          \nx17_bound\n[\n0\n]\n    \nx18\n = x18_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x18_bound[\n1\n] - x18_bound[\n0\n]) + \\\n          \nx18_bound\n[\n0\n]\n    \nx19\n = x19_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x19_bound[\n1\n] - x19_bound[\n0\n]) + \\\n          \nx19_bound\n[\n0\n]\n    \nx20\n = x20_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x20_bound[\n1\n] - x20_bound[\n0\n]) + \\\n          \nx20_bound\n[\n0\n]\n    \nx21\n = x21_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x21_bound[\n1\n] - x21_bound[\n0\n]) + \\\n          \nx21_bound\n[\n0\n]\n    \nx22\n = x22_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x22_bound[\n1\n] - x22_bound[\n0\n]) + \\\n          \nx22_bound\n[\n0\n]\n    \nx23\n = x23_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x23_bound[\n1\n] - x23_bound[\n0\n]) + \\\n          \nx23_bound\n[\n0\n]\n    \nx24\n = x24_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x24_bound[\n1\n] - x24_bound[\n0\n]) + \\\n          \nx24_bound\n[\n0\n]\n    \nx25\n = x25_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x25_bound[\n1\n] - x25_bound[\n0\n]) + \\\n          \nx25_bound\n[\n0\n]\n    \nx26\n = x26_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x26_bound[\n1\n] - x26_bound[\n0\n]) + \\\n          \nx26_bound\n[\n0\n]\n    \nx27\n = x27_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x27_bound[\n1\n] - x27_bound[\n0\n]) + \\\n          \nx27_bound\n[\n0\n]\n    \nx28\n = x28_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x28_bound[\n1\n] - x28_bound[\n0\n]) + \\\n          \nx28_bound\n[\n0\n]\n    \nx29\n = x29_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x29_bound[\n1\n] - x29_bound[\n0\n]) + \\\n          \nx29_bound\n[\n0\n]\n    \nx30\n = x30_pop.dot(\n2\n ** np.arange(DNA_size)[::-\n1\n]) / float(\n2\n ** DNA_size - \n1\n) * (x30_bound[\n1\n] - x30_bound[\n0\n]) + \\\n          \nx30_bound\n[\n0\n]\n\n    \nreturn\n x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30\n\n\n\ndef\n crossover(pop):\n    \nnew_pop\n =\n []\n\n    \npred\n = get_fitness(pop)\n    \nindex\n = np.argmax(pred)\n    \nindex2\n = np.argmin(pred)\n    \nfor\n father in pop:\n        \nif\n (father == pop[index2]).\nall\n() == True:\n            \nchild\n = pop[index]\n        \nelse\n:\n            \nchild\n = father\n        \nif\n np.random.rand() < jiaohuan:\n            \nmother\n = pop[np.random.randint(pops)]\n            \ncross_point\n = np.random.randint(low=\n0\n, high=DNA_size * \n2\n)\n            \nchild\n[cross_point:] = mother[cross_point:]\n        \nchild\n = mutation(child)\n        \nnew_pop\n.append(child)\n    \nreturn\n new_pop\n\n\n\ndef\n mutation(child):\n    \nif\n np.random.rand() < bianyi:\n        \nmutate_point\n = np.random.randint(\n0\n, DNA_size)\n        \nchild\n[mutate_point] = child[mutate_point] ^ \n1\n\n    \nreturn\n child\n\n\n\ndef\n select(pop, fitness):\n    \nx1\n, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30 = trans(\n        \npop\n)\n    \nif\n (x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10) / \n0\n.\n6\n + (\n            \nx11\n + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20) / \n0\n.\n66\n + (\n            \nx21\n + x22 + x23 + x24 + x25 + x26 + x27 + x28 + x29 + x30) / \n0\n.\n72\n <= \n56400\n:\n        \nif\n (x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10) <= \n4642\n:\n            \nif\n (x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20) <= \n23833\n:\n                \nif\n (x21 + x22 + x23 + x24 + x25 + x26 + x27 + x28 + x29 + x30) <= \n8155\n:\n                    \nidx\n = np.random.choice(np.arange(pops), size=pops, replace=True,\n                                           \np\n=(fitness) / (fitness.sum()))\n                    \nreturn\n pop[idx]\n\n\n\ndef\n print_info(pop):\n    \nfitness\n = get_fitness(pop)\n    \nmax_index\n = np.argmax(fitness)\n    \nprint\n(\n\"最优解：\"\n, fitness[max_index])\n    \nx1\n, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30 = trans(\n        \npop\n)\n    \nprint\n(\n\"最优基因型：\"\n, pop[max_index])\n    \nprint\n(\n\"最优解的x,y :\"\n, x1[max_index], x2[max_index], x3[max_index], x4[max_index], x5[max_index], x6[max_index],\n          \nx7\n[max_index], x8[max_index], x9[max_index], x10[max_index], x11[max_index], x12[max_index], x13[max_index],\n          \nx14\n[max_index], x15[max_index], x16[max_index], x17[max_index], x18[max_index], x19[max_index],\n          \nx20\n[max_index], x21[max_index], x22[max_index], x23[max_index], x24[max_index], x25[max_index],\n          \nx26\n[max_index], x27[max_index], x28[max_index], x29[max_index], x30[max_index])\n\n\n\nif\n __name__ == \n\"__main__\"\n:\n    \npop\n = np.random.randint(\n2\n, size=(pops, DNA_size * \n2\n))\n    \nfitness\n = get_fitness(pop)\n    \nfor\n i in range(gen):\n        \npop\n = np.array(crossover(pop))\n        \nfitness\n = get_fitness(pop)\n        \npop\n = select(pop, fitness)\n    \nprint_info\n(pop)\n\n\n\n\n我想加入约束条件，是应该加到select函数里吗\n\n\n\n\n为什么会报这样的错啊\n\n", "Tag": "算法分析"}
{"Answer": "采用np.isnan()方法\r\n\r\n```\r\nIn [3]: import pandas as pd  \r\n\r\nIn [4]: import numpy as np   \r\n\r\nIn [5]: a_df = pd.DataFrame([[1, 3, np.nan], [3, 4, 5]]) \r\n\r\nIn [6]: a_df  \r\nOut[6]: \r\n   0  1    2\r\n0  1  3  NaN\r\n1  3  4  5.0\r\n\r\nIn [7]: a_df.iloc[0, 2]  \r\nOut[7]: nan\r\n\r\nIn [8]: np.isnan(a_df.iloc[0,2])   \r\nOut[8]: True\r\n\r\nIn [9]: a_df.iloc[0, 1]\r\nOut[9]: 3\r\n\r\nIn [10]: np.isnan(a_df.iloc[0,1])\r\nOut[10]: False\r\n\r\n```", "Konwledge_Point": "应对NP完全问题", "Question": "数据显示是NaN，却不能通过判断是否是nan来选中\n数据在数据框中显示为NaN，输出结果也是nan，却无法根据is np.nan选中。\n\n\n\ninput:data.iloc[1,:].mission_complete\noutput: nan\ninput: data.iloc[1,:].mission_complete == np.nan\noutput: False\ninput: data.iloc[1,:].mission_complete is np.nan\noutput:False\ninput: data.iloc[1,:].mission_complete == 'nan'\noutput: False\n\n\n\n\n\n我想要做的是，能够根据根据是否是NaN值来筛选：\n\n\n\ndata['if_mission_complete'] = data.apply(lambda x: 0 if x['mission_complete'] is np.nan or x['mission_complete'] >= 0.9 else 1, axis=1)\n\n\n\n\n但是因为选中nan的值为False导致输出结果不正确。", "Tag": "算法分析"}
{"Answer": "把你的第7行代码，data.files删除\n望采纳", "Konwledge_Point": "应对NP完全问题", "Question": "Python这是哪里的错误啊？\n\n\n\n\n#建立模型\n\n\nimport\n numpy \nas\n np\n\nimport\n tensorflow \nas\n tf\n\n#读取数据\n\ndata = np.load(\n'mnist.npz'\n)\ndata.files\ntrain_images, train_labels, test_images, test_labels = data[\n'x_train'\n], data[\n'y_train'\n], data[\n'x_test'\n],\ndata[\n'y_test'\n]\n\nprint\n(train_images.shape)\n\nprint\n(train_labels.shape)\n\nprint\n(test_images.shape)\n\nprint\n(test_labels.shape)\n\n#交叉熵\n\ntarget_y = np.array([\n1\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n])\npredicted_y1 = np.array([\n0.4\n, \n0.5\n, \n0.1\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n])\npredicted_y2 = np.array([\n0.1\n, \n0.2\n, \n0.7\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n, \n0\n])\n-np.\nsum\n( target_y * np.log(predicted_y1+\n0.0000001\n))\n-np.\nsum\n( target_y * np.log(predicted_y2+\n0.0000001\n))\n\n#搭建网络结构\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Flatten(input_shape=(\n28\n,\n28\n)))\nmodel.add(tf.keras.layers.Dense(\n128\n, activation = \n'relu'\n))\nmodel.add(tf.keras.layers.Dense(\n10\n, activation = \n'softmax'\n))\n\n#编译模型\n\nmodel.\ncompile\n(optimizer = \n'adam'\n, loss = \n'sparse_categorical_crossentropy'\n, metrics = [\n'accuracy'\n])\n\n#训练\n\nmodel.fit(train_images, train_labels, verbose = \n1\n, epochs = \n20\n, validation_data = (test_images,\ntest_labels))\n\n#模型保存\n\nmodel.save(\n'model_mnist.h5'\n)\n\n#用模型进行预测\n\n\nimport\n tensorflow \nas\n tf\n\nimport\n matplotlib.pyplot \nas\n plt\nmodel = tf.keras.models.load_model(\n'model_mnist.h5'\n)\nmodel.summary()\n\nfor\n i \nin\n \nrange\n(\n30\n):\n    image = plt.imread(\n'testimages/'\n + \nstr\n(i) + \n'.jpg'\n)\n    image_new = image.reshape([\n1\n, \n28\n, \n28\n])\n    result = model.predict(image_new)[\n0\n].argmax()\n    \nprint\n(\n'The'\n, i + \n1\n, \n'th picture shows:'\n, result)\n\n", "Tag": "算法分析"}
{"Answer": "试试看 pip install keras==1.2.2\r\n或者\r\npip install --upgrade --user keras\r\n\r\nkeras分为1和2两个版本np_utils和你的不兼容", "Konwledge_Point": "应对NP完全问题", "Question": "python2.7安装keras时报错，什么原因呢？\n安装Keras时出错，np__utils是什么模块？\n\n\n\n Traceback (most recent call last):\n  File \"\", line 1, in \n    import keras\n  File \"C:\\Python27\\lib\\site-packages\\keras\\__init__.py\", line 3, in \n    from . import utils\n  File \"C:\\Python27\\lib\\site-packages\\keras\\utils\\__init__.py\", line 2, in \n    from . import np_utils\nImportError: cannot import name np_utils\n", "Tag": "算法分析"}
{"Answer": "iloc方法是使用位置进行索引，参数为位置。pandas自带的索引，也是可以的。只是，以该例为例，写为，z[a][b]形式，其中对DataFrame使用单个索引z[a]，a必须是列名，不能是索引值。z[a]得到一个Series。然后再对这个Series进行索引，该索引的方式默认优先标签索引，如果标签不存在，则尝试位置索引。相比使用loc和iloc这样稍有麻烦。", "Konwledge_Point": "应对NP完全问题", "Question": "pandas中iloc函数的使用\n\nimport numpy as np\nimport matplotlib\n.pyplot\n as plt\nimport pandas as pd\n\nx = np\n.linspace\n(\n0\n, \n50\n*\n873\n, \n874\n)\ny = np\n.linspace\n(\n0\n, \n50\n*\n1164\n, \n1165\n)\nz = np\n.zeros\n((\n1165\n, \n874\n)) \n\na\n = pd\n.read_excel\n(\n'程序及数据\\\\程序及数据\\\\02第2章  数据处理与可视化\\\\附件1：区域高程数据.xlsx'\n, sheet_name=\n'Sheet1'\n, header=None)\n\nb\n = \na\n.drop\n(\n[874, 875]\n, axis=\n0\n)\n\nprint\n(b[\n1\n][\n1\n])\n\n\nfor\n \ni\n \nin\n range(\n0\n, \n874\n):\n    \nfor\n j \nin\n range(\n0\n, \n1165\n):\n        z\n[j, i]\n = int(\nb\n.iloc\n[i, j]\n)\n\nax = plt\n.axes\n(projection=\n'3d'\n)\nX, Y = np\n.meshgrid\n(x, y)\nax\n.plot_surface\n(X, Y, z, cmap=\n'viridis'\n)\nplt\n.show\n()\n\n\n\n\n请问\n\n\n\nz\n[j, i]\n = int(\nb\n.iloc\n[i, j]\n)\n\n\n\n将b中的值赋给z矩阵为什么必须使用iloc（）函数 而不能用pandas自带的索引", "Tag": "算法分析"}
{"Answer": "分类：\nkm = KMeans(n_clusters=4)\nkm.fit_predict(data)\n\n分完类之后得到了聚类中心，也就是km.cluster_centers_，你可以print出来：\n[[1525.81533333  478.672       322.88266667  232.4         236.41866667\n   457.53133333  344.81866667  190.21933333]\n [2549.658       582.118       488.366       268.998       397.442\n   618.92        477.946       295.172     ]\n [2004.785       429.48        347.8925      190.955       287.66625\n   581.16125     437.2375      233.09625   ]\n [3242.22333333  544.92        735.78        405.51333333  602.25\n  1016.62        760.52333333  446.82666667]]\n\n至于之后的np.sum是对聚类中心的8个值进行了求和，axis是用来指定行还是列的，求和的结果是expenses=[3788.758      5678.62       4512.27375    7754.65666667]聚类还是按照八个特征来聚类的，分为四类，只不过最后对四个聚类的聚类中心的值消费水平进行求和", "Konwledge_Point": "应对NP完全问题", "Question": "Kmeans聚类算法应用问题，八维数据的分类\n这是我看到别人写的根据消费水平划分城市的代码，我理解K-means的作用应该是根据数据八个维度的表现，去划分出4个比较相似的类，但是这个作者在代码中指定了np.sum(),是按照总分高低去进行划分了吗？而且用np.sum()和np.average()的分类结果并不一样，如果np.average()是按平均分高低去划分，那python里的k-means应用解决不就只是计算量问题了吗？如何实现自动迭代聚类呢？\n\n\n原始数据：\n北京,2959.19,730.79,749.41,513.34,467.87,1141.82,478.42,457.64\n天津,2459.77,495.47,697.33,302.87,284.19,735.97,570.84,305.08\n河北,1495.63,515.90,362.37,285.32,272.95,540.58,364.91,188.63\n山西,1406.33,477.77,290.15,208.57,201.50,414.72,281.84,212.10\n内蒙古,1303.97,524.29,254.83,192.17,249.81,463.09,287.87,192.96\n辽宁,1730.84,553.90,246.91,279.81,239.18,445.20,330.24,163.86\n吉林,1561.86,492.42,200.49,218.36,220.69,459.62,360.48,147.76\n黑龙江,1410.11,510.71,211.88,277.11,224.65,376.82,317.61,152.85\n上海,3712.31,550.74,893.37,346.93,527.00,1034.98,720.33,462.03\n江苏,2207.58,449.37,572.40,211.92,302.09,585.23,429.77,252.54\n浙江,2629.16,557.32,689.73,435.69,514.66,795.87,575.76,323.36\n安徽,1844.78,430.29,271.28,126.33,250.56,513.18,314.00,151.39\n福建,2709.46,428.11,334.12,160.77,405.14,461.67,535.13,232.29\n江西,1563.78,303.65,233.81,107.90,209.70,393.99,509.39,160.12\n山东,1675.75,613.32,550.71,219.79,272.59,599.43,371.62,211.84\n河南,1427.65,431.79,288.55,208.14,217.00,337.76,421.31,165.32\n湖南,1942.23,512.27,401.39,206.06,321.29,697.22,492.60,226.45\n湖北,1783.43,511.88,282.84,201.01,237.60,617.74,523.52,182.52\n广东,3055.17,353.23,564.56,356.27,811.88,873.06,1082.82,420.81\n广西,2033.87,300.82,338.65,157.78,329.06,621.74,587.02,218.27\n海南,2057.86,186.44,202.72,171.79,329.65,477.17,312.93,279.19\n重庆,2303.29,589.99,516.21,236.55,403.92,730.05,438.41,225.80\n四川,1974.28,507.76,344.79,203.21,240.24,575.10,430.36,223.46\n贵州,1673.82,437.75,461.61,153.32,254.66,445.59,346.11,191.48\n云南,2194.25,537.01,369.07,249.54,290.84,561.91,407.70,330.95\n西藏,2646.61,839.70,204.44,209.11,379.30,371.04,269.59,389.33\n陕西,1472.95,390.89,447.95,259.51,230.61,490.90,469.10,191.34\n甘肃,1525.57,472.98,328.90,219.86,206.65,449.69,249.66,228.19\n青海,1654.69,437.77,258.78,303.00,244.93,479.53,288.56,236.51\n宁夏,1375.46,480.89,273.84,317.32,251.08,424.75,228.73,195.93\n新疆,1608.82,536.05,432.46,235.82,250.28,541.30,344.85,214.40\n\n\n版权声明：本文为CSDN博主「L_imbo」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n原文链接：\n\n      \n        \n机器学习——K-Means聚类算法及其应用_L_imbo的博客-CSDN博客\n\n        \n      \n概括首先说一下聚类，多用于机器学习中的无监督学习，通俗来说是将具有相似性的数据分为多类（在相似的基础上收集数据来分类）。由聚类所生成的簇是一组数据对象的集合，这些对象与同一个簇中的对象彼此相似，与其他簇中的对象相异。这里采用传统的聚类划分方法：k-means算法。k-means算法以k为参数，把n个对象分成k个簇，使簇内具有较高的相似度，而簇间的相似度较低。算法步骤1.随机选取k个点作为初始聚类中心。2.对于剩下的点，根据其余聚类中心的距离，将其归入最近的簇。3.对每个簇，计算所有点的均值作为\n\n      \n\n      \n\n        \n\n          \nhttps://blog.csdn.net/qq_43662165/article/details/108057598\n\n        \n\n      \n\n\n\n\n# 31省市居民家庭消费调查\n\n\n# 1.建立工程，导入sklearn相关包\n\n\nimport\n numpy \nas\n np\n\nfrom\n sklearn.cluster \nimport\n KMeans\n\n\n# 2.加载数据data，创建K-Means算法实例，并进行训练，获得标签label：\n\n\n# a.利用loadData方法读取数据\n\n\n# b.创建实例\n\n\n# c.调用Kmeans（）和fit_predict()方法进行计算\n\n\n\nif\n __name__ == \n'__main__'\n:\n\n    \ndef\n \nloadData\n(\nfilePath\n):\n        fr = \nopen\n(filePath, \n'r+'\n)  \n# r+读写打开一个文本文件\n\n        lines = fr.readlines()     \n# readlines（）一次读取整个文件\n\n        retData = []\n        retCityName = []\n        \nfor\n line \nin\n lines:\n            items = line.strip().split(\n\",\"\n)\n            retCityName.append(items[\n0\n])\n            retData.append([\nfloat\n(items[i]) \nfor\n i \nin\n \nrange\n(\n1\n, \nlen\n(items))])\n        \nreturn\n retData, retCityName\n\n    data, cityName = loadData(\n'city.txt'\n)\n    km = KMeans(n_clusters=\n4\n)  \n# n_clusters用于指定聚类中心的个数，init初始聚类中心的初始化方法，max_iter最大的迭代次数，init默认是k-means++ max_iter默认300\n\n    label = km.fit_predict(data)  \n# 计算簇中心以及为簇分配序号\n\n    expenses = np.\nsum\n(km.cluster_centers_, axis=\n1\n)  \n# 平均消费水平\n\n    \n# print(expenses)\n\n    CityCluster = [[], [], [], []] \n# 将城市按label分成设定的簇\n\n    \nfor\n i \nin\n \nrange\n(\nlen\n(cityName)): \n# 将每个簇的城市输出\n\n        CityCluster[label[i]].append(cityName[i]) \n    \nfor\n i \nin\n \nrange\n(\nlen\n(CityCluster)): \n# 将每个簇的平均花费输出\n\n        \nprint\n(\n\"Expenses:%.2f\"\n % expenses[i])\n        \nprint\n(CityCluster[i])\n\n", "Tag": "算法分析"}
{"Answer": "我现在写给你，望采纳！！点击该回答右侧的“采纳”按钮即可采纳！", "Konwledge_Point": "应对NP完全问题", "Question": "使用蒙特卡洛方法对几何分布进行序贯概率比检验 (SPRT)\n问题遇到的现象和发生背景\n\n\n在 2 个简单假设的情况下，用计算机实施序贯概率比检验 (SPRT)。\n\n\n使用蒙特卡洛方法估计其错误类型 I 和 II 的概率，以及条件预期样本大小（运行测试 10 000 次，$H_0$ 为真，\n计算有利于 $H_1$ 的 FALSE 决策的数量，除以 10 000 -> 你得到 $\\alpha$ 的估计值；启动一个计数器，\n将你停止测试的观察次数添加到一个计数器，每次运行时 $H_0$ 为真，然后除以 10 000 ->\n你将得到 $ 的估计值t_0$；$\\beta$ 和 $t_1$ 的估计是相似的）。\n\n\n实现：几何分布，$\\theta$ 是它唯一的参数。\n\n\n分析错误概率和假设接近度的预期样本大小的依赖关系。将 $\\alpha$ 和 $\\beta$ 用于您选择的阈值计算。\n\n\n我的解答思路和尝试过的方法，不写自己思路的，回答率下降 60%\n\n\n我找到的方法代码如下，但是仅仅只是实现几何分布的蒙特卡洛方法，不知道该怎样实现使用蒙特卡洛方法对几何分布进行序贯概率比检验 (SPRT)。\n\n\nimport\n numpy as np\n\nimport\n matplotlib.pyplot as plt\n\n\nnp\n.random.seed(\n222\n)\n\n\n# 把计算得到的函数写成一个函数\n\n\ndef\n distribution_z(z, p, max_k=\n200\n):\n    \nimport\n math\n    \nj\n = int(math.floor(z))\n    \nA\n = \n0\n\n    \nfor\n m in range(\n1\n, j + \n1\n):\n        \nA\n += (\n1\n - p) ** (m - \n1\n)\n    \nA\n *= p\n\n    \nB\n = \n0\n\n    \nfor\n k in range(j + \n1\n, max_k + \n1\n):\n        \na\n = (\n1\n - p) ** (k - \n1\n)\n        \na\n /= k\n        \nB\n += a\n    \nB\n *= z * p\n\n    \nreturn\n A + B\n\n\n\ndef\n pdf_z(z, p, max_k=\n200\n):\n    \nimport\n math\n    \nj\n = int(math.floor(z))\n    \nB\n = \n0\n\n    \nfor\n k in range(j + \n1\n, max_k + \n1\n):\n        \na\n = (\n1\n - p) ** (k - \n1\n)\n        \na\n /= k\n        \nB\n += a\n    \nreturn\n B * p\n\n\n\np\n = \n0\n.\n1\n\n\n# 选取数据点，点越多越精确\n\n\ndataPoints\n = \n10000\n\n\n\nUnit\n = np.random.rand(dataPoints)\n\nGeom\n = np.random.geometric(p, dataPoints)\n\ndistri_of_Monte\n = Geom * Unit\n\n\n# 概率密度函数 PDF\n\n\nplt\n.hist(distri_of_Monte, bins=\n40\n, range=(\n0\n, \n40\n))\n\npoints_of_z\n = np.arange(\n0\n, \n41\n, \n0\n.\n01\n)\n\npdf_of_z\n = np.array([pdf_z(zi, p) for zi in points_of_z]) * dataPoints\n\nplt\n.plot(points_of_z, pdf_of_z)\n\n# print(pdf_of_z)\n\n\nplt\n.show()\n\n\nhist\n, bin_edges = np.histogram(distri_of_Monte, bins=\n40\n, range=(\n0\n, \n40\n))\n\n\n# 概率分布函数 CDF\n\n\nhist_list\n = np.cumsum(hist) / dataPoints\n\n\nplt\n.plot(bin_edges[\n1\n:], hist_list)\n\n\npoints_of_z\n = np.arange(\n1\n, \n41\n, \n0\n.\n1\n)\n\ndistri_of_z\n =\n [distribution_z(zi, p) for zi in points_of_z]\n\n\n\nplt\n.plot(points_of_z, distri_of_z)\n\n\nplt\n.show()\n\n\n\n\n\nimport\n sprt as sprt\n\nimport\n numpy as np\n\n\n# Null value\n\n\nh0\n = \n0.5\n\n\n# Alternative value\n\n\nh1\n = \n0.55\n\n\n# Type I error rate = 0.05\n\n\nalpha\n = \n0.05\n\n\n# Type II error rate = 0.2\n\n\nbeta\n = \n0.2\n\n\n# Values\n\n\nvalues\n = np.random.binomial(\n1\n, \n0.55\n, \n100\n)\n\ntest\n = sprt.SPRTBinomial(\nh0\n = h0, \nh1\n = h1, \nalpha\n = alpha, \nbeta\n = beta, \nvalues\n = values)\n\ntest.plot()\n\n\n# Plot the data and boundary but without fill the color\n\ntest.plot(\nfill\n = False)\n\n\n\n\n\n\n\n\n我想要达到的结果，如果你需要快速回答，请尝试 “付费悬赏”\n\n\n使用蒙特卡洛方法，对几何分布进行序贯概率比检验 (SPRT)，同时满足题目的要求。", "Tag": "算法分析"}
{"Answer": "你只是赋值,没显示呀\r\nplt.show()看看", "Konwledge_Point": "应对NP完全问题", "Question": "用vscope编写python程序，运行后无结果显示？\nimport numpy as np\n\nimport matplotlib.pyplot as plt \n\nt = np.arange(0, 4, 0.1)\n\nplt.plot(t,t,t,t+2,t,t**2)\n\n\n\nnumpy包和matplotlib都安装到最新版本。\n", "Tag": "算法分析"}
{"Answer": "把.int改成.int32或者.int64试试", "Konwledge_Point": "应对NP完全问题", "Question": "AttributeError: module 'numpy' has no attribute 'int'\n今天在学习yolov5训练模型时，发生错误  AttributeError: module 'numpy' has no attribute 'int'\n\n\n\n\nTraceback (most recent \ncall\n \nlast\n):\n  File \n\"D:\\yolov5\\train.py\"\n, \nline\n \n543\n, in \n\n\n    train(hyp, \nopt\n, device, tb_writer)\n  File \n\"D:\\yolov5\\train.py\"\n, \nline\n \n189\n, in train\n    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, \nopt\n,\n  File \n\"D:\\yolov5\\utils\\datasets.py\"\n, \nline\n \n63\n, in create_dataloader\n    dataset = LoadImagesAndLabels(path, imgsz, batch_size,\n  File \n\"D:\\yolov5\\utils\\datasets.py\"\n, \nline\n \n411\n, in __init__\n    bi = np.\nfloor\n(np.arange(n) / batch_size).astype(np.\nint\n)  # batch \nindex\n\n  File \n\"D:\\ProgramData\\Anaconda3\\envs\\yolov5\\lib\\site-packages\\numpy\\__init__.py\"\n, \nline\n \n284\n, in __getattr__\n    raise AttributeError(\n\"module {!r} has no attribute \"\n\nAttributeError: module \n'numpy'\n \nhas\n \nno\n attribute \n'int'\n\n\n\n\n\n在之前没有安装numpy，只是用CPU进行训练时未发生错误。今天换成GPU，安装numpy后在运行train.py就发生了该错误，不知道如何解决\n\n\n提示发生错误的文件：\n\n\n\n", "Tag": "算法分析"}
{"Answer": "torch.from_numpy 函数需要传输的对象是np.ndarry 类型的东西，你现在传输的sourceR  他的对象类型是PlyData需要进行更改https://blog.csdn.net/weixin_42445581/article/details/105069655", "Konwledge_Point": "应对NP完全问题", "Question": "请问这个问题该怎么解决？\n遇到报错：\nTypeError: expected np.ndarray (got PlyData)\n\n\n报错部分：\n\n\ndef\n \nmain\n():\n    args = options()\n    torch.backends.cudnn.deterministic = \nTrue\n\n\n    \nif\n args.user_data:\n        sourceR = PlyData.read(\nr'E:\\bunny\\data\\bun000.ply'\n)  \n# The source point cloud is a rotated and offset defect\n\n        \n# source=s3.float()\n\n\n        templateR = PlyData.read(\nr'E:\\bunny\\data\\bun045.ply'\n)  \n# Template point cloud is complete\n\n        \n# template=s5.float()\n\n\n\n        source = torch.from_numpy(sourceR)      \n#此处报错\n\n        template = torch.from_numpy(templateR)\n\n      testset = UserData(template=template, source=source, tpcc=\nNone\n, igt=\nNone\n)\n\n", "Tag": "算法分析"}
{"Answer": "我知道了..\r\ntabPanel.frames[frmid]\r\n这肯定不对啦\r\n\r\n1.tabpanel是一个ext的组件,他没有什么frames的属性\r\n2.你是iframe,也不该是用frames[]的方法来取\r\n\r\n试着windowB.document.getElementById('myIfrId').src=\"http://www.g.cn\"\r\n之类的方法找下.\r\n\r\n或者你给出a和b的一些关键代码,不然我们是没办法本地测试", "Konwledge_Point": "应对NP完全问题", "Question": "请问如何实现点击按钮刷新已打开页面的功能\n用ExtJs实现如下：\n\n\n\nA.jsp中点击“显示”按钮弹出B.jsp窗口，若在没有关闭B页面前提下再次点击A的“显示”按钮实现B页面整个刷新并自动切换到B页面\n\n\n\n如何实现？？\n\n\n\n我写的代码\n\n\n\n[code=\"java\"]\n\nvar tabPanel = window.parent.tabs;\n\nvar lp = tabPanel.getComponent(id);\n\n\n\nif(!np){\n\n    tabPanel.add({\n\n    id: id,\n\n    title: name,\n\n    html: '',\n\n    closable: true\n\n    }).show();\n\n}\n\nelse{\n\n    var frmid = tabPanel.items.indexOf(np);\n\n\n\ntabPanel.frames[frmid].location.reload();\nnp.show();\n\n\n\n\n}\n\n[/code]\n\n\n\n在第二次点击时总是提示“frames为空或不为对象”，可是这个页面明明已经有了啊？为什么？请指教\n\n[b]问题补充：[/b]\n\nnp就是lp手误了\n\n其次我取得是下标 用下标也能定位的吧 不一定要id的\n\n[b]问题补充：[/b]\n\n而且用id我也试过了 还是同样的错误\n\n[b]问题补充：[/b]\n\n这些代码是写在a.jsp中的 要弹出的就是iframe中的b.jsp页面 a页面最外层还有个frame\n\n\n\n用Ext.get方法也取不到这个iframe啊 我做tabPanel.frames[frmid]其实就是为了获取这个内嵌的iframe，但是代码跑到frames这边就报错了，请问除了我写的这种方法 还能用什么方法获取这个iframe？加id或name我都试过了 还是同样错\n\n\n\n还有，我并没有点击关闭按钮，close应该还没调到吧\n\n[b]问题补充：[/b]\n\n代码太多我不可能全部贴出来的 其实我就是想知道如何获取一个tabPanel里面的iframe整个页面啊。取到了这个页面我才能去做刷新\n\n[b]问题补充：[/b]\n\n对了 说错了一点 tabPanel部分我是单独写在c.jsp中的，这个a.jsp也是c生成的一个iframe中，a和b其实是同级的，应该说c每次生成一个新的iframe。现在a想通过操作父页面c来增加一个兄弟页面b就是这样", "Tag": "算法分析"}
{"Answer": "\nYou can't access a constant like you do with a static property, the constant function is your solution :\n$type = constant('NP_PostTypeType::' . $postType);\n\nBut be careful, your $postType case must match your NP_PostTypeType constant names (currently not the case), you should update your NP_PostType class to :\nabstract class NP_PostType extends BasicEnum {\n    const Event = \"Event\";\n    const Job = \"Job\";\n    const Quote = \"Quote\";\n    const Status = \"Status\";\n    const Video = \"Video\";\n}\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "具有可变索引的枚举\n\n\n\nI'm not sure what the correct wording of my question is, but I added an \nenum\n (\nNP_PostTypeType\n) because I need to know what the type of each item in my initial enum (\nNP_PostType\n) is.\n\n\n\nI store the current PostType in \n$postType\n which gets fed into a method and now in that method I need to extract the type for each type.\n\n\n\nWhat I tried doing was: \nswitch(NP_PostTypeType::$type)\n, but this yields: \nFatal error: Access to undeclared static property: NP_PostTypeType::$type\n\n\n\nThese are my 2 enums:\n\n\n\nabstract class NP_PostType extends BasicEnum {\n    const Event = \"event\";\n    const Job = \"job\";\n    const Quote = \"quote\";\n    const Status = \"status\";\n    const Video = \"video\";\n}\n\nabstract class NP_PostTypeType extends BasicEnum {\n    const Event = \"type\";\n    const Job = \"type\";\n    const Quote = \"format\";\n    const Status = \"format\";\n    const Video = \"format\";\n}\n\n\n\n\nHow do I go about this?\n\n    ", "Tag": "算法分析"}
{"Answer": "[参数写错了](https://scikit-neuralnetwork.readthedocs.io/en/latest/module_mlp.html#sknn.mlp.Regressor)\r\n\r\nLayers=\r\n换成\r\nlayers", "Konwledge_Point": "应对NP完全问题", "Question": "解决python __init__() missing 1 required positional argument: 'layers'\n运行的时候出现了__init__() missing 1 required positional argument: 'layers'，有大佬知道怎么解决吗\n\n\n\nfrom sklearn import datasets\nboston=datasets.load_boston()\nx,y=boston.data, boston.target\n\nfrom sklearn import preprocessing\nx_MinMax=preprocessing.MinMaxScaler()\ny_MinMax=preprocessing.MinMaxScaler()\n\nimport numpy as np\ny=np.array(y).reshape((len(y),1)) #np.array确保y是numpy数组\nx=x_MinMax.fit_transform(x) #fit_transform先拟合数据，然后转化它将其转化为标准形式\ny=y_MinMax.fit_transform(y)\nx.mean(axis=0) #均值为0\n\nimport random\nfrom sklearn.cross_validation import train_test_split\nnp.random.seed(2016)\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n\nfrom sknn.mlp import Regressor,Layer #预测模型\nfit1=Regressor(Layers=[Layer('Sigmoid',units=6),Layer('Sigmoid',units=14),\n                       Layer('Linear')],learning_rate=0.02,\n               random_state=2016,\n               n_iter=10\n               )\nfit1.fit(x_train,y_train)\npredict_train=fit1.predict(x_train)\n\nfrom sklearn.metrics import mean_squared_error\nmse_1=mean_squared_error(predict_train,y_train)\nprint(mse_1)\n", "Tag": "算法分析"}
{"Answer": "你直说了剩一个数字的情况，如果剩余两个或多个数字呢？想要怎么排列？排列几行？", "Konwledge_Point": "应对NP完全问题", "Question": "python生成一个想要的矩阵array\n问题遇到的现象和发生背景\n\n\n我有N个数字和K个组，我想把这N个数字随机分配到每个K组内，形成一个[K * N/K]的矩阵array。比如我有1000个数字我想将这1000个数字随机并均匀的分配到20个组内，组成一个20*50的矩阵数组。\n\n\n但是问题是N不能经常被N整除，比如我有27个N和13个K组，27就除不开13余1，余下了1个数字无法被分配。我想把这1个数组也成为一行array，做成一个3*13的矩阵，但是最后一行只包含这1个被剩下的数。\n\n\n没太多头绪，所以请求一个好的思路达到这个结果。十分感谢\n\n\n\n\nnew\n_N\n = np.arange(N)\nnp.random.shuffle(\nnew\n_N\n)\n\nreturn\n \nnew\n_N\n.reshape((K, int(N/K))).\n", "Tag": "算法分析"}
{"Answer": "建议你把报错提示 以文本的形式复制出来大家解答", "Konwledge_Point": "应对NP完全问题", "Question": "使用pyqtgraph无法在子窗口和父窗口都绘制3D图\n在使用pyqt5和pyqtgraph绘制3D散点图的时候，我设置了两个窗口，实现在父窗口中根据读取文件绘制了3D图，在父窗口中成功启动子窗口。但是，当我想在子窗口中根据父窗口中读取的文件绘制3D图时报错。即，父窗口中绘制了3D图，子窗口报错；子窗口绘制了3D图，父窗口报错。\n部分代码：\n\n\n\n\nclass MainWindow(QtWidgets.QMainWindow,Ui_zhibeichuli):#主界面\n\n\ndef\n \n__init__\n(\nself,parent=\nNone\n):\n    \nsuper\n(MainWindow,self).__init__(parent)\n    self.setupUi(self)\n    \n'''定义信号槽'''\n\n\n    self.actionfile_PcdLas_open.triggered.connect(self.open_file_pointCloud_pcd_las)\n    \n#self.action_exit.triggered.connect(MainWindow.close)\n\n\n    self.action_save.triggered.connect(self.save_point_cloud)\n    self.action_slope_chuli.triggered.connect(self.open_son_slope)\n    \n'''初始化显示布局'''\n\n    self.pointView = GLViewWidget()\n    self.gridLayout.addWidget(self.pointView, \n0\n, \n1\n)\n    self.grid_item = gl.GLGridItem(QVector3D(\n100\n, \n100\n, \n1\n))\n#生成网格\n\n    self.pointView.addItem(self.grid_item)\n    \n'''初始三维坐标'''\n\n    self.x_axis_item = gl.GLLinePlotItem(pos=np.array([[\n0\n, \n0\n, \n0\n], [\n10\n, \n0\n, \n0\n]], dtype=np.float32),\n                                         color=(\n1\n, \n0\n, \n0\n, \n1\n),\n                                         width=\n2\n)\n    self.pointView.addItem(self.x_axis_item)\n    self.y_axis_item = gl.GLLinePlotItem(pos=np.array([[\n0\n, \n0\n, \n0\n], [\n0\n, \n10\n, \n0\n]], dtype=np.float32),\n                                         color=(\n0\n, \n1\n, \n0\n, \n1\n),\n                                         width=\n2\n)\n    self.pointView.addItem(self.y_axis_item)\n    self.z_axis_item = gl.GLLinePlotItem(pos=np.array([[\n0\n, \n0\n, \n0\n], [\n0\n, \n0\n, \n10\n]], dtype=np.float32),\n                                         color=(\n0\n, \n0\n, \n1\n, \n1\n),\n                                         width=\n2\n)\n    self.pointView.addItem(self.z_axis_item)\n\n\n\n打开文件部分（父窗口）：\n'''槽函数，打开点云文件，*.pcd *.las'''\n    def open_file_pointCloud_pcd_las(self):\n        global slope_pos_pcd_las#存储xyz坐标\n\n\n    \nfrom\n PyQt5.QtWidgets \nimport\n QFileDialog\n    \ndir\n=QFileDialog()\n#创建文件对话框\n\n    \ndir\n.setFileMode(QFileDialog.ExistingFiles)\n#设置多选\n\n    \n#设置只显示的文件类型\n\n    \ndir\n.setNameFilter(\n'点云文件(*.pcd *.las)'\n)\n\n    \nif\n \ndir\n.exec_():\n#判断是否选择了文件\n\n\n        (raod_pcd_las_name, extension) = os.path.splitext(\ndir\n.selectedFiles()[\n0\n])\n#获取文件后缀\n\n        \nif\n extension==\n'.pcd'\n:\n            pcd = o3d.io.read_point_cloud(\ndir\n.selectedFiles()[\n0\n])\n#根据文件地址读取文件\n\n            xyz_pcd = np.array(pcd.points)\n#转为np数组\n\n            \n#Point_Cloud_chuli().show_point_before(xyz_pcd)#显示pcd\n\n            slope_pos_pcd_las=xyz_pcd\n#全局变量存储，用于坡度处理调用\n\n            self.pointView.clear()\n#清除布局\n\n            \n'''重新添加布局'''\n\n            self.pointView = GLViewWidget()\n            self.gridLayout.addWidget(self.pointView, \n0\n, \n1\n)\n            self.grid_item = gl.GLGridItem(QVector3D(\n100\n, \n100\n, \n1\n))\n            self.pointView.addItem(self.grid_item)\n            points = xyz_pcd\n            \n'''用于点云的坐标变换'''\n\n            x_list = points[..., \n0\n]\n            y_list = points[..., \n1\n]\n            z_list = points[..., \n2\n]\n            \n# x, y, z = points[0, 0], points[0, 1], points[0, 2]\n\n            x, y, z = \nmin\n(x_list), \nmin\n(y_list), \nmin\n(z_list)\n            x_trans = x_list - x\n            y_trans = y_list - y\n            z_trans = z_list - z\n            n = \nlen\n(x_trans)\n            points_trans = np.zeros((n, \n3\n))\n            points_trans[..., \n0\n] = x_trans\n            points_trans[..., \n1\n] = y_trans\n            points_trans[..., \n2\n] = z_trans\n            \n'''坐标轴xyz'''\n\n            self.x_axis_item = gl.GLLinePlotItem(pos=np.array([[\n0\n, \n0\n, \n0\n], [\n10\n, \n0\n, \n0\n]], dtype=np.float32),\n                                                 color=(\n1\n, \n0\n, \n0\n, \n1\n),\n                                                 width=\n2\n)\n            self.pointView.addItem(self.x_axis_item)\n            self.y_axis_item = gl.GLLinePlotItem(pos=np.array([[\n0\n, \n0\n, \n0\n], [\n0\n, \n10\n, \n0\n]], dtype=np.float32),\n                                                 color=(\n0\n, \n1\n, \n0\n, \n1\n),\n                                                 width=\n2\n)\n            self.pointView.addItem(self.y_axis_item)\n            self.z_axis_item = gl.GLLinePlotItem(pos=np.array([[\n0\n, \n0\n, \n0\n], [\n0\n, \n0\n, \n10\n]], dtype=np.float32),\n                                                 color=(\n0\n, \n0\n, \n1\n, \n1\n),\n                                                 width=\n2\n)\n            self.pointView.addItem(self.z_axis_item)\n            \n'''点云显示样式'''\n\n            colors = np.ones((n, \n4\n))\n            size = np.zeros(shape=points.shape[\n0\n])\n            self.points_plot_item = gl.GLScatterPlotItem(pos=points_trans,\n                                                         color=colors,\n                                                         size=size,\n                                                         pxMode=\nFalse\n)\n            self.pointView.addItem(self.points_plot_item)\n\n\n\n打开子窗口：\n    def open_son_slope(self):\n\n\n    self.\nop\n=Son_slopeWindow()\n    self.\nop\n.\nshow\n()\n\n\n\n子窗口：\nclass Son_slopeWindow(QtWidgets.QMainWindow,Ui_slope_chuli):\n    def \ninit\n(self):\n        super(Son_slopeWindow,self).\ninit\n()\n        self.setupUi(self)\n        self.pushButton.clicked.connect(self.slope_show)\n        '''----------'''\n        pcd = o3d.io.read_point_cloud(\"D:/desk/002.pcd\")\n\n\n    filtered_cloud = Point_Cloud_chuli().slope_chuli(pcd, \n2\n, \n0.5\n,\n1\n)\n\n    points = np.array(filtered_cloud.points)\n\n    \n'''用于点云的坐标变换'''\n\n    x_list = points[..., \n0\n]\n    y_list = points[..., \n1\n]\n    z_list = points[..., \n2\n]\n    \n# x, y, z = points[0, 0], points[0, 1], points[0, 2]\n\n    x, y, z = \nmin\n(x_list), \nmin\n(y_list), \nmin\n(z_list)\n    x_trans = x_list - x\n    y_trans = y_list - y\n    z_trans = z_list - z\n    n = \nlen\n(x_trans)\n\n    points_trans = np.zeros((n, \n3\n))\n    points_trans[..., \n0\n] = x_trans\n    points_trans[..., \n1\n] = y_trans\n    points_trans[..., \n2\n] = z_trans\n\n    \n'''--------'''\n\n\n    self.points_afterView=GLViewWidget()\n    self.gridLayout_2.addWidget(self.points_afterView,\n0\n,\n1\n)\n\n    self.grid_ite=gl.GLGridItem(QVector3D(\n100\n,\n100\n,\n1\n))\n    self.points_afterView.addItem(self.grid_ite)\n\n    self.x_axis=gl.GLLinePlotItem(pos=np.array([[\n0\n,\n0\n,\n0\n],[\n10\n,\n0\n,\n0\n]]),\n                                  color=(\n1\n,\n0\n,\n0\n,\n1\n),\n                                  width=\n2\n)\n    self.points_afterView.addItem(self.x_axis)\n    self.y_axis = gl.GLLinePlotItem(pos=np.array([[\n0\n, \n0\n, \n0\n], [\n0\n, \n10\n, \n0\n]]),\n                                    color=(\n0\n, \n1\n, \n0\n, \n1\n),\n                                    width=\n2\n)\n    self.points_afterView.addItem(self.y_axis)\n    self.z_axis = gl.GLLinePlotItem(pos=np.array([[\n0\n, \n0\n, \n0\n], [\n0\n, \n0\n, \n10\n]]),\n                                    color=(\n0\n, \n0\n, \n1\n, \n1\n),\n                                    width=\n2\n)\n    self.points_afterView.addItem(self.z_axis)\n    \n'''-----'''\n\n    colors = np.ones((n, \n4\n))\n    size = np.zeros(n)\n\n    self.points_plot_after = gl.GLScatterPlotItem(pos=points_trans,\n                                                  color=colors,\n                                                  size=size,\n                                                  pxMode=\nFalse\n)\n    self.points_afterView.addItem(self.points_plot_after)\n\n\n\n报错类型：\n\n\n\n\n可以说明的是，文件及处理后的文件的数据没问题。", "Tag": "算法分析"}
{"Answer": "看着是1.wav 的原文件被损坏了，换个文件试试？", "Konwledge_Point": "应对NP完全问题", "Question": "求各位解答一下，没有动过包，之前还可以运行，后面打开就不能运行了\n目的是提取语音文件，并且画图\n代码如下：\n\n\nimport wave\nimport numpy \nas\n np\n\nfrom\n matplotlib import pyplot \nas\n plt\n\npath = \n\"E:/voice/1.wav\"\n\nf = wave.\nopen\n(path, \n\"rb\"\n)  \n# 打开需要处理的内容\n\n\nparams\n = f.getparams()\nnchannels, sampwidth, framerate, nframes = \nparams\n[:\n4\n]\nstr_data = f.readframes(nframes)\nwave_data = np.frombuffer(str_data, dtype=np.\nshort\n)\nprint(\nlen\n(wave_data))\nprint(wave_data)\nprint(\n\"len of wave_data:\"\n, \nlen\n(wave_data))\nf.\nclose\n()\n\ntime\n = np.arange(\n0\n, nframes) * (\n1.0\n / framerate)\nprint(\nlen\n(\ntime\n))\nplt.plot(\ntime\n, wave_data, \n\"r-\"\n)\nplt.xlabel(\n'Time/s'\n)\nplt.ylabel(\n'Ampltitude'\n)\nplt.title(\n'waveform of voice'\n)\nplt.show()\n\n", "Tag": "算法分析"}
{"Answer": "建议使用代码插入功能重新写一遍问题，markdown会吞格式", "Konwledge_Point": "应对NP完全问题", "Question": "大lao，这个为啥出不来图像啊#python\nimport pyaudio\nimport struct\nimport numpy as np\nimport matplotlib.pyplot as plt\nCHUNK = 1024 * 4\nFORMAT = pyaudio.paInt16\nCHANNELS = 1\nRATE = 44100\np = pyaudio.PyAudio()\nstream = p.open(\n    format=FORMAT,\n    channels=CHANNELS,\n    rate=RATE,\n    input=True,\n    output=True,\n    frames_per_buffer=CHUNK\n)\n\n\nfig, ax = plt.subplots()\n\n\nx = np.arange(0, 2 * CHUNK, 2)\nline, = ax.plot(x, np.random.rand(CHUNK))\nax.set_ylim(0, 255)\nax.set_xlim(0, CHUNK)\n\n\nwhile True:\n    data = stream.read(CHUNK)\n    data_int = np.array(struct.unpack(str(2 * CHUNK) + 'B', data), dtype='b')[::2] + 127\n    line.set_ydata(data_int)\n    fig.canvas.draw()\n    fig.canvas.flush_events()", "Tag": "算法分析"}
{"Answer": "这道题是求给定函数的 傅里叶级数，其中采用 复化辛普森数值积分公式 计算傅里叶系数。下面索性倒着往前解答。\n\n傅里叶级数\n记不住傅里叶级数也没关系，随手搜一下：\n\n（1）式对应最后一空，只不过这题只需取前 N 项\n# Fourier series at order N (over 3 periods)\ntt = np.linspace(a-T,b+T,500)\nff = np.ones(np.size(tt))\nff = an[0] * ff\nfor k in range(1,N+1):\n    ff += an[k]*np.cos(k*w*tt) + bn[k]*np.sin(k*w*tt)   # <- 填空\n\n继续往前，我们需要求解傅里叶系数 an 和 bn\n\n以 an （系数数组的第 n 项）为例，它是一个积分式，被积函数 f(t) * cos(nwt)，积分区间 [a, b]。结合题意，采用辛普森方法进行数值积分，也就是前面定义的 SimpsonMethodValues()函数。\n# Fourier coefficients an and bn :\nan = np.zeros(N+1)\nbn = np.zeros(N+1)\nan[0] = SimpsonMethodValues(ft, a, b) / T\nfor k in range(1,N+1):\n    ft_cos = ft * np.cos(k*w*t)    # <- 填空\n    ft_sin = ft * np.sin(k*w*t)      # <- 填空\n    an[k] = 2/T * SimpsonMethodValues(ft_cos, a, b)  # <- 填空\n    bn[k] = 2/T * SimpsonMethodValues(ft_sin, a, b)   # <- 填空\n\n复化辛普森数值积分公式\n自然地，来到了辛普森数值积分函数的实现部分。辛普森方法是一种机械求积方式，通过复化（细分）积分区间的方式提升精度，具体参考百度百科的描述：\n\n根据不同的表述方式，上图表达式稍微有所差异，但含义完全一致。注意这里采用的表述方式是：分为 2n 个子区间。于是，参数 fvalues 表示这 2n 个子区间的共计 2n+1 个端点，也就是 N = 2n+1 = len(fvalues)，即 n = (N-1)/2，进而 h = (b-a)/h = 2(b-a)/(N-1)。\n结合已知的代码和上图，表达式右侧归纳为三部分：\nS：  两个端点对应的函数值S1：端点除外的所有偶数编号子节点对应函数值S2：端点除外的所有奇数编号子节点对应函数值\n借助 Python 列表的切片表达式，很容易写出留空的部分：\ndef SimpsonMethodValues(fvalues, a, b):\n    \"\"\" Simpson method for determining an approximation of\n    the integral of a function over the interval [a, b]\n    The function is known by its sampling \"fvalues\"\n    Input : fvalues = sampling of the function to be integrated (array)\n    : a,b = bounds of the interval\n    Output : S = Simpson approximation of the integral\n    \"\"\"\n    N = len(fvalues)\n    h = 2 * (b-a) / (N-1)\n    # partial sum S1\n    S1 = np.sum(fvalues[2:N-2:2])    # <- 填空\n    S1 = S1 / 3.\n    # partial sum S2\n    S2 = np.sum(fvalues[1:N-1:2])     # <- 填空\n    S2 = 2*S2 / 3.\n    # sum S\n    S = (fvalues[0] + fvalues[-1]) / 6   # <- 填空\n    S = S + S1 + S2\n    S *= h\n    return S\n\n结果\n填完空了，顺便跑一下结果吧。整个流程大意是用 N=7 阶傅里叶级数来近似原函数，然后在3个周期内画出对比图像。对了，最后加一个 plt.show()才能显示结果。以最后一个，[0, 1] 区间的平方根函数 f = √ x 为例：\n红线为原函数蓝线为傅里叶级数近似曲线\n", "Konwledge_Point": "应对NP完全问题", "Question": "如何使用Python用辛普森方法确定一个函数在区间[a, b]上的积分的近似值\n要求将代码的“-----”部分补充完整，救救孩子的作业吧啊啊啊啊\n以下为代码\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n#####################################################\n\n\nFUNCTIONS\n\n\n#####################################################\ndef SimpsonMethodValues(fvalues, a, b):\n    \"\"\" Simpson method for determining an approximation of\n        the integral of a function over the interval [a, b]\n        The function is known by its sampling \"fvalues\"\n        Input : fvalues = sampling of the function to be integrated (array)\n              : a,b = bounds of the interval\n        Output : S = Simpson approximation of the integral\n    \"\"\"\n    N = len(fvalues)\n    h = 2 * (b-a) / (N-1)\n    # partial sum S1\n    S1 = -----\n    S1 = S1 / 3.\n    # partial sum S2\n    S2 = -----\n    S2 = 2*S2 / 3.\n    # sum S\n    S = -----\n    S = S + S1 + S2\n    S *= h\n    return S\n\n\nfunctions to be approximated :\n\n\ndef g0(x):\n    return np.ceil(x) # on ]-1,1]\ndef g1(x):\n    return x\ndef g2(x):\n    return x**2\ndef g3(x):\n    return np.abs(x)\ndef g4(x):\n    return np.sqrt(x)\n\n\n#####################################################\n\n\nMAIN\n\n\n#####################################################\nplt.cla()\nplt.grid()\n\n\neps = 1e-10\n\n\nChoice of a function :\n\n\nf = g0; a =  -1+eps; b = 1;\n#f = g1; a =  0; b = 1;\n#f = g2; a = -1; b = 1;\n#f = g3; a = -1; b = 1;\n#f = g4; a = 0; b = 1;\n\n\nPeriod and pulsation\n\n\nT = b-a;\nw = 2*np.pi / T\n\n\nPlot of the function to be approximated :\n\n\nn = 8   # ==> 2^n+1 evaluation points (odd number of points)\nt = np.linspace(a,b,2**n + 1) # for graph plotting\nft = f(t)\nplt.plot(t-T,ft,'r')\nplt.plot(t,  ft,'r')\nplt.plot(t+T,ft,'r')\n\n\nN = 7\n\n\nFourier coefficients an and bn :\n\n\nan = np.zeros(N+1)\nbn = np.zeros(N+1)\nan[0] = SimpsonMethodValues(ft, a, b) / T\nfor k in range(1,N+1):\n    -----\n    -----\n    -----\n    -----\n\n\nFourier series at order N (over 3 periods)\n\n\ntt = np.linspace(a-T,b+T,500)\nff = np.ones(np.size(tt))\nff = an[0] * ff\nfor k in range(1,N+1):\n    -----\n\n\nplt.plot(tt,ff)", "Tag": "算法分析"}
{"Answer": "u1 , b是啥呀，只有u1==max(b)成立才会进入到里面的，而且max函数要传入至少俩参数，才能实现大小比较的功能看看你的完整程序", "Konwledge_Point": "应对NP完全问题", "Question": "Python输出的结果不对\n您好，想请教一下下面这个问题\nfor m in np.arange(0.1,1,0.01):\nif u1==max(b):\n                m1=m\n    print(m1)\n将m的值赋给m1，为啥输出的m1的值是0，m是从0.1开始呀？", "Tag": "算法分析"}
{"Answer": "img=np.zeros((8,8),dtype=np.uint8)使用无符号的才是0-255，你的int8取值是[-128,127]", "Konwledge_Point": "应对NP完全问题", "Question": "为什么这个跑出来img[0,3]=-1,而且窗口上看不到白点\n\nimport cv2\nimport numpy as np\nimg=np\n.zeros\n((\n8\n,\n8\n),dtype=np.int8)\n\nprint\n(\n\"img=\\n\"\n,img)\n\ncv2\n.imshow\n(\n'one'\n,img)\n\nprint\n(\n\"读取像素点img[0,3]=\"\n,img[\n0\n,\n3\n])\n\n\nimg\n[0,3]\n=\n255\n\n\nprint\n(\n\"修改后img=\\n\"\n,img)\n\n\nprint\n(\n\"读取修改后的像素点img[0,3]=\"\n,img[\n0\n,\n3\n])\n\ncv2\n.imshow\n(\n\"two\"\n,img)\ncv2\n.waitKey\n()\ncv2\n.destroyAllWindows\n()\n\n\n", "Tag": "算法分析"}
{"Answer": "data_origin['q']赋值语句后面不要用单引号", "Konwledge_Point": "应对NP完全问题", "Question": "python怎么把循环得到的列表写入dataframe\n问题遇到的现象和发生背景\n\n\n通过循环得到p，q。结果截图里显示了尝试写入的结果，发现写入的是固定的值而不是每个p，q的值\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport numpy as np\nimport pandas as pd\ndata = pd.read_csv(\n\"C:/Users/DELL/Desktop/dt1.csv\"\n, \nheader\n=0, usecols=[\n'v'\n])\ndata = np.array(data)\n\nT=[] #定义为列表\n\nV\n=0   #初始成交量\n\np\n=0\np=[]\nq=[]\n\nA\n=326881602  #每个交易篮子的交易量\n\nfor\n i \nin\n range(len(data)):\n    \nV\n=V+data[i]   #迭代，成交量=上一时刻成交量+这一时刻的成交量\n    \nk\n=np.ceil(V/A)  #对篮子数向上取整\n    T.append(k)\n    T[i]=k    #第i时刻所需要的篮子数\n    p.append(0)\n    p[i]=0\n    \nif\n np.ceil(V/A)-V/\nA\n==0:  #篮子数为整数\n        p.append(1)\n        p[i]=1  # p[i]=1#如果篮子数为整数，那么下一时刻从新的交易篮子开始填充\n    q.append(T[i])\n    q[0]=[1,T[0]]  #第一个时刻对应的篮子编号\n\nfor\n i \nin\n range(len(data)):\n    \nif\n i>=1:\n        q[i] = [T[i - 1] + p[i - 1], T[i]]\n\n\nprint\n(T)\n\nprint\n(p)\n\nprint\n(q)\n\nprint\n(type(p))\n\nprint\n(type(q))\n\nfilepath = \n'C:/Users/DELL/Desktop/dt1.csv'\n\ndataheader = [\n'date'\n,\n'time'\n,\n'v'\n,\n'p'\n,\n'vum'\n]\ndata_original = pd.read_csv(filepath, \nsep\n=\n','\n ,\nheader\n=0, \nnames\n=dataheader, \nskip_blank_lines\n=\nTrue\n)\n\nprint\n(data_original)\ndata_original[\n'e'\n]=\n'p'\n\ndata_original[\n'q'\n]=\n'[T[i - 1] + p[i -  1], T[i]]'\n\n\nprint\n(data_original)\n\n\n\n运行结果及报错内容\n\n\n\n\n\n\n我想要达到的结果\n\n\n想把q这个list写进dataframe里应该怎么办", "Tag": "算法分析"}
{"Answer": "不是bugatoms2 = atoms1，166行这里两个变量指向了同一个地址，所以他俩改一个另外一个也会改", "Konwledge_Point": "应对NP完全问题", "Question": "python对一个变量进行修改时，会同时对另外一个变量做同样的修改操作，是不是bug\n下边是我的代码\n运行时，对变量atoms2进行赋值操作时，会同时修改atoms1的内容\n按理说不应该这样的啊，是我写的不对吗\n\n\nimport numpy as np\nimport random\nimport math\nimport scipy.optimize as opt\n\n\n\ncount = \n0\n\nre = \n0.1564\n\nu_exp =  \n3.07\n\na_f = \n1.3\n\na_ca = \n4.4\n\nx = []\nwrites = []\nresult = \n''\n\n\n\ndef calc_x2(atoms):\n    x = \n0\n\n    y = \n0\n\n    z  = \n0\n\n    a2d = \n2.542\n  \n    b2a = \n0.529177249\n\n\n    for atom in atoms:\n        x += (atom[\n'site'\n][\n0\n]/b2a )* atom[\n'q'\n]\n\n        y += (atom[\n'site'\n][\n1\n]/b2a )* atom[\n'q'\n]\n        z += (atom[\n'site'\n][\n2\n]/b2a )* atom[\n'q'\n]\n    x,y,z =x*a2d, y*a2d,z*a2d\n    u = math.sqrt(x**\n2\n+y**\n2\n+z**\n2\n)\n    x2 = (u \n-3.07\n) ** \n2\n\n    return x2\n\ndef make_atoms(atoms,x0):\n    #x0 = [q_ca,beta_ca]\n    q_ca = x0[\n0\n]\n    beta_ca = x0[\n1\n]\n    \n    atoms[\n4\n][\n'q'\n] = q_ca\n    atoms[\n5\n][\n'q'\n] = -(q_ca\n-2\n)\n    \n    atoms[\n4\n][\n'beta'\n] = beta_ca\n    atoms[\n5\n][\n'beta'\n] = beta_ca\n\n    return atoms\n \ndef cacl_ener(x0,atoms):\n    k = \n1\n/(\n4\n*math.pi*\n0.08854187818\n)\n    v_sum = \n0\n\n    atoms[\n1\n][\n'site'\n] = np.array([x0[\n0\n],x0[\n1\n],x0[\n2\n]])\n    atoms[\n3\n][\n'site'\n] = np.array([x0[\n3\n],x0[\n4\n],x0[\n5\n]])\n    atoms[\n5\n][\n'site'\n] = np.array([x0[\n6\n],x0[\n7\n],x0[\n8\n]])\n    atom = [\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n5\n]\n    for i in range(\n0\n,\n5\n):\n        atom = atom[\n1\n:]\n        bi = atoms[i][\n'beta'\n]\n        qi = atoms[i][\n'q'\n]\n        for j in atom:\n            \n            bj = atoms[j][\n'beta'\n]\n            \n            \n            qj = atoms[j][\n'q'\n]\n\n            r = np.linalg.norm(atoms[i][\n'site'\n] - atoms[j][\n'site'\n])\n            bij = (bi * bj) /(math.sqrt(bi*bi+bj*bj))            \n            erf= math.erf(bij * r)\n\n            e_coul = k * qi * qj *erf /r\n            v_sum += e_coul\n\n\n    r_f_1 = np.linalg.norm(atoms[\n0\n][\n'site'\n] - atoms[\n1\n][\n'site'\n])\n    r_f_2 = np.linalg.norm(atoms[\n2\n][\n'site'\n] - atoms[\n3\n][\n'site'\n])\n    r_ca= np.linalg.norm(atoms[\n4\n][\n'site'\n] - atoms[\n5\n][\n'site'\n])\n    v_pol_f_1 = k * (atoms[\n1\n][\n'q'\n]**\n2\n/(\n2\n*a_f)) * r_f_1**\n2\n\n    v_pol_f_2 = k * (atoms[\n3\n][\n'q'\n]**\n2\n/(\n2\n*a_f)) * r_f_2**\n2\n\n    v_pol_ca = k * (atoms[\n5\n][\n'q'\n]**\n2\n/(\n2\n*a_ca)) * r_ca**\n2\n\n    v_sum = v_sum + v_pol_f_1+ v_pol_ca   + v_pol_f_2\n    return(v_sum)\n\ndef p_min(delta_x2,t):\n    probability = np.exp(-delta_x2/t)\n    return probability\n\ndef deal(atoms1,atoms2,delta_x2,t):\n    if delta_x2 < \n0\n:\n        return atoms2\n    else:\n        return atoms1\n\ndef solve():\n    atoms1 = atoms\n    count = \n0\n\n    skl = \n0\n\n    t = \n1000\n\n    t_min = \n1\n\n    jishu = \n0\n\n    r = \n1000\n\n    count_skl_1 = \n0\n\n    count_skl_2 = \n0\n\n    while t >= t_min:\n        jishu += \n1\n\n        t *= \n0.95\n\n    t = \n1000\n\n    x0 =([\n0\n, \n1\n, \n-0.461\n,\n0\n, \n-1.669\n, \n-0.561\n,\n0.\n, \n0.\n    , \n0.355\n])\n    with open(\n'x2.txt'\n,\n'w'\n) as f:\n        f.write(\n''\n)\n    while t >= t_min:\n        for i in range(\n0\n,r): \n            \n            write = {\n'id'\n:\n0\n,\n'x2'\n:\n''\n,\n'atoms'\n:atoms1}\n            if skl == \n1\n:\n                count_skl_1 += \n1\n\n            elif skl == \n2\n:\n                count_skl_2 += \n1\n\n            if count \n% jishu == 0:\n\n                count_1 = (count/jishu) /(r/\n100\n)\n                s = str(count) + \n'|'\n + str(jishu*\n100\n)\n                print(count_1,\n':'\n,\n'  1:'\n,count_skl_1,\n'  2:'\n,count_skl_2)\n                count_skl_1 = \n0\n\n                count_skl_2 = \n0\n\n                if count == jishu * \n100\n \n-1\n:\n                    print(\n'\\n'\n)\n            count += \n1\n\n            if count ==  \n1\n:\n                result = opt.minimize(cacl_ener,x0,method=\n'L-BFGS-B'\n,args = atoms1)\n                x0 = (result[\n'x'\n][\n0\n],result[\n'x'\n][\n1\n],result[\n'x'\n][\n2\n],result[\n'x'\n][\n3\n],result[\n'x'\n][\n4\n],result[\n'x'\n][\n5\n],result[\n'x'\n][\n6\n],result[\n'x'\n][\n7\n],result[\n'x'\n][\n8\n])\n                atoms1[\n1\n][\n'site'\n] = np.array([result[\n'x'\n][\n0\n],result[\n'x'\n][\n1\n],result[\n'x'\n][\n2\n]])\n                atoms1[\n3\n][\n'site'\n] = np.array([result[\n'x'\n][\n3\n],result[\n'x'\n][\n4\n],result[\n'x'\n][\n5\n]])\n                atoms1[\n5\n][\n'site'\n] = np.array([result[\n'x'\n][\n6\n],result[\n'x'\n][\n7\n],result[\n'x'\n][\n8\n]])\n                x2_1 = calc_x2(atoms1)\n        \n            q_ca = atoms1[\n4\n][\n'q'\n]\n            beta_ca = atoms1[\n4\n][\n'beta'\n]\n\n            delta_q_ca = random.uniform(\n-0.1\n,\n0.1\n)\n            if q_ca + delta_q_ca > \n20\n or q_ca + delta_q_ca < \n2\n:\n                q_ca = q_ca - delta_q_ca\n            else:\n                q_ca = q_ca + delta_q_ca\n            \n            delta_beta_ca = random.uniform(\n-0.1\n,\n0.1\n)\n            if beta_ca + delta_beta_ca >\n20\n or beta_ca + delta_beta_ca < \n0\n:\n                beta_ca -= delta_beta_ca\n            else:\n                beta_ca += delta_beta_ca\n\n\n            delta_atoms = [q_ca,beta_ca]\n            atoms2 = make_atoms(atoms1,delta_atoms)\n\n\n            result = opt.minimize(cacl_ener,x0,method=\n'L-BFGS-B'\n,args = atoms2)\n            x0 = (result[\n'x'\n][\n0\n],result[\n'x'\n][\n1\n],result[\n'x'\n][\n2\n],result[\n'x'\n][\n3\n],result[\n'x'\n][\n4\n],result[\n'x'\n][\n5\n],result[\n'x'\n][\n6\n],result[\n'x'\n][\n7\n],result[\n'x'\n][\n8\n])\n            atoms2[\n1\n][\n'site'\n] = np.array([result[\n'x'\n][\n0\n],result[\n'x'\n][\n1\n],result[\n'x'\n][\n2\n]])\n            atoms2[\n3\n][\n'site'\n] = np.array([result[\n'x'\n][\n3\n],result[\n'x'\n][\n4\n],result[\n'x'\n][\n5\n]])\n            atoms2[\n5\n][\n'site'\n] = np.array([result[\n'x'\n][\n6\n],result[\n'x'\n][\n7\n],result[\n'x'\n][\n8\n]])\n            x2_2 = calc_x2(atoms2)\n            delta_x2 = x2_2 - x2_1\n            print(delta_x2)\n            atoms1 = deal(atoms1,atoms2,delta_x2,t)\n\n            if atoms1 == atoms2:                \n                skl = \n2\n\n                print(skl)\n                x2_1 = x2_2\n            else:\n                skl = \n1\n\n            write[\n'id'\n] = count\n            write[\n'x2'\n] =x2_1\n            write[\n'atoms'\n] = atoms1\n            writes.append(write)\n            with open(\n'x2.txt'\n,\n'a'\n) as f:\n                f.write(str(write) + \n'\\n'\n)\n        t = t * \n0.95\n\n    result = atoms1\n            \n\nif \n__name__\n == \n\"__main__\"\n:\n    atoms = [{\n'beta'\n: \n9.139\n, \n'q'\n: \n5.590\n, \n'site'\n: np.array([\n0\n, \n1.869\n, \n-0.361\n])}, \\\n                  {\n'beta'\n: \n9.139\n, \n'q'\n: \n-6.590\n, \n'site'\n: np.array([\n-0.23324962\n,  \n0.16647118\n,  \n0.29554485\n])}, \\\n                  {\n'beta'\n: \n9.139\n, \n'q'\n: \n5.590\n, \n'site'\n: np.array([\n0\n, \n-1.869\n, \n-0.361\n])}, \\\n                  {\n'beta'\n: \n9.139\n, \n'q'\n: \n-6.590\n, \n'site'\n: np.array([\n-0.23324962\n,  \n0.16647118\n,  \n0.29554485\n])}, \\\n                  {\n'beta'\n: \n13.772\n, \n'q'\n: \n20\n, \n'site'\n: np.array([\n0.\n, \n0.\n    , \n0.325\n    ])}, \\\n                  {\n'beta'\n: \n13.772\n, \n'q'\n: \n-22\n, \n'site'\n: np.array([\n0.15000612\n, \n0.0927579\n , \n0.29707778\n])}]\n    count = \n0\n\n    re = \n0.1564\n\n    u_exp =  \n3.07\n\n    a_f = \n1.3\n\n    a_ca = \n4.4\n\n    x = []\n    writes = []\n    result = \n''\n\n    solve()\n\n", "Tag": "算法分析"}
{"Answer": "\nThe original Rules field changes because pointers and slices (which are references as well) are used.\nBefore calling OstarCF, the ChainsTo method is called. It uses the grammar object by value, so a copy is done, but the Rules field is a slice of pointers on Rules. So when this field is copied, it still points to the data of the original object.\nThen, in method ChainsTo, there is a loop on the Rules field. It copies the Right field which is a slice of strings (so it still points to data of the original object):\nrhs := rule.Right\n\nFinally, a ns variable is declared by slicing rhs:\nns := rhs[:i]\nns = append(ns, rhs[i+1:]...)\n\nAt this stage, the ns variable still points to the buffer containing the slice of strings of the original object. Initially, i=0, so ns is an empty slice reusing the buffer. When items are appended, they replace the original data.\nThat's why your data are changed.\nYou can fix this problem by explicitly making a copy, for instance by replacing the above lines by:\nns := make( []string, 0, len(rhs) )\nns = append( ns, rhs[:i]...)\nns = append( ns, rhs[i+1:]...)\n\nGo slices have replaced C pointer arithmetic, but they can be almost as dangerous/misleading in some cases.\n", "Konwledge_Point": "应对NP完全问题", "Question": "非常令人困惑的变量更改\n\n\n\nhttp://play.golang.org/p/Vd3meom5VF\n\n\n\nI have this code for some context free grammar in Go\n\n\n\nAnd I am looking at this code so many times and still don't see any reason for the struct values to be changed. Could anybody see why the change like the following happens?\n\n\n\nRules:\n\nS -> . [DP VP]\n\nVP -> . [V DP]\n\nVP -> . [V DP AdvP]\n\n\n\nAfter I run some functions as in the line \n\n\n\n or2 = append(or2, OstarCF([]QRS{q}, []string{\"sees\"}, g2.Nullables(), g2.ChainsTo(g2.Nullables()))...)\n\n\n\n\nSomehow my struct value is changed... I don't know why...\n\n\n\nRules:\n\nS -> . [VP VP]\n\nVP -> . [DP DP]\n\nVP -> . [AdvP AdvP AdvP]\n\n\n\nThis should have been same as above.\n\n\n\n Rules:\n S -> DP,VP\n VP -> V,DP\n VP -> V,DP,AdvP\n\n or2 := []QRS{}\n g2 := ToGrammar(cfg2)\n fmt.Printf(\"%s\n\", g2)\n\n for _, rule := range g2.Rules {\n        q := QRS{\n            one:   rule.Src,\n            two:   []string{},\n            three: rule.Right,\n        }\n        or2 = append(or2, OstarCF([]QRS{q}, []string{\"sees\"}, g2.Nullables(), g2.ChainsTo(g2.Nullables()))...)\n    }\n\n    fmt.Printf(\"%s\n\", g2)\n\n\n\n\nAs you see, I do not use any pointer the variable \nrule\n, and they are only used to instantiate another struct value, but how come the original struct field \nrule\n has changed? The function OstarCF does not do anything about this field \nrule\n\n\n\n func OstarCF(Qs []QRS, R []string, nD map[string]bool, cD map[string][]string) []QRS {\n    symbols := []string{}\n    for _, r := range R {\n        symbols = append(symbols, cD[r]...)\n    }\n    product := []QRS{}\n    for _, Q := range Qs {\n        a := Q.one\n        b := Q.two\n        c := Q.three\n        if len(c) > 0 && CheckStr(c[0], symbols) {\n            b = append(b, c[0])\n            np := QRS{\n                one:   a,\n                two:   b,\n                three: c[1:],\n            }\n            product = append(product, np)\n\n            for len(np.three) > 0 && nD[np.three[0]] == true {\n                np.two = append(np.two, np.three[0])\n                np = QRS{\n                    one:   np.one,\n                    two:   np.two,\n                    three: np.three[1:],\n                }\n                product = append(product, np)\n            }\n        }\n    }\n    return product\n }\n\n\n    ", "Tag": "算法分析"}
{"Answer": "都一樣的，第二種只是第一種的合并寫法。", "Konwledge_Point": "应对NP完全问题", "Question": "请问这么计算银行的本息是可以吗？哪个是对？\n问题：谭浩强《C程序设计》（第四版）3-2习题\n\n\n用的是编译器是devC++\n\n\n大一新生，和室友，磨了俩小时出来的\n\n\n这是第一版\n\n\n\n\n#include \n\n\n#include\n\n\nint\n main()\n{\n    float \np0\n=1000\n,\nr1\n,\nr2\n,\nr3\n,\nr5\n,\nr0\n,\np1\n,\np2\n,\np3\n,\np4\n,\np5\n,w2,w3,w4,w5,i\n;\n\n    \nr1\n=4\n.\n14\n/\n100\n,\nr2\n=4\n.\n68\n/\n100\n,\nr3\n=5\n.\n4\n/\n100\n,\nr5\n=5\n.\n85\n/\n100\n,\nr0\n=0\n.\n72\n/\n100\n,i\n=1\n;\n\n    \n    \np1\n=(\n1\n+\nr5\n*\n5\n)*\np0\n;\n\n    \n    w2=(\n1\n+\nr2\n*\n2\n)*\np0\n;\n\n    \np2\n=(\n1\n+\nr3\n*\n3\n)*w2\n;\n\n    \n    w3=(\n1\n+\nr3\n*\n3\n)*\np0\n;\n\n    \np3\n=(\n1\n+\nr2\n*\n2\n)*w3\n;\n\n    \n    \n    \np4\n=(\n1\n.\n0\n+\nr1\n)*\np0\n;\n\n    \nwhile\n(i<\n5\n)\n    {\n    \np4\n=(\n1\n.\n0\n+\nr1\n)*\np4\n;\n\n    i\n=i\n+\n1\n;    \n\n    }\n    \n    \np5\n=(\n1\n+\nr0\n/\n4\n)*\np0\n;\n\n    \nwhile\n(i<\n20\n)\n    {\n        \np5\n=(\n1\n+\nr0\n/\n4\n)*\np5\n;\n\n        i\n=i\n+\n1\n;\n\n    }\n    \n    printf(\n\"p1=%f\\np2=%f\\np3=%f\\np4=%f\\np5=%f\"\n,\np1\n,\np2\n,\np3\n,\np4\n,\np5\n)\n;\n\n    return \n0\n;\n\n}\n\n\n\n这是第二版\n\n\n#include \n\n\n#include\n\n\nint\n main()\n{\n    float \np0\n=1000\n,\nr1\n,\nr2\n,\nr3\n,\nr5\n,\nr0\n,\np1\n,\np2\n,\np3\n,\np4\n,\np5\n,n\n;\n\n    \nr1\n=4\n.\n14\n/\n100\n,\nr2\n=4\n.\n68\n/\n100\n,\nr3\n=5\n.\n4\n/\n100\n,\nr5\n=5\n.\n85\n/\n100\n,\nr0\n=0\n.\n72\n/\n100\n,n\n=5\n;\n\n    \n    \np1\n=(\n1\n+\nr5\n*n)*\np0\n;\n\n    \n    \np2\n=(\n1\n+\nr2\n*\n2\n)*(\n1\n+\nr3\n*\n3\n)*\np0\n;\n\n    \n    \np3\n=(\n1\n+\nr2\n*\n2\n)*(\n1\n+\nr3\n*\n3\n)*\np0\n;\n\n    \n    \n    \np4\n=p0\n*pow((\nr1\n+\n1\n),n)\n;\n\n    \n    \np5\n=p0\n*pow((\nr0\n/\n4\n+\n1\n),\n4\n*n)\n;\n\n    \n    printf(\n\"p1=%f\\np2=%f\\np3=%f\\np4=%f\\np5=%f\"\n,\np1\n,\np2\n,\np3\n,\np4\n,\np5\n)\n;\n\n    return \n0\n;\n\n}\n\n", "Tag": "算法分析"}
{"Answer": "说明你arr里存的有str有float，必须类型一致", "Konwledge_Point": "应对NP完全问题", "Question": "can only concatenate str (not \"float\") to str\n运行这个命令提示类型报错\nmean = np.array(arr).mean()", "Tag": "算法分析"}
{"Answer": "我破了这个报错了家人们！！我看了一下fbank函数的解释，fbank的函数返回是个元组，这个元组应该是有俩元素，一个是特征向量一个是每一帧的对数能量。我把fbank函数全换成logfbank函数就行啦！因为logfbank函数返回的只有一个特征向量！", "Konwledge_Point": "应对NP完全问题", "Question": "报错'tuple' object has no attribute 'shape'原因是元组不可改变？应该怎么破？\n这是我的代码，是提取fbank特征的\n\n\n代码如下：\n\n\n\n\n# -*- coding: utf-8 -*-\n\n\n#导入相关的库\n\n\nfrom\n keras.models \nimport\n Model\n\nfrom\n keras.layers \nimport\n Input, Activation, Conv1D, Lambda, Add, Multiply, BatchNormalization\n\nfrom\n keras.optimizers \nimport\n Adam, SGD\n\nfrom\n keras \nimport\n backend \nas\n K\n\nfrom\n keras.callbacks \nimport\n ModelCheckpoint, ReduceLROnPlateau\n\n\nimport\n numpy \nas\n np\n\nimport\n matplotlib.pyplot \nas\n plt\n\nfrom\n mpl_toolkits.axes_grid1 \nimport\n make_axes_locatable\n\n\nimport\n random\n\nimport\n pickle\n\nimport\n glob\n\nfrom\n tqdm \nimport\n tqdm\n\nimport\n os\n\n\nfrom\n python_speech_features \nimport\n fbank\n\nimport\n scipy.io.wavfile \nas\n wav\n\nimport\n librosa\n\nfrom\n IPython.display \nimport\n Audio\n\n\n\n\n#读取数据集文件\n\ntext_paths = glob.glob(\n'E:\\叶儿\\课程课程\\毕设\\data2try/*.trn'\n)\ntotal = \nlen\n(text_paths)\n\nprint\n(total)\n\n\nwith\n \nopen\n(text_paths[\n0\n], \n'r'\n, encoding=\n'utf8'\n) \nas\n fr:\n    lines = fr.readlines()\n    \nprint\n(lines)\n\n\n\n\n#数据集文件trn内容读取保存到数组中\n\ntexts = []\npaths = []\n\nfor\n path \nin\n text_paths:\n    \nwith\n \nopen\n(path, \n'r'\n, encoding=\n'utf8'\n) \nas\n fr:\n        lines = fr.readlines()\n        line = lines[\n0\n].strip(\n'\\n'\n).replace(\n' '\n, \n''\n)\n        texts.append(line)\n        paths.append(path.rstrip(\n'.trn'\n))\n\n\nprint\n(paths[\n0\n], texts[\n0\n])\n\n\n\n\n\n\n#根据数据集标定的音素读入\n\n\ndef\n \nload_and_trim\n(\npath\n):\n    audio, sr = librosa.load(path)\n    energy = librosa.feature.rms(audio)\n    frames = np.nonzero(energy >= np.\nmax\n(energy) / \n5\n)\n    indices = librosa.core.frames_to_samples(frames)[\n1\n]\n    audio = audio[indices[\n0\n]:indices[-\n1\n]] \nif\n indices.size \nelse\n audio[\n0\n:\n0\n]\n\n    \nreturn\n audio, sr\n\n\n#可视化，显示语音文件的Fbank图\n\n\ndef\n \nvisualize\n(\nindex\n):\n    path = paths[index]\n    text = texts[index]\n    \nprint\n(\n'Audio Text:'\n, text)\n\n    audio, sr = load_and_trim(path)\n    plt.figure(figsize=(\n12\n, \n3\n))\n    plt.plot(np.arange(\nlen\n(audio)), audio)\n    plt.title(\n'Raw Audio Signal'\n)\n    plt.xlabel(\n'Time'\n)\n    plt.ylabel(\n'Audio Amplitude'\n)\n    plt.show()\n\n    feature = fbank(audio, sr,nfft=\n512\n)\n    \nprint\n(\n'Shape of Fbank:'\n, feature.shape)\n\n\n\n\n就是最后一行代码有报错，报错如下：\n\n\n  File \"\n\", line 91, in \n    Audio(visualize(0))\n\n\n  File \"\n\", line 76, in visualize\n    print('Shape of Fbank:', feature.shape)\n\n\nAttributeError: 'tuple' object has no attribute 'shape'\n\n\n这个no attribute 要怎么办啊？", "Tag": "算法分析"}
{"Answer": "1.建立系统环境变量 变量名填PYTHONPATH变量值为安装后的库地址。2.在解决方案管理器中，右击搜索路径，点击将PYTHONPATH添加到搜索路径。", "Konwledge_Point": "应对NP完全问题", "Question": "用vs2022写python代码，导入的第三方库经常莫名失效。有时显示未导入却能用。\n问题遇到的现象和发生背景\n\n\n本来用vs2022编写c语言，用的挺舒服的，想着偷懒，直接也用vs2022编写python了。但是遇到了很奇怪的问题，导入的第三方库，经常性出现失效。本来编的好好的，关闭了vs2022然后重新打开一样的文件，没有其它操作，import后面就是绿色波浪线了。这种现象随机出现，可能再重启一下vs就好了。除此之外，还会可能与到一种情况，虽然显示导入不成功，但库中包含的内容却能用，只是函数不高亮。\n\n\n错误信息\n\n\n\n\n第一张图，左边python环境里显示了我安装了numpy，然而import还是显示绿色波浪线。 这问题太折磨人了，莫名其妙的，有人能指点一下这是vs2022的问题还是其他的呢。\n第二张图，显示导入未成功，但是opencv的功能却实现了。\n\n\n代码块\n\n\nimport cv2\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport time\n\n\n我的解答思路和尝试过的方法。\n\n\n一、考虑到可能是包的问题，我删掉了所有包并重新下载，没能解决问题。\n二、之前总报错UnicodeDecodeError: ‘utf-8‘ codec can‘t decode byte 0xc5 in position 13: invalid continuation byte，觉得是字的编码问题，我就把保存方式改成utf-8了，不知道和字的编码有没有关系。", "Tag": "算法分析"}
{"Answer": "应该是这个吧，你训练之后得到的模型是H，prediction = H.predict(img)  ", "Konwledge_Point": "应对NP完全问题", "Question": "keras 二分类预测结果几乎全是一个值\n程序是用来对蜜蜂(bee)和黄蜂(wasp)分类的，用的模型是在vgg16上拼接的，代码如下\n\n\n\n\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.layers import Dense, Flatten, Activation, Dropout\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nimport keras\nimport shutil\nimport os\n\ndef creatDataGenerator(train_dir, test_dir):\n    train_data_generator = ImageDataGenerator(rescale=.1/255)\n    test_data_generator = ImageDataGenerator(rescale=.1/255)\n\n    train_generator = train_data_generator.flow_from_directory(train_dir,\n                                                            target_size=(150,150),\n                                                            batch_size=32,\n                                                            class_mode='binary')\n    test_generator = test_data_generator.flow_from_directory(test_dir,\n                                                            target_size=(150,150),\n                                                            batch_size=32,\n                                                            class_mode='binary')\n    return train_generator, test_generator\n\nvgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))\n\ncla_model = Sequential()\ncla_model.add(Flatten())\ncla_model.add(Dense(512, activation='relu'))\ncla_model.add(Dropout(0.5))\ncla_model.add(Dense(1, activation='sigmoid'))\n\nmodel = Sequential()\nmodel.add(vgg_model)\nmodel.add(cla_model)\n\nmodel.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n\ntrain_generator, test_generator = creatDataGenerator(train_dir=r'C:\\Users\\ayana\\.keras\\datasets\\bee-vs-wasp\\train',\n                                                    test_dir=r'C:\\Users\\ayana\\.keras\\datasets\\bee-vs-wasp\\test')\nH = model.fit(train_generator,\n              steps_per_epoch=50,\n              epochs=30,\n              validation_data=test_generator,\n              validation_steps=50)\n\n\n\n然后训练以后进行预测，选择的是黄蜂的10张图（蜜蜂预测出来也是同样的结果）\n\n\n\n顺便训练的准确率也比较低，不到0.6，也一直不知道怎么能高一些\n\n\n\n\n\nfrom keras.preprocessing.image import load_img, img_to_array\nimport numpy as np\n\ndef predict(i):\n    img_path = os.listdir(r'C:\\Users\\ayana\\.keras\\datasets\\bee-vs-wasp\\test\\wasp')[i]\n    img = load_img(path='C:\\\\Users\\\\ayana\\\\.keras\\\\datasets\\\\bee-vs-wasp\\\\test\\\\wasp\\\\'+img_path, \n                    target_size=(150,150))\n    img = np.expand_dims(img, axis=0)/255\n    prediction = model.predict(img)\n    return prediction\n\nfor i in range(10):\n    print(predict(i))\n\n#>>>[[0.4714901]]\n#    [[0.4714901]]\n#    [[0.4714901]]\n#    [[0.4714901]]\n#    [[0.4714901]]\n#    [[0.4714901]]\n#    [[0.4714901]]\n#    [[0.4714901]]\n#    [[0.4714901]]\n#    [[0.4714901]]\n\n\n\n再用np.argmax()的话就都是0了\n\n\n\n被困了一天了，#求救", "Tag": "算法分析"}
{"Answer": "你这好多变量看不到定义，不知道格式不好分析啊按照你的代码GHI是一个24xdays的矩阵，GHI[hour]是一个长度为days的数组，但是houston_monthly[houston_monthly['Hour'] == hour]按houston_monthly['Hour'] == hour取出来的行数明显与days是不同的，一个是28，一个是31，因为没有数据，也不知道你的意图，无法判断应该怎么改，两个长度一个是最大天数，一个是按小时数取的，除非你的数据中一个月每天每小时都有唯一一条数据，这两个数才会相等，否则的话总是会有偏差的，你可以按照这个思路去检查一下你的数据和代码，如果搞不明白，那请把数据附上来，把你的具体的目的描述一下", "Konwledge_Point": "应对NP完全问题", "Question": "取每个月的最大天数报错\n我希望取每个月的最大天数代入下一个循环，但是取出来的只有一个最大值31，报错\n\n\n###### ```python\n\n我的代码\n {for month in range(1,13):\n    houston_monthly=houston\n[houston\n['Month']\n==month]\n\n    X=houston_monthly\n[\n['GHI','DNI','Temperature','Price']\n]\n\n    C=X.corr()\n    days = max(houston_monthly\n['Day']\n)\n    GHI = np.zeros(\n[24, days]\n)\n    Temp = np.zeros(\n[24, days]\n)\n    DNI=np.zeros(\n[24, days]\n)\n    rtp=np.zeros(\n[24, days]\n)\n    c=np.zeros(\n[24,3,3]\n)\n    for hour in range(24):\n            GHI\n[hour]\n = houston_monthly\n[houston_monthly\n['Hour']\n == hour]\n['GHI']\n/1000\n            Temp\n[hour]\n = houston_monthly\n[houston_monthly\n['Hour']\n == hour]\n['Temperature']\n\n            DNI\n[hour]\n = houston_monthly\n[houston_monthly\n['Hour']\n == hour]\n['DNI']\n / 1000\n            rtp\n[hour]\n = houston_monthly\n[houston_monthly\n['Hour']\n == hour]\n['Price']\n\n            c = np.corrcoef(np.array(\n[GHI\n[hour]\n,Temp\n[hour]\n,DNI\n[hour]\n,rtp\n[hour]\n]\n))}\n```，\n\n\n\n\n\n\n{---------------------------------------------------------------------------\n\nValueError\n                                \nTraceback\n (most recent call last)\n/var/folders/\n1\nt/mwj66mvx0mv00rs70yc3hgk40000gn/\nT\n/ipykernel_14153/\n1750111386.\npy in \n     \n17\n     c=np.zeros([\n24\n,\n3\n,\n3\n])\n     \n18\n     for hour in range(\n24\n):\n---> \n19\n             \nGHI\n[hour] = houston_monthly[houston_monthly[\n'Hour'\n] == hour][\n'GHI'\n]/\n1000\n\n     \n20\n             \nTemp\n[hour] = houston_monthly[houston_monthly[\n'Hour'\n] == hour][\n'Temperature'\n]\n     \n21\n             \nDNI\n[hour] = houston_monthly[houston_monthly[\n'Hour'\n] == hour][\n'DNI'\n] / \n1000\n\n\n\nValueError\n: could not broadcast input array from shape (\n28\n,) into shape (\n31\n,)} \n\n\n\n\n求指教应该怎么改？", "Tag": "算法分析"}
{"Answer": "\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef main():\n    matplotlib.rcParams['font.family'] = 'SimHei'\n    stuScore = np.loadtxt('student_score.csv', delimiter=',')  # 读入成绩文件,返回数组\n    sumEach = np.sum(stuScore[:, 1:], axis=1)  # 返回每个学生3门课程总分\n    avgEach = np.average(stuScore[:, 1:], axis=0)  # 返回每个学生每门课程平均分\n    # 取出各科成绩\n    mathScore = stuScore[:, 1]\n    engScore = stuScore[:, 2]\n    pythonScore = stuScore[:, 3]\n    # Performanceanalysis(avgEach, stuScore, sumEach)\n    while True:\n        print(\"\"\"成绩分析与可视化系统  \n 1: 基本信息显示      \n 2: 成绩分析          \n 3: 可视化         \n 4: 退出系统\"\"\")\n        operation = input(\"请输入你的操作\")\n        if operation.isdigit():\n            operation = int(operation)\n            if operation == 1:\n                print(\" 学号  高数  英语  python\")\n                for i in stuScore:\n                    print(f\"{int(i[0])} {i[1]} {i[2]} {i[3]}\")\n            elif operation == 2:\n                Performanceanalysis(avgEach, stuScore, sumEach)\n            elif operation == 3:\n                # name= input(\"请输入课程名\")\n                # if name=='xxx':\n                # 由于不清楚你的课程名是啥,你这里自己填 if elif else结构就可以\n                Highnumberhistogram(mathScore)\n                Englishhistogram(engScore)\n                Scorehistogram(pythonScore)\n            elif operation == 4:\n                import sys\n                sys.exit(0)\n            else:\n                print(\"输入错误,请重新输入\")\n\n\ndef Performanceanalysis(avgEach, stuScore, sumEach):\n    # 返回最高分和最低分\n    maxMath = np.max(stuScore[:, 1])\n    maxEng = np.max(stuScore[:, 2])\n    maxPython = np.max(stuScore[:, 3])\n    minMath = np.max(stuScore[:, 1])\n    minEng = np.max(stuScore[:, 2])\n    minPython = np.max(stuScore[:, 3])\n    print(\"个人总分情况是：\")\n    print(sumEach)\n    print(\"个人平均分情况是：\")\n    print(avgEach)\n    print(\"班级每门课程最高分：\")\n    print(maxMath, maxEng, maxPython)\n    print(\"班级每门课程最低分：\")\n    print(minMath, minEng, minPython)\n\n\ndef Highnumberhistogram(mathScore):\n    # 绘制高数直方图\n    plt.suptitle(\"成绩分布直方图\")\n    plt.subplot(3, 1, 1)\n    plt.hist(mathScore, bins=10, range=(0, 100), color='red')  # 0-100分,分成10段\n    plt.xlabel(\"高数成绩分数段\")  # 设置x轴标签\n    plt.ylabel(\"人数\")  # 设置y轴标签\n    plt.xlim(0, 100)  # 设置x轴区间\n    plt.xticks([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])  # 设置x轴刻度\n    plt.yticks([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])  # 设置y轴刻度\n    # plt.grid()\n    plt.show()\n\n\ndef Englishhistogram(engScore):\n    # 绘制英语直方图\n    plt.subplot(3, 1, 2)\n    plt.hist(engScore, bins=10, range=(0, 100), color='blue')  # 0-100分,分成10段\n    plt.xlabel(\"英语成绩分数段\")  # 设置x轴标签\n    plt.ylabel(\"人数\")  # 设置y轴标签\n    plt.xlim(0, 10)  # 设置x轴区间\n    plt.xticks([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])  # 设置x轴刻度\n    plt.yticks([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])  # 设置y轴刻度\n    # plt.grid()\n    plt.show()\n\n\ndef Scorehistogram(pythonScore):\n    # 绘制python直方图\n    plt.suptitle(\"成绩分布直方图\")\n    plt.subplot(3, 1, 3)\n    plt.hist(pythonScore, bins=10, range=(0, 100), color='green')  # 0-100分,分成10段\n    plt.xlabel(\"Python成绩分数段\")  # 设置x轴标签\n    plt.ylabel(\"人数\")  # 设置y轴标签\n    plt.xlim(0, 100)  # 设置x轴区间\n    plt.xticks([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])  # 设置x轴刻度\n    plt.yticks([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])  # 设置y轴刻度\n    # plt.grid()\n    plt.show()\n\n\nif __name__ == '__main__':\n    main()\n\n有帮助请点一下右上角的采纳,谢谢", "Konwledge_Point": "应对NP完全问题", "Question": "某班有30名学生的3门课程成绩，请统计每个学生课程的总分、平均分，每门课程的最高分、最低分，并绘制图形，统计成绩分布。\n                 成绩分析及可视化实例\n\n\n\n【功能要求】某班有30名学生的3门课程成绩，请统计每个学生课程的总分、平均分，每门课程的最高分、最低分，并绘制图形，统计成绩分布。\n【教学目标】强化numpy和matplotlib的应用能力，numpy读取csv。\n1.模块化（菜单1分）\n\n\n\n\n \n成绩分析与可视化系统  \n 1\n:\n \n基本信息显示      \n\n \n2\n:\n \n成绩分析          \n\n \n3\n:\n \n可视化         \n\n \n4\n:\n \n退出系统           \n\n\n\n\n\n\n2.显示：打开文件显示全部信息（3分）\n3.成绩分析：显示各科最高分、最低分、平均分，每个学生的总分（3分）\n4.可视化：成绩直方图：输入某门课程名，显示对应的直方图（3分）\n【参考代码】\n#成绩分析及可视化实例\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.rcParams['font.family'] = 'SimHei'\nstuScore = np.loadtxt('student_score.csv',delimiter = ',')#读入成绩文件,返回数组\nsumEach = np.sum(stuScore[:,1:],axis = 1)#返回每个学生3门课程总分\navgEach = np.average(stuScore[:,1:],axis = 0)#返回每个学生每门课程平均分\n#返回最高分和最低分\nmaxMath = np.max(stuScore[:,1])\nmaxEng = np.max(stuScore[:,2])\nmaxPython = np.max(stuScore[:,3])\nminMath = np.max(stuScore[:,1])\nminEng = np.max(stuScore[:,2])\nminPython = np.max(stuScore[:,3])\nprint(\"个人总分情况是：\")\nprint(sumEach)\nprint(\"个人平均分情况是：\")\nprint(avgEach)\nprint(\"班级每门课程最高分：\")\nprint(maxMath,maxEng,maxPython)\nprint(\"班级每门课程最低分：\")\nprint(minMath,minEng,minPython)\n#取出各科成绩\nmathScore = stuScore[:,1]\nengScore = stuScore[:,2]\npythonScore = stuScore[:,3]\n#绘制高数直方图\nplt.suptitle(\"成绩分布直方图\")\nplt.subplot(3,1,1)\nplt.hist(mathScore,bins=10,range=(0,100),color='red')#0-100分,分成10段\nplt.xlabel(\"高数成绩分数段\")#设置x轴标签\nplt.ylabel(\"人数\")#设置y轴标签\nplt.xlim(0,100)#设置x轴区间\nplt.xticks([0,10,20,30,40,50,60,70,80,90,100])#设置x轴刻度\nplt.yticks([0,10,20,30,40,50,60,70,80,90,100]) #设置y轴刻度\nplt.grid()\n#绘制英语直方图\nplt.subplot(3,1,2)\nplt.hist(engScore,bins=10,range=(0,100),color='blue')#0-100分,分成10段\nplt.xlabel(\"英语成绩分数段\")#设置x轴标签\nplt.ylabel(\"人数\")#设置y轴标签\nplt.xlim(0,10)#设置x轴区间\nplt.xticks([0,10,20,30,40,50,60,70,80,90,100])#设置x轴刻度\nplt.yticks([0,10,20,30,40,50,60,70,80,90,100]) #设置y轴刻度\nplt.grid()\n#绘制python直方图\nplt.suptitle(\"成绩分布直方图\")\nplt.subplot(3,1,3)\nplt.hist(pythonScore,bins=10,range=(0,100),color='green')#0-100分,分成10段\nplt.xlabel(\"Python成绩分数段\")#设置x轴标签\nplt.ylabel(\"人数\")#设置y轴标签\nplt.xlim(0,100)#设置x轴区间\nplt.xticks([0,10,20,30,40,50,60,70,80,90,100])#设置x轴刻度\nplt.yticks([0,10,20,30,40,50,60,70,80,90,100]) #设置y轴刻度\nplt.grid()\nplt.show()", "Tag": "算法分析"}
{"Answer": "```\r\ndef loadDataSet(self,fileName):\r\n```", "Konwledge_Point": "应对NP完全问题", "Question": "python类内部的方法调用\nclass Adaboosting中demo_train方法中要调用同个类中的loadDataSet方法，但是根本没进到程序之中，求教是什么原因，谢谢大佬们了\n\ndemo_train的代码如下\n\n\n\n        def demo_train(self):\n                print(\"已进入到了内部函数\")\n                print(self.model)\n                if self.model==None:\n                        self.model=\"test.txt\"\n                dataArr, LabelArr = self.loadDataSet(self.model)\n                weakClassArr, aggClassEst = Adaboosting.adaBoostTrainDS(dataArr, LabelArr)\n                # from my_adaboosting_SVM_ROC import adaClassify\n                print(\"正在训练模型...\")\n                predictions = self.adaClassify(dataArr, weakClassArr)\n                errArr = np.mat(np.ones((len(dataArr), 1)))\n                number=1-float(errArr[predictions != np.mat(LabelArr).T].sum() / len(dataArr) * 100)\n                print('训练集的错误率:%.3f%%' % float(errArr[predictions != np.mat(LabelArr).T].sum() / len(dataArr) * 100))\n                # print(\"33333333\")\n                print(self.train)\n                if self.train==None:\n                        self.train=\"test6.txt\"\n                # print(\"444444444\")\n                print(self.train)\n                # train = \"test6.txt\"\n                testArr, testLabelArr = self.loadDataSet(self.train)\n                predictions = self.adaClassify(testArr, weakClassArr)\n                print(predictions)\n                errArr = np.mat(np.ones((len(testArr), 1)))\n                number1=1-float(errArr[predictions != np.mat(testLabelArr).T].sum() / len(testArr) * 100)\n                print('测试集的错误率:%.3f%%' % float(errArr[predictions != np.mat(testLabelArr).T].sum() / len(testArr) * 100))\n                from my_adaboosting_SVM_ROC import plotROC\n                plotROC(aggClassEst.T, LabelArr)\n                return number ,number1\n\n\n\n\nloadDataSet的代码如下\n\n\n\n        # 导入数据\n        def loadDataSet(fileName):\n                print(\"hhhhhhhhhhhhhhhhh\")\n                numFeat = len((open(fileName).readline().split('\\t')))\n                dataMat = [];\n                labelMat = []\n                fr = open(fileName)\n                for line in fr.readlines():\n                        lineArr = []\n                        curLine = line.strip().split('\\t')\n                        for i in range(numFeat - 1):\n                                lineArr.append(float(curLine[i]))\n                        dataMat.append(lineArr)\n                        labelMat.append(float(curLine[-1]))\n\n                return dataMat, labelMat\n\n\n\n\n运行结果：", "Tag": "算法分析"}
{"Answer": "在dx=(t[1]-t[0])/2之前，加print(t)输出t看看是不是只有1个维度", "Konwledge_Point": "应对NP完全问题", "Question": "matplotlib.colors.LogNorm代码设置横纵参数报错\n问题遇到的现象和发生背景\n\n\nmatplotlib.colors.LogNorm代码设置横纵参数的时候出错\n\n\n问题相关代码，请勿粘贴截图\n\n\n    \nnm\n= matplotlib.colors.LogNorm(vmin=psd.min(),vmax=psd.max())\n    \ndx\n=(t[\n1\n]-t[\n0\n])/\n2\n\n    \ndy\n=(f[\n1\n]-f[\n0\n])/\n2\n\n    \nxmesh\n=t-dx\n    \nxmesh\n=np.append(xmesh,t[-\n1\n]+dx)\n    \nymesh\n=f-dy\n    \nymesh\n=np.append(ymesh,f[-\n1\n]+dy)\n\n\n\n\n运行结果及报错内容\n\n\nwarnings\n.warn('nperseg = {\n0\n:d} is greater than input length '\n\nUserWarning\n: nperseg = \n256\n is greater than input length  = \n100\n, using nperseg = \n100\n\n\nTraceback\n (most recent call last):\n  \nFile\n \n\"E:\\python practice\\1234\\plmi.py\"\n, line \n83\n, in \n    \ndx\n=(t[\n1\n]-t[\n0\n])/\n2\n\n\nIndexError\n: index \n1\n is out of bounds for axis \n0\n with size \n1\n\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果\n\n", "Tag": "算法分析"}
{"Answer": "代码是用于读取tif图形文件立体呈现地形，在用数据文件data\\alaska\\clipped_elev.tif正常显示，使用其他数据文件显示不正常，说明代码没有问题，是数据文件本身问题，检查数据格式是否符合要求。", "Konwledge_Point": "应对NP完全问题", "Question": "mayvai使用tif绘制地形图\n问题遇到的现象和发生背景\n\n\n使用在国家地理云数据平台下载的DEM高程数据绘制地形图时，x与y轴似乎合在了一起\n\n\n问题相关代码，请勿粘贴截图\n\n\n\n\nimport\n numpy \nas\n np\n\nfrom\n mayavi \nimport\n mlab\n\nfrom\n osgeo \nimport\n gdal\ngdal.UseExceptions()\n\ndef \nread\n(filename):\n    ds = gdal.\nOpen\n(filename)\n    elev = ds.ReadAsArray()\n\n    # \nTrue\n x, y coordinates\n    x0, dx, dxdy, y0, dydx, dy = ds.GetGeoTransform()\n    i, j = np.mgrid[:elev.shape[\n0\n], :elev.shape[\n1\n]]\n    x = x0 + dx * j + dxdy * i\n    y = y0 + dy * i + dydx * j\n\n    \nreturn\n ds.ReadAsArray(), x, y\n\nz, x, y = \nread\n(\n'ASTGTMV003_N02W064_dem.tif'\n)\n\n\nfig = mlab.figure()\nmlab.mesh(x, y, z, colormap=\n'gist_earth'\n)\nmlab.axes()\nmlab.\nshow\n()\n\n\n\n运行结果及报错内容\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n使用原GitHub中的tif文件则可绘出地形图 \nhttps://github.com/joferkington/scipy2015-3d_printing\n\n\n我想要达到的结果\n\n\n问题出在哪", "Tag": "算法分析"}
{"Answer": "截取前10个字符就行了啊，望采纳", "Konwledge_Point": "应对NP完全问题", "Question": "首次发言，请各位帮助一下，excel导入后显示的年月日问题\n1.问题遇到的现象和发生背景\n经过学习开始了第一次的编程尝试，目的是做一个按年月日显示温度的表格，目前目标是显示一个区域的温度。\n最终目标是显示3个区域的温度以及从网络实时读取当日的温度，并且X轴显示的日期为间隔10天或者20天。\n目前遇到的问题是，在导入excel中的时间和温度以后，时间的显示出现了问题，一直显示为：2021-12-11 00:00:00。\n这种带有后面时间的数据，导致生成的表格X轴显示不正确。\n\n\n2.问题相关代码，请勿粘贴截图\n以下为我的详细代码\nimport openpyxl as op\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime\nimport xlrd\n\n\nwb = op.load_workbook('R&D温度湿度_2021.xlsx')\nsheet = wb['Sheet1']\ndate_1 = []\ntemp_1 = []\nn = 4\nwhile 1:\n    p_date = 'A'+str(n)\n    p_temp = 'B'+str(n)\n    if sheet[p_date].value == None:\n        break\n    date_1.append(sheet[p_date].value)\n    temp_1.append(sheet[p_temp].value)\n    n = n + 1\nfor i in  range(len(date_1)):\n    print(date_1[i],' ',temp_1[i])\n\n\nx = date_1\ny = temp_1\nplt.figure(figsize=(30,8),dpi=720)\nplt.plot(x,y,color=\"r\",linestyle='-.',label='wenshidu')\nplt.legend()\nx_lable=[\"{}月30日\".format(i)for i in x]\nplt.xticks(x[::5],x_lable[::5])#设置横轴大小\nplt.yticks(range(0,40,5)) #y轴温度范围\nplt.grid(linestyle='--',alpha=0.5)#添加网格显示\nplt.xlabel('riqi')#X轴标题\nplt.ylabel('wendu')#Y轴标题\nplt.title('tianqi')#表头\nplt.show\n\n\n3.运行结果及报错内容\n部分运行结果如下所示：\n2021-09-29 00:00:00   21.6\n2021-09-30 00:00:00   24.6\n2021-10-01 00:00:00   23.2\n2021-10-02 00:00:00   23\n2021-10-03 00:00:00   21.4\n2021-10-04 00:00:00   22.1\n2021-10-05 00:00:00   20.5\n2021-10-06 00:00:00   19.6\n2021-10-07 00:00:00   24.8\n2021-10-08 00:00:00   23.4\n\n\nexcel部分数据如下所示：\n    1F回风\n日付    温度    湿度\n2021/9/29    21.6    76.6\n2021/9/30    24.6    68.4\n2021/10/1    23.2    75.3\n2021/10/2    23    61.5\n2021/10/3    21.4    85.8\n2021/10/4    22.1    48.2\n2021/10/5    20.5    48.7\n2021/10/6    19.6    54.1\n\n\n4.我的解答思路和尝试过的方法\n尝试了使用datetime.date 但是没有成功，转换的方式还没有掌握\n也可能是从一开始思路错误，自己无法判断\n\n\n5.我想要达到的结果\n想要正确的将时间显示为年/月/日 并且可以在图表中反应出来\n\n\n最后，感谢各位的协助。\n谢谢", "Tag": "算法分析"}
{"Answer": "请采纳\nimport datetime\nimport time\n\n# time\nprint(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\nprint(time.strftime(\"%Y-%m-%d\", time.localtime()))\nprint(time.strftime(\"%H:%M:%S\", time.localtime()))\n# datetime\nprint(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n", "Konwledge_Point": "应对NP完全问题", "Question": "datetime.datetime'>怎么去掉时分秒\n\nimport numpy as np\nimport pandas as pd\nimport datetime\nfrom dateutil\n.parser\n import parse\n\ndata1 = pd\n.read_csv\n(\n'result.txt'\n,sep=\n'\\t'\n)\ndata1 = pd\n.DataFrame\n(data1)\ndata1 = np\n.array\n(data1)\n\n\nprint\n(data1[\n0\n][\n3\n])\n\ndata1\n[0]\n[3]\n = parse(data1\n[0]\n[3]\n)\n\nprint\n(type(data1[\n0\n][\n3\n])\n)\n", "Tag": "算法分析"}
{"Answer": "CV_EXPORTS_W void inRange(InputArray src, InputArray lowerb,\r\n                          InputArray upperb, OutputArray dst);\r\nCV_THRESH_BINARY_INV\r\n你查下python中你对应opencv版本定义的代码，看这个常量参数，有没有，就可以反向把区间外的东西置白", "Konwledge_Point": "应对NP完全问题", "Question": "python-opencv2的cv2.inRange把特定颜色提取到白色背景\n    lower=np.array(lower,dtype=\"uint8\")\n    upper=np.array(upper,dtype=\"uint8\")\n    hsv=cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n    cv2.imshow(\"hsv\",hsv)\n    mask=cv2.inRange(hsv,lower,upper)\n\n\n\n\n这里mask得到了在lower和upper之间的颜色，但是还是在原图里面的，如何把它提取到一张白色的背景里面呢，或者怎么把原图其他部分变成白色？", "Tag": "算法分析"}
{"Answer": "1.image要求是CV_8UC1或者CV_8UC3，你这个numpy操作之后变成浮点数了，canny不支持的。2.改成edges = cv.Canny(image, threshold1=50, threshold2=200, apertureSize=3)", "Konwledge_Point": "应对NP完全问题", "Question": "OpenCv cv.Canny函数总是报错\nimport\n cv2 as cv\n\nimport\n numpy as np\n\n\ndef\n line_detect(image):\n\n    \nedges\n = cv.Canny(image, \n50\n, \n200\n, apertureSize=\n3\n)\n    \nprint\n(\n\"edges\"\n, edges)\n    \nlines\n = cv.HoughLines(edges, \n1\n, np.pi/\n180\n, \n150\n)\n    \nfor\n line in lines:\n        \nrho\n, theta = line[\n0\n]\n        \na\n = np.cos(theta)\n        \nb\n = np.sin(theta)\n        \nx0\n = a * rho\n        \ny0\n = b * rho\n        \nx1\n = int(x0 + \n1000\n * (-b))\n        \ny1\n = int(y0 + \n1000\n * (a))\n        \nx2\n = int(x0 - \n1000\n * (-b))\n        \ny2\n = int(y0 - \n1000\n * (a))\n        \ncv\n.line(image, (x1, y1), (x2, y2), (\n0\n, \n0\n, \n255\n), \n2\n)\n    \ncv\n.imshow(\n\"image_lines\"\n, image)\n\n\nprint\n(\n\"Hello\"\n)\n\nsrc\n = cv.imread(\n\"F:/2.png\"\n)\n\n\nsrc1\n = np.array(src)\n\n\nsrc1\n[..., \n0\n] = src1[..., \n0\n] * \n0\n.\n299\n\n\nsrc1\n[..., \n1\n] = src1[..., \n1\n] * \n0\n.\n587\n\n\nsrc1\n[..., \n2\n] = src1[..., \n2\n] * \n0\n.\n114\n\n\n\nsrc1\n = np.sum(src1, axis=-\n1\n)\n\n\nline_detect\n(src1)\n\n\ncv\n.waitKey(\n0\n)\n\n\n\nHello\nTraceback (most recent call last):\n  File \"E:/PycharmProjects/learnPython/venv/test.py\", line 33, in \n    line_detect(src1)\n  File \"E:/PycharmProjects/learnPython/venv/test.py\", line 6, in line_detect\n    edges = cv.Canny(image, 50, 200, apertureSize=3)\ncv2.error: OpenCV(4.5.2) :-1: error: (-5:Bad argument) in function 'Canny'\n\n\n\n\nOverload resolution failed:\n\n\nimage data type = 8 is not supported\nExpected Ptrcv::UMat for argument 'image'\nRequired argument 'threshold2' (pos 4) not found\nRequired argument 'threshold2' (pos 4) not found\n\n\n\n\n![img](", "Tag": "算法分析"}
{"Answer": "File \"d:\\OneDrive - 东南大学\\python practice\\cs2.py\", line 37, ina=float(conversion_degree_total.pop(0))这个代码在哪里，咋没看见你发的有", "Konwledge_Point": "应对NP完全问题", "Question": "关于数据处理的问题，如何解决？\n程序一直报错\n\n\n\n\n\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n math\n\nimport\n pandas \nas\n pd\n\nimport\n numpy \nas\n np\n\nimport\n csv\n\nfrom\n scipy.optimize \nimport\n curve_fit\n\nplt.close(\n\"all\"\n)\n\n\ndef\n \nf_1\n(\nx, k, b\n):\n    \nreturn\n k * x + b\n\nRa=\n8.314\n\n\n#源文件地址和目标文件夹\n\ndata_address=\nr'C:\\Users\\keyang\\Desktop\\test.csv'\n\nsave_floder=\nr'C:\\Users\\keyang\\Desktop'\n\n\n#生成文件的头部行\n\nhead_name = [\n'T_start'\n,\n'T_end'\n,\n'Conversion_degree'\n,\n'Activation_energy'\n]\n\n#写入CSV文件\n\n\nwith\n \nopen\n(save_floder + \n'\\\\'\n + \n'result.csv'\n,\n'w'\n, newline=\n''\n,encoding=\n'utf-8-sig'\n) \nas\n list_writer:\n    writer = csv.writer(list_writer)\n    writer.writerow(head_name)\n\n#指定转化率间隔\n\nconversion_value = \n0.05\n\n\n#读取源文件\n\ndf = pd.read_csv(data_address)\nconversion_degree_total = df[\n'Degree'\n]\nTemperature_total = df[\n'Temp'\n]\n\n#循环次数\n\nnum = \nint\n(\n0.9\n/conversion_value)\n\nfor\n i \nin\n \nrange\n(num):\n    x_range=[]\n    y_range=[]\n    Temperature_range=[]\n    Top=conversion_degree_total[\n0\n]\n    \nwhile\n conversion_degree_total \nis\n \nnot\n \nNone\n:\n        a=conversion_degree_total.pop(\n0\n)\n        t=Temperature_total.pop(\n0\n)\n        Temperature_range.append(t)\n        x_num=\n1\n/t\n        x_range.append(x_num)\n        y_num=math.log((-math.log(\n1\n-a))/(t**(\n2\n)))\n        y_range.append(y_num)\n        \nif\n \nfloat\n(conversion_degree_total.values[\n0\n]-Top)>conversion_value:\n            \nbreak\n\n    \n#x_array,y_array是我们要拟合的数据\n\n    x_array = np.array(x_range)\n    y_array = np.array(y_range)\n    k, b = curve_fit(f_1, x_array, y_array)[\n0\n]\n    Ea=-Ra*k\n    \nwith\n \nopen\n(save_floder +\n'\\\\'\n+ \n'result.csv'\n,\n'a'\n, newline=\n''\n,encoding=\n'utf-8-sig'\n) \nas\n list_writer:\n         writer = csv.writer(list_writer)\n         writer.writerow(Temperature_range[\n0\n],Temperature_range[-\n1\n],conversion_value*(i+\n1\n),Ea)\n\n\n\n运行结果及报错内容\n\n\nTraceback (most recent call last):\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3361, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas\n_libs\\index.pyx\", line 76, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas\n_libs\\index.pyx\", line 108, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas\n_libs\\hashtable_class_helper.pxi\", line 2131, in pandas._libs.hashtable.Int64HashTable.get_item\n  File \"pandas\n_libs\\hashtable_class_helper.pxi\", line 2140, in pandas._libs.hashtable.Int64HashTable.get_item\nKeyError: 0\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n  File \"d:\\OneDrive - 东南大学\\python practice\\cs2.py\", line 37, in \n    a=float(conversion_degree_total.pop(0))\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\", line 4850, in pop\n    return super().pop(item=item)\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 870, in pop\n    result = self[item]\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\", line 942, in \ngetitem\n    return self._get_value(key)\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\", line 1051, in _get_value\n    loc = self.index.get_loc(label)\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3363, in get_loc\n    raise KeyError(key) from err\nKeyError: 0\n\n\n我的解答思路和尝试过的方法\n\n\n无\n\n\nexcel表格里面有一列数据，想要每间隔固定的差值的一部分数据进行计算，然后将计算的结果写入一个新的csv文件\n\n\n数据实例\n\n\n\n", "Tag": "算法分析"}
{"Answer": "解决了。。。。。。。", "Konwledge_Point": "应对NP完全问题", "Question": "bp神经网络怎么实际应用啊？Python\n原始数据输入，但是输出节点使用激活函数之后结果都是小于一的数，求出来的误差很大，而且误差会先减小后变大，感觉错误好多但不知道在哪 \n\n\n\n\n import numpy as np\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef main():\n    # 14条数据\n    data = np.array([\n        [1, 0, 0, 1, 0, 1, 1, 1],\n        [0, 0, 1, 1, 0, 0, 1, 0],\n        [1, 1, 0, 1, 1, 1, 0, 1],\n        [0, 1, 0, 1, 0, 0, 1, 1],\n        [1, 0, 1, 1, 0, 1, 1, 1],\n        [1, 1, 0, 0, 1, 1, 1, 0],\n        [0, 0, 0, 1, 0, 0, 1, 1],\n        [1, 0, 1, 1, 0, 1, 1, 1],\n        [1, 1, 0, 1, 0, 1, 0, 1],\n        [1, 0, 0, 0, 1, 0, 1, 1],\n        [1, 0, 0, 1, 0, 1, 1, 0],\n        [0, 0, 1, 1, 0, 1, 0, 1],\n        [1, 0, 0, 1, 0, 0, 1, 1],\n        [0, 1, 0, 1, 0, 1, 1, 1]])\n    print(\"原始数据：\\n\", data)\n    # 十四条数据的跳高成绩\n    highJump = np.array(\n        [0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0])\n    print(\"十四条数据的跳高成绩：\\n\", highJump)\n    # 第十五条数据的输入\n    data15 = np.array([0, 1, 0, 1, 1, 0, 1, 0])\n    print(\"第十五条数据的输入：\\n\", data15)\n    # 设置输入层与隐藏层之间的权值和阈值\n    wInput = np.random.random(size=(6, 8))/10\n    print(\"输入层与隐藏层之间的六组权值:\\n\", wInput)\n    bInput = np.random.random(size=(6, 8))/10\n    print(\"输入层与隐藏层之间的六组阈值:\\n\", bInput)\n    # 设置隐藏层与输出层之间的权值和阈值\n    wOutput = np.random.random(size=6)/10\n    print(\"隐藏层与输出层之间的一组权值\", wOutput)\n    bOutput = np.random.random(size=6)/10\n    print(\"隐藏层与输出层之间的一组阈值\", bOutput)\n    loss = 1\n    count = 0\n    while loss > 0.1:\n        count = count + 1\n        loss = 0\n        outputNode = []\n        for i in range(0, 14):\n\n            # 正向传播\n            # 计算隐藏层节点输入\n            hide = []\n            for j in range(0, 6):\n                hideNode = 0\n                for k in range(0, 8):\n                    hideNode = data[i, k] * wInput[j, k] + \\\n                        bInput[j, k] + hideNode\n                # print(hideNode)\n                hideNode = sigmoid(hideNode)  # 激活函数\n                hide.append(hideNode)\n            hide = np.array(hide)\n            # print(\"隐藏层结点\", hide)\n            output = 0\n            for j in range(0, 6):\n                output = hide[j] * wOutput[j] + bOutput[j] + output\n            output = sigmoid(output)\n            outputNode.append(output)\n            # print(\"输出层结点\", output)\n            loss = ((output - highJump[i]) * (output - highJump[i])) / 2 + loss\n        outputNode = np.array(outputNode)\n        # 反向传播\n        # print(\"隐藏层结点\", hide)\n        for i in range(0, 14):\n            # 隐藏层与输出层之间权值阈值更新\n            wOutputLoss = []\n            for j in range(0, 6):\n                wOutputLoss.append((outputNode[i] - highJump[i]) *\n                                   outputNode[i] * (1 - outputNode[i])\n                                   * hide[j])\n            wOutputLoss = np.array(wOutputLoss)\n            # print(\"wOutputLoss\", wOutputLoss)\n            bOutputLoss = []\n            for j in range(0, 6):\n                bOutputLoss.append((outputNode[i] - highJump[i]) *\n                                   outputNode[i] * (1 - outputNode[i]))\n            bOutputLoss = np.array(bOutputLoss)\n            # print(\"bOutputLoss\", bOutputLoss)\n            for j in range(0, 6):\n                wOutput[j] = wOutput[j] - 0.1 * wOutputLoss[j]\n                bOutput[j] = bOutput[j] - 0.1 * bOutputLoss[j]\n            # print(\"隐藏层与输出层更新后权值和阈值\", wOutput, bOutput)\n            # 输入层与隐藏层之间权值更新\n            wInputLoss = np.ones((6, 8)) * 0\n            for j in range(0, 6):\n                for k in range(0, 8):\n                    wInputLoss[j][k] = ((outputNode[i] - highJump[i]) *\n                                        outputNode[i] *\n                                        (1 - outputNode[i]) * wOutput[j]\n                                        * hide[j] * (1 - hide[j]) * data[i][k])\n            wInputLoss = np.array(wInputLoss)\n            # print(\"wIutputLoss\", wInputLoss)\n            bInputLoss = np.ones((6, 8)) * 0\n            for j in range(0, 6):\n                for k in range(0, 8):\n                    bInputLoss[j][k] = ((outputNode[i] - highJump[i]) *\n                                        outputNode[i] * (1 - outputNode[i]) *\n                                        wOutput[j] * hide[j] * (1 - hide[j]))\n            bInputLoss = np.array(bInputLoss)\n            # print(\"bIutputLoss\", bInputLoss)\n            for j in range(0, 6):\n                for k in range(0, 8):\n                    wInput[j][k] = wInput[j][k] - 0.1 * wInputLoss[j][k]\n                    bInput[j][k] = bInput[j][k] - 0.1 * bInputLoss[j][k]\n            # print(\"输入层与隐藏层之间更新后的权值和阈值\", wInput, bInput)\n            # print(\"输出\", output)\n        print(\"学习前的loss\", loss)\n        loss = 0\n        for i in range(0, 14):\n            # 正向传播\n            # 计算隐藏层节点输入\n            hide = []\n            for j in range(0, 6):\n                hideNode = 0\n                for k in range(0, 8):\n                    hideNode = data[i, k] * wInput[j, k] + \\\n                        bInput[j, k] + hideNode\n                hideNode = sigmoid(hideNode)  # 激活函数\n                hide.append(hideNode)\n            hide = np.array(hide)\n            output = 0\n            for j in range(0, 6):\n                output = hide[j] * wOutput[j] + bOutput[j] + output\n            output = sigmoid(output)\n            loss = ((output - highJump[i]) * (output - highJump[i])) / 2 + loss\n            # print(\"输出\", output)\n        print(\"学习后的loss\", loss)\n\n    # 预测\n    hide = []\n    for j in range(0, 6):\n        hideNode = 0\n        for k in range(0, 8):\n            hideNode = data15[k] * wInput[j, k] + \\\n                bInput[j, k] + hideNode\n            hideNode = sigmoid(hideNode)  # 激活函数\n        hide.append(hideNode)\n    hide = np.array(hide)\n    output = 0\n    for j in range(0, 6):\n        output = hide[j] * wOutput[j] + bOutput[j] + output\n    output = sigmoid(output)\n    print(output)\n    print(loss)\n    print(count)\n\n\nif __name__ == '__main__':\n    main()\n\n", "Tag": "算法分析"}
{"Answer": "参考GPT和自己的思路：\n针对你提出的问题，我来逐一回答：\nAttributeError: module 'paddle.fluid.dygraph' has no attribute 'to_varlable'这个错误是因为拼写错误，正确的应该是 to_variable，将代码中 to_varlable 修改为 to_variable 即可。\nNameError: name 'best_test_acc' is not defined这个错误是因为没有定义 best_test_acc 变量，需要在模型训练前先定义这个变量。可以在代码前面加上一行 best_test_acc = 0。\nValueError: Model saved directory './work/24' is not exists.这个错误是因为模型保存路径不正确，你需要检查一下模型保存路径是否正确（即模型训练部分的代码中是否有保存模型的代码，以及你的模型保存目录是否存在）。\n\n希望以上回答能够帮到你解决问题，祝你成功实现手写数字识别！", "Konwledge_Point": "应对NP完全问题", "Question": "深度学习入门实践的典型例题-手写数字识别\n深度学习入门实践-手写数字识别\n代码如下，是在aistudio上运行的，但是不知道怎么回事运行不了\n部分错误如下：\n模型训练部分出现的错误\nAttributeError: module 'paddle.fluid.dygraph' has no attribute 'to_varlable'\n\n\n模型验证部分出现的错误\nNameError: name 'best_test_acc' is not defined\n\n\n预测的问题\nValueError: Model saved directory './work/24' is not exists.\n\n\n#导入需要的包\n\nimport numpy as np\nimport paddle as paddle\n\nfrom\n PIL import Image\nimport matplotlib.pyplot as plt\nimport os\n\nfrom\n paddle.fluid.dygraph import Linear\n\n#训练数据集准备\n\n\nBUF_SIZE\n=512\n\nBATCH_SIZE\n=128\ntrain_reader = paddle.batch(\n    paddle.reader.shuffle(paddle.dataset.mnist.train(),\nbuf_size\n=BUF_SIZE),batch_size=BATCH_SIZE \n)\n\n\n#测试数据集准备\n\n\n#用于训练的数据提供器，每次从缓存中随机读取批次大小的数据\n\ntest_reader = paddle.batch(\n    paddle.reader.shuffle(paddle.dataset.mnist.test(),\nbuf_size\n=BUF_SIZE),batch_size=BATCH_SIZE \n)\n\n#定义多层感知器\n\n\n#动态图定义多层感知器\n\nimport paddle.fluid as fluid #防止出现name fluid is \nnot\n defined\nclass multilayer_perceptron(fluid.dygraph.Layer):\n    def __init__(self):\n        super(multilayer_perceptron,self).__init__()\n        self.fc1 = Linear(\ninput_dim\n=28*28,output_dim=100,act='relu')\n        self.fc2 = Linear(\ninput_dim\n=100,output_dim=100,act='relu')\n        self.fc3 = Linear(\ninput_dim\n=100,output_dim=10,act='softmax')\n    def forward(self,input_):\n        x = fluid.layers.reshape(input_,[input_.shape[0],-1])\n        x = self.fc1(x)\n        x = self.fc2(x)\n        y = self.fc3(x)\n        return y\n\n#模型训练\n\nwith fluid.dygraph.guard():\n    \nmodel\n=multilayer_perceptron() #模型实例化\n    model.train() #训练模式\n    \nopt\n=fluid.optimizer.Adam(learning_rate=fluid.dygraph.ExponentialDecay(\n        \nlearning_rate\n=0.01,\n        \ndecay_steps\n=4000,\n        \ndecay_rate\n=0.1,\n        \nstaircase\n=\nTrue\n\n    ),\nparameter_list\n=model.parameters())\n\n    \nepochs_num\n=30 #选代次数\n    \n    \nfor\n pass_num \nin\n range(epochs_num):\n        lr = opt.current_step_lr() \n        \nprint\n(\n\"learning-rate:\"\n,lr)\n\n        \nfor\n batch_id,date \nin\n enumerate(train_reader()):\n            \nimages\n=np.array([x[0].reshape(1,28,28) \nfor\n x \nin\n data],np.float32) \n\n            labels = np.array([x[1] \nfor\n x \nin\n data]).astype(\n'int64'\n)\n            labels = labels[:,np.newaxis]\n            \n            \nimage\n=fluid.dygraph.to_variable(images) \n            \nlabel\n=fluid.dygraph.to_varlable(labels) \n            \npredict\n=model(image) #预测\n            #\nprint\n(predict)\n            \nloss\n=fluid.layers.cross_entropy(predict,label) \n            \navg_loss\n=fluid.layers.mean(loss)#获取loss\n            \nacc\n=fluid.layers.eccuracy(predict,label)#计算精度 \n            avg_loss.backvard()\n            opt.minimize(avg_loss) \n            model.clear_gredients()\n\n            \nall_train_iter\n=all_train_iter+256 \n            all_train_iters.append(all_train_iter) \n            all_train_costs.append(loss.numpy()[0]) \n            a1l_train_accs.append(acc,numpy()[0]) \n            \n            \n            \nif\n batch_idlse \nand\n batch_id%\n50\n==0:\n                prinf(\n\"train_pass:{},batch_id:{},train_loss:{},train_acc:{}\"\n,format(pass_num,batch_id,avg_loss.numpy(),acc.numpy())) \n\n\n#模型验证\n\nwith fluid.dygraph.guard():\n    accs = []\n    model.eval()#评估模式\n    \nfor\n batch_id,data \nin\n enumerate(test_reader()):#测试集\n        \nimages\n=np.array([x[0].reshape(1,28,28) \nfor\n x \nin\n data],np.float32)\n        labels = np.array([x[1] \nfor\n x \nin\n data]).astype(\n'int64'\n)\n        labels = labels[:,np.newaxis]\n\n        \nimage\n=fluid.dygraph.to_variable(images)\n        \nlabel\n=fluid.dygraph.to_variable(labels)\n\n        \npredict\n=model(image)#预测\n        \nacc\n=fluid.layers.accuracy(predict,label)\n        accs.append(acc.numpy()[0])\n        avg_acc = np.mean(accs)\n\n    \n    \nif\n avg_acc >= best_test_acc:\n        best_test_acc = avg_acc\n        \nif\n pass_num > 10:\n            fluid.save_dygraph(model.state_dict(),\n'./work/{}'\n.format(pass_num))#保存模型\n\n    \nprint\n(\n'Test:%d,Accuracy:%0.5f, Best: %0.5f'\n% (pass_num,avg_acc,best_test_acc))\n\n\n#图片预处理\n\ndef load_image(flie):\n    im = Image.open(flie).convert(\n'L'\n)\n    im = im.resize((28,28),Image.ANTIALIAS)\n    im = np.array(im).reshape(1,1,28,28).astype(np.float32)\n    #\nprint\n(im)\n    im = im/255.0 * 2.0 - 1.0\n    return im\n\n\n#开始预测\n\n\n#构建预测动态图过程\n\nmodel_path = \n'./work/24'\n\nwith fluid.dygraph.guard():\n    \nmodel\n=multilayer_perceptron()#模型实例化\n    model_dict,\n_\n=fluid.load_dygraph(model_path)\n    model.load_dict(model_dict)\n    model.eval()\n    infer_img = load_image(infer_path)\n    \ninfer_img\n=np.array(infer_img).astype('float32')\n    \ninfer_img\n=infer_img[np.newaxis,:,:,:]\n    infer_img = fluid.dygraph.to_variable(infer_img)\n    \nresult\n=model(infer_img)\n\n    \nprint\n(\n\"infer results: %s\"\n% label_list[np.argmax(result.numpy())])\n\n", "Tag": "算法分析"}
{"Answer": "这里的错误提示是说第107行self.conv_1(x)的输入x的维度不对，输入x需要是1 channels而不是64 channels。x的维度应该是[64, 1, 28, 28]而不是[64, 28, 28]，这里64是batch_size，28是图片大小，还需要一个channel的数目（和nn.Conv2d第一个参数对应）。你写的数据预处理的部分需要改一下。或者最简单的办法是在107行前面加一句对x的预处理：\n    def forward(self, x):\n        x = torch.unsqueeze(x,1).type(torch.FloatTensor) #增加一个channel的维度同时把tensor的数据类型改为float\n        #print(x.size(), x.type())\n        x = self.conv_1(x)\n        x = self.conv_2(x)\n        x = x.view(x.size(0), -1) #将结果转化为向量\n        output = self.out(x)\n        return output\n", "Konwledge_Point": "应对NP完全问题", "Question": "关于#深度学习#的问题，如何解决？\n问题遇到的现象和发生背景\n\n\n深度学习，模型训练遇到的问题\n\n\n\"\"\"\n卷积神经网络对MUIST数据集分类\n\"\"\"\n\n\n\nimport\n torch\n\nfrom\n torch \nimport\n nn\n\nimport\n numpy \nas\n np\n\nimport\n torch.optim \nas\n optim\n\nfrom\n matplotlib \nimport\n pyplot\n\nimport\n torch.nn.functional \nas\n F\n\nfrom\n sklearn.model_selection \nimport\n train_test_split\n\nfrom\n torch.utils.data \nimport\n TensorDataset,DataLoader\n\n\ndef\n \nread_image\n(\nfile_path\n):\n    \n\"\"\"读取MNIST图片\n\n    Args:\n        file_path (str): 图片文件位置\n\n    Returns:\n        list: 图片列表\n    \"\"\"\n\n    \nwith\n \nopen\n(file_path,\n'rb'\n) \nas\n f:\n        file = f.read()\n        img_num = \nint\n.from_bytes(file[\n4\n:\n8\n],byteorder=\n'big'\n) \n#图片数量\n\n        img_h = \nint\n.from_bytes(file[\n8\n:\n12\n],byteorder=\n'big'\n) \n#图片h\n\n        img_w = \nint\n.from_bytes(file[\n12\n:\n16\n],byteorder=\n'big'\n) \n#图片w\n\n        img_data = []\n        file = file[\n16\n:]\n        data_len = img_h*img_w\n\n        \nfor\n i \nin\n \nrange\n(img_num):\n            data = [item/\n255\n \nfor\n item \nin\n file[i*data_len:(i+\n1\n)*data_len]]\n            img_data.append(np.array(data).reshape(img_h,img_w))\n\n        \nreturn\n img_data\n\n\ndef\n \nread_label\n(\nfile_path\n):\n    \nwith\n \nopen\n(file_path,\n'rb'\n) \nas\n f:\n        file = f.read()\n        label_num = \nint\n.from_bytes(file[\n4\n:\n8\n],byteorder=\n'big'\n) \n#label的数量\n\n        file = file[\n8\n:]\n        label_data = []\n        \nfor\n i \nin\n \nrange\n(label_num):\n            label_data.append(file[i])\n        \nreturn\n label_data\n\n\ntrain = read_image(\n\"data/mnist/train-images-idx3-ubyte/train-images.idx3-ubyte\"\n)\nlabel = read_label(\n\"data/mnist/train-labels-idx1-ubyte/train-labels.idx1-ubyte\"\n)\n\n# pyplot.imshow(train[0].reshape((28,28)),cmap=\"gray\")\n\n\n# pyplot.show()\n\n\nprint\n(train[\n1\n].shape)\n\n\n#定义超参数\n\ninput_size = \n28\n \n#图像总尺寸28*28\n\nnum_classes = \n10\n \n#标签的种类\n\nnum_epochs = \n3\n \n#训练的总循环周期\n\nbatch_size = \n64\n \n#一个批次的大小，64张图片\n\n\n\n#划分数据集\n\ntrain_img,valid_img,train_label,valid_label = train_test_split(train,label,test_size=\n0.2\n,shuffle=\nTrue\n)\n\n# train_img,valid_img,train_label,valid_label = map(torch.tensor,(train_img,valid_img,train_label,valid_label))\n\n\n# print(type(train_img),type(train_label))\n\n\ntrain_img = np.array(train_img)\ntrain_label = np.array(train_label)\nvalid_img = np.array(valid_img)\nvalid_label = np.array(valid_label)\n\ntrain_img = torch.from_numpy(train_img)\ntrain_label = torch.from_numpy(train_label)\nvalid_img = torch.from_numpy(valid_img)\nvalid_label = torch.from_numpy(valid_label)\n\n\n#构建batch数据\n\ntrain_ds = TensorDataset(train_img,train_label)\ntrain_loader = DataLoader(dataset=train_ds,batch_size=batch_size,shuffle=\nTrue\n)\ntest_dl = TensorDataset(valid_img,valid_label)\ntest_loader = DataLoader(dataset=test_dl,batch_size=batch_size,shuffle=\nTrue\n)\n\n\n\n\nclass\n \nMyNet_CNN\n(nn.Module):\n    \ndef\n \n__init__\n(\nself\n):\n        \nsuper\n(MyNet_CNN, self).__init__()\n        self.conv_1 = nn.Sequential( \n#输入 （1，28，28）\n\n            nn.Conv2d(\n                \n1\n, \n#输入的特征图数（原始图为灰度图一层）\n\n                \n16\n,  \n#得到16个特征图\n\n                kernel_size=\n5\n, \n#卷积核的大小\n\n                stride=\n1\n, \n#步长\n\n                padding=\n2\n \n#图片填充层数\n\n            ), \n#输出（16，28，28）\n\n            nn.ReLU(), \n#relu层（激活函数）\n\n            nn.MaxPool2d(kernel_size=\n2\n), \n#进行池化操作（2*2区域），输出结果为 （16，24，24）\n\n        )\n        self.conv_2 = nn.Sequential( \n#进入下一个层 输入（16，24，24）\n\n            nn.Conv2d(\n16\n, \n32\n, kernel_size=\n5\n, stride=\n1\n,padding=\n2\n), \n#输出（32，14，14）\n\n            nn.ReLU(),\n            nn.MaxPool2d(\n2\n),\n#输出（32，7，7）\n\n        )\n        self.out = nn.Linear(\n32\n*\n7\n*\n7\n,\n10\n) \n#全连接层的得到的结果\n\n\n    \n\"\"\"前向传播\"\"\"\n\n    \ndef\n \nforward\n(\nself, x\n):\n        x = self.conv_1(x)\n        x = self.conv_2(x)\n        x = x.view(x.size(\n0\n), -\n1\n) \n#将结果转化为向量\n\n        output = self.out(x)\n        \nreturn\n output\n\n\n\"\"\"准确率作为评估标准\"\"\"\n\n\ndef\n \naccuracy\n(\npredictions,labels\n):\n    pred = torch.\nmax\n(predictions.data,\n1\n)[\n1\n]\n    rights = pred.eq(labels.data.view_as(pred)).\nsum\n()\n    \nreturn\n rights,\nlen\n(labels)\n\n\n\"\"\"训练网络模型\"\"\"\n\n\n#实例化\n\nnet = MyNet_CNN()\n\n#s损失函数\n\ncriterion = nn.CrossEntropyLoss()\n\n#优化器\n\noptimizer = optim.Adam(net.parameters(),lr=\n0.001\n)\n\n\n#开始训练循环\n\n\nfor\n epoch \nin\n \nrange\n(num_epochs):\n    \n#当前的epoch结果(准确率)保存下来\n\n    train_rights = []\n\n    \nfor\n batch_idx,(data,target) \nin\n \nenumerate\n(test_loader):  \n#针对容器的每一个批次进行循环\n\n        net.train()\n        output =net(data)\n        loss =criterion(output,target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        right = accuracy(output,target)\n        train_rights.append(right)\n\n        \nif\n batch_idx % \n100\n == \n0\n:\n            net.\neval\n()\n            val_rights = []\n\n            \nfor\n (bata,target) \nin\n test_loader:\n                output = net(data)\n                right =accuracy(output,target)\n\n            \n#准确率计算\n\n            train_r = (\nsum\n(tup[\n0\n] \nfor\n tup \nin\n train_rights),\nsum\n(tup[\n1\n] \nfor\n tup \nin\n train_rights))\n            val_r = (\nsum\n(tup[\n0\n] \nfor\n tup \nin\n val_rights),\nsum\n(tup[\n1\n] \nfor\n tup \nin\n val_rights))\n\n            \nprint\n(\n'当前epoch: {} [{}/{} ({:.0f}%)]\\t损失: {:.6f}\\t训练集准确率: {:.2f}%\\t测试集正确率: {:.2f}%'\n.\nformat\n(\n                epoch,batch_idx*batch_size,\nlen\n(train_loader.dataset),\n                \n100.\n * batch_idx / \nlen\n(train_loader),\n                loss.data,\n                \n100.\n * train_r[\n0\n].numpy() / train_r[\n1\n],\n                \n100.\n * val_r[\n0\n].numpy() / val_r[\n1\n]\n            ))\n\n\n\n\n运行结果及报错内容\n\n\nRuntimeError                              Traceback (most recent call last)\nInput In [1], in ()\n   \n 132 \nfor batch_idx,(data,target) in enumerate(test_loader):  \n#针对容器的每一个批次进行循环\n\n   \n 133 \n    net.train()\n-->\n 134 \n    output =net(data)\n   \n 135 \n    loss =criterion(output,target)\n   \n 136 \n    optimizer.zero_grad()\n\nFile D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130, in Module._call_impl(self, *input, **kwargs)\n  \n 1126 \n# If we don't have any hooks, we want to skip the rest of the logic in\n\n  \n 1127 \n# this function, and just call forward.\n\n  \n 1128 \nif not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n  \n 1129 \n        or _global_forward_hooks or _global_forward_pre_hooks):\n->\n 1130 \n    return forward_call(*input, **kwargs)\n  \n 1131 \n# Do not call functions when jit is used\n\n  \n 1132 \nfull_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [1], in MyNet_CNN.forward(self, x)\n   \n 106 \ndef forward(self, x):\n-->\n 107 \n    x = self.conv_1(x)\n   \n 108 \n    x = self.conv_2(x)\n   \n 109 \n    x = x.view(x.size(0), -1) \n#将结果转化为向量\n\n\n\n```c\nFile D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130, in Module._call_impl(self, *input, **kwargs)\n  \n 1126 \n# If we don't have any hooks, we want to skip the rest of the logic in\n\n  \n 1127 \n# this function, and just call forward.\n\n  \n 1128 \nif not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n  \n 1129 \n        or _global_forward_hooks or _global_forward_pre_hooks):\n->\n 1130 \n    return forward_call(*input, **kwargs)\n  \n 1131 \n# Do not call functions when jit is used\n\n  \n 1132 \nfull_backward_hooks, non_full_backward_hooks = [], []\n\nFile D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139, in Sequential.forward(self, input)\n   \n 137 \ndef forward(self, input):\n   \n 138 \n    for module in self:\n-->\n 139 \n        input = module(input)\n   \n 140 \n    return input\n\nFile D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130, in Module._call_impl(self, *input, **kwargs)\n  \n 1126 \n# If we don't have any hooks, we want to skip the rest of the logic in\n\n  \n 1127 \n# this function, and just call forward.\n\n  \n 1128 \nif not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n  \n 1129 \n        or _global_forward_hooks or _global_forward_pre_hooks):\n->\n 1130 \n    return forward_call(*input, **kwargs)\n  \n 1131 \n# Do not call functions when jit is used\n\n  \n 1132 \nfull_backward_hooks, non_full_backward_hooks = [], []\n\nFile D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457, in Conv2d.forward(self, input)\n   \n 456 \ndef forward(self, input: Tensor) -> Tensor:\n-->\n 457 \n    return self._conv_forward(input, self.weight, self.bias)\n\nFile D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453, in Conv2d._conv_forward(self, input, weight, bias)\n   \n 449 \nif self.padding_mode != 'zeros':\n   \n 450 \n    return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n   \n 451 \n                    weight, bias, self.stride,\n   \n 452 \n                    _pair(0), self.dilation, self.groups)\n-->\n 453 \nreturn F.conv2d(input, weight, bias, self.stride,\n   \n 454 \n                self.padding, self.dilation, self.groups)\n\nRuntimeError: Given groups=1, weight of size [16, 1, 5, 5], expected input[1, 64, 28, 28] to have\n 1 \nchannels, but got\n 64 \nchannels instead\n\n\n\n\n\n###### 我的解答思路和尝试过的方法 \n毫无思路，不知道错哪\n```typescript\n\n\n", "Tag": "算法分析"}
{"Answer": "这样吗\n", "Konwledge_Point": "应对NP完全问题", "Question": "程序没有输出结果是什么问题\n#include\nint main()\n{\n    int month,n;\n    float p,x,sum;\n    scanf(\"%d%d%f\",&month,&n,&p);\n    if(month==1||(month>=7&&month<=9))\n    {\n        if(n>=20)\n        {\n            x=p\n0.05\nn;\n            sum=n\np\n0.95;\n            printf(\"%s,%.2f,%.2f\",'5%',x,sum);\n        }\n        else\n            printf(\"%s,%.2f,%.2f\",'0%',0,n*p);\n\n\n}\n\nelse\n \nif\n((month>=\n10\n&&month<=\n12\n)||(month>=\n2\n&&month<=\n6\n))\n{\n    \nif\n(n>=\n20\n)\n    {\n        \nx\n=p*\n0\n.\n2\n*n;\n        sum=n*p*\n0\n.\n8\n;\n        \nprintf\n(\n\"%s,%.2f,%.2f\"\n,\n'20%'\n,\nx\n,sum);\n    }\n    \nelse\n\n        \nprintf\n(\n\"%s,%.2f,%.2f\"\n,\n'0%'\n,\n0\n,n*p);\n}\n\nelse\n\n    \nprintf\n(\n\"month error\"\n);\n\nreturn\n \n0\n;\n\n\n\n}", "Tag": "算法分析"}
{"Answer": "增大p的取值100倍即可，我取p1=p2=2成功了", "Konwledge_Point": "应对NP完全问题", "Question": "（simulink）永磁同步电机（S函数搭建）的转子位置动态面跟踪时改变给定频率>1，转子位置不正确,跟不上给定\n永磁同步电动机永磁同步电机（S函数搭建）的转子位置动态面跟踪时改变给定频率，当角频率>1时，给定为2sin（2t），转子位置不正确,跟不上给定\n\n\n\n我认为是参数问题但不知道如何调。\n\n以下是建模：\n\n\n\n\n\n\n以下是S函数代码：\n\n\n\nfunction [sys,x0,str,ts,simStateCompliance] = controller20 (t,x,u,flag)\n\n\n\nswitch flag,\n\n\n\ncase 0,\n\n    [sys,x0,str,ts,simStateCompliance]=mdlInitializeSizes;\n\n\n\ncase 1,\n\n    sys=mdlDerivatives(t,x,u);\n\n\n\ncase 2,\n\n    sys=mdlUpdate(t,x,u);\n\n\n\ncase 3,\n\n    sys=mdlOutputs(t,x,u);**\n\n\n\ncase 4,\n\n    sys=mdlGetTimeOfNextVarHit(t,x,u);\n\n\n\notherwise\n\n    DAStudio.error('Simulink:blocks:unhandledFlag', num2str(flag));\n\n\n\nend\n\n\n\nfunction [sys,x0,str,ts,simStateCompliance]=mdlInitializeSizes\n\n\n\nsizes = simsizes;\n\n\n\nsizes.NumContStates  = 4;\n\n\n\nsizes.NumDiscStates  = 0;\n\n\n\nsizes.NumOutputs     = 4;\n\n\n\nsizes.NumInputs      = 5;\n\n\n\nsizes.DirFeedthrough = 1;\n\n\n\nsizes.NumSampleTimes = 1;   \n\n\n\nsys = simsizes(sizes);\n\n\n\nx0  = [0,0,0,0];\n\n\n\nstr = [];\n\n\n\nts  = [0 0];\n\n\n\nfunction sys=mdlDerivatives(t,x,u)\n\n%微分函数\n\n%以下系数皆可调\n\nk1=150;%动态面参数（主要调）\n\n\n\nk2=150;  %动态面参数（主要调）\n\n\n\nnp=3;%极对数（不可调）\n\n\n\nB=0.001158;\n\n\n\nJ=0.003798;\n\n\n\nRs=0.68;\n\n\n\ny=0.1245;\n\n\n\ntt2=0.01;\n\n\n\ntt3=0.01;\n\n\n\nr1=2;\n\n\n\nr2=2;\n\n\n\nTL=1.5;\n\n\n\np1=0.02;\n\n\n\np2=0.02;\n\n\n\nLd=0.00285;\n\n\n\nLq=0.00315;\n\n\n\n%以下a-c为系数，不用管\n\n\n\na1=1.5*np*y;\n\n\n\na2=1.5*np*(Ld-Lq);\n\n\n\nb1=-Rs/Lq;\n\n\n\nb2=-np*Ld/Lq;\n\n\n\nb3=-np*y/Lq;\n\n\n\nb4=1/Lq;\n\n\n\nc1=-Rs/Ld;\n\n\n\nc2=np*Lq/Ld;\n\n\n\nc3=1/Ld;\n\n\n\n%以下为状态方程\n\n\n\ndx(1)=(-k1*u(1)+u(2)-x(1))/tt2;\n\n\n\ndx(2)=((-k2-1)*(u(3)-x(1))+x(3)*u(3)+x(4)*(-k1*u(1)+u(2)-x(1))/tt2-a1*x(2))/(a1*tt3);\n\n\n\ndx(3)=-r1*((u(3)-x(1))*u(3)+p1*x(3));\n\n\n\ndx(4)=-r2*((u(3)-x(1))*(-k1*u(1)+u(2)-x(1))/tt2+p2*x(4));\n\n\n\nsys=[dx(1);dx(2);dx(3);dx(4)];\n\n\n\nfunction sys=mdlUpdate(t,x,u)\n\n\n\nsys = [];\n\n\n\nfunction sys=mdlOutputs(t,x,u)\n\n%输出函数\n\n\n\nk1=150;%动态面参数（主要调）\n\n\n\nk2=150; %动态面参数（主要调）\n\n\n\nk3=10;%动态面参数（主要调）\n\n\n\nk4=5;%动态面参数（主要调）\n\n\n\nnp=3;\n\n\n\nB=0.001158;\n\n\n\nJ=0.003798;\n\n\n\nRs=0.68;\n\n\n\ny=0.1245;\n\n\n\ntt2=0.01;\n\n\n\ntt3=0.01;\n\n\n\nr1=2;\n\n\n\nr2=2;\n\n\n\nTL=1.5;\n\n\n\np1=0.02;\n\n\n\np2=0.02;\n\n\n\nLd=0.00285;\n\n\n\nLq=0.00315;\n\n\n\na1=1.5*np*y;\n\n\n\na2=1.5*np*(Ld-Lq);\n\n\n\nb1=-Rs/Lq;\n\n\n\nb2=-np*Ld/Lq;\n\n\n\nb3=-np*y/Lq;\n\n\n\nb4=1/Lq;\n\n\n\nc1=-Rs/Ld;\n\n\n\nc2=np*Lq/Ld;\n\n\n\nc3=1/Ld;\n\n\n\nf1=b1*u(4)+b2*u(3)*u(5)+b3*u(3);\n\n\n\nf2=c1*u(5)+c2*u(3)*u(4)+a2*(u(3)-x(1))*u(4);\n\n\n\na=((-k3-0.5)*(u(4)-x(2))-f1+((-k2-1)*(u(3)-x(1))+x(3)*u(3)+x(4)*(-k1*u(1)+u(2)-x(1))/tt2-a1*x(2))/(a1*tt3))/b4;\n\n\n\nb=((-k4-0.5)*u(5)-f2)/c3;\n\n\n\nsys=[a;b;f1;f2];", "Tag": "算法分析"}
{"Answer": "请看👉 ：曲线拟合", "Konwledge_Point": "应对NP完全问题", "Question": "曲线拟合  为什么拟合效果不好\n为什么拟合效果不好\n\n\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef func(x,a, b, c, d):\n    return ((-1)\na\n(b**2)\nnp.exp((-2)\nb\nx+2\nc))/(1+np.exp((-1)\nb\nx+c))**2\nif \nname\n=='\nmain\n':\n    data=pd.read_excel('C:/Users/admin/XX LDPM-and-GRM-main/1生长曲线.xlsx')\n    xdata=np.array(data['1Time'])\n    ydata =np.array(data['growth'])\n    #画出真实数据\n    plt.plot(xdata,ydata,'b-')\n\n\n#指数函数拟合\npopt, pcov = curve_fit(func, xdata, ydata,bounds=(-\n8\n, \n[-0.5,-0.15, 2,100]\n))#popt数组中，三个值分别是待求参数\na\n,\nb\n,c\n#预测值\ny_pred = \n[func(i, popt[0]\n,popt\n[1]\n,popt\n[2]\n,popt\n[3]\n) \nfor\n \ni\n \nin\n xdata]\n#画图\nplt\n.plot\n(xdata,y_pred,\n'r--'\n)\n\nprint\n(popt)\n\n\n#输出R方\nfrom sklearn\n.metrics\n import r2_score\nr2 = r2_score(ydata , y_pred )\n\nprint\n(\n'指数函数拟合R方为:'\n,r2)\n\n", "Tag": "算法分析"}
{"Answer": "该回答内容部分引用GPT，GPT_Pro更好的解决问题\nimport numpy as np \n#二维数组，用来存储7个区间的范围\nintervals = [[133445, 3344647621]] \n\n# 选出四个不重复的区间\nselected_intervals = np.random.choice(intervals, size=4, replace=False) \n\n# 在这四个区间里随机抽取一个数\nrandom_number = np.random.choice(selected_intervals) \n\nprint(\"随机选取的数字是：\", random_number) \n\n上面的代码实现了从7个区间中随机抽取4个不重复的区间，然后再从这4个区间中随机抽取一个数的功能。具体步骤如下：\n首先，我们使用numpy中的np.random.choice函数，从7个区间中随机抽取4个不重复的区间；然后，我们再使用numpy中的np.random.choice函数，从这4个区间中随机抽取一个数；最后，将结果打印出来。如果回答有帮助，望采纳。", "Konwledge_Point": "应对NP完全问题", "Question": "Python；挑战性；随机数\n先用一个二维数组来存储区间范围[[1\n33145], [33146\n47621],]（一共7个区间）\n然后 np.random.choice(n, size=4, replace=False)选出四个不重复的区间\n最后再这四个区间里np.random.choice直接取一个数\n\n\n这种代码具体怎么写呢，研究一晚上老是报错", "Tag": "算法分析"}
{"Answer": "p[i] = 1这块就应该出问题了，你的p是一个空的list 啊", "Konwledge_Point": "应对NP完全问题", "Question": "python提示list index out of range\n\n\n不知道是哪里有问题，是因为原文件里v这一列有的数值为0吗？该怎么解决？\n代码如下：\n\n\ndata\n = pd.read_excel(\n\"C:/Users/DELL/Desktop/dt1.xls\"\n, \nheader=0,\n \nusecols=['v'])\n\n\ndata\n = np.array(data)\n\nT=[]\n \n\nV=0\n   \n#初始成交量\n\n\np=0\n\n\np=[\n ]\n\nA=326881602\n  \n#每个交易篮子的交易量\n\nfor i \nin\n range(len(data)):\n    \nV=V+data[i]\n   \n#迭代，成交量=上一时刻成交量+这一时刻的成交量\n\n    \nk=np.ceil(V/A)\n  \n#对篮子数向上取整\n\n    T.append(k)\n    T[i]=k    \n#第i时刻所需要的篮子数\n\n    \nif\n np.ceil(V/A)-V/\nA==0:\n  \n#篮子数为整数\n\n        p[i]=\n1\n     \n#如果篮子数为整数，那么下一时刻从新的交易篮子开始填充\n\n        q[\n0\n]=[\n1\n,T[\n0\n]]  \n#第一个时刻对应的篮子编号\n\nfor i \nin\n range(len(data)):\n    \nif\n i>=\n1\n:\n        q[i]=[T[i-\n1\n]+p[i-\n1\n],T[i]]  \n#第i个时刻对应的篮子编号\n\n\n``` 问题相关代码，请勿粘贴截图 \n\n\n###### 运行结果及报错内容 \n\n\n    q[i]=[T[i-\n1\n]+p[i-\n1\n],T[i]]  \n#第i个时刻对应的篮子编号\n\nIndexError: list index out of range\n", "Tag": "算法分析"}
{"Answer": "应该是torch版本的问题，不同torch对应的后缀不同，我正在尝试修改这个问题，推荐查一下torch英文手册\r\n\r\n我之前的问题是torch版本问题过低，我需要将data变成train_data\r\n把targets变成train_labels，就好了，希望对你有帮助~", "Konwledge_Point": "应对NP完全问题", "Question": "运行mixmatch源码CIFAR10数据集时报错AttributeError: 'CIFAR10' object has no attribute 'targets'，是怎么回事？\n\n\n在运行mixmatch程序的时候，用torchvision.datasets载入CIFAT10的时候出现AttributeError: 'CIFAR10' object has no attribute 'targets'，错误 \n\n\n还有一个问题就是：由于用torchvision下载太慢，我先把数据集下下来了，然后放在了data目录下面，这个对结果会有影响嘛？\n\n希望大家可以给点建议和意见，谢谢。\n\n\n\n加载数据集的代码如下：\n\n\n\ndef get_cifar10(root, n_labeled,\n                 transform_train=None, transform_val=None,\n                 download=True):\n\n    base_dataset = torchvision.datasets.CIFAR10(root, train=True, target_transform=True, download=download,)\n    train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(base_dataset.targets, int(n_labeled/10))\n\n    train_labeled_dataset = CIFAR10_labeled(root, train_labeled_idxs, train=True, transform=transform_train)\n    train_unlabeled_dataset = CIFAR10_unlabeled(root, train_unlabeled_idxs, train=True, transform=TransformTwice(transform_train))\n    val_dataset = CIFAR10_labeled(root, val_idxs, train=True, transform=transform_val, download=True)\n    test_dataset = CIFAR10_labeled(root, train=False, transform=transform_val, download=True)\n\n    print (f\"#Labeled: {len(train_labeled_idxs)} #Unlabeled: {len(train_unlabeled_idxs)} #Val: {len(val_idxs)}\")\n    return train_labeled_dataset, train_unlabeled_dataset, val_dataset, test_dataset\n\n\n\n\ndef train_val_split(labels, n_labeled_per_class):\n    labels = np.array(labels)\n    train_labeled_idxs = []\n    train_unlabeled_idxs = []\n    val_idxs = []\n\n    for i in range(10):\n        idxs = np.where(labels == i)[0]\n        np.random.shuffle(idxs)\n        train_labeled_idxs.extend(idxs[:n_labeled_per_class])\n        train_unlabeled_idxs.extend(idxs[n_labeled_per_class:-500])\n        val_idxs.extend(idxs[-500:])\n    np.random.shuffle(train_labeled_idxs)\n    np.random.shuffle(train_unlabeled_idxs)\n    np.random.shuffle(val_idxs)\n\n    return train_labeled_idxs, train_unlabeled_idxs, val_idxs\n\n\n\n\n错误信息如下\n\n(base) D:\\CSStudy\\PycharmProject\\MixMatch-pytorch-master>python train.py --gpu 0 --n-labeled 250 --out cifar10@250\n\n==> Preparing cifar10\n\nUsing downloaded and verified file: ./data\\cifar-10-python.tar.gz\n\nTraceback (most recent call last):\n\n  File \"train.py\", line 431, in \n\n    main()\n\n  File \"train.py\", line 88, in main\n\n    train_labeled_set, train_unlabeled_set, val_set, test_set = dataset.get_cifar10('./data', args.n_labeled, transform_train=transform_train, transf\n\norm_val=transform_val)\n\n  File \"D:\\CSStudy\\PycharmProject\\MixMatch-pytorch-master\\dataset\\cifar10.py\", line 21, in get_cifar10\n\n    train_labeled_idxs, train_unlabeled_idxs, val_idxs = train_val_split(base_dataset.targets, int(n_labeled/10))\n\nAttributeError: 'CIFAR10' object has no attribute 'targets'", "Tag": "算法分析"}
{"Answer": "该回答引用GPTᴼᴾᴱᴺᴬᴵ这段代码的意思是，如果当前文件是被当做主程序运行的，则执行以下代码：\n对于列表[(0, 0), (1, 0), (0, 1), (1, 1)]中的每个元素xs，调用函数AND(xs[0], xs[1])，并将结果赋值给y。\n将xs和y以特定的格式打印出来，其中+号用于连接字符串。\n所以程序的输出是：\n(0, 0) -> 0(1, 0) -> 0(0, 1) -> 0(1, 1) -> 1\n这个程序是实现了与门，它只有在x1和x2都为1时输出1，否则输出0。", "Konwledge_Point": "应对NP完全问题", "Question": "pycharm实现与门中有一串代码没看懂\n在与门的实现过程中\n\n\n# coding: utf-8\n\n\nimport\n numpy \nas\n np\n\n\n\ndef\n \nAND\n(\nx1, x2\n):\n    x = np.array([x1, x2])\n    w = np.array([\n0.5\n, \n0.5\n])\n    b = -\n0.7\n\n    tmp = np.\nsum\n(w*x) + b\n    \nif\n tmp <= \n0\n:\n        \nreturn\n \n0\n\n    \nelse\n:\n        \nreturn\n \n1\n\n\n\nif\n __name__ == \n'__main__'\n:\n    \nfor\n xs \nin\n [(\n0\n, \n0\n), (\n1\n, \n0\n), (\n0\n, \n1\n), (\n1\n, \n1\n)]:\n        y = AND(xs[\n0\n], xs[\n1\n])\n        \nprint\n(\nstr\n(xs) + \n\" -> \"\n + \nstr\n(y))\n\n\n\n\n\n\n\n这一串是什么意思呢，书上没有，我也没太搞懂\n\n\n\n\n\nif\n __name__ == \n'__main__'\n:\n    \nfor\n xs \nin\n [(\n0\n, \n0\n), (\n1\n, \n0\n), (\n0\n, \n1\n), (\n1\n, \n1\n)]:\n        y = AND(xs[\n0\n], xs[\n1\n])\n        \nprint\n(\nstr\n(xs) + \n\" -> \"\n + \nstr\n(y))\n\n\n", "Tag": "算法分析"}
{"Answer": "toad对于模型的改造收不容易的，因为逻辑回归模型才带.coef_,你必须得根据线性模型才能应用于评分卡。你可以结合以下逻辑回归模型与自动机器学习，这样容易改。", "Konwledge_Point": "应对NP完全问题", "Question": "toad评分卡构建与改写问题\n我用toad包构建了一个评分卡模型，我想对他进行改造，我把里面用的逻辑回归模型换成了别的模型，但是我发现不管我运不运行这个a模型，toad都可以直接出一个评分卡结果，难道我这个改写是错误的吗？toad是内置的逻辑回归，我是没法简单改写吗？\n\n\n\n\nimport pandas as pd\n\ntrain\n=pd.read_csv(\"训练集.csv\",index_col=0)\n\ntest\n=pd.read_csv(\"测试集.csv\",index_col=0)\n\n\nXtr\n=train.loc[:,\n\"ficoRangeLow\"\n:\n\"n14\"\n]\n\nYtr\n=train.loc[:,\n\"isDefault\"\n]\n\n\nXts\n=test.loc[:,\n\"ficoRangeLow\"\n:\n\"n14\"\n]\n\nYts\n=test.loc[:,\n\"isDefault\"\n]\n\n\ndata_tr = pd.concat([Xtr,Ytr],\naxis\n=1)\ndata_tr[\n'type'\n] = \n'train'\n\n\ndata_ts = pd.concat([Xts,Yts],\naxis\n=1)\ndata_ts[\n'type'\n] = \n'test'\n\nimport toad\ntoad.detector.detect(data_tr).columns\ntoad.detector.detect(data_tr)\n\nquality = toad.quality(data_tr,\n'isDefault'\n,\niv_only\n=\nTrue\n)\nquality.sort_values(\n'iv'\n,\nascending\n=\nFalse\n)\n\n\nselected_data, dropped = toad.selection.select(data_tr,target = \n'isDefault'\n, empty = 0.5, iv = 0.02, corr = 0.9, \nreturn_drop\n=\nTrue\n,exclude=[\n'type'\n])\n\nprint\n(dropped)\n\nprint\n(selected_data.shape)\n\n\n\nquality = toad.quality(selected_data,\n'isDefault'\n,\niv_only\n=\nTrue\n)\nquality.sort_values(\n'iv'\n,\nascending\n=\nFalse\n)\n\n\n\n\nivzhi\n=quality.sort_values('iv',ascending=False)\nivzhi.to_csv(\n\"iv值排序.csv\"\n)\n\n\ncombiner = toad.transform.Combiner()\ncombiner.fit(selected_data, y = \n'isDefault'\n, method = \n'chi'\n, min_samples = 0.05) #empty_separate = \nFalse\n\n\n\n\nfrom\n toad.plot import bin_plot\n\nfor\n i \nin\n range(0,14,1):\n    col = selected_data.columns[i]\n    bin_plot(combiner.transform(selected_data[[col,\n'isDefault'\n]], \nlabels\n=\nTrue\n), \nx\n=col, \ntarget\n=\n'isDefault'\n)\n\n\nbins = combiner.\nexport\n()\nselected_test = data_ts[selected_data.columns]\ncombiner.set_rules(bins)\nbinned_data = combiner.transform(selected_data)\ntranser = toad.transform.WOETransformer()\ndata_tr_woe = transer.fit_transform(binned_data, binned_data[\n'isDefault'\n], exclude=[\n'isDefault'\n,\n'type'\n])\ndata_ts_woe = transer.transform(combiner.transform(selected_test))\nXtr_woe = data_tr_woe.drop([\n'isDefault'\n,\n'type'\n],\naxis\n=1)\nYtr_woe = data_tr_woe[\n'isDefault'\n]\nXts_woe = data_ts_woe.drop([\n'isDefault'\n,\n'type'\n],\naxis\n=1)\nYts_woe = data_ts_woe[\n'isDefault'\n]\n\nimport autogluon\n\nfrom\n autogluon.tabular import TabularDataset,TabularPredictor  \nimport pandas as pd\nimport numpy as np\n\nlabel\n=\n'isDefault'\n\n\ntrain_data\n=TabularDataset(data_tr_woe.drop([\n\"type\"\n],\naxis\n=1)  )\nmetric = \n'roc_auc'\n\n\ntime_limit\n=60\n\npredictor\n=TabularPredictor(label=label,eval_metric=metric).fit(train_data,presets='best_quality',time_limit=time_limit,auto_stack=True)\n\n\ntest_data\n=TabularDataset(data_ts_woe.drop([\n'isDefault'\n,\n'type'\n],\naxis\n=1))\n\npredictor.predict_proba(train_data)\n\ntrain_proba\n=predictor.predict_proba(train_data)\n\ntrain_proba\n=train_proba.values[:,1]\ntrain_proba = np.array(train_proba).flatten() \n\npredictor.predict_proba(test_data)\n\nauto_proba\n=predictor.predict_proba(test_data)\n\nauto_proba\n=auto_proba.values[:,1]\nauto_proba = np.array(auto_proba).flatten() \n\npsi = toad.metrics.PSI(data_tr_woe,data_ts_woe)\npsi.sort_values(0,\nascending\n=\nFalse\n)\n\n\npsizhi\n=psi.sort_values(0,ascending=False)\npsizhi.to_csv(\n\"psi值.csv\"\n)\n\n\ntr_bucket = toad.metrics.KS_bucket(train_proba,Ytr,\nbucket\n=10,method='quantile')\ntr_bucket\n\n\nselected_data\n\nx_card\n=selected_data.loc[:,\n\"ficoRangeLow\"\n:\n\"n14\"\n]\n\ny_card\n=selected_data.loc[:,\n\"isDefault\"\n]\n\ncard = ScoreCard(\ncombiner\n=combiner, \n                 \ntranser\n=transer, \n                 \nC\n=0.1, \n                 \nclass_weight\n=\n'balanced'\n, \n                 \nbase_score\n=600,\n                 \nbase_odds\n=1,\n                 \npdo\n=50,\n                 \nrate\n=2)\n \n\n# 使用评分卡模型进行拟合\n\ncard.fit(x_card,y_card)\n\n\n\n\n\n最后评分卡生成的这一步和我上面调试的模型没有任何关系，请问该如何改写才能让评分卡的构建基于我的模型。", "Tag": "算法分析"}
{"Answer": "DnCNN类在models模块中,需要这样导入from models import DnCNN，另外项目里的所有文件要按照原来的目录结构排列，然后进行操作。参考：https://github.com/SaoYan/DnCNN-PyTorch", "Konwledge_Point": "应对NP完全问题", "Question": "1,我想用CPU来处理DnCNN的去噪问题，请问有什么好的方法吗？ 2，为什么显示\n问题遇到的现象和发生背景\n\n\n1,我想用CPU来处理DnCNN的去噪问题，请问有什么好的方法吗？ 2，为什么显示没有找到DnCNN model呢？如果想要添加DnCNNmodel应该在哪里添加呢？\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport os\nimport numpy as np\nimport argparse\nimport tensorflow as tf\nimport DnCNN\n\n\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n\nparser = argparse.ArgumentParser(decription=\"Tensorflow DnCNN Testing\")\nparser.add_argument(\"--epochs\", default=50, type=int, help=\"Testing epochs\")\nparser.add_argument(\"--patch-size\", default=40, type=int, help=\"patch size\")\nparser.add_argument(\"--c-dim\", default=1, type=int, help=\"#of channels\")\nparser.add_argument(\"--batch-size\", default=64, type=int, help=\"mini-batch size\")\nparser.add_argument(\"--lr\", default=1e-3, type=float, help=\"Learning rate\")\nparser.add_argument(\"--lr-decay\", default=30, type=int, help=\"Step of learning rate decay\")\nparser.add_argument(\"--weight-decay\", default=1e-4, type=float, help=\"weight decay\")\nparser.add_argument(\"--sigma\", default=25, type=int, help=\"noise level(default 25)\")\nparser.add_argument(\"--Testing-path\", default='./desktop/Training_BrainImages_256x256_100.tfrecords', type=str,\n                    help=\"path to Testingset\")\nparser.add_argument(\"--validate-dir\", default='C:/Users/13145', type=str, help=\"path to Testingset\")\n\n\nopt = parser.parse_args()\nopt.checkpoint_path = './' + opt.modale_name + '_checkpoints'\n\n\ndef main():\n    model = DnCNN()\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as sess:\n        model.Testing(sess, opt)\n\n\nif \nname\n := '\nmain\n':\n    var = tf.app.run\n\n\n运行结果及报错内容\n\n\n2022-05-16 08:03:48.333941: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n2022-05-16 08:03:48.334140: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nTraceback (most recent call last):\n  File \"C:\\Users\\13145\\Desktop\\Test\\full noise.py\", line 29, in \n    import DnCNN\nModuleNotFoundError: No module named 'DnCNN'\n\n\nProcess finished with exit code 1\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果\n\n\n用CPU来基于DnCNN的方法处理给mat文件去噪的问题", "Tag": "算法分析"}
{"Answer": "python中base64串的长度为4的整数倍，因此长度不为4整数倍的base64串需要用\"='补全代码稍微修改了下，你再试试\nimport obsws_python as obs\nimport numpy as np\nimport base64\nimport cv2\nimport sys\n\ncl = obs.ReqClient(host='localhost', port=4444, password=\"931122\")\n\nimage_file_path = sys.path[0] + \"\\Screenshot.png\"\nsource = \"Video Capture Device\"\nScreenshot = cl.get_source_screenshot(source,\"jpg\",None,None,100)\nScreenshot = Screenshot.image_data\n\n#长度不为4整数倍的base64串需要用\"='补全\nnum = len(Screenshot)%4\nif num != 0:\n   Screenshot=Screenshot + '=' * (4-num)\n\nimg = base64.b64decode(Screenshot)\nimg_array = np.fromstring(img, np.uint8) # 转换np序列\nprint('numpy: ', img_array.shape)\ncv2.imshow(\"img\", img_array)\ncv2.waitKey(0)\n", "Konwledge_Point": "应对NP完全问题", "Question": "python中如何将从obs-websocket获得的截图转化为opencv可以操作的图片。\n问题遇到的现象和发生背景\n\n\n在python中通过obs websocket获取截图后想要将其转化为opencv可以操作的格式，但是试了很多办法都失败了\nOBS28.0.1更新后自带的obs-websocket 5\n\n\n用代码块功能插入代码，请勿粘贴截图\n\n\nimport obsws_python as obs\nimport numpy as np\nimport base64\nimport cv2\nimport sys\n\n\ncl = obs.ReqClient(host='localhost', port=4444, password=\"111111\") #连接到websocket服务器\n\n\nimage_file_path = sys.path[0] + \"\\Screenshot.png\"\nsource = \"Video Capture Device\" #这里填添加到OBS中的源的名字，显示在OBS主界面来源中的名字\nScreenshot = cl.get_source_screenshot(source,\"jpg\",None,None,100) #通过websocket从obs请求对源进行截图\nScreenshot = Screenshot.image_data\n\n\nimg = base64.b64decode(Screenshot)\nimg_array = np.fromstring(img, np.uint8)  # 转换np序列\nprint('numpy: ', img_array.shape)\ncv2.imshow(\"img\", img_array)\ncv2.waitKey(0)\n\n\n运行结果及报错内容\n\n\n在b64decode处报错：Incorrect padding\n\n\n我的解答思路和尝试过的方法\n\n\nwebsocket官方说明书里面说这个image_data是“base64 encoded”，于是找了很多种base64转np array的方法，都以失败告终\n修改get_source_screenshot的图片格式，会出现不同的报错\n\n\n我想要达到的结果\n\n\n在python中通过obs websocket获取截图后想要将其转化为opencv可以操作的格式\n但是不希望用save_source_screenshot保存图片后读取的方式，因为我可能会要求程序循环获取截图，这样会严重降低硬盘寿命。\n\n\nOBS和obs-wesocket的安装和使用\n\n\n可能需要大家安装一下OBS进行调试，如果能直接看到cl.get_source_screenshot获取的数据应该能更快找到解决方法吧\nOBS下载：\nhttps://obsproject.com/\nobs-websocket 5 已经内置在该版本中，说明书：\nhttps://github.com/obsproject/obs-websocket/blob/master/docs/generated/protocol.md#getsourcescreenshot\nOBS安装好后：\n在OBS主界面上方 工具 - obs-websocket设置 - 开启WebSocket服务器，开启鉴权，设置服务器端口（int）和服务器密码(string)并将端口和密码填写到cl = obs.ReqClient(host='localhost', port=端口号, password=\"密码\")\n随便加个源，把自己设置的源名称填写到source中。", "Tag": "算法分析"}
{"Answer": "基于Monster 组和GPT的调写：\n\nimport numpy as np\nimport pickle\n\n#回归模型\ndef f(x, theta):\n    return np.dot(theta, x.T)\n\n#评估函数（求均方误差）\ndef mse(x, y, theta):\n    return np.sum((f(x, theta) - y)**2) / len(y)\n\n#训练模型\nlearning_rate = 0.00001\n\ndf['RM2'] = df['RM']**2\ndf['RM_LSTAT'] = df['RM'] * df['LSTAT']\ndf['RM_LSTAT_PTRATIO'] = df['RM'] * df['LSTAT'] * df['PTRATIO']\ntrain_x = df[['RM', 'LSTAT', 'PTRATIO', 'RM2', 'RM_LSTAT', 'RM_LSTAT_PTRATIO']]\ntrain_x = (train_x - train_x.mean()) / train_x.std()  #标准化\ntrain_x = np.insert(train_x.to_numpy(), 0, 1, axis=1)\ntrain_y = df['MEDV']\n\ntheta = np.random.rand(7)\n\nfor i in range(10000):\n    theta = theta - learning_rate * np.dot(f(train_x, theta) - train_y, train_x)\n    metrics = mse(train_x, train_y, theta)\n    print('第%d轮 均方误差为%.04f theta%s' % (i+1, metrics, str(theta)))\n\n#保存模型\nwith open('model.pickle', 'wb') as f:\n    pickle.dump(theta, f)\n\n#加载模型\nwith open('model.pickle', 'rb') as f:\n    theta = pickle.load(f)\n\n\n训练好的模型被保存到了名为'model.pickle'的文件中，并通过pickle库进行了序列化。在加载模型时，只需调用pickle库的load()函数即可将模型反序列化为一个Numpy数组。", "Konwledge_Point": "应对NP完全问题", "Question": "如何保存训练好的模型\n请问如何保存训练好的模型？\n模型代码如下\n\n\n#RM,LSTAT-->MEDV\n\n\n#y=theta_0+theta_1*RM+theta_2*LSTAT+theta_3*PTRATIO+theta_4*RM^2+theta_5*RM*LSTAT+theta_6*RM*LSTAT*PTRATIO\n\n\nimport\n numpy \nas\n np\n\ntheta=np.random.rand(\n7\n,)\ntheta\n\n\n#回归模型\n\n\ndef\n \nf\n(\nx\n):\n    \nreturn\n np.dot(theta,x.T)\n\n#dot():返回两个数组的点积，x.T为矩阵转置\n\n\n\n#评估函数（求均方误差）\n\n\ndef\n \nmse\n(\nx,y\n):\n    \nreturn\n np.\nsum\n((f(x)-y)**\n2\n) / \nlen\n(y)\n\n\n\n\n#训练模型\nlearning_rate = \n0.00001\n\n\ndf[\n'RM2'\n] = df[\n'RM'\n]**\n2\n\ndf[\n'RM_LSTAT'\n] = df[\n'RM'\n]*df[\n'LSTAT'\n]\ndf[\n'RM_LSTAT_PTRATIO'\n] = df[\n'RM'\n]*df[\n'LSTAT'\n]*df[\n'PTRATIO'\n]\ntrain_x = df[[\n'RM'\n,\n'LSTAT'\n,\n'PTRATIO'\n,\n'RM2'\n,\n'RM_LSTAT'\n,\n'RM_LSTAT_PTRATIO'\n]]\nprint(train_x)\ntrain_x = (train_x -train_x.mean()) / train_x.std()#标准化， mean()求平均值\ntrain_x = np.insert(train_x.to_numpy(),\n0\n,\n1\n,axis=\n1\n)\ntrain_y = df[\n'MEDV'\n]\n\nfor i in range(\n10000\n):\n    theta = theta - learning_rate * np.dot(f(train_x)-train_y,train_x)\n    metrics = mse(train_x,train_y)\n    print(\n'第%d轮 均方误差为%.04f theta%s'\n \n% (i+1,metrics,str(theta)))\n\n \n\n", "Tag": "算法分析"}
{"Answer": "isExists = os.path.exists(curDir+\"\\\\xls\")\nif not isExists :\n            os.makedirs(curDir+\"\\\\xls\")\n\n这段代码（上述代码在第9、10行）是你每次都会多一次的原因所在，由于是遍历所有子目录，所以肯定不是每个文件夹都有xls这个子目录的，这样就导致了你每次都会新建一个xls文件夹，导致每次多输出一次。", "Konwledge_Point": "应对NP完全问题", "Question": "关于Python中os.walk(path)循环的问题\n使用Python对文件夹中的  .sig 后缀文件进行操作，操作后转存到路径下的xls文件夹下，如果文件夹中没有.sig 后缀文件， 会输出  “所给路径不存在.sig文件”  ，但是每执行一次就会多输出一次，关终端、重启软件都无法制止它的增长。不懂其中奥妙，求解。。\n\n\n\n\n\n\npath = \n\"E:\\\\数据\\\\scr2 - 副本\"\n\nhave_sig = \n0\n\nisExists = os.path.\nexists\n(path)\n\nif\n not isExists:\n    \nprint\n(\n\"提供的存储.sig文件路径不存在\"\n)\n\nelse\n:\n    \nfor\n curDir, dirs, \nfiles\n in os.walk(path):\n        isExists = os.path.\nexists\n(curDir+\n\"\\\\xls\"\n)\n        \nif\n not isExists :\n            os.makedirs(curDir+\n\"\\\\xls\"\n)\n        \nfor\n \nfile\n in \nfiles\n:\n            \nif\n \nfile\n.endswith(\n\".sig\"\n):\n                have_sig = have_sig+\n1\n\n                # \nprint\n(curDir+\n\"\\\\\"\n+\nfile\n)\n                # \nprint\n(curDir+\n\"\\\\\"\n+\nfile\n[:-\n3\n])\n                os.\nrename\n(curDir+\n\"\\\\\"+file,curDir+\"\n\\\\\n\"+file[:-3]+\"\ntxt\n\")\n\n                pass\n                \nfo\n = \nopen\n(curDir+\n\"\\\\\"+file[:-3]+\"\ntxt\n\", \"\nr\n\")\n\n                date = \nfo\n.\nread\n()\n                date = date.\nsplit\n(\n\"\\n\"\n)\n                datas = np.array(date[\n30\n:]) \n                code = np.\nempty\n(\n0\n)\n                \nfor\n i in datas:\n                    i = i.\nsplit\n()\n                    \nfor\n \nj\n in i:\n                        code = np.\nappend\n(code,float(\nj\n))\n                code = code.reshape(-\n1\n,\n4\n)\n                # \nprint\n(code.shape())#\n1023\n,\n4\n\n                \nfo\n.\nclose\n()\n\n                workbook = xlwt.Workbook(encoding = \n'ascii'\n)\n                worksheet = workbook.add_sheet(\n'sheet'\n)\n                worksheet = workbook.get_sheet(\n'sheet'\n)\n                \nfor\n row in \nrange\n(\n0\n,\nlen\n(code),\n1\n):\n                    \nfor\n column in \nrange\n(\n0\n,\n4\n,\n1\n):\n                        worksheet.\nwrite\n(row, column, code[row][column])\n                workbook.save(curDir+\n\"\\\\xls\\\\\"+file[:-3]+\"\nxls\n\")\n\n        \nif\n have_sig == \n0\n:\n            \nprint\n(\n\"所给路径不存在.sig文件\"\n)\n        \nelse\n:\n            \nprint\n(\n\"处理文件数量：\"\n,have_sig)\n \n\n\n\n\n运行结果及报错内容\n\n\n（18次）\nE:\\9-python\\opencv>python -u \"c:\\Users\\star\\Desktop\\Untitled-1.py\"\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n（19次）\nE:\\9-python\\opencv>python -u \"c:\\Users\\star\\Desktop\\Untitled-1.py\"\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n所给路径不存在.sig文件\n\n\n我的解答思路和尝试过的方法\n\n\n尝试输出print(os.walk(path))\n但是结果都一样：\n\n\n\n我想要达到的结果\n\n\n应该是自己的知识盲区，望指导", "Tag": "算法分析"}
{"Answer": "我试了一下，如果你用window系统，在绝对路经中用双“\\\\\", 列如： srcImage = cv2.imread('C:\\\\\\\\Users\\\\\\\\fw\\\\\\\\Anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\skimage\\\\\\\\data\\\\\\\\rocket.jpg')。 这因该没问题。", "Konwledge_Point": "应对NP完全问题", "Question": "python opencv 图片前景与背景的分割，拜大神求如何改错\n在网上找到了一个用Kmeans算法对图片前景与背景的分割的例子，很适合现在的学习，可一直有一个错误不会修改，跪求大神了。\n\n\n\n\n```# -*- coding: utf-8 -*-\nimport cv2\nimport numpy as np\nimport math\ndef panelAbstract(srcImage):\n    #   read pic shape\n    imgHeight,imgWidth = srcImage.shape[:2]\n    imgHeight = int(imgHeight);imgWidth = int(imgWidth)\n    # 均值聚类提取前景:二维转一维\n    imgVec = np.float32(srcImage.reshape((-1,3)))\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,10,1.0)\n    flags = cv2.KMEANS_RANDOM_CENTERS \n    label,clusCenter = cv2.kmeans(imgVec,2,None,criteria,10,flags)\n    clusCenter = np.uint8(clusCenter)\n    clusResult = clusCenter[label.flatten()]\n    imgres = clusResult.reshape((srcImage.shape))\n    imgres = cv2.cvtColor(imgres,cv2.COLOR_BGR2GRAY)\n    bwThresh = int((np.max(imgres)+np.min(imgres))/2)\n    _,thresh = cv2.threshold(imgres,bwThresh,255,cv2.THRESH_BINARY_INV)\n    threshRotate = cv2.merge([thresh,thresh,thresh])\n# 确定前景外接矩形\n    #find contours\n    contours = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n    minvalx = np.max([imgHeight,imgWidth]);maxvalx = 0\n    minvaly = np.max([imgHeight,imgWidth]);maxvaly = 0\n    maxconArea = 0;maxAreaPos = -1\n    for i in range(len(contours)):\n        if maxconArea < cv2.contourArea(contours[i]):\n            maxconArea = cv2.contourArea(contours[i])\n            maxAreaPos = i\n    objCont = contours[maxAreaPos]\n    # 旋转校正前景\n    rect = cv2.minAreaRect(objCont)\n    for j in range(len(objCont)):\n        minvaly = np.min([minvaly,objCont[j][0][0]])\n        maxvaly = np.max([maxvaly,objCont[j][0][0]])\n        minvalx = np.min([minvalx,objCont[j][0][1]])\n        maxvalx = np.max([maxvalx,objCont[j][0][1]])\n    if rect[2] <=-45:\n        rotAgl = 90 +rect[2]\n    else:\n        rotAgl = rect[2]\n    if rotAgl == 0:\n        panelImg = srcImage[minvalx:maxvalx,minvaly:maxvaly,:]\n    else:\n        rotCtr = rect[0]\n        rotCtr = (int(rotCtr[0]),int(rotCtr[1]))\n        rotMdl = cv2.getRotationMatrix2D(rotCtr,rotAgl,1)\n        imgHeight,imgWidth = srcImage.shape[:2]\n        #图像的旋转\n        dstHeight = math.sqrt(imgWidth *imgWidth + imgHeight*imgHeight)\n        dstRotimg = cv2.warpAffine(threshRotate,rotMdl,(int(dstHeight),int(dstHeight)))\n        dstImage = cv2.warpAffine(srcImage,rotMdl,(int(dstHeight),int(dstHeight)))\n        dstRotimg = cv2.cvtColor(dstRotimg,cv2.COLOR_BGR2GRAY)\n        _,dstRotBW = cv2.threshold(dstRotimg,127,255,0)\n        contours = cv2.findContours(dstRotBW,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n        maxcntArea = 0;maxAreaPos = -1\n        for i in range(len(contours)):\n            if maxcntArea < cv2.contourArea(contours[i]):\n                maxcntArea = cv2.contourArea(contours[i])\n                maxAreaPos = i\n        x,y,w,h = cv2.boundingRect(contours[maxAreaPos])\n        #提取前景：panel\n        panelImg = dstImage[int(y):int(y+h),int(x):int(x+w),:]\n\n    return panelImg\nif __name__==\"__main__\":\n    srcImage = cv2.imread('11.jpg')\n    a=panelAbstract(srcImage)\n    cv2.imshow('figa',a)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()  \n\n\n这是原地址https://blog.csdn.net/Dawn__Z/article/details/82115160\n报错如下（知道错是什么意思就是不会改）：Traceback (most recent call last):\n  File \"D:\\Workspaces\\MyEclipse 2015\\pythonTest\\src\\cc.py\", line 70, in \n    a=panelAbstract(srcImage)\n  File \"D:\\Workspaces\\MyEclipse 2015\\pythonTest\\src\\cc.py\", line 7, in panelAbstract\n    imgHeight,imgWidth = srcImage.shape[:2]\nAttributeError: 'NoneType' object has no attribute 'shape'\n", "Tag": "算法分析"}
{"Answer": "在计算复杂性理论中，多项式时间归约是指假设已有解决一个问题的子程序，利用它在多项式时间内（不考虑子程序运行所用时间）解决另一个问题的归约方法", "Konwledge_Point": "应对NP完全问题", "Question": "简要地描述多项式时间约简的思想\nBriefly describe the idea of the polynomial time reduction. Explain how to use it to\nprove a problem is NP-complete.\n\n\n4-SAT Problem: for a Boolean formula in CNF in which each clause has exactly 4\nliterals, determine if there is an assignment of Boolean value to its variables so that\nthe formula evaluates to true? (i.e., the formula is satisfiable). Prove 4-SAT Problem\nis NP-Complete.\n\n", "Tag": "算法分析"}
{"Answer": "select Date,Close from stock trading date where Date >= '2005-01-01' AND Date <= '2018-12-31' AND Stkcd = '000041'从报错看，应该是这句查询语句错了，然后select Data,Close(这是属性) from stock trading data(这3个单词应该是表吧，你看下表的名称具体是什么，应该不是这么写的，至少应该是有下划线什么的连起来的) where。。。", "Konwledge_Point": "应对NP完全问题", "Question": "~_~ python读取数据库错误！！我实在是找不到问题在哪里了\nimport\n numpy \nas\n np\n\nimport\n pandas \nas\n pd\n\nfrom\n sqlalchemy \nimport\n create_engine\n\nmysql_config = {\n'host'\n: \n'59.78.102.118'\n,\n                \n'port'\n: \n23208\n,\n                \n'user'\n: \n'student'\n,\n                \n'password'\n: \n'Python_123456'\n,\n                \n'dbname'\n: \n'research'\n}\nmysql_engine = create_engine(\n'mysql+pymysql://student:Python_123456@59.78.102.118:23208/research'\n.format(**mysql_config), pool_pre_ping=\nTrue\n)\n\n\nsql\n = \"select Date,Close from stock trading date where Date >= '2005-01-01' AND Date <= '2018-12-31' AND Stkcd = '000041'\"\npd_data = pd.read_sql(\nsql\n,mysql_engine)\nprint(pd_data)\n", "Tag": "算法分析"}
{"Answer": "修改如下：\nclass strss :public strs      //目的：构建一个strs类的对象的数组，然后通过 创建strss类的对象完成所有创建 \n{\npublic:\n    int nums;\n    strs* arr;  //这里声明一下\n    strss()\n    {\n        cin>>nums;\n        arr=new strs[nums];  //这里去掉类型前缀\n        cout<<\"strss构造\"<<endl;\n    }\n};\n\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "c++类的继承创建一个基类的数组\n以下是我写的一段源码，主要问题是类strss中的动态创建与继承之间关系的问题，简单来说strs就是一个简单地字符串类，我想用strss继承strs，并动态的创建一个strs类的数组，整个代码的功能：输入整数，再输入相应个数个字符串，判断其是否符合，所有字符串判断完后一次性输出YES或者NO。\n代码报错arr不存在。\n另外可能有的小伙伴可能知道这个题，会说我题目没读对，我发现了，但是题的问题不难，主要是这个\n另外\n感谢各位的观看！\nc++路上加油！\n\n\n\n\n#\ninclude\n\n\n\n#\ninclude\n\n\n\nusing\n \nnamespace\n std;\n\nclass\n \nstrs\n                     \n//字符串类 \n\n{\n    \npublic\n:\n        string str;            \n        \nint\n np,nt;            \n//分别记录PAT的数量，正常来说，PT只含有一个且有序，A无限制  \n\n        \nbool\n flag;\n        \n        \nstrs\n()                \n//构造函数 \n\n        {\n            cin>>str;\n            flag=\ntrue\n;\n            np=\n0\n;\n            nt=\n0\n;\n            \nfor\n(\nint\n i=\n0\n;i>nums;\n            strs *arr=\nnew\n strs[nums];  \n            cout<<\n\"strss构造\"\n<<endl;\n        }\n};\n\n\n\n\nint\n \nmain\n()\n\n\n{\n    strss a1;\n    \nfor\n(\nint\n j=\n0\n;j<a1.nums;j++)\n    {\n        a1.arr[j].\njudgeit\n();\n    }\n    \nreturn\n \n0\n;\n}\n", "Tag": "算法分析"}
{"Answer": ".name取他的列名。觉得有帮助还请点采纳哦", "Konwledge_Point": "应对NP完全问题", "Question": "DataFrame中的name属性值\na = pd.DataFrame(np.arange(12).reshape(3,4),index=list(\"abc\"),columns=list(\"vxyz\"))\na = a.apply(lambda m:np.square(m) if m.name == \"x\" else m)\nprint(a)\n\n\nm.name是什么意思，DataFrame中的name属性值是什么？", "Tag": "算法分析"}
{"Answer": "你现在有什么问题呢？编译错误还是运行结果错误？", "Konwledge_Point": "应对NP完全问题", "Question": "可以帮我看一下登录进去之后浏览函数和借阅函数哪里出问题了吗\n问题遇到的现象和发生背景\n\n\n问题相关代码，请勿粘贴截图\n\n\n运行结果及报错内容\n\n\n#include \n#include \n#include \n#include <\ntime\n.h>\n\ntypedef struct book\n{\n    char bnum[\n10\n];\n//书籍编号 \n\n    char bname[\n30\n];\n//书名 \n\n    char bauthor[\n20\n];\n//作者 \n\n    char bclassfy[\n10\n];\n//类别编号 \n\n    float bprice;\n//价格 \n\n    int quantity;\n//数量 \n\n    struct book* next;\n//链表指针 \n\n }BookInfo;\n\n BookInfo* CreateBooksList();\n//创建链表\n\nvoid Insert(BookInfo* head);\n//插入\n\nvoid Delete(BookInfo* head);\n//删除\n\nvoid Print(BookInfo* head);\n//浏览 \n\nvoid Search(BookInfo* head);\n//查询 \n\nvoid Update(BookInfo* head);\n//修改 \n\nvoid Save(BookInfo* head);\n//保存 \n\nvoid Borrowedbooks();\n//图书借阅 \n\ntypedef struct User\n{\n    char \nname\n[\n10\n];\n//名字\n\n    char password[\n10\n];\n//密码\n\n    char book[\n10\n];\n//借的书 \n\n    struct User* next;  \n//下一位用户\n\n}user;\nstruct bk\n//查找 \n\n{\n    char bnum[\n10\n];\n//书籍编号 \n\n    char bname[\n30\n];\n//书名 \n\n    char bauthor[\n20\n];\n//作者 \n\n    char bclassfy[\n10\n];\n//类别编号 \n\n    float bprice;\n//价格 \n\n    int quantity;\n//数量 \n\n};\nstruct ur\n//查找 \n\n{\n    char \nname\n[\n10\n];\n//名字\n\n    char password[\n10\n];\n//密码\n\n    char book[\n10\n];\n//借的书 \n\n};\n\nuser* CreateusersList();\n//创建链表 \n\nvoid userInsert(user* head);\n//注册用户信息 \n\nvoid userDelete(user* head);\n//删除用户\n\nvoid userprint(user* head);\n//浏览用户 \n\nuser* serch_username(char* \nname\n);\n//根据名字查找用户 \n\nvoid user_login();  \n//用户登录\n\nvoid userUpdate(user* head);\n//更改用户信息 \n\nvoid userSave(user* head);\n//保存用户信息 \n\nint usermenu();\n//用户系统\n\nvoid adminlogin();\n//管理员登录 \n\n\n\n//创建用户名单 \n\nuser* CreateusersList()\n{\n    user* head;\n    head=(user*)malloc(sizeof(user));\n//为头结点分配空间 \n\n    \nhead\n->\nnext=NULL;\n//初始化头指针\n\n    return head; \n } \n \n//注册用户信息 \n\nvoid userInsert(user* head)\n{\n    user *b, *p;\n    char flag=\n'Y'\n;\n    p=head;\n    \nwhile\n (p->\nnext !=NULL)\n        \np\n=p->\nnext;\n    \n//开辟新空间，存储用户信息，并加入链表\n\n    \nwhile\n (flag==\n'Y'\n||flag==\n'y'\n)\n    {\n        b=(user*)malloc(sizeof(user));\n//开辟新空间\n\n        printf(\n\"请输入用户名：\"\n);\n//获取名称信息 \n\n        fflush(stdin); \n//清空缓存区\n\n        \nscanf\n(\"%s\",b->\nname\n);\n        printf(\n\"请输入密码：\"\n);\n        fflush(stdin);\n        \nscanf\n(\"%s\",b->\npassword);\n        \np\n->\nnext=b;\n//将新增加的节点加入链表\n\n        p=b;\n//指针p向后移动，指向尾结点\n\n        \nb\n->\nnext=NULL;\n        printf(\n\"注册成功！\\n\"\n);\n        fflush(stdin);\n        break;\n     } \n     return;\n }\nuser* user_head;  \n//用户头指针\n\nuser* serch_username(char* \nname\n)  \n//查找用户名\n\n{\n    \nuser\n* np = user_head->\nnext;\n    \nwhile\n (np)\n    {\n        \nif\n (!strcmp(np->\nname\n, \nname\n)) return np;\n        \nnp\n = np->\nnext;\n    }\n    return NULL;\n}\n\n\n//用户登录 \n\nvoid user_login()  \n//用户登录\n\n{\n    char \nname\n[\n30\n];\n    char password[\n30\n];\n    printf(\n\"请输入您的用户名(不超过10个字母)：\\n\"\n);\n    scanf(\n\"%s\"\n, \nname\n);\n    printf(\n\"请输入您的账号密码(不超过10个字母)：\\n\"\n);\n    scanf(\n\"%s\"\n, password);\n    int i,j=\n0\n;\n    FILE* fp;\n    struct ur u;\n    fp=fopen(\n\"F:\\\\c语言课设\\\\userslist.txt\"\n,\n\"r\"\n); \n    \nif\n(j=\n0\n)\n    {\n        \nfor\n(i=\n0\n;i<\n10\n;i++)\n    {\n        fscanf(fp,\n\"%s %s %s\\n\"\n,&u.\nname\n,&u.password,&u.book);\n        \nif\n(strcmp(u.\nname\n,\nname\n)==\n0\n&&strcmp(u.password,password)==\n0\n)\n        {\n            printf(\n\"恭喜您登录成功,即将跳转用户界面...\\n\"\n);\n            system(\n\"cls\"\n);\n            j=\n1\n;\n            usermenu();\n        }\n    }\n    fclose(fp);\n    }\n    \nif\n(j=\n0\n)\n    {\n        printf(\n\"用户名或密码错误\\n\"\n);\n    }\n    }\n\n\n//\n\n\n \n \n//删除用户 \n\nvoid userDelete(user* head)\n{\n    user *b, *p;\n    char tmp[\n30\n];\n    int flag;\n    flag=\n0\n;\n    b=head;\n    p=head;\n    printf(\n\"请输入要删除的用户名：\"\n);\n    fflush(stdin);\n    scanf(\n\"%s\"\n,tmp);\n    \n//遍历链表\n\n    \nwhile\n (p!=NULL)\n    {\n        \nif\n(strcmp(p->\nname\n,tmp)==\n0\n)\n        {\n            flag=\n1\n;\n            break;\n        }\n        \np\n=p->\nnext;\n     } \n    \nif\n (flag==\n1\n)\n    {\n        \nfor\n (;b->\nnext !=p;)\n        {\n            \nb\n=b->\nnext;\n        }\n        \nb\n->\nnext\n=p->\nnext;\n        free(p);\n        printf(\n\"删除成功！\\n\"\n);\n    }\n    \nelse\n\n        printf(\n\"该用户不存在！\"\n);\n    return;\n  } \n\n//浏览用户 \n\nvoid userprint(user* head)\n{\n    user *p;\n    FILE *fp;\n    fp=fopen(\n\"F:\\\\c语言课设\\\\userslist.txt\"\n,\n\"r\"\n);\n    rewind(fp);\n    char ch=fgetc(fp);\n    \nwhile\n(ch!=-\n1\n)\n    {\n        putchar(ch);\n        ch=fgetc(fp);\n    }\n    fclose(fp);\n\n }\n\n//查找用户 \n\nvoid userSearch(user* head)\n{\n    user *p;\n    char tmp[\n30\n];\n    int flag=\n0\n;\n    p=head;\n    \nif\n(head==NULL||head->\nnext==NULL)\n    {\n        printf(\n\"清单为空！\\n\"\n);\n    }\n    \nelse\n\n    {\n        printf(\n\"请输入用户名：\"\n);\n        fflush(stdin);\n        scanf(\n\"%s\"\n,tmp);\n        \nwhile\n(p->\nnext !=NULL)\n        {\n            \np\n=p->\nnext;\n            \nif\n(strcmp(p->\nname\n,tmp)==\n0\n)\n            {\n                flag=\n1\n;\n                \nprintf\n(\"用户名：%s\\n\",p->\nname\n);\n                return;\n            }\n            \nif\n(p->\nnext==NULL)\n            {\n                printf(\n\"\\n查询完毕！\"\n);\n            }\n            \n        }\n        \nif\n(flag==\n0\n)\n        {\n            printf(\n\"没有找到该用户!\\n\"\n,tmp);\n        }\n    }\n    return;\n }\n \n\n//保存用户信息 \n\nvoid userSave(user* head)\n{\n    user *p;\n    FILE *fp;\n    p=head;\n    \n//以只写的方式打开文件\n\n    fp=fopen(\n\"F:\\\\c语言课设\\\\userslist.txt\"\n,\n\"a\"\n);\n    \nwhile\n (p->\nnext !=NULL)\n    {\n        \np\n=p->\nnext;\n        \nfprintf\n(fp,\"%-6s %-10s\\n\",p->\nname\n,p->\npassword);\n    }\n    fclose(fp);\n    printf(\n\"保存成功！\\n\"\n);\n    printf(\n\"数据已成功保存到F:\\\\c语言课设\\\\userslist.txt\\n\"\n);\n     \n }   \nint menu();\n//菜单 \n\n\n\n//创建书单\n\nBookInfo* CreateBooksList()\n{\n    BookInfo* head;\n    head=(BookInfo*)malloc(sizeof(BookInfo));\n//为头结点分配空间 \n\n    \nhead\n->\nnext=NULL;\n//初始化头指针\n\n    return head; \n } \n\n//插入记录\n\nvoid Insert(BookInfo* head)\n{\n    BookInfo *b, *p;\n    char flag=\n'Y'\n;\n    p=head;\n    \nwhile\n (p->\nnext !=NULL)\n        \np\n=p->\nnext;\n    \n//开辟新空间，存储书籍信息，并加入链表\n\n    \nwhile\n (flag==\n'Y'\n||flag==\n'y'\n)\n    {\n        b=(BookInfo*)malloc(sizeof(BookInfo));\n//开辟新空间\n\n        printf(\n\"请输入图书编号：\"\n);\n//获取书籍信息 \n\n        fflush(stdin); \n//清空缓存区\n\n        \nscanf\n(\"%s\",b->\nbnum);\n        printf(\n\"请输入书名：\"\n);\n        fflush(stdin);\n        \nscanf\n(\"%s\",b->\nbname);\n        printf(\n\"请输入作者：\"\n);\n        fflush(stdin);\n        \nscanf\n(\"%s\",b->\nbauthor);\n        printf(\n\"请输入类别编号：\"\n);\n        fflush(stdin);\n        \nscanf\n(\"%s\",b->\nbclassfy);\n        printf(\n\"请输入图书价格：\"\n);\n        fflush(stdin);\n        \nscanf\n(\"%f\",&b->\nbprice);\n        printf(\n\"请输入图书数量：\"\n);\n        fflush(stdin);\n        \nscanf\n(\"%d\",&b->\nquantity);\n        \np\n->\nnext=b;\n//将新增加的节点加入链表\n\n        p=b;\n//指针p向后移动，指向尾结点\n\n        \nb\n->\nnext=NULL;\n        printf(\n\"添加成功！\\n继续添加？(Y/N):\"\n);\n        fflush(stdin);\n        scanf(\n\"%c\"\n,&flag);\n        \nif\n(flag==\n'N'\n||flag==\n'n'\n)break;\n        \nelse\n \nif\n (flag==\n'Y'\n||flag==\n'y'\n)continue; \n     } \n     return;\n }\n\n//删除记录\n\nvoid Delete(BookInfo* head)\n{\n    BookInfo *b, *p;\n    char tmp[\n30\n];\n    int flag;\n    flag=\n0\n;\n    b=head;\n    p=head;\n    printf(\n\"请输入要删除的书籍名：\"\n);\n    fflush(stdin);\n    scanf(\n\"%s\"\n,tmp);\n    \n//遍历链表\n\n    \nwhile\n (p!=NULL)\n    {\n        \nif\n(strcmp(p->\nbname,tmp)==\n0\n)\n        {\n            flag=\n1\n;\n            break;\n        }\n        \np\n=p->\nnext;\n     } \n    \nif\n (flag==\n1\n)\n    {\n        \nfor\n (;b->\nnext !=p;)\n        {\n            \nb\n=b->\nnext;\n        }\n        \nb\n->\nnext\n=p->\nnext;\n        free(p);\n        printf(\n\"删除成功！\\n\"\n);\n    }\n    \nelse\n\n        printf(\n\"该书不存在！\"\n);\n    return;\n  }  \n\n//浏览书单\n\nvoid print(BookInfo* head)\n{\n    BookInfo *p;\n    FILE *fp;\n    fp=fopen(\n\"F:\\\\c语言课设\\\\bookslist.txt\"\n,\n\"r\"\n);\n    rewind(fp);\n    printf(\n\"-----------------------------------------------------\\n\"\n);\n    printf(\n\"|编号   |书名   |作者    |类别编号   |价格   |数量  |\\n\"\n);\n    printf(\n\"-----------------------------------------------------\\n\"\n);\n    char ch=fgetc(fp);\n    \nwhile\n(ch!=-\n1\n)\n    {\n        putchar(ch);\n        ch=fgetc(fp);\n    }\n    fclose(fp);\n\n } \n\n//查找书籍\n\nvoid Search(BookInfo* head)\n{\n    BookInfo *p;\n    char tmp[\n30\n];\n    int flag=\n0\n;\n    p=head;\n    \nif\n(head==NULL||head->\nnext==NULL)\n    {\n        printf(\n\"清单为空！\\n\"\n);\n    }\n    \nelse\n\n    {\n        printf(\n\"请输入书籍名：\"\n);\n        fflush(stdin);\n        scanf(\n\"%s\"\n,tmp);\n        \nwhile\n(p->\nnext !=NULL)\n        {\n            \np\n=p->\nnext;\n            \nif\n(strcmp(p->\nbname,tmp)==\n0\n)\n            {\n                flag=\n1\n;\n                \nprintf\n(\"编号：%s\\n 书名：《%s》\\n作者：%s\\n分类：%s\\n价格：%.2f\\n\",p->\nbnum\n,p->\nbname\n,p->\nbauthor\n,p->\nbclassfy\n,p->\nbprice);\n                return;\n            }\n            \nif\n(p->\nnext==NULL)\n            {\n                printf(\n\"\\n查询完毕！\"\n);\n            }\n            \n        }\n        \nif\n(flag==\n0\n)\n        {\n            printf(\n\"没有找到《%s》!\\n\"\n,tmp);\n        }\n    }\n    return;\n }\nvoid creat_user_list(char* \nname\n, char* password)  \n//创建用户链表\n\n{\n    user* head;\n    user* np = (user*)malloc(sizeof(user));\n    np = user_head;\n    \nwhile\n (np->\nnext\n) np = np->\nnext;\n    User* tp = (user*)malloc(sizeof(user));\n    \nstrcpy\n(tp->\nname\n, \nname\n);\n    \nstrcpy\n(tp->\npassword, password);\n    \ntp\n->\nnext = NULL;\n    \nnp\n->\nnext = tp;\n    userSave(head);\n    \n}\n\n//修改信息\n\nvoid Update(BookInfo* head)\n{\n    BookInfo *p;\n    int flag=\n0\n;\n    char tmp[\n30\n];\n    p=head;\n    printf(\n\"请输入书名：\"\n);\n    fflush(stdin);\n    scanf(\n\"%s\"\n,tmp);\n    \nwhile\n(p->\nnext!=NULL)\n    {\n        \np\n=p->\nnext;\n        \nif\n(strcmp(p->\nbname,tmp)==\n0\n)\n        {\n            flag=\n1\n;\n//标志找到所要修改修改的书籍\n\n            printf(\n\"请输入编号：\"\n);\n            fflush(stdin);\n            \nscanf\n(\"%s\",p->\nbnum);\n            printf(\n\"请输入书名：\"\n);\n            fflush(stdin);\n            \nscanf\n(\"%s\",p->\nbname);\n            printf(\n\"请输入作者：\"\n);\n            fflush(stdin);\n            \nscanf\n(\"%s\",p->\nbauthor);\n            printf(\n\"请输入类别编号：\"\n);\n            fflush(stdin);\n            \nscanf\n(\"%s\",p->\nbclassfy);\n            printf(\n\"请输入价格：\"\n);\n            fflush(stdin);\n            \nscanf\n(\"%s\",p->\nbprice);\n\n        }\n    }\n    \nif\n (flag==\n0\n)\n    {\n        printf(\n\"没有找到《%s》!\\n\"\n,tmp);\n    }\n    return;\n } \n \n\n//保存书单到文件\n\nvoid Save(BookInfo* head)\n{\n    BookInfo *p;\n    FILE *fp;\n    p=head;\n    \n//以只写的方式打开文件\n\n    fp=fopen(\n\"F:\\\\c语言课设\\\\bookslist.txt\"\n,\n\"a\"\n);\n    \nwhile\n (p->\nnext !=NULL)\n    {\n        \np\n=p->\nnext;\n        \nfprintf\n(fp,\"%-6s %-10s %-10s %-10s %.2lf %d\\n\",p->\nbnum\n,p->\nbname\n,p->\nbauthor\n,p->\nbclassfy\n,p->\nbprice\n,p->\nquantity);\n    }\n    fclose(fp);\n    printf(\n\"保存成功！\\n\"\n);\n    printf(\n\"数据已成功保存到F:\\\\c语言课设\\\\bookslist.txt\\n\"\n);\n     \n } \nvoid user_register()  \n//用户注册\n\n{\n    char \nname\n[\n30\n];\n    printf(\n\"请输入您需要注册的用户名(不超过25个字母)：\\n\"\n);\n    scanf(\n\"%s\"\n, \nname\n);\n    User* account;\n    \nwhile\n (account = serch_username(\nname\n), account != NULL)\n    {\n        printf(\n\"该用户名已存在，请重新输入！\\n\"\n);\n        scanf(\n\"%s\"\n, \nname\n);\n    }\n    printf(\n\"请输入您的账号密码(不超过25个字母)：\\n\"\n);\n    char password[\n30\n];\n    scanf(\n\"%s\"\n, password);\n    creat_user_list(\nname\n, password);\n    printf(\n\"恭喜您注册成功！\\n\"\n);\n}\n\n//菜单\n\nvoid main_menu()  \n//主菜单\n\n{\n    user *head;\n    BookInfo *headd;\n    int a,b,c,d;\n    head=NULL;\n    \nwhile\n (\n1\n)\n    {\n        printf(\n\"+---------------------------------------------+\\n\"\n);\n        printf(\n\"*            欢迎进入本图书管理系统!          *\\n\"\n);\n        printf(\n\"*         请输入选项前的数字以确认操作！      *\\n\"\n);\n        printf(\n\"*               1、用户注册                   *\\n\"\n);\n        printf(\n\"*               2、用户登陆                   *\\n\"\n);\n        printf(\n\"*               3、管理员登陆                 *\\n\"\n);\n        printf(\n\"*               0、退出               *\\n\"\n);\n        printf(\n\"+---------------------------------------------+\\n\"\n);\n        int op; \n        scanf(\n\"%d\"\n, &op); \n        switch (op)\n        {\n        case \n1\n: \nif\n(head==NULL)\n                {\n                    head=CreateusersList();\n                }\n                userInsert(head);\n                userSave(head);\n                break;\n        case \n2\n: user_login();\n                usermenu();\n                b=usermenu();\n                switch (b)\n                {\n                    case \n1\n:print(headd); usermenu();\n                    case \n2\n:Search(headd); usermenu();\n                    case \n3\n:Borrowedbooks(); usermenu();\n                    case \n4\n:\n                    case \n5\n:userSave(head); usermenu();\n                    case \n6\n:exit(\n0\n); break;\n                }\n                break;\n        case \n3\n: adminlogin();  break;\n        case \n0\n: exit(\n0\n); break;\n        default: printf(\n\"错误的指令，请重新输入！\\n\"\n); \n        }\n    }\n}\nint adminmenu()\n{\n    int sec;\n    user *head;\n    BookInfo *headd;\n    headd=NULL;\n    printf(\n\"                    管理员系统                     \\n\"\n);\n    printf(\n\"-----------------------------------------------------\\n\"\n);\n    printf(\n\"                   1-图书信息录入\\n\"\n);\n    printf(\n\"                   2-图书信息浏览\\n\"\n);\n    printf(\n\"                   3-图书信息查询\\n\"\n);\n    printf(\n\"                   4-图书信息修改\\n\"\n);\n    printf(\n\"                   5-图书信息删除\\n\"\n);\n    printf(\n\"                   6-图书信息保存\\n\"\n);\n    printf(\n\"                   7-删除用户\\n\"\n);\n    printf(\n\"                   8-浏览用户\\n\"\n);\n    printf(\n\"                   9-退出\\n\"\n);\n    printf(\n\"-----------------------------------------------------\\n\"\n);\n    printf(\n\"请选择：\"\n);\n    fflush(stdin);\n    scanf(\n\"%d\"\n,&sec);\n    switch (sec)\n    {\n        case \n1\n:\n            \nif\n(headd==NULL)\n            {\n                headd=CreateBooksList();\n            }\n            Insert(headd);\n            Save(headd); \n            break;\n        case \n2\n:print(headd); break;\n        case \n3\n:Search(headd); break;\n        case \n4\n:Update(headd); break;\n        case \n5\n:Delete(headd); break;\n        case \n6\n:Save(headd); break;\n        case \n7\n:\n        case \n8\n:\n        case \n9\n:exit(\n0\n); break;\n    }\n    \nwhile\n(sec>\n7\n||sec<\n0\n)\n    {\n        printf(\n\"选择有误！\\n请重新输入：\"\n);\n        scanf(\n\"%d\"\n,&sec);\n     } \n    return sec;\n }\n \n//管理员登录 \n\nvoid adminlogin()\n{\n    char tmp[\n30\n];\n    char tmpp[\n30\n];\n    int flag=\n0\n;\n        printf(\n\"请输入用户名：\"\n);\n        fflush(stdin);\n        scanf(\n\"%s\"\n,tmp);\n        printf(\n\"请输入密码：\"\n);\n        fflush(stdin);\n        scanf(\n\"%s\"\n,tmpp);\n            \nif\n(strcmp(\n\"dyq\"\n,tmp)==\n0\n&&strcmp(\n\"123\"\n,tmpp)==\n0\n)\n            {\n                flag=\n1\n;\n                printf(\n\"登陆成功\\n\"\n);\n                system(\n\"cls\"\n);\n                adminmenu();\n                \n            }\n            \n        \nif\n(flag==\n0\n)\n        {\n            printf(\n\"用户名或密码错误!\\n\"\n,tmp);\n        }\n        \n    return;\n }\n int usermenu()\n {\n     int b;\n    printf(\n\"                    用户系统                     \\n\"\n);\n    printf(\n\"-----------------------------------------------------\\n\"\n);\n    printf(\n\"                   1-图书信息浏览\\n\"\n);\n    printf(\n\"                   2-图书信息查询\\n\"\n);\n    printf(\n\"                   3-图书借阅\\n\"\n);\n    printf(\n\"                   4-图书归还\\n\"\n);\n    printf(\n\"                   5-保存用户信息\\n\"\n);\n    printf(\n\"                   6-退出\\n\"\n);\n    printf(\n\"-----------------------------------------------------\\n\"\n);\n    printf(\n\"请选择：\"\n);\n    fflush(stdin);\n    scanf(\n\"%d\"\n,&b);\n    \nwhile\n(b>\n6\n||b<\n0\n)\n    {\n        printf(\n\"选择有误！\\n请重新输入：\"\n);\n        scanf(\n\"%d\"\n,&b);\n     } \n    return b;    \n }\nint tjzs()\n//统计图书文本个数\n\n{\nFILE *fp;\nint n;\nfloat bprice=\n0\n;\nint quantity=\n0\n;\nchar bum[\n10\n]={\n'\\0'\n},bname[\n30\n]={\n'\\0'\n},bauthor[\n20\n]={\n'\\0'\n},bclassfy[\n10\n]={\n'\\0'\n};\nfp=fopen(\n\"library.txt\"\n,\n\"r\"\n);\n//打开文件\n\n\nfor\n (n=\n0\n;!feof(fp);n++)\n//逐个读文件\n\nfscanf(fp,\n\"%-6s %-10s %-10s %-10s %.2lf %d\\n\"\n,bum,bname,bauthor,bclassfy,&bprice,&quantity);\nn--;\nfclose(fp);\n//关闭文件\n\nreturn (n);\n//返回个数\n\n}\n\nvoid Borrowedbooks()\n//借书函数\n\n{\n    FILE *fp,*fp3;\n    BookInfo *head=NULL;\n    BookInfo *p,*p1,*p2;\n    int i,\nloop\n,k,n=\n0\n,flag=\n0\n,s=\n0\n;\n    int quantity=\n0\n;\n    float bprice=\n0\n;\n    char bnum[\n10\n]={\n'\\0'\n},bname[\n30\n]={\n'\\0'\n},bauthor[\n20\n]={\n'\\0'\n},bclassfy[\n10\n]={\n'\\0'\n},\n         \nname\n[\n30\n]={\n'\\0'\n},password[\n30\n]={\n'\\0'\n},riqi[\n20\n]={\n'\\0'\n},book[\n10\n];\n    char hit=\n0\n;\n        printf(\n\"\\n请输入借阅书名:\\n请输入:\"\n);\n        scanf(\n\"%s\"\n,book);\n    k= tjzs();\n//统计图书馆文件个数\n\n\n    \nfor\n (i=\n0\n;i\nnext=p1;\n                p2=p1;\n                p1=(BookInfo*)malloc(sizeof(BookInfo));\n\n            }\n            \nstrcpy\n(p1->\nbnum,bnum);\n//复制书号\n\n            \nstrcpy\n(p1->\nbname,\nname\n);\n//复制书名\n\n            \nstrcpy\n(p1->\nbauthor,bauthor);\n//复制作者\n\n            \nstrcpy\n(p1->\nbclassfy,bclassfy);\n//复制出版社\n\n            \np1\n->\nquantity=quantity;\n//复制现存量\n\n            \np1\n->\nbprice=bprice;\n//复制单价\n\n        }\n        \nif\n(n==\n0\n)\n         head=NULL;\n        \nelse\n\n         {\n             \np2\n->\nnext=p1;\n             \np1\n->\nnext=NULL;\n             fclose(fp);\n         }\n\n    p=head;\n    \n    \nfor\n (;p!=NULL;)\n    {\n        \nif\n(!(strcmp(p->\nbname,book)))\n//判断要借书的是否存在，标记等于1，存在库存减一\n\n        {\n            \n            flag=\n1\n;\n             \nloop\n=p->\nquantity;\n             (\np\n->\nquantity)--;\n         }\n            \np\n=p->\nnext;\n  }\n    \nif\n(flag&&(\nloop\n>\n0\n))\n//存在被借的图书且现存量大于0，把库存量变化后的链表存入文件\n\n    {\n\n        fp=fopen(\n\"F:\\\\c语言课设\\\\bookslist.txt\"\n,\n\"w\"\n);\n        fclose(fp);\n        fp=fopen(\n\"F:\\\\c语言课设\\\\bookslist.txt\"\n,\n\"a\"\n);\n        p=head;\n\n        \nfor\n(;p !=NULL;)\n        {\n            \nfprintf\n(fp,\"%-6s %-10s %-10s %-10s %.2lf %d\\n\",p->\nbnum\n,p->\nbname\n,p->\nbauthor\n,p->\nbclassfy\n,p->\nbprice\n,p->\nquantity);\n                  \np\n=p->\nnext;\n        }\n        free(p);\n        fclose(fp);\n\n        \nif\n ((fp3=fopen(\n\"F:\\\\c语言课设\\\\userslist.txt\"\n,\n\"r\"\n))==NULL)\n//建读者文件夹存入借书信息\n\n        {\n            fp3=fopen(\n\"F:\\\\c语言课设\\\\userslist.txt\"\n,\n\"w\"\n);\n//打开只读文件\n\n            fclose(fp3);\n        }\n        fp3=fopen(\n\"F:\\\\c语言课设\\\\userslist.txt\"\n,\n\"a\"\n);\n\n        printf(\n\"\\n请按以下格式输入读者信息:\\n姓名 密码 借书书名\\n请输入:\"\n);\n//录入读者信息\n\n         scanf(\n\"%d %s %s %s\"\n,&\nname\n,password,book);\n        fprintf(fp,\n\"%s %s %s\\n\"\n,\nname\n,password,book);\n        fclose(fp3);\n        printf(\n\"借书成功!请两个月内归还！！！按任意键返回\\n\"\n);\n        usermenu();\n//调用借阅系统\n\n   }\n    \nelse\n\n    {\n        \nif\n(flag!=\n0\n)\n        printf(\n\"此书已被借完!按任意键返回!\"\n);\n//否则输出此书已被借完\n\n        \nelse\n\n        printf(\n\"查找无此书!按任意键返回\"\n);\n    }\n        usermenu();\n//调用借阅系统\n\n\n}\n\nint main()\n{\n    user *head;\n    BookInfo *headd;\n    int a,b,c,d;\n    head=NULL;\n    \nfor\n (;;)\n    {\n        main_menu();\n    }\n    return \n0\n;\n}\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "name里用英文", "Konwledge_Point": "应对NP完全问题", "Question": "利用numpy创建一个包含学生信息的数据类型\n尝试创建stu1 数据类型，包括姓名，学号，身份证号码，手机号码，宿舍地址。数据类型自行确定，并采用该类型来初始化一个学生对象\n\n\nimport numpy as np\nstu1=np\n.dtype\n(\n[(\n'name'\n, \n'S20'\n),(\n'id'\n,\n'S20'\n),(\n'sfz'\n,\n'i1'\n),(\n'phone'\n,\n'i1'\n),(\n'address'\n,\n'S20'\n)]\n)\nstu1_1=np\n.array\n(\n[(\n'张三'\n,\n'10'\n,\n'12'\n,\n'13962'\n,\n'15502'\n)]\n,stu1)\n\nprint\n(stu1)\n\n\nprint\n(stu1_1)\n\n\n\n\n报错：'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)", "Tag": "算法分析"}
{"Answer": "可变对象，+ 操作改变了值，id肯定会变，而+= 是本地操作，其值原地修改\n1.对于+号操作，可变对象和不可变对象调用的都是__add__操作2.对于+=号操作，可变对象调用__add__，不可变对象调用的是__iadd__(不可变对象没有__iadd__)   __iadd__是原地修改", "Konwledge_Point": "应对NP完全问题", "Question": "(python)函数接口里等价表示+=输出的结果却不同\nmove_on 函数dot的迭代那两个表达式应该是等价的,但为什么换了一次输出结果就变了\n问题背景是运动学迭代方程求轨迹,然后对固定轨迹搜索每一步的参数\n\n\nfrom\n PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#让图表显示出中文\n\nplt.rcParams[\n'font.sans-serif'\n]=[\n'SimHei'\n]\nplt.rcParams[\n'axes.unicode_minus'\n] = \nFalse\n\nimport random\nimport math as m\ndef new():\n    x0,y0,vl,vr,\ntheta\n=0.0,0.0,0.1,0.1,0.0\n    \ndot\n=np.array([x0,y0,vl,vr,theta])\n    return dot\ndef move_on(dot,al,ar,dt):\n    \ntheta\n=dot[4]\n    \nmid\n=np.mat(np.array([[m.cos(theta),m.sin(theta),0],[0,0,1]]))\n    \nmid\n=np.transpose(mid)\n    \nmid1\n=np.mat(np.array([[0.5,0.5],[1,-1]]))\n    \nvm\n=np.transpose(np.mat([dot[3]+ar\n*d\nt,dot[2]+al\n*d\nt]))\n    \nchange\n=mid*mid1*vm\n    dot+=np.array([float(change[0])\n*d\nt,float(change[1])\n*d\nt,al\n*d\nt,ar\n*d\nt,float(change[2])\n*d\nt])\n    #\ndot\n=dot+np.array([float(change[0])\n*d\nt,float(change[1])\n*d\nt,al\n*d\nt,ar\n*d\nt,float(change[2])\n*d\nt])\n    return dot\n\ndef dif_one(dot1,dot2):\n    return (dot1[0]-dot2[0])*\n*2\n+(dot1[1]-dot2[1])*\n*2\n\ndef one_p_best(dot_pre,dot,a_max,N,dt):\n    \nmin_dif\n=1000000000\n    best_alr=[0,0,10000]\n    \nfor\n i \nin\n range(2*N):\n        \nfor\n j \nin\n range(2*N):\n            al,\nar\n=a_max*(i-N)/N,a_max*(j-N)/N\n            #\nprint\n(1)\n            \nif\n dif_one(move_on(dot_pre,al,ar,dt),dot)<min_dif:\n                #\nprint\n(123)\n                \nmin_dif\n=dif_one(move_on(dot_pre,al,ar,dt),dot)\n                best_alr=[a_max*(i-N)/N,a_max*(j-N)/N,min_dif]\n    \nprint\n((move_on(dot_pre,best_alr[0],best_alr[1],dt)[0],move_on(dot_pre,best_alr[0],best_alr[1],dt)[1]))\n    \nprint\n(best_alr)\n    \nprint\n(dif_one(move_on(dot_pre,best_alr[0],best_alr[1],dt),dot))\n    return best_alr\n\ndef one_by_search(dot,x_set,y_set,a_max,N,dt):\n    \nn\n=len(x_set)\n    l_x=[];l_y=[];l_dif=[]\n    l_x.append(dot[0]);l_y.append(dot[1])\n    \nfor\n i \nin\n range(1,n):\n        dot_true=[x_set[i],y_set[i]]\n        \nl\n=one_p_best(dot,dot_true,a_max,N,dt)\n        al,ar,\ndif\n=l[0],l[1],l[2]\n        l_x.append(dot[0]);l_y.append(dot[1]);l_dif.append(dif)\n        \ndot\n=move_on(dot,al,ar,dt)\n    plt.plot(x_set,y_set,\nlabel\n=\n'预设轨迹'\n,color='green')\n    plt.legend(\nloc\n=0)\n    plt.plot(l_x,l_y,\nlabel\n=\n'实际轨迹'\n,color='red')\n    plt.legend(\nloc\n=0)\n    plt.show()\n    plt.plot(x_set[1:],l_dif,\nlabel\n=\n'累积残差'\n)\n    plt.legend(\nloc\n=0)\n    plt.show()\nnew()\ntrue_x=[i/10 \nfor\n i \nin\n range(10)]\ntrue_y=[m.sin(i) \nfor\n i \nin\n true_x]\none_by_search(new(),true_x,true_y,1,10,0.1)\n\n#one_p_best(new(),[true_x[1],true_y[1]],1,100,0.1)\n\n", "Tag": "算法分析"}
{"Answer": "opencv或者numpy里面，parsings == idx，idx是一个数字，parsings 数组里面等于这个idx的数字全部是True，而不等于的就会变成False，parsings == idx这句话的返回值（假设返回值为parsings_return）和parsings 的维度是一样的，只不过数据由原来的parsings 的原始数据全部变成了True或者False。img[parsings == idx] = 128这句话就表示将parsings_return 中为True的位置（x,y,z）上的img中的值改成128.举个例子来说\nimport numpy as np\nparsings =np.array([0,1,2,3,3,4,5])\nidx=3\nimg=np.array([6,7,8,9,10,11,12])\nparsings_return=parsings==idx\nprint(parsings_return)\nimgs=img[parsings==idx]\nprint(imgs)\n\n\npython下面的bool不像c++下面可以使用0和非零表示的，他就是True和False.", "Konwledge_Point": "应对NP完全问题", "Question": "python中一个三维数组等于一个数值，怎么理解？\n  labels_to_mask = \n[0,14,15,16,18]\n\n    \nfor\n idx \nin\n labels_to_mask:\n        \nimg\n[parsings == idx]\n = \n128\n\n\n\n\nimg和parsings均是一个三维数组，parsing = np.array(parsing.getdata(), dtype=np.uint8).reshape(parsing.size[1], parsing.size[0], 3)\n谢谢！", "Tag": "算法分析"}
{"Answer": "next()函数实际上调用了传入函数的.__next()__成员函数。所以，如果传入的函数没有这个成员，则会报错参考代码理解\nimport torch\n# 生成一些测试数据\nX = torch.normal(0, 1, (1000, 2)) # x: sample size = 1000, feature_dim = 2\ny = torch.normal(0, 1, (1000, 1)) # y: sample size = 1000, dim = 1\n\n# 定义一个函数，返回dataloader\ndef load_array(data_and_label, batch_size, is_train=True):\n    \"\"\"Construct a PyTorch data iterator.\"\"\"\n    dataset = data.TensorDataset(*data_and_label)\n    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n\nbatch_size = 10\ndata_iter = load_array((X, y), batch_size)\n\n#这句会报错：next(data_iter)\nnext(iter(data_iter))\n\"\"\"\n输出前10组数据\n\"\"\"\n\n\n\n这里，为什么 next(data_iter) 报错，而 next(iter(data_iter)) 可以返回数据呢？这是因为，pytorch的DataLoader函数没有 next 成员，但有 iter 成员（见源文件）。所以，需要首先通过 iter() 函数返回一个 iter 成员，再找这个 iter 的 next", "Konwledge_Point": "应对NP完全问题", "Question": "关于pytorch中缺少next属性的报错\n在pytorch学习深度学习时，按照官方文档尝试简单的分类器，但在运行的时候出现报错：（\nhttps://github.com/zergtant/pytorch-handbook/blob/master/chapter1/4_cifar10_tutorial.ipynb\n）\n\n\n\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(\nroot\n=\n'./data'\n, \ntrain\n=\nTrue\n,\n                                        \ndownload\n=\nTrue\n, \ntransform\n=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, \nbatch_size\n=4,\n                                          \nshuffle\n=\nTrue\n, \nnum_workers\n=2)\n\ntestset = torchvision.datasets.CIFAR10(\nroot\n=\n'./data'\n, \ntrain\n=\nFalse\n,\n                                       \ndownload\n=\nTrue\n, \ntransform\n=transform)\ntestloader = torch.utils.data.DataLoader(testset, \nbatch_size\n=4,\n                                         \nshuffle\n=\nFalse\n, \nnum_workers\n=2)\n\nclasses = (\n'plane'\n, \n'car'\n, \n'bird'\n, \n'cat'\n,\n           \n'deer'\n, \n'dog'\n, \n'frog'\n, \n'horse'\n, \n'ship'\n, \n'truck'\n)\n\n\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n numpy \nas\n np\n\n# 展示图像的函数\n\n\ndef\n \nimshow\n(\nimg\n):\n    img = img / \n2\n + \n0.5\n     \n# unnormalize\n\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (\n1\n, \n2\n, \n0\n)))\n\n\n# 获取随机数据\n\ndataiter = \niter\n(trainloader)\nimages, labels = dataiter.\nnext\n()\n\n\n# 展示图像\n\nimshow(torchvision.utils.make_grid(images))\n\n# 显示图像标签\n\n\nprint\n(\n' '\n.join(\n'%5s'\n % classes[labels[j]] \nfor\n j \nin\n \nrange\n(\n4\n)))\n\n\n\n运行结果及报错内容：\n\n\nAttributeError                            Traceback (most recent \ncall\n last)\n\nInput\n \nIn\n [\n3\n], \nin\n ()\n     \n12\n # 获取随机数据\n     \n13\n dataiter = iter(trainloader)\n\n---> 14 images, labels = dataiter.next()\n\n     \n16\n # 展示图像\n     \n17\n imshow(torchvision.utils.make_grid(images))\n\nAttributeError: \n'_MultiProcessingDataLoaderIter'\n \nobject\n has \nno\n \nattribute\n \n'next'\n\n\n\n\n搜索发现\n\n\n_MultiProcessingDataLoaderIter作为DataLoader的iters，应该具有next属性啊\n\n\n请问是我环境安装出现什么问题了吗\n\n\n目前torch： 1.13.0+cu117    torchvision ： 0.14.0+cu117", "Tag": "算法分析"}
{"Answer": "第一是B，C第二是A,C第三是A,C第四是A,C", "Konwledge_Point": "应对NP完全问题", "Question": "求答python数据分析题卷，大lao们做一下，万分ganxie！\n二、 多选题 （共 4 题，20 分）\n1、给定多维数组 arr: arr =[[1 2 3] [4 5 6] [7 8 9]], 可以得到[[5 6] [8 9]]的操作是[2 分]（5.0）\nA、 arr[1:2,1:2] B、 arr[1:3,1:3] C、 arr[1:,1:] D、 arr[:,:] 正确答案： 解析：\n2、有 Series 结构数据 ser_obj = pd.Series([2,4,5,9,1,6]) ser_obj.index = ['a','b','c','d','e','f'] 能够获得[5,9,1]数据的操作是[2 分]（5.0）\nA、 print(ser_obj[2:5]) B、 print(ser_obj[1:6]) C、 print(ser_obj['c':'e']) D、 print(ser_obj['c':'f']) 正确答案： 解析：\n3、存在 Series 的结构数据 ser = pd.Series(['L', 'L', 'E', 'S','L']) ,将 ser 中的’L’替换为 None[2 分]（5.0）\nA、 ser=ser.replace({'L': None}) B、 ser=ser.replace({'L',None}) C、 ser= ser.replace(['L'],[None]) D、 ser= ser.replace('L',[None]) 正确答案： 解析：\n4、有 DataFrame 结构的 df_obj 数据,在水平方向上,对 df_obj 的每行进行后向填充,即水平方 向上进行后填充。 df_obj = pd.DataFrame(np.array([[1,3,5],[np.nan,np.nan,3],[2,np.nan,np.nan],[np.nan,2,np.na n]]))[2 分]（5.0）\nA、 df_obj.bfill(axis=1) B、 df_obj.bfill() C、 df_obj.fillna(method=\"bfill\",axis=1) D、 df_obj.fillna(method=\"bfill\") 正确答案： 解析：", "Tag": "算法分析"}
{"Answer": "np.sin 和plt.show 是函数后面少了()应该是这样\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.rcParams['axes.unicode_minus'] = False\n\nx=np.linspace(0,6)\ny = (np.sin(x-2)**2) * (np.e**(-x**2))\n\nplt.plot(x,y)\nplt.show() #加上()\n\n\n如有帮助，请点击我的回答下方的【采纳该答案】按钮帮忙采纳下，谢谢!\n", "Konwledge_Point": "应对NP完全问题", "Question": "利用matplotlib画出函数图像\n大概是y=np.sin 那里出错了？f(x)函数的式子要怎么改？\n\n", "Tag": "算法分析"}
{"Answer": "说下个人看法，如与你的观点不符，以你的观点为主。python 不是比速度的， 而是解决问题的高效，即完成某些问题的解决方案时间比其他语言要好。其他库不知道，我觉得pandas库倒是挺快", "Konwledge_Point": "应对NP完全问题", "Question": "判断下列观点是否正确\n众所周知，python是一个简单且低效的语言。关于速度深深困扰着实战派，一下观点是我的个人观点，求指点下。（所有数据都至少十万级）\n1、while 慢得离谱，海豹表达式作用不大\n2、for慢得离谱，但比while快一些。能用map用map，能用np用np\n3、pytorch（cpu）速度比numpy慢", "Tag": "算法分析"}
{"Answer": "加一个返回值应该可以\n \ndef options():\n    parser = argparse.ArgumentParser(description='tpccNet: A Fully-Convolutional Network For Inlier Estimation (Training)')\n    parser.add_argument('--exp_name', type=str, default='exp_tpccnet', metavar='N',\n                        help='Name of the experiment')\n    \n    \n    # settings for on training\n \n    parser.add_argument('--seed', type=int, default=1234)\n    return parser\n", "Konwledge_Point": "应对NP完全问题", "Question": "请问我的代码哪里出问题了呢？\n在运行代码的过程中遇到报错：\nAttributeError: 'NoneType' object has no attribute 'seed'\n\n\n报错部分代码：\n\n\n\ndef options():\n    parser = argparse.ArgumentParser(\ndescription\n=\n'tpccNet: A Fully-Convolutional Network For Inlier Estimation (Training)'\n)\n    parser.add_argument(\n'--exp_name'\n, \ntype\n=str, \ndefault\n=\n'exp_tpccnet'\n, \nmetavar\n=\n'N'\n,\n                        \nhelp\n=\n'Name of the experiment'\n)\n    \n    \n    #\n settings \nfor\n on training\n \n    parser.add_argument(\n'--seed'\n, \ntype\n=int, \ndefault\n=1234)\n    \n    args = parser.parse_args()\n    return args\n\n\n\ndef main\n()\n:\n    args = options\n()\n\n    torch.backends.cudnn.deterministic = True\n    torch.manual\n_seed(\nargs\n.\nseed\n)\n                                  #此处报错\n    torch.cuda.manual\n_seed_all(\nargs\n.\nseed\n)\n\n    np.random.seed(args.seed)\n \n    boardio = \nSummaryWriter(\nlog_dir\n='\ncheckpoints\n/\n' + \nargs\n.\nexp_name\n)\n\n    \n_init_(\nargs\n)\n\n \n    textio = \nIOStream('\ncheckpoints\n/\n' + \nargs\n.\nexp_name\n + '\n/\nrun\n.\nlog\n')\n\n    textio.cprint(str(args))\n\n\n\n\n请问是什么原因呢？有没有什么解决方法？\n\n\n谢谢！", "Tag": "算法分析"}
{"Answer": "对于NumPy的ndarray对象来说，不同长度的数组是无法垂直堆叠的。除了补齐长度，似乎没有更好的方法。", "Konwledge_Point": "应对NP完全问题", "Question": "python如何沿列堆叠不同维度的矩阵\n我在创建不同长度RNA序列的独热编码，因为长度不同，所以不同序列生成的矩阵维度不同，但是我想沿列堆叠这些不同维度的矩阵，而这是np.vstack()函数不支持的，想问一下有什么方法可以解决这个问题吗？因为序列长度差异太大，所以我不想把短的序列补齐。\n\n\n\n这是我所用的代码：\n\n\n\n\n\nimport numpy as np\n\nseq = []\n\nwith open('rna.fa', 'r') as f:\n    for line in f:\n        if line[0] == '>':\n            tokens = line[1:].split()\n            sequence = tokens[-1].strip().upper()\n            seq.append(sequence)\n        else:\n            pass\nf.close()\n\nprint(len(seq))\n\ndef one_hot_encode(seq):\n    mapping = dict(zip(\"ACGTN\", range(5)))    \n    seq2 = [mapping[i] for i in sequence]\n    \n    return np.eye(5)[seq2]\n\nfor i in range(1,5110):\n    xi = one_hot_encode(seq[i])\n    X = np.vstack((x,seq[i]))", "Tag": "算法分析"}
{"Answer": "https://www.cnblogs.com/lzying/p/11364647.html", "Konwledge_Point": "应对NP完全问题", "Question": "基于Python Opencv 更改指定矩阵数组\n如何利用奇偶量化进行图像水印\n\n比如想要更改图片【1】【2】矩阵中的数字，将所有的1换为0，将所有的0换成1或者-1\n\n如何在下面调取出来这个【1】【2】矩阵\n\n\n\nimport numpy as np\nfrom scipy import ndimage\nimport cv2\nimport random\nimport os\n#量子化テーブル\nQ = np.array(((16, 11, 10, 16, 24, 40, 51, 61),\n                (12, 12, 14, 19, 26, 58, 60, 55),\n                (14, 13, 16, 24, 40, 57, 69, 56),\n                (14, 17, 22, 29, 51, 87, 80, 62),\n                (18, 22, 37, 56, 68, 109, 103, 77),\n                (24, 35, 55, 64, 81, 104, 113, 92),\n                (49, 64, 78, 87, 103, 121, 120, 101),\n                (72, 92, 95, 98, 112, 100, 103, 99)), dtype=np.float32)\n\ny = cv2.imread(r'C:\\Users\\Owner\\Desktop\\so\\sample.jpg', 0)\ndef psnr1(img1, img2):\n   mse = np.mean((img1/1.0 - img2/1.0) ** 2 )\n   if mse < 1.0e-10:\n      return 100\n   return 10 * math.log10(255.0**2/mse)\n\ndef get_FileSize(filePath):\n\n    fsize = os.path.getsize(filePath)\n    fsize = fsize/float(1024 * 1024)\n\n    return round(fsize, 2)\n\ny1 = y.astype(np.float32)\n# print(y1.dtype)\nm, n = y1.shape\nhdata = np.vsplit(y1,n/8) # 縦方向から8個にする\nfor i in range(0, n//8):\n        blockdata = np.hsplit(hdata[i],m/8) \n     #水平方向にも８にする\n        for j in range(0, m//8):\n            block = blockdata[j]\n            #print(\"block[{},{}] data \\n{}\".format(i,j,blockdata[j]))\n            Yb = cv2.dct(block.astype(np.float))\n            F1 = Yb * Q\n            F = F1 // Q \n            #print(\"block[{},{}] data\\n{}\".format(i,j,F))\n            iblock = cv2.idct(Yb)\n            #print(iblock)\nY = cv2.dct(y1)\n\nprint(Y.shape)\ncv2.imshow(\"Dct\",Y)\n\n\n\ny2 = cv2.idct(Y)\nprint(psnr1(y,y2))\nsize1 = get_FileSize(r\"C:\\Users\\Owner\\Desktop\\so\\sample.jpg\")\nprint(\"文件大小：%.2f MB\"%(size1))\n\n\nsize = get_FileSize(r\"C:\\Users\\Owner\\Desktop\\so\\sample1.jpg\")\nprint(\"文件大小：%.2f MB\"%(size))\n\nprint(size/size1 - 1)\ncv2.imshow(\"iDCT\",y2.astype(np.uint8))\ncv2.waitKey(0)\ncv2.imwrite(r'C:\\Users\\Owner\\Desktop\\so\\sample1.jpg', y2)\n", "Tag": "算法分析"}
{"Answer": "它是csv不是scv\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.io import loadmat\nfrom scipy.stats import norm\n \nssec_daily = pd.read_csv('1LShowData_SSEC_daily.csv',encoding='GB2312',usecols=[6])\n \np_daily = ssec_daily['收盘'].values()\nplt.plot(p_daily)\nplt.show()\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "读取文件出现问题，这个怎么解决啊\nimport\n numpy \nas\n np\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n pandas \nas\n pd\n\nfrom\n scipy.io \nimport\n loadmat\n\nfrom\n scipy.stats \nimport\n norm\n\nssec_daily = pd.read_scv(\n'1LShowData_SSEC_daily.csv'\n,encoding=\n'GB2312'\n,usecols=[\n6\n])\n\np_daily = ssec_daily[\n'收盘'\n].\nvalues\n()\nplt.plot(p_daily)\nplt.\nshow\n()\n\n\n\n显示：\n\n\nTraceback (most recent \ncall\n last):\n  File \"C:\\Users\\86135\\PycharmProjects\\pythonProject1\\pythonProject\\20220303-实验1.py\", \nline\n \n8\n, \nin\n \n    ssec_daily = pd.read_scv(\n'1LShowData_SSEC_daily.csv'\n,encoding=\n'GB2312'\n,usecols=[\n6\n])\n  File \"C:\\Users\\86135\\anaconda3\\envs\\pythonProject1\\lib\\site-packages\\pandas\\__init__.py\", \nline\n \n261\n, \nin\n __getattr__\n    \nraise\n AttributeError(f\"module 'pandas' has no attribute '{name}'\")\nAttributeError: module \n'pandas'\n has \nno\n \nattribute\n \n'read_scv'\n\n\n\n\n试过重新安装pandas，但没有用", "Tag": "算法分析"}
{"Answer": "就代码中词云图问题，一是代码中wordlist是一个生成器对象，需要遍历出来，文本分割是全分割，其中有很多标点符号和其他字符不是中文，需要对其进行筛选剔除。wl=' '.join([x for x in wordlist if x!='\\n' and x not in string.punctuation])，你需要先导入内置模块 import string,用于处理标点符号。二是要对字体路径进行指定。在WordCloud函数中指定font_path='simhei.ttf'，用于显示中文。\n ", "Konwledge_Point": "应对NP完全问题", "Question": "python词云出现KeyError问题\n做包含中英文的txt的词云。\n\n\n\n开始是出不来图，词云图的黑色背景中显示类似这样一行<0x00000021FA66>的乱码\n\n\n\n\n\n\n我以为是字体问题，就找到wordcloud，用simhei.ttf替换了自带的DroidSansMono.ttf，同时修改了.py文件\n\n\n\n但是还是没有变化，黑色背景图里还是上面那个样子\n\n\n\n于是开始逐个排查发现2个问题：\n\n\n\n1.\n\n\n\nword_list无法打印出东西，print（）也不行。（见下图）\n\n\n\n\n\n\n后来百度看别人代码发现加上 wl = \" \".join(wordlist) 可以解决问题，但是不知道什么原理\n\n\n\n2.继续排查发现KeyError问题，但是我代码里好像没用字典，不知道是否是跟内置函数有冲突\n\n\n\n\n\n\n\n\n\n\n\n\n于是我就去.py找到了57行这个bigram，发现是一个分词函数但是我不知道这行代码是什么意思，他运行到第一个字“被”，就出现问题了，我该如何修改才能生成正确词频词云图？谢谢！\n\n\n\n \n\n\n\n最后附一下全的代码：\n\n\n\n\nimport os\n\nimport re\n\nimport time\n\nimport random\n\n\n\nimport requests\n\nimport jieba\n\nimport numpy as np\n\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\n\nfrom wordcloud import WordCloud\n\n\n\n# 生成Session对象，用于保存Cookie\n\ns = requests.Session()\n\n# 词云形状图片\n\nWC_MASK_IMG = 'K:/test.jpg'\n\n# 影评数据保存文件\n\nCOMMENTS_FILE_PATH = 'K:/douban_comments.txt'\n\n# 词云字体\n\nWC_FONT_PATH = '/Library/Fonts/Songti.ttc'\n\n# 统计词频\n\nfrom collections import defaultdict \n\n\n\ndef login_douban():\n\n    \"\"\"\n\n    登录豆瓣\n\n    :return:\n\n    \"\"\"\n\n    # 登录URL\n\n    login_url = 'https://accounts.douban.com/j/mobile/login/basic'\n\n    # 请求头\n\n    headers = {'user-agent': 'Mozilla/5.0', 'Referer': 'https://accounts.douban.com/passport/login?source=main'}\n\n    # 传递用户名和密码\n\n    data = {'name': '你的账号',\n\n            'password': '你的密码',\n\n            'remember': 'false'}\n\n    try:\n\n        r = s.post(login_url, headers=headers, data=data)\n\n        r.raise_for_status()\n\n    except:\n\n        print('登录请求失败')\n\n        return 0\n\n    # 打印请求结果\n\n    print(r.text)\n\n    return 1\n\n\n\n\ndef spider_comment(page=0):\n\n    \"\"\"\n\n    爬取某页影评\n\n    :param page: 分页参数\n\n    :return:\n\n    \"\"\"\n\n    print('开始爬取第%d页' % int(page))\n\n    start = int(page * 20)\n\n    comment_url = 'https://movie.douban.com/subject/3011091/comments?start=%d&limit=20&sort=new_score&status=P' % start\n\n    # 请求头\n\n    headers = {'user-agent': 'Mozilla/5.0'}\n\n    try:\n\n        r = s.get(comment_url, headers=headers)\n\n        r.raise_for_status()\n\n    except:\n\n        print('第%d页爬取请求失败' % page)\n\n        return 0\n\n    # 使用正则提取影评内容\n\n    comments = re.findall('(.*)', r.text)\n\n    if not comments:\n\n        return 0\n\n    # 写入文件\n\n    with open(COMMENTS_FILE_PATH, 'a+', encoding='utf-8') as file:\n\n        file.writelines('\\n'.join(comments))\n\n    return 1\n\n\n\n\ndef batch_spider_comment():\n\n    \"\"\"\n\n    批量爬取豆瓣影评\n\n    :return:\n\n    \"\"\"\n\n    # 写入数据前先清空之前的数据\n\n    if os.path.exists(COMMENTS_FILE_PATH):\n\n        os.remove(COMMENTS_FILE_PATH)\n\n    page = 0\n\n    while spider_comment(page):\n\n        page += 1\n\n        # 模拟用户浏览，设置一个爬虫间隔，防止ip被封\n\n        time.sleep(random.random() * 3)\n\n    print('爬取完毕')\n\n\n\n\ndef cut_word():\n\n    \"\"\"\n\n    对数据分词\n\n    :return: 分词后的数据\n\n    \"\"\"\n\n    with open(COMMENTS_FILE_PATH, encoding='utf-8') as file:\n\n        comment_txt = file.read()\n\n        wordlist = jieba.cut(comment_txt, cut_all=True)\n\n        wl = \" \".join(wordlist)\n\n        print(wl)\n\n        return wl\n\n\n\n\ndef create_word_cloud():\n\n    \"\"\"\n\n    生成词云\n\n    :return:\n\n    \"\"\"\n\n    \n\n    # 设置词云形状图片\n\n    wc_mask = np.array(Image.open(WC_MASK_IMG))\n\n    # 数据清洗词列表\n\n    stop_words = ['就是', '不是', '但是', '还是', '只是', '这样', '这个', '一个', '什么', '电影', '没有']\n\n    # 设置词云的一些配置，如：字体，背景色，词云形状，大小\n\n    wc = WordCloud(background_color=\"white\", max_words=50, mask=wc_mask, scale=4,\n\n                   max_font_size=50, random_state=42, stopwords=stop_words, font_path=WC_FONT_PATH)\n\n    # 生成词云\n\n    wc.generate(cut_word())\n\n\n\n    # 在只设置mask的情况下,你将会得到一个拥有图片形状的词云\n\n    plt.imshow(wc, interpolation=\"bilinear\")\n\n    plt.axis(\"off\")\n\n    plt.figure()\n\n    plt.show()\n\n\n\n    \n\nif __name__ == '__main__':\n\n    # 登录成功才爬取\n\n    # if login_douban():\n\n    #     # spider_comment(30)\n\n    batch_spider_comment()\n\n    create_word_cloud()", "Tag": "算法分析"}
{"Answer": "后面不是有个 ->吗，它定义了返回类型，你可以点进去看看，不过直接看下面return的是什么就行。对象.属性 这就已经得到一个返回值了，如果结果是如int类型这样的数字123，那肯定123()不行，但这里返回的是_LocIndexer处理实例化后的对象，那它后面是能加括号的。 希望能帮助到你，谢谢。", "Konwledge_Point": "应对NP完全问题", "Question": "pandas里面loc方法是属性方法有@property装饰器，但是为什么df.loc()系统不报  \" is not callable\"的错？\n代码如下;\n\n\nimport\n pandas \nas\n pd\n\nimport\n numpy \nas\n np\ndata = {\n'name'\n: [\n'Joe'\n, \n'Mike'\n, \n'Jack'\n, \n'Rose'\n, \n'David'\n, \n'Marry'\n, \n'Wansi'\n, \n'Sidy'\n, \n'Jason'\n, \n'Even'\n],\n        \n'age'\n: [\n25\n, \n32\n, \n18\n, np.\nnan\n, \n15\n, \n20\n, \n41\n, np.\nnan\n, \n37\n, \n32\n],\n        \n'gender'\n: [\n1\n, \n0\n, \n1\n, \n1\n, \n0\n, \n1\n, \n0\n, \n0\n, \n1\n, \n0\n],\n        \n'isMarried'\n: [\n'yes'\n, \n'yes'\n, \n'no'\n, \n'yes'\n, \n'no'\n, \n'no'\n, \n'no'\n, \n'yes'\n, \n'no'\n, \n'no'\n]}\n\nlabels = [\n'name'\n, \n'age'\n, \n'c'\n, \n'd'\n, \n'e'\n, \n'f'\n, \n'g'\n, \n'h'\n, \n'i'\n, \n'j'\n]\ndf = pd.DataFrame(data, \nindex\n=labels)\ndf.loc()\n\n\n\n按住CTRL鼠标点击loc，定位到loc函数，发现loc是属性方法，带有\n@property装饰器如图所示：\n\n\n\n\n问题：\n既然loc是属性方法，那代码中 \ndf.loc()\n应该报错 如“”\n* is not callable“才对吧？为什么系统不会报错？", "Tag": "算法分析"}
{"Answer": "是不是在Pycharm中启动的Jupyter，我也是这样的，当我使用Anaconda启动Jupyter进行交互式绘图不会出现这样的情况。", "Konwledge_Point": "应对NP完全问题", "Question": "Jupyter 使用%matplotlib notebook 进行交互式绘图时异常显示?\n当我在Jupyter中进行交互式绘图时，绘图窗口显示了，但只有左上角有显示。\n\n\n\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.plot(np.random.randn(50).cumsum())\n\n\n\n\n\n\n\n对此，我感到很疑惑，在我使用其他的命令绘图时并不会出现这样的错误。\n\n\n\n大佬们能帮帮我吗T_T", "Tag": "算法分析"}
{"Answer": "看下是不是这个问题\n成功解决TypeError: 'float' object cannot be interpreted as an integer_一个处女座的程序猿-CSDN博客\n成功解决TypeError: 'float' object cannot be interpreted as an integer目录解决问题解决思路解决方法解决问题TypeError: 'float' object cannot be interpreted as an integer解决思路类型错误：“...\n\n\n\nhttps://blog.csdn.net/qq_41185868/article/details/100942855?utm_source=app&app_version=4.16.0\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "TypeError: 'float' object cannot be interpreted as an integer这个是哪儿有问题呀\n(IPdb [4]): runfile('E:/EDEM/Matrial Calculate/Static angle of repose test/Static_angle_of_repose_analyst.py', wdir='E:/EDEM/Matrial Calculate/Static angle of repose test')\n\n\nLoading: Static_angle_of_repose_example.dem\n\n\n\n\nProcessing: Static_angle_of_repose_example.dem\n\n\nTraceback (most recent call last):\n\n\n  File \"E:\\EDEM\\Matrial Calculate\\Static angle of repose test\\Static_angle_of_repose_analyst.py\", line 62, in \n    spacing=np.linspace(top_rad,base_rad,(base_rad-top_rad)/bin_size)\n\n\n  File \"<__array_function__ internals>\", line 5, in linspace\n\n\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\function_base.py\", line 120, in linspace\n    num = operator.index(num)\n\n\nTypeError: 'float' object cannot be interpreted as an integer", "Tag": "算法分析"}
{"Answer": "你输入的 Tensor 是需要 rank >= 1 的。也就是，Tensor 的维度应该>=1。代码中看来，你的 y_train 是一个秩为0的 Tensor，所以需要增加维度。\n可以通过以下代码解决：\n\ny_train = y_train[..., tf.newaxis]\ny_test = y_test[..., tf.newaxis]\n\n\n这样就不会再报错了。还有，你需要对 y_test 做同样的处理。\n而语法可能会随着 TensorFlow 的版本更新而变化。你要看看正在使用与你看到的教程相同版本的 TensorFlow是否相同。", "Konwledge_Point": "应对NP完全问题", "Question": "tensotflow中from_tensor_slices报错\n在学习单层神经网络时使用from_tensor_slices，发生错误\n\n\n报错ValueError: Unbatching a tensor is only supported for rank >= 1\n\n\n\n\nimport tensorflow as tf\nfrom sklearn import datasets\nfrom pandas import DataFrame\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nx_data = datasets\n.load_iris\n()\n.data\n\ny_data = datasets\n.load_iris\n()\n.target\n\nnp\n.random\n.seed\n(\n116\n)\nnp\n.random\n.shuffle\n(x_data)\nnp\n.random\n.seed\n(\n116\n)\nnp\n.random\n.shuffle\n(y_data)\ntf\n.random\n.set_seed\n(\n116\n)\n\nx_train = x_data\n[:-30]\n\ny_train = y_data\n[:-30]\n\nx_test = x_data\n[-30:]\n\ny_test = y_data\n[-30]\n\nx_train = tf\n.cast\n(x_train, dtype=tf.float32)\nx_test = tf\n.cast\n(x_test, dtype=tf.float32)\n\ntrain_data = tf\n.data\n.Dataset\n.from_tensor_slices\n((x_train, y_train))\n.batch\n(\n32\n)\ntest_data = tf\n.data\n.Dataset\n.from_tensor_slices\n((x_test, y_test))\n.batch\n(\n32\n)\n\n\n\n\n运行结果及详细报错内容\n\n\n在这一行：\n\n\ntrain_data\n = tf.\ndata\n.\nDataset\n.from_tensor_slices((\nx_train\n, \ny_train\n)).batch(32)\n\n\n\n\n\n报错ValueError: Unbatching a tensor is only supported for rank >= 1\n\n\n尝试去掉括号里的y_train再加上，train_data一行不再报错，但test_data一行无变化\n\n\n这一语法是否随版本更新而变化？我看的教程里用的是Python3.7，但我装的是3.10", "Tag": "算法分析"}
{"Answer": "先给个基础的例子：\nfrom cProfile import label\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n \nplt.figure(figsize=(30,30),dpi=80)\nplt.rcParams['font.sans-serif'] = ['SimHei']\nplt.rcParams['axes.unicode_minus'] = False\n \nplt.title('资产负债率',fontsize=16)\ndf=pd.read_excel('源数据.xlsx')\nx=df[(df['股票代码']==1)]['截止日期']\ny1=df[(df['股票代码']==1)]['资产负债率']\ny2=df[(df['股票代码']==2)]['资产负债率']\nplt.xlabel('截止日期')\nplt.ylabel('资产负债率')\nplt.plot(x,y1,'r',label='000001')\nplt.plot(x,y2,'b',label='000002')\nplt.show()\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "关于matplotlib导入excel表格数据的代码问题\n代码的要求为：\n使用pandas的DataFrame进行数据绘图，形成一个画布1个子图。000001公司为红色线条，000002为蓝色线条，时间范围从2000年6月，到2019年12月。子图是关于“资产负债率”的折线图。\n\n\n我自己尝试写的代码：\n\n\nimport matplotlib\n.pyplot\n as plt\nimport numpy as np\nimport pandas as pd\n\nplt\n.figure\n(figsize=(\n30\n,\n30\n),dpi=\n80\n)\n\nplt\n.rcParams\n[\n'font.sans-serif'\n]\n = \n[\n'SimHei'\n]\n\nplt\n.rcParams\n[\n'axes.unicode_minus'\n]\n = False\n\nplt\n.subplot\n(\n2\n,\n2\n,\n1\n)\nplt\n.title\n(\n'资产负债率'\n,fontsize=\n10\n)\ndf=pd\n.read_excel\n(\n'F:\\\\源数据.xlsx'\n)\nx=df\n[\n'截止日期'\n]\n\ny1=df\n.loc\n[(df[\n'资产负债率'\n]\n) & (df\n[\n'股票代码'\n]\n == \n'000001'\n)]\ny2=df\n.loc\n[(df[\n'资产负债率'\n]\n) & (df\n[\n'股票代码'\n]\n == \n'000002'\n)]\nplt\n.xlabel\n(\n'截止日期'\n)\nplt\n.ylabel\n(\n'资产负债率'\n)\n\nplt\n.plot\n(df\n[\n\"x\"\n]\n,df\n[\n\"y1\"\n]\n)\nplt\n.plot\n(df\n[\n\"x\"\n]\n,df\n[\n\"y2\"\n]\n)\n\n\n\n\n运行结果及报错内容 ：\n\n\n\n\n\n\n希望能够完整按照要求生成结果", "Tag": "算法分析"}
{"Answer": "语法错误，第10行df['语文']，，df['英语']多了个逗号", "Konwledge_Point": "应对NP完全问题", "Question": "excel的数据通过pyecharts生成柱型图表,图中需要显示EXCEL中多个列数据\n现在有一学生各位成绩表格，需要通过pyecharts生成柱型图表,图中需要显示EXCEL中各科成绩列数据\n\n\n\n\nimport\n numpy \nas\n np\n\nimport\n pandas \nas\n pd\n\nfrom\n pyecharts.charts \nimport\n Bar\n\nfilepath = \nr'/Users/allen/Downloads/my1stCharts.html'\n\ndf = pd.read_excel(\n\"/Users/allen/Downloads/Testresults1.xlsx\"\n, sheet_name=\n\"Sheet1\"\n, header=\n0\n)\ndf.head()\n\nx = np.array(df[\n'月份'\n])\ny = np.array(df[\n'数学'\n]，df[\n'语文'\n]，，df[\n'英语'\n]，df[\n'科学'\n]，df[\n'地理'\n])\n\nbar = Bar(\n'成绩'\n,\n'每月情况'\n)\nbar.add(\n'X学生数学情况'\n,x,y)\nbar\n\n", "Tag": "算法分析"}
{"Answer": "在第11行，两个列表可能有一个为空，也有可能都是空你可以看看11行前面有什么错误，或者看看你的csv文件有什么错误", "Konwledge_Point": "应对NP完全问题", "Question": "zhouqi[0][0]=data1[0][5] IndexError: index 0 is out of bounds for axis 0 with size 0\n\nimport numpy as np\nimport pandas as pd\ndata1 = pd\n.read_csv\n(\n'采暖制冷用户-采暖_日.csv'\n, encoding = \n'gbk'\n)\ndata1 = pd\n.DataFrame\n(data1)\ndata1 = np\n.array\n(data1)\nj=\n0\n\nzhouqi =\n[[]\n]\nzhouqi = pd\n.DataFrame\n(zhouqi)\nzhouqi = np\n.array\n(zhouqi)\nzhouqi\n[0]\n[0]\n=data1\n[0]\n[5]\n\n\nfor\n \ni\n \nin\n range(data1\n.shape\n[0]\n-\n1\n):\n    \nif\n data1\n[i]\n[1]\n == data1\n[i+1]\n[1]\n:\n        zhouqi\n[j]\n[1]\n=data1\n[i+1]\n[5]\n\n    \nelse\n:\n        zhouqi\n[j+1]\n[0]\n=data1\n[i+1]\n[5]\n\n        j=j+\n1\n\n", "Tag": "算法分析"}
{"Answer": "def test(args, io):\n    all_true_cls = []\n    all_pred_cls = []\n    all_true_seg = []\n    all_pred_seg = []\n    for test_area in range(1,7):\n        visual_file_index = 0\n        test_area = str(test_area)\n        if os.path.exists(\"data/indoor3d_sem_seg_hdf5_data_test/room_filelist.txt\"):\n            with open(\"data/indoor3d_sem_seg_hdf5_data_test/room_filelist.txt\") as f:\n                for line in f:\n                    if (line[5]) == test_area:\n                        break\n                    visual_file_index = visual_file_index + 1\n        if (args.test_area == 'all') or (test_area == args.test_area):\n            test_loader = DataLoader(S3DIS(partition='test', num_points=args.num_points, test_area=test_area),\n                                     batch_size=args.test_batch_size, shuffle=False, drop_last=False)\n\n            device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n                        \n            #Try to load models\n            semseg_colors = test_loader.dataset.semseg_colors\n            if args.model == 'dgcnn':\n                model = DGCNN_semseg(args).to(device)\n            else:\n                raise Exception(\"Not implemented\")\n                \n            model = nn.DataParallel(model)\n            model.load_state_dict(torch.load(os.path.join(args.model_root, 'model_%s.t7' % test_area)))\n            model = model.eval()\n            test_acc = 0.0\n            count = 0.0\n            test_true_cls = []\n            test_pred_cls = []\n            test_true_seg = []\n            test_pred_seg = []\n            with torch.no_grad():\n                for data, seg in test_loader:\n                    data, seg = data.to(device), seg.to(device)\n                    data = data.permute(0, 2, 1)\n                    batch_size = data.size()[0]\n                    seg_pred = model(data)\n                    seg_pred = seg_pred.permute(0, 2, 1).contiguous()\n                    pred = seg_pred.max(dim=2)[1]\n                    seg_np = seg.cpu().numpy()\n                    pred_np = pred.detach().cpu().numpy()\n                    test_true_cls.append(seg_np.reshape(-1))\n                    test_pred_cls.append(pred_np.reshape(-1))\n                    test_true_seg.append(seg_np)\n                    test_pred_seg.append(pred_np)\n                    # visiualization\n                    visualization(args.visu, args.visu_format, args.test_area, data, seg, pred, visual_file_index, semseg_colors)\n                    visual_file_index = visual_file_index + data.shape[0]\n                if visual_warning and args.visu != '':\n                    print('Visualization Failed: You can only choose a room to visualize within the scope of the test area')\n                test_true_cls = np.concatenate(test_true_cls)\n                test_pred_cls = np.concatenate(test_pred_cls)\n                test_acc = metrics.accuracy_score(test_true_cls, test_pred_cls)\n                avg_per_class_acc = metrics.balanced_accuracy_score(test_true_cls, test_pred_cls)\n                test_true_seg = np.concatenate(test_true_seg, axis=0)\n                test_pred_seg = np.concatenate(test_pred_seg, axis=0)\n                test_ious = calculate_sem_IoU(test_pred_seg, test_true_seg)\n                outstr = 'Test :: test area: %s, test acc: %.6f, test avg acc: %.6f, test iou: %.6f' % (test_area,\n                                                                                                        test_acc,\n                                                                                                        avg_per_class_acc,\n                                                                                                        np.mean(test_ious))\n                io.cprint(outstr)]\n#变动地方\n    all_true_cls = np.append(all_true_cls, test_true_cls)\n    all_pred_cls = np.append(all_pred_cls, test_pred_cls)\n    all_true_seg = np.append(all_true_seg, test_true_seg)\n    all_pred_seg = np.append(all_pred_seg, test_pred_seg)\n\n        if args.test_area == 'all':\n            all_true_cls = np.concatenate(all_true_cls)\n            all_pred_cls = np.concatenate(all_pred_cls)\n            all_acc = metrics.accuracy_score(all_true_cls, all_pred_cls)\n            avg_per_class_acc = metrics.balanced_accuracy_score(all_true_cls, all_pred_cls)\n            all_true_seg = np.concatenate(all_true_seg, axis=0)\n            all_pred_seg = np.concatenate(all_pred_seg, axis=0)\n            all_ious = calculate_sem_IoU(all_pred_seg, all_true_seg)\n            outstr = 'Overall Test :: test acc: %.6f, test avg acc: %.6f, test iou: %.6f' % (all_acc,\n                                                                                             avg_per_class_acc,\n                                                                                             np.mean(all_ious))\n            io.cprint(outstr)\n", "Konwledge_Point": "应对NP完全问题", "Question": "DGCNN.pytorch在S3DIS上运行错误，如何解决？\n报错如下:\n\n\n> Namespace(\nbatch_size\n=32, \ndataset\n=\n'S3DIS'\n, \ndropout\n=0.5, \nemb_dims\n=1024, \nepochs\n=100, \neval\n=\nTrue\n, \nexp_name\n=\n'semseg_eval'\n, \nk\n=20, \nlr\n=0.001, \nmodel\n=\n'dgcnn'\n, \nmodel_root\n=\n'outputs/semseg_6/models/'\n, \nmomentum\n=0.9, \nno_cuda\n=\nFalse\n, \nnum_points\n=4096, \nscheduler\n=\n'cos'\n,\n \nseed\n=1, \ntest_area\n=\n'all'\n, \ntest_batch_size\n=16, \nuse_sgd\n=\nTrue\n, \nvisu\n=\n''\n, \nvisu_format\n=\n'ply'\n)\nUsing GPU : 0 \nfrom\n 1 devices\nTest :: test area: 1, test acc: 0.893883, test avg acc: 0.805912, test iou: 0.702250\nOverall Test :: test acc: 0.893883, test avg acc: 0.805912, test iou: 0.702250\nTest :: test area: 2, test acc: 0.833375, test avg acc: 0.557771, test iou: 0.444156\nTraceback (most recent call last):\n  File \n\"main_semseg.py\"\n, line 454, \nin\n \n    test(args, io)\n  File \n\"main_semseg.py\"\n, line 369, \nin\n test\n    all_true_cls.append(test_true_cls)\nAttributeError: \n'numpy.ndarray'\n object has \nno\n attribute \n'append'\n\n\n\n\n代码如下：\n\n\ndef test(args, io):\n    all_true_cls = []\n    all_pred_cls = []\n    all_true_seg = []\n    all_pred_seg = []\n    for test_area in \nrange\n(\n1\n, \n7\n):\n        visual_file_index = \n0\n\n        test_area = \nstr\n(test_area)\n        if os.path.\nexists\n(\n\"data/indoor3d_sem_seg_hdf5_data_test/room_filelist.txt\"\n):\n            with \nopen\n(\n\"data/indoor3d_sem_seg_hdf5_data_test/room_filelist.txt\"\n) as f:\n                for line in f:\n                    if (line[\n5\n]) == test_area:\n                        break\n                    visual_file_index = visual_file_index + \n1\n\n        if (args.test_area == \n'all'\n) or (test_area == args.test_area):\n            test_loader = \nDataLoader\n(\nS3DIS\n(partition=\n'test'\n, num_points=args.num_points, test_area=test_area),\n                                     batch_size=args.test_batch_size, shuffle=False, drop_last=False)\n\n            device = torch.\ndevice\n(\n\"cuda\"\n if args.cuda else \n\"cpu\"\n)\n\n            # Try to load models\n            semseg_colors = test_loader.dataset.semseg_colors\n            if args.model == \n'dgcnn'\n:\n                model = \nDGCNN_semseg\n(args).\nto\n(device)\n            else:\n                raise \nException\n(\n\"Not implemented\"\n)\n\n            model = nn.\nDataParallel\n(model)\n            model.\nload_state_dict\n(torch.\nload\n(os.path.\njoin\n(args.model_root, \n'model_%s.t7'\n % test_area)))\n            model = model.\neval\n()\n            test_acc = \n0.0\n\n            count = \n0.0\n\n            test_true_cls = []\n            test_pred_cls = []\n            test_true_seg = []\n            test_pred_seg = []\n            with torch.\nno_grad\n():\n                for data, seg in test_loader:\n                    data, seg = data.\nto\n(device), seg.\nto\n(device)\n                    data = data.\npermute\n(\n0\n, \n2\n, \n1\n)\n                    batch_size = data.\nsize\n()[\n0\n]\n                    seg_pred = \nmodel\n(data)\n                    seg_pred = seg_pred.\npermute\n(\n0\n, \n2\n, \n1\n).\ncontiguous\n()\n                    pred = seg_pred.\nmax\n(dim=\n2\n)[\n1\n]\n                    seg_np = seg.\ncpu\n().\nnumpy\n()\n                    pred_np = pred.\ndetach\n().\ncpu\n().\nnumpy\n()\n                    test_true_cls.\nappend\n(seg_np.\nreshape\n(-\n1\n))\n                    test_pred_cls.\nappend\n(pred_np.\nreshape\n(-\n1\n))\n                    test_true_seg.\nappend\n(seg_np)\n                    test_pred_seg.\nappend\n(pred_np)\n                    # visiualization\n                    \nvisualization\n(args.visu, args.visu_format, args.test_area, data, seg, pred, visual_file_index,\n                                  semseg_colors)\n                    visual_file_index = visual_file_index + data.shape[\n0\n]\n                if visual_warning and args.visu != \n''\n:\n                    \nprint\n(\n                        \n'Visualization Failed: You can only choose a room to visualize within the scope of the test area'\n)\n                test_true_cls = np.\nconcatenate\n(test_true_cls)\n                test_pred_cls = np.\nconcatenate\n(test_pred_cls)\n                test_acc = metrics.\naccuracy_score\n(test_true_cls, test_pred_cls)\n                avg_per_class_acc = metrics.\nbalanced_accuracy_score\n(test_true_cls, test_pred_cls)\n                test_true_seg = np.\nconcatenate\n(test_true_seg, axis=\n0\n)\n                test_pred_seg = np.\nconcatenate\n(test_pred_seg, axis=\n0\n)\n                test_ious = \ncalculate_sem_IoU\n(test_pred_seg, test_true_seg)\n                outstr = \n'Test :: test area: %s, test acc: %.6f, test avg acc: %.6f, test iou: %.6f'\n % (test_area,\n                                                                                                        test_acc,\n                                                                                                        avg_per_class_acc,\n                                                                                                        np.\nmean\n(\n                                                                                                            test_ious))\n                io.\ncprint\n(outstr)\n                all_true_cls.\nappend\n(test_true_cls)\n                all_pred_cls.\nappend\n(test_pred_cls)\n                all_true_seg.\nappend\n(test_true_seg)\n                all_pred_seg.\nappend\n(test_pred_seg)\n\n        if args.test_area == \n'all'\n:\n            all_true_cls = np.\nconcatenate\n(all_true_cls)\n            all_pred_cls = np.\nconcatenate\n(all_pred_cls)\n            all_acc = metrics.\naccuracy_score\n(all_true_cls, all_pred_cls)\n            avg_per_class_acc = metrics.\nbalanced_accuracy_score\n(all_true_cls, all_pred_cls)\n            all_true_seg = np.\nconcatenate\n(all_true_seg, axis=\n0\n)\n            all_pred_seg = np.\nconcatenate\n(all_pred_seg, axis=\n0\n)\n            all_ious = \ncalculate_sem_IoU\n(all_pred_seg, all_true_seg)\n            outstr = \n'Overall Test :: test acc: %.6f, test avg acc: %.6f, test iou: %.6f'\n % (all_acc,\n                                                                                             avg_per_class_acc,\n                                                                                             np.\nmean\n(all_ious))\n            io.\ncprint\n(outstr)\n\n\n\n请问该如何修改呢？", "Tag": "算法分析"}
{"Answer": "请在你的python里执行以下代码之后再进行后续计算：\n\nimport os\nos.system('pip install numpy')\n", "Konwledge_Point": "应对NP完全问题", "Question": "pathon import 模块名\n\n\n\n\n刚刚\n学到这里的时候不大清楚是该提前创建np吗\n感觉这块不对后面from import 也是错的", "Tag": "算法分析"}
{"Answer": "x=nn.Conv2d(1, 256, kernel_size=7, stride=2, padding=3, bias=False)(input1)请问(input1)是什么？python语法中好似没有这样的写法", "Konwledge_Point": "应对NP完全问题", "Question": "conv2d(): argument 'input' (position 1) must be Tensor, not Tensor，不知道怎么解决。\nimport torch\nimport torch.nn as nn\n\nfrom\n tensorflow import keras\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom\n torch.autograd import Variable\nimport tensorflow as tf\n\nfrom\n tensorflow import keras\nimport matplotlib.pyplot as plt\nimport paddle\nimport warnings\n\nfrom\n paddle.metric import Accuracy\n\n\n# 训练数据\n\n\nfeature_name1\n=\n\"J:\\\\multi-scale\\\\data_test\\\\data_33.txt\"\n\n\nfeature_data1\n=np.loadtxt(feature_name1)\n\nfeature_data1\n=np.int_(feature_data1)\n\ntrain_X1\n=np.reshape(feature_data1,(100,32,32,1))\ntrain_X1 = torch.tensor(train_X1)\n\nprint\n(train_X1.shape)\n\nfeature_name2\n=\n\"J:\\\\multi-scale\\\\data_test\\\\data_17.txt\"\n\n\nfeature_data2\n=np.loadtxt(feature_name2)\n\nfeature_data2\n=np.int_(feature_data2)\n\ntrain_X2\n=np.reshape(feature_data2,(100,16,16,1))\n\ntrain_X2 = torch.tensor(train_X2)\n\nprint\n(train_X2.shape)\n\n\ntag_name\n=\n\"J:\\\\multi-scale\\\\data_test\\\\ch_lable_33.txt\"\n\n\ntag_data\n=np.loadtxt(tag_name)\ntrain_Y = keras.utils.to_categorical(tag_data, \nnum_classes\n=8)\n\nprint\n(train_Y.shape)\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, \nstride\n=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, \nkernel_size\n=1, \nbias\n=\nFalse\n)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, \nkernel_size\n=3, \nstride\n=stride, \npadding\n=1, \nbias\n=\nFalse\n)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, \nkernel_size\n=1, \nbias\n=\nFalse\n)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        \nif\n stride != 1 \nor\n in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, \nkernel_size\n=1, \nstride\n=stride, \nbias\n=\nFalse\n),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\ndef _make_layer(in_planes, block, planes, num_blocks, stride):\n    strides = [stride] + [1]*(num_blocks-1)\n    layers = []\n    \nfor\n stride \nin\n strides:\n        layers.append(block(in_planes, planes, stride))\n        in_planes = planes * block.expansion\n    return nn.Sequential(*layers)\n\n\n#定义网络\n\ninput1 = keras.Input(shape=(32, 32,1))\ninput2 = keras.Input(shape=(16, 16,1))\n\n\n# Stage 1\n\n\nx\n=nn.Conv2d(1, 256, \nkernel_size\n=7, \nstride\n=2, \npadding\n=3, \nbias\n=\nFalse\n)(input1)\n\nx\n=nn.BatchNorm2d(64)(x)\nc1 = F.relu(x)\nc1 =x= F.max_pool2d(c1, \nkernel_size\n=3, \nstride\n=2, \npadding\n=1)(x)\n\n# Stage 2\n\n\nc2\n=_make_layer(Bottleneck, 128, 2, \nstride\n=2)(c1)\n\nc3\n=_make_layer(Bottleneck, 128, 2, \nstride\n=2)(c2)\n\np2\n=nn.Conv2d( 512, 256, \nkernel_size\n=1, \nstride\n=1, \npadding\n=0)(input2)\n\n#p3=nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)(input3)\n\n\np2\n=nn.Conv2d( 256, 128, \nkernel_size\n=1, \nstride\n=1, \npadding\n=0)(p2)\np3= keras.layers.concatenate([c2, p2])\n\np3\n=keras.layers.Flatten()(p3)\np3 = keras.layers.Dense(64, \nactivation\n=\n'relu'\n)(p3)\noutput = keras.layers.Dense(8, \nactivation\n=\n'sigmoid'\n)(p3)\nmodel = keras.Model(inputs=[input1, input2], \noutputs\n=output)\n\nmodel.summary()\nmodel.compile(\noptimizer\n=\n'adam'\n,\n              \nloss\n=\n'sparse_categorical_crossentropy'\n,\n              metrics=[\n'accuracy'\n])\n\nhistory = model.fit([train_X1,train_X2],\n                    train_Y,\n                    \nepochs\n=30, \nvalidation_split\n=0.3)\n\n\n\n错误提示：\n\n", "Tag": "算法分析"}
{"Answer": "你是在这里看的吗 https://blog.csdn.net/m0_37393514/article/details/81010587?utm_source=app&app_version=5.3.1", "Konwledge_Point": "应对NP完全问题", "Question": "tensorflow导入自己的数据集\n问题遇到的现象和发生背景\n\n\n修改以下代码，导入自己的数据集（train和test文件，目录下各有十类图像），之后的训练要用到：train_images,train_labels,test_images,test_labels\n\n\n\n\nfashion_mnist=tf.keras.datasets.fashion_minst\n(train_images,train_labels),(test_images,test_labels)=fashion_minst.load_data()\nclass_names={\"T-shirt/top\",'Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot'}\ntrain_images=np.expend_dims(train_images,-1)/255\ntest_images=np.expend_dims(train_images,-1)/255\n\n\n请教各位!万分感谢！", "Tag": "算法分析"}
{"Answer": "我测试代码是能运行和用filter处理的啊\n", "Konwledge_Point": "应对NP完全问题", "Question": "不清楚这个为何不成功\n\n\nimport\n numpy \nas\n np\n\nimport\n cv2\n\n\nfrom\n scipy \nimport\n misc, ndimage, signal\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n numpy \nas\n np\n\nfrom\n PIL \nimport\n Image\n\n\n\ndef\n \nplot_two\n(\nleft, right\n):\n    fig = plt.figure()\n    ax1 = fig.add_subplot(\n121\n)\n    ax2 = fig.add_subplot(\n122\n)\n    ax1.imshow(left)\n    ax2.imshow(right)\n    plt.show()\n\n\n\ndef\n \ncv_2d_conv_lower_pass_filer\n(\nimg_src\n):\n    \n# prepare the 5x5 shaped filter\n\n    kernel = np.array([[\n1\n, \n1\n, \n1\n, \n1\n, \n1\n],\n                       [\n1\n, \n1\n, \n1\n, \n1\n, \n1\n],\n                       [\n1\n, \n1\n, \n1\n, \n1\n, \n1\n],\n                       [\n1\n, \n1\n, \n1\n, \n1\n, \n1\n],\n                       [\n1\n, \n1\n, \n1\n, \n1\n, \n1\n]])\n    kernel = kernel/\nsum\n(kernel)\n\n    \n# filter the source image\n\n    img_rst = cv2.filter2D(img_src, -\n1\n, kernel)\n    \nreturn\n img_rst\n\n\n\ndef\n \ncv_2d_conv_high_pass_filter\n(\nimg_src\n):\n    \n# edge detection filter\n\n    kernel = np.array([[\n0.0\n, -\n1.0\n, \n0.0\n],\n                       [-\n1.0\n, \n4.0\n, -\n1.0\n],\n                       [\n0.0\n, -\n1.0\n, \n0.0\n]])\n\n    kernel = kernel//(np.\nsum\n(kernel) \nif\n np.\nsum\n(kernel) != \n0\n \nelse\n \n1\n)\n\n    \n# filter the source image\n\n    img_rst = cv2.filter2D(img_src, -\n1\n, kernel)\n    \nreturn\n img_rst\n\n\n\ndef\n \ncv_2d_conv_customer_filter\n(\nimg_src\n):\n    \n# edge detection filter\n\n    kernel = np.array([[-\n1.0\n, -\n1.0\n],\n                       [\n2.0\n, \n2.0\n],\n                       [-\n1.0\n, -\n1.0\n]])\n\n    kernel = kernel/(np.\nsum\n(kernel) \nif\n np.\nsum\n(kernel) != \n0\n \nelse\n \n1\n)\n\n    \n# filter the source image\n\n    img_rst = cv2.filter2D(img_src, -\n1\n, kernel)\n    \nreturn\n img_rst\n\n\n\ndef\n \nselect_filter\n(\nimage\n):\n    \nprint\n(image.shape)\n    filters = [\n        {\n            \n'option'\n: \n'lcov'\n,\n            \n'name'\n: \n'2维卷积低通滤镜( filter2D )'\n,\n            \n'filter'\n: \nlambda\n: cv_2d_conv_lower_pass_filer(image)\n        },\n        {\n            \n'option'\n: \n'hcov'\n,\n            \n'name'\n: \n'2维卷积高通滤镜( filter2D )'\n,\n            \n'filter'\n: \nlambda\n: cv_2d_conv_high_pass_filter(image)\n        },\n        {\n            \n'option'\n: \n'ccov'\n,\n            \n'name'\n: \n'2维卷积自定义滤镜( filter2D )'\n,\n            \n'filter'\n: \nlambda\n: cv_2d_conv_customer_filter(image)\n        },\n    ]\n\n    \nprint\n(\n\"本迷你PS软件支持以下滤镜：\"\n)\n    filter_dict = {}\n    \nfor\n \nfilter\n \nin\n filters:\n        filter_dict[\nfilter\n[\n'option'\n]] = \nfilter\n\n        \nprint\n(\n\"* {} : {}\"\n.\nformat\n(\nfilter\n[\n'option'\n], \nfilter\n[\n'name'\n]))\n\n    options = \n'/'\n.join(\nlist\n(\nmap\n(\nlambda\n f: f[\n'option'\n], filters)))\n\n    \nwhile\n \nTrue\n:\n        ret = \ninput\n(\nf\"请选择滤镜[ \n{options}\n]:\"\n)\n        \nfilter\n = filter_dict.get(ret)\n        \nif\n \nfilter\n \nis\n \nNone\n:\n            \nprint\n(\n\"不支持的选项，请重新选择。\"\n)\n        \nelse\n:\n            \nreturn\n \nfilter\n[\n'filter'\n]()\n\n\n\nif\n __name__ == \n'__main__'\n:\n    image = misc.face()\n    blurred = select_filter(image)\n    plot_two(image, blurred)\n", "Tag": "算法分析"}
{"Answer": "出现错误在于这行：xf = np.linspace(0.0, 1.0/(2.0*T), N/2)，N/2是浮点数，而点数应该是整数，所以报错。将其改成：N//2即可，另外代码中还有些弃用的函数，比如time.clock()要改成time.perf_counter()\n如对你有帮助，请点击采纳。", "Konwledge_Point": "应对NP完全问题", "Question": "有时间请大家给解惑下？ python一直出TypeError: 'numpy.float64' object cannot be interpreted as an integer，不知如何是好啊\n版本：Python 3.9.1+numpy 1.21.3\n运行出错！！，求解惑。\n\n\n#Compute and Plot FFT\n\n\ntic\n = time.perf_counter()\n\nplt\n.figure(\n3\n)  \n\nxf\n = np.linspace(\n0\n.\n0\n, \n1\n.\n0\n/(\n2\n.\n0\n*T), N/\n2\n)\n\nyf\n = fft(x)\n\nplt\n.plot(xf, \n2\n.\n0\n/N * np.abs(yf[\n0\n:np.int(N/\n2\n)]))\n\nplt\n.grid()\n\nplt\n.xlabel('Frequency (Hz)')\n\nplt\n.ylabel('Accel (g)')\n\nplt\n.title('FFT - ' + file_path)\n\ntoc\n = time.clock()\n\nprint\n(\n\"FFT Time:\"\n,toc-tic)\n\nplt\n.show()\n\n\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Administrator\\Desktop\\vibration analysis\\vibration-analysis-MATLAB-Python-functions\\Load_Plot_RMS_FFT.py\", line 58, in \n    xf = np.linspace(0.0, 1.0/(2.0*T), N/2)\n  File \"<__array_function__ internals>\", line 5, in linspace\n  File \"D:\\python\\lib\\site-packages\\numpy\\core\\function_base.py\", line 120, in linspace\n    num = operator.index(num)\nTypeError: 'numpy.float64' object cannot be interpreted as an integer", "Tag": "算法分析"}
{"Answer": "源码中有一个弃用警告，就是说pymc3中的有关plot函数的关键字参数varnames已经弃用，改成了var_names，用var_names=['theta']试试。", "Konwledge_Point": "应对NP完全问题", "Question": "关于pymc3报错问题\nimport pymc3 as pm\n\nimport numpy as np\n\nimport scipy.stats as stats\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\npalette = 'muted'\n\nsns.set_palette(palette); sns.set_color_codes(palette)\n\n\n\nnp.random.seed(123)\n\nn_experiments = 4\n\ntheta_real = 0.35  # unkwon value in a real experiment\n\ndata = stats.bernoulli.rvs(p=theta_real, size=n_experiments)\n\n\n\nwith pm.Model() as our_first_model:\n\n    # a priori\n\n    theta = pm.Beta('theta', alpha=1, beta=1)\n\n    # likelihood\n\n    y = pm.Bernoulli('y', p=theta, observed=data)\n\n    #y = pm.Binomial('theta',n=n_experimentos, p=theta, observed=sum(datos))\n\n    start = pm.find_MAP()\n\n    step = pm.Metropolis()\n\n    trace = pm.sample(1000, step=step, start=start,cores=1)\n\n\n\nwith our_first_model:\n\n    step = pm.Metropolis()\n\n    multi_trace = pm.sample(1000, step=step,cores=1)\n\n\n\nif __name__ == '__main__':\n\n    burnin = 0  # no burnin\n\n    multi_chain = multi_trace[burnin:]\n\n    pm.traceplot(multi_chain, lines={'theta':theta_real})\n\n    plt.show()\n\n    # print(pm.gelman_rubin(multi_chain))\n\n    pm.forestplot(multi_chain, varnames=['theta'])\n\n    plt.show()\n\n\n\n总这样报错呢！？？？？？？？？？？？？？？？？？？？？？？？？？\n\n\n\n\nTypeError                                 Traceback (most recent call last)\n in \n     34     plt.show()\n     35     # print(pm.gelman_rubin(multi_chain))\n---> 36     pm.forestplot(multi_chain, varnames=['theta'])\n     37     plt.show()\n     38 \n\n~\\anaconda3\\lib\\site-packages\\pymc3\\plots\\__init__.py in wrapped(*args, **kwargs)\n     37         if \"varnames\" in kwargs:\n     38             raise DeprecationWarning(\n---> 39                 f\"The `varnames` kwarg was renamed to `var_names`.\", stacklevel=2\n     40             )\n     41         original = func.__name__\n\nTypeError: DeprecationWarning() takes no keyword arguments", "Tag": "算法分析"}
{"Answer": "得看你模型保存的方式，如果是通过save_model函数保存的模型，读取方法就得是model_lgb  = lgb.Booster(model_file_path)", "Konwledge_Point": "应对NP完全问题", "Question": "lightgbm调用模型pkl，出现需要先调用拟合？\n问题遇到的现象和发生背景\n\n\njoblib调用模型，预测数据时出现raise LGBMNotFittedError(\"Estimator not fitted, call fit before exploiting the model.\")\nsklearn.exceptions.NotFittedError: Estimator not fitted, call fit before exploiting the model.\n\n\n用代码块功能插入代码，请勿粘贴截图\n\n\nimport\n numpy \nas\n np\n\nimport\n pandas \nas\n pd\n\nimport\n os\n\nimport\n collections\n\nimport\n lightgbm \nas\n lgb\n\nimport\n graphviz\n\nimport\n joblib\n\nfrom\n lightgbm \nimport\n LGBMClassifier\n\nfrom\n tools \nimport\n globalTool \nas\n gt\n\nfrom\n sklearn.metrics \nimport\n accuracy_score, confusion_matrix, cohen_kappa_score, f1_score, precision_score, recall_score, precision_recall_fscore_support\n\nfrom\n sklearn.model_selection \nimport\n train_test_split, KFold\n\n\nclass\n \nmodel\n:\n    \ndef\n \n__init__\n(\nself, f_path\n):\n        \nprint\n(f_path)\n        self.data = \nNone\n\n        self.x_train = \nNone\n\n        self.x_test = \nNone\n\n        self.y_train = \nNone\n\n        self.y_test = \nNone\n\n        self.model_path = \n''\n\n        self.row = \n0\n\n        self.col = \n0\n\n        self.data_columns = []  \n# 列名\n\n        self.in_data(f_path)  \n# 初始化部分类变量\n\n\n    \ndef\n \nin_data\n(\nself,f_path\n):\n        self.data = pd.read_csv(f_path)  \n# 读取数据\n\n        y_data = self.data[\n'oreClass'\n]  \n#\n\n        x_data = self.data.drop(\n'oreClass'\n, axis=\n1\n)\n        \n# x_data = gt.lgb_dropList(x_data)\n\n        self.data_columns = x_data.columns\n\n        self.data_split(x_data, y_data)\n\n    \ndef\n \ndata_split\n(\nself, x_data, y_data\n):\n        x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=\n0.4\n, random_state=\n2020\n)\n\n        self.x_train = np.asarray(x_train)\n        self.x_test = np.asarray(x_test)\n        self.y_train = np.asarray(y_train)\n        self.y_test = np.asarray(y_test)\n\n    \ndef\n \ntrain\n(\nself\n):\n        x_train, y_train, x_test, y_test = self.x_train, self.y_train, self.x_test, self.y_test\n        kfolder = KFold(n_splits=\n5\n, shuffle=\nTrue\n, random_state=\n2020\n)  \n# 5折交叉验证\n\n        kfold = kfolder.split(x_train, y_train)  \n# 返回分类后数据集的index\n\n\n        oof_cb = np.zeros(\nlen\n(x_train))\n        pred_cb = np.asarray([np.zeros(\nlen\n(x_test))], dtype=np.int64)\n\n        param = {\n'boosting_type'\n: \n'gbdt'\n,\n                 \n'num_leaves'\n: \n20\n,\n                 \n'objective'\n: \n'multiclass'\n,\n                 \n'max_depth'\n: \n3\n,\n                 \n'learning_rate'\n: \n0.1\n,\n                 \n'num_class'\n: \n45\n,  \n# 一共45种矿物\n\n                 }  \n# 设置模型参数\n\n        model_lgb = LGBMClassifier(**param)  \n# 创建分类器\n\n\n        \nfor\n train_index, vali_index \nin\n kfold:\n            k_x_train = x_train[train_index]\n            k_y_train = y_train[train_index]\n            k_x_vali = x_train[vali_index]\n            k_y_vali = y_train[vali_index]\n\n            model_lgb = model_lgb.fit(k_x_train, k_y_train, eval_set=[(k_x_vali, k_y_vali)], verbose=\n1\n,\n                                      early_stopping_rounds=\n15\n)  \n# eval_set:评估数据集,list类型;verbose:True显示,False不显示\n\n            \n# early_stopping_rounds:提前结束模型训练\n\n\n            oof_cb[vali_index] = model_lgb.predict(k_x_vali, num_iteration=model_lgb.best_iteration_)\n            tmp = model_lgb.predict(x_test, num_iteration=model_lgb.best_iteration_).ravel()\n            pred_cb = np.append(pred_cb, [tmp], axis=\n0\n)\n\n        pred_cb = pred_cb[\n1\n:].T  \n# 去除第一行0值，得到最终的预测结果\n\n        result_pred, result_true = [], y_test\n        \nfor\n line \nin\n pred_cb:\n            result_pred.append(np.argmax(np.bincount(line)))\n\n        \n# 计算评价指标\n\n        acc_score = accuracy_score(result_true, result_pred)\n        kappa = cohen_kappa_score(result_true, result_pred)\n        macro_f1 = f1_score(result_true, result_pred, average=\n'macro'\n)\n        precision = precision_score(result_true, result_pred, average=\n'macro'\n)\n        recall = recall_score(result_true, result_pred, average=\n'macro'\n)\n\n        \nprint\n(\n'AC score: {:.3f} Kappa:{} macro_f1:{} precision:{} recall:{}'\n\n              .\nformat\n(acc_score, kappa, macro_f1, precision, recall))\n\n        now = gt.timeTitle()\n        self.evaluate(result_true, result_pred, \n'./分析结果/性能评价/'\n + now + \n'.csv'\n)\n        joblib.dump(model_lgb,\n                    \n'./model/saveModel_lgb/lgb_model'\n + now + \n'ac'\n + \n'{:.3f}'\n.\nformat\n(acc_score) + \n'.pkl'\n)  \n# 保存模型\n\n\n        \n# self.getImportance(model_lgb, now)\n\n        \n# self.getCmat(result_true, result_pred, now)\n\n        \n# self.getViewer(model_lgb)\n\n        \n# self.getCmat(y_test, res)\n\n\n    \ndef\n \nloadMod\n(\nself, f_path\n):\n        \nfor\n root, dirs, files \nin\n os.walk(f_path):\n            m_path = \nmax\n(files)\n\n        \nprint\n(\n'lgb_model : '\n, m_path)\n        model_lgb = joblib.load(f_path + \n'/'\n + m_path)\n        \nreturn\n model_lgb\n\n    \ndef\n \nclassify\n(\nself, data\n):  \n# 用准确率最大的模型预测数据形成一列数组\n\n        f_path = \n'D:/Personality/paper/矿物识别/model/saveModel_lgb'\n\n        model_lgb = self.loadMod(f_path)\n\n        data = pd.DataFrame(data)\n        result = model_lgb.predict(data).ravel()\n        \nreturn\n result\n\n    \ndef\n \ngetResult\n(\nself, f_path=\n'D:/Personality/paper/矿物识别/datasource/classData'\n):\n        Data = pd.DataFrame(columns=self.data_columns, dtype=\nfloat\n)  \n# 用以存储所需样本的所有元素值\n\n        \n# print(Data.columns)\n\n\n        files = []  \n# 存储当前文件夹下的所有文件名\n\n        \nfor\n root, dirs, f \nin\n os.walk(f_path):\n            files = f\n            \nbreak\n\n\n        \nfor\n file \nin\n files:\n            elem = file.split(\n'_'\n)[-\n1\n].split(\n'.'\n)[\n0\n]  \n# 获取当前文件代表的元素\n\n            dtmp = pd.read_csv(f_path + \n'/'\n + file, header=\nNone\n)  \n# 读取当前元素的csv\n\n            self.row, self.col = dtmp.shape  \n# 将当前元素的行列号存储到row、col中\n\n            dtmp = np.asarray(dtmp).ravel()  \n# 将当前元素的值变为一列\n\n            Data[elem] = dtmp  \n# 将当前元素的值存储到Data对应的元素中\n\n\n        \n# Data = gt.lgb_dropList(Data)\n\n        Data = np.array(Data)  \n# 将dataframe转换为array\n\n        \n# print(Data.shape)\n\n        \n# print(self.classify_prob(Data))\n\n        result = self.classify(Data)  \n# 对数据进行分类预测\n\n        \nprint\n(result,result.shape)\n\n\n\n\n\n\n\n    \n# ---- 计算各类性能 ----#\n\n    \ndef\n \nevaluate\n(\nself, y_true, y_pred, save_path\n):\n        dt_CSV = pd.DataFrame()\n        unique_Value = np.unique(\nlist\n(y_true)+\nlist\n(y_pred))\n        \nprint\n(\n\"当前种类数：\"\n, \nlen\n(unique_Value))\n        \nprint\n(\n'类别序号：'\n, \nsorted\n(unique_Value))\n        dt_CSV[\n'oreClass'\n] = \nsorted\n(unique_Value)\n\n        name = []\n        \nfor\n i \nin\n \nsorted\n(unique_Value):\n            name.append(gt.no_remap(i))\n        dt_CSV[\n'name'\n] = name\n\n        p_class, r_class, f_class, support_micro = precision_recall_fscore_support(y_true=y_true, y_pred=y_pred)\n        \nprint\n(\nlen\n(p_class), \nlen\n(r_class), \nlen\n(f_class))\n        dt_CSV[\n'precision'\n], dt_CSV[\n'recall'\n], dt_CSV[\n'f1'\n] = p_class, r_class, f_\nclass\n\n\n        \ndt_CSV\n.to_csv(save_path, index=\nFalse\n, encoding=\n'utf_8_sig'\n)\n\n\n\n\nmodel_m = model(\n\"D:/Personality/paper/矿物识别/datasource/fullData/data20201101_160248.csv\"\n)\n\nre = model_m.getResult()\n\n\n\n\n\n运行结果及报错内容\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n在网上查找时发现我的预测数据存在NAN，后续我将NAN转换为0，结果问题相同。还尝试将lightgbm3.3.2降到3.3.0，但问题还是存在。一直在说调用函数时要先调用拟合，但调用的模型是训练完成的pkl格式模型，这应该是没问题的。", "Tag": "算法分析"}
{"Answer": "你改的文件路径中 D之后的：冒号写成了中文的，要改成英文冒号:name 'data' is not defined，name 'np' is not defined等错误是 data 和 np 等变量没定义，可能是定义这些变量模块文件没有引入。比如缺少import numpy as np", "Konwledge_Point": "应对NP完全问题", "Question": "jupyter运行别人的代码有三个问题：1.No such file or directory；2.出现很多name '某某' is not defined3.该出现的图表都不能出现但是并没有报错，\n1.No such file or directory，而我明明已经改过了文件路径的名字，真的改过了；\n2.出现很多name '某某' is not defined\n3.该出现的图表都不能出现但是并没有报错，\n错误详情：\n错误1.No such file or directory: 'D：/Desktop/LSTM_PM2.5多步预测/PRSA_data_2010.1.1-2014.12.31.csv'\n错误2.name 'data' is not defined，name 'np' is not defined等等\n原作者的代码是可以运行出图表的，但是我怎么运行都没有图表出现，但是又没有这方面的报错。原作者联系不上……\npython小白求大家救命！", "Tag": "算法分析"}
{"Answer": "如果是该vc，那么就第二个参数也用LPSTR类型好了。再加一个int类型等做errorcode", "Konwledge_Point": "应对NP完全问题", "Question": "关于VB.net调用C生成的dll，遇到的比字符集不匹配还纠结的问题\n升级项目。从6.0到2012。\n\n升级后出现调用dll中c的方法：\n\nDLLEXPORT BOOL WINAPI InsUpdDel( LPSTR lpSqlStr, LPRTNINFO RtnInfo )\n\n其中 \nLPSTR lpSqlStr\n 是_char × \n类型，在API _winnt.h\n中定义：\n\nwinnt.h\n\n\n\n// ANSI (Multi-byte Character) types\n\n//\n\ntypedef \nNull_terminated\n CHAR *NPSTR, *LPSTR, *PSTR;\n为传入的sql，LPRTNINFO RtnInfo 是自己在头文件中定义的构造体构造体：\n\ntypedef struct {\n\n       long ErrCode;\n\n       char ErrMsg[71];\n\n} RTN_INFO;\n\n\n\ntypedef RTN_INFO* LPRTNINFO;\n\n_当ErrMsg 传入汉字时，报错：传递给系统调用的数据区域太小。HERSULT：0x8007007A。\n\n当lpSqlStr传入汉字时，却不会报错。 \n\n求大神指点，这到底是怎么一回事？\n\nvb中如果要改，实在太麻烦，有没有什么能够在VC下修改的方法？", "Tag": "算法分析"}
{"Answer": "代码这样这样写看是不是想要得到的结果：\nimport numpy as np\nclust=np.zeros((8,3))\nresult=np.matrix('1 3 5;2 7 7;6 5 3')\nfor i  in range(result.shape[0]):\n    for j in range(result.shape[1]):\n        index=result[i,j]\n        clust[index,j]=result[i,j]\nprint(clust)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "numpy数组变长度\n代码如下\n要把上图改为clust=np.zeros（）\n\n\n想要得到如下图所示的效果", "Tag": "算法分析"}
{"Answer": "\nimport os\n\ndef BatchProcessPointClouds(inputDir, outputDir):\n    for file_name in os.listdir(inputDir):\n        if not file_name.endswith(\".pcd\"):\n            continue\n        SimplifyPointClouds(inputDir, file_name, outputDir)\n        RegulateData(file_name, outputDir)\n\n这个函数将遍历指定目录中的所有 .pcd 文件，并对每个文件执行 SimplifyPointClouds() 和 RegulateData() 函数。", "Konwledge_Point": "应对NP完全问题", "Question": "python pcd文件读取\n别人给了一段代码，但是没有给传参设置，只有函数那部分，希望加一部分传参函数，进行批量读取点云数据（pcd格式）文件，将代码运行起来。\n\n\ndef SimplifyPointClouds(inputDir, file_name, outputDir):\n    print(\n\"simplify point clouds\"\n, file_name)\n\n    \ncurFileName\n = inputDir + \n\"/\"\n + file_name\n    print(\n\"processing\"\n+curFileName)\n    \n# readfile\n\n    \npcd\n = o3d.geometry.PointCloud()\n\n    \nif\n file_name.endswith(\n\".pcd\"\n):\n        \npcd\n = o3d.io.read_point_cloud(curFileName)\n    elif file_name.endswith(\n\".txt\"\n):\n        \npcdPts\n = np.loadtxt(curFileName)[:, \n0\n:\n3\n]\n        pcd.\npoints\n = o3d.utility.Vector3dVector(pcdPts)\n    print(\n\"The input point cloud has\"\n, pcd)\n\n    \n'''# denoising\n    print(\"denoising...\")\n    cl, ind = pcd.remove_statistical_outlier(nb_neighbors=30,\n                                             std_ratio=2.0)\n    pcd = pcd.select_by_index(ind)\n    print(\"Denoised\", pcd)''\n'\n\n    \n# down sampling\n\n    \npcd\n = pcd.voxel_down_sample(\nvoxel_size=2.0)\n\n    print(\n\"Down_sampled\"\n, pcd)\n    o3d.io.write_point_cloud(outputDir+\n\"/_1Ds_\"\n+file_name, pcd)\n\ndef RegulateData(file_name, outputDir):\n    \npcd\n = o3d.io.read_point_cloud(outputDir+\n\"/_1Ds_\"\n+file_name)\n    \nnormal\n = [\n0.0\n, -\n1.0\n, \n0.0\n]\n    \nR\n = pyrsc.get_rotationMatrix_from_vectors(normal, [\n0\n, \n0\n, \n1\n])\n    \nT\n = np.eye(\n4\n)\n    T[:\n3\n, :\n3\n] = R\n    \npcd_r\n = pcd.transform(T)\n\n    \n# estimate bonding box\n\n    \naabb\n = pcd_r.get_axis_aligned_bounding_box()\n    \naabb_box_length\n = np.asarray(aabb.get_extent())\n    print(\n\"bonding box length: \"\n, aabb_box_length)\n    \n# adjust x and y direction\n\n    \nif\n aabb_box_length[\n1\n] > aabb_box_length[\n0\n]:\n        print(\n\"Rotate around Z-axis\"\n)\n        \nR\n = o3d.geometry.get_rotation_matrix_from_axis_angle((\n0\n, \n0\n, np.pi / \n2\n))\n        \npcd_r\n = pcd_r.rotate(R)\n    o3d.io.write_point_cloud(outputDir+\n\"/_2Ms_\"\n+file_name, pcd_r) \n# maize stands with ground\n\n\n    \n# detect ground again, remove ground and translate maize canopy to 0 level height\n\n    \nxyz\n = np.asarray(pcd_r.points)\n    \ncsf\n = CSF.CSF()\n\n    \n# CSF算法的必要参数设置\n\n    csf.params.\nbSloopSmooth\n = False\n    csf.params.\ncloth_resolution\n = \n1.0\n  \n# 布料网格分辨率\n\n    csf.params.\nrigidness\n = \n3\n  \n# 布料刚性参数，只能是1、2、3\n\n    csf.params.\ntime_step\n = \n0.65\n\n    csf.params.\nclass_threshold\n = \n0.2\n  \n# 点云与布料模拟点的距离阈值\n\n    csf.params.\ninterations\n = \n500\n  \n# 最大迭代次数\n\n    \n# 更多参数详细信息见: http://ramm.bnu.edu.cn/projects/CSF/download/\n\n    csf.setPointCloud(xyz)\n    \nground\n = CSF.VecInt()  \n# 存储地面点索引的list列表\n\n    \nnon_ground\n = CSF.VecInt()  \n# 存储非地面点的list列表\n\n    csf.do_filtering(ground, non_ground)  \n# 执行CSF滤波\n\n    \nground_cloud\n = pcd.select_by_index(ground)  \n# 保存地面点\n\n    \nnon_ground_cloud\n = pcd.select_by_index(non_ground)  \n# 保存非地面点\n\n    \ngroundPts\n = np.asarray(ground_cloud.points)\n    \ncur_Z\n = np.mean(groundPts[:, \n2\n]) \n# 这个值作为平移的依据（但是这个检测到的地面是最下面的，所以要加上thickness/2.0?）。\n\n    \nthickness\n = \n15.0\n  \n# 地面厚度\n\n    \ndists\n = non_ground_cloud.compute_point_cloud_distance(ground_cloud)\n    \ndists\n = np.asarray(dists)\n    \nind\n = np.where(dists > thickness)[\n0\n]\n    \nCanopyPcd\n = non_ground_cloud.select_by_index(ind)\n    \nCanopyPts\n = np.asarray(CanopyPcd.points)\n    \nind\n = np.where(CanopyPts[:, \n2\n] > cur_Z)[\n0\n]\n    \nCanopyPcd\n = CanopyPcd.select_by_index(ind)\n    o3d.io.write_point_cloud(outputDir + \n\"/_3Te_\"\n + file_name, CanopyPcd)\n\n", "Tag": "算法分析"}
{"Answer": "刚回答了你的另外一个问题，这又一个加功能的问题下有代码，可直接复制使用。如有帮助，敬请采纳，你的采纳是我前进的动力，O(∩_∩)O谢谢！！！！！！！！\nimport re\nimport pandas as pd\n# 字符串\nsrc_str = \"[2724606325618536238,91530112343713723J,备注61],[2692025102277631266,91330482MA2BCAQRXE,备注et],[2748256921818730219,91430600MA4R1DQT25,备注59],[2701373294338164869,91500105569920960F,备注43],[2730012660790633301,91510100052546574C,备注2],[1590976417749808642,91110111MA04F5YY0G,备注88],[2702239201936713104,91440300MA5GT9NP45,备注66],\"\nsrc_list = re.findall('\\[(.*?)\\]', src_str)\n\ndf_data = {}\nfor src in src_list:\n    x = src.split(\",\")\n    # 需要订单号重复时，取消下面代码注释,并注释掉下一行代码\n    # df_data[x[0]] = x\n    df_data[x[0]] = x[1:]\ng_variPandasDfFpx = pd.DataFrame(df_data)\ng_variPandasDfFpx.to_csv(r'./字符串转df.csv', encoding='GBK')\n", "Konwledge_Point": "应对NP完全问题", "Question": "字符串想转换成 pandas 的 df\n问题遇到的现象和发生背景\n\n\n问题相关代码，请勿粘贴截图\n\n\n运行结果及报错内容\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果\n\n\n字符串想转换成 pandas 的 df\n订单号，税号 ，备注信息\n[2724606325618536238,91530112343713723J,备注61],[2692025102277631266,91330482MA2BCAQRXE,备注et],[2748256921818730219,91430600MA4R1DQT25,备注59],[2701373294338164869,91500105569920960F,备注43],[2730012660790633301,91510100052546574C,备注2],[1590976417749808642,91110111MA04F5YY0G,备注88],[2702239201936713104,91440300MA5GT9NP45,备注66],\n希望订单号作为索引", "Tag": "算法分析"}
{"Answer": "将数据转换成Decimal型进行计算", "Konwledge_Point": "应对NP完全问题", "Question": "python编写的计算程序得不到正确答案\n材料学中的几个计算公式我转换成了python程序想便于计算，但是出现重大偏差，这是怎么回事？万分感谢！\n\n\nimport\n numpy as np\n\n\nfor\n i in range(\n10\n):\n    \ne\n = float(input('输入等效应变：'))\n    \nfirst_number\n = float(input('输入Z非零部分：'))\n    \nsecond_number\n = float(input('输入Z是\n10\n的几次方:'))\n    \nz\n = first_number*\n10\n**second_number\n    \nee\n = \n0\n.\n0016\n*\n30\n.\n0\n**\n0\n.\n2709\n*z**\n0\n.\n1147\n\n    \nddrx\n = \n3\n.\n66\n*\n10\n**\n5\n*z**(-\n0\n.\n2218\n)\n\n    \ndfin\n = (ddrx-\n30\n.\n0\n)*(\n1\n.\n0\n-np.exp(-np.log(\n2\n.\n0\n*(e/ee)**\n2\n.\n13\n)))+\n30\n.\n0\n\n    \nprint\n(dfin)\n\n\n\n\n\n重大计算偏差如下：\n\n\n\n\n原公式如下图：\n\n\n\n\n\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "数据类型错误，检查一下histdict是什么类型数据，sp12数据是否符合EMA参数传递要求，可以输出一下看看，print(sp12)。", "Konwledge_Point": "应对NP完全问题", "Question": "Python QMT系统引用talib遇到问题，帮看看怎么改？\nhisdict = ContextInfo.get_history_data(ContextInfo.line1,'1d','close',dividend_type=1)\n\n\nsp12=np.array(hisdict)\n\n\nema_list=talib.EMA(sp12,timeperiod=12)\n\n\nprint(ema_list)\n\n\n【2021-10-22 15:00:39.172】 start trading mode\n\n\n【2021-10-22 15:00:39.172】 0D:\\python\\EMA.py_SH00030064Exception:real is not double\n\n\n【2021-10-22 15:00:39.173】 Traceback (most recent call last):\n\n\nFile \"D:\\bin.x64\\lib\\site-packages\\talib_\ninit\n_.py\", line 20, in wrapper\n\n\nStopIteration\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n\nFile \"\", line 23, in handlebar\n\n\nFile \"D:\\bin.x64\\lib\\site-packages\\talib_\ninit\n_.py\", line 24, in wrapper\n\n\nFile \"_func.pxi\", line 7076, in talib._ta_lib.EMA\n\n\nException: real is not double", "Tag": "算法分析"}
{"Answer": "你在13行前面输出一下task_type看看值是什么啊。", "Konwledge_Point": "应对NP完全问题", "Question": "python程序问题，传递参数\n这个代码，为什么定义了 task_type = ‘multiclass', 但是最后一行打印出来的score 还是以\n这个得出来的呢？这个分数是 task_type =’regression‘时得出的分数\n    else:\n        assert task_type == 'regression'\n        score = sklearn.metrics.mean_squared_error(target, prediction) ** 0.5 * y_std\n    return score\n\n\n\n\n下面是源代码\n\n\n\n\ntask_type\n = 'multiclass'\n\n@torch.no_grad()\ndef evaluate(part, task_type):\n    model.eval()\n    \nprediction\n = []\n    for batch \nin\n delu.iter_batches(X[part], \n1024\n):\n        prediction.append(apply_model(batch))\n    \nprediction\n = torch.cat(prediction).squeeze(\n1\n).cpu().numpy()\n    \ntarget\n = y[part].cpu().numpy()\n\n    \nif\n \ntask_type\n == 'binclass':\n        \nprediction\n = np.round(scipy.special.expit(prediction))\n        \nscore\n = sklearn.metrics.accuracy_score(target, prediction)\n    elif \ntask_type\n == 'multiclass':\n        \nprediction\n = prediction.argmax(\n1\n)\n        \nscore\n = sklearn.metrics.accuracy_score(target, prediction)\n    \nelse\n:\n        \nassert\n \ntask_type\n == 'regression'\n        \nscore\n = sklearn.metrics.mean_squared_error(target, prediction) ** \n0.5\n * y_std\n    return score\n\n\n\n# Create a dataloader for batches of indices\n\n\n# Docs: https://yura52.github.io/zero/reference/api/zero.data.IndexLoader.html\n\n\nbatch_size\n = \n256\n\n\ntrain_loader\n = delu.data.IndexLoader(len(X['train']), batch_size, \ndevice=device)\n\n\n\n# Create a progress tracker for early stopping\n\n\n# Docs: https://yura52.github.io/zero/reference/api/zero.ProgressTracker.html\n\n\nprogress\n = delu.ProgressTracker(\npatience=100)\n\nprint(f'Test score before training: {evaluate(\n\"test\"\n, task_type):.\n4\nf}')\n\n", "Tag": "算法分析"}
{"Answer": "一、1、检查你的pycharm 的解释器（interpreter）2、打开命令行模式， 更换目录到 1 所在的目录3、运行  pip install numpy或者 pip install numpy -i https://mirrors.aliyun.com/pypi/simple/\n这样就可以确保你的pycharm 可以用numpy \n二、或者你自己把你的pycharm 的解释器指向 改为你已经装了np 的python ", "Konwledge_Point": "应对NP完全问题", "Question": "关于pycharm安装Numpy出错\n要怎么处理呢，（我的电脑有两个Python，...一个安了np一个没安）", "Tag": "算法分析"}
{"Answer": "\nbox = np.array([x,y], dtype=\"int\")\nbox = np.int0(box)\n\n将[x,y]使用np.array转换，然后np.int0转换", "Konwledge_Point": "应对NP完全问题", "Question": "怎么构造opencv图象点的数据类型\n我定义一个函数G01\ndef G01(PointA,PointB):\n    XQ = int(tuple(Point0[0]))#起点x坐标\n    YQ = int(tuple(Point0[1]))#起点y坐标\n    XZ = int(tuple(PointA[0]))#终点x坐标\n    YZ = int(tuple(PointA[1]))#终点y坐标\n\n\nbox = cv2.minAreaRect(cnt)#轮廓最小外接矩形\nbox = cv2.boxPoints(box)#矩形四个角点\nbox = np.array(box, dtype=\"int\")\nbox = np.int0(box)\nG01(box[0],box[1])这样调用函数没有问题\nx=20\ny=30\nG01([x,y],box[0])这样调用函数出错，怎么把已知点的xy坐标值构造成和box[0]一样的数据类型", "Tag": "算法分析"}
{"Answer": "while count <= np:count 的值一直是0，np一直是500你这不是死循环嘛。都没出循环，plt.show()都没执行", "Konwledge_Point": "应对NP完全问题", "Question": "matplotlib随机漫步\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n time\n\nimport\n random \nas\n rd\nplt.style.use(\n'seaborn'\n)\nfig, ax = plt.subplots()\n\nclass\n \nRandomWalk\n:\n    \n\"\"\"生成一个随机漫步\"\"\"\n\n    \ndef\n \n__init__\n(\nself,np = \n500\n):\n\n        \n        self.x = [\n0\n]\n        self.y = [\n0\n]\n\n    \ndef\n \nfill\n(\nself,np = \n500\n):\n        self.np = np\n        count = \n0\n\n        \n\n        plt.style.use(\n'seaborn'\n)\n        fig, ax = plt.subplots()\n        \nwhile\n count <= np:\n            xf = rd.choice([\n1\n,-\n1\n])\n            xb = rd.choice([\n1\n,\n2\n,\n3\n,\n4\n,\n5\n])\n            xz = xf*xb\n\n            yf = rd.choice([\n1\n,-\n1\n])\n            yb = rd.choice([\n1\n,\n2\n,\n3\n,\n4\n,\n5\n])\n            yz = yf*yb\n            \n            ax.scatter(self.x[-\n1\n],self.y[-\n1\n])\n            self.x.append(xz)\n            self.y.append(yz)\n        plt.show()\n\n\na = RandomWalk()\na.fill()\n\n\n\n\n一个随机漫步的类，各位看一下，为什么我的一直显示不出来？", "Tag": "算法分析"}
{"Answer": "24行  # 1：30之后改成这样：a=[lcal[i] for i in range(0,2920,8)]print(a)\n测试结果：\n", "Konwledge_Point": "应对NP完全问题", "Question": "python列表赋值\npython列表赋值\n\n\n对列表赋值时，全部赋值为同一元素，这里问题为：列表a的所有元素都赋值为了lca1[2912]是错的\n\n\n\n\nimport\n numpy as np\n\nfrom\n os import listdir\n\nfrom\n os.path import join   \n\nimport\n netCDF4           \n\nimport\n xarray as xr\n\nimport\n pandas as pd\n\nimport\n math\n\nimport\n netCDF4 as nc \n\nf0\n = xr.open_dataset(r\n\"E:/1/DATA1/CERES_SYN1deg-3H_Terra-Aqua-MODIS_Ed4.1_Subset_20150101-20151231.nc\"\n)\n\nlca\n = f0['cldarea_low_3h']\n\nv0_p1_d\n = lca.sel(lat=slice(\n35\n.\n5\n,\n35\n.\n5\n),\n                \nlon\n=slice(\n103\n.\n5\n,\n103\n.\n5\n))\n\nv1\n=np.array(v0_p1_d)\n\nv0_p2_d\n = lca.sel(lat=slice(\n36\n.\n5\n,\n36\n.\n5\n),\n                \nlon\n=slice(\n103\n.\n5\n,\n103\n.\n5\n))\n\nv2\n=np.array(v0_p2_d)\n\nv0_p3_d\n = lca.sel(lat=slice(\n35\n.\n5\n,\n35\n.\n5\n),\n                \nlon\n=slice(\n104\n.\n5\n,\n104\n.\n5\n))\n\nv3\n=np.array(v0_p3_d)\n\nv0_p4_d\n = lca.sel(lat=slice(\n36\n.\n5\n,\n36\n.\n5\n),\n                \nlon\n=slice(\n104\n.\n5\n,\n104\n.\n5\n))\n\nv4\n=np.array(v0_p4_d)\n\nlca1\n = (v1+v2+v3+v4)/\n4\n\n\n# 1：30\n\n\na\n=[\n0\n]*\n365\n\n\nfor\n j in range(len(a)):\n    \nfor\n i in range(\n0\n,\n2920\n,\n8\n):\n        \na\n[j]=lca1[i]\n\nprint\n(a)\n\n\n\n\n\n\n\n\n我想要达到的结果是，lca1数组里从第一个数据开始每隔8个数据赋值到列表a", "Tag": "算法分析"}
{"Answer": "##你的函数中参数传错了，而且你并没有返回计算相似度的结果。我没有你的模型，用的自己的计算可以正常返回\n\n修改后的你试一下。有问题再看下\n\n# -*- coding: utf-8 -*-\n\n\nimport json\nimport cv2\nfrom PIL import Image\nimport numpy as np\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nfrom modelscope.outputs import OutputKeys\nfrom flask import request\nfrom flask import jsonify\nfrom flask import Flask,make_response\nimport base64\nface_recognition = pipeline(Tasks.face_recognition, model='damo/cv_ir101_facerecognition_cfglint')\ndef face_recognition(image1,image2):\n    get_img1 = image1\n    get_img2 = image2\n    emb1 = face_recognition(get_img1)[OutputKeys.IMG_EMBEDDING]\n    emb2 = face_recognition(get_img2)[OutputKeys.IMG_EMBEDDING]\n    sim = np.dot(emb1[0], emb2[0])\n    return (f'Face cosine similarity={sim:.3f}, get_img1:{get_img1}  get_img2:{get_img2}')\napp = Flask(__name__)\n@app.route(\"/\",methods=[\"POST\"])\ndef first_post():\n    my_json = {\"msg\": None\n                }\n    data = request.get_data()\n    if not data:\n        my_json[\"msg\"] = \"No data obtained!\"\n        return jsonify(msg=\"缺少参数\")\n    try:\n        data=json.loads(data)\n        image1=data[\"image1\"].encode()\n        image2=data[\"image2\"].encode()\n        return face_recognition(image1,image2)\n    except Exception as e:\n        print(e)\n        return jsonify(msg=\"出错了，请查看是否正确访问\")\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=6000, debug=True, use_reloader=False)\napi文件\nimport os\nimport sys\nimport time\nimport json\nimport base64\nimport requests\nurl = \"http://0.0.0.0:6000\"\nbase64_data=None\nbase64_data = base64.b64encode(base64_data,encoding=\"utf-8\")\nbase64_data = base64_data.decode()\ndata={\n    'image1':None,\n    'image2':None\n}\nstart_time = time.time()\nresponse = requests.post(url, data=json.dumps(data))\nend_time = time.time()\nprint(\"time:\", end_time-start_time)\nprint(response.text)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "在编写flask接口封装人脸识别能力的时候，出现了一些问题\n在写一个flask的接口时，我调用post的图片为我实现功能（两张图片的相似度比对），但是我有点想不到我应该怎么写才能调用到\n\n\n\n\n\n\nimport json\nimport cv2\n\nfrom\n PIL import Image\nimport numpy as np\n\nfrom\n modelscope.pipelines import pipeline\n\nfrom\n modelscope.utils.constant import Tasks\n\nfrom\n modelscope.outputs import OutputKeys\n\nfrom\n flask import request\n\nfrom\n flask import jsonify\n\nfrom\n flask import Flask,make_response\nimport base64\nface_recognition = pipeline(Tasks.face_recognition, \nmodel\n=\n'damo/cv_ir101_facerecognition_cfglint'\n)\ndef face_recognition(get_img1,get_img2):\n    get_img1 = image1\n    get_img2 = image2\n    emb1 = face_recognition(get_img1)[OutputKeys.IMG_EMBEDDING]\n    emb2 = face_recognition(get_img2)[OutputKeys.IMG_EMBEDDING]\n    sim = np.dot(emb1[0], emb2[0])\n    return (f\n'Face cosine similarity={sim:.3f}, get_img1:{get_img1}  get_img2:{get_img2}'\n)\napp = Flask(__name__)\n@app.route(\n\"/\"\n,methods=[\n\"POST\"\n])\ndef first_post():\n    my_json = {\n\"msg\"\n: None\n                }\n    data = request.get_data()\n    \nif\n \nnot\n data:\n        my_json[\n\"msg\"\n] = \n\"No data obtained!\"\n\n        return jsonify(\nmsg\n=\n\"缺少参数\"\n)\n    try:\n        \ndata\n=json.loads(data)\n        \nimage1\n=data[\n\"image1\"\n].encode()\n        \nimage2\n=data[\n\"image2\"\n].encode()\n    except Exception as e:\n        \nprint\n(e)\n        return jsonify(\nmsg\n=\n\"出错了，请查看是否正确访问\"\n)\n\nif\n __name__ == \n\"__main__\"\n:\n    app.\nrun\n(\nhost\n=\n'0.0.0.0'\n, \nport\n=6000, \ndebug\n=\nTrue\n, \nuse_reloader\n=\nFalse\n)\n\napi文件\nimport os\nimport sys\nimport time\nimport json\nimport base64\nimport requests\nurl = \n\"http://0.0.0.0:6000\"\n\n\nbase64_data\n=None\nbase64_data = base64.b64encode(base64_data,\nencoding\n=\n\"utf-8\"\n)\nbase64_data = base64_data.decode()\ndata={\n    \n'image1'\n:None,\n    \n'image2'\n:None\n}\nstart_time = time.time()\nresponse = requests.post(url, \ndata\n=json.dumps(data))\nend_time = time.time()\n\nprint\n(\n\"time:\"\n, end_time-start_time)\n\nprint\n(response.text)\n\n\n\nface_recognition中的两个image爆红  但是程序可以跑，只是就算给它post了两张图片，最后依然会显示“出错了，请查看是否正确访问\"\n\n\n暂时没什么思路 可以做成全局变量的方式吗？\n\n\n给它post的两张图片能够正确接受并且给我返回我需要的相似度的值", "Tag": "算法分析"}
{"Answer": "写个简单的测试下就知道了。\n\nimport cv2\nimport numpy as np\nimg=np.ones((9,9),dtype=np.uint8)\ntemp=img.copy().astype(np.int8)\ncv2.circle(temp,(5,5),3,-100,1)\ntest=[temp==-100]\nprint(test)\n\n#output:\n[[False, False, False, False, False, False, False, False, False],\n[False, False, False, False, False, False, False, False, False],\n[False, False, False, False, False,  True, False, False, False],\n[False, False, False,  True,  True, False,  True,  True, False],\n[False, False, False,  True, False, False, False,  True, False],\n[False, False,  True, False, False, False, False, False,  True],\n[False, False, False,  True, False, False, False,  True, False],\n[False, False, False,  True,  True, False,  True,  True, False],\n[False, False, False, False, False,  True, False, False, False]]\n所以，这个函数的作用就很明显了。如果图片是单通道的图片，那么处理后的img就只有在画圆的地方的位置是true，其他为false ；如果是三通道的话，那就是蓝色通道的圆为true，其他为false。\n根据这段代码和你的描述，这应该是用来产生掩膜用的，如果thickness=-1，那么叠加在圆原图像上面的话就是将圆内的区域显示出来，圆外的区域为黑色。如果thickness>=1,那么就只会显示在圆上的点，其他区域为黑色。\n ", "Konwledge_Point": "应对NP完全问题", "Question": "OpenCV的一段代码问题\n这是在学习一位博主的一段代码，旨在获取中心⚪部分的色域集，最后两行的目的是什么意思非常疑惑，求求各位好哥哥姐姐指导一下！\n\n\n\n\n\ndef get_circle_field_color(img,center,r,thickness):\n    '''获取中心圆形区域的色值集'''\n    temp=img.copy().astype(np.int)\n    cv2.circle(temp,center,r,-100,thickness=thickness)\n    return img[temp == -100]", "Tag": "算法分析"}
{"Answer": "while count <= np:count 的值一直是0，np一直是500你这不是死循环嘛。都没出循环，plt.show()都没执行", "Konwledge_Point": "应对NP完全问题", "Question": "matplotlib随机漫步\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n time\n\nimport\n random \nas\n rd\nplt.style.use(\n'seaborn'\n)\nfig, ax = plt.subplots()\n\nclass\n \nRandomWalk\n:\n    \n\"\"\"生成一个随机漫步\"\"\"\n\n    \ndef\n \n__init__\n(\nself,np = \n500\n):\n\n        \n        self.x = [\n0\n]\n        self.y = [\n0\n]\n\n    \ndef\n \nfill\n(\nself,np = \n500\n):\n        self.np = np\n        count = \n0\n\n        \n\n        plt.style.use(\n'seaborn'\n)\n        fig, ax = plt.subplots()\n        \nwhile\n count <= np:\n            xf = rd.choice([\n1\n,-\n1\n])\n            xb = rd.choice([\n1\n,\n2\n,\n3\n,\n4\n,\n5\n])\n            xz = xf*xb\n\n            yf = rd.choice([\n1\n,-\n1\n])\n            yb = rd.choice([\n1\n,\n2\n,\n3\n,\n4\n,\n5\n])\n            yz = yf*yb\n            \n            ax.scatter(self.x[-\n1\n],self.y[-\n1\n])\n            self.x.append(xz)\n            self.y.append(yz)\n        plt.show()\n\n\na = RandomWalk()\na.fill()\n\n\n\n\n一个随机漫步的类，各位看一下，为什么我的一直显示不出来？", "Tag": "算法分析"}
{"Answer": "24行  # 1：30之后改成这样：a=[lcal[i] for i in range(0,2920,8)]print(a)\n测试结果：\n", "Konwledge_Point": "应对NP完全问题", "Question": "python列表赋值\npython列表赋值\n\n\n对列表赋值时，全部赋值为同一元素，这里问题为：列表a的所有元素都赋值为了lca1[2912]是错的\n\n\n\n\nimport\n numpy as np\n\nfrom\n os import listdir\n\nfrom\n os.path import join   \n\nimport\n netCDF4           \n\nimport\n xarray as xr\n\nimport\n pandas as pd\n\nimport\n math\n\nimport\n netCDF4 as nc \n\nf0\n = xr.open_dataset(r\n\"E:/1/DATA1/CERES_SYN1deg-3H_Terra-Aqua-MODIS_Ed4.1_Subset_20150101-20151231.nc\"\n)\n\nlca\n = f0['cldarea_low_3h']\n\nv0_p1_d\n = lca.sel(lat=slice(\n35\n.\n5\n,\n35\n.\n5\n),\n                \nlon\n=slice(\n103\n.\n5\n,\n103\n.\n5\n))\n\nv1\n=np.array(v0_p1_d)\n\nv0_p2_d\n = lca.sel(lat=slice(\n36\n.\n5\n,\n36\n.\n5\n),\n                \nlon\n=slice(\n103\n.\n5\n,\n103\n.\n5\n))\n\nv2\n=np.array(v0_p2_d)\n\nv0_p3_d\n = lca.sel(lat=slice(\n35\n.\n5\n,\n35\n.\n5\n),\n                \nlon\n=slice(\n104\n.\n5\n,\n104\n.\n5\n))\n\nv3\n=np.array(v0_p3_d)\n\nv0_p4_d\n = lca.sel(lat=slice(\n36\n.\n5\n,\n36\n.\n5\n),\n                \nlon\n=slice(\n104\n.\n5\n,\n104\n.\n5\n))\n\nv4\n=np.array(v0_p4_d)\n\nlca1\n = (v1+v2+v3+v4)/\n4\n\n\n# 1：30\n\n\na\n=[\n0\n]*\n365\n\n\nfor\n j in range(len(a)):\n    \nfor\n i in range(\n0\n,\n2920\n,\n8\n):\n        \na\n[j]=lca1[i]\n\nprint\n(a)\n\n\n\n\n\n\n\n\n我想要达到的结果是，lca1数组里从第一个数据开始每隔8个数据赋值到列表a", "Tag": "算法分析"}
{"Answer": "##你的函数中参数传错了，而且你并没有返回计算相似度的结果。我没有你的模型，用的自己的计算可以正常返回\n\n修改后的你试一下。有问题再看下\n\n# -*- coding: utf-8 -*-\n\n\nimport json\nimport cv2\nfrom PIL import Image\nimport numpy as np\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\nfrom modelscope.outputs import OutputKeys\nfrom flask import request\nfrom flask import jsonify\nfrom flask import Flask,make_response\nimport base64\nface_recognition = pipeline(Tasks.face_recognition, model='damo/cv_ir101_facerecognition_cfglint')\ndef face_recognition(image1,image2):\n    get_img1 = image1\n    get_img2 = image2\n    emb1 = face_recognition(get_img1)[OutputKeys.IMG_EMBEDDING]\n    emb2 = face_recognition(get_img2)[OutputKeys.IMG_EMBEDDING]\n    sim = np.dot(emb1[0], emb2[0])\n    return (f'Face cosine similarity={sim:.3f}, get_img1:{get_img1}  get_img2:{get_img2}')\napp = Flask(__name__)\n@app.route(\"/\",methods=[\"POST\"])\ndef first_post():\n    my_json = {\"msg\": None\n                }\n    data = request.get_data()\n    if not data:\n        my_json[\"msg\"] = \"No data obtained!\"\n        return jsonify(msg=\"缺少参数\")\n    try:\n        data=json.loads(data)\n        image1=data[\"image1\"].encode()\n        image2=data[\"image2\"].encode()\n        return face_recognition(image1,image2)\n    except Exception as e:\n        print(e)\n        return jsonify(msg=\"出错了，请查看是否正确访问\")\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=6000, debug=True, use_reloader=False)\napi文件\nimport os\nimport sys\nimport time\nimport json\nimport base64\nimport requests\nurl = \"http://0.0.0.0:6000\"\nbase64_data=None\nbase64_data = base64.b64encode(base64_data,encoding=\"utf-8\")\nbase64_data = base64_data.decode()\ndata={\n    'image1':None,\n    'image2':None\n}\nstart_time = time.time()\nresponse = requests.post(url, data=json.dumps(data))\nend_time = time.time()\nprint(\"time:\", end_time-start_time)\nprint(response.text)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "在编写flask接口封装人脸识别能力的时候，出现了一些问题\n在写一个flask的接口时，我调用post的图片为我实现功能（两张图片的相似度比对），但是我有点想不到我应该怎么写才能调用到\n\n\n\n\n\n\nimport json\nimport cv2\n\nfrom\n PIL import Image\nimport numpy as np\n\nfrom\n modelscope.pipelines import pipeline\n\nfrom\n modelscope.utils.constant import Tasks\n\nfrom\n modelscope.outputs import OutputKeys\n\nfrom\n flask import request\n\nfrom\n flask import jsonify\n\nfrom\n flask import Flask,make_response\nimport base64\nface_recognition = pipeline(Tasks.face_recognition, \nmodel\n=\n'damo/cv_ir101_facerecognition_cfglint'\n)\ndef face_recognition(get_img1,get_img2):\n    get_img1 = image1\n    get_img2 = image2\n    emb1 = face_recognition(get_img1)[OutputKeys.IMG_EMBEDDING]\n    emb2 = face_recognition(get_img2)[OutputKeys.IMG_EMBEDDING]\n    sim = np.dot(emb1[0], emb2[0])\n    return (f\n'Face cosine similarity={sim:.3f}, get_img1:{get_img1}  get_img2:{get_img2}'\n)\napp = Flask(__name__)\n@app.route(\n\"/\"\n,methods=[\n\"POST\"\n])\ndef first_post():\n    my_json = {\n\"msg\"\n: None\n                }\n    data = request.get_data()\n    \nif\n \nnot\n data:\n        my_json[\n\"msg\"\n] = \n\"No data obtained!\"\n\n        return jsonify(\nmsg\n=\n\"缺少参数\"\n)\n    try:\n        \ndata\n=json.loads(data)\n        \nimage1\n=data[\n\"image1\"\n].encode()\n        \nimage2\n=data[\n\"image2\"\n].encode()\n    except Exception as e:\n        \nprint\n(e)\n        return jsonify(\nmsg\n=\n\"出错了，请查看是否正确访问\"\n)\n\nif\n __name__ == \n\"__main__\"\n:\n    app.\nrun\n(\nhost\n=\n'0.0.0.0'\n, \nport\n=6000, \ndebug\n=\nTrue\n, \nuse_reloader\n=\nFalse\n)\n\napi文件\nimport os\nimport sys\nimport time\nimport json\nimport base64\nimport requests\nurl = \n\"http://0.0.0.0:6000\"\n\n\nbase64_data\n=None\nbase64_data = base64.b64encode(base64_data,\nencoding\n=\n\"utf-8\"\n)\nbase64_data = base64_data.decode()\ndata={\n    \n'image1'\n:None,\n    \n'image2'\n:None\n}\nstart_time = time.time()\nresponse = requests.post(url, \ndata\n=json.dumps(data))\nend_time = time.time()\n\nprint\n(\n\"time:\"\n, end_time-start_time)\n\nprint\n(response.text)\n\n\n\nface_recognition中的两个image爆红  但是程序可以跑，只是就算给它post了两张图片，最后依然会显示“出错了，请查看是否正确访问\"\n\n\n暂时没什么思路 可以做成全局变量的方式吗？\n\n\n给它post的两张图片能够正确接受并且给我返回我需要的相似度的值", "Tag": "算法分析"}
{"Answer": "写个简单的测试下就知道了。\n\nimport cv2\nimport numpy as np\nimg=np.ones((9,9),dtype=np.uint8)\ntemp=img.copy().astype(np.int8)\ncv2.circle(temp,(5,5),3,-100,1)\ntest=[temp==-100]\nprint(test)\n\n#output:\n[[False, False, False, False, False, False, False, False, False],\n[False, False, False, False, False, False, False, False, False],\n[False, False, False, False, False,  True, False, False, False],\n[False, False, False,  True,  True, False,  True,  True, False],\n[False, False, False,  True, False, False, False,  True, False],\n[False, False,  True, False, False, False, False, False,  True],\n[False, False, False,  True, False, False, False,  True, False],\n[False, False, False,  True,  True, False,  True,  True, False],\n[False, False, False, False, False,  True, False, False, False]]\n所以，这个函数的作用就很明显了。如果图片是单通道的图片，那么处理后的img就只有在画圆的地方的位置是true，其他为false ；如果是三通道的话，那就是蓝色通道的圆为true，其他为false。\n根据这段代码和你的描述，这应该是用来产生掩膜用的，如果thickness=-1，那么叠加在圆原图像上面的话就是将圆内的区域显示出来，圆外的区域为黑色。如果thickness>=1,那么就只会显示在圆上的点，其他区域为黑色。\n ", "Konwledge_Point": "应对NP完全问题", "Question": "OpenCV的一段代码问题\n这是在学习一位博主的一段代码，旨在获取中心⚪部分的色域集，最后两行的目的是什么意思非常疑惑，求求各位好哥哥姐姐指导一下！\n\n\n\n\n\ndef get_circle_field_color(img,center,r,thickness):\n    '''获取中心圆形区域的色值集'''\n    temp=img.copy().astype(np.int)\n    cv2.circle(temp,center,r,-100,thickness=thickness)\n    return img[temp == -100]", "Tag": "算法分析"}
{"Answer": "不同的类都实现了相同的方法而已。用哪一个要看你当前用的是二维数组还是numpy库了。用二维数组的时候，就没必要用numpy库去搞了。世界上很多事都不止一种办法，很正常。条条道路通罗马。用哪个方法好，要看你当时的具体场景，选择最合适和方便的才是最好的", "Konwledge_Point": "应对NP完全问题", "Question": "这两种写法有什么不同吗，为什么有的函数有这样两种写法，有的函数就不行\na.reshape（2，10）\nnp.reshape（a，（2，10））\n其中a是一个二维数组，np是numpy库，这两种方法都能将数组的形状改成两行十列，但是有什么区别吗，记起来好麻烦", "Tag": "算法分析"}
{"Answer": "第十行代码你把time包覆盖了。", "Konwledge_Point": "应对NP完全问题", "Question": "我想将代码的计算时间存进列表，然后输出图，但是不知道为什么这段代码会报错，求指导。\n问题遇到的现象和发生背景\n\n\n问题相关代码，请勿粘贴截图\n\n\nfor\n i \nin\n \nrange\n(\n10\n):\n    ni=\n4\n+i*\n4\n\n    nj=\n3\n+i*\n3\n\n    criterion1=\n0.001\n*\nnp\n.ones((ni,nj))\n    criterion2=\n0.001\n*\nnp\n.ones((nj,ni))\n    plt_num.\nappend\n(ni+nj)\n    start = \ntime\n.\ntime\n()\n    plt_k.\nappend\n(fun1(ni,nj,\ngamma\n,eta,phi))#保存迭代次数\n    end = \ntime\n.\ntime\n()\n    \ntime\n=end-start\n    plt_time.\nappend\n(\ntime\n)\n\n\n\n\n运行结果及报错内容\n\n\nTraceback (most recent \ncall\n last):\n  File \"C:\\Users\\30575\\Desktop\\21个参与者.py\", \nline\n \n78\n, \nin\n \n    start = \ntime\n.time()\nAttributeError: \n'float'\n \nobject\n has \nno\n \nattribute\n \n'time'\n\n\n", "Tag": "算法分析"}
{"Answer": "import pandas as pd\nimport numpy as np\n\n\ndef copy(df, row, exist):  # 自定义函数，用以根据计算结果进行行的复制任务\n    idx = []\n    for r in range(exist):\n        idx.append(row)\n    return df.iloc[idx, :]\n\n\nDF = pd.read_csv(\"test.csv\", header=0)\ntau = np.array(DF[['tau']])\n\"\"\"\n源文件的内容是这样的\n   date  time     tau\n0     1    11  [1, 3]\n1     2    22  [3, 4]\n2     3    33  [5, 5]\n\"\"\"\n\ndfs = []\nfor t in range(len(tau)):  # 遍历每一行，计算应产生几个表，然后将各部分存在dfs列表里\n    left = eval(tau[t][0])[0]\n    right = eval(tau[t][0])[1]\n    compare = right - left\n    EXIST = compare + 1\n    dfs.append(copy(DF, t, EXIST))\n\nresult = dfs[0]\nfor index in range(1, len(dfs)):  # 拼接每个在dfs中的子结构\n    step = dfs[index]\n    result = pd.concat([result, step])\n\nresult.reset_index(drop = True, inplace = True)  # 结果是拼凑而成的，所以可以重置以消除重复的索引\n\nprint(result)  # to_csv()也可以写\n\"\"\"\n运行后的结果是这样的\n   date  time     tau\n0     1    11  [1, 3]\n1     1    11  [1, 3]\n2     1    11  [1, 3]\n3     2    22  [3, 4]\n4     2    22  [3, 4]\n5     3    33  [5, 5]\n\"\"\"\n", "Konwledge_Point": "应对NP完全问题", "Question": "根据条件向dataframe中增加行\n问题遇到的现象和发生背景\n\n\n这里，对tau这一列，如果tau里面，右边的-左边的=1则增加一行，并且增加的一行为重复本行的值。\n即对第一个数据[1,3]，3-1=2，所以重复两行数据。变成下图\n\n\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport\n numpy \nas\n np\n\nimport\n pandas \nas\n pd\n\nimport\n csv\n\nfrom\n scipy.stats \nimport\n norm\n\nfrom\n pandas \nimport\n DataFrame\ntau = pd.read_csv(\"C:/Users/DELL/Desktop/dthu.csv\", \nheader\n=\n0\n, usecols=[\n'tau'\n])\ntau = np.\narray\n(tau)\n\nbuck=[]\n\nfor\n x,y \nin\n range(len(tau)):\n    \nif\n y-x==\n0\n:\n        \ncontinue\n\n    \nif\n y-x!=\n0\n:\n\n\n\n\n运行结果及报错内容\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "\nIt looks like you'll need to use AND instead of the comma.\n\"... AND NP.P_ID=NM.P_ID AND N.N_ID='\".$_GET['nurse_id'].\"';\");\n\nIf the nurse id is an integer, you don't need the single quotes:\n\"... AND NP.P_ID=NM.P_ID AND N.N_ID=\".$_GET['nurse_id'].\";\");\n\nI strongly recommend that you sanitize $_GET values before using them to query the database. Even better, use prepared statements.\n", "Konwledge_Point": "应对NP完全问题", "Question": "如何使用PHP将HTML输入与mysql数据库进行比较？  [关闭]\n\n\n\n$query = \"\nSELECT N.N_ID\n     , P.P_ID\n     , P.P_F_NAME\n     , P.P_L_NAME\n     , P.P_ADDR\n     , NP.VISIT_TIME\n     , N.N_LN_NAME\n     , N.N_SPEC\n     , M.M_DESC\n     , NM.M_DOSE \n  FROM NURSE N\n     , patient P\n     , n_visit_p NP\n     , MEDIC M\n     , n_provide_m NM \n WHERE N.N_ID  = NP.N_ID \n   AND P.P_ID  = NP.P_ID \n   AND M.M_ID  = NM.M_ID \n   AND NP.N_ID = NM.N_ID \n   AND NP.P_ID = NM.P_ID\n     , **N.N_ID='{$_GET[\"nurse_id\"]}'**;\n     \");\n\n $result = mysqli_query($con,$query);\n\n\n\n\nThis is a part of my code, where i tried to compare the HTML input through $_GET....\n\"nurse_id\" is the form name in HTML through which i am receiving input..\nI even tried it with \n$N_ID= $_GET[\"nurse_id\"]\"\n\nbut its not working either way... \nthe result that i am getting is all nurse details but not the specific selected nurse...\n\n    ", "Tag": "算法分析"}
{"Answer": "版本问题，参考：\npython - AttributeError：'GMM'对象没有属性'covariances_'|| AttributeError：'module'对象没有属性'GaussianMixture' - Thinbug\n我有一段代码可以适合我的数据的guassian模型。我从sklearn进口了混合物。然而，即使我使用mixture.GaussianM\n\n\n\nhttps://www.thinbug.com/q/49375927\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "'GaussianMixture' object has no attribute 'covariances_'\n'GaussianMixture' object has no attribute 'covariances_'\n\n\ngmm = GaussianMixture(5)\ngmm.means_ = np.array([[10], [20], [60], [80], [110]])\ngmm.covars_ = np.array([[3], [3], [2], [2], [1]]) ** 2\ngmm.weights_ = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n\n\nX = gmm.sample(200000)\n然后就报错 'GaussianMixture' object has no attribute 'covariances_'，请问这个怎么解决", "Tag": "算法分析"}
{"Answer": "你改一下散点的z坐标会发现,无论怎么改z的坐标都是0scatter函数z坐标默认是0，要先指定z为一个标量，即指定一个s，zs才能代表z轴坐标\nplt.scatter(1,1,zs=45.715999999999994,s=10)\n", "Konwledge_Point": "应对NP完全问题", "Question": "matplotib出3D图后，再做一个散点，发现散点并不在曲面上\n哪位帮我看看，我刚学matplotlib\n\n\nimport\n matplotlib.pyplot as plt\n\nimport\n numpy as np\n\nX\n = np.linspace(\n0\n,\n4\n,\n100\n)\n\nY\n = np.linspace(\n0\n,\n4\n,\n100\n)\n\nX\n,Y = np.meshgrid(X,Y) #没加这个就报错，不是二维数组\n\n# Z = 5.99 + 17.438*X + 29.787*Y - 3.558*X*X + 0.357*X*Y -8.070*Y*Y \n\n\nZ\n = -\n48\n.\n810\n+\n37\n.\n557\n*X + \n130\n.\n130\n*Y +\n8\n.\n389\n*X*X - \n33\n.\n166\n*X*Y - \n62\n.\n740\n*Y*Y - \n4\n.\n133\n*X*X*X +\n6\n.\n138\n*X*X*Y + \n2\n.\n566\n*X*Y*Y+\n9\n.\n785\n*Y*Y*Y\n\nax\n = plt.figure().add_subplot(projection='\n3\nd')\n\ntick\n = np.linspace(\n0\n,\n4\n,\n8\n)\n\n# Plot the 3D surface\n\n\nax\n.plot_surface(X, Y,Z,cmap = 'rainbow')\n\nplt\n.xticks(tick)\n\nplt\n.yticks(tick)\n\nplt\n.scatter(\n1\n,\n1\n,\n45\n.\n715999999999994\n)#图上某个点的坐标\n\nplt\n.show()\n\n\n\n一开始没加x,y = np.meshgrid(x,y）报错\n加上之后就出了图\n然后我就想看看这图对不对，\n就在图上做了一个点的位置，\n然后我看图那个点并不在曲面上，\n\n", "Tag": "算法分析"}
{"Answer": "基于Monster 组和GPT的调写：这段代码是在实现从数组a中取出指定索引位置的元素，其中索引位置由数组index中的元素确定。具体来说，index是一个包含3个随机整数的一维数组，每个整数都是在0到a数组长度之间随机生成的，因此它表示了a数组中任意3个元素的索引位置。a[index]的结果是一个一维数组，其中的元素分别是a数组中对应索引位置上的元素，也就是a[0]、a[2]和a[3]。因此，这行代码输出的是a数组中索引位置为0、2、3的元素，即[1, 3, 4]。\n需要注意的是，这里的乘法符号 * 并不是表示两个数组相乘，而是表示将index数组作为a数组的索引来取值，这种操作在numpy中称为“花式索引”（fancy indexing）。", "Konwledge_Point": "应对NP完全问题", "Question": "向您请教：数组的访问\n请问index    #  array([0,2,3])\na[index]    #    array([1,3,4])\n这是在干什么？\nindex和a数组相乘？什么意思？\n\n\n\n\n#数组的访问与其他语言类似，但是支持同时访问多个元素。\n\n\na\n=np.array(\n1\n,\n2\n,\n3\n,\n4\n,\n5\n)\n\nindex\n=np.array(np.random.randint(\n0\n,len(a),\n3\n))\n\nindex\n    #  array([\n0\n,\n2\n,\n3\n])\n\na\n[index]    #    array([\n1\n,\n3\n,\n4\n])\n\n", "Tag": "算法分析"}
{"Answer": "\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nmu, sigma = 100, 20\na = np.random.normal(mu,sigma,size=100)\n\nplt.hist(a, 20,histtype='stepfilled',facecolor='b',alpha=0.75)\nplt.title('Histogram')\n\nplt.show()\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "plt.hist()测试\nmatplotlip.pyplot画图，图像标题无法显示出来\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nnp.random.seed(0)\nmu, sigma = 100, 20\na = np.random.normal(mu,sigma,size=100)\n\n\nplt.hist(a, 20,normed=1,histtype='stepfilled',facecolor='b',alpha=0.75)\nplt.title('Histogram')\n\n\nplt.show()\n\n\n参照这个解决了\nhttps://www.freesion.com/article/46451414816/\n\n\nnormed=1 表示归一化？将频数转化成频率？", "Tag": "算法分析"}
{"Answer": "参考一下这里的解决方法：https://blog.csdn.net/liangjiu2009/article/details/104371329", "Konwledge_Point": "应对NP完全问题", "Question": "concatenate()出错：ValueError: all the input arrays must have same number of dimensions\n\nu_c_new = clf1.predict(u_d)  \n# 这里直接使用有标签数据训练得到的SVM模型对无标签数据进行分类，将其分类结果作为无标签数据的类别\n\ncu, cl = 0.0001, 1           \n# 初始化有标签数据无标签数据重要程度的折中【参数】\n\nsample_weight = np.ones(n)   \n# 样本权重， 直接让有标签数据的权重为Cl,无标签数据的权重为Cu\n\n\nprint\n(u_c_new.shape)\n\nprint\n(\ntype\n(u_c_new))\n\nprint\n(l_c.shape)\n\nprint\n(\ntype\n(l_c))\n\n# sample_weight[len(l_c):] = cu\n\n\n# id_set = np.arange(len(u_d))\n\nlu_c = np.concatenate((l_c, u_c_new))\n\n\n\n结果为：\n\n\n(113, 1)\n\n(60, 1)\n\nTraceback (most recent call last):\n  File \n\"E:/PYTHON/PYCHARM/Demo/TSVM2.py\"\n, line 48, in \n    lu_c = np.concatenate((l_c, u_c_new))\n  File \n\"<__array_function__ internals>\"\n, line 6, in concatenate\nValueError: all the input arrays must have same number of dimensions, but the\n array \nat index 0 has 2 dimension(s)\n and \nthe\n array \nat index 1 has 1 dimension(s)\n\n\n\n从Debugger中看到u_c_new：{ndarray:(113,)}   ; 而l_c：{ndarray:(60,1)} ,这是为什么呀？怎么解决呀？", "Tag": "算法分析"}
{"Answer": "astype全部小写", "Konwledge_Point": "应对NP完全问题", "Question": "AttributeError: 'numpy.ndarray' object has no attribute 'asType'\nAttributeError: 'numpy.ndarray' object has no attribute 'asType'\n\n\ntopHat = cv2.morphology\nEx(\ngray\n, \ncv2\n.MORPH_TOPHAT, \nrectKernel\n)\n\ngradX = cv2.\nSobel(\ntopHat\n, \nddepth\n=\ncv2\n.CV_32F, \ndx\n=1, \ndy\n=0,\nksize\n=-1)\n \ngradX = np.absolute(gradX)\n(minVal, maxVal) = (np.min(gradX), np.max(gradX))\ngradX = (\n255\n*((gradX - minVal)\n / \n(maxVal - minVal)))\ngradX = gradX.\nas\nType(\n\"uint8\"\n)\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "39行的array改成全局的再运行看看\narray = [[0] * 1] * 165312\n", "Konwledge_Point": "应对NP完全问题", "Question": "Python 栈溢出 Process finished with exit code -1073741571 (0xC00000FD)\n问题遇到的现象和发生背景\n\n\n运行时出现栈溢出的现象，试过网上大家说的修改栈大小、深度等，但都没有解决问题。我的数据有16万多条，不知道是不是数据太大的问题，还是其他问题。\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport\n numpy \nas\n np\n\nimport\n sys\n\nimport\n os\n\nfrom\n osgeo \nimport\n gdal\n\nfrom\n sklearn \nimport\n ensemble\n\nfrom\n sklearn.model_selection \nimport\n train_test_split\n\nimport\n joblib\n\nfrom\n tqdm \nimport\n tqdm\n\nimport\n time\n\n\n# 显示进度条\n\ntext = \n\"\"\n\n\nfor\n char \nin\n tqdm([\n\"a\"\n, \n\"b\"\n]):\n    text = text + char\n    time.sleep(\n0.5\n)\n\n\n\n# 读取tif数据\n\n\ndef\n \nRead_img2array\n(\nimg_file_path\n):\n    \n\"\"\"\n    读取栅格数据，将其转换成对应数组\n    img_file_path: 栅格数据路径\n    :return: 返回投影，几何信息，和转换后的数组\n    \"\"\"\n\n    dataset = gdal.Open(img_file_path)  \n# 读取栅格数据\n\n    \nprint\n(\n'处理图像波段数总共有：'\n, dataset.RasterCount)\n    \n# 判断是否读取到数据\n\n    \nif\n dataset \nis\n \nNone\n:\n        \nprint\n(\n'Unable to open *.tif'\n)\n        sys.exit(\n1\n)  \n# 退出\n\n   \n# 直接读取dataset\n\n    img_array = dataset.ReadAsArray()\n    \nreturn\n img_array\n\n\n# tif -> array\n\n\ndef\n \nread_tif_array\n(\npath, filetype\n):\n    pathDir = os.listdir(path)  \n# 文件放置在当前文件夹中，用来获取当前文件夹内所有文件目录\n\n    i = \n0\n\n    array = [[\n0\n] * \n1\n] * \n165312\n\n    array = np.array(array)\n    array = array.reshape(-\n2\n, \n1\n)\n    \nfor\n x \nin\n pathDir:\n        index = x.rfind(\n'.'\n)\n        \nif\n x[index:] == filetype:\n            img_array = Read_img2array(path + \n\"/\"\n + x)\n            mul = np.array(img_array).reshape(-\n2\n, \n1\n)\n            array = np.column_stack((array, mul))\n            i = i + \n1\n\n        \nelse\n:\n            i = i\n    array = array[:, \n1\n:]\n    \nreturn\n array\n\n\n# 读取特征值\n\nfeature = read_tif_array(\n\"D:/Personality/paper/GBDT/train\"\n, \n'.tif'\n)\n\n# print(feature.shape)  # 一共有15个特征值\n\n\n\n# 读取地质类为标签\n\nlabel = read_tif_array(\n\"D:/Personality/paper/GBDT/label\"\n, \n'.tif'\n)\nlabel = label.ravel()\n\n\nprint\n(feature.shape, label.shape)\n\nX, y = feature, label\nlabels, y = np.unique(y, return_inverse=\nTrue\n)  \n# 标签\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)  \n# 创建数据集\n\n\noriginal_params = {\n    \n\"n_estimators\"\n: \n400\n,\n    \n\"max_leaf_nodes\"\n: \n4\n,\n    \n\"max_depth\"\n: \n6\n,\n    \n\"random_state\"\n: \n2\n,\n    \n\"min_samples_split\"\n: \n5\n,\n}  \n# 设置树的基本参数，用于后面计算\n\nsetting = {\n\"learning_rate\"\n: \n0.2\n, \n\"subsample\"\n: \n1.0\n}\nparams = \ndict\n(original_params)  \n# 转化为字典\n\nparams.update(setting)  \n# 更新字典键值对\n\nclf = ensemble.GradientBoostingClassifier(**params)  \n# 梯度\n\nclf.fit(X_train, y_train)  \n# 训练数据集\n\njoblib.dump(clf, \n'train_model_result.m'\n)   \n# 保存模型\n\n\ny_gbr = clf.predict(X_train)\ny_gbr1 = clf.predict(X_test)\nacc_train = clf.score(X_train, y_train)\nacc_test = clf.score(X_test, y_test)\n\nprint\n(acc_train)\n\nprint\n(acc_test)\n\n\n\n\n\n运行结果及报错内容\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n我想要达到的结果", "Tag": "算法分析"}
{"Answer": "看到这里遇到了 TypeError: unsupported operand type(s) for +: 'builtin_function_or_method' and 'builtin_function_or_method' 的错误。\n这个错误通常是由于在代码中使用了不支持的操作符导致的。在这种情况下，可以试试使用加号（+）操作符来进行某些操作，但是操作数的类型不支持这个操作。\n在代码中，这个错误发生在下列代码的第 31 行：\ns[i,j],_,r[i,j],p[i,j],_,_ = linregress(sst[:,i,j],nao1[:])\n\n看起来 linregress 函数中的参数 x 和 y 都是 NumPy 数组，但是它们的类型并不是数组，而是 builtin_function_or_method。这就是为什么会发生错误的原因。\n要解决这个问题，需要检查 nao1 变量的类型，并确保它是一个 NumPy 数组。可能是因为忘记使用括号调用了 mean 函数，导致 nao1 的类型是函数。\n要修复这个问题，可以试试使用括号来调用 mean 函数，例如：\nnaod.append(nao[11+12*i:13+12*i].mean())\n\n这样 nao1 就应该是一个 NumPy 数组，不会再出现错误。望采纳。", "Konwledge_Point": "应对NP完全问题", "Question": "【python】TypeError: unsupported operand type(s) for +: 'builtin_function_or_method' and\n问题\n写代码的错误TypeError: unsupported operand type(s) for +: 'builtin_function_or_method' and 'builtin_function_or_method'\n\n\n源代码\n\n\nimport\n xarray \nas\n xr\n\nimport\n pandas \nas\n pd\n\nimport\n scipy.signal\n\nimport\n numpy \nas\n np\n\nimport\n datetime \nas\n dt\n\n\nfrom\n scipy.stats \nimport\n linregress\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n cartopy.crs \nas\n ccrs\n\n\n# 读取海温sst========\n\nds = xr.open_dataset(\nr'E:\\huibao\\task3\\HadISST_sst.nc'\n)\nds\nsst = ds[\n'sst'\n]\nsst = sst.loc[sst.time.dt.month.isin([\n12\n,\n1\n,\n2\n])].loc[\n'1979-12-01'\n:\n'2020-03-01'\n,\n'80'\n:\n'30'\n,\n'-100'\n:\n'40'\n]\nsst = sst.to_numpy()\nsst = sst[:,:,:].reshape(\n41\n,\n3\n,sst.shape[\n1\n],sst.shape[\n2\n]).mean(\n1\n)\n\n# 读取nao指数=========\n\nnaom = pd.read_csv(\nr'E:\\huibao\\task3\\NAO_monthly.txt'\n,header=\nNone\n) \nnao = scipy.signal.detrend(naom,\ntype\n=\n'linear'\n)\n#去趋势化\n\nnaod=[]\n\nfor\n i \nin\n \nrange\n(\n41\n):\n    naod.append(nao[\n11\n+\n12\n*i:\n13\n+\n12\n*i].mean)\nnao1 = np.array(naod)\nsst = np.array(sst)\ns,r,p=np.zeros((sst.shape[\n1\n],sst.shape[\n2\n])),np.zeros((sst.shape[\n1\n],sst.shape[\n2\n])),np.zeros((sst.shape[\n1\n],sst.shape[\n2\n]))\n\n# print(type(sst))\n\n\nfor\n i \nin\n \nrange\n(sst.shape[\n1\n]):\n    \nfor\n j \nin\n \nrange\n(sst.shape[\n2\n]):\n        s[i,j],_,r[i,j],p[i,j],_,_ = linregress(sst[:,i,j],nao1[:])\n\n\n\n\n出错代码\n\n\nfor\n \ni\n \nin\n range(sst\n.shape\n[1]\n):\n    \nfor\n j \nin\n range(sst\n.shape\n[2]\n):\n        s\n[i,j]\n,_,r\n[i,j]\n,\np\n[i,j]\n,_,_ = linregress(sst\n[:,i,j]\n,nao1\n[:]\n)\n\n\n\n出现错误\n\n\nTypeError                                 Traceback (most recent call last)\n\nInput\n \nIn\n [17], \nin\n ()\n     29 \nfor\n i \nin\n \nrange\n(sst.shape[1]):\n     30     \nfor\n j \nin\n \nrange\n(sst.shape[2]):\n---> 31         s[i,j],_,r[i,j],p[i,j],_,_ = linregress(sst[:,i,j],nao1[:])\n\n\nFile\n \nD\n:\\anacconda\\envs\\myenv1\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:155, \nin\n linregress(x, y, alternative)\n    153 \nn\n = len(x)\n    154 xmean = np.\nmean\n(x, None)\n--> 155 ymean = np.\nmean\n(y, None)\n    157 # Average sums of square differences from the \nmean\n\n    158 #   ssxm = \nmean\n( (x-\nmean\n(x))^2 )\n    159 #   ssxym = \nmean\n( (x-\nmean\n(x)) * (y-\nmean\n(y)) )\n    160 ssxm, ssxym, _, ssym = np.cov(x, y, \nbias\n=1).flat\n\n\nFile\n <__array_function__ internals>:180, \nin\n \nmean\n(*\nargs\n, **kwargs)\n\n\nFile\n \nD\n:\\anacconda\\envs\\myenv1\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3474, \nin\n \nmean\n(a, axis, dtype, \nout\n, keepdims, where)\n   3471     \nelse\n:\n   3472         \nreturn\n \nmean\n(axis=axis, dtype=dtype, \nout\n=\nout\n, **kwargs)\n-> 3474 \nreturn\n _methods._mean(a, axis=axis, dtype=dtype,\n   3475                       \nout\n=\nout\n, **kwargs)\n\n\nFile\n \nD\n:\\anacconda\\envs\\myenv1\\lib\\site-packages\\numpy\\core\\_methods.py:179, \nin\n _mean(a, axis, dtype, \nout\n, keepdims, where)\n    176         dtype = mu.dtype('f4')\n    177         is_float16_result = True\n--> 179 \nret\n = umr_sum(arr, axis, dtype, \nout\n, keepdims, where=where)\n    180 \nif\n isinstance(\nret\n, mu.ndarray):\n    181     \nret\n = um.true_divide(\n    182             \nret\n, rcount, \nout\n=\nret\n, casting='unsafe', subok=False)\n\nTypeError: unsupported operand \ntype\n(s) \nfor\n +: 'builtin_function_or_method' and 'builtin_function_or_method'\n\n\n\n开始时，以为产生上述错误的原因是：虽python变量没有提前声明，但定义了nao1 = np.array(naod)\nsst = np.array(sst)后仍产生相同错误，实在不清楚应该如何解决，希望可以有人帮忙指点一下，感激不尽。", "Tag": "算法分析"}
{"Answer": "不要用tf.cast，用np的函数\n解决Cannot convert a symbolic Tensor to a numpy array._yulanf的博客-CSDN博客_symbolic tensor\n今天做Tensorflow的手写体识别模型，用的Lenet5,跑完之后想自己输入个图片测试一下，处理图片并导入数据的时候一直报错，Cannot convert a symbolic Tensor to a numpy array.最后发现reshaped_xs = np.reshape(xs_c, (BATCH_SIZE, mnist_inference_Lenet5_update.IMAGE...\n\n\n\nhttps://blog.csdn.net/yulanf/article/details/104494492\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "NotImplementedError: Cannot convert a symbolic Tensor (truediv:0) to a numpy array.\n用训练好的模型识别自己手写的图片时报错\ndef load_preprosess_image(path):\n    img_raw = tf.io.read_file(path)\n    img_tensor = tf.cast(tf.image.decode_jpeg(img_raw, channels=1), tf.float32)\n    img_tensor = tf.image.resize(img_tensor, [28, 28])\n    img = img_tensor / 255.0\n    return img\n\n\nif \nname\n == '\nmain\n':\n    app = Painter('./img/')\n    app.run()\n    ID = []\n    for count in range(app.count):\n        img = load_preprosess_image('./img/' + str(count) + '.jpg')\n\n\n    np.expand_dims(img, \naxis\n=-1)\n\n    model = tf.keras.models.load_model(\n'./model.h5'\n)\n    prediction = model.predict(img.numpy().reshape(1, 28, 28, 1))\n    result = np.argmax(prediction)\n    ID.append(result)\n\nprint\n(\n'识别的数字为：'\n, \nend\n=\n''\n)\n\nfor\n item \nin\n ID:\n    \nprint\n(item, \nend\n=\n''\n)\n\n\n\n环境Python3.6，tensorflow1.15，看到网上说是numpy版本的问题，我卸载分别重新装了1.17，1.18，1.19版本，还是报错，", "Tag": "算法分析"}
{"Answer": "1）你可以自己用简单的几何问题，下图只是一个例子（不全面）2）你也可以完全用用opencv的功能，下图来自Stack Overflow", "Konwledge_Point": "应对NP完全问题", "Question": "左右倾斜图像 两种情况下 都得出w大于h angle大于0\n问题遇到的现象和发生背景\n\n\n左右倾斜图像 最小矩形框绘制正确\n两种情况下 都得出w大于h angle大于0\n原理上 根据下面的图 左斜应该wh angle在[-90,0）之间\n\n\n问题相关代码，请勿粘贴截图\n\n\nimport cv2\nimport numpy as np\n\n\n读取原图像\n\n\nimg = cv2.imread('C:/Users/1.png')\n\n\n读取灰度图像\n\n\nimg1 = cv2.imread('C:/Users/1.png', 0)\n\n\n使用中值滤波\n\n\nimg2 = cv2.medianBlur(img1, 15)\n\n\n二值化\n\n\nret, thresh1 = cv2.threshold(img2, 127, 255, cv2.THRESH_BINARY)\n\n\n形态学运算中的开运算（opening）:先腐蚀再膨胀\n\n\nkernel = np.ones((5, 5), np.uint8)\nopening = cv2.morphologyEx(thresh1, cv2.MORPH_OPEN, kernel)\n\n\nopening = ~opening\n\n\ncontours, hierarchy = cv2.findContours(opening, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\ncnt = contours[0]\nrect = cv2.minAreaRect(cnt)\nbox = cv2.boxPoints(rect)\nbox = np.int0(box)\ncv2.drawContours(img, [box], 0, (0, 0, 255), 2)\nx, y, w, h = cv2.boundingRect(cnt)  # （x,y）是旋转的边界矩形左上角的点，w ,h分别是宽和高\n\n\nangle = int(rect[2])\nprint(angle)\nprint(w, h)\n\n\n运行结果及报错内容\n\n\n4\n1014 327\n\n\n\n\n\n\n我的解答思路和尝试过的方法\n\n\n参考 \nhttps://blog.csdn.net/vansbred/article/details/112312409\n\n\n我想要达到的结果\n\n\n希望可以分辨出左右斜", "Tag": "算法分析"}
{"Answer": "\nnumpy 造成的，重装 numpy, 试试。", "Konwledge_Point": "应对NP完全问题", "Question": "tensorflow运行问题\n有大佬遇到过这个问题吗，望指点\n\n\n\n\n\n\n\nAttributeError                            Traceback (most recent call last)\n in ()\n----> 1 import tensorflow as tf\n\nF:\\Program Files (x86)\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in ()\n     26 \n     27 # pylint: disable=g-bad-import-order\n---> 28 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n     29 from tensorflow.python.tools import module_util as _module_util\n     30 \n\nF:\\Program Files (x86)\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in ()\n     45 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\n     46 \n---> 47 import numpy as np\n     48 \n     49 from tensorflow.python import pywrap_tensorflow\n\nF:\\Program Files (x86)\\anaconda3\\lib\\site-packages\\numpy\\__init__.py in ()\n    140     from . import _distributor_init\n    141 \n--> 142     from . import core\n    143     from .core import *\n    144     from . import compat\n\nF:\\Program Files (x86)\\anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py in ()\n     55 from . import umath\n     56 from . import _internal  # for freeze programs\n---> 57 from . import numerictypes as nt\n     58 multiarray.set_typeDict(nt.sctypeDict)\n     59 from . import numeric\n\nF:\\Program Files (x86)\\anaconda3\\lib\\site-packages\\numpy\\core\\numerictypes.py in ()\n    109 )\n    110 \n--> 111 from ._type_aliases import (\n    112     sctypeDict,\n    113     sctypeNA,\n\nF:\\Program Files (x86)\\anaconda3\\lib\\site-packages\\numpy\\core\\_type_aliases.py in ()\n     61         _concrete_typeinfo[k] = v\n     62 \n---> 63 _concrete_types = {v.type for k, v in _concrete_typeinfo.items()}\n     64 \n     65 \n\nF:\\Program Files (x86)\\anaconda3\\lib\\site-packages\\numpy\\core\\_type_aliases.py in (.0)\n     61         _concrete_typeinfo[k] = v\n     62 \n---> 63 _concrete_types = {v.type for k, v in _concrete_typeinfo.items()}\n     64 \n     65 \n\nAttributeError: 'tuple' object has no attribute 'type'\n", "Tag": "算法分析"}
{"Answer": "思路其实有多种，如你所说用户数据现在是可以上传了的，也就是说实际上你现在需要的是读取用户数据和你的数据，展示到同一个图表中，这个有很多种方案：1、类似于你现在的方案，把数据绘图保存图片，然后把图片展示到前端，这个不是很推荐2、利用pyecharts库，在后端把数据封装成option对象，然后给前端展示3、把后端数据处理一下，直接发到前端，前端用echarts组件展示以下是第3种方案的一个简单例子\nimport json\nfrom random import randint\nfrom flask import Flask, render_template\nimport numpy as np\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef index():\n    return render_template('show.html')\n\n@app.route(\"/getdata\")\ndef getData():\n    y1 = [randint(1,100) for i in range(1,10)]\n    y2 = [randint(1,100) for i in range(1,10)]\n    x = [1,2,3,4,5,6,7,8,9]\n    data = {}\n    data['x'] = x\n    data['y1'] = y1\n    data['y2'] = y2\n    print(data)\n    return json.dumps(data,ensure_ascii=False)\nif __name__ == \"__main__\":\n    app.run()\n\nshow.html\n<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Title</title>\n    <script src=\"https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js\"></script>\n    <script src=\"https://cdn.bootcdn.net/ajax/libs/echarts/5.3.3/echarts.min.js\"></script>\n    <script>\n        $(document).ready(function () {\n            var main = echarts.init(document.getElementById(\"main\"))\n            $.get('/getdata', function (resp) {\n                data = eval(\"(\" + resp + \")\")\n                option = {\n                    title: {\n                        text: 'Line'\n                    },\n                    tooltip: {\n                        trigger: 'axis'\n                    },\n                    legend: {\n                        data: ['y1', 'y2']\n                    },\n                    xAxis: {\n                        type: 'category',\n                        boundaryGap: false,\n                        data: data['x']\n                    },\n                    yAxis: {\n                        type: 'value'\n                    },\n                    series: [\n                        {\n                            name: 'y1',\n                            type: 'line',\n                            stack: 'Total',\n                            data: data['y1']\n                        },\n                        {\n                            name: 'y2',\n                            type: 'line',\n                            stack: 'Total',\n                            data: data['y2']\n                        }\n                    ]\n                };\n                main.setOption(option);\n            });\n        });\n    </script>\n</head>\n\n<body>\n    <div id=\"main\" style='width: 800px;height:500px'></div>\n</body>\n\n</html>\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "请问如何在网页上利用数据库的数据作图并展示出来\n目前设计了一个网页，用户可以通过excel上传数据到数据库，下一步我想在网页上增加一个按钮，点击这个按钮，用户可以把他所上传的数据与我的数据通过图片进行对比（python代码已经写好，就是不知道怎么应用到flask框架中在网页展示出来）。或者还有没有其他的解决办法，如果解决必定采纳！感谢！！（下面是我写的代码，其实就是机械地和路由结合起来，想的是作图保存好以后，一点击按钮，展示图片）\n\n\nfrom\n flask \nimport\n Flask, render_template, request\n\nimport\n pymysql\n\nimport\n xlrd\n\nimport\n numpy \nas\n np\n\nimport\n matplotlib \nas\n mp\n\n\n\n@app.route(\n\"/seedling/sevendays/figure1\"\n)\ndef index6():\n    kwargs = {\n        \n\"host\"\n: \n\"localhost\"\n,\n        \n\"port\"\n: 3306,\n        \n\"user\"\n: \n\"root\"\n,\n        \n\"passwd\"\n: \n\"10868325\"\n,\n        \n\"database\"\n: \n\"lxd\"\n,\n        \n\"charset\"\n: \n\"utf8\"\n\n    }\n    db = pymysql.connect(**kwargs)\n    cur = db.cursor()\n    sql1 = \n\"select R400,R401 from four_ss_seven where id=1;\"\n\n    cur.execute(sql1)\n    a = []\n    \nfor\n row \nin\n cur:\n        a.append(np.array(row))\n    arr1 = np.array(a).T\n    # \nprint\n(arr1)\n    # \nprint\n(arr1.shape)\n    sql2 = \n\"SELECT R400,R401 FROM four_ss_seven ORDER BY id DESC LIMIT 1;\"\n\n    cur.execute(sql2)\n    b = []\n    \nfor\n row \nin\n cur:\n        b.append(np.array(row))\n    arr2 = np.array(b).T\n    # \nprint\n(arr2)\n    # \nprint\n(arr2.shape)\n    com = np.array([arr1, arr2]).T\n    com1 = np.reshape(com, (601, 2))\n    # \nprint\n(com1)\n    # \nprint\n(com1.shape)\n    arr3 = np.arange(400, 1001)\n    mp.figure(figsize=(8, 6), \ndpi\n=100)\n    mp.plot(arr3, com1, \nlinestyle\n=\n'-'\n)\n    mp.xlabel(\ns\n=\n'wavelength'\n, \nfontsize\n=18)\n    mp.ylabel(\ns\n=\n'reflectance'\n, \nfontsize\n=18)\n    mp.title(\ns\n=\n'spectral reflectance curve'\n, \nfontsize\n=25)\n    mp.savefig(\n'./static/img/1.png'\n)\n    return render_template(\n\"sd_seven_fig1.html\"\n)\n", "Tag": "算法分析"}
{"Answer": "你的矩阵是numpy 格式吧。用这个函数试试？这个是我的例子\n\nd1=np.array([[1,2,3,4,5,1],[1,2,3,4,5,2],[1,2,3,4,5,3],[1,2,3,4,5,4],[1,2,3,4,5,5],[1,2,3,4,5,6],[1,2,3,4,5,7],[1,2,3,4,5,8]])\nd2=np.array([[1,2,3,4,5,1],[1,2,3,4,5,2],[1,2,3,4,5,3]])\nprint(\"原始：\")\nprint(d1)\nprint(d2)\ndef reshape(d):\n    m,n=d.shape\n    if m>=5:\n        return d[0:5,:]\n    else:\n        return np.row_stack((d,[[0]*n]*(5-m)))\n        \nd1=reshape(d1)\nd2=reshape(d2)\nprint(\"转换后：\")\nprint(d1)\nprint(d2)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "矩阵的维数问题（想统一不同市场音频mfcc的维数）。\n\n\ndef get_wav_mfcc(wav_path):#(wav_path)\n    y,sr = librosa.load(wav_path)\n    wav_feature = mfcc( y, sr, numcep=13, winlen=0.025, winstep=0.01,\n                       nfilt=26, nfft=1024, lowfreq=0, highfreq=None, preemph=0.97)\n    print(wav_feature.shape)\n 输出：由于自己音频有的市场不同，所以shape有（80，13）、（99，13）（122，13）等\n    total_mfcc = []\n    d_mfcc_feat = delta(wav_feature, 1)\n    print('一阶差分mfcc:', d_mfcc_feat.shape)\n 输出：shape同上：（80，13）、（99，13）（122 ，13）等\n    d_mfcc_feat2 = delta(wav_feature, 2)\n    print('二阶差分mfcc:', d_mfcc_feat2.shape)\n 输出：shape同上：（80，13）、（99，13）（122，13）等\n    feature = np.hstack((wav_feature, d_mfcc_feat, d_mfcc_feat2))\n    print(feature.shape)\n 输出：shape：（80，39）（99，39）（122，39）等\n    total_mfcc.extend(feature)\n    total_mfcc = np.array(total_mfcc)\n    print(total_mfcc)\n 输出：（80，39）（99，39）（122，39）等形式的矩阵\n\n最终想要实现，通过补0或者其他方式，使得输出的矩阵统一成（99，39），需要接下来加些什么代 码？？？？？？\n下面是本人代码，想补0，没有成功\n    data = list(np.array( total_mfcc))\n    print('list:',data)#根据整个输出\n    while len(data)>122:#修改此数值\n        del data[len(waveData[0])-1]\n        del data[0]\n    # print(len(data))\n    while len(data)<122:\n        data.append(0)\n    print('add0:',data)\n    data=np.array(data)\n    return data\n", "Tag": "算法分析"}
{"Answer": "可以参照以下代码，（cmap参数: 为调整显示颜色  viridis是一种颜色组合，加_r取反）\n    ax1.imshow(I1, cmap=plt.cm.viridis_r)\n    ax2.imshow(I2, cmap=plt.cm.viridis_r)\n    ax3.imshow(I3, cmap=plt.cm.viridis_r)\n    ax4.imshow(I4, cmap=plt.cm.viridis_r)\n", "Konwledge_Point": "应对NP完全问题", "Question": "这个matplotlib图表，能不能把这个螺旋的颜色换一下（就是亮的地方变成暗的，暗的地方变成亮的）\n看看这个图片，我们来看第一个螺旋，中心是紫的，如何把它变成黄的，并且其他螺旋的颜色都把它们颠倒一下。\n这是我的代码\n\n\n\n\nimport\n math as m\n\nimport\n numpy as np \n\nimport\n matplotlib.pyplot as plt \n\ndef intensity(x, y, \nR=1.343,\n \nlamd=589.3e-9,\n \nDelta_d=0):\n\n    \nr2\n = x**\n2\n + y**\n2\n\n    \ntheta\n = m.pi*r2/(R*lamd) + (\n2\n*m.pi*Delta_d)/lamd\n    l, \nc\n = np.shape(theta)\n    \ni\n = np.zeros((l, c))\n    \ni\n = np.array([[m.sin(theta[i, j]) for j \nin\n range(c)] for i \nin\n range(l)])\n    print('\n1\n',i,type(i))\n    return i\n\ndef CalculateI(width, N):\n    \nx\n = np.linspace(-width, width, N)\n    \ny\n = np.linspace(-width, width, N)\n    [X, Y] = np.meshgrid(x, y)\n    \nI\n = intensity(-X, -Y)\n    print('\n2\n',I,type(I))\n    return I \n\ndef MakePlot():\n    \n# 建立子图\n\n    \nfig\n = plt.figure()\n    \nax1\n = fig.add_subplot(\n221\n)\n    \nax2\n = fig.add_subplot(\n222\n)\n    \nax3\n = fig.add_subplot(\n223\n)\n    \nax4\n = fig.add_subplot(\n224\n)\n\n    \n# 计算四种环的光强分布\n\n    \nI1\n = CalculateI(\n0.004\n, \n1000\n)\n    \nI2\n = CalculateI(\n0.003\n, \n1000\n)\n    \nI3\n = CalculateI(\n0.002\n, \n1000\n)\n    \nI4\n = CalculateI(\n0.001\n, \n1000\n)\n    \n#print(I1,I2,I3,I4)\n\n\n    \n# 作图\n\n    ax1.imshow(I1)\n    ax2.imshow(I2)\n    ax3.imshow(I3)\n    ax4.imshow(I4)\n    plt.show()\n\nMakePlot()\n\n\n\n\n\n能帮我改一下吗，谢谢！", "Tag": "算法分析"}
{"Answer": "以下答案引用自GPT-3大模型,请合理使用：\nimport cv2\nimport numpy as np\n \nimg = np.zeros([580, 500, 3],np.uint8)\nx=150\nwhile True:\n    cv2.rectangle(img,(200+x,200),(350+x,300),(255,0,0),2)\n    cv2.rectangle(img,(150+x,150),(200+x,200),(0,255,0),2)\n    cv2.imshow('img', img)\n    if (cv2.waitKey(5)==ord('q')): break\n    x+=1     #循环移动步长，可以调节\n    if (x>350): x=150  #控制矩形框在显示区域中不间断移动\n\n如果我的回答解决了您的问题，请采纳我的回答", "Konwledge_Point": "应对NP完全问题", "Question": "opencv图形移动\n条件：实现绿色矩形框从界面最左端移动到最右端，循环不间断移动，基础代码不能大改\n\n\n\n\nimport\n cv2\n\nimport\n numpy as np\n\n\nimg\n=np.zeros([\n580\n,\n500\n,\n3\n],np.uint8)\n\n\ncv2\n.rectangle(img,(\n200\n,\n200\n),(\n350\n,\n300\n),(\n255\n,\n0\n,\n0\n),\n2\n)\n\ncv2\n.rectangle(img,(\n150\n,\n150\n),(\n200\n,\n200\n),(\n0\n,\n255\n,\n0\n),\n2\n)\n\n\n\n\n\ncv2\n.imshow('img',img)\n\ncv2\n.waitKey(\n0\n)\n\ncv2\n.destroyAllWindows()\n\n\n\n\n\n\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "代码用控件提交，你这代码乱的.这个报错你要检查下是不是你优化器中loss的问题，你的数据标签是什么样子的，直接类别id的话不能用CategoricalCrossentropy()换成SparseCategoricalCrossentropy()或者binary_crossentropy试试看，如果你真的要用的话，要对标签进行编码才行。至于两者的区别，你可以看下链接https://blog.csdn.net/qq_40212975/article/details/108245786", "Konwledge_Point": "应对NP完全问题", "Question": "tensorflow怎么解决这个问题，是什么问题，解决方法？\n问题遇到的现象和发生背景 模型无法按照我想的运行，不知道是否是shape还是什么没搞好？\n\n\n问题相关代码，请勿粘贴截图\n\n\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D,MaxPool2D,Activation,Dropout,Flatten,Dense\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\nimport numpy as np\nimport tensorflow as tf\nimport pathlib\ndata_dir = tf.keras.utils.get_file(origin='\nhttps://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'\n,\n                                         fname='flower_photos', untar=True)\ndata_root = pathlib.Path(data_dir)\nprint(data_root)\nimport random\nall_image_paths=list(data_root.glob('\n/\n'))\nall_image_paths=[str(path) for path in all_image_paths]\nrandom.shuffle(all_image_paths)\nprint(len(all_image_paths))\nlabel_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())#读取目录并排序为类别名\nlabel_to_index = dict((name, index) for index, name in enumerate(label_names))#创建类别字典\nall_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n                    for path in all_image_paths] #图像parent path 对应类\n@tf.function\ndef preprocess_image(path):\n    image_size=224\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [image_size, image_size])\n\n\n# 数据增强\n\n\n\n\nx=tf.image.random_brightness(x, 1)#亮度调整\n\n\nx = tf.image.random_flip_up_down(x) #上下颠倒\n\n\nx= tf.image.random_flip_left_right(x) # 左右镜像\n\n\nx = tf.image.random_crop(x, [image_size, image_size, 3]) # 随机裁剪\n\n\nimage\n /= \n255\n.\n0\n  # normalize to\n [0,1] range\n\n\n\n\nimage= normalize(image) # 标准化\n\n\nreturn\n image\n\n\n\nds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\ndef load_and_preprocess_from_path_label(path, label):\n    return preprocess_image(path), label\n\n\nimage_label_ds = ds.map(load_and_preprocess_from_path_label)\nimage_label_ds\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 16\nimage_count = len(all_image_paths)\n\n\n设置一个和数据集大小一致的 shuffle buffer size（随机缓冲区大小）以保证数据\n\n\n被充分打乱。\n\n\nds = image_label_ds.shuffle(buffer_size=image_count) # buffer_size等于数据集大小确保充分打乱\nds = ds.repeat() #repeat 适用于next(iter(ds))\nds = ds.batch(BATCH_SIZE)\n\n\n当模型在训练的时候，\nprefetch\n 使数据集在后台取得 batch。\n\n\nds = ds.prefetch(buffer_size=AUTOTUNE)#随机缓冲区相关\nvgg16_model = VGG16(weights='imagenet',include_top=False, input_shape=(224,224,3))\nvgg16_model.summary()\n\n\n搭建全连接层\n\n\ntop_model = Sequential()\ntop_model.add(Flatten(input_shape=vgg16_model.output_shape[1:]))\ntop_model.add(Dense(256,activation='relu'))\ntop_model.add(Dropout(0.5))\ntop_model.add(Dense(5,activation='softmax'))\n\n\nmodel = Sequential()\nmodel.add(vgg16_model)\nmodel.add(top_model)\ndef change_range(image,label):\n  return 2*image-1, label\nkeras_ds = ds.map(change_range)\n\n\n数据集可能需要几秒来启动，因为要填满其随机缓冲区。\n\n\nimage_batch, label_batch = next(iter(keras_ds))\nfeature_map_batch = vgg16_model(image_batch)\nprint(feature_map_batch.shape)\n\n\n定义优化器，代价函数，训练过程中计算准确率\n\n\nmodel.compile(optimizer=SGD(lr=1e-3,momentum=0.9),loss='categorical_crossentropy',metrics=['accuracy'])\n\n\nmodel.fit(ds, epochs=1, steps_per_epoch=3)\n\n\n运行结果及报错内容\n\n\nmodel.compile(optimizer=SGD(lr=1e-3,momentum=0.9),loss='categorical_crossentropy',metrics=['accuracy'])\n\n\nmodel.fit(ds, epochs=1, steps_per_epoch=3)\n    ValueError: Shapes (None, 1) and (None, 5) are incompatible\n\n\n我的解答思路和尝试过的方法 将全连接层改了\n\n\ntop_model.add(Dense(5,activation='softmax'))改成top_model.add(Dense(1,activation='softmax'))\n可以运行- 8s 347ms/step - loss: 0.0000e+00 - accuracy: 0.2083\n\n但我想要输出5个分类\n\n\n我想要达到的结果可以对图片进行预测输出5个类别中的一个，就是全连接层输出为5个分类可以运行，可以预测，可以输出准确率，召回率，损失率三率", "Tag": "算法分析"}
{"Answer": "你好，逐行解释一下是干什么的。\ndef intersectionAndUnion(imPred, imLab, numClass):\n定义一个计算交集和并集的函数，这里的交集和并集是针对每个类别而言的。\nimPred = np.asarray(imPred).copy()\n复制一个预测值的拷贝副本。\nimLab = np.asarray(imLab).copy()\n复制一个真实值的拷贝副本。\n\n \n# 开始\n \nimPred = imPred * (imLab > 0)\nimLab>0返回一个bool类型的矩阵变量，然后与imPred进行逐个元素乘积的运算。\n由于是逐个元素进行的乘积运算，bool值为真的地方，Pred的数值被保留；为假的地方，Pred位置被置零。\n也就是说，只保留imPred中，与imLab>0处于同一位置的元素。\nintersection = imPred * (imPred == imLab)\nimPred==imLab同样也是返回bool类型的矩阵变量，然后做逐元素的乘法运算。\n这里intersection是保留了imPred预测值中，那些预测值与真实值相同的矩阵，对应位置的元素。\n \n(area_intersection, _) = np.histogram(intersection, bins=numClass, range=(1, numClass))\n直方图运算，注意bins=numClass，将数据分为了class类别，实际上就是对每个类别元素进行了一个统计，计算每一个类别的元素，被正确分类的有多少个。\narea_intersection是一个行向量，长度与类别的数目相等，每个元素的数值是对应类别被正确分类的个数。\n(area_pred, _) = np.histogram(imPred, bins=numClass, range=(1, numClass))\n同理，直方图运算，统计的是每个类别，被预测得到的数目。\n(area_lab, _) = np.histogram(imLab, bins=numClass, range=(1, numClass))\n直方图运算，统计的是每个类别，真实值所对应的数目。\n# 结束\n\n\n \narea_union = area_pred + area_lab - area_intersection\n这里就是一个并集运算了，A并B=A+B-A交B。\nreturn (area_intersection, area_union)", "Konwledge_Point": "应对NP完全问题", "Question": "计算语义分割结果IoU的函数，有几行不太理解，求解释\nGithub上的代码，作用是求语义分割结果的IoU，和IoU计算公式对比着看了实在不懂，请逐行解释开始结束注释之间的语句，谢谢\n\n\n\n\n\ndef intersectionAndUnion(imPred, imLab, numClass):\n    imPred = np.asarray(imPred).copy()\n    imLab = np.asarray(imLab).copy()\n\n# 开始\n    imPred = imPred * (imLab > 0)\n\n    intersection = imPred * (imPred == imLab)\n\n    (area_intersection, _) = np.histogram(intersection, bins=numClass, range=(1, numClass))\n    (area_pred, _) = np.histogram(imPred, bins=numClass, range=(1, numClass))\n    (area_lab, _) = np.histogram(imLab, bins=numClass, range=(1, numClass))\n# 结束\n\n    area_union = area_pred + area_lab - area_intersection\n    return (area_intersection, area_union)", "Tag": "算法分析"}
{"Answer": "你用np.isnan(y_train).all()检查一下，看是否有空值，再作处理。", "Konwledge_Point": "应对NP完全问题", "Question": "valueerror: input contains nan, infinity\n数据没有空值，不过用np.isfinite(y_train).all()和np.isinf(y_train).all()对训练集标签进行检测，两个函数检测出来都是False，想问下这是什么情况", "Tag": "算法分析"}
{"Answer": "感觉是len(train_data) // batch_size", "Konwledge_Point": "应对NP完全问题", "Question": "Tensorflow相关问题\n我的代码如下所示：\n\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\n\n\n#全局变量\n\nclasses = {\n'grass'\n:0,\n'soldiers'\n:1}#数据分类\n\npath_grass\n=\n'D:/AIR_space/data/grass'\n #草丛图片路径\n\npath_soldiers\n=\n'D:/AIR_space/data/soldiers'\n #士兵图片路径\npicture=[] #存放图片\nlabels=[] #存放标签\n\nfiles_grass\n=os.listdir(path_grass)\n\nfiles_soldiers\n=os.listdir(path_soldiers)\n\n\n#读取图片数据并且将其存入到图片和标签变量中\n\ndef read_image(path,files,shape=(32,32)):\n    \nfor\n f \nin\n files:\n        \nf_path\n=path+'/'+f\n        \nimg\n=cv2.imread(f_path)\n        \nimg\n=cv2.resize(img,shape)\n        \nimg\n=img.astype(np.float32)\n        picture.append(img)\n        \nsplit_f\n=f.split('_')\n        \nlabel_f\n=int(classes[split_f[0]])\n        labels.append(label_f)\n\n\n\n#建立dataset\n\ndef data_set(data,label):\n    \ntrain_data\n=tf.data.Dataset.from_tensor_slices(data)\n    \ntrain_labels\n=tf.data.Dataset.from_tensor_slices(label).map(lambda z: tf.one_hot(z,len(classes)))\n    \ntrain_dataset\n=tf.data.Dataset.zip((train_data,train_labels)).shuffle(1000).repeat(10).batch(256)\n    return train_dataset\n\n\n#建立CNN模型\n\ndef build_model():\n    \nmodel\n=tf.keras.Sequential()\n    #第一层卷积\n    model.\nadd\n(tf.keras.layers.Conv2D(64,(3,3),\npadding\n=\n'same'\n,activation='relu',input_shape=(32,32,3)))\n    model.\nadd\n(tf.keras.layers.MaxPooling2D(\npadding\n=\n'same'\n))\n    #第二层卷积\n    model.\nadd\n(tf.keras.layers.Conv2D(128,(3,3),\npadding\n=\n'same'\n,activation='relu'))\n    model.\nadd\n(tf.keras.layers.Conv2D(256,(3,3),\npadding\n=\n'same'\n,activation='relu'))\n    model.\nadd\n(tf.keras.layers.MaxPooling2D(\npadding\n=\n'same'\n))\n    #全连接层\n    model.\nadd\n(tf.keras.layers.Flatten())\n    model.\nadd\n(tf.keras.layers.Dense(64,\nactivation\n=\n'relu'\n))\n    model.\nadd\n(tf.keras.layers.Dense(32,\nactivation\n=\n'relu'\n))\n    model.\nadd\n(tf.keras.layers.Dropout(0.3))\n    model.\nadd\n(tf.keras.layers.Dense(2,\nactivation\n=\n'softmax'\n))\n    return model\n\n\n\n#主函数部分\n\nread_image(path_grass,files_grass)\nread_image(path_soldiers,files_soldiers)\n\ntrain_data\n=data_set(picture,labels)\n\nprint\n(train_data)\n\nmodel_cnn\n=build_model()\n\n#model_cnn.build(input_shape=[None,32,32,3])\n\nmodel_cnn.summary()\nmodel_cnn.compile(\noptimizer\n=tf.keras.optimizers.Adam(0.001),loss='binary_crossentropy',metrics=[\n'accuracy'\n])\nmodel_cnn.fit(train_data,\nbatch_size\n=19,epochs=20)\n\n\n\n\n数据集是192张图片，小兵96张，对应soldiers,草丛96张，对应grass，然后我在训练的时候，发现输出如下图：\n\n\n我很不理解epoch下面那个x/8的那个8表示什么，哪里来的，看参考书说表示训练集数量，可我加载的数据集是192张，求解释那个8是怎么回事，如何修改，谢谢啦", "Tag": "算法分析"}
{"Answer": "类似于这样操作，参考一下：\nimport pandas as pd \n\ndf=pd.DataFrame({\"1\":[0.3524,-0.3124,0.2257,-0.1214],\"2\":[0.6214,0.3217,-0.2111,-0.2389],\"3\":[0.0124,-0.01478,0.2013,0.1624],\"4\":[0.3333,0.2222,0.1111,-0.2131]},index=range(1,5))\nprint(df)\ndf0=[]\nfor x in df.index:\n    df0.append(df.loc[x,:])\nd=pd.concat(df0,keys=df.index)\n\nprint(d)\n\n1  1    0.35240\n   2    0.62140\n   3    0.01240\n   4    0.33330\n2  1   -0.31240\n   2    0.32170\n   3   -0.01478\n   4    0.22220\n3  1    0.22570\n   2   -0.21110\n   3    0.20130\n   4    0.11110\n4  1   -0.12140\n   2   -0.23890\n   3    0.16240\n   4   -0.21310\n", "Konwledge_Point": "应对NP完全问题", "Question": "输出的相关矩阵转换为一列\n我的问题很简单 就是如何将输出的相关矩阵\n\n\n 变成下列形式\n\n\n\n\n代码附上 求修改\nimport numpy as np\nimport pandas as pd\ndf = pd.read_csv(r'C:\\Users\\wxy\\Desktop\\re\\444.csv')\na=df.corr()\nprint(a)", "Tag": "算法分析"}
{"Answer": "建议百度查看下pandas的read_csv()函数参数的意思就知道了。前面的是路径，后面的index_col是将原来的列名设置为\"ssn\"至于coding换成点不行，那就是路径不对，这个涉及到相对路径，也就是相对于你运行目前代码的py文件所在的路径，习题那么写2的话，py文件就是datasets这个文件夹同一级，而你这么写你是在coding这个文件夹同一级，你的coding下面一级才是datasets。如果你搞不懂就设置绝对路径，也就是从盘符开始写起", "Konwledge_Point": "应对NP完全问题", "Question": "jupyter notebook  AI人工智能数据预处理中 数据脱敏 碰到的问题\nimport numpy as np\nimport pandas as pd \nimport datetime\ndfraw=pd\n.read_csv\n(\n'coding/datasets/contacts.csv'\n,index_col=\n'ssn'\n)\ndfraw\n.head\n()\n\ndf\n[\n'ssn'\n]\n=dfraw\n[\n'ssn'\n]\n.apply\n(\n    lambda ssn:ssn\n[:6]\n + \n'*'\n*\n8\n + ssn\n[14:]\n)\ndf\n.head\n()\n\n\n\n然后会提示出错KeyError: 'ssn'\n这是为什么呢？\n还有我想知道路径('coding/datasets/contacts.csv',index_col='ssn')代表了什么含义？ 为什么把coding换成 . 不可以。因为习题里面原路径是('./datasets/contacts.csv',index_col='ssn')，但是会报错", "Tag": "算法分析"}
{"Answer": "过大的list列表不建议直接放内存里呢，列表里不是所有的值都需要同时处理的话建议存储磁盘。", "Konwledge_Point": "应对NP完全问题", "Question": "如何在Python中存储二维大列表\n比如我现在会生成近千万个向量，每个向量都是100维的，我先前都是用列表存储的，即使我不断的改，可这个内存占用依然很高，模拟代码如下，几十个g一下就用完了，要怎么处理了，\n\n\nimport numpy as \nnp\n\ny = []\ndef f():\n    \nreturn\n \nnp\n.\nrandom\n.rand(\n100\n)\n\nfor\n i \nin\n \nrange\n(\n10000000\n):\n    y.\nappend\n(f())\n", "Tag": "算法分析"}
{"Answer": "题主可以试着写下,提供下思路:\n读取文件 pd.read_excel()筛选前10行, data.iloc[:10]一个图表中多个系列和多子图的做法, 下面这个写法可以参考下\n\n# 一个图多个系列模板\nplt.subplots(figsize=(8,4))\nx = np.arange(len(data['姓名']))\nplt.bar(x - 0.35/2, data['2018年'], width=0.35, label='2018年')\nplt.bar(x + 0.35/2, data['2019年'], width=0.35, label='2019年')\nplt.ylabel('销售额')\nplt.xlabel('姓名')\nplt.title('不同年份销售额')\nplt.xticks(np.arange(len(data['姓名'])), data['姓名'], rotation=0, fontsize=10) \n\n# 多子图模板\nfig,axes = plt.subplots(2,2,figsize=(12,8))\ndata['A'].plot.pie( ax = axes[0,0],autopct = '%1.1f%%',colormap='Blues')\ndata['B'].plot.pie( ax = axes[0,1],autopct = '%1.1f%%',colormap='Blues')\ndata['C'].plot.pie( ax = axes[1,0],autopct = '%1.1f%%',colormap='Reds')\ndata['D'].plot.pie( ax = axes[1,1],autopct = '%1.1f%%',colormap='Reds')\n ", "Konwledge_Point": "应对NP完全问题", "Question": "Matplotlib统计绘图 创建一个Python脚本，命名为test1.py，完成以下功能：？\n（希望能把运行结果图和py文件发出）感谢！\n\n\n\n（1）今有2018年1月前半个月的猪肉价格和牛肉价格数据，他们存在于一个Excel表格中，如下表所示。将其读入Python中并用一个数据框变量df来保存。分别绘制前10天的猪肉价格和牛肉价格走势图在同一个figure中，用一个2*1的子图分别绘制2018年1月前半个月的猪肉价格和牛肉价格走势图。\n\n\n\n\n\n\n日期\n\n\n\n猪肉价格\n\n\n\n牛肉价格\n\n\n\n2018/1/1\n\n\n\n11\n\n\n\n38\n\n\n\n2018/1/2\n\n\n\n12\n\n\n\n39\n\n\n\n2018/1/3\n\n\n\n11.5\n\n\n\n41.3\n\n\n\n2018/1/4\n\n\n\n12\n\n\n\n40\n\n\n\n2018/1/5\n\n\n\n12\n\n\n\n43\n\n\n\n2018/1/6\n\n\n\n11.2\n\n\n\n44\n\n\n\n2018/1/7\n\n\n\n13\n\n\n\n47\n\n\n\n2018/1/8\n\n\n\n12.6\n\n\n\n43\n\n\n\n2018/1/9\n\n\n\n13.5\n\n\n\n42.3\n\n\n\n2018/1/10\n\n\n\n13.9\n\n\n\n42\n\n\n\n2018/1/11\n\n\n\n13.8\n\n\n\n43.1\n\n\n\n2018/1/12\n\n\n\n14\n\n\n\n42\n\n\n\n2018/1/13\n\n\n\n13.5\n\n\n\n39\n\n\n\n2018/1/14\n\n\n\n14.5\n\n\n\n38\n\n\n\n2018/1/15\n\n\n\n14.8\n\n\n\n37.5\n\n\n\n（希望能把运行结果图和py文件发出）感谢！", "Tag": "算法分析"}
{"Answer": "根据报错信息可以看出，输入数据的类型为 torch.FloatTensor，而模型参数的类型为 torch.cuda.FloatTensor，两者不匹配导致出现了错误。这通常是因为在模型和输入数据之间存在设备（CPU和GPU）不匹配的情况。\n在加载模型时，可以指定 map_location 参数来将模型加载到CPU上，即使模型最初在GPU上训练。可以尝试如下代码来解决这个问题：\nmodel = torch.load('save_weights/savename', map_location=torch.device('cpu'))\n\n此外，还需要确保输入数据的类型与模型的输入匹配，可以使用 to 方法将输入数据移动到相同的设备上：\nimg = img.to(device)\n\n最后再运行代码看看是否能够成功。", "Konwledge_Point": "应对NP完全问题", "Question": "在模型训练的最后一步写测试代码时出现问题\n在学习使用自己的数据训练RenNet模型的最后一步 写测试代码时出现的问题\n\n\nimport\n os\n\nimport\n torch\n\nimport\n numpy \nas\n np\n\nimport\n pandas \nas\n pd\n\nfrom\n tqdm \nimport\n tqdm\n\nfrom\n PIL \nimport\n Image\n\nimport\n torch.nn.functional \nas\n F\n\nfrom\n torchvision \nimport\n datasets\n\nfrom\n torchvision \nimport\n transforms\n\ndevice = torch.device(\n'cuda:0'\n \nif\n torch.cuda.is_available() \nelse\n \n'cpu'\n)\n\nimg_path = \nr\"D:\\workspace\\demo\\output\\val\\三管塔\\44010650000000043119111511_20181119111521_AaoMTR.jpg\"\n\ndataset_dir = \n'output'\n\ntest_path = os.path.join(dataset_dir, \n'val'\n)\ntest_transform = transforms.Compose([transforms.Resize(\n256\n),\n                                     transforms.CenterCrop(\n224\n),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize(\n                                         mean=[\n0.485\n, \n0.456\n, \n0.406\n],\n                                         std=[\n0.229\n, \n0.224\n, \n0.225\n])\n                                     ])\ntest_dataset = datasets.ImageFolder(test_path, test_transform)\n\nclasses = [\n\"抱杆\"\n, \n\"不确定增高架\"\n, \n\"单管塔\"\n, \n\"地面支撑杆\"\n, \n\"角钢塔\"\n, \n\"楼面增高架\"\n, \n\"美化树\"\n, \n\"美化天线\"\n, \n\"美化外罩\"\n,\n           \n\"三管塔\"\n, \n\"市政路灯杆\"\n]\nframe = Image.\nopen\n(img_path)\nimg = test_transform(frame)\nimg = torch.unsqueeze(img,dim=\n0\n)\nmodel = torch.load(\n'save_weights/savename'\n)\nmodel = model.\neval\n().to(device)\n\n\nwith\n torch.no_grad():\n    \n# 前向传播\n\n    outputs = model(img)\n    \n# 只有一张图就挤压掉batch维度\n\n    outputs = torch.squeeze(outputs)\n    \n# 计算图片属于4个类别的概率\n\n    predict = torch.softmax(outputs, dim=\n0\n)\n    \n# 得到类别索引\n\n    predict_cla = torch.argmax(predict).numpy()\n\n\n# 获取最大预测类别概率\n\npredict_score = \nround\n(torch.\nmax\n(predict).item(), \n4\n)\n\n# 获取预测类别的名称\n\n\npredict_name = classes[predict_cla]\n\n\n\n\n上边是我的代码（还没有写完）但是运行时出现的问题是\n\n\nTraceback (most recent call last):\n  \nFile\n \n\"D:\\workspace\\demo\\测试.py\"\n, \nline\n 35, \nin\n \n    outputs = model(img)\n  \nFile\n \n\"C:\\Users\\12301\\.conda\\envs\\cj-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\n, \nline\n 1130, \nin\n _call_impl\n    \nreturn\n forward_call(*\ninput\n, **kwargs)\n  \nFile\n \n\"D:\\workspace\\demo\\Model.py\"\n, \nline\n 73, \nin\n forward\n    \nout\n = self.conv1(x)\n  \nFile\n \n\"C:\\Users\\12301\\.conda\\envs\\cj-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\n, \nline\n 1130, \nin\n _call_impl\n    \nreturn\n forward_call(*\ninput\n, **kwargs)\n  \nFile\n \n\"C:\\Users\\12301\\.conda\\envs\\cj-env\\lib\\site-packages\\torch\\nn\\modules\\conv.py\"\n, \nline\n 457, \nin\n forward\n    \nreturn\n self._conv_forward(\ninput\n, self.weight, self.\nbias\n)\n  \nFile\n \n\"C:\\Users\\12301\\.conda\\envs\\cj-env\\lib\\site-packages\\torch\\nn\\modules\\conv.py\"\n, \nline\n 453, \nin\n _conv_forward\n    \nreturn\n F.conv2d(\ninput\n, weight, \nbias\n, self.stride,\nRuntimeError: \nInput\n \ntype\n (torch.FloatTensor) and weight \ntype\n (torch.cuda.FloatTensor) should be the same or \ninput\n should be a MKLDNN tensor and weight is a dense tensor\n\n\n\n我的思路是将模型加载的GPU上  但是试了一下百度的方法解决不了\n我的python版本是3.9的", "Tag": "算法分析"}
{"Answer": "1、是不真实存在的2、Series和DataFrame3、不对，[0 1 2 3]4、对5、对", "Konwledge_Point": "应对NP完全问题", "Question": "数据分析 求解 怎么做\n1、在数组中,切片和索引只是数据的观测,不是真实存在吗？\n2、Pandas中引入了两种新的数据类型结构:\n和\n3、arr=np.arange(8) print('索引结果为:',arr[:4] 索引结果为_4\n 对吗？\n4、Python中,3**3的运算结果为_27_对吗？\n5、阅读下面的代码, 程序的执行结果是( A)import numpy as nparr2 = np.array([[1, 2, 3, 4],[4, 5, 6, 7], [7, 8, 9, 10]]) print(arr2.shape)print(arr2.size)（3.0）\nA、 (3,4 12 )\nB、 (4,3 12 )\nC、 (2,4 6 )\n选A对吗？", "Tag": "算法分析"}
{"Answer": "thresh1 = np.array([190, 0.35, 0.3]) # 目标的低阈值thresh2 = np.array([245, 1, 1]) # 目标的高阈值 这2行代码的问题，thresh1.dtype为float64，但是thresh2.dtype为int32，类型不一致导致的错误。要么把thresh1里的元素都改成整数型，要么把thresh2里的某一个元素改成浮点型，比如thresh2 = np.array([245, 1, 1.0])", "Konwledge_Point": "应对NP完全问题", "Question": "cv报错，cv2.inRange函数调用报错\n问题遇到的现象和发生背景\n\n\n定位到目标图像区域\n\n\n问题相关代码，请勿粘贴截图\n\n\nimg_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # 转化为 HSV 格式\nthresh1 = np.array([190, 0.35, 0.3]) # 目标的低阈值\nthresh2 = np.array([245, 1, 1]) # 目标的高阈值\nimg_1 = cv2.inRange(img_hsv, lowerb = thresh1, upperb = thresh2)\ncv2.imshow('原始图像', img) # 显示图像\ncv2.imshow('截取图像', img_1)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\n\n运行结果及报错内容\n\n\nTraceback (most recent call last):\nFile \"D:/Program Files/pythonProject233/6.py\", line 12, in\nimg_1 = cv2.inRange(img_hsv, lowerb = thresh1, upperb = thresh2)\ncv2.error: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:1782: error: (-215:Assertion failed) lb.type() == ub.type() in function 'cv::inRange'", "Tag": "算法分析"}
{"Answer": "只要报错信息里出现了NoneType，肯定是你有变量值是None了具体你这个，报错说None不是可迭代的对象那肯定就是lines是None了呗", "Konwledge_Point": "应对NP完全问题", "Question": "python报错，霍夫变化 'NoneType' object is not iterable，如何解决\nlines = cv2.HoughLines(edges, 1, np.pi / 180, 110)\nfor line in lines:\nTypeError: 'NoneType' object is not iterable", "Tag": "算法分析"}
{"Answer": "list没有values属性，如果y_train已被转换成了一个列表，可用pd.Series再转换成一维数组结构，就可用values属性。\nx=[1,2,3,5]\nprint(pd.Series(x).values)\n", "Konwledge_Point": "应对NP完全问题", "Question": "'list' object has no attribute 'values'\n运行fasttext形成文本的时候，出现了下面的问题：\n\n\n# coding=utf-8\n\n\nimport\n pandas \nas\n pd\n\nimport\n numpy \nas\n np\n\nimport\n fasttext\n\nfrom\n sklearn \nimport\n metrics\n\n\nwith\n \nopen\n(\n'train_abstract.txt'\n,\n'w'\n,encoding=\n'utf-8'\n) \nas\n f:\n    \nfor\n i \nin\n \nrange\n(\nlen\n(X_train.todense())):\n        str1 = \nstr\n(X_train.todense()[i])+\n\"\\t\"\n+\n\"__label__\"\n+\nstr\n(y_train.values[i])+\n'\\n'\n\n        f.write(str1)\n\n\nwith\n \nopen\n(\n'test_abstract.txt'\n,\n'w'\n,encoding=\n'utf-8'\n) \nas\n f:\n    \nfor\n i \nin\n \nrange\n(\nlen\n(X_test)):\n        str1 = \nstr\n(X_test.values[i])+\n\"\\t\"\n+\n\"__label__\"\n+\nstr\n(y_test.values[i])+\n'\\n'\n\n        f.write(str1)\n\n\n\n\nAttributeError                            Traceback (most recent \ncall\n \nlast\n)\n\n\n in \n\n()\n      \n7\n with \nopen\n(\n'train_abstract.txt'\n,\n'w'\n,encoding=\n'utf-8'\n) \nas\n \nf\n:\n      \n8\n     \nfor\n i in \nrange\n(\nlen\n(X_train.todense())):\n----> \n9\n         str1 = str(X_train.todense()[i])+\n\"\\t\"\n+\n\"__label__\"\n+str(y_train.\nvalues\n[i])+\n'\\n'\n\n     \n10\n         \nf\n.\nwrite\n(str1)\n     \n11\n \n\nAttributeError: \n'list'\n object \nhas\n \nno\n attribute \n'values'\n\n\n\n\n\n请教大家如何解决这个问题。", "Tag": "算法分析"}
{"Answer": "\nimport numpy as np\nimport warnings\n\n\ndef swapRows(A, i, j):\n    \"\"\"\n    interchange two rows of A\n    operates on A in place\n    \"\"\"\n    tmp = A[i].copy()\n    A[i] = A[j]\n    A[j] = tmp\n\n\ndef relError(a, b):\n    \"\"\"\n    compute the relative error of a and b\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\")\n        try:\n            return np.abs(a - b) / np.max(np.abs(np.array([a, b])))\n        except:\n            return 0.0\n\n\ndef rowReduce(A, i, j, pivot):\n    \"\"\"\n    reduce row j using row i with pivot pivot, in matrix A\n    operates on A in place\n    \"\"\"\n    factor = A[j][pivot] / A[i][pivot]\n    for k in range(len(A[j])):\n        if np.isclose(A[j][k], factor * A[i][k]):\n            A[j][k] = 0.0\n        else:\n            A[j][k] = A[j][k] - factor * A[i][k]\n\n\n# stage 1 (forward elimination)\ndef forwardElimination(B):\n    \"\"\"\n    Return the row echelon form of B\n    \"\"\"\n    A = B.copy().astype(float)\n    m, n = np.shape(A)\n    for i in range(m - 1):\n        # Let lefmostNonZeroCol be the position of the leftmost nonzero value\n        # in row i or any row below it\n        leftmostNonZeroRow = m\n        leftmostNonZeroCol = n\n        ## for each row below row i (including row i)\n        for h in range(i, m):\n            ## search, starting from the left, for the first nonzero\n            for k in range(i, n):\n                if (A[h][k] != 0.0) and (k < leftmostNonZeroCol):\n                    leftmostNonZeroRow = h\n                    leftmostNonZeroCol = k\n                    break\n        # if there is no such position, stop\n        if leftmostNonZeroRow == m:\n            break\n        # If the leftmostNonZeroCol in row i is zero, swap this row\n        # with a row below it\n        # to make that position nonzero. This creates a pivot in that position.\n        if (leftmostNonZeroRow > i):\n            swapRows(A, leftmostNonZeroRow, i)\n        # Use row reduction operations to create zeros in all positions\n        # below the pivot.\n        for h in range(i + 1, m):\n            rowReduce(A, i, h, leftmostNonZeroCol)\n    return A\n\n\n####################\n\n# If any operation creates a row that is all zeros except the last element,\n# the system is inconsistent; stop.\ndef inconsistentSystem(A):\n    temp = forwardElimination(A)\n    return max(np.nonzero(temp[:, :-1])[0]) != max(np.nonzero(temp)[0])\n\n\n# 判断一个矩阵是否是不相容的,只能用numpy的nonzero方法\n\n##原文链接：https://blog.csdn.net/mr_jjpolarbear/article/details/88649587\ndef rsmat(arbmat):\n    \"\"\" Convert an arbitrary matrix to a simplest matrix \"\"\"\n    arbmat = arbmat.astype(float)\n    row_number, column_number = arbmat.shape\n    if row_number == 1:\n        if arbmat[0, 0] != 0:\n            return (arbmat / arbmat[0, 0])\n        else:\n            return arbmat\n    else:\n        rc_number = min(row_number, column_number)\n        anarbmat = arbmat.copy()\n        r = 0\n        for n in range(rc_number):\n            s_row = -1\n            for i in arbmat[r:row_number, n]:\n                s_row += 1\n                if abs(i) > 1e-10:\n                    anarbmat[r, :] = arbmat[s_row + r, :]\n                    for j in range(r, row_number):\n                        if j < s_row + r:\n                            anarbmat[j + 1, :] = arbmat[j, :]\n                    arbmat = anarbmat.copy()\n            if abs(anarbmat[r, n]) > 1e-10:\n                anarbmat[r, :] = anarbmat[r, :] / anarbmat[r, n]\n                for i in range(row_number):\n                    if i != r:\n                        anarbmat[i, :] -= \\\n                            anarbmat[i, n] * anarbmat[r, :]\n            arbmat = anarbmat.copy()\n            if abs(arbmat[r, n]) < 1e-10:\n                r = r\n            else:\n                r = r + 1\n        for m in range(column_number):\n            if abs(arbmat[-1, m]) > 1e-10:\n                arbmat[-1, :] = arbmat[-1, :] / arbmat[-1, m]\n                for i in range(row_number - 1):\n                    arbmat[i, :] -= \\\n                        arbmat[i, m] * arbmat[-1, :]\n                break\n\n        return arbmat\n\n\ndef backsubstitution(B):\n    \"\"\"\n    return the reduced row echelon form matrix of B\n    \"\"\"\n    if not inconsistentSystem(B):\n        return rsmat(B)\n\n\n#####################\n# 测试的值\na = np.array([[1, 2, 3, 24], [5, 4, 6, 24], [10, 9, 8, 9]])\nprint(backsubstitution(a))\n# 最后的结果应该是[1,0,0,-8],[0,1,0,1],[0,0,1,10]\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python 中numpy的矩阵函数\n通过python判断一个矩阵是否是不相容的,并且再写一个backsubstitution函数,来解出矩阵.判断是否是不相容(inconsistent)需要用numpy.nonzero方法\n\n\nimport\n numpy \nas\n np\n\nimport\n warnings\n\n\ndef\n \nswapRows\n(\nA, i, j\n):\n    \n\"\"\"\n    interchange two rows of A\n    operates on A in place\n    \"\"\"\n\n    tmp = A[i].copy()\n    A[i] = A[j]\n    A[j] = tmp\n\n\ndef\n \nrelError\n(\na, b\n):\n    \n\"\"\"\n    compute the relative error of a and b\n    \"\"\"\n\n    \nwith\n warnings.catch_warnings():\n        warnings.simplefilter(\n\"error\"\n)\n        \ntry\n:\n            \nreturn\n np.\nabs\n(a-b)/np.\nmax\n(np.\nabs\n(np.array([a, b])))\n        \nexcept\n:\n            \nreturn\n \n0.0\n\n\n\ndef\n \nrowReduce\n(\nA, i, j, pivot\n):\n    \n\"\"\"\n    reduce row j using row i with pivot pivot, in matrix A\n    operates on A in place\n    \"\"\"\n\n    factor = A[j][pivot] / A[i][pivot]\n    \nfor\n k \nin\n \nrange\n(\nlen\n(A[j])):\n        \nif\n np.isclose(A[j][k], factor * A[i][k]):\n            A[j][k] = \n0.0\n\n        \nelse\n:\n            A[j][k] = A[j][k] - factor * A[i][k]\n\n\n\n# stage 1 (forward elimination)\n\n\ndef\n \nforwardElimination\n(\nB\n):\n    \n\"\"\"\n    Return the row echelon form of B\n    \"\"\"\n\n    A = B.copy().astype(\nfloat\n)\n    m, n = np.shape(A)\n    \nfor\n i \nin\n \nrange\n(m-\n1\n):\n        \n# Let lefmostNonZeroCol be the position of the leftmost nonzero value \n\n        \n# in row i or any row below it \n\n        leftmostNonZeroRow = m\n        leftmostNonZeroCol = n\n        \n## for each row below row i (including row i)\n\n        \nfor\n h \nin\n \nrange\n(i,m):\n            \n## search, starting from the left, for the first nonzero\n\n            \nfor\n k \nin\n \nrange\n(i,n):\n                \nif\n (A[h][k] != \n0.0\n) \nand\n (k < leftmostNonZeroCol):\n                    leftmostNonZeroRow = h\n                    leftmostNonZeroCol = k\n                    \nbreak\n\n        \n# if there is no such position, stop\n\n        \nif\n leftmostNonZeroRow == m:\n            \nbreak\n\n        \n# If the leftmostNonZeroCol in row i is zero, swap this row \n\n        \n# with a row below it\n\n        \n# to make that position nonzero. This creates a pivot in that position.\n\n        \nif\n (leftmostNonZeroRow > i):\n            swapRows(A, leftmostNonZeroRow, i)\n        \n# Use row reduction operations to create zeros in all positions \n\n        \n# below the pivot.\n\n        \nfor\n h \nin\n \nrange\n(i+\n1\n,m):\n            rowReduce(A, i, h, leftmostNonZeroCol)\n    \nreturn\n A\n\n\n#################### \n\n\n\n# If any operation creates a row that is all zeros except the last element,\n\n\n# the system is inconsistent; stop.\n\n\ndef\n \ninconsistentSystem\n(\nA\n):\n    \n# 判断一个矩阵是否是不相容的,只能用numpy的nonzero方法\n\n\n\n\n\ndef\n \nbacksubstitution\n(\nB\n):\n    \n\"\"\"\n    return the reduced row echelon form matrix of B\n    \"\"\"\n\n\n\n\n\n#####################\n\n\n# 测试的值\n\na = np.array([[\n1\n, \n2\n, \n3\n,\n24\n], [\n5\n, \n4\n, \n6\n,\n24\n], [\n10\n, \n9\n, \n8\n,\n9\n]])\nbacksubstitution(a)\n\n# 最后的结果应该是[1,0,0,-8],[0,1,0,1],[0,0,1,10]\n\n\n\n\n\n只用写最后两个函数就好了,之前写完了", "Tag": "算法分析"}
{"Answer": "也许是matplotlib图片引擎的问题，windos下没问题\r\n![图片说明](https://img-ask.csdn.net/upload/201704/15/1492264086_209484.png)", "Konwledge_Point": "应对NP完全问题", "Question": "matplotlib库在ubuntu下的一个bug？\n当我在Ubuntu 16.10下使用matplotlib绘制以下程序的图形时：\n\n程序是：\n\n\n\n# -*- coding: utf-8 -*-\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 10, 1000)\ny = np.sin(x)\nz = np.cos(x**2)\n\nplt.figure(figsize=(8,4))\nplt.plot(x,y,label=\"$sin(x)$\",color=\"red\",linewidth=2)\nplt.plot(x,z,\"b--\",label=\"$cos(x^2)$\")\nplt.xlabel(\"Time(s)\")\nplt.ylabel(\"Volt\")\nplt.title(\"PyPlot First Example\")\nplt.ylim(-1.2,1.2)\nplt.legend()\nplt.show()\n\n\n\n\n\n这是官方给出的win下的标准输出：\n\n\n\n然而，我在Ubuntu 16.10下得到的输出却是：\n\n\n\n右上角蓝色虚线后应该是上图一样显示cosx的平方的，但实际得到的却不一样，这是为何？", "Tag": "算法分析"}
{"Answer": "看你取到的df数据是不是含有其他非数值的列，或者你索引的时候就只取各科目列就行，warning只要不报错也没关系", "Konwledge_Point": "应对NP完全问题", "Question": "Python使用pd.mean()函数后出现Warning，请问该如何解决？\nimport\n pandas \nas\n pd\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n numpy \nas\n np\ndf = pd.read_excel(\nr'成绩表.xlsx'\n)\nplt.rcParams[\n'font.sans-serif'\n]=[\n'SimHei'\n]                     \n# 解决中文乱码\n\nlabels = np.array([\n'语文'\n,\n'数学'\n,\n'英语'\n,\n'物理'\n,\n'化学'\n,\n'生物'\n])      \n# 标签\n\ndataLenth = \n6\n       \n# 数据长度\n\n\n# 计算女生、男生各科平均成绩\n\ndf1 = df[df[\n'性别'\n]==\n'女'\n].mean().\nround\n(\n2\n)\ndf2 = df[df[\n'性别'\n]==\n'男'\n].mean().\nround\n(\n2\n)\n\nprint\n(df1-df2)\n\n\n\n\nwarning信息：\n\n\nD:/\n06.\nDifferential analysis.py:\n9\n: FutureWarning: Dropping \nof\n nuisance \ncolumns\n \nin\n DataFrame reductions (\nwith\n \n'numeric_only=None'\n) \nis\n deprecated; \nin\n a future \nversion\n this will \nraise\n TypeError.  \nSelect\n \nonly\n \nvalid\n \ncolumns\n \nbefore\n calling the reduction.\n  df1 = df[df[\n'性别'\n]==\n'女'\n].mean().round(\n2\n)\nD:/\n06.\nDifferential analysis.py:\n10\n: FutureWarning: Dropping \nof\n nuisance \ncolumns\n \nin\n DataFrame reductions (\nwith\n \n'numeric_only=None'\n) \nis\n deprecated; \nin\n a future \nversion\n this will \nraise\n TypeError.  \nSelect\n \nonly\n \nvalid\n \ncolumns\n \nbefore\n calling the reduction.\n  df2 = df[df[\n'性别'\n]==\n'男'\n].mean().round(\n2\n)  \n\n", "Tag": "算法分析"}
{"Answer": "matlab是高度优化的软件，你的A在matlab里面甚至可以储存为1k以下，要是它只用压缩存储的话，那么存储的东西就是（1）矩阵大小：3个整型数[62, 500, 200]；（2）矩阵值：1个双精度实型[0]就储存三个整形加一个双精度实型变量就OK了，这是因为你的matlab把全零的矩阵给压缩了。python只是仿造matlab，得其形未得其神，储存得mat文件完全按照62×500×200个双精度实型变量来存储，没有数据压缩，所以储存空间特别大。你可以计算出来这么多双精度实型储存下来的0，需要占用多少空间，下面是计算结果\n62*500*200*8/1024^2%单位M\n\nans =\n\n  47.302246093750000\n\n47.3M啊，跟你说的python运行后生成得文件大小差不多。\n当然，matlab压缩存储是建立在有很多重复元素的基础上面，如果你把zeros命令改了，改成rand(随机0-1的数字)，那么matlab生成的mat文件储存空间也会特别大！！！！\nA = rand(62, 500, 200);\nsave('matlabA.mat', 'A' )\n\n\n整整44.9M啊！！可见matlab只是优化了简单重复元素的存储空间，到了不重复杂乱无章的数据时，压缩方法也是会失效的。哈哈", "Konwledge_Point": "应对NP完全问题", "Question": "python保存的.mat文件过大\n同一个矩阵A，用Matlab生成的.mat文件只有22KB，\n用Python中sio.savemat函数生成的.mat文件却有48MB。\n这是为什么呢?\nMatlab代码：\nA  = zeros(62, 500, 200);\nsave('matlabA.mat', 'A' )\n\n\nPython代码:\nimport numpy as np\nimport scipy.io as sio\nA  = np.zeros([62, 500, 200])\nsio.savemat('pythonA.mat', {'A':A})\n\n", "Tag": "算法分析"}
{"Answer": "\n# 你数据中target应该是个分类变量吧, 直接用可以处理分类变量的作图就可以\n\n# 用pandas, 颜色需要是数值\nx_dr.plot.scatter(x='x轴变量',y='y轴变量',c='颜色变量')\n\n# 方法2 如果颜色是分类变量\nimport seaborn as sns\nsns.scatterplot(x=\"x轴变量\", # x轴\n                y=\"y轴变量\",# y轴\n                hue=\"颜色变量\",   # 颜色分类\n                palette='Spectral_r', # 调色盘\n                sizes=(10, 200),      # 大小区间\n                data=x_dr)\n# 方法3\nimport pandas_bokeh\nx_dr.plot_bokeh.scatter(\n    x=\"x轴变量\",\n    y=\"y轴变量\",\n    category=\"颜色\",  # 作为分类的字段列\n    title=\"\")\n参考下这个:\nmatplotlib画点-Python-CSDN问答", "Konwledge_Point": "应对NP完全问题", "Question": "关于python中plt制图的问题，如何将Bunch中高维度的数组可视化下？\n\nx_dr.shape\n\n\n\n\n(24261, 2)\n\n\n\n数据其中 target有12类\n\n\n\nN = 12\n\ny = np.random.rand(N)\n\n\n\n\nplt.scatter(\nx_dr[:,0],x_dr[:,1],c=y\n)\nplt.show()\n\n报错'c' argument has 2 elements, which is not acceptable for use with 'x' with size 24261, 'y' with size 24261.\n\n\n\n  我已经将原高维度数组降维至2维数组，但是绘图时依旧需要我提供24261大小的Color值，有办法将color值设为target类型大小么？\n\n\n\n ", "Tag": "算法分析"}
{"Answer": "这是因为你在倒数第五行有一句‘print(predict3\\_train)’啊，所以输出了它的维度", "Konwledge_Point": "应对NP完全问题", "Question": "python深度学习分类后的 混淆矩阵的意义\n有大佬可以解释混淆矩阵上面还有（200，2）是什么意思吗，我设置的目标值为1或者2，但不知道为什么200后面是2，这个2和设置的1，2有关系吗\n\n\n\nimport numpy as np\nimport urllib.request\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport pandas as pd\nimport xlrd\nfrom sklearn import preprocessing\ndef excel_to_matrix(path):\n    table = xlrd.open_workbook(path).sheets()[0]  # 获取第一个sheet表\n    row = table.nrows  # 行数\n    col = table.ncols  # 列数\n    datamatrix = np.zeros((row, col))\n    for x in range(col):\n        cols = np.matrix(table.col_values(x))\n\n        datamatrix[:, x] = cols\n    return datamatrix\n\n\ndatafile = u'C:\\\\Users\\\\asus\\\\PycharmProjects\\\\2\\\\venv\\\\Lib\\\\附件2：数据.xls'\ndatamatrix=excel_to_matrix(datafile)\ndata=pd.DataFrame(datamatrix)\n\ny=data[10]\ndata=data.drop(10,1)\nx=data\n\n\nfrom sklearn import preprocessing\nx_MinMax=preprocessing.MinMaxScaler()\n\ny=np.array(y).reshape((len(y),1))\n\nx=x_MinMax.fit_transform(x)\nx.mean(axis=0)\nimport random\nfrom sklearn.cross_validation import train_test_split\nnp.random.seed(2016)\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n\nfrom sknn.mlp import Classifier,Layer #预测模型\n\nfit3=Classifier(layers=[Layer('Tanh',units=38),Layer('Tanh',units=45),\n                       Layer('Tanh', units=28),\n                       Layer('Softmax')],\n               learning_rate=0.02,\n               random_state=2016,\n               n_iter=100,\n               dropout_rate=0.05,\n                batch_size=50,\n                learning_rule=u'adadelta',\n                learning_momentum=0.005\n\n\n               )\nfit3.fit(x_train,y_train)\n\nfrom sklearn.metrics import confusion_matrix\npredict3_train=fit3.predict(x_train)\nprint(predict3_train)\n\npredict3_test=fit3.predict(x_test)\nconfu3_test=confusion_matrix(y_test,predict3_test)\nprint(confu3_test)\n", "Tag": "算法分析"}
{"Answer": "那就做两次筛选，  不用切片用 isna 或者 isnull 函数", "Konwledge_Point": "应对NP完全问题", "Question": "Python读取csv文件如何计算某列为空的另一列的和\n怎样得出K列为空E列的和呢\n\n\n\n\nimport\n pandas as pd\n\nimport\n numpy as np\n\nimport\n xlwt\n\n\ncount\n = \n0\n\nworkbook = xlwt.Workbook()\nsheet = workbook.add_sheet(\n\"Sheet Name1\"\n)\n\ndf = pd.read_csv(\n\"D:RULER_LINE_ABSOLUTE_PAVEMENT_EGDE_-distinct.csv\"\n)\nhad = df[[\n\"feature_length\"\n]]\nshen = df[[\n\"report_length\"\n]]\n\nsheet.\nwrite\n(\ncount\n, \n0\n, \nint\n(df.iloc[\n2\n][\n\"order_id\"\n]))\nsheet.\nwrite\n(\ncount\n, \n1\n, \nfloat\n(np.sum(shen)))\nsheet.\nwrite\n(\ncount\n, \n2\n, \nfloat\n(np.sum(had)))\n\nworkbook.save( \n'sum.xls'\n)\n\n\n\n我这里只会得出两列的总和，如何能得出图中框里的数据之和呢\n因为要处理大量的类似文件，所以不能确定是几行几列，只能确定对应的是空", "Tag": "算法分析"}
{"Answer": "import numpy as np\nimport more_itertools as mi\n\n\na=np.array([1,2,3,4,5,6,7,8,9,10])\nb=np.array([3,7])\n\nres = list(mi.split_when(a, lambda x, y: x in b))\nresult = [res[i] if i == 0 else [b[i-1]] + res[i] for i in range(len(res ))]\nprint(result)\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "python代码也不会写 numpy\n假如我有个区间\na=np.array([1,2,3,4,5,6,7,8,9,10])\nb=np.array([3,7])\n我怎么才可以把向量a分成三类\n一类为a1=[1,2,3] 一类为a2=[3,4,5,6,7]，最后一类为a3=[7,8,9,10]呢\n就是怎么用b来分割a呀？\n求解答", "Tag": "算法分析"}
{"Answer": "from PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimg=np.array(Image.open(\"F:\\\\PythonDemo\\\\face.jpg\").convert(\"L\"))\n# plt.imshow(img,plt.cm.gray)\n\nprint(img)\nrow=(img.shape[0])\nprint(row)\nimg1=img[::-1]\nprint(img1)\n\nimg3 = Image.fromarray(np.fliplr(img))\nplt.imshow(img3,plt.cm.gray)\n\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "请问如何在此基础上写代码实现图片的水平翻转\n代码\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimg=np.array(Image.open(\"C:\\Users\\lenovo\\Pictures\\a\\女孩.jpg\").convert(\"L\"))\nplt.imshow(img,plt.cm.gray)\nprint(img)\nrow=(img.shape[0])\nprint(row)\nimg1=img[::-1]\nprint(img1)\nplt.imshow(img,plt.cm.gray)", "Tag": "算法分析"}
{"Answer": "你是想把数据框转成列表吗？digits.values.tolist()\n如果你是划分特征和标签，可以不用转列表，直接数据框也是可以的。比如：X=digits[['x1','x2','x3','x4']]Y=digits.yx_train, x_test, y_train, y_test = train_test_split(X, Y,test_size=0.3)", "Konwledge_Point": "应对NP完全问题", "Question": "请问这种情况该怎么解决\n问题遇到的现象和发生背景\n\n\n运行的时候出现了这个情况：AttributeError: 'DataFrame' object has no attribute 'data'\n\n\n问题相关代码\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_digits #手写数字数据集\nfrom sklearn.preprocessing import LabelBinarizer #标签二值化处理\nfrom sklearn.model_selection import train_test_split #训练和测试集分隔\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n\n  载入数据\ndigits = pd.read_csv('F:QQ/FileRecv/MobileFile/信贷.csv')\nprint(digits.shape) #结果：\n\n\n  输入的数据\nX = digits.data.tolist()\n\n\n运行结果及报错内容\n\n", "Tag": "算法分析"}
{"Answer": "你好，不要用math啦，math下面的sin只能对单独的值进行求正弦。请改用np.sinmath.sin(x)改成np.sin(x)即可有帮助望采纳", "Konwledge_Point": "应对NP完全问题", "Question": "用logspace和linspace计算正弦值，报错\n\nimport \ntime\n\nimport numpy as np\nimport \nmath\n\n\nt2 = \ntime\n.\ntime\n()          \ndef content_l_math():\n    \nfor\n i \nin\n np.logspace(\n0\n,\n8\n,\n9\n):\n        x = np.arange(\n0\n,\n10\n,\n10\n/i)\n        \nmath\n.\nsin\n(x)\n        \nprint\n(\n\"取点数10^%d耗时->%s\"\n%(\nmath\n.\nlog\n(i,\n10\n),\ntime\n.\ntime\n()-t2))\n        \nprint\n(\nmath\n.\nsin\n(x))\ncontent_l_math()\n\n\n\n出现\nTraceback (most recent call last):\n\n\n  File \"C:\\Users\\zzw918\n.spyder-py3\\未命名0.py\", line 12, in \n    content_l_math()\n\n\n  File \"C:\\Users\\zzw918\n.spyder-py3\\未命名0.py\", line 9, in content_l_math\n    math.sin(x)\n\n\nTypeError: only size-1 arrays can be converted to Python scalars", "Tag": "算法分析"}
{"Answer": "根据您的问题描述，要分析甲型流感病毒的氨基酸分子进化树，那你首先要下载甲型流感病毒的相关数据下来，确保自己电脑上安装了MLGA分析工具，然后使用mlga工具进行分析。然后通过bootstrap法进行检验，bootstrap检验也叫自举法检验，就是放回式抽样统计法，通过对数据集多次重复取样，构建多个进化树，用来检查给定树的分枝可信度。", "Konwledge_Point": "应对NP完全问题", "Question": "如下所示， 给出解答过程\n甲型流感病毒（ Inifasuza A vinus ）含有8个基因片段。分别编码多个酶或蛋白质。请在 ncbi 网站上查询 IHINI 的 aeuraminidee (N1）的氨基酸序列和 cDNA 序列【或 leuuagghutinin ( HI ）的氰基酸序到和 cDNA 序列。或 tmacleogeotoim ( NP ）的氨基酸序列和 cNDA 序列，任选一个］，利用 MLGA 分析工具。绘制基于 NI 氨基酸序列（或 HI 氨基酸序列，或 NP 氨基酸配序列，任选一个）的分子进化树并用 bootstrap 法评价可面性。委求地出完些的分析过程和说明。", "Tag": "算法分析"}
{"Answer": "\nThe default backend is specified in the matplotlibrc file -- you could try changing it there.\nIt may also be that whatever environment you are running python in may already import matplotlib and specify the backend as part of the configuration. Also, some post suggest the following line may be needed:\nos.environ[ 'MPLCONFIGDIR' ] = '/tmp/'\n\n", "Konwledge_Point": "应对NP完全问题", "Question": "在Web应用程序服务器（php）中使用matplotlib时出现问题\n\n\n\nI have a python program that starts with: \n\n\n\nfrom optparse import OptionParser\nimport math \n#import wx\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pylab import *\nfrom numpy import *\nimport scipy as scipy\nfrom scipy import *\nfrom scipy import constants\nimport scipy.signal as signal\nimport matplotlib.pyplot as plt\n\n\n\n\nIt gives me error when I try to open it with php.\nI have googled and apparently if I do this before importing pylab or pyplot:\n\n\n\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\n\n\n\nProblem should be solved. But the error I get is:\n\n\n\n  /usr/lib/pymodules/python2.7/matplotlib/__init__.py:923:     UserWarning: This call to matplotlib.use() has no effect\nbecause the the backend has already been chosen;\nmatplotlib.use() must be called *before* pylab, matplotlib.pyplot,\nor matplotlib.backends is imported for the first time\n\n\n\n\nAny idea what's going on??\n\n    ", "Tag": "算法分析"}
{"Answer": "你在自己电脑上面用的是链表吗？还是你讲list当做链表了？你看看lecode里面提示你的ListNode的数据结构，人家提供给你就是要求你用这个数据结构来完成，你这写法就没用到链表指针，都是list。没有提供len的方法，你要么自己实现，要么去遍历才能知道链表的长度。\n ", "Konwledge_Point": "应对NP完全问题", "Question": "LeeCode21题合并有序链表报错？\n\n\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nimport numpy as np\nclass Solution:\n    def mergeTwoLists(self, l1: ListNode, l2: ListNode) -> ListNode:\n        l=l1\n        m = len(l)-1\n        for i in l2:\n            m = m + 1\n            l = np.insert(l,m,i)\n\n        l = np.sort(l)\n        return l \n\n\n\n在做LeeCode简单题目21合并两个有序链表……在pycharm上跑没有问题，在网页上报错：\n\n\n\n\n\nTypeError: object of type 'ListNode' has no len()\n    m = len(l)-1\nLine 10 in mergeTwoLists (Solution.py)\n    ret = Solution().mergeTwoLists(param_1, param_2)\nLine 42 in _driver (Solution.py)\n    _driver()\nLine 53 in  (Solution.py)\n\n\n\n疯了，感觉自己python学了个寂寞！大神求救！", "Tag": "算法分析"}
{"Answer": "# 采纳率太差，如果希望得到更多帮助，请及时采纳\r\n\r\n\r\nlog路径的输出为止，不要写 c:\\，因为是相对路径，而冒号是不允许的。", "Konwledge_Point": "应对NP完全问题", "Question": "InvalidArgumentError: Failed to create a directory: log/C:; Invalid argument这是什么原因呀\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Failed to create a directory: log/C:; Invalid argument\n\n这个代码我改了一点就运行不了了，提示无法创建文件夹是什么原因？\n\n我在123文件夹里建立了TO文件夹里面也建立了log 但是还是报这个错误\n\n\n\n# train_model.py\n\nimport numpy as np\nfrom alexnet import alexnet\nWIDTH = 214\nHEIGHT = 132\nLR = 1e-3\nEPOCHS = 10\nMODEL_NAME = 'C:/Users/Administrator/Desktop/123/pygta5-car-fast-{}-{}-{}-epochs-300K-data.model'.format(LR, 'alexnetv2',EPOCHS)\n\nmodel = alexnet(WIDTH, HEIGHT, LR)\n\nhm_data = 22\nfor i in range(EPOCHS):\n    for i in range(1,hm_data+1):\n        train_data = np.load('C:/Users/Administrator/Desktop/123/training_data-{}-balanced.npy'.format(i))\n\n        train = train_data[:-100]\n        test = train_data[-100:]\n\n        X = np.array([i[0] for i in train]).reshape(-1,WIDTH,HEIGHT,1)\n        Y = [i[1] for i in train]\n\n        test_x = np.array([i[0] for i in test]).reshape(-1,WIDTH,HEIGHT,1)\n        test_y = [i[1] for i in test]\n\n        model.fit({'input': X}, {'targets': Y}, n_epoch=1, validation_set=({'input': test_x}, {'targets': test_y}), \n            snapshot_step=500, show_metric=True, run_id=MODEL_NAME)\n\n        model.save(MODEL_NAME)\n\n\n\n# tensorboard --logdir=foo:C:/path/to/log\n\n\n", "Tag": "算法分析"}
{"Answer": "shape会返回一个元组,因为你的a只有一行,形状为3,但是出于保留元组格式,在3后面加了一个逗号输出(3)和(3,)在python中代表不同含义有帮助望采纳", "Konwledge_Point": "应对NP完全问题", "Question": "numpy.array().shape的解释,如下描述\na=np\n.array\n(\n[1,1,1]\n)\n\nprint\n(a.shape)\n\n\n\n\n如何解释输出结果：（3，）", "Tag": "算法分析"}
{"Answer": "第一段代码直接和第二段合并，第一段去掉 if __name__语句，将open(r'C:\\Users\\wxy\\Desktop\\re\\555.csv','r',encoding='utf-8')读取的文件名，对应于图一中输出的文件outfo即可。补充：如果读取多个csv,合并写入一个有格式要求的csv中，合并多个df,一次写入文件，可用如下代码：\nimport os\nimport re\nimport datetime\nimport numpy as np\nimport pandas as pd\n\nfilenames_in = r'C:\\Users\\1\\Desktop\\111'  # 输入文件的文件地址\n#filenames_out = r'C:\\Users\\2\\Desktop\\444'  # 新文件的地址\npathDir = os.listdir(filenames_in)\ndfs=pd.DataFrame()\nfor i,allDir in enumerate(pathDir):\n    child = re.findall(r\"(.+?).csv\", allDir)  # 正则的方式读取文件名，去扩展名\n    if len(child) >= 0:  # 去掉没用的系统文件\n        newfile = ''\n        needdate = child  #### 这个就是所要的文件名\n    domain1 = os.path.abspath(filenames_in)  # 待处理文件位置\n    info = os.path.join(domain1, allDir)  # 拼接出待处理文件名字\n    # ------------数据处理过程---------------\n    print(info, \"开始处理\")\n    df = pd.DataFrame(pd.read_csv(info))\n    dfs=dfs.append(df,ignore_index=True)\nprint(dfs)\nwith open(r'C:\\Users\\1\\Desktop\\666\\222.csv','w', encoding='utf-8') as f1:\n    f1.write('#' * 20 + '\\n')\n    f1.write('# i j Corrcoef(c) Dist=sqrt(2(1-c))\\n')\n    f1.write('#' * 20 + '\\n')    \n    dfs.to_csv(f1, encoding='utf-8',index=False)\n\n\n写入不同文件：\nimport os\nimport re\nimport datetime\nimport numpy as np\nimport pandas as pd\n\nfilenames_in = r'C:\\Users\\1\\Desktop\\111'  # 输入文件的文件地址\n#filenames_out = r'C:\\Users\\2\\Desktop\\444'  # 新文件的地址\npathDir = os.listdir(filenames_in)\nfor i,allDir in enumerate(pathDir):\n    child = re.findall(r\"(.+?).csv\", allDir)  # 正则的方式读取文件名，去扩展名\n    if len(child) >= 0:  # 去掉没用的系统文件\n        newfile = ''\n        needdate = child  #### 这个就是所要的文件名\n    domain1 = os.path.abspath(filenames_in)  # 待处理文件位置\n    info = os.path.join(domain1, allDir)  # 拼接出待处理文件名字\n    # ------------数据处理过程---------------\n    print(info, \"开始处理\")\n    df = pd.DataFrame(pd.read_csv(info))\n    print(df)\n    \n    with open(f'C:/Users/1/Desktop/666/{i}.csv','w', encoding='utf-8') as f1:\n        f1.write('#' * 24 + '\\n')\n        f1.writelines('# i  j  Corrcoef(c)  Dist=sqrt(2(1-c))\\n')\n        f1.write('#' * 24 + '\\n')    \n        df.to_csv(f1, encoding='utf-8',index=False,header=None,line_terminator='\\n')\n", "Konwledge_Point": "应对NP完全问题", "Question": "py csv文件按行分解输出和 py文件整合问题\n问题一：如何以等量的间隔输出多个文件 例如 第一个文件输出1-21 第二个文件输出21-42 以此类推\n问题二：如何将下列两程序整合到一起\n\n\nimport\n os\n\nimport\n re\n\nimport\n datetime\n\nimport\n numpy \nas\n np\n\nimport\n pandas \nas\n pd\n\n\nif\n __name__ == \n'__main__'\n:\n    filenames_in = \nr'C:\\Users\\wxy\\Desktop\\111'\n  \n# 输入文件的文件地址\n\n    filenames_out = \nr'C:\\Users\\wxy\\Desktop\\333'\n  \n# 新文件的地址\n\n    pathDir = os.listdir(filenames_in)\n    \nfor\n allDir \nin\n pathDir:\n        child = re.findall(\nr\"(.+?).csv\"\n, allDir)  \n# 正则的方式读取文件名，去扩展名\n\n        \nif\n \nlen\n(child) >= \n0\n:  \n# 去掉没用的系统文件\n\n            newfile = \n''\n\n            needdate = child  \n#### 这个就是所要的文件名\n\n        domain1 = os.path.abspath(filenames_in)  \n# 待处理文件位置\n\n        info = os.path.join(domain1, allDir)  \n# 拼接出待处理文件名字\n\n\n        \n# ------------数据处理过程---------------\n\n        \nprint\n(info, \n\"开始处理\"\n)\n        df = pd.DataFrame(pd.read_csv(info))\n        a = df.corr()\n        a = \nround\n(a, \n4\n)  \n# print(round(a, 4))\n\n        b = a.applymap(\nlambda\n x: (\n2\n * (\n1\n - x)) ** \n0.5\n)\n        b = \nround\n(b, \n4\n)\n        \n# print(a)\n\n        \n# print(b)\n\n        df0 = []\n        \nfor\n x \nin\n a.index:\n            df0.append(a.loc[x, :])\n        c = pd.concat(df0, keys=a.index).to_frame().reset_index()\n        c.columns = [\n'i'\n, \n'j'\n, \n'Corrcoef(c)'\n]\n        df1 = []\n        \nfor\n x \nin\n b.index:\n            df1.append(b.loc[x, :])\n        d = pd.concat(df1, keys=b.index).to_frame().reset_index()\n        d.columns = [\n'i'\n, \n'j'\n, \n'Dist=sqrt(2(1-c))'\n]\n        d = d.drop([\n'i'\n, \n'j'\n], axis=\n1\n)\n        df = pd.concat([c, d], axis=\n1\n, join=\n'inner'\n, ignore_index=\nFalse\n,\n                       keys=\nNone\n, levels=\nNone\n, names=[\n'i'\n, \n'j'\n, \n'Corrcoef(c)'\n, \n'Dist=sqrt(2(1-c))'\n],\n                       verify_integrity=\nFalse\n)\n        domain2 = os.path.abspath(filenames_out)  \n# 处理完文件保存地址\n\n        outfo = os.path.join(domain2, allDir)  \n# 拼接出新文件名字\n\n        df.to_csv(outfo, encoding=\n'utf-8'\n,index=\nFalse\n)\n        \nprint\n(info, \n\"处理完\"\n)\n\nendtime = datetime.datetime.now()\n\n\n\n\n\n\n\n和\n\n\nopen\n(\nr'C:\\Users\\wxy\\Desktop\\re\\555.csv'\n,\n'r'\n,encoding=\n'utf-8'\n) \nas\n f, \nopen\n(\nr'C:\\Users\\wxy\\Desktop\\re\\666.csv'\n,\n'w'\n,encoding=\n'utf-8'\n) \nas\n f1:\n    a=f.readlines()\n    f1.write(\n'#'\n*\n10\n+\n'\\n'\n)\n    f1.write(\n'# i  j Corrcoef(c) Dist=sqrt(2(1-c))\\n'\n)\n    f1.write(\n'#'\n*\n10\n+\n'\\n'\n)\n    \nfor\n line \nin\n a:\n        f1.write(\n' '\n.join(line.split(\n','\n)) + \n'\\n'\n)\n\n\n\n\n代码二是处理代码一输出出来的文件的 如能帮助一定采纳谢谢", "Tag": "算法分析"}
{"Answer": "试试看http://192.168.1.102:8080/?action=stream你传一个html不是一个网页吗？", "Konwledge_Point": "应对NP完全问题", "Question": "YOLOv5无法成功读取树莓派通过mjpg传来的视频流\n使用树莓派做下位机获取视频图像，用在同一局域网下的电脑获取视频流并以YOLO进行目标检测的处理。实操中直接将mjpg地址传给detect.py显示读取失败，遂尝试用opencv进行帧处理后传给yolo，亦失败。附源码。\n\n\n下为直接传递：\n\n\nif\n __name__\n == \n'__main__':\n    parser = argparse.\nArgumentParser()\n\n    parser.add\n_argument('--\nweights\n', \nnargs\n='+', \ntype\n=\nstr\n, \ndefault\n='\nyolov5s\n.\npt\n', \nhelp\n='\nmodel\n.\npt\n \npath\n(\ns\n)\n')\n    parser.add\n_argument('--\nsource\n', \ntype\n=\nstr\n, \ndefault\n='\nhttp\n:\n/\n/\n192.168.1.102:8080\n/\nstream\n.\nhtml\n', \nhelp\n='\nsource\n')\n  # file/folder, \n0\n \nfor\n webcam\n    parser.add\n_argument('--\nimg\n-\nsize\n', \ntype\n=\nint\n, \ndefault\n=640, \nhelp\n='\ninference\n \nsize\n (\npixels\n)\n')\n    parser.add\n_argument('--\nconf\n-\nthres\n', \ntype\n=\nfloat\n, \ndefault\n=0.25, \nhelp\n='\nobject\n \nconfidence\n \nthreshold\n')\n\n    parser.add\n_argument('--\niou\n-\nthres\n', \ntype\n=\nfloat\n, \ndefault\n=0.45, \nhelp\n='IOU \nthreshold\n \nfor\n NMS')\n\n    parser.add\n_argument('--\ndevice\n', \ndefault\n='', \nhelp\n='\ncuda\n \ndevice\n, \ni\n.\ne\n. 0 \nor\n 0,1,2,3 \nor\n \ncpu\n')\n\n    parser.add\n_argument('--\nview\n-\nimg\n', \naction\n='\nstore_true\n', \nhelp\n='\ndisplay\n \nresults\n')\n\n    parser.add\n_argument('--\nsave\n-\ntxt\n', \naction\n='\nstore_true\n', \nhelp\n='\nsave\n \nresults\n \nto\n \n*\n.\ntxt\n')\n\n    parser.add\n_argument('--\nsave\n-\nconf\n', \naction\n='\nstore_true\n', \nhelp\n='\nsave\n \nconfidences\n \nin\n --\nsave\n-\ntxt\n \nlabels\n')\n\n    parser.add\n_argument('--\nnosave\n', \naction\n='\nstore_true\n', \nhelp\n='\ndo\n \nnot\n \nsave\n \nimages\n/\nvideos\n')\n\n    parser.add\n_argument('--\nclasses\n', \nnargs\n='+', \ntype\n=\nint\n, \nhelp\n='\nfilter\n \nby\n \nclass\n: --\nclass\n 0, \nor\n --\nclass\n 0 2 3')\n\n    parser.add\n_argument('--\nagnostic\n-\nnms\n', \naction\n='\nstore_true\n', \nhelp\n='\nclass\n-\nagnostic\n NMS')\n\n    parser.add\n_argument('--\naugment\n', \naction\n='\nstore_true\n', \nhelp\n='\naugmented\n \ninference\n')\n\n    parser.add\n_argument('--\nupdate\n', \naction\n='\nstore_true\n', \nhelp\n='\nupdate\n \nall\n \nmodels\n')\n\n    parser.add\n_argument('--\nproject\n', \ndefault\n='\nruns\n/\ndetect\n', \nhelp\n='\nsave\n \nresults\n \nto\n \nproject\n/\nname\n')\n\n    parser.add\n_argument('--\nname\n', \ndefault\n='\nexp\n', \nhelp\n='\nsave\n \nresults\n \nto\n \nproject\n/\nname\n')\n\n    parser.add\n_argument('--\nexist\n-\nok\n', \naction\n='\nstore_true\n', \nhelp\n='\nexisting\n \nproject\n/\nname\n \nok\n, \ndo\n \nnot\n \nincrement\n')\n\n    opt = parser.parse\n_args()\n\n    print(opt)\n    check\n_requirements(\nexclude\n=('\npycocotools\n', '\nthop\n')\n)\n\n    \nwith\n torch.no\n_grad()\n:\n        \nif\n opt.update:  # update all models (\nto\n fix SourceChangeWarning)\n            \nfor\n opt.weights \nin\n \n['\nyolov5s\n.\npt\n', '\nyolov5m\n.\npt\n', '\nyolov5l\n.\npt\n', '\nyolov5x\n.\npt\n']\n:\n                detect\n()\n\n                strip\n_optimizer(\nopt\n.\nweights\n)\n\n        \nelse\n:\n            detect\n()\n\n\n\n\n\n\n\n下为帧处理传递\n\n\nurl = \n\"http://192.168.1.102:8080/stream.html\"\n\n\ndef download\nImg()\n:\n    global url\n    \nwith\n request.urlopen(url) \nas\n f:\n        data = f.read\n()\n\n        img1 = np.frombuffer(data, np.uint8)\n        #print(\n\"img1 shape \"\n, img1.shape) # (\n83653\n,)\n        img_cv = cv2.imdecode(img1, cv2.IMREAD_ANYCOLOR)\n        return img_cv\n\nif\n __name__\n == \n'__main__':\n    parser = argparse.\nArgumentParser()\n\n    parser.add\n_argument('--\nweights\n', \nnargs\n='+', \ntype\n=\nstr\n, \ndefault\n='\nmodels\n/\nyolov5s\n.\npt\n', \nhelp\n='\nmodel\n.\npt\n \npath\n(\ns\n)\n')\n    parser.add\n_argument('--\nsource\n', \ntype\n=\nstr\n, \ndefault\n=\n\"\"\n, \nhelp\n='\nsource\n')\n  # file/folder, \n0\n \nfor\n webcam\n    parser.add\n_argument('--\nimg\n-\nsize\n', \ntype\n=\nint\n, \ndefault\n=640, \nhelp\n='\ninference\n \nsize\n (\npixels\n)\n')\n    parser.add\n_argument('--\nconf\n-\nthres\n', \ntype\n=\nfloat\n, \ndefault\n=0.25, \nhelp\n='\nobject\n \nconfidence\n \nthreshold\n')\n\n    parser.add\n_argument('--\niou\n-\nthres\n', \ntype\n=\nfloat\n, \ndefault\n=0.45, \nhelp\n='IOU \nthreshold\n \nfor\n NMS')\n\n    parser.add\n_argument('--\ndevice\n', \ndefault\n='', \nhelp\n='\ncuda\n \ndevice\n, \ni\n.\ne\n. 0 \nor\n 0,1,2,3 \nor\n \ncpu\n')\n\n    parser.add\n_argument('--\nview\n-\nimg\n', \naction\n='\nstore_true\n', \nhelp\n='\ndisplay\n \nresults\n')\n\n    parser.add\n_argument('--\nsave\n-\ntxt\n', \naction\n='\nstore_true\n', \nhelp\n='\nsave\n \nresults\n \nto\n \n*\n.\ntxt\n')\n\n    parser.add\n_argument('--\nsave\n-\nconf\n', \naction\n='\nstore_true\n', \nhelp\n='\nsave\n \nconfidences\n \nin\n --\nsave\n-\ntxt\n \nlabels\n')\n\n    parser.add\n_argument('--\nnosave\n', \naction\n='\nstore_true\n', \nhelp\n='\ndo\n \nnot\n \nsave\n \nimages\n/\nvideos\n')\n\n    parser.add\n_argument('--\nclasses\n', \nnargs\n='+', \ntype\n=\nint\n, \nhelp\n='\nfilter\n \nby\n \nclass\n: --\nclass\n 0, \nor\n --\nclass\n 0 2 3')\n\n    parser.add\n_argument('--\nagnostic\n-\nnms\n', \naction\n='\nstore_true\n', \nhelp\n='\nclass\n-\nagnostic\n NMS')\n\n\n\n\n\n考虑用opencv保存帧读取图像再用YOLO处理，担心延迟过高，未果，请求指教", "Tag": "算法分析"}
{"Answer": "应该和插件无关，你先测试一个简单的程序，来确认是不是软件安装的问题，直接上cv2，也有可能是加载太慢造成的。另外就是终端是不是被关闭了也需要检查。", "Konwledge_Point": "应对NP完全问题", "Question": "在jupyter notebook为什么运行没反应，安装了nbextensions_configurator之后第一次运行有反应，显示img没定义，之后再运行就没反应了\n\n\nimport\n cv2\n\nimport\n matplotlib.pyplot \nas\n plt\n\nimport\n numpy \nas\n np\n%matplotlib inline\n\nimg\n=cv2.imread('style.jpg')\n\n\n\n\nimg\n\n\n", "Tag": "算法分析"}
{"Answer": "过拟合了可能", "Konwledge_Point": "应对NP完全问题", "Question": "利用梯度下降训练参数\nerror是一个和方差sse，如下所示，一开始随着训练次数的增加sse确实在减少，但是到了差不多300次的时候又开始增加了，这是为啥，如下是我写的梯度下降，损失函数如下\n\n\nloss\n = np.square(np.dot(u_vector, i_vector) - real_rating) - REG * (u_vector * u_vector + i_vector * i_vector)\n\n\n\n\n实现的梯度下降如下\n\n\nfor epoch \nin\n range(LFM_EPOCHS):\n    for uid, iid, real_rating \nin\n train_data.itertuples(\nindex=False):\n\n        \nu_vector\n = user_matrix[uid]\n        \ni_vector\n = movie_matrix[iid]\n        \nerr\n = np.float32(real_rating - np.dot(u_vector, i_vector))  \n# 真实值和预测值的偏差\n\n        \n# 更新梯度\n\n        \nu_vector_cache\n = u_vector  \n# 下面更新用户向量会覆盖掉，而更新物品向量需要用户向量旧值，所以这里得保存旧值\n\n        u_vector += LEARN * (err * i_vector - REG * u_vector)  \n# 这是一个numpy对象\n\n        i_vector += LEARN * (err * u_vector_cache - REG * i_vector)  \n# 这是一个Numpy对象\n\n        user_matrix[uid] = u_vector\n        movie_matrix[iid] = i_vector\n    \nif\n epoch % \n10\n == \n0\n:  \n# 每10次训练，计算一次sse\n\n        test_1(epoch)\n\n\n\n\n\n\n\n", "Tag": "算法分析"}
{"Answer": "import pandas as pd\nimport numpy as np\ndata={\n'AUD/USD': [1.72,1.74,np.nan,168,1.75,1.71,],\n'EUR/AUD': [0.62,0.63,0.59,0.64,np.nan,0.61,],}\nindex = ['2022-10-1','2022-10-2','2022-10-3','2032-10-4','2022-10-7','2022-10-8']\ndf =pd.DataFrame(data,index)\n \nsel_q1=['2022-10-1','2022-10-7']\nq1=df.loc[sel_q1]\nprint(q1)\n \nprint('\\n\\n')\n \n(start_q2,stop_q2)=(0,2)\nq2=df.iloc[start_q2:stop_q2]\nprint(q2)\n \nprint('\\n\\n')\n \n(start_q3,stop_q3)=(0,2)\nq3=df[start_q3:stop_q3]\nprint(q3)\n \nprint('\\n\\n')\n \nrow_sel_q4=['2022-10-1','2022-10-2']\ncol_sel_q4=['AUD/USD']\nq4=df.loc[row_sel_q4,col_sel_q4]\nprint(q4)\n \n\n", "Konwledge_Point": "应对NP完全问题", "Question": "根据提供的DataFrame df回的下列四个问题\nimport pandas as pd\nimport numpy as np\ndata={\n    'AUD/USD': [1.72, 1.74, no.nan, 168, 1.75, 1.71,],\n    'EUR/AUD': [0.62, 0.63, 0.59, 0.64, no.nan, 0.61,],}\nindex = ['2022-10-1', '2022-10-2', '2022-10-3',2032-10-4', '2022-10-7', '2022-10-8',]\ndf =pd.DataFrame(data, index)\n\n\n(1)\n输出结果为\n|                   |AUD/USD|EUR/AUD|\n|2022-10-1 |   1.72      |     0.62    |\n|2022-10-7 |   1.75      |   no.nan  |\n问\nsel_q1=？\nq1=df loc[sel_q1]\n\n\n（2）(3)\n输出结果为\n|                  |AUD/USD|EUR/AUD|\n|2022-10-1|   1.72      |   0.62      |\n|2022-10-2|   1.74      |   0.63      |\n问(2)\n（start_q2, stop_q2）= ( ?, ? )\n  q2 = df.iloc[start_q2:stop_q2]\n问（3）\n  (start_q3, stop_q3) = ( ?, ? )\n  q3 = df[start_q3:stop_q3]\n\n\n(4)\n输出结果为\n|                  |AUD/USD|\n|2022-10-1|    1.72     |\n|2022-10-2|    1.74     |\n问\nrow_sel_q4 = ?\ncol_sel_q4 = ?\nq4 = df. loc[row_sel_q4, col_sel_q4]", "Tag": "算法分析"}
{"Answer": "已解决", "Konwledge_Point": "应对NP完全问题", "Question": "python在多线程用cv2的imshow显示不出窗口的问题\n代码如下，运行之后第一次可以显示图片，关闭之后再打开就显示不了了，但从打印信息来看是可以运行显示的线程的\n\n\n\nimport cv2\nimport numpy as np\nimport threading\n\nfrom\n easygui import *\nimg1 = cv2.imread(\n'01.png'\n)\nimg2 = cv2.imread(\n'02.png'\n)\nimg_show=[\nTrue\n]\ndef img1_method():\n    \nprint\n(\n'打开img1'\n)\n    \nwhile\n img_show[0]:\n       cv2.imshow(\n\"img1\"\n,img1)\n       cv2.waitKey(5)\n    \nprint\n(\n'关闭img1'\n)\ndef img2_method():\n    \nprint\n(\n'打开img2'\n)\n    \nwhile\n img_show[0]:\n       cv2.imshow(\n\"img2\"\n,img2)\n       cv2.waitKey(5)\n    \nprint\n(\n'关闭img2'\n)\ndef open_close():\n    \nwhile\n \nTrue\n:\n        choices = [\n'[1]打开'\n, \n'[2]关闭'\n, \n'[3]退出'\n]\n        \nchoice\n=buttonbox(msg='请选择', \ntitle\n=\n'cv2测试'\n, \nchoices\n=choices,\n                  \ndefault_choice\n=choices[2], \ncancel_choice\n=choices[2], \ncallback\n=None, \nrun\n=\nTrue\n)\n        \nchoose\n=choices.index(choice)+1\n        \nif\n \nchoose\n==1:\n            img_show[0]=\nTrue\n\n            threading.Thread(\ntarget\n=img1_method).start()\n            threading.Thread(\ntarget\n=img2_method).start()\n        elif \nchoose\n==2:\n            img_show[0]=\nFalse\n\n        \nelse\n:\n            break\n\nthreading.Thread(\ntarget\n=img1_method).start()\nthreading.Thread(\ntarget\n=img2_method).start()\nthreading.Thread(\ntarget\n=open_close).start()\n", "Tag": "算法分析"}
{"Answer": "因为没有无参数构造函数\r\n\r\n要么：\r\n\r\nmyPoint(double x0  , double y0 ) :x(x0), y(y0) {}\r\n->\r\nmyPoint(double x0 =0 , double y0 =0) :x(x0), y(y0) {}\r\n\r\n要么加上\r\n\r\nmyPoint() :x(0), y(0) {}\r\n\r\n# 问题解决的话，请点下采纳", "Konwledge_Point": "应对NP完全问题", "Question": "为什么构造函数不含有默认参数后就提示默认构造函数已被删除？\n#include\n#include\nusing namespace std;\n\nclass myPoint {\npublic:\n    myPoint(double x0  , double y0 ) :x(x0), y(y0) {}\n    myPoint(myPoint& np) :x(np.x), y(np.y) {}\n    //得到X或Y坐标\n    double GetX() { return x; }\n    double GetY() { return y; }\n    //设置X或Y坐标\n    void SetX(double x0) { x = x0; }\n    void SetY(double y0) { y = y0; }\n    //设置点坐标\n    void SetPoint(double x0, double y0) { x = x0; y = y0; }\n    void SetPoint(myPoint& np) { x = np.x; y = np.y; }\n    //计算三角形边长\n    double  GetLength(myPoint p) {\n        return sqrt((x - p.x) * (x - p.x) + (y - p.y) * (y - p.y));\n    }\n    void Printit() { cout << \" (\" << x << \",\" << y << \") \"; }\nprivate:\n    double x, y;\n};\n\nclass Triangle\n{\npublic:\n    //计算三角形面积\n    double area(double a, double b, double c);\n    //计算三角形周长\n    double perimeter(double a, double b, double c)\n    {\n        return a + b + c;\n    }\n    myPoint p1, p2, p3;\n};\n\ndouble Triangle::area(double a, double b, double c)\n{\n    double p = (a + b + c) / 2;\n    return sqrt(p * (p - a) * (p - b) * (p - c));\n}\n\nint main()\n{\n    Triangle t1;\n    t1.p1.SetPoint(0.0, 0.0);\n    t1.p2.SetPoint(4.0, 0.0);\n    t1.p3.SetPoint(0.0, 3.0);\n    cout << \"The triangle's area is  \" << t1.area(t1.p1.GetLength(t1.p3), t1.p2.GetLength(t1.p1), t1.p2.GetLength(t1.p3)) << endl;\n    cout << \"The triangle's area is  \" << t1.perimeter(t1.p1.GetLength(t1.p3), t1.p2.GetLength(t1.p1), t1.p2.GetLength(t1.p3)) << endl;\n    return 0;\n}\n\n\n\n\n\n\n\n\n\n\n在给构造函数的每个形参都加上默认参数就好了，请问是什么原因", "Tag": "算法分析"}
