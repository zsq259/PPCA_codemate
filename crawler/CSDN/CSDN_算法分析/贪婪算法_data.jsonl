{"Answer": "&lt;p&gt;A*&amp;#61;耗散g&amp;#43;启发式h&amp;#xff0c;当g&amp;#61;0时&amp;#xff0c;A*与GBFS等价&lt;/p&gt;", "Konwledge_Point": "贪婪算法", "Question": ["路径规划问题 A*算法和GBFS算法的联系", ["请问在路径规划问题中 A*算法和贪婪最佳优先GBFS算法的联系是什么？"]], "Tag": "算法设计"}
{"Answer": "&lt;p&gt;你可以看看CGAL的表面重建&lt;br /&gt;&lt;a href=\"https://doc.cgal.org/latest/Manual/tuto_reconstruction.html\" id=\"textarea_1644917442012_1644917605457_0\" target=\"_blank\" rel=\"noopener noreferrer\"&gt;&lt;span class=\"md_link_url\"&gt;https://doc.cgal.org/latest/Manual/tuto_reconstruction.html&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;", "Konwledge_Point": "贪婪算法", "Question": ["开放式表面三维点云重建", ["开放式表面（正面例子：室内扫描，反面例子：史坦佛兔）重建的最佳解法是不是得用非隐式曲面的写法", "\n", "问题一：想问是否有人也碰巧在看pcl中的贪婪投影演算法，对里面的gp3.h gp3.hpp或演算法中可见不可见的选点方式有研究( 还是小妹直接去私点云侠！", "问题二：若不用贪婪投影演算法，是否开放式表面还有更好的点云重建方法？泊松问题在于封闭式表面，ball pivot 问题在于不适应不均匀点云", "\n", "想找人讨论却发现网上没怎么找到资料⋯"]], "Tag": "算法设计"}
{"Answer": "&lt;p&gt;你把gym换成0.25.2版本就行了。  pip install gym&amp;#61;&amp;#61;0.25.2&lt;/p&gt;", "Konwledge_Point": "贪婪算法", "Question": ["强化学习DQN:AttributeError: 'CartPoleEnv' object has no attribute 'seed'", ["运行动手学强化学习中DQN算法时出现问题，求帮助啊", "\n", "\n", "import", " random\n", "import", " gym\n", "import", " numpy ", "as", " np\n", "import", " collections\n", "from", " tqdm ", "import", " tqdm\n", "import", " torch\n", "import", " torch.nn.functional ", "as", " F\n", "import", " matplotlib.pyplot ", "as", " plt\n", "import", " rl_utils\n\n\n", "class", " ", "ReplayBuffer", ":\n    ", "''' 经验回放池 '''", "\n\n    ", "def", " ", "__init__", "(", "self, capacity", "):\n        self.buffer = collections.deque(maxlen=capacity)  ", "# 队列,先进先出", "\n\n    ", "def", " ", "add", "(", "self, state, action, reward, next_state, done", "):  ", "# 将数据加入buffer", "\n        self.buffer.append((state, action, reward, next_state, done))\n\n    ", "def", " ", "sample", "(", "self, batch_size", "):  ", "# 从buffer中采样数据,数量为batch_size", "\n        ", "# random.sample(x,size) 随机截取列表x指定size长度，顺序不变", "\n        transitions = random.sample(self.buffer, batch_size)\n        ", "# transitions 包含很多transition，而transition中又包含state, action, reward, next_state, done", "\n        ", "# *transitions 是将transition的参数解包出来state, action, reward, next_state, done", "\n        ", "# zip(*transitions)是将属于一种属性的封装在一起，如所有state(s1,s2,s3,...)", "\n        state, action, reward, next_state, done = ", "zip", "(*transitions)\n        ", "return", " np.array(state), action, reward, np.array(next_state), done\n\n    ", "def", " ", "size", "(", "self", "):  ", "# 目前buffer中数据的数量", "\n        ", "return", " ", "len", "(self.buffer)\n\n\n", "class", " ", "Qnet", "(torch.nn.Module):\n    ", "''' 只有一层隐藏层的Q网络 '''", "\n\n    ", "def", " ", "__init__", "(", "self, state_dim, hidden_dim, action_dim", "):\n        ", "super", "(Qnet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n\n    ", "def", " ", "forward", "(", "self, x", "):\n        x = F.relu(self.fc1(x))  ", "# 隐藏层使用ReLU激活函数", "\n        ", "return", " self.fc2(x)\n\n\n", "class", " ", "DQN", ":\n    ", "''' DQN算法 '''", "\n\n    ", "def", " ", "__init__", "(", "self, state_dim, hidden_dim, action_dim, learning_rate, gamma,\n                 epsilon, target_update, device", "):\n        self.action_dim = action_dim\n        self.q_net = Qnet(state_dim, hidden_dim,\n                          self.action_dim).to(device)  ", "# Q网络", "\n        ", "# 目标网络", "\n        self.target_q_net = Qnet(state_dim, hidden_dim,\n                                 self.action_dim).to(device)\n        ", "# 使用Adam优化器", "\n        self.optimizer = torch.optim.Adam(self.q_net.parameters(),\n                                          lr=learning_rate)\n        self.gamma = gamma  ", "# 折扣因子", "\n        self.epsilon = epsilon  ", "# epsilon-贪婪策略", "\n        self.target_update = target_update  ", "# 目标网络更新频率", "\n        self.count = ", "0", "  ", "# 计数器,记录更新次数", "\n        self.device = device\n\n    ", "def", " ", "take_action", "(", "self, state", "):  ", "# epsilon-贪婪策略采取动作", "\n        ", "if", " np.random.random() < self.epsilon:\n            action = np.random.randint(self.action_dim)\n        ", "else", ":\n            state = torch.tensor([state], dtype=torch.", "float", ").to(self.device)\n            action = self.q_net(state).argmax().item()\n        ", "return", " action\n\n    ", "def", " ", "update", "(", "self, transition_dict", "):\n        states = torch.tensor(transition_dict[", "'states'", "],\n                              dtype=torch.", "float", ").to(self.device)\n        actions = torch.tensor(transition_dict[", "'actions'", "]).view(-", "1", ", ", "1", ").to(\n            self.device)\n        rewards = torch.tensor(transition_dict[", "'rewards'", "],\n                               dtype=torch.", "float", ").view(-", "1", ", ", "1", ").to(self.device)\n        next_states = torch.tensor(transition_dict[", "'next_states'", "],\n                                   dtype=torch.", "float", ").to(self.device)\n        dones = torch.tensor(transition_dict[", "'dones'", "],\n                             dtype=torch.", "float", ").view(-", "1", ", ", "1", ").to(self.device)\n\n        q_values = self.q_net(states).gather(", "1", ", actions)  ", "# Q值", "\n        ", "# 下个状态的最大Q值", "\n        max_next_q_values = self.target_q_net(next_states).", "max", "(", "1", ")[", "0", "].view(\n            -", "1", ", ", "1", ")\n        q_targets = rewards + self.gamma * max_next_q_values * (", "1", " - dones\n                                                                )  ", "# TD误差目标", "\n        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  ", "# 均方误差损失函数", "\n        self.optimizer.zero_grad()  ", "# PyTorch中默认梯度会累积,这里需要显式将梯度置为0", "\n        dqn_loss.backward()  ", "# 反向传播更新参数", "\n        self.optimizer.step()\n\n        ", "if", " self.count % self.target_update == ", "0", ":\n            self.target_q_net.load_state_dict(\n                self.q_net.state_dict())  ", "# 更新目标网络", "\n        self.count += ", "1", "\n\n\nlr = ", "2e-3", "\nnum_episodes = ", "500", "\nhidden_dim = ", "128", "\ngamma = ", "0.98", "\nepsilon = ", "0.01", "\ntarget_update = ", "10", "\nbuffer_size = ", "10000", "\nminimal_size = ", "500", "\nbatch_size = ", "64", "\ndevice = torch.device(", "\"cuda\"", ") ", "if", " torch.cuda.is_available() ", "else", " torch.device(\n    ", "\"cpu\"", ")\n\nenv_name = ", "'CartPole-v1'", "\nenv = gym.make(env_name)\nrandom.seed(", "0", ")\nnp.random.seed(", "0", ")\nenv.seed(", "0", ")\ntorch.manual_seed(", "0", ")\nreplay_buffer = ReplayBuffer(buffer_size)\nstate_dim = env.observation_space.shape[", "0", "]\naction_dim = env.action_space.n\nagent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,\n            target_update, device)\n\nreturn_list = []\n", "for", " i ", "in", " ", "range", "(", "10", "):\n    ", "with", " tqdm(total=", "int", "(num_episodes / ", "10", "), desc=", "'Iteration %d'", " % i) ", "as", " pbar:\n        ", "for", " i_episode ", "in", " ", "range", "(", "int", "(num_episodes / ", "10", ")):\n            episode_return = ", "0", "\n            state = env.reset()\n            done = ", "False", "\n            ", "while", " ", "not", " done:\n                action = agent.take_action(state)\n                next_state, reward, done, _ = env.step(action)\n                replay_buffer.add(state, action, reward, next_state, done)\n                state = next_state\n                episode_return += reward\n                ", "# 当buffer数据的数量超过一定值后,才进行Q网络训练", "\n                ", "if", " replay_buffer.size() > minimal_size:\n                    b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n                    transition_dict = {\n                        ", "'states'", ": b_s,\n                        ", "'actions'", ": b_a,\n                        ", "'next_states'", ": b_ns,\n                        ", "'rewards'", ": b_r,\n                        ", "'dones'", ": b_d\n                    }\n                    agent.update(transition_dict)\n            return_list.append(episode_return)\n            ", "if", " (i_episode + ", "1", ") % ", "10", " == ", "0", ":\n                pbar.set_postfix({\n                    ", "'episode'", ":\n                        ", "'%d'", " % (num_episodes / ", "10", " * i + i_episode + ", "1", "),\n                    ", "'return'", ":\n                        ", "'%.3f'", " % np.mean(return_list[-", "10", ":])\n                })\n            pbar.update(", "1", ")\n\n\n", "\n", "运行结果及报错内容", "\n", "\nTraceback (most recent ", "call", " last):\n  File \"E:\\graduate student\\Python\\Reinforcement learning\\HANDS-ON Reinforcement learning\\04_DQN\\01_DQN.py\", ", "line", " ", "120", ", ", "in", " <module>\n    env.seed(", "0", ")\n  File \"D:\\Python39\\lib\\site-packages\\gym\\core.py\", ", "line", " ", "241", ", ", "in", " __getattr__\n    ", "return", " getattr(self.env, ", "name", ")\n  File \"D:\\Python39\\lib\\site-packages\\gym\\core.py\", ", "line", " ", "241", ", ", "in", " __getattr__\n    ", "return", " getattr(self.env, ", "name", ")\n  File \"D:\\Python39\\lib\\site-packages\\gym\\core.py\", ", "line", " ", "241", ", ", "in", " __getattr__\n    ", "return", " getattr(self.env, ", "name", ")\nAttributeError: ", "'CartPoleEnv'", " ", "object", " has ", "no", " ", "attribute", " ", "'seed'", "\n"]], "Tag": "算法设计"}
{"Answer": "&lt;p&gt;应该是没有读取进来数据。在getstate函数里&amp;#xff0c;for循环没有运行&amp;#xff0c;所以state是None。也可能len(block)刚好等于1&amp;#xff0c;而你又用len(block)-1,所以循环没有进行。我不知道你是不是想用for去枚举block&amp;#xff0c;如果是&amp;#xff0c;应该是for i in range(len(block))&lt;/p&gt;", "Konwledge_Point": "贪婪算法", "Question": ["请各位大神帮帮忙,因为这是毕业论文所用的代码,我想问问到底是否是对的,看看是我代码错了,还是逻辑错了", ["首先我这里有一个yunfuzai_main.py:", "\n\n", "\n", "from dqn_agent import Agent\nfrom model import QNetwork\nimport matplotlib.pyplot as plt\nimport  numpy as np\nimport torch\nimport xlrd\nfrom openpyxl import load_workbook\nimport gc\n\n\nSTATE_SIZE = 10\nEPISODE_COUNT = 1000\n\n# def dqn(n_episodes=EPISODE_COUNT,eps_start=2.0,eps_end=0.03,eps_decay=0.990):\n#     scores = []\n#     for i_episode in range(1, n_episodes + 1):\n#         print(\"Episode\" + str(i_episode))\n#         state = getState(stockData, 0, STATE_SIZE + 1)\n#         total_profit = 0\n#         agent.inventory = []\n#         eps = eps_start\n#\n#         for t in range(l):\n#             action = agent.act(state, eps)\n#             next_state = getState(stockData, t + 1, STATE_SIZE + 1)\n#             reward = 0\n#\n#             if action == 1:  # 买入\n#                 agent.inventory.append(stockData[t])\n#                 # print(\"buy\" + str(stockData[t]))\n#             elif action == 2 and len(agent.inventory) > 0:  # 卖出\n#                 bought_price = agent.inventory.pop(0)\n#                 total_profit += stockData[t] - bought_price\n#                 # reward = max(stockData[t] - bought_price, 0)\n#                 reward = stockData[t] - bought_price\n#                 # print(\"Sell: \" + str(stockData[t]) + \" | Profit: \" + str(stockData[t] - bought_price))\n#             done = 1 if t == l - 1 else 0\n#             agent.step(state, action, reward, next_state, done)\n#             eps = max(eps_end, eps * eps_decay)\n#             state = next_state\n#\n#             # if done:\n#             #     print(\"------------------------------\")\n#             #     print(\"total_profit = \" + str(total_profit))\n#             #     print(\"------------------------------\")\n#         scores.append(total_profit)\n#     return scores\n\ndef dqn1(n_episodes1=EPISODE_COUNT, eps_start1=1.0, eps_end1=0.01, eps_decay1=0.9995):\n    scores1 = []\n    for i_episode in range(1,n_episodes1+1):\n        print(\"Episode\" + str(i_episode))\n\n        state = getState(stockData, 0, STATE_SIZE + 1)\n        agent.inventory = []\n        eps = eps_start1\n        #global loss\n        print(1)\n        for t in range(l):\n            print(2)\n            action = agent.act(state,eps)\n            print(1)\n            next_state = getState(stockData, t + 1, STATE_SIZE + 1)\n            reward = 0\n            done = 1 if t == l - 1 else 0\n            global loss\n            loss = agent.step(state, action, reward, next_state, done)\n\n            print(\"loss\", + str(loss))\n            scores1.append(loss)\n            gc.collect()\n        gc.collect()\n\n    return scores1\n\n            # if action == 1:# 过载\n            #     agent.inventory.append(stockData[t])\n            # elif action == 2 and len(agent.inventory) > 0:\n\n\ndef getState(data, t, n):\n    d = t - n + 1\n    # block = data[d:t + 1] if d>= 0 else -d * [data[0]]+ data[0:t+1]\n    block = data[d:t + 1]\n    #res = [0 for x in range(0, n)]\n    #res = []\n    buffer = []\n    for i in range(len(block) - 1):\n        print(\"res=\",buffer[i])\n        buffer.append(block[i + 1]-block[i])\n        #print(\"res=\",res[i])\n        return np.array([buffer])\n    #return np.array([res])\n\nif __name__== '__main__':\n    print(1)\n    #stockData = []\n    #stockData = []\n    stockData = [None]*801\n    datas1 =xlrd.open_workbook(r'C:\\Users\\86138\\Desktop\\zi_ding_yi.xlsx',{'constant_memory':True})\n\n    #datas1 = xlrd.open_workbook(r'C:\\Users\\86138\\Desktop\\zi_ding_yi.xlsx')\n    print(2)\n    table = datas1.sheets()[0]\n    row_num = 0\n    print(3)\n    # for item in table[1:]:\n    #      stockData.append(float(table.col_values(6)))\n    col = table.col_values(5)\n    while row_num <= 800 :\n        # stockData[row_num] = table.col_values(5,0,row_num)\n        #stockData[row_num] = table.cell_value(row_num,5)\\\n        stockData.append(table.cell_value(row_num,5))\n        row_num += 1\n     # stockData{row_num} = table.col_values(5,0,row_num)\n     # row_num =+1\n\n    agent = Agent(state_size=STATE_SIZE, action_size=3)\n    print(4)\n    l = len(stockData) - 1\n\n    # scores = dqn()\n    scores1 =dqn1()", "\n\n", "它说我的错误是：", "\n\n", "\n", "C:\\Users\\86138\\anaconda3\\python.exe C:/Users/86138/Desktop/stockPrediction-master/yunfuzai_main.py\n1\n2\n3\n4\nEpisode1\n1\n2\nTraceback (most recent call last):\n  File \"C:/Users/86138/Desktop/stockPrediction-master/yunfuzai_main.py\", line 123, in <module>\n    scores1 =dqn1()\n  File \"C:/Users/86138/Desktop/stockPrediction-master/yunfuzai_main.py\", line 61, in dqn1\n    action = agent.act(state,eps)\n  File \"C:\\Users\\86138\\Desktop\\stockPrediction-master\\dqn_agent.py\", line 136, in act\n    state = torch.tensor(state).float.unsqueeze(0).to(device)\nRuntimeError: Could not infer dtype of NoneType\n", "\n\n", "而我的dqn_agent.py:", "\n\n", "\n", "import numpy as np\nimport random\nfrom collections import namedtuple, deque\n\nfrom model import QNetwork\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n#初始化超参数\nBUFFER_SIZE = int(1e5)\n#缓冲去大小，重播缓冲区大小\nBATCH_SIZE = 64\n#批处理大小，最小批量大小，minbatch size\nGAMMA = 0.99\n##折扣率\nTAU = 1e-3\n#用于目标参数的软更新\nLR = 5e-4\n#学习率\nUPDATE_EVERY = 4\n#更新网络的快慢\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#首先看下你的设备有无cuda可用：\n\nclass ReplayBuffer:\n    def __init__(self, action_size, buffer_size, batch_size):# 初始化记忆库\n        self.action_size = action_size\n        self.memory = deque(maxlen=buffer_size)\n        #使用 deque(maxlen=N) 构造函数会创建一个固定大小的队列。当新的元素加入并且这个队列已满的时候，最老的元素会自动被移除掉\n        self.batch_size = batch_size\n        #batch字面上是批量的意思，在深度学习中指的是计算一次cost需要的输入数据个数。\n        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n        #经验池的作用就是把每次进行的游戏回合transition（episode，step）记录下来存储起来。\n        # 在训练的时候则是在经验池中随机取一组transition batch对Q网络进行优化。同时，也需要及时丢掉过老的记录，及时更新。\n        #首先，定义了一个名为Experience的namedtuple。包括内容除了上面算法中提到的（s,a,r,s'）.还有结束的标识’done’。\n    def add(self, state, action, reward, next_state, done):# 向记忆库中加入一个记忆\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n\n    def sample(self):# 随机取出一个minibatch\n        experiences = random.sample(self.memory, k=self.batch_size)\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).float().to(device)\n\n        return (states, actions, rewards, next_states, dones)\n\n    def __len__(self):\n        return len(self.memory)\n\n\nclass Agent:\n\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n\n        # Q-Network\n        #在学习过程中，我们使用两个不相关的Q网络（Q_network_local和Q_network_target）来计算预测值（权重θ）和目标值（权重θ’）。\n        # 经过若干步骤后，目标网络会被冻结，然后拷贝实际的Q网络的权重到目标网络权重。\n        # 冻结目标Q网络一段时间再用实际Q网络的权重更新其权重，可以稳定训练过程\n        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n        self.qnetwork_target = QNetwork(state_size, action_size).to(device)#目标策略，智能体要学习的策略\n        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n        #在线性回归或者监督学习中，我们会计算预测值与真实值之间的差距，也就是loss\n        #在计算得出loss之后，通常会使用Optimizer对所构造的数学模型/网络模型进行参数优化，\n        #通常情况下，优化的最终目的是使得loss趋向于最小。\n\n\n        # Replay Buffer，所以我们设置一个replay_buffer，获得新的交互数据，抛弃旧的数据，\n        # 并且每次从这个replay_buffer中随机取一个batch，来训练我们的系统\n        self.memory = ReplayBuffer(action_size, buffer_size=BUFFER_SIZE,batch_size=BATCH_SIZE)\n        # 初始化迭代步数\n        self.t_step = 0\n        # 初始化持仓\n        self.inventory = []\n    # Experience Replay就是这样的一种技术，在游戏数据的采集过程中，所有的经验数据<script type=\"math/tex\" id=\"MathJax-Element-85\">< s, a, r, s'\n    # ></script>都被存储到一个回放池(replay memory)中。当训练网络时，从回放池中随机地取一小撮数据，\n    # 而不是最新连续的几条转换数据，进行训练。\n\n    def step(self, state, action, reward, next_state, done):\n        # 每一步需要先存储记忆库\n        self.memory.add(state, action, reward, next_state, done)\n\n        # 每隔若干步学习一次\n        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n        if self.t_step == 0:\n            if len(self.memory) > BATCH_SIZE:\n                experience = self.memory.sample()\n                self.learn(experience, GAMMA)\n\n    def learn(self, experience, gamma):\n        # 更新迭代\n        states, actions, rewards, next_states, dones = experience\n\n        # target network:compute and minimize the loss.计算并最小化损失\n        # Get max predicted Q values(for next states) from target model:从目标模型得到最大的预测Q值(下一个状态)\n        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n        # compute Q target for current states:计算当前状态的Q目标。\n        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n\n        Q_expected = self.qnetwork_local(states).gather(1, actions.long())# 固定行号，确认列\n        # Compute loss\n        loss = F.mse_loss(Q_expected, Q_targets)\n        # Minimize the loss\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        # update target network\n        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=TAU)\n        return loss\n\n\n    def soft_update(self, local_model, target_model, tau):\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n    #实现Q'到Q的逼近 use .data and .data.copy#\n    def act(self, state, eps = 0.):\n        #Returns actions for given state as per current policy.\n        #Params\n        # state (array_like): current state\n        # eps (float): epsilon, for epsilon-greedy action selection\n        #参数个数\n        #状态(array_like):当前状态\n        #eps (float):用于epsilon-贪婪动作选择\n\n\n        # 返回动作值orch.unsqueeze()这个函数主要是对数据维度进行扩充\n        #state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        state = torch.tensor(state).float.unsqueeze(0).to(device)\n        self.qnetwork_local.eval()\n        with torch.no_grad():\n            action_values = self.qnetwork_local(state)\n        self.qnetwork_local.train()\n# # Epsilon-greedy action selection\n        if random.random() > eps:\n            return np.argmax(action_values.cpu().data.numpy())\n        else:\n            return random.choice(np.arange(self.action_size))\n        #eps:根据当前策略返回给定状态的操作参数个数", "\n\n", "我也是真没辙了，所以我想问问，到底是怎么回事，因为我真实想法是把那个loss输出来。但似乎不行，想请各位大神帮我看看，因为是毕业论文用的代码，所以就想问问。拜托了"]], "Tag": "算法设计"}
{"Answer": "&lt;p&gt;A*&amp;#61;耗散g&amp;#43;启发式h&amp;#xff0c;当g&amp;#61;0时&amp;#xff0c;A*与GBFS等价&lt;/p&gt;", "Konwledge_Point": "贪婪算法", "Question": ["路径规划问题 A*算法和GBFS算法的联系", ["请问在路径规划问题中 A*算法和贪婪最佳优先GBFS算法的联系是什么？"]], "Tag": "算法设计"}
{"Answer": "&lt;p&gt;你可以看看CGAL的表面重建&lt;br /&gt;&lt;a href=\"https://doc.cgal.org/latest/Manual/tuto_reconstruction.html\" id=\"textarea_1644917442012_1644917605457_0\" target=\"_blank\" rel=\"noopener noreferrer\"&gt;&lt;span class=\"md_link_url\"&gt;https://doc.cgal.org/latest/Manual/tuto_reconstruction.html&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;", "Konwledge_Point": "贪婪算法", "Question": ["开放式表面三维点云重建", ["开放式表面（正面例子：室内扫描，反面例子：史坦佛兔）重建的最佳解法是不是得用非隐式曲面的写法", "\n", "问题一：想问是否有人也碰巧在看pcl中的贪婪投影演算法，对里面的gp3.h gp3.hpp或演算法中可见不可见的选点方式有研究( 还是小妹直接去私点云侠！", "问题二：若不用贪婪投影演算法，是否开放式表面还有更好的点云重建方法？泊松问题在于封闭式表面，ball pivot 问题在于不适应不均匀点云", "\n", "想找人讨论却发现网上没怎么找到资料⋯"]], "Tag": "算法设计"}
{"Answer": "&lt;p&gt;你把gym换成0.25.2版本就行了。  pip install gym&amp;#61;&amp;#61;0.25.2&lt;/p&gt;", "Konwledge_Point": "贪婪算法", "Question": ["强化学习DQN:AttributeError: 'CartPoleEnv' object has no attribute 'seed'", ["运行动手学强化学习中DQN算法时出现问题，求帮助啊", "\n", "\n", "import", " random\n", "import", " gym\n", "import", " numpy ", "as", " np\n", "import", " collections\n", "from", " tqdm ", "import", " tqdm\n", "import", " torch\n", "import", " torch.nn.functional ", "as", " F\n", "import", " matplotlib.pyplot ", "as", " plt\n", "import", " rl_utils\n\n\n", "class", " ", "ReplayBuffer", ":\n    ", "''' 经验回放池 '''", "\n\n    ", "def", " ", "__init__", "(", "self, capacity", "):\n        self.buffer = collections.deque(maxlen=capacity)  ", "# 队列,先进先出", "\n\n    ", "def", " ", "add", "(", "self, state, action, reward, next_state, done", "):  ", "# 将数据加入buffer", "\n        self.buffer.append((state, action, reward, next_state, done))\n\n    ", "def", " ", "sample", "(", "self, batch_size", "):  ", "# 从buffer中采样数据,数量为batch_size", "\n        ", "# random.sample(x,size) 随机截取列表x指定size长度，顺序不变", "\n        transitions = random.sample(self.buffer, batch_size)\n        ", "# transitions 包含很多transition，而transition中又包含state, action, reward, next_state, done", "\n        ", "# *transitions 是将transition的参数解包出来state, action, reward, next_state, done", "\n        ", "# zip(*transitions)是将属于一种属性的封装在一起，如所有state(s1,s2,s3,...)", "\n        state, action, reward, next_state, done = ", "zip", "(*transitions)\n        ", "return", " np.array(state), action, reward, np.array(next_state), done\n\n    ", "def", " ", "size", "(", "self", "):  ", "# 目前buffer中数据的数量", "\n        ", "return", " ", "len", "(self.buffer)\n\n\n", "class", " ", "Qnet", "(torch.nn.Module):\n    ", "''' 只有一层隐藏层的Q网络 '''", "\n\n    ", "def", " ", "__init__", "(", "self, state_dim, hidden_dim, action_dim", "):\n        ", "super", "(Qnet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n\n    ", "def", " ", "forward", "(", "self, x", "):\n        x = F.relu(self.fc1(x))  ", "# 隐藏层使用ReLU激活函数", "\n        ", "return", " self.fc2(x)\n\n\n", "class", " ", "DQN", ":\n    ", "''' DQN算法 '''", "\n\n    ", "def", " ", "__init__", "(", "self, state_dim, hidden_dim, action_dim, learning_rate, gamma,\n                 epsilon, target_update, device", "):\n        self.action_dim = action_dim\n        self.q_net = Qnet(state_dim, hidden_dim,\n                          self.action_dim).to(device)  ", "# Q网络", "\n        ", "# 目标网络", "\n        self.target_q_net = Qnet(state_dim, hidden_dim,\n                                 self.action_dim).to(device)\n        ", "# 使用Adam优化器", "\n        self.optimizer = torch.optim.Adam(self.q_net.parameters(),\n                                          lr=learning_rate)\n        self.gamma = gamma  ", "# 折扣因子", "\n        self.epsilon = epsilon  ", "# epsilon-贪婪策略", "\n        self.target_update = target_update  ", "# 目标网络更新频率", "\n        self.count = ", "0", "  ", "# 计数器,记录更新次数", "\n        self.device = device\n\n    ", "def", " ", "take_action", "(", "self, state", "):  ", "# epsilon-贪婪策略采取动作", "\n        ", "if", " np.random.random() < self.epsilon:\n            action = np.random.randint(self.action_dim)\n        ", "else", ":\n            state = torch.tensor([state], dtype=torch.", "float", ").to(self.device)\n            action = self.q_net(state).argmax().item()\n        ", "return", " action\n\n    ", "def", " ", "update", "(", "self, transition_dict", "):\n        states = torch.tensor(transition_dict[", "'states'", "],\n                              dtype=torch.", "float", ").to(self.device)\n        actions = torch.tensor(transition_dict[", "'actions'", "]).view(-", "1", ", ", "1", ").to(\n            self.device)\n        rewards = torch.tensor(transition_dict[", "'rewards'", "],\n                               dtype=torch.", "float", ").view(-", "1", ", ", "1", ").to(self.device)\n        next_states = torch.tensor(transition_dict[", "'next_states'", "],\n                                   dtype=torch.", "float", ").to(self.device)\n        dones = torch.tensor(transition_dict[", "'dones'", "],\n                             dtype=torch.", "float", ").view(-", "1", ", ", "1", ").to(self.device)\n\n        q_values = self.q_net(states).gather(", "1", ", actions)  ", "# Q值", "\n        ", "# 下个状态的最大Q值", "\n        max_next_q_values = self.target_q_net(next_states).", "max", "(", "1", ")[", "0", "].view(\n            -", "1", ", ", "1", ")\n        q_targets = rewards + self.gamma * max_next_q_values * (", "1", " - dones\n                                                                )  ", "# TD误差目标", "\n        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  ", "# 均方误差损失函数", "\n        self.optimizer.zero_grad()  ", "# PyTorch中默认梯度会累积,这里需要显式将梯度置为0", "\n        dqn_loss.backward()  ", "# 反向传播更新参数", "\n        self.optimizer.step()\n\n        ", "if", " self.count % self.target_update == ", "0", ":\n            self.target_q_net.load_state_dict(\n                self.q_net.state_dict())  ", "# 更新目标网络", "\n        self.count += ", "1", "\n\n\nlr = ", "2e-3", "\nnum_episodes = ", "500", "\nhidden_dim = ", "128", "\ngamma = ", "0.98", "\nepsilon = ", "0.01", "\ntarget_update = ", "10", "\nbuffer_size = ", "10000", "\nminimal_size = ", "500", "\nbatch_size = ", "64", "\ndevice = torch.device(", "\"cuda\"", ") ", "if", " torch.cuda.is_available() ", "else", " torch.device(\n    ", "\"cpu\"", ")\n\nenv_name = ", "'CartPole-v1'", "\nenv = gym.make(env_name)\nrandom.seed(", "0", ")\nnp.random.seed(", "0", ")\nenv.seed(", "0", ")\ntorch.manual_seed(", "0", ")\nreplay_buffer = ReplayBuffer(buffer_size)\nstate_dim = env.observation_space.shape[", "0", "]\naction_dim = env.action_space.n\nagent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,\n            target_update, device)\n\nreturn_list = []\n", "for", " i ", "in", " ", "range", "(", "10", "):\n    ", "with", " tqdm(total=", "int", "(num_episodes / ", "10", "), desc=", "'Iteration %d'", " % i) ", "as", " pbar:\n        ", "for", " i_episode ", "in", " ", "range", "(", "int", "(num_episodes / ", "10", ")):\n            episode_return = ", "0", "\n            state = env.reset()\n            done = ", "False", "\n            ", "while", " ", "not", " done:\n                action = agent.take_action(state)\n                next_state, reward, done, _ = env.step(action)\n                replay_buffer.add(state, action, reward, next_state, done)\n                state = next_state\n                episode_return += reward\n                ", "# 当buffer数据的数量超过一定值后,才进行Q网络训练", "\n                ", "if", " replay_buffer.size() > minimal_size:\n                    b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n                    transition_dict = {\n                        ", "'states'", ": b_s,\n                        ", "'actions'", ": b_a,\n                        ", "'next_states'", ": b_ns,\n                        ", "'rewards'", ": b_r,\n                        ", "'dones'", ": b_d\n                    }\n                    agent.update(transition_dict)\n            return_list.append(episode_return)\n            ", "if", " (i_episode + ", "1", ") % ", "10", " == ", "0", ":\n                pbar.set_postfix({\n                    ", "'episode'", ":\n                        ", "'%d'", " % (num_episodes / ", "10", " * i + i_episode + ", "1", "),\n                    ", "'return'", ":\n                        ", "'%.3f'", " % np.mean(return_list[-", "10", ":])\n                })\n            pbar.update(", "1", ")\n\n\n", "\n", "运行结果及报错内容", "\n", "\nTraceback (most recent ", "call", " last):\n  File \"E:\\graduate student\\Python\\Reinforcement learning\\HANDS-ON Reinforcement learning\\04_DQN\\01_DQN.py\", ", "line", " ", "120", ", ", "in", " <module>\n    env.seed(", "0", ")\n  File \"D:\\Python39\\lib\\site-packages\\gym\\core.py\", ", "line", " ", "241", ", ", "in", " __getattr__\n    ", "return", " getattr(self.env, ", "name", ")\n  File \"D:\\Python39\\lib\\site-packages\\gym\\core.py\", ", "line", " ", "241", ", ", "in", " __getattr__\n    ", "return", " getattr(self.env, ", "name", ")\n  File \"D:\\Python39\\lib\\site-packages\\gym\\core.py\", ", "line", " ", "241", ", ", "in", " __getattr__\n    ", "return", " getattr(self.env, ", "name", ")\nAttributeError: ", "'CartPoleEnv'", " ", "object", " has ", "no", " ", "attribute", " ", "'seed'", "\n"]], "Tag": "算法设计"}
{"Answer": "&lt;p&gt;应该是没有读取进来数据。在getstate函数里&amp;#xff0c;for循环没有运行&amp;#xff0c;所以state是None。也可能len(block)刚好等于1&amp;#xff0c;而你又用len(block)-1,所以循环没有进行。我不知道你是不是想用for去枚举block&amp;#xff0c;如果是&amp;#xff0c;应该是for i in range(len(block))&lt;/p&gt;", "Konwledge_Point": "贪婪算法", "Question": ["请各位大神帮帮忙,因为这是毕业论文所用的代码,我想问问到底是否是对的,看看是我代码错了,还是逻辑错了", ["首先我这里有一个yunfuzai_main.py:", "\n\n", "\n", "from dqn_agent import Agent\nfrom model import QNetwork\nimport matplotlib.pyplot as plt\nimport  numpy as np\nimport torch\nimport xlrd\nfrom openpyxl import load_workbook\nimport gc\n\n\nSTATE_SIZE = 10\nEPISODE_COUNT = 1000\n\n# def dqn(n_episodes=EPISODE_COUNT,eps_start=2.0,eps_end=0.03,eps_decay=0.990):\n#     scores = []\n#     for i_episode in range(1, n_episodes + 1):\n#         print(\"Episode\" + str(i_episode))\n#         state = getState(stockData, 0, STATE_SIZE + 1)\n#         total_profit = 0\n#         agent.inventory = []\n#         eps = eps_start\n#\n#         for t in range(l):\n#             action = agent.act(state, eps)\n#             next_state = getState(stockData, t + 1, STATE_SIZE + 1)\n#             reward = 0\n#\n#             if action == 1:  # 买入\n#                 agent.inventory.append(stockData[t])\n#                 # print(\"buy\" + str(stockData[t]))\n#             elif action == 2 and len(agent.inventory) > 0:  # 卖出\n#                 bought_price = agent.inventory.pop(0)\n#                 total_profit += stockData[t] - bought_price\n#                 # reward = max(stockData[t] - bought_price, 0)\n#                 reward = stockData[t] - bought_price\n#                 # print(\"Sell: \" + str(stockData[t]) + \" | Profit: \" + str(stockData[t] - bought_price))\n#             done = 1 if t == l - 1 else 0\n#             agent.step(state, action, reward, next_state, done)\n#             eps = max(eps_end, eps * eps_decay)\n#             state = next_state\n#\n#             # if done:\n#             #     print(\"------------------------------\")\n#             #     print(\"total_profit = \" + str(total_profit))\n#             #     print(\"------------------------------\")\n#         scores.append(total_profit)\n#     return scores\n\ndef dqn1(n_episodes1=EPISODE_COUNT, eps_start1=1.0, eps_end1=0.01, eps_decay1=0.9995):\n    scores1 = []\n    for i_episode in range(1,n_episodes1+1):\n        print(\"Episode\" + str(i_episode))\n\n        state = getState(stockData, 0, STATE_SIZE + 1)\n        agent.inventory = []\n        eps = eps_start1\n        #global loss\n        print(1)\n        for t in range(l):\n            print(2)\n            action = agent.act(state,eps)\n            print(1)\n            next_state = getState(stockData, t + 1, STATE_SIZE + 1)\n            reward = 0\n            done = 1 if t == l - 1 else 0\n            global loss\n            loss = agent.step(state, action, reward, next_state, done)\n\n            print(\"loss\", + str(loss))\n            scores1.append(loss)\n            gc.collect()\n        gc.collect()\n\n    return scores1\n\n            # if action == 1:# 过载\n            #     agent.inventory.append(stockData[t])\n            # elif action == 2 and len(agent.inventory) > 0:\n\n\ndef getState(data, t, n):\n    d = t - n + 1\n    # block = data[d:t + 1] if d>= 0 else -d * [data[0]]+ data[0:t+1]\n    block = data[d:t + 1]\n    #res = [0 for x in range(0, n)]\n    #res = []\n    buffer = []\n    for i in range(len(block) - 1):\n        print(\"res=\",buffer[i])\n        buffer.append(block[i + 1]-block[i])\n        #print(\"res=\",res[i])\n        return np.array([buffer])\n    #return np.array([res])\n\nif __name__== '__main__':\n    print(1)\n    #stockData = []\n    #stockData = []\n    stockData = [None]*801\n    datas1 =xlrd.open_workbook(r'C:\\Users\\86138\\Desktop\\zi_ding_yi.xlsx',{'constant_memory':True})\n\n    #datas1 = xlrd.open_workbook(r'C:\\Users\\86138\\Desktop\\zi_ding_yi.xlsx')\n    print(2)\n    table = datas1.sheets()[0]\n    row_num = 0\n    print(3)\n    # for item in table[1:]:\n    #      stockData.append(float(table.col_values(6)))\n    col = table.col_values(5)\n    while row_num <= 800 :\n        # stockData[row_num] = table.col_values(5,0,row_num)\n        #stockData[row_num] = table.cell_value(row_num,5)\\\n        stockData.append(table.cell_value(row_num,5))\n        row_num += 1\n     # stockData{row_num} = table.col_values(5,0,row_num)\n     # row_num =+1\n\n    agent = Agent(state_size=STATE_SIZE, action_size=3)\n    print(4)\n    l = len(stockData) - 1\n\n    # scores = dqn()\n    scores1 =dqn1()", "\n\n", "它说我的错误是：", "\n\n", "\n", "C:\\Users\\86138\\anaconda3\\python.exe C:/Users/86138/Desktop/stockPrediction-master/yunfuzai_main.py\n1\n2\n3\n4\nEpisode1\n1\n2\nTraceback (most recent call last):\n  File \"C:/Users/86138/Desktop/stockPrediction-master/yunfuzai_main.py\", line 123, in <module>\n    scores1 =dqn1()\n  File \"C:/Users/86138/Desktop/stockPrediction-master/yunfuzai_main.py\", line 61, in dqn1\n    action = agent.act(state,eps)\n  File \"C:\\Users\\86138\\Desktop\\stockPrediction-master\\dqn_agent.py\", line 136, in act\n    state = torch.tensor(state).float.unsqueeze(0).to(device)\nRuntimeError: Could not infer dtype of NoneType\n", "\n\n", "而我的dqn_agent.py:", "\n\n", "\n", "import numpy as np\nimport random\nfrom collections import namedtuple, deque\n\nfrom model import QNetwork\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n#初始化超参数\nBUFFER_SIZE = int(1e5)\n#缓冲去大小，重播缓冲区大小\nBATCH_SIZE = 64\n#批处理大小，最小批量大小，minbatch size\nGAMMA = 0.99\n##折扣率\nTAU = 1e-3\n#用于目标参数的软更新\nLR = 5e-4\n#学习率\nUPDATE_EVERY = 4\n#更新网络的快慢\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#首先看下你的设备有无cuda可用：\n\nclass ReplayBuffer:\n    def __init__(self, action_size, buffer_size, batch_size):# 初始化记忆库\n        self.action_size = action_size\n        self.memory = deque(maxlen=buffer_size)\n        #使用 deque(maxlen=N) 构造函数会创建一个固定大小的队列。当新的元素加入并且这个队列已满的时候，最老的元素会自动被移除掉\n        self.batch_size = batch_size\n        #batch字面上是批量的意思，在深度学习中指的是计算一次cost需要的输入数据个数。\n        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n        #经验池的作用就是把每次进行的游戏回合transition（episode，step）记录下来存储起来。\n        # 在训练的时候则是在经验池中随机取一组transition batch对Q网络进行优化。同时，也需要及时丢掉过老的记录，及时更新。\n        #首先，定义了一个名为Experience的namedtuple。包括内容除了上面算法中提到的（s,a,r,s'）.还有结束的标识’done’。\n    def add(self, state, action, reward, next_state, done):# 向记忆库中加入一个记忆\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n\n    def sample(self):# 随机取出一个minibatch\n        experiences = random.sample(self.memory, k=self.batch_size)\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).float().to(device)\n\n        return (states, actions, rewards, next_states, dones)\n\n    def __len__(self):\n        return len(self.memory)\n\n\nclass Agent:\n\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n\n        # Q-Network\n        #在学习过程中，我们使用两个不相关的Q网络（Q_network_local和Q_network_target）来计算预测值（权重θ）和目标值（权重θ’）。\n        # 经过若干步骤后，目标网络会被冻结，然后拷贝实际的Q网络的权重到目标网络权重。\n        # 冻结目标Q网络一段时间再用实际Q网络的权重更新其权重，可以稳定训练过程\n        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n        self.qnetwork_target = QNetwork(state_size, action_size).to(device)#目标策略，智能体要学习的策略\n        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n        #在线性回归或者监督学习中，我们会计算预测值与真实值之间的差距，也就是loss\n        #在计算得出loss之后，通常会使用Optimizer对所构造的数学模型/网络模型进行参数优化，\n        #通常情况下，优化的最终目的是使得loss趋向于最小。\n\n\n        # Replay Buffer，所以我们设置一个replay_buffer，获得新的交互数据，抛弃旧的数据，\n        # 并且每次从这个replay_buffer中随机取一个batch，来训练我们的系统\n        self.memory = ReplayBuffer(action_size, buffer_size=BUFFER_SIZE,batch_size=BATCH_SIZE)\n        # 初始化迭代步数\n        self.t_step = 0\n        # 初始化持仓\n        self.inventory = []\n    # Experience Replay就是这样的一种技术，在游戏数据的采集过程中，所有的经验数据<script type=\"math/tex\" id=\"MathJax-Element-85\">< s, a, r, s'\n    # ></script>都被存储到一个回放池(replay memory)中。当训练网络时，从回放池中随机地取一小撮数据，\n    # 而不是最新连续的几条转换数据，进行训练。\n\n    def step(self, state, action, reward, next_state, done):\n        # 每一步需要先存储记忆库\n        self.memory.add(state, action, reward, next_state, done)\n\n        # 每隔若干步学习一次\n        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n        if self.t_step == 0:\n            if len(self.memory) > BATCH_SIZE:\n                experience = self.memory.sample()\n                self.learn(experience, GAMMA)\n\n    def learn(self, experience, gamma):\n        # 更新迭代\n        states, actions, rewards, next_states, dones = experience\n\n        # target network:compute and minimize the loss.计算并最小化损失\n        # Get max predicted Q values(for next states) from target model:从目标模型得到最大的预测Q值(下一个状态)\n        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n        # compute Q target for current states:计算当前状态的Q目标。\n        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n\n        Q_expected = self.qnetwork_local(states).gather(1, actions.long())# 固定行号，确认列\n        # Compute loss\n        loss = F.mse_loss(Q_expected, Q_targets)\n        # Minimize the loss\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        # update target network\n        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=TAU)\n        return loss\n\n\n    def soft_update(self, local_model, target_model, tau):\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n    #实现Q'到Q的逼近 use .data and .data.copy#\n    def act(self, state, eps = 0.):\n        #Returns actions for given state as per current policy.\n        #Params\n        # state (array_like): current state\n        # eps (float): epsilon, for epsilon-greedy action selection\n        #参数个数\n        #状态(array_like):当前状态\n        #eps (float):用于epsilon-贪婪动作选择\n\n\n        # 返回动作值orch.unsqueeze()这个函数主要是对数据维度进行扩充\n        #state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        state = torch.tensor(state).float.unsqueeze(0).to(device)\n        self.qnetwork_local.eval()\n        with torch.no_grad():\n            action_values = self.qnetwork_local(state)\n        self.qnetwork_local.train()\n# # Epsilon-greedy action selection\n        if random.random() > eps:\n            return np.argmax(action_values.cpu().data.numpy())\n        else:\n            return random.choice(np.arange(self.action_size))\n        #eps:根据当前策略返回给定状态的操作参数个数", "\n\n", "我也是真没辙了，所以我想问问，到底是怎么回事，因为我真实想法是把那个loss输出来。但似乎不行，想请各位大神帮我看看，因为是毕业论文用的代码，所以就想问问。拜托了"]], "Tag": "算法设计"}
